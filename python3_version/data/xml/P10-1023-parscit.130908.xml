<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000355">
<title confidence="0.956826">
BabelNet: Building a Very Large Multilingual Semantic Network
</title>
<author confidence="0.806319">
Roberto Navigli Simone Paolo Ponzetto
</author>
<affiliation confidence="0.520561">
Dipartimento di Informatica Department of Computational Linguistics
</affiliation>
<address confidence="0.18545">
Sapienza Universit`a di Roma Heidelberg University
</address>
<email confidence="0.87205">
navigli@di.uniroma1.it ponzetto@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.995769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998982916666667">
In this paper we present BabelNet – a
very large, wide-coverage multilingual se-
mantic network. The resource is automat-
ically constructed by means of a method-
ology that integrates lexicographic and en-
cyclopedic knowledge from WordNet and
Wikipedia. In addition Machine Transla-
tion is also applied to enrich the resource
with lexical information for all languages.
We conduct experiments on new and ex-
isting gold-standard datasets to show the
high quality and coverage of the resource.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983704918033">
In many research areas of Natural Language Pro-
cessing (NLP) lexical knowledge is exploited to
perform tasks effectively. These include, among
others, text summarization (Nastase, 2008),
Named Entity Recognition (Bunescu and Pas¸ca,
2006), Question Answering (Harabagiu et al.,
2000) and text categorization (Gabrilovich and
Markovitch, 2006). Recent studies in the diffi-
cult task of Word Sense Disambiguation (Nav-
igli, 2009b, WSD) have shown the impact of the
amount and quality of lexical knowledge (Cuadros
and Rigau, 2006): richer knowledge sources can
be of great benefit to both knowledge-lean systems
(Navigli and Lapata, 2010) and supervised classi-
fiers (Ng and Lee, 1996; Yarowsky and Florian,
2002).
Various projects have been undertaken to make
lexical knowledge available in a machine read-
able format. A pioneering endeavor was Word-
Net (Fellbaum, 1998), a computational lexicon of
English based on psycholinguistic theories. Sub-
sequent projects have also tackled the significant
problem of multilinguality. These include Eu-
roWordNet (Vossen, 1998), MultiWordNet (Pianta
et al., 2002), the Multilingual Central Repository
(Atserias et al., 2004), and many others. How-
ever, manual construction methods inherently suf-
fer from a number of drawbacks. First, maintain-
ing and updating lexical knowledge resources is
expensive and time-consuming. Second, such re-
sources are typically lexicographic, and thus con-
tain mainly concepts and only a few named enti-
ties. Third, resources for non-English languages
often have a much poorer coverage since the con-
struction effort must be repeated for every lan-
guage of interest. As a result, an obvious bias ex-
ists towards conducting research in resource-rich
languages, such as English.
A solution to these issues is to draw upon
a large-scale collaborative resource, namely
Wikipedia1. Wikipedia represents the perfect com-
plement to WordNet, as it provides multilingual
lexical knowledge of a mostly encyclopedic na-
ture. While the contribution of any individual user
might be imprecise or inaccurate, the continual in-
tervention of expert contributors in all domains re-
sults in a resource of the highest quality (Giles,
2005). But while a great deal of work has been re-
cently devoted to the automatic extraction of struc-
tured information from Wikipedia (Wu and Weld,
2007; Ponzetto and Strube, 2007; Suchanek et
al., 2008; Medelyan et al., 2009, inter alia), the
knowledge extracted is organized in a looser way
than in a computational lexicon such as WordNet.
In this paper, we make a major step towards the
vision of a wide-coverage multilingual knowledge
resource. We present a novel methodology that
produces a very large multilingual semantic net-
work: BabelNet. This resource is created by link-
ing Wikipedia to WordNet via an automatic map-
ping and by integrating lexical gaps in resource-
</bodyText>
<footnote confidence="0.9997174">
1http://download.wikipedia.org. We use the
English Wikipedia database dump from November 3, 2009,
which includes 3,083,466 articles. Throughout this paper, we
use Sans Serif for words, SMALL CAPS for Wikipedia pages
and CAPITALS for Wikipedia categories.
</footnote>
<page confidence="0.959762">
216
</page>
<note confidence="0.9448275">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216–225,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.988781">
...world’s first hydrogen balloon flight.
...an interim balloon altitude record...
...from a British balloon near B´ecourt...
...look at the balloon and the...
...suspended like a huge balloon, in...
...the balloon would go up...
</bodyText>
<figure confidence="0.997746846153846">
Machine Translation system
WIKIPEDIA SENTENCES
+
SEMCOR SENTENCES
ballooning
Montgolfier
brothers
Wikipedia WordNet
cluster
Fermi gas
hot-air
balloon
balloon
gas
wind
is-a
has-part
high wind
blow gas
is-a
gasbag
balloonEN, BallonDE,
aerostatoES, globusCA,
pallone aerostaticoIT,
ballonFR, montgolfi`ereFR
BABEL SYNSET
</figure>
<figureCaption confidence="0.999954">
Figure 1: An illustrative overview of BabelNet.
</figureCaption>
<bodyText confidence="0.9998978">
poor languages with the aid of Machine Transla-
tion. The result is an “encyclopedic dictionary”,
that provides concepts and named entities lexical-
ized in many languages and connected with large
amounts of semantic relations.
</bodyText>
<sectionHeader confidence="0.997581" genericHeader="introduction">
2 BabelNet
</sectionHeader>
<bodyText confidence="0.998904068965517">
We encode knowledge as a labeled directed graph
G = (V, E) where V is the set of vertices – i.e.
concepts2 such as balloon – and E C_ V xRxV is
the set of edges connecting pairs of concepts. Each
edge is labeled with a semantic relation from R,
e.g. {is-a, part-of, ..., c}, where c denotes an un-
specified semantic relation. Importantly, each ver-
tex v E V contains a set of lexicalizations of the
concept for different languages, e.g. { balloonEN,
BallonDE, aerostatoES, ... , montgolfi`ereFR }.
Concepts and relations in BabelNet are har-
vested from the largest available semantic lexi-
con of English, WordNet, and a wide-coverage
collaboratively edited encyclopedia, the English
Wikipedia (Section 3.1). We collect (a) from
WordNet, all available word senses (as concepts)
and all the semantic pointers between synsets (as
relations); (b) from Wikipedia, all encyclopedic
entries (i.e. pages, as concepts) and semantically
unspecified relations from hyperlinked text.
In order to provide a unified resource, we merge
the intersection of these two knowledge sources
(i.e. their concepts in common) by establishing a
mapping between Wikipedia pages and WordNet
senses (Section 3.2). This avoids duplicate con-
cepts and allows their inventories of concepts to
complement each other. Finally, to enable mul-
tilinguality, we collect the lexical realizations of
the available concepts in different languages by
</bodyText>
<footnote confidence="0.979069666666667">
2Throughout the paper, unless otherwise stated, we use
the general term concept to denote either a concept or a
named entity.
</footnote>
<bodyText confidence="0.998579473684211">
using (a) the human-generated translations pro-
vided in Wikipedia (the so-called inter-language
links), as well as (b) a machine translation sys-
tem to translate occurrences of the concepts within
sense-tagged corpora, namely SemCor (Miller et
al., 1993) – a corpus annotated with WordNet
senses – and Wikipedia itself (Section 3.3). We
call the resulting set of multilingual lexicalizations
of a given concept a babel synset. An overview of
BabelNet is given in Figure 1 (we label vertices
with English lexicalizations): unlabeled edges are
obtained from links in the Wikipedia pages (e.g.
BALLOON (AIRCRAFT) links to WIND), whereas
labeled ones from WordNet3 (e.g. balloon&apos; has-
part gasbag&apos;). In this paper we restrict ourselves
to concepts lexicalized as nouns. Nonetheless, our
methodology can be applied to all parts of speech,
but in that case Wikipedia cannot be exploited,
since it mainly contains nominal entities.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999845">
3.1 Knowledge Resources
</subsectionHeader>
<bodyText confidence="0.9952721">
WordNet. The most popular lexical knowledge
resource in the field of NLP is certainly WordNet,
a computational lexicon of the English language.
A concept in WordNet is represented as a synonym
set (called synset), i.e. the set of words that share
the same meaning. For instance, the concept wind
is expressed by the following synset:
{ wind&apos;, air current&apos;,, current of air&apos; },
where each word’s subscripts and superscripts in-
dicate their parts of speech (e.g. n stands for noun)
</bodyText>
<footnote confidence="0.998222">
3We use in the following WordNet version 3.0. We de-
note with wP the i-th sense of a word w with part of speech
p. We use word senses to unambiguously denote the corre-
sponding synsets (e.g. planes for { airplanes, aeroplane��,
plane�� }). Hereafter, we use word sense and synset inter-
changeably.
</footnote>
<page confidence="0.998254">
217
</page>
<bodyText confidence="0.99955072">
and sense number, respectively. For each synset,
WordNet provides a textual definition, or gloss.
For example, the gloss of the above synset is: “air
moving from an area of high pressure to an area of
low pressure”.
Wikipedia. Our second resource, Wikipedia,
is a Web-based collaborative encyclopedia. A
Wikipedia page (henceforth, Wikipage) presents
the knowledge about a specific concept (e.g. BAL-
LOON (AIRCRAFT)) or named entity (e.g. MONT-
GOLFIER BROTHERS). The page typically con-
tains hypertext linked to other relevant Wikipages.
For instance, BALLOON (AIRCRAFT) is linked to
WIND, GAS, and so on. The title of a Wikipage
(e.g. BALLOON (AIRCRAFT)) is composed of
the lemma of the concept defined (e.g. balloon)
plus an optional label in parentheses which speci-
fies its meaning if the lemma is ambiguous (e.g.
AIRCRAFT vs. TOY). Wikipages also provide
inter-language links to their counterparts in other
languages (e.g. BALLOON (AIRCRAFT) links to
the Spanish page AEROSTATO). Finally, some
Wikipages are redirections to other pages, e.g.
the Spanish BAL ´ON AEROST ´ATICO redirects to
AEROSTATO.
</bodyText>
<subsectionHeader confidence="0.999892">
3.2 Mapping Wikipedia to WordNet
</subsectionHeader>
<bodyText confidence="0.99994075">
The first phase of our methodology aims to estab-
lish links between Wikipages and WordNet senses.
We aim to acquire a mapping µ such that, for each
Wikipage w, we have:
</bodyText>
<equation confidence="0.973350333333333">
µ(w) = { s E SensesWN(w) if a link can be
established,
E otherwise,
</equation>
<bodyText confidence="0.997124916666667">
where SensesWN(w) is the set of senses of the
lemma of w in WordNet. For example, if our map-
ping methodology linked BALLOON (AIRCRAFT)
to the corresponding WordNet sense balloon&apos;,
we would have µ(BALLOON (AIRCRAFT)) = bal-
loon&apos;.
In order to establish a mapping between the
two resources, we first identify the disambigua-
tion contexts for Wikipages (Section 3.2.1) and
WordNet senses (Section 3.2.2). Next, we inter-
sect these contexts to perform the mapping (see
Section 3.2.3).
</bodyText>
<subsectionHeader confidence="0.533158">
3.2.1 Disambiguation Context of a Wikipage
</subsectionHeader>
<bodyText confidence="0.9920985">
Given a Wikipage w, we use the following infor-
mation as disambiguation context:
</bodyText>
<listItem confidence="0.997132625">
• Sense labels: e.g. given the page BALLOON
(AIRCRAFT), the word aircraft is added to the
disambiguation context.
• Links: the titles’ lemmas of the pages linked
from the target Wikipage (i.e., outgoing links).
For instance, the links in the Wikipage BAL-
LOON (AIRCRAFT) include wind, gas, etc.
• Categories: Wikipages are typically classi-
</listItem>
<bodyText confidence="0.805548916666667">
fied according to one or more categories.
For example, the Wikipage BALLOON (AIR-
CRAFT) is categorized as BALLOONS, BAL-
LOONING, etc. While many categories are
very specific and do not appear in Word-
Net (e.g., SWEDISH WRITERS or SCIEN-
TISTS WHO COMMITTED SUICIDE), we
use their syntactic heads as disambiguation con-
text (i.e. writer and scientist, respectively).
Given a Wikipage w, we define its disambiguation
context Ctx(w) as the set of words obtained from
all of the three sources above.
</bodyText>
<subsectionHeader confidence="0.68685">
3.2.2 Disambiguation Context of a WordNet
Sense
</subsectionHeader>
<bodyText confidence="0.9930905">
Given a WordNet sense s and its synset S, we col-
lect the following information:
</bodyText>
<listItem confidence="0.983738571428572">
• Synonymy: all synonyms of s in S. For in-
stance, given the sense airplane&apos; and its cor-
responding synset { airplane&apos;, aeroplane&apos;,
plane&apos; }, the words contained therein are in-
cluded in the context.
• Hypernymy/Hyponymy: all synonyms in the
synsets H such that H is either a hypernym
(i.e., a generalization) or a hyponym (i.e., a
specialization) of S. For example, given bal-
loon&apos;, we include the words from its hypernym
{ lighter-than-air craft&apos; } and all its hyponyms
(e.g. { hot-air balloon&apos; }).
• Sisterhood: words from the sisters of S. A sis-
ter synset S&apos; is such that S and S&apos; have a com-
mon direct hypernym. For example, given bal-
loon&apos;, it can be found that { balloon&apos; } and
{ airship&apos;, dirigible&apos; } are sisters. Thus air-
ship and dirigible are included in the disam-
biguation context of s.
• Gloss: the set of lemmas of the content words
occurring within the WordNet gloss of S.
</listItem>
<bodyText confidence="0.998327333333333">
We thus define the disambiguation context Ctx(s)
of sense s as the set of words obtained from all of
the four sources above.
</bodyText>
<page confidence="0.990191">
218
</page>
<subsectionHeader confidence="0.939657">
3.2.3 Mapping Algorithm
</subsectionHeader>
<bodyText confidence="0.9999725">
In order to link each Wikipedia page to a WordNet
sense, we perform the following steps:
</bodyText>
<listItem confidence="0.989061">
• Initially, our mapping µ is empty, i.e. it links
each Wikipage w to E.
• For each Wikipage w whose lemma is monose-
mous both in Wikipedia and WordNet we map
w to its only WordNet sense.
• For each remaining Wikipage w for which no
</listItem>
<bodyText confidence="0.9664129">
mapping was previously found (i.e., µ(w) = E),
we assign the most likely sense to w based on
the maximization of the conditional probabili-
ties p(s|w) over the senses s E SensesWN(w)
(no mapping is established if a tie occurs).
To find the mapping of a Wikipage w, we need
to compute the conditional probability p(s|w) of
selecting the WordNet sense s given w. The sense
s which maximizes this probability is determined
as follows:
</bodyText>
<equation confidence="0.99549275">
µ(w) = argmax p(s|w) = argmax p(s, w)
sESensesWNW s p(w)
= argmax p(s, w)
s
</equation>
<bodyText confidence="0.999425166666667">
The latter formula is obtained by observing that
p(w) does not influence our maximization, as it is
a constant independent of s. As a result, determin-
ing the most appropriate sense s consists of find-
ing the sense s that maximizes the joint probability
p(s, w). We estimate p(s, w) as:
</bodyText>
<equation confidence="0.914805">
score(s, w)
p(s, w) =
</equation>
<bodyText confidence="0.999807">
where score(s, w) = |Ctx(s) n Ctx(w) |+ 1 (we
add 1 as a smoothing factor). Thus, in our al-
gorithm we determine the best sense s by com-
puting the intersection of the disambiguation con-
texts of s and w, and normalizing by the scores
summed over all senses of w in Wikipedia and
WordNet. More details on the mapping algorithm
can be found in Ponzetto and Navigli (2010).
</bodyText>
<subsectionHeader confidence="0.999239">
3.3 Translating Babel Synsets
</subsectionHeader>
<bodyText confidence="0.999704372093023">
So far we have linked English Wikipages to Word-
Net senses. Given a Wikipage w, and provided it
is mapped to a sense s (i.e., µ(w) = s), we cre-
ate a babel synset S U W, where S is the WordNet
synset to which sense s belongs, and W includes:
(i) w; (ii) all its inter-language links (that is, trans-
lations of the Wikipage to other languages); (iii)
the redirections to the inter-language links found
in the Wikipedia of the target language. For in-
stance, given that µ(BALLOON) = balloon&apos;, the
corresponding babel synset is { balloonEN, Bal-
lonDE, aerostatoES, bal´on aerost´aticoES, ...,
pallone aerostaticoIT }. However, two issues
arise: first, a concept might be covered only in
one of the two resources (either WordNet or
Wikipedia), meaning that no link can be estab-
lished (e.g., FERMI GAS or gasbag&apos; in Figure
1); second, even if covered in both resources, the
Wikipage for the concept might not provide any
translation for the language of interest (e.g., the
Catalan for BALLOON is missing in Wikipedia).
In order to address the above issues and thus
guarantee high coverage for all languages we de-
veloped a methodology for translating senses in
the babel synset to missing languages. Given a
WordNet word sense in our babel synset of interest
(e.g. balloon&apos;) we collect its occurrences in Sem-
Cor (Miller et al., 1993), a corpus of more than
200,000 words annotated with WordNet senses.
We do the same for Wikipages by retrieving sen-
tences in Wikipedia with links to the Wikipage of
interest. By repeating this step for each English
lexicalization in a babel synset, we obtain a col-
lection of sentences for the babel synset (see left
part of Figure 1). Next, we apply state-of-the-art
Machine Translation4 and translate the set of sen-
tences in all the languages of interest. Given a spe-
cific term in the initial babel synset, we collect the
set of its translations. We then identify the most
frequent translation in each language and add it to
the babel synset. Note that translations are sense-
specific, as the context in which a term occurs is
provided to the translation system.
</bodyText>
<subsectionHeader confidence="0.937888">
3.4 Example
</subsectionHeader>
<bodyText confidence="0.960399692307692">
We now illustrate the execution of our method-
ology by way of an example. Let us focus on
the Wikipage BALLOON (AIRCRAFT). The word
is polysemous both in Wikipedia and WordNet.
In the first phase of our methodology we aim
to find a mapping µ(BALLOON (AIRCRAFT)) to
an appropriate WordNet sense of the word. To
4We use the Google Translate API. An initial prototype
used a statistical machine translation system based on Moses
(Koehn et al., 2007) and trained on Europarl (Koehn, 2005).
However, we found such system unable to cope with many
technical names, such as in the domains of sciences, litera-
ture, history, etc.
</bodyText>
<equation confidence="0.82307475">
,
score(s&apos;, w&apos;)
s&apos;ESensesWN(w),
w&apos;ESensesWiki(w)
</equation>
<page confidence="0.993685">
219
</page>
<bodyText confidence="0.997761857142857">
this end we construct the disambiguation context
for the Wikipage by including words from its la-
bel, links and categories (cf. Section 3.2.1). The
context thus includes, among others, the follow-
ing words: aircraft, wind, airship, lighter-than-
air. We now construct the disambiguation context
for the two WordNet senses of balloon (cf. Sec-
tion 3.2.2), namely the aircraft (#1) and the toy
(#2) senses. To do so, we include words from
their synsets, hypernyms, hyponyms, sisters, and
glosses. The context for balloon1� includes: air-
craft, craft, airship, lighter-than-air. The con-
text for balloon2 contains: toy, doll, hobby. The
sense with the largest intersection is #1, so the
following mapping is established: µ(BALLOON
(AIRCRAFT)) = balloon1�. After the first phase,
our babel synset includes the following English
words from WordNet plus the Wikipedia inter-
language links to other languages (we report Ger-
man, Spanish and Italian): { balloonEN, BallonDE,
aerostatoES, bal´on aerost´aticoES, pallone aero-
staticoIT }.
In the second phase (see Section 3.3), we col-
lect all the sentences in SemCor and Wikipedia in
which the above English word sense occurs. We
translate these sentences with the Google Trans-
late API and select the most frequent transla-
tion in each language. As a result, we can en-
rich the initial babel synset with the following
words: mongolfi`ereFR, globusCA, globoES, mon-
golfieraIT. Note that we had no translation for
Catalan and French in the first phase, because the
inter-language link was not available, and we also
obtain new lexicalizations for the Spanish and Ital-
ian languages.
</bodyText>
<sectionHeader confidence="0.995176" genericHeader="method">
4 Experiment 1: Mapping Evaluation
</sectionHeader>
<bodyText confidence="0.999898928571429">
Experimental setting. We first performed an
evaluation of the quality of our mapping from
Wikipedia to WordNet. To create a gold stan-
dard for evaluation we considered all lemmas
whose senses are contained both in WordNet and
Wikipedia: the intersection between the two re-
sources contains 80,295 lemmas which corre-
spond to 105,797 WordNet senses and 199,735
Wikipedia pages. The average polysemy is 1.3
and 2.5 for WordNet senses and Wikipages, re-
spectively (2.8 and 4.7 when excluding monose-
mous words). We then selected a random sam-
ple of 1,000 Wikipages and asked an annotator
with previous experience in lexicographic annota-
</bodyText>
<table confidence="0.9956585">
P R F1 A
Mapping algorithm 81.9 77.5 79.6 84.4
MFS BL 24.3 47.8 32.2 24.3
Random BL 23.8 46.8 31.6 23.9
</table>
<tableCaption confidence="0.999909">
Table 1: Performance of the mapping algorithm.
</tableCaption>
<bodyText confidence="0.999970628571429">
tion to provide the correct WordNet sense for each
page (an empty sense label was given, if no correct
mapping was possible). The gold-standard dataset
includes 505 non-empty mappings, i.e. Wikipages
with a corresponding WordNet sense. In order to
quantify the quality of the annotations and the dif-
ficulty of the task, a second annotator sense tagged
a subset of 200 pages from the original sample.
Our annotators achieved a K inter-annotator agree-
ment (Carletta, 1996) of 0.9, indicating almost
perfect agreement.
Results and discussion. Table 1 summarizes the
performance of our mapping algorithm against
the manually annotated dataset. Evaluation is per-
formed in terms of standard measures of preci-
sion, recall, and F1-measure. In addition we calcu-
late accuracy, which also takes into account empty
sense labels. As baselines we use the most fre-
quent WordNet sense (MFS), and a random sense
assignment.
The results show that our method achieves al-
most 80% F1 and it improves over the baselines by
a large margin. The final mapping contains 81,533
pairs of Wikipages and word senses they map to,
covering 55.7% of the noun senses in WordNet.
As for the baselines, the most frequent sense is
just 0.6% and 0.4% above the random baseline in
terms of F1 and accuracy, respectively. A x2 test
reveals in fact no statistical significant difference
at p &lt; 0.05. This is related to the random distri-
bution of senses in our dataset and the Wikipedia
unbiased coverage of WordNet senses. So select-
ing the first WordNet sense rather than any other
sense for each target page represents a choice as
arbitrary as picking a sense at random.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="method">
5 Experiment 2: Translation Evaluation
</sectionHeader>
<bodyText confidence="0.9996518">
We perform a second set of experiments concern-
ing the quality of the acquired concepts. This is as-
sessed in terms of coverage against gold-standard
resources (Section 5.1) and against a manually-
validated dataset of translations (Section 5.2).
</bodyText>
<page confidence="0.970269">
220
</page>
<table confidence="0.999860833333333">
Language Word senses Synsets
German 15,762 9,877
Spanish 83,114 55,365
Catalan 64,171 40,466
Italian 57,255 32,156
French 44,265 31,742
</table>
<tableCaption confidence="0.998331">
Table 2: Size of the gold-standard wordnets.
</tableCaption>
<subsectionHeader confidence="0.989414">
5.1 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.999982772727273">
Datasets. We compare BabelNet against gold-
standard resources for 5 languages, namely: the
subset of GermaNet (Lemnitzer and Kunze, 2002)
included in EuroWordNet for German, Multi-
WordNet (Pianta et al., 2002) for Italian, the Mul-
tilingual Central Repository for Spanish and Cata-
lan (Atserias et al., 2004), and WOrdnet Libre
du Franc¸ais (Benoit and Fiˇser, 2008, WOLF) for
French. In Table 2 we report the number of synsets
and word senses available in the gold-standard re-
sources for the 5 languages.
Measures. Let B be BabelNet, F our gold-
standard non-English wordnet (e.g. GermaNet),
and let E be the English WordNet. All the gold-
standard non-English resources, as well as Babel-
Net, are linked to the English WordNet: given a
synset SF ∈ F, we denote its corresponding babel
synset as SB and its synset in the English Word-
Net as SE. We assess the coverage of BabelNet
against our gold-standard wordnets both in terms
of synsets and word senses. For synsets, we calcu-
late coverage as follows:
</bodyText>
<equation confidence="0.59339">
SynsetCov(B, F) = ES ∈F δ(SB, SF)
|{SF ∈ F} |,
</equation>
<bodyText confidence="0.9999705">
where δ(SB, SF) = 1 if the two synsets SB and
SF have a synonym in common, 0 otherwise. That
is, synset coverage is determined as the percentage
of synsets of F that share a term with the corre-
sponding babel synsets. For word senses we cal-
culate a similar measure of coverage:
</bodyText>
<equation confidence="0.9706705">
WordCov(B, F) = Ess F ElI∈S, δ0 (sF, SB)
{F ∈ SF : SF ∈ F} |,
</equation>
<bodyText confidence="0.990412854545455">
where sF is a word sense in synset SF and
δ0(sF, SB) = 1 if sF ∈ SB, 0 otherwise. That
is we calculate the ratio of word senses in our
gold-standard resource F that also occur in the
corresponding synset SB to the overall number of
senses in F.
However, our gold-standard resources cover
only a portion of the English WordNet, whereas
the overall coverage of BabelNet is much higher.
We calculate extra coverage for synsets as follows:
SynsetExtraCov(B, F) ES� _ E∈E\ 6(SB, SE)
Similarly, we calculate extra coverage for word
senses in BabelNet corresponding to WordNet
synsets not covered by the reference resource F.
Results and discussion. We evaluate the cov-
erage and extra coverage of word senses and
synsets at different stages: (a) using only the inter-
language links from Wikipedia (WIKI Links); (b)
and (c) using only the automatic translations of the
sentences from Wikipedia (WIKI Transl.) or Sem-
Cor (WN Transl.); (d) using all available transla-
tions, i.e. BABELNET.
Coverage results are reported in Table 3. The
percentage of word senses covered by BabelNet
ranges from 52.9% (Italian) to 66.4 (Spanish)
and 86.0% (French). Synset coverage ranges from
73.3% (Catalan) to 76.6% (Spanish) and 92.9%
(French). As expected, synset coverage is higher,
because a synset in the reference resource is con-
sidered to be covered if it shares at least one word
with the corresponding synset in BabelNet.
Numbers for the extra coverage, which pro-
vides information about the percentage of word
senses and synsets in BabelNet but not in the gold-
standard resources, are given in Figure 2. The re-
sults show that we provide for all languages a high
extra coverage for both word senses – between
340.1% (Catalan) and 2,298% (German) – and
synsets – between 102.8% (Spanish) and 902.6%
(German).
Table 3 and Figure 2 show that the best results
are obtained when combining all available trans-
lations, i.e. both from Wikipedia and the machine
translation system. The performance figures suf-
fer from the errors of the mapping phase (see Sec-
tion 4). Nonetheless, the results are generally high,
with a peak for French, since WOLF has been cre-
ated semi-automatically by combining several re-
sources, including Wikipedia. The relatively low
word sense coverage for Italian (55.4%) is, in-
stead, due to the lack of many common words in
the Italian gold-standard synsets. Examples in-
clude whipEN translated as staffileIT but not as the
more common frustaIT, playboyEN translated as
vitaioloIT but not gigol`oIT, etc.
</bodyText>
<equation confidence="0.603479">
|{SF ∈ F} |.
</equation>
<page confidence="0.827425">
221
</page>
<figure confidence="0.999777428571429">
2500%
2000%
1500%
1000%
500%
0%
Wiki Links
Wiki Transl.
WN Transl.
BabelNet
German Spanish Catalan Italian French
1000%
900%
800%
700%
600%
500%
400%
300%
200%
100%
0%
Wiki Links
Wiki Transl.
WN Transl.
BabelNet
German Spanish Catalan Italian French
(a) word senses (b) synsets
</figure>
<figureCaption confidence="0.999746">
Figure 2: Extra coverage against gold-standard wordnets: word senses (a) and synsets (b).
</figureCaption>
<table confidence="0.999924576923077">
Resource Method SENSES SYNSETS
German � Links 39.6 50.7
WIKI 42.6 58.2
Transl. 21.0 28.6
WN Transl. 57.6 73.4
BABELNET All
Spanish � Links 34.4 40.7
WIKI 47.9 56.1
Transl. 25.2 30.0
WN Transl. 66.4 76.6
BABELNET All
Catalan � Links 20.3 25.2
WIKI 46.9 54.1
Transl. 25.0 29.6
WN Transl. 64.0 73.3
BABELNET All
Italian � Links 28.1 40.0
WIKI 39.9 58.0
Transl. 19.7 28.7
WN Transl. 52.9 73.7
BABELNET All
French � Links 70.0 72.4
WIKI 69.6 79.6
Transl. 16.3 19.4
WN Transl. 86.0 92.9
BABELNET All
</table>
<tableCaption confidence="0.940919">
Table 3: Coverage against gold-standard wordnets
(we report percentages).
</tableCaption>
<subsectionHeader confidence="0.999676">
5.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.999899574468085">
Experimental setup. The automatic evaluation
quantifies how much of the gold-standard re-
sources is covered by BabelNet. However, it
does not say anything about the precision of the
additional lexicalizations provided by BabelNet.
Given that our resource has displayed a remark-
ably high extra coverage – ranging from 340%
to 2,298% of the national wordnets (see Figure
2) – we performed a second evaluation to assess
its precision. For each of our 5 languages, we
selected a random set of 600 babel synsets com-
posed as follows: 200 synsets whose senses ex-
ist in WordNet only, 200 synsets in the intersec-
tion between WordNet and Wikipedia (i.e. those
mapped with our method illustrated in Section
3.2), 200 synsets whose lexicalizations exist in
Wikipedia only. Therefore, our dataset included
600 x 5 = 3,000 babel synsets. None of the synsets
was covered by any of the five reference wordnets.
The babel synsets were manually validated by ex-
pert annotators who decided which senses (i.e.
lexicalizations) were appropriate given the corre-
sponding WordNet gloss and/or Wikipage.
Results and discussion. We report the results in
Table 4. For each language (rows) and for each
of the three regions of BabelNet (columns), we
report precision (i.e. the percentage of synonyms
deemed correct) and, in parentheses, the over-
all number of synonyms evaluated. The results
show that the different regions of BabelNet con-
tain translations of different quality: while on av-
erage translations for WordNet-only synsets have
a precision around 72%, when Wikipedia comes
into play the performance increases considerably
(around 80% in the intersection and 95% with
Wikipedia-only translations). As can be seen from
the figures in parentheses, the number of trans-
lations available in the presence of Wikipedia is
higher. This quantitative difference is due to our
method collecting many translations from the redi-
rections in the Wikipedia of the target language
(Section 3.3), as well as to the paucity of examples
in SemCor for many synsets. In addition, some of
the synsets in WordNet with no Wikipedia coun-
terpart are very difficult to translate. Examples
include terms like stammel, crape fern, base-
ball clinic, and many others for which we could
</bodyText>
<page confidence="0.992822">
222
</page>
<table confidence="0.999595166666667">
Language WN WN n Wiki Wiki
German 73.76 (282) 78.37 (777) 97.74 (709)
Spanish 69.45 (275) 78.53 (643) 92.46 (703)
Catalan 75.58 (258) 82.98 (517) 92.71 (398)
Italian 72.32 (271) 80.83 (574) 99.09 (552)
French 67.16 (268) 77.43 (709) 96.44 (758)
</table>
<tableCaption confidence="0.997153">
Table 4: Precision of BabelNet on synonyms in
</tableCaption>
<bodyText confidence="0.895855888888889">
WordNet (WN), Wikipedia (Wiki) and their inter-
section (WN n Wiki): percentage and total num-
ber of words (in parentheses) are reported.
not find translations in major editions of bilingual
dictionaries. In contrast, good translations were
produced using our machine translation method
when enough sentences were available. Examples
are: chaudr´ee de poissonFR forfish chowderEN,
grano de caf´eES for coffee beanEN, etc.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999971835820896">
Previous attempts to manually build multilingual
resources have led to the creation of a multi-
tude of wordnets such as EuroWordNet (Vossen,
1998), MultiWordNet (Pianta et al., 2002), Balka-
Net (Tufis¸ et al., 2004), Arabic WordNet (Black
et al., 2006), the Multilingual Central Repository
(Atserias et al., 2004), bilingual electronic dic-
tionaries such as EDR (Yokoi, 1995), and fully-
fledged frameworks for the development of multi-
lingual lexicons (Lenci et al., 2000). As it is of-
ten the case with manually assembled resources,
these lexical knowledge repositories are hindered
by high development costs and an insufficient cov-
erage. This barrier has led to proposals that ac-
quire multilingual lexicons from either parallel
text (Gale and Church, 1993; Fung, 1995, inter
alia) or monolingual corpora (Sammer and Soder-
land, 2007; Haghighi et al., 2008). The disam-
biguation of bilingual dictionary glosses has also
been proposed to create a bilingual semantic net-
work from a machine readable dictionary (Nav-
igli, 2009a). Recently, Etzioni et al. (2007) and
Mausam et al. (2009) presented methods to pro-
duce massive multilingual translation dictionaries
from Web resources such as online lexicons and
Wiktionaries. However, while providing lexical
resources on a very large scale for hundreds of
thousands of language pairs, these do not encode
semantic relations between concepts denoted by
their lexical entries.
The research closest to ours is presented by de
Melo and Weikum (2009), who developed a Uni-
versal WordNet (UWN) by automatically acquir-
ing a semantic network for languages other than
English. UWN is bootstrapped from WordNet and
is built by collecting evidence extracted from ex-
isting wordnets, translation dictionaries, and par-
allel corpora. The result is a graph containing
800,000 words from over 200 languages in a hier-
archically structured semantic network with over
1.5 million links from words to word senses. Our
work goes one step further by (1) developing an
even larger multilingual resource including both
lexical semantic and encyclopedic knowledge, (2)
enriching the structure of the ‘core’ semantic net-
work (i.e. the semantic pointers from WordNet)
with topical, semantically unspecified relations
from the link structure of Wikipedia. This result
is essentially achieved by complementing Word-
Net with Wikipedia, as well as by leveraging the
multilingual structure of the latter. Previous at-
tempts at linking the two resources have been pro-
posed. These include associating Wikipedia pages
with the most frequent WordNet sense (Suchanek
et al., 2008), extracting domain information from
Wikipedia and providing a manual mapping to
WordNet concepts (Auer et al., 2007), a model
based on vector spaces (Ruiz-Casado et al., 2005),
a supervised approach using keyword extraction
(Reiter et al., 2008), as well as automatically
linking Wikipedia categories to WordNet based
on structural information (Ponzetto and Navigli,
2009). In contrast to previous work, BabelNet
is the first proposal that integrates the relational
structure of WordNet with the semi-structured in-
formation from Wikipedia into a unified, wide-
coverage, multilingual semantic network.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99859075">
In this paper we have presented a novel methodol-
ogy for the automatic construction of a large multi-
lingual lexical knowledge resource. Key to our ap-
proach is the establishment of a mapping between
a multilingual encyclopedic knowledge repository
(Wikipedia) and a computational lexicon of En-
glish (WordNet). This integration process has
several advantages. Firstly, the two resources
contribute different kinds of lexical knowledge,
one is concerned mostly with named entities, the
other with concepts. Secondly, while Wikipedia
is less structured than WordNet, it provides large
</bodyText>
<page confidence="0.995377">
223
</page>
<bodyText confidence="0.999925387755102">
amounts of semantic relations and can be lever-
aged to enable multilinguality. Thus, even when
they overlap, the two resources provide comple-
mentary information about the same named enti-
ties or concepts. Further, we contribute a large
set of sense occurrences harvested from Wikipedia
and SemCor, a corpus that we input to a state-of-
the-art machine translation system to fill in the gap
between resource-rich languages – such as English
– and resource-poorer ones. Our hope is that the
availability of such a language-rich resource5 will
enable many non-English and multilingual NLP
applications to be developed.
Our experiments show that our fully-automated
approach produces a large-scale lexical resource
with high accuracy. The resource includes millions
of semantic relations, mainly from Wikipedia
(however, WordNet relations are labeled), and
contains almost 3 million concepts (6.7 labels per
concept on average). As pointed out in Section
5, such coverage is much wider than that of ex-
isting wordnets in non-English languages. While
BabelNet currently includes 6 languages, links to
freely-available wordnets6 can immediately be es-
tablished by utilizing the English WordNet as an
interlanguage index. Indeed, BabelNet can be ex-
tended to virtually any language of interest. In
fact, our translation method allows it to cope with
any resource-poor language.
As future work, we plan to apply our method
to other languages, including Eastern European,
Arabic, and Asian languages. We also intend to
link missing concepts in WordNet, by establish-
ing their most likely hypernyms – e.g., a` la Snow
et al. (2006). We will perform a semi-automatic
validation of BabelNet, e.g. by exploiting Ama-
zon’s Mechanical Turk (Callison-Burch, 2009) or
designing a collaborative game (von Ahn, 2006)
to validate low-ranking mappings and translations.
Finally, we aim to apply BabelNet to a variety of
applications which are known to benefit from a
wide-coverage knowledge resource. We have al-
ready shown that the English-only subset of Ba-
belNet allows simple knowledge-based algorithms
to compete with supervised systems in standard
coarse-grained and domain-specific WSD settings
(Ponzetto and Navigli, 2010). We plan in the near
future to apply BabelNet to the challenging task of
cross-lingual WSD (Lefever and Hoste, 2009).
</bodyText>
<footnote confidence="0.999799666666667">
5BabelNet can be freely downloaded for research pur-
poses at http://lcl.uniroma1.it/babelnet.
6http://www.globalwordnet.org.
</footnote>
<sectionHeader confidence="0.95587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999183087719298">
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The MEANING multilingual central
repository. In Proc. of GWC-04, pages 80–210.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. Dbpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722–735.
Sagot Benoit and Darja Fiˇser. 2008. Building a free
French WordNet from multilingual resources. In
Proceedings of the Ontolex 2008 Workshop.
William Black, Sabri Elkateb Horacio Rodriguez,
Musa Alkhalifa, Piek Vossen, and Adam Pease.
2006. Introducing the Arabic WordNet project. In
Proc. of GWC-06, pages 295–299.
Razvan Bunescu and Marius Pas¸ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9–16.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In Proc. of EMNLP-09, pages 286–
295.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249–254.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proc. of EMNLP-06, pages 534–541.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proc. of CIKM-09, pages 513–522.
Oren Etzioni, Kobi Reiter, Stephen Soderland, and
Marcus Sammer. 2007. Lexical translation with ap-
plication to image search on the Web. In Proceed-
ings of Machine Translation Summit XI.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proc. of ACL-95, pages
236–243.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
Wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proc. of AAAI-06,
pages 1301–1306.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75–102.
Jim Giles. 2005. Internet encyclopedias go head to
head. Nature, 438:900–901.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08,
pages 771–779.
</reference>
<page confidence="0.980712">
224
</page>
<reference confidence="0.999898279661017">
Sanda M. Harabagiu, Dan Moldovan, Marius Pas¸ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu.
2000. FALCON: Boosting knowledge for answer
engines. In Proc. of TREC-9, pages 479–488.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Comp. Vol. to Proc. of ACL-07, pages 177–180.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2009. Semeval-
2010 task 3: Cross-lingual Word Sense Disambigua-
tion. In Proc. of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82–87, Boulder, Colorado.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet – representation, visualization, application.
In Proc. of LREC ’02, pages 1485–1491.
Alessandro Lenci, Nuria Bel, Federica Busa, Nico-
letta Calzolari, Elisabetta Gola, Monica Monachini,
Antoine Ogonowski, Ivonne Peters, Wim Peters,
Nilda Ruimy, Marta Villegas, and Antonio Zam-
polli. 2000. SIMPLE: A general framework for the
development of multilingual lexicons. International
Journal of Lexicography, 13(4):249–263.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262–270.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716–
754.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross Bunker. 1993. A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human
Language Technology, pages 303–308, Plainsboro,
N.J.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
activation spreading. In Proc. of EMNLP-08, pages
763–772.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study on graph connectivity for unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Anaylsis and Machine Intelligence,
32(4):678–692.
Roberto Navigli. 2009a. Using cycles and quasi-
cycles to disambiguate dictionary glosses. In Proc.
of EACL-09, pages 594–602.
Roberto Navigli. 2009b. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1–69.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In Proc. of
ACL-96, pages 40–47.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing an aligned
multilingual database. In Proc. of GWC-02, pages
21–25.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proc. of IJCAI-09,
pages 2083–2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rival-
ing supervised system. In Proc. of ACL-10.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440–1445.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381–387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lecture
Notes in Computer Science. Springer Verlag.
Marcus Sammer and Stephen Soderland. 2007. Build-
ing a sense-distinguished multilingual lexicon from
monolingual corpora and bilingual lexicons. In Pro-
ceedings of Machine Translation Summit XI.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proc. of COLING-ACL-06, pages 801–
808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203–217.
Dan Tufis¸, Dan Cristea, and Sofia Stamou. 2004.
BalkaNet: Aims, methods, results and perspectives.
a general overview. Romanian Journal on Science
and Technology of Information, 7(1-2):9–43.
Luis von Ahn. 2006. Games with a purpose. IEEE
Computer, 6(39):92–94.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Fei Wu and Daniel Weld. 2007. Automatically se-
mantifying Wikipedia. In Proc. of CIKM-07, pages
41–50.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation across diverse parameter
spaces. Natural Language Engineering, 9(4):293–
310.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42–44.
</reference>
<page confidence="0.99882">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486478">
<title confidence="0.999716">BabelNet: Building a Very Large Multilingual Semantic Network</title>
<author confidence="0.999712">Roberto Navigli Simone Paolo Ponzetto</author>
<affiliation confidence="0.9985085">Dipartimento di Informatica Department of Computational Linguistics Sapienza Universit`a di Roma Heidelberg University</affiliation>
<address confidence="0.491335">navigli@di.uniroma1.it ponzetto@cl.uni-heidelberg.de</address>
<abstract confidence="0.999339230769231">In this paper we present BabelNet – a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Luis Villarejo</author>
<author>German Rigau</author>
<author>Eneko Agirre</author>
<author>John Carroll</author>
<author>Bernardo Magnini</author>
<author>Piek Vossen</author>
</authors>
<title>The MEANING multilingual central repository.</title>
<date>2004</date>
<booktitle>In Proc. of GWC-04,</booktitle>
<pages>80--210</pages>
<contexts>
<context position="1935" citStr="Atserias et al., 2004" startWordPosition="271" endWordPosition="274"> knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. However, manual construction methods inherently suffer from a number of drawbacks. First, maintaining and updating lexical knowledge resources is expensive and time-consuming. Second, such resources are typically lexicographic, and thus contain mainly concepts and only a few named entities. Third, resources for non-English languages often have a much poorer coverage since the construction effort must be repeated for every language of interest. As a result, an obvious bias exists towards conducting research in resource-rich languages, such as English. A solution to these issu</context>
<context position="21270" citStr="Atserias et al., 2004" startWordPosition="3441" endWordPosition="3444">old-standard resources (Section 5.1) and against a manuallyvalidated dataset of translations (Section 5.2). 220 Language Word senses Synsets German 15,762 9,877 Spanish 83,114 55,365 Catalan 64,171 40,466 Italian 57,255 32,156 French 44,265 31,742 Table 2: Size of the gold-standard wordnets. 5.1 Automatic Evaluation Datasets. We compare BabelNet against goldstandard resources for 5 languages, namely: the subset of GermaNet (Lemnitzer and Kunze, 2002) included in EuroWordNet for German, MultiWordNet (Pianta et al., 2002) for Italian, the Multilingual Central Repository for Spanish and Catalan (Atserias et al., 2004), and WOrdnet Libre du Franc¸ais (Benoit and Fiˇser, 2008, WOLF) for French. In Table 2 we report the number of synsets and word senses available in the gold-standard resources for the 5 languages. Measures. Let B be BabelNet, F our goldstandard non-English wordnet (e.g. GermaNet), and let E be the English WordNet. All the goldstandard non-English resources, as well as BabelNet, are linked to the English WordNet: given a synset SF ∈ F, we denote its corresponding babel synset as SB and its synset in the English WordNet as SE. We assess the coverage of BabelNet against our gold-standard wordnet</context>
<context position="29092" citStr="Atserias et al., 2004" startWordPosition="4744" endWordPosition="4747">e reported. not find translations in major editions of bilingual dictionaries. In contrast, good translations were produced using our machine translation method when enough sentences were available. Examples are: chaudr´ee de poissonFR forfish chowderEN, grano de caf´eES for coffee beanEN, etc. 6 Related Work Previous attempts to manually build multilingual resources have led to the creation of a multitude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has als</context>
</contexts>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>Jordi Atserias, Luis Villarejo, German Rigau, Eneko Agirre, John Carroll, Bernardo Magnini, and Piek Vossen. 2004. The MEANING multilingual central repository. In Proc. of GWC-04, pages 80–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ive</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC</booktitle>
<pages>722--735</pages>
<contexts>
<context position="31474" citStr="Auer et al., 2007" startWordPosition="5110" endWordPosition="5113">(2) enriching the structure of the ‘core’ semantic network (i.e. the semantic pointers from WordNet) with topical, semantically unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilin</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ive, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ive. 2007. Dbpedia: A nucleus for a web of open data. In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC 2007), pages 722–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sagot Benoit</author>
<author>Darja Fiˇser</author>
</authors>
<title>Building a free French WordNet from multilingual resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the Ontolex</booktitle>
<note>Workshop.</note>
<marker>Benoit, Fiˇser, 2008</marker>
<rawString>Sagot Benoit and Darja Fiˇser. 2008. Building a free French WordNet from multilingual resources. In Proceedings of the Ontolex 2008 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Black</author>
<author>Sabri Elkateb Horacio Rodriguez</author>
<author>Musa Alkhalifa</author>
<author>Piek Vossen</author>
<author>Adam Pease</author>
</authors>
<title>Introducing the Arabic WordNet project.</title>
<date>2006</date>
<booktitle>In Proc. of GWC-06,</booktitle>
<pages>295--299</pages>
<contexts>
<context position="29031" citStr="Black et al., 2006" startWordPosition="4736" endWordPosition="4739">: percentage and total number of words (in parentheses) are reported. not find translations in major editions of bilingual dictionaries. In contrast, good translations were produced using our machine translation method when enough sentences were available. Examples are: chaudr´ee de poissonFR forfish chowderEN, grano de caf´eES for coffee beanEN, etc. 6 Related Work Previous attempts to manually build multilingual resources have led to the creation of a multitude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008</context>
</contexts>
<marker>Black, Rodriguez, Alkhalifa, Vossen, Pease, 2006</marker>
<rawString>William Black, Sabri Elkateb Horacio Rodriguez, Musa Alkhalifa, Piek Vossen, and Adam Pease. 2006. Introducing the Arabic WordNet project. In Proc. of GWC-06, pages 295–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>9--16</pages>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proc. of EACL-06, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP-09,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="34294" citStr="Callison-Burch, 2009" startWordPosition="5537" endWordPosition="5538"> can immediately be established by utilizing the English WordNet as an interlanguage index. Indeed, BabelNet can be extended to virtually any language of interest. In fact, our translation method allows it to cope with any resource-poor language. As future work, we plan to apply our method to other languages, including Eastern European, Arabic, and Asian languages. We also intend to link missing concepts in WordNet, by establishing their most likely hypernyms – e.g., a` la Snow et al. (2006). We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon’s Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. Finally, we aim to apply BabelNet to a variety of applications which are known to benefit from a wide-coverage knowledge resource. We have already shown that the English-only subset of BabelNet allows simple knowledge-based algorithms to compete with supervised systems in standard coarse-grained and domain-specific WSD settings (Ponzetto and Navigli, 2010). We plan in the near future to apply BabelNet to the challenging task of cross-lingual WSD (Lefever and Hoste, 2009). 5BabelNet can be free</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In Proc. of EMNLP-09, pages 286– 295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="19313" citStr="Carletta, 1996" startWordPosition="3126" endWordPosition="3127"> A Mapping algorithm 81.9 77.5 79.6 84.4 MFS BL 24.3 47.8 32.2 24.3 Random BL 23.8 46.8 31.6 23.9 Table 1: Performance of the mapping algorithm. tion to provide the correct WordNet sense for each page (an empty sense label was given, if no correct mapping was possible). The gold-standard dataset includes 505 non-empty mappings, i.e. Wikipages with a corresponding WordNet sense. In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample. Our annotators achieved a K inter-annotator agreement (Carletta, 1996) of 0.9, indicating almost perfect agreement. Results and discussion. Table 1 summarizes the performance of our mapping algorithm against the manually annotated dataset. Evaluation is performed in terms of standard measures of precision, recall, and F1-measure. In addition we calculate accuracy, which also takes into account empty sense labels. As baselines we use the most frequent WordNet sense (MFS), and a random sense assignment. The results show that our method achieves almost 80% F1 and it improves over the baselines by a large margin. The final mapping contains 81,533 pairs of Wikipages </context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>Quality assessment of large scale knowledge resources.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06,</booktitle>
<pages>534--541</pages>
<contexts>
<context position="1305" citStr="Cuadros and Rigau, 2006" startWordPosition="180" endWordPosition="183">sting gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repo</context>
</contexts>
<marker>Cuadros, Rigau, 2006</marker>
<rawString>Montse Cuadros and German Rigau. 2006. Quality assessment of large scale knowledge resources. In Proc. of EMNLP-06, pages 534–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Towards a universal wordnet by learning from combined evidence.</title>
<date>2009</date>
<booktitle>In Proc. of CIKM-09,</booktitle>
<pages>513--522</pages>
<marker>de Melo, Weikum, 2009</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2009. Towards a universal wordnet by learning from combined evidence. In Proc. of CIKM-09, pages 513–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Kobi Reiter</author>
<author>Stephen Soderland</author>
<author>Marcus Sammer</author>
</authors>
<title>Lexical translation with application to image search on the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="29831" citStr="Etzioni et al. (2007)" startWordPosition="4860" endWordPosition="4863">ilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research closest to ours is presented by de Melo and Weikum (2009), who developed a Universal WordNet (UWN) by automatically acquiring a semantic network for languages other than English. UWN is bootstrapped from WordNet and is built </context>
</contexts>
<marker>Etzioni, Reiter, Soderland, Sammer, 2007</marker>
<rawString>Oren Etzioni, Kobi Reiter, Stephen Soderland, and Marcus Sammer. 2007. Lexical translation with application to image search on the Web. In Proceedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A pattern matching method for finding noun and proper noun translations from noisy parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of ACL-95,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="29544" citStr="Fung, 1995" startWordPosition="4817" endWordPosition="4818">ordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations be</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Pascale Fung. 1995. A pattern matching method for finding noun and proper noun translations from noisy parallel corpora. In Proc. of ACL-95, pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge.</title>
<date>2006</date>
<booktitle>In Proc. of AAAI-06,</booktitle>
<pages>1301--1306</pages>
<contexts>
<context position="1121" citStr="Gabrilovich and Markovitch, 2006" startWordPosition="149" endWordPosition="152">ic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subse</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2006</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2006. Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge. In Proc. of AAAI-06, pages 1301–1306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="29532" citStr="Gale and Church, 1993" startWordPosition="4813" endWordPosition="4816"> (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic </context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Giles</author>
</authors>
<title>Internet encyclopedias go head to head.</title>
<date>2005</date>
<journal>Nature,</journal>
<pages>438--900</pages>
<contexts>
<context position="2949" citStr="Giles, 2005" startWordPosition="431" endWordPosition="432">truction effort must be repeated for every language of interest. As a result, an obvious bias exists towards conducting research in resource-rich languages, such as English. A solution to these issues is to draw upon a large-scale collaborative resource, namely Wikipedia1. Wikipedia represents the perfect complement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic nature. While the contribution of any individual user might be imprecise or inaccurate, the continual intervention of expert contributors in all domains results in a resource of the highest quality (Giles, 2005). But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet. In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet v</context>
</contexts>
<marker>Giles, 2005</marker>
<rawString>Jim Giles. 2005. Internet encyclopedias go head to head. Nature, 438:900–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="29632" citStr="Haghighi et al., 2008" startWordPosition="4829" endWordPosition="4832"> (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research closest to ours is present</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pas¸ca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>FALCON: Boosting knowledge for answer engines.</title>
<date>2000</date>
<booktitle>In Proc. of TREC-9,</booktitle>
<pages>479--488</pages>
<marker>Harabagiu, Moldovan, Pas¸ca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2000</marker>
<rawString>Sanda M. Harabagiu, Dan Moldovan, Marius Pas¸ca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2000. FALCON: Boosting knowledge for answer engines. In Proc. of TREC-9, pages 479–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Comp. Vol. to Proc. of ACL-07,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="16196" citStr="Koehn et al., 2007" startWordPosition="2624" endWordPosition="2627">h language and add it to the babel synset. Note that translations are sensespecific, as the context in which a term occurs is provided to the translation system. 3.4 Example We now illustrate the execution of our methodology by way of an example. Let us focus on the Wikipage BALLOON (AIRCRAFT). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON (AIRCRAFT)) to an appropriate WordNet sense of the word. To 4We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. , score(s&apos;, w&apos;) s&apos;ESensesWN(w), w&apos;ESensesWiki(w) 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Comp. Vol. to Proc. of ACL-07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation Summit X.</booktitle>
<contexts>
<context position="16234" citStr="Koehn, 2005" startWordPosition="2632" endWordPosition="2633">te that translations are sensespecific, as the context in which a term occurs is provided to the translation system. 3.4 Example We now illustrate the execution of our methodology by way of an example. Let us focus on the Wikipage BALLOON (AIRCRAFT). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON (AIRCRAFT)) to an appropriate WordNet sense of the word. To 4We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. , score(s&apos;, w&apos;) s&apos;ESensesWN(w), w&apos;ESensesWiki(w) 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, w</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>Semeval2010 task 3: Cross-lingual Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>82--87</pages>
<location>Boulder, Colorado.</location>
<marker>Lefever, Hoste, 2009</marker>
<rawString>Els Lefever and Veronique Hoste. 2009. Semeval2010 task 3: Cross-lingual Word Sense Disambiguation. In Proc. of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 82–87, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lothar Lemnitzer</author>
<author>Claudia Kunze</author>
</authors>
<title>GermaNet – representation, visualization, application.</title>
<date>2002</date>
<booktitle>In Proc. of LREC ’02,</booktitle>
<pages>1485--1491</pages>
<contexts>
<context position="21102" citStr="Lemnitzer and Kunze, 2002" startWordPosition="3414" endWordPosition="3417">Experiment 2: Translation Evaluation We perform a second set of experiments concerning the quality of the acquired concepts. This is assessed in terms of coverage against gold-standard resources (Section 5.1) and against a manuallyvalidated dataset of translations (Section 5.2). 220 Language Word senses Synsets German 15,762 9,877 Spanish 83,114 55,365 Catalan 64,171 40,466 Italian 57,255 32,156 French 44,265 31,742 Table 2: Size of the gold-standard wordnets. 5.1 Automatic Evaluation Datasets. We compare BabelNet against goldstandard resources for 5 languages, namely: the subset of GermaNet (Lemnitzer and Kunze, 2002) included in EuroWordNet for German, MultiWordNet (Pianta et al., 2002) for Italian, the Multilingual Central Repository for Spanish and Catalan (Atserias et al., 2004), and WOrdnet Libre du Franc¸ais (Benoit and Fiˇser, 2008, WOLF) for French. In Table 2 we report the number of synsets and word senses available in the gold-standard resources for the 5 languages. Measures. Let B be BabelNet, F our goldstandard non-English wordnet (e.g. GermaNet), and let E be the English WordNet. All the goldstandard non-English resources, as well as BabelNet, are linked to the English WordNet: given a synset </context>
</contexts>
<marker>Lemnitzer, Kunze, 2002</marker>
<rawString>Lothar Lemnitzer and Claudia Kunze. 2002. GermaNet – representation, visualization, application. In Proc. of LREC ’02, pages 1485–1491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Nuria Bel</author>
<author>Federica Busa</author>
<author>Nicoletta Calzolari</author>
<author>Elisabetta Gola</author>
<author>Monica Monachini</author>
<author>Antoine Ogonowski</author>
</authors>
<title>Ivonne Peters, Wim Peters, Nilda Ruimy, Marta Villegas, and Antonio Zampolli.</title>
<date>2000</date>
<journal>International Journal of Lexicography,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="29248" citStr="Lenci et al., 2000" startWordPosition="4768" endWordPosition="4771">d when enough sentences were available. Examples are: chaudr´ee de poissonFR forfish chowderEN, grano de caf´eES for coffee beanEN, etc. 6 Related Work Previous attempts to manually build multilingual resources have led to the creation of a multitude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al</context>
</contexts>
<marker>Lenci, Bel, Busa, Calzolari, Gola, Monachini, Ogonowski, 2000</marker>
<rawString>Alessandro Lenci, Nuria Bel, Federica Busa, Nicoletta Calzolari, Elisabetta Gola, Monica Monachini, Antoine Ogonowski, Ivonne Peters, Wim Peters, Nilda Ruimy, Marta Villegas, and Antonio Zampolli. 2000. SIMPLE: A general framework for the development of multilingual lexicons. International Journal of Lexicography, 13(4):249–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
<author>Daniel Weld</author>
<author>Michael Skinner</author>
<author>Jeff Bilmes</author>
</authors>
<title>Compiling a massive, multilingual dictionary via probabilistic inference.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP09,</booktitle>
<pages>262--270</pages>
<contexts>
<context position="29856" citStr="Mausam et al. (2009)" startWordPosition="4865" endWordPosition="4868">t al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research closest to ours is presented by de Melo and Weikum (2009), who developed a Universal WordNet (UWN) by automatically acquiring a semantic network for languages other than English. UWN is bootstrapped from WordNet and is built by collecting evidence ex</context>
</contexts>
<marker>Mausam, Etzioni, Weld, Skinner, Bilmes, 2009</marker>
<rawString>Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld, Michael Skinner, and Jeff Bilmes. 2009. Compiling a massive, multilingual dictionary via probabilistic inference. In Proc. of ACL-IJCNLP09, pages 262–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>David Milne</author>
<author>Catherine Legg</author>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>Int. J. Hum.-Comput. Stud.,</journal>
<volume>67</volume>
<issue>9</issue>
<pages>754</pages>
<contexts>
<context position="3168" citStr="Medelyan et al., 2009" startWordPosition="467" endWordPosition="470">w upon a large-scale collaborative resource, namely Wikipedia1. Wikipedia represents the perfect complement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic nature. While the contribution of any individual user might be imprecise or inaccurate, the continual intervention of expert contributors in all domains results in a resource of the highest quality (Giles, 2005). But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet. In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet via an automatic mapping and by integrating lexical gaps in resource1http://download.wikipedia.org. We use the English Wikipedia database dump from November 3, 2009, which includes 3,083,466 articles. Throughout this pap</context>
</contexts>
<marker>Medelyan, Milne, Legg, Witten, 2009</marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and Ian H. Witten. 2009. Mining meaning from Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716– 754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, N.J.</location>
<contexts>
<context position="6674" citStr="Miller et al., 1993" startWordPosition="995" endWordPosition="998">nses (Section 3.2). This avoids duplicate concepts and allows their inventories of concepts to complement each other. Finally, to enable multilinguality, we collect the lexical realizations of the available concepts in different languages by 2Throughout the paper, unless otherwise stated, we use the general term concept to denote either a concept or a named entity. using (a) the human-generated translations provided in Wikipedia (the so-called inter-language links), as well as (b) a machine translation system to translate occurrences of the concepts within sense-tagged corpora, namely SemCor (Miller et al., 1993) – a corpus annotated with WordNet senses – and Wikipedia itself (Section 3.3). We call the resulting set of multilingual lexicalizations of a given concept a babel synset. An overview of BabelNet is given in Figure 1 (we label vertices with English lexicalizations): unlabeled edges are obtained from links in the Wikipedia pages (e.g. BALLOON (AIRCRAFT) links to WIND), whereas labeled ones from WordNet3 (e.g. balloon&apos; haspart gasbag&apos;). In this paper we restrict ourselves to concepts lexicalized as nouns. Nonetheless, our methodology can be applied to all parts of speech, but in that case Wikip</context>
<context position="14976" citStr="Miller et al., 1993" startWordPosition="2413" endWordPosition="2416">s (either WordNet or Wikipedia), meaning that no link can be established (e.g., FERMI GAS or gasbag&apos; in Figure 1); second, even if covered in both resources, the Wikipage for the concept might not provide any translation for the language of interest (e.g., the Catalan for BALLOON is missing in Wikipedia). In order to address the above issues and thus guarantee high coverage for all languages we developed a methodology for translating senses in the babel synset to missing languages. Given a WordNet word sense in our babel synset of interest (e.g. balloon&apos;) we collect its occurrences in SemCor (Miller et al., 1993), a corpus of more than 200,000 words annotated with WordNet senses. We do the same for Wikipages by retrieving sentences in Wikipedia with links to the Wikipage of interest. By repeating this step for each English lexicalization in a babel synset, we obtain a collection of sentences for the babel synset (see left part of Figure 1). Next, we apply state-of-the-art Machine Translation4 and translate the set of sentences in all the languages of interest. Given a specific term in the initial babel synset, we collect the set of its translations. We then identify the most frequent translation in ea</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 303–308, Plainsboro, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Topic-driven multi-document summarization with encyclopedic knowledge and activation spreading.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP-08,</booktitle>
<pages>763--772</pages>
<contexts>
<context position="964" citStr="Nastase, 2008" startWordPosition="131" endWordPosition="132">lingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available i</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Topic-driven multi-document summarization with encyclopedic knowledge and activation spreading. In Proc. of EMNLP-08, pages 763–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study on graph connectivity for unsupervised Word Sense Disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Anaylsis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="1413" citStr="Navigli and Lapata, 2010" startWordPosition="196" endWordPosition="199">research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. However, manual construction methods inherently suffer from</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study on graph connectivity for unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Anaylsis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Using cycles and quasicycles to disambiguate dictionary glosses.</title>
<date>2009</date>
<booktitle>In Proc. of EACL-09,</booktitle>
<pages>594--602</pages>
<contexts>
<context position="1203" citStr="Navigli, 2009" startWordPosition="164" endWordPosition="166"> resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These</context>
<context position="29796" citStr="Navigli, 2009" startWordPosition="4856" endWordPosition="4858">for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research closest to ours is presented by de Melo and Weikum (2009), who developed a Universal WordNet (UWN) by automatically acquiring a semantic network for languages other than English. UWN is boot</context>
<context position="31730" citStr="Navigli, 2009" startWordPosition="5149" endWordPosition="5150">dia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wikipedia) and a computational lexicon of English (WordNet). This integration process has several advantages. F</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009a. Using cycles and quasicycles to disambiguate dictionary glosses. In Proc. of EACL-09, pages 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1203" citStr="Navigli, 2009" startWordPosition="164" endWordPosition="166"> resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These</context>
<context position="29796" citStr="Navigli, 2009" startWordPosition="4856" endWordPosition="4858">for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research closest to ours is presented by de Melo and Weikum (2009), who developed a Universal WordNet (UWN) by automatically acquiring a semantic network for languages other than English. UWN is boot</context>
<context position="31730" citStr="Navigli, 2009" startWordPosition="5149" endWordPosition="5150">dia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wikipedia) and a computational lexicon of English (WordNet). This integration process has several advantages. F</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009b. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proc. of ACL-96,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1458" citStr="Ng and Lee, 1996" startWordPosition="204" endWordPosition="207">xical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. However, manual construction methods inherently suffer from a number of drawbacks. First, maintaining an</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In Proc. of ACL-96, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Pianta</author>
<author>Luisa Bentivogli</author>
<author>Christian Girardi</author>
</authors>
<title>MultiWordNet: Developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In Proc. of GWC-02,</booktitle>
<pages>21--25</pages>
<contexts>
<context position="1874" citStr="Pianta et al., 2002" startWordPosition="263" endWordPosition="266">lity of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. However, manual construction methods inherently suffer from a number of drawbacks. First, maintaining and updating lexical knowledge resources is expensive and time-consuming. Second, such resources are typically lexicographic, and thus contain mainly concepts and only a few named entities. Third, resources for non-English languages often have a much poorer coverage since the construction effort must be repeated for every language of interest. As a result, an obvious bias exists towards conducting research in resou</context>
<context position="21173" citStr="Pianta et al., 2002" startWordPosition="3425" endWordPosition="3428">oncerning the quality of the acquired concepts. This is assessed in terms of coverage against gold-standard resources (Section 5.1) and against a manuallyvalidated dataset of translations (Section 5.2). 220 Language Word senses Synsets German 15,762 9,877 Spanish 83,114 55,365 Catalan 64,171 40,466 Italian 57,255 32,156 French 44,265 31,742 Table 2: Size of the gold-standard wordnets. 5.1 Automatic Evaluation Datasets. We compare BabelNet against goldstandard resources for 5 languages, namely: the subset of GermaNet (Lemnitzer and Kunze, 2002) included in EuroWordNet for German, MultiWordNet (Pianta et al., 2002) for Italian, the Multilingual Central Repository for Spanish and Catalan (Atserias et al., 2004), and WOrdnet Libre du Franc¸ais (Benoit and Fiˇser, 2008, WOLF) for French. In Table 2 we report the number of synsets and word senses available in the gold-standard resources for the 5 languages. Measures. Let B be BabelNet, F our goldstandard non-English wordnet (e.g. GermaNet), and let E be the English WordNet. All the goldstandard non-English resources, as well as BabelNet, are linked to the English WordNet: given a synset SF ∈ F, we denote its corresponding babel synset as SB and its synset i</context>
<context position="28962" citStr="Pianta et al., 2002" startWordPosition="4724" endWordPosition="4727">s in WordNet (WN), Wikipedia (Wiki) and their intersection (WN n Wiki): percentage and total number of words (in parentheses) are reported. not find translations in major editions of bilingual dictionaries. In contrast, good translations were produced using our machine translation method when enough sentences were available. Examples are: chaudr´ee de poissonFR forfish chowderEN, grano de caf´eES for coffee beanEN, etc. 6 Related Work Previous attempts to manually build multilingual resources have led to the creation of a multitude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or m</context>
</contexts>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>Emanuele Pianta, Luisa Bentivogli, and Christian Girardi. 2002. MultiWordNet: Developing an aligned multilingual database. In Proc. of GWC-02, pages 21–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proc. of IJCAI-09,</booktitle>
<pages>2083--2088</pages>
<contexts>
<context position="31730" citStr="Ponzetto and Navigli, 2009" startWordPosition="5147" endWordPosition="5150">t with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wikipedia) and a computational lexicon of English (WordNet). This integration process has several advantages. F</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating Wikipedia. In Proc. of IJCAI-09, pages 2083–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich Word Sense Disambiguation rivaling supervised system.</title>
<date>2010</date>
<booktitle>In Proc. of ACL-10.</booktitle>
<contexts>
<context position="13622" citStr="Ponzetto and Navigli (2010)" startWordPosition="2180" endWordPosition="2183">t p(w) does not influence our maximization, as it is a constant independent of s. As a result, determining the most appropriate sense s consists of finding the sense s that maximizes the joint probability p(s, w). We estimate p(s, w) as: score(s, w) p(s, w) = where score(s, w) = |Ctx(s) n Ctx(w) |+ 1 (we add 1 as a smoothing factor). Thus, in our algorithm we determine the best sense s by computing the intersection of the disambiguation contexts of s and w, and normalizing by the scores summed over all senses of w in Wikipedia and WordNet. More details on the mapping algorithm can be found in Ponzetto and Navigli (2010). 3.3 Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S U W, where S is the WordNet synset to which sense s belongs, and W includes: (i) w; (ii) all its inter-language links (that is, translations of the Wikipage to other languages); (iii) the redirections to the inter-language links found in the Wikipedia of the target language. For instance, given that µ(BALLOON) = balloon&apos;, the corresponding babel synset is { balloonEN, BallonDE, aerostatoES, bal´on aero</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich Word Sense Disambiguation rivaling supervised system. In Proc. of ACL-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of AAAI-07,</booktitle>
<pages>1440--1445</pages>
<contexts>
<context position="3122" citStr="Ponzetto and Strube, 2007" startWordPosition="459" endWordPosition="462">h as English. A solution to these issues is to draw upon a large-scale collaborative resource, namely Wikipedia1. Wikipedia represents the perfect complement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic nature. While the contribution of any individual user might be imprecise or inaccurate, the continual intervention of expert contributors in all domains results in a resource of the highest quality (Giles, 2005). But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet. In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet via an automatic mapping and by integrating lexical gaps in resource1http://download.wikipedia.org. We use the English Wikipedia database dump from November 3, 2009, which in</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proc. of AAAI-07, pages 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Reiter</author>
<author>Matthias Hartung</author>
<author>Anette Frank</author>
</authors>
<title>A resource-poor approach for linking ontology classes to Wikipedia articles.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing,</booktitle>
<volume>1</volume>
<pages>381--387</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications,</publisher>
<location>London, England.</location>
<contexts>
<context position="31603" citStr="Reiter et al., 2008" startWordPosition="5130" endWordPosition="5133"> unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowle</context>
</contexts>
<marker>Reiter, Hartung, Frank, 2008</marker>
<rawString>Nils Reiter, Matthias Hartung, and Anette Frank. 2008. A resource-poor approach for linking ontology classes to Wikipedia articles. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing, volume 1 of Research in Computational Semantics, pages 381–387. College Publications, London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets.</title>
<date>2005</date>
<booktitle>In Advances in Web Intelligence,</booktitle>
<volume>3528</volume>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="31533" citStr="Ruiz-Casado et al., 2005" startWordPosition="5120" endWordPosition="5123">etwork (i.e. the semantic pointers from WordNet) with topical, semantically unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the</context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets. In Advances in Web Intelligence, volume 3528 of Lecture Notes in Computer Science. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Sammer</author>
<author>Stephen Soderland</author>
</authors>
<title>Building a sense-distinguished multilingual lexicon from monolingual corpora and bilingual lexicons.</title>
<date>2007</date>
<booktitle>In Proceedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="29608" citStr="Sammer and Soderland, 2007" startWordPosition="4824" endWordPosition="4828">t al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. The research cl</context>
</contexts>
<marker>Sammer, Soderland, 2007</marker>
<rawString>Marcus Sammer and Stephen Soderland. 2007. Building a sense-distinguished multilingual lexicon from monolingual corpora and bilingual lexicons. In Proceedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogeneous evidence.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL-06,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="34169" citStr="Snow et al. (2006)" startWordPosition="5518" endWordPosition="5521">ting wordnets in non-English languages. While BabelNet currently includes 6 languages, links to freely-available wordnets6 can immediately be established by utilizing the English WordNet as an interlanguage index. Indeed, BabelNet can be extended to virtually any language of interest. In fact, our translation method allows it to cope with any resource-poor language. As future work, we plan to apply our method to other languages, including Eastern European, Arabic, and Asian languages. We also intend to link missing concepts in WordNet, by establishing their most likely hypernyms – e.g., a` la Snow et al. (2006). We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon’s Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. Finally, we aim to apply BabelNet to a variety of applications which are known to benefit from a wide-coverage knowledge resource. We have already shown that the English-only subset of BabelNet allows simple knowledge-based algorithms to compete with supervised systems in standard coarse-grained and domain-specific WSD settings (Ponzetto and Navigli, 2010). We plan in th</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogeneous evidence. In Proc. of COLING-ACL-06, pages 801– 808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from Wikipedia and WordNet.</title>
<date>2008</date>
<journal>Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="3145" citStr="Suchanek et al., 2008" startWordPosition="463" endWordPosition="466"> these issues is to draw upon a large-scale collaborative resource, namely Wikipedia1. Wikipedia represents the perfect complement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic nature. While the contribution of any individual user might be imprecise or inaccurate, the continual intervention of expert contributors in all domains results in a resource of the highest quality (Giles, 2005). But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet. In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet via an automatic mapping and by integrating lexical gaps in resource1http://download.wikipedia.org. We use the English Wikipedia database dump from November 3, 2009, which includes 3,083,466 articl</context>
<context position="31357" citStr="Suchanek et al., 2008" startWordPosition="5093" endWordPosition="5096">rther by (1) developing an even larger multilingual resource including both lexical semantic and encyclopedic knowledge, (2) enriching the structure of the ‘core’ semantic network (i.e. the semantic pointers from WordNet) with topical, semantically unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network.</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from Wikipedia and WordNet. Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis¸</author>
<author>Dan Cristea</author>
<author>Sofia Stamou</author>
</authors>
<title>BalkaNet: Aims, methods, results and perspectives. a general overview. Romanian</title>
<date>2004</date>
<booktitle>Journal on Science and Technology of Information,</booktitle>
<pages>7--1</pages>
<marker>Tufis¸, Cristea, Stamou, 2004</marker>
<rawString>Dan Tufis¸, Dan Cristea, and Sofia Stamou. 2004. BalkaNet: Aims, methods, results and perspectives. a general overview. Romanian Journal on Science and Technology of Information, 7(1-2):9–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>IEEE Computer,</journal>
<volume>6</volume>
<issue>39</issue>
<marker>von Ahn, 2006</marker>
<rawString>Luis von Ahn. 2006. Games with a purpose. IEEE Computer, 6(39):92–94.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</booktitle>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel Weld</author>
</authors>
<title>Automatically semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM-07,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="3095" citStr="Wu and Weld, 2007" startWordPosition="455" endWordPosition="458">rich languages, such as English. A solution to these issues is to draw upon a large-scale collaborative resource, namely Wikipedia1. Wikipedia represents the perfect complement to WordNet, as it provides multilingual lexical knowledge of a mostly encyclopedic nature. While the contribution of any individual user might be imprecise or inaccurate, the continual intervention of expert contributors in all domains results in a resource of the highest quality (Giles, 2005). But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (Wu and Weld, 2007; Ponzetto and Strube, 2007; Suchanek et al., 2008; Medelyan et al., 2009, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet. In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet via an automatic mapping and by integrating lexical gaps in resource1http://download.wikipedia.org. We use the English Wikipedia database dump from</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel Weld. 2007. Automatically semantifying Wikipedia. In Proc. of CIKM-07, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>4</issue>
<pages>310</pages>
<contexts>
<context position="1487" citStr="Yarowsky and Florian, 2002" startWordPosition="208" endWordPosition="211"> exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repository (Atserias et al., 2004), and many others. However, manual construction methods inherently suffer from a number of drawbacks. First, maintaining and updating lexical knowledge </context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 9(4):293– 310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshio Yokoi</author>
</authors>
<title>The EDR electronic dictionary.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="29153" citStr="Yokoi, 1995" startWordPosition="4755" endWordPosition="4756">naries. In contrast, good translations were produced using our machine translation method when enough sentences were available. Examples are: chaudr´ee de poissonFR forfish chowderEN, grano de caf´eES for coffee beanEN, etc. 6 Related Work Previous attempts to manually build multilingual resources have led to the creation of a multitude of wordnets such as EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a</context>
</contexts>
<marker>Yokoi, 1995</marker>
<rawString>Toshio Yokoi. 1995. The EDR electronic dictionary. Communications of the ACM, 38(11):42–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>