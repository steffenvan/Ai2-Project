<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000185">
<title confidence="0.994189">
Word Ordering with Phrase-Based Grammars
</title>
<author confidence="0.99008">
Adri`a de Gispert, Marcus Tomalin, William Byrne
</author>
<affiliation confidence="0.999723">
Department of Engineering, University of Cambridge, UK
</affiliation>
<email confidence="0.995608">
ad465@cam.ac.uk, mt126@cam.ac.uk, wjb31@cam.ac.uk
</email>
<sectionHeader confidence="0.993779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814789473684">
We describe an approach to word ordering
using modelling techniques from statisti-
cal machine translation. The system in-
corporates a phrase-based model of string
generation that aims to take unordered
bags of words and produce fluent, gram-
matical sentences. We describe the gen-
eration grammars and introduce parsing
procedures that address the computational
complexity of generation under permuta-
tion of phrases. Against the best previous
results reported on this task, obtained us-
ing syntax driven models, we report huge
quality improvements, with BLEU score
gains of 20+ which we confirm with hu-
man fluency judgements. Our system in-
corporates dependency language models,
large n-gram language models, and mini-
mum Bayes risk decoding.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999285">
Word ordering is a fundamental problem in NLP
and has been shown to be NP-complete in dis-
course ordering (Althaus et al., 2004) and in SMT
with arbitrary word reordering (Knight, 1999).
Typical solutions involve constraints on the space
of permutations, as in multi-document summari-
sation (Barzilay and Elhadad, 2011) and preorder-
ing in SMT (Tromble and Eisner, 2009; Genzel,
2010).
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al.
(2009) use a dependency grammar to address word
ordering, while Zhang and Clark (2011; 2012)
use CCG and large-scale n-gram language models.
These techniques are applied to the unconstrained
problem of generating a sentence from a multi-set
of input words.
We describe GYRO (Get Your Order Right), a
phrase-based approach to word ordering. Given a
bag of words, the system first scans a large, trusted
text collection and extracts phrases consisting of
words from the bag. Strings are then generated
by concatenating these phrases in any order, sub-
ject to the constraint that every string is a valid
reordering of the words in the bag, and the re-
sults are scored under an n-gram language model
(LM). The motivation is that it is easier to make
fluent sentences from phrases (snippets of fluent
text) than from words in isolation.
GYRO builds on approaches developed for syn-
tactic SMT (Chiang, 2007; de Gispert et al., 2010;
Iglesias et al., 2011). The system generates strings
in the form of weighted automata which can be
rescored using higher-order n-gram LMs, depen-
dency LMs (Shen et al., 2010), and Minimum
Bayes Risk decoding, either using posterior prob-
abilities obtained from GYRO or SMT systems.
We report extensive experiments using BLEU
and conclude with human assessments. We
show that despite its relatively simple formulation,
GYRO gives BLEU scores over 20 points higher
than the best previously reported results, gener-
ated by a syntax-based ordering system. Human
fluency assessments confirm these substantial im-
provements.
</bodyText>
<sectionHeader confidence="0.973401" genericHeader="method">
2 Phrase-based Word Ordering
</sectionHeader>
<bodyText confidence="0.9996084">
We take as input a bag of N words Q =
{wi, ... , wN}. The words are sorted, e.g. alpha-
betically, so that it is possible to refer to the ith
word in the bag, and repeated words are distinct
tokens. We also take a set of phrases, L(Q) that
</bodyText>
<page confidence="0.976142">
259
</page>
<note confidence="0.992932">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259–268,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996794">
are extracted from large text collections, and con-
tain only words from Ω. We refer to phrases as u,
i.e. u E G(Ω). The goal is to generate all permu-
tations of Ω that can be formed by concatenation
of phrases from G(Ω).
</bodyText>
<subsectionHeader confidence="0.991832">
2.1 Word Order Generation Grammar
</subsectionHeader>
<bodyText confidence="0.9920262">
Consider a subset A C Ω. We can represent A by
an N-bit binary string I(A) = I1(A) ... IN(A),
where Ii(A) = 1 if wi E A, and Ii(A) = 0 other-
wise. A Context-Free Grammar (CFG) for gener-
ation can then be defined by the following rules:
</bodyText>
<equation confidence="0.834814125">
Phrase-based Rules: VA C Ω and Vu E G(A)
I(A) — *u
Concatenation Rules: VA C Ω, B C A, C C A
such that I(A) = I(B)+I(C) and I(B)•I(C) =
0
I(A) —* I(B) I(C)
where • is the bit-wise logical AND
Root: 5 —* I(Ω)
</equation>
<bodyText confidence="0.842353269230769">
We use this grammar to ‘parse’ the list of the
words in the bag Ω. The grammar has one non-
terminal per possible binary string, so potentially
2N distinct nonterminals might be needed to gen-
erate the language. Each nonterminal can produce
either a phrase u E G(A), or the concatenation of
two binary strings that share no bits in common. A
derivation is sequence of rules that starts from the
bit string I(Ω). Rules are unweighted in this basic
formulation.
For example, assume the following bag
Ω = {a, b, c, d, e}, which we sort alphabet-
ically. Assume the phrases are G(Ω) =
{“a b”, “b a”, “d e c”}. The generation grammar
contains the following 6 rules:
11000—* ab
11000—* ba
00111—* dec
11111—* 11000 00111
11111—* 00111 11000
5—* 11111
Figure 1 represents all the possible derivations
in a hypergraph, which generate four alternative
strings. For example, string “d e c b a” is ob-
tained with derivation R6R5R3R2, whereas string
“a b d e c” is obtained via R6R4R1R3.
</bodyText>
<subsectionHeader confidence="0.999812">
2.2 Parsing a Bag of Words
</subsectionHeader>
<bodyText confidence="0.9998405">
We now describe a general algorithm for parsing a
bag of words with phrase constraints. The search
</bodyText>
<figureCaption confidence="0.842916333333333">
Figure 1: Hypergraph representing gen-
eration from {a, b, c, d, e} with phrases
{“a b”, “b a”, “d e c”}.
</figureCaption>
<bodyText confidence="0.992749829268293">
is organized along a two-dimensional grid M[x, y]
of 2N–1 cells, where each cell is associated with
a unique nonterminal in the grammar (a bit string
I with at least one bit set to 1). Each row x in
the grid has (N) cells, representing all the possible
x
ways of covering exactly x words from the bag.
There are N rows in total.
For a bit string I, X(I) is the length of I, i.e.
the number of 1’s in I. In this way X(I(A))
points to the row associated with set A. There
is no natural ordering of cells within a row, so
we introduce a second function Y (I) which indi-
cates which cell in row X(I) is associated with I.
Hence M[X(I),Y(I)] is the cell associated with
bit string I. In the inverse direction, we using the
notation Ix,y to indicate a bit string associated with
the cell M[x, y].
The basic parsing algorithm is given in Figure 2.
We first initialize the grid by filling the cells linked
to phrase-based rules (lines 1-4 of Figure 2). Then
parsing proceeds as follows. For each row in in-
creasing order (line 5), and for each of the non-
empty cells in the row (line 6), try to combine its
bit string with any other bit strings (lines 7-8). If
combination is admitted, then form the resultant
bit string and add the concatenation rule to the as-
sociated cell in the grid (lines 9-10). The combi-
nation will always yield a bit string that resides in
a higher row of the grid, so search is exhaustive.
If a rule is found in cell M[N,1], there is a parse
(line 11); otherwise none exists. The complexity
of the algorithm is O(2N ·K). If back-pointers are
kept, traversing these from cell M[N,1] yields all
the generated word sequences.
The number of cells will grow exponentially as
the bag grows in size. In practice, the number of
{&amp;quot;a b d e c&amp;quot;,
&amp;quot;b a d e c&amp;quot;,
&amp;quot;d e c a b&amp;quot;,
&amp;quot;d e c b a&amp;quot;}
</bodyText>
<figure confidence="0.979299185185185">
11111
1
{&amp;quot;a b&amp;quot;, &amp;quot;b a&amp;quot;}
{&amp;quot;d e c&amp;quot;}
1 2 2 1
3 1 2
a b c d e
2 2 1
11000 00111
260
PARSE-BAG-OF-WORDS
Input: bag of words Ω of size N
Input: list of phrases G(Ω)
Initialize - Add phrase-based rules:
1 M[x, y] +— 0
2 for each subset A E Ω
3 for each phrase u E G(A)
4 add rule I(A) --+ u to cell M[X(I(A)),Y(I(A))]
Parse:
5 for each row x = 1, ... ,N
6 for each y = 1, ... ,(v�
x
7 for each valid A E Ω
8 if Ix,y 9 I(A) = 0, then
9 I&apos; +— Ix,y + I(A)
10 add rule I&apos; --+ Ix,y I(A) to cell M[X(I&apos;), Y (I&apos;)]
11 if |M[N, 1] |&gt; 0, success.
</figure>
<figureCaption confidence="0.999845">
Figure 2: Parsing algorithm for a bag of words.
</figureCaption>
<bodyText confidence="0.999883833333333">
cells actually used in parsing can be smaller than
2N − 1. This depends strongly on the number of
distinct phrase-based rules and the distinct subsets
of Q they cover. For example, if we consider 1-
word subsets of Q, then all cells are needed and
GYRO attempts all word permutation. However,
if only 10 distinct 5-word phrases and 20 distinct
4-word phrases are considered for a bag of N=9
words, then fewer than 431 cells will be used (20
+ 10 for the initial cells at rows 4 and 5; plus all
combinations of 4-word subsets into row 8, which
is less than 400; plus 1 for the last cell at row 9).
</bodyText>
<subsectionHeader confidence="0.998307">
2.3 Generation from Exact Parsing
</subsectionHeader>
<bodyText confidence="0.946382675">
We are interested in producing the space of word
sequences generated by the grammar, and in scor-
ing each of the sequences according to a word-
based n-gram LM. Assuming that parsing the bag
of words suceeded, this is a very similar scenario
to that of syntax-based approaches to SMT: the
output is a large collection of word sequences,
which are built by putting together smaller units
and which can be found by a process of expansion,
i.e. by traversing the back-pointers from an initial
cell in a grid structure. A significant difference is
that in syntax-based approaches the parsing stage
tends to be computationally easier than the pars-
ing stage has only a quadratic dependency on the
length of the input sentence.
We borrow techniques from SMT to represent
and manipulate the space of generation hypothe-
ses. Here we follow the approach of expand-
ing this space onto a Finite-State Automata (FSA)
described in (de Gispert et al., 2010; Iglesias et
al., 2011). This means that in parsing, each cell
M[x, y] is associated with an FSA Fz,y, which en-
codes all the sequences generated by the grammar
Figure 3: RTN representing generation from
{a, b, c, d, e} with phrases {“a b”, “b a”, “d e c”}
(top) and its expansion as an FSA (bottom).
when covering the words marked by the bit string
of that cell. When a rule is added to a cell, a new
path from the initial to the final state of Fz,y is
created so that each FSA is the union of all paths
arising from the rules added to the cell. Impor-
tantly, when an instance of the concatenation rule
is added to a cell, the new path is built with only
two arcs. These point to other FSAs at lower rows
in the grid so that the result has the form of a
Recursive Transition Network with a finite depth
of recursion. Following the example from Sec-
tion 2.1, the top three FSAs in Figure 3 represent
the RTN for example from Figure 1.
The parsing algorithm is modified as follows:
</bodyText>
<figure confidence="0.825792333333333">
4 add rule I(A) --+ u
as path to FSA FX(I(A)),Y (I(A))
...
10 add rule I&apos; --+ Ix,y I(A)
as path to FSA FX(I,),Y (I,)
11 if NumStates(Fv,1)&gt;1,success.
</figure>
<bodyText confidence="0.969687666666667">
At this point we specify two strategies:
Algorithm 1: Full expansion is described by the
pseudocode in Figure 4, excluding lines 2-3. A
recursive FSA replacement operation (Allauzen et
al., 2007) can be used to expand the FSA in the
top-most cell. In our running example, the result
</bodyText>
<figure confidence="0.998952333333333">
11000
0 3
b a
a
2
1
b
Expansion of RTN 11111
0
b
d
a
2
7
1
b
a
e
d e
3 4 5
8
c
9
b
a
10
11
c
b
a
6
11111
0 00111 11000 3
00111
11000
d e c
0 1 2 3
2
1 00111
</figure>
<page confidence="0.989828">
261
</page>
<bodyText confidence="0.99954755882353">
is the FSA at the bottom of Figure 3. We then
apply a word-based LM to the resulting FSA via
standard FSA composition. This outputs the com-
plete (unpruned) language of interest, where each
word sequence generated from the bag according
to the phrasal constraints is scored by the LM.
Algorithm 2: Pruned expansion is described by
the pseudocode in Figure 4, now including lines
2-3. We introduce pruning because full, unpruned
expansion may not be feasible for large bags with
many phrasal rules. Once parsing is done, we in-
troduce the following bottom-up pruning strategy.
For each row starting at row r, we union all FSAs
of the row and expand the unioned FSA through
the recursive replacement operation. This yields
the space of all generation hypotheses of length
r. We then apply the language model to this lat-
tice and reduce it under likelihood-based pruning
at weight Q. We then update each cell in the row
with a new FSA obtained as the intersection of its
original FSA and the pruned FSA.1 This intersec-
tion may yield an empty FSA for a particular cell
(meaning that all its hypotheses were pruned out
of the row), but it will always leave at least one
surviving FSA per row, guaranteeing that if pars-
ing succeeds, the top-most cell will expand into
a non-empty FSA. As we process higher rows,
the replacement operation will yield smaller FSAs
because some back-pointers will point to empty
FSAs. In this way memory usage can be con-
trolled through parameters r and Q. Of course,
when pruning in this way, the final output lattice
L will not contain the complete space of hypothe-
ses that could be generated by the grammar.
</bodyText>
<subsectionHeader confidence="0.9728835">
2.4 Algorithm 3: Pruned Parsing and
Generation
</subsectionHeader>
<bodyText confidence="0.99945625">
The two generation algorithms presented above
rely on a completed initial parsing step. However,
given that the complexity of the parsing stage is
O(2N · K), this may not be achievable in prac-
tice. Leaving aside time considerations, the mem-
ory required to store 2N FSAs will grow exponen-
tially in N, even if the FSAs contain only pointers
to other FSAs. Therefore we also describe an al-
gorithm to perform bottom-up pruning guided by
1This step can be performed much more efficiently with
a single forward pass of the resultant lattice. This is possible
because the replace operation can yield a transducer where
the input symbols encode a pointer to the original FSA, so
in traversing the arcs of the pruned lattice, we know which
arcs will belong to which cell FSAs. However, for ease of
explanation we avoid this detail.
</bodyText>
<figure confidence="0.98423885">
FULL-PARSE-EXPANSION
Input: bag of words Q of size N
Input: list phrases L(Q)
Input: word-based LM G
Output: word lattice L of generated sequences
Generate:
1 PARSE-BAG-OF-WORDS(Q)
2 for each row x = r, ... , N − 1
3 PRUNE-ROW(x)
4 F FSA-REPLACE(FN,l)
5 return L F ~ G
6 function PRUNE-ROW(x) :
F �
7 r Fx,r
8 F FSA-REPLACE(F)
9 F F ~ G
10 F FSA-PRUNE(F, β)
11 for each cell y = 1 ... (N) x
12 Fx,r Fx,r · F
13 return
</figure>
<figureCaption confidence="0.999805">
Figure 4: Pseudocode for Algorithm 1 (excluding
lines 2-3) and Algorithm 2 (including all lines).
</figureCaption>
<bodyText confidence="0.9810214">
the LM during parsing. The pseudocode is identi-
cal to that of Algorithm 1 except for the following
changes: in parsing (Figure 2) we pass G as input
and we call the row pruning function of Figure 4
after line 5 if x ≥ r.
We note that there is a strong connection be-
tween GYRO and the IDL approach of Soricut
and Marcu (2005; 2006). Our bag of words parser
could be cast in the IDL-formalism, and the FSA
‘Replace’ operation would be expressed by an
IDL ‘Unfold’ operation. However, whereas their
work applies pruning in the creation of the IDL-
expression prior to LM application, GYRO uses
unweighted phrase constraints so the LM must be
considered for pruning while parsing.
</bodyText>
<sectionHeader confidence="0.997845" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999982384615385">
We now report various experiments evaluating the
performance of the generation approach described
above. The system is evaluated using the MT08-
nw, and MT09-nw testsets. These correspond to
the first English reference of the newswire por-
tion of the Arabic-to-English NIST MT evalua-
tion sets2. They contain 813 and 586 sentences
respectively (53,325 tokens in total; average sen-
tence length = 38.1 tokens after tokenization). In
order to reduce the computational complexity, all
sentences with more than 20 tokens were divided
into sub-sentences, with 20 tokens being the up-
per limit. Between 70-80% of the sentences in the
</bodyText>
<footnote confidence="0.964281">
2http://www.itl.nist.gov/iad/mig/tests/mt
</footnote>
<page confidence="0.984163">
262
</page>
<figure confidence="0.9995836">
bag of words size
1000
100
10
Number of n-grams
1
2grams
3grams
4grams
5grams
6 8 10 12 14 16 18 20
Size of the bag of words
20
18
memory consumption (in GB)
16
14
12
10
8
6
Algorithm 1
Algorithm 2 β=10
Algorithm 3 β=10
Algorithm 3 β=5
</figure>
<figureCaption confidence="0.960164">
Figure 5: Average number of extracted phrases as
a function of the bag of word size.
</figureCaption>
<figure confidence="0.9930634">
4
2
0
4 6 8 10 12 14 16 18 20
10000
</figure>
<bodyText confidence="0.999492111111111">
testsets were divided in this way. For each of these
sentences we create a bag.
The GYRO system uses a n-gram LM estimated
over 1.3 billion words of English text, including
the AFP and Xinhua portions of the GigaWord
corpus version 4 (1.1 billion words) and the En-
glish side of various Arabic-English parallel cor-
pora typically used in MT evaluations (0.2 billion
words).
Phrases of up to length 5 are extracted for each
bag from a text collection containing 10.6 bil-
lion words of English news text. We use efficient
Hadoop-based look-up techniques to carry out this
extraction step and to retrieve rules for genera-
tion (Pino et al., 2012). The average number of
phrases extracted as a function of the size of the
bag is shown in Figure 5. These are the phrase-
based rules of our generation grammar.
</bodyText>
<subsectionHeader confidence="0.998214">
3.1 Computational Analysis
</subsectionHeader>
<bodyText confidence="0.991502470588235">
We analyze here the computational requirements
of the three alternative GYRO algorithms pre-
sented in Sections 2.3 and 2.4. We carry out this
analysis on a subset of 200 random subsentences
from MT08-nw and MT09-nw chosen to have the
same sentence length distribution as the whole
data set. For a fixed generation grammar com-
prised of 3-gram, 4-gram and 5-gram rules only,
we run each algorithm with a memory limitation
of 20GB. If the process reaches this limit, then it
is killed. Figure 6 reports the worst-case memory
memory required by each algorithm as a function
of the size of the bag.
As shown, Full Expansion (Algorithm 1) is only
feasible for bags that contain at most 12 words.
By contrast, Pruned Expansion (Algorithm 2) with
β = 10 is feasible for bags of up to 18 words. For
Figure 6: Worst-case memory required (GB) by
each GYRO algorithm relative to the size of the
bags.
bigger bags, the requirements of unpruned pars-
ing make generation intractable under the mem-
ory limit. Finally, Pruned Parsing and Generation
(Algorithm 3) is feasible at all bag sizes (up to 20
words), and its memory requirements can be con-
trolled via the beam-width pruning parameter β.
Harsher pruning (i.e. lower β) will incur more
coverage problems, so it is desirable to use the
highest feasible value of β.
We emphasise that Algorithm 3, with suitable
pruning strategies, can scale up to larger problems
quite readily and generate output from much larger
input sets than reported here. We focus here on
generation quality for moderate sized problems.
</bodyText>
<subsectionHeader confidence="0.999361">
3.2 Generation Performance
</subsectionHeader>
<bodyText confidence="0.999972466666667">
We now compare the GYRO system with the
Combinatory Categorial Grammar (CCG)-based
system described in (Zhang et al., 2012). By
means of extracted CCG rules, the CCG sys-
tem searches for an optimal parse guided by
large-margin training. Each partial hypothesis (or
‘edge’) is scored using the syntax model and a 4-
gram LM trained similarly on one billion words of
English Gigaword data. Both systems are evalu-
ated using BLEU (Papineni et al., 2002; Espinosa
et al., 2010).
For GYRO, we use the pruned parsing algo-
rithm of Section 2.4 with r = 6 and β = 10
and a memory usage limit of 20G. The phrase-
based rules of the grammar contain only 3-grams,
</bodyText>
<page confidence="0.997316">
263
</page>
<table confidence="0.999832428571429">
LM System MT08-nw MT09-nw
4g CCG 48.0 48.8
3g GYRO 59.0 58.4
GYRO +3g 63.0 64.1
4g GYRO +4g 65.5 65.9
100-best oracle 76.1 76.1
lattice oracle 80.4 80.2
</table>
<tableCaption confidence="0.999833">
Table 1: CCG and GYRO BLEU scores.
</tableCaption>
<bodyText confidence="0.999297350000001">
4-grams and 5-grams.3 Under these conditions,
GYRO finds an output for 91.4% of the bags. For
the remainder, we obtain an output either by prun-
ing less or by adding bigram rules (in 7.2% of the
bags), or simply by adding all words as unigram
rules (1.4% of the bags).
Table 1 gives the results obtained by CCG and
GYRO under a 3-gram or a 4-gram LM. Because
GYRO outputs word lattices as opposed to a 1-
best hypothesis, we can reapply the same LM to
the concatenated lattices of any sentences longer
than 20 to take into account context in subsentence
boundaries. This is the result in the third row in
the Table, labeled ‘GYRO +3g’. We can see that
GYRO benefits significantly from this rescoring,
beating the CCG system across both sets. This is
possibly explained by the CCG system’s depen-
dence upon in-domain data that have been explic-
itly marked-up using the CCG formalism. The fi-
nal row reports the positive impact of increasing
the LM order to 4.
Impact of generation grammar. To measure
the benefits of using high-order n-grams as con-
straints for generation, we also ran GYRO with
unigram rules only. This effectively does permu-
tation under the LM with the pruning mechanisms
described. The BLEU scores are 54.0 and 54.5 for
MT08-nw and MT09 respectively. This indicates
that a strong GYRO grammar is very much needed
for this type of parsing and generation.
Quality of generated lattices. We assess the
quality of the lattices output by GYRO under the
4-gram LM by computing the oracle BLEU score
of either the 100-best lists or the whole lattices4
in the last two rows of Table 1. In order to com-
pute the latter, we use the linear approximation
to BLEU that allows an efficient FST-based im-
plementation of an Oracle search (Sokolov et al.,
2012). We draw two conclusions from these re-
sults: (a) that there is a significant potential for im-
</bodyText>
<footnote confidence="0.992467666666667">
3Any word in the bag that does not occur in the large col-
lection of English material is added as a 1-gram rule.
4Obtained by pruning at β = 10 in generation.
</footnote>
<bodyText confidence="0.99961">
provement from rescoring, in that even for small
100-best lists the improvement found by the Ora-
cle can exceed 10 BLEU points; and (b) that the
output lattices are not perfect in that the Oracle
score is not 100.
</bodyText>
<subsectionHeader confidence="0.880533">
3.2.1 Rescoring GYRO output
</subsectionHeader>
<bodyText confidence="0.999363162790698">
We now report on rescoring procedures intended
to improve the first-pass lattices generated by
GYRO.
Higher-order language models. The first row
in Table 2 reports the result obtained when apply-
ing a 5-gram LM to the GYRO lattices generated
under a 4-gram. The 5-gram is estimated over the
complete 10.6 billion word collection using the
uniform backoff strategy of (Brants et al., 2007).
We find improvements of 3.0 and 1.9 BLEU with
respect to the 4-gram baseline.
Dependency language models. We now in-
vestigate the benefits of applying a dependency
LM (Shen et al., 2010) in a rescoring mode. We
run the MALT dependency parser5 on the gener-
ation hypotheses and rescore them according to
lo9(pLM) + Adlo9(pdepLM), i.e. a weighted com-
bination of the word-based LM and the depen-
dency LM scores. Since it is not possible to run the
parser on the entire lattice, we carry out this exper-
iment using the 100-best lists generated from the
previous experiment (‘+5g’). The dependency LM
is a 3-gram estimated on the entire GigaWord ver-
sion 5 collection (∼5 billion words). Results are
shown in rows 2 and 3 in Table 2, where in each
row the performance over the set used to tune the
parameter Ad is marked with *. In either case, we
observe modest but consistent gains across both
sets. We find this very promising considering that
the parser has been applied to noisy input sen-
tences.
Minimum Bayes Risk Decoding. We also use
Lattice-based Minimum Bayes Risk (LMBR) de-
coding (Tromble et al., 2008; Blackwood et al.,
2010a). Here, the posteriors over n-grams are
computed over the output lattices generated by the
GYRO system. The result is shown in row labeled
‘+5g +LMBR’, where again we find modest but
consistent gains across the two sets with respect to
the 5-gram rescored lattices.
LMBR with MT posteriors. We investigate
LMBR decoding when applying to the generation
lattice a linear combination of the n-gram pos-
</bodyText>
<footnote confidence="0.995707">
5Available at www.maltparser.org
</footnote>
<page confidence="0.987586">
264
</page>
<table confidence="0.998990571428571">
4g GYRO rescoring: MT08-nw MT09-nw
+5g 68.5 67.8
+5g +depLM Ad = 0.4 68.7 ? 68.1
+5g +depLM Ad = 0.33 68.7 68.2 ?
+5g +LMBR 68.6 68.3
+5g +LMBR-mt α = 0.25 70.8 ? 72.2
+5g +LMBR-mt α = 0.25 70.8 72.2 ?
</table>
<tableCaption confidence="0.993151">
Table 2: Results in BLEU when rescoring the lat-
tices generated by GYRO using various strategies.
Tuning conditions are marked by *.
</tableCaption>
<figure confidence="0.98372988">
Size of the bag of words
1-5 6-10 11-15 16-20 21-25 26+ all sizes
BLEU score
90
85
80
75
70
65
60
55
50
BLEU
SPR
40
30
90
80
70
60
50
20
10
0
Sentence Precision Rate (SPR)
</figure>
<bodyText confidence="0.999809380952381">
terior probabilities extracted from (a) the same
generation lattice, and (b) from lattices produced
by an Arabic-to-English hierarchical-phrase based
MT system developed for the NIST 2012 OpenMT
Evaluation. As noted, LMBR relies on a posterior
distribution over n-grams as part of its computa-
tion or risk. Here, we use LMBR with a posterior
of the form α pGYRO + (1–α) pMT. This is effec-
tively performing a system combination between
the GYRO generation system and the MT system
(de Gispert et al., 2009; DeNero et al., 2010) but
restricting the hypothesis space to be that of the
GYRO lattice (Blackwood et al., 2010b). Results
are reported in the last two rows of Table 2. Rel-
ative to 5-gram LM rescoring alone, we see gains
in BLEU of 2.3 and 4.4 in MT08-nw and MT09-
nw, suggesting that posterior distributions over n-
grams provided by SMT systems can give good
guidance in generation. These results also suggest
that if we knew what words to use, we could gen-
erate very good quality translation output.
</bodyText>
<subsectionHeader confidence="0.999223">
3.3 Analysis and examples
</subsectionHeader>
<bodyText confidence="0.967252571428571">
Figure 7 gives GYRO generation examples. These
are often fairly fluent, and it is striking how the
output can be improved with guidance from the
SMT system. The examples also show the harsh-
ness of BLEU, e.g. ‘german and turkish officials’
is penalised with respect to ‘ turkish and german
officials.’ Metrics based on richer meaning rep-
resentations, such as HyTER, could be valuable
here (Dreyer and Marcu, 2012).
Figure 8 shows BLEU and Sentence Preci-
sion Rate (SPR), the percentage of exactly recon-
structed sentences. As expected, performance is
sensitive to length. For bags of up to 10, GYRO
reconstructs the reference perfectly in over 65%
of the cases. This is a harsh performance metric,
and performance falls to less than 10% for bags
of size 16-20. For bags of 6-10 words, we find
BLEU scores of greater than 85. Performance is
Figure 8: GYRO BLEU score and Sentence Pre-
cision Rate as a function of the bag of words size.
Computed on the concatenation of MT08-nw and
MT09-nw.
not as good for shorter segments, since these are
often headlines and bylines that can be ambiguous
in their ordering. The BLEU scores for bags of
size 21 and higher are an artefact of our sentence
splitting procedure. However, even for bag sizes
of 16-to-20 GYRO has BLEU scores above 55.
</bodyText>
<subsectionHeader confidence="0.997224">
3.4 Human Assessments
</subsectionHeader>
<bodyText confidence="0.999972923076923">
Finally, the CCG and 4g-GYRO+5g systems were
compared using crowd-sourced fluency judge-
ments gathered on CrowdFlower. Judges were
asked ‘Please read the reference sentence and
compare the fluency of items 1 &amp; 2.’ The test was
a selection of 75 fluent sentences of 20 words or
less taken from the MT dev sets. Each comparison
was made by at least 3 judges. With an average se-
lection confidence of 0.754, GYRO was preferred
in 45 cases, CCG was preferred in 14 cases, and
systems were tied 16 times. This is consistent with
the significant difference in BLEU between these
systems.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="related work">
4 Related Work and Conclusion
</sectionHeader>
<bodyText confidence="0.999969538461539">
Our work is related to surface realisation within
natural language generation (NLG). NLG typi-
cally assumes a relatively rich input representation
intended to provide syntactic, semantic, and other
relationships to guide generation. Example input
representations are Abstract Meaning Represen-
tations (Langkilde and Knight, 1998), attribute-
value pairs (Ratnaparkhi, 2000), lexical predicate-
argument structures (Bangalore and Rambow,
2000), Interleave-Disjunction-Lock (IDL) expres-
sions (Nederhof and Satta, 2004; Soricut and
Marcu, 2005; Soricut and Marcu, 2006), CCG-
bank derived grammars (White et al., 2007),
</bodyText>
<page confidence="0.99478">
265
</page>
<table confidence="0.920646545454545">
Hypothesis SBLEU
REF a third republican senator joins the list of critics of bush ’s policy in iraq .
critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2
critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8
critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1
the list of critics of bush ’s policy in iraq a third republican senator joins . 82.9
REF it added that these messages were sent to president bashar al-asad through turkish and german officials .
(a-c) it added that president bashar al-asad through these messages were sent to german and turkish officials . 61.5
(d) it added that these messages were sent to president bashar al-asad through german and turkish officials . 80.8
REF a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling
for a new strategy just days before a new confrontation in congress
</table>
<figure confidence="0.75187975">
a prominent republican senator george has joined the ranks of critics of bush ’s policy in iraq , just days 66.7
before a new strategy in congress calling for a new confrontation
a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , just days 77.8
before congress calling for a new strategy in a new confrontation
a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , just days 82.3
before a new strategy in congress calling for a new confrontation
a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling 100
for a new strategy just days before a new confrontation in congress
</figure>
<figureCaption confidence="0.972104">
Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b)
GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical
hypotheses.
</figureCaption>
<bodyText confidence="0.999242733333333">
meaning representation languages (Wong and
Mooney, 2007) and unordered syntactic depen-
dency trees (Guo et al., 2011; Bohnet et al., 2011;
Belz et al., 2011; Belz et al., 2012)6.
These input representations are suitable for ap-
plications such as dialog systems, where the sys-
tem maintains the information needed to gener-
ate the input representation for NLG (Lemon,
2011), or summarisation, where representations
can be automatically extracted from coherent,
well-formed text (Barzilay and Elhadad, 2011; Al-
thaus et al., 2004). However, there are other appli-
cations, such as automatic speech recognition and
SMT that could possibly benefit from NLG, but
which do not generate reliable linguistic annota-
tion in their output. For these problems it would
be useful to have systems, as described in this pa-
per, which do not require rich input representa-
tions. We plan to investigate these applications in
future work.
There is much opportunity for future develop-
ment. To improve coverage, the grammars of Sec-
tion 2.1 could perform generation with overlap-
ping, rather than concatenated, n-grams; and fea-
tures could be included to define tuneable log-
linear rule probabilities (Och and Ney, 2002; Chi-
ang, 2007). The GYRO grammar could be ex-
tended using techniques from string-to-tree SMT,
in particular by modifying the grammar so that
output derivations respect dependencies (Shen et
</bodyText>
<footnote confidence="0.971484666666667">
6Surface Realisation Task, Generation Challenges 2011,
www.nltg.brighton.ac.uk/research/
genchal11
</footnote>
<bodyText confidence="0.991972">
al., 2010); this will make it easier to integrate de-
pendency LMs into GYRO. Finally, it would be
interesting to couple the GYRO architecture with
automata-based models of poetry and rhythmic
text (Greene et al., 2010).
</bodyText>
<sectionHeader confidence="0.959487" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99995475">
The research leading to these results has received
funding from the European Union Seventh
Framework Programme (FP7-ICT-2009-4)
under grant agreement number 247762, the
FAUST project faust-fp7.eu/faust/,
and the EPSRC (UK) Programme Grant
EP/I031022/1 (Natural Speech Technology)
natural-speech-technology.org .
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9985725">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11–23,
Prague, Czech Republic.
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent dis-
courses. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, page
399. Association for Computational Linguistics.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th conference on
Computational linguistics - Volume 1, COLING ’00,
pages 42–48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.991192">
266
</page>
<reference confidence="0.996555372727273">
Regina Barzilay and Noemie Elhadad. 2011. In-
ferring strategies for sentence ordering in multi-
document news summarization. arXiv preprint
arXiv:1106.1820.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and eval-
uation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 217–226,
Nancy, France.
Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,
and Michael White. 2012. The surface realisation
task: Recent developments and future plans. In Pro-
ceedings of the 7th International Natural Language
Generation Conference, pages 136–140, Utica, IL,
USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010a. Efficient path counting transducers
for minimum Bayes-risk decoding of statistical ma-
chine translation lattices. In Proceedings of ACL:
Short Papers, pages 27–32, Uppsala, Sweden.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010b. Fluency constraints for minimum
Bayes-risk decoding of statistical machine transla-
tion lattices. In Proceedings of COLING, pages 71–
79, Beijing, China.
Bernd Bohnet, Simon Mille, Benoit Favre, and Leo
Wanner. 2011. &lt;StuMaBa&gt;: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232–235,
Nancy, France.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-CoNLL, pages 858–867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
HLT-NAACL: Short Papers, pages 73–76, Boulder,
CO, USA.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite-state transducers and shallow-n grammars.
Computational Linguistics, 36(3):505–533.
John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In Proceedings of HTL-NAACL, pages
975–983, Los Angeles, CA, USA.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of NAACL-HLT, pages 162–
171, Montr´eal, Canada.
Dominic Espinosa, Rajakrishnan Rajkumar, Michael
White, and Shoshana Berleant. 2010. Further
meta-evaluation of broad-coverage surface realiza-
tion. In Proceedings of EMNLP, pages 564–574,
Cambridge, MA, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376–384,
Beijing, China.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of EMNLP, pages 524–533, Cambridge,
MA, USA.
Yuqing Guo, Josef Van Genabith, and Haifeng Wang.
2011. Dependency-based n-gram models for gen-
eral purpose sentence realisation. Natural Language
Engineering, 17(04):455–483.
Gonzalo Iglesias, Cyril Allauzen, William Byrne,
Adri`a de Gispert, and Michael Riley. 2011. Hi-
erarchical phrase-based translation representations.
In Proceedings of EMNLP, pages 1373–1383, Edin-
burgh, Scotland, UK.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607–615.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proceedings of ACL/COLING, pages 704–
710, Montreal, Quebec, Canada.
Oliver Lemon. 2011. Learning what to say and how to
say it: Joint optimisation of spoken dialogue man-
agement and natural language generation. Com-
puter Speech &amp; Language, 25(2):210–221.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-
expressions: A formalism for representing and pars-
ing finite languages in natural language processing.
Journal of Artificial Intelligence Research, 21:287–
317.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295–302, Philadelphia, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318, Philadelphia, PA, USA.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98:5–24.
</reference>
<page confidence="0.961421">
267
</page>
<reference confidence="0.999403113207547">
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194–201, Seattle, WA, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649–671.
Artem Sokolov, Guillaume Wisniewski, and Francois
Yvon. 2012. Computing lattice bleu oracle scores
for machine translation. In Proceedings of EACL,
pages 120–129, Avignon, France.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of ACL, pages 66–74, Ann
Arbor, MI, USA.
Radu Soricut and Daniel Marcu. 2006. Stochastic
Language Generation Using WIDL-Expressions and
its Application in Machine Translation and Summa-
rization. In Proceedings of ACL, pages 1105–1112,
Sydney, Australia.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007–1016, Singapore.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, pages 620–629, Honolulu,
Hawaii, USA.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of EACL, pages 852–
860, Athens, Greece.
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface real-
ization with ccg. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+ MT).
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. Proceedings of Hu-
man Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
172–179.
Yue Zhang and Stephen Clark. 2011. Syntax-
based Grammaticality Improvement using CCG and
Guided Search. In Proceedings of EMNLP, pages
1147–1157, Edinburgh, Scotland, U.K.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating
a large-scale language model. In Proceedings of
EACL, pages 736–746, Avignon, France.
</reference>
<page confidence="0.996852">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839867">
<title confidence="0.995546">Word Ordering with Phrase-Based Grammars</title>
<author confidence="0.972998">Adri`a de_Gispert</author>
<author confidence="0.972998">Marcus Tomalin</author>
<author confidence="0.972998">William</author>
<affiliation confidence="0.999911">Department of Engineering, University of Cambridge,</affiliation>
<email confidence="0.91433">ad465@cam.ac.uk,mt126@cam.ac.uk,wjb31@cam.ac.uk</email>
<abstract confidence="0.9973148">We describe an approach to word ordering using modelling techniques from statistical machine translation. The system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent, grammatical sentences. We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10646" citStr="Allauzen et al., 2007" startWordPosition="1927" endWordPosition="1930">ws in the grid so that the result has the form of a Recursive Transition Network with a finite depth of recursion. Following the example from Section 2.1, the top three FSAs in Figure 3 represent the RTN for example from Figure 1. The parsing algorithm is modified as follows: 4 add rule I(A) --+ u as path to FSA FX(I(A)),Y (I(A)) ... 10 add rule I&apos; --+ Ix,y I(A) as path to FSA FX(I,),Y (I,) 11 if NumStates(Fv,1)&gt;1,success. At this point we specify two strategies: Algorithm 1: Full expansion is described by the pseudocode in Figure 4, excluding lines 2-3. A recursive FSA replacement operation (Allauzen et al., 2007) can be used to expand the FSA in the top-most cell. In our running example, the result 11000 0 3 b a a 2 1 b Expansion of RTN 11111 0 b d a 2 7 1 b a e d e 3 4 5 8 c 9 b a 10 11 c b a 6 11111 0 00111 11000 3 00111 11000 d e c 0 1 2 3 2 1 00111 261 is the FSA at the bottom of Figure 3. We then apply a word-based LM to the resulting FSA via standard FSA composition. This outputs the complete (unpruned) language of interest, where each word sequence generated from the bag according to the phrasal constraints is scored by the LM. Algorithm 2: Pruned expansion is described by the pseudocode in Fig</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernst Althaus</author>
<author>Nikiforos Karamanis</author>
<author>Alexander Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>399</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1081" citStr="Althaus et al., 2004" startWordPosition="156" endWordPosition="159"> We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of gene</context>
<context position="29524" citStr="Althaus et al., 2004" startWordPosition="5241" endWordPosition="5245">YRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features could be included to d</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 399. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00,</booktitle>
<pages>42--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26958" citStr="Bangalore and Rambow, 2000" startWordPosition="4810" endWordPosition="4813">ed in 45 cases, CCG was preferred in 14 cases, and systems were tied 16 times. This is consistent with the significant difference in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1 the list of critics of bush ’s policy in iraq a third republican </context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00, pages 42–48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization. arXiv preprint arXiv:1106.1820.</title>
<date>2011</date>
<contexts>
<context position="1271" citStr="Barzilay and Elhadad, 2011" startWordPosition="183" endWordPosition="186">sults reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, tr</context>
<context position="29501" citStr="Barzilay and Elhadad, 2011" startWordPosition="5237" endWordPosition="5240">vel BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features</context>
</contexts>
<marker>Barzilay, Elhadad, 2011</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2011. Inferring strategies for sentence ordering in multidocument news summarization. arXiv preprint arXiv:1106.1820.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Mike White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first surface realisation shared task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>217--226</pages>
<location>Nancy, France.</location>
<contexts>
<context position="29156" citStr="Belz et al., 2011" startWordPosition="5187" endWordPosition="5190">olicy in iraq , just days 82.3 before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling 100 for a new strategy just days before a new confrontation in congress Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be use</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 217–226, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Bernd Bohnet</author>
<author>Simon Mille</author>
<author>Leo Wanner</author>
<author>Michael White</author>
</authors>
<title>The surface realisation task: Recent developments and future plans.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th International Natural Language Generation Conference,</booktitle>
<pages>136--140</pages>
<location>Utica, IL, USA.</location>
<contexts>
<context position="29176" citStr="Belz et al., 2012" startWordPosition="5191" endWordPosition="5194">t days 82.3 before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling 100 for a new strategy just days before a new confrontation in congress Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems,</context>
</contexts>
<marker>Belz, Bohnet, Mille, Wanner, White, 2012</marker>
<rawString>Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner, and Michael White. 2012. The surface realisation task: Recent developments and future plans. In Proceedings of the 7th International Natural Language Generation Conference, pages 136–140, Utica, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Efficient path counting transducers for minimum Bayes-risk decoding of statistical machine translation lattices.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL: Short Papers,</booktitle>
<pages>27--32</pages>
<location>Uppsala,</location>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010a. Efficient path counting transducers for minimum Bayes-risk decoding of statistical machine translation lattices. In Proceedings of ACL: Short Papers, pages 27–32, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Fluency constraints for minimum Bayes-risk decoding of statistical machine translation lattices.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>71--79</pages>
<location>Beijing, China.</location>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010b. Fluency constraints for minimum Bayes-risk decoding of statistical machine translation lattices. In Proceedings of COLING, pages 71– 79, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Simon Mille</author>
<author>Benoit Favre</author>
<author>Leo Wanner</author>
</authors>
<title>From deep representation to surface.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>232--235</pages>
<location>Nancy, France.</location>
<contexts>
<context position="29137" citStr="Bohnet et al., 2011" startWordPosition="5183" endWordPosition="5186">s of george bush ’s policy in iraq , just days 82.3 before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling 100 for a new strategy just days before a new confrontation in congress Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these probl</context>
</contexts>
<marker>Bohnet, Mille, Favre, Wanner, 2011</marker>
<rawString>Bernd Bohnet, Simon Mille, Benoit Favre, and Leo Wanner. 2011. &lt;StuMaBa&gt;: From deep representation to surface. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 232–235, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>858--867</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="21511" citStr="Brants et al., 2007" startWordPosition="3873" endWordPosition="3876">t from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Rescoring GYRO output We now report on rescoring procedures intended to improve the first-pass lattices generated by GYRO. Higher-order language models. The first row in Table 2 reports the result obtained when applying a 5-gram LM to the GYRO lattices generated under a 4-gram. The 5-gram is estimated over the complete 10.6 billion word collection using the uniform backoff strategy of (Brants et al., 2007). We find improvements of 3.0 and 1.9 BLEU with respect to the 4-gram baseline. Dependency language models. We now investigate the benefits of applying a dependency LM (Shen et al., 2010) in a rescoring mode. We run the MALT dependency parser5 on the generation hypotheses and rescore them according to lo9(pLM) + Adlo9(pdepLM), i.e. a weighted combination of the word-based LM and the dependency LM scores. Since it is not possible to run the parser on the entire lattice, we carry out this experiment using the 100-best lists generated from the previous experiment (‘+5g’). The dependency LM is a 3</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-CoNLL, pages 858–867, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2372" citStr="Chiang, 2007" startWordPosition="369" endWordPosition="370">rder Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag. Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessment</context>
<context position="30201" citStr="Chiang, 2007" startWordPosition="5354" endWordPosition="5356">h recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features could be included to define tuneable loglinear rule probabilities (Och and Ney, 2002; Chiang, 2007). The GYRO grammar could be extended using techniques from string-to-tree SMT, in particular by modifying the grammar so that output derivations respect dependencies (Shen et 6Surface Realisation Task, Generation Challenges 2011, www.nltg.brighton.ac.uk/research/ genchal11 al., 2010); this will make it easier to integrate dependency LMs into GYRO. Finally, it would be interesting to couple the GYRO architecture with automata-based models of poetry and rhythmic text (Greene et al., 2010). Acknowledgement The research leading to these results has received funding from the European Union Seventh </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL: Short Papers,</booktitle>
<pages>73--76</pages>
<location>Boulder, CO, USA.</location>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and William Byrne. 2009. Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions. In Proceedings of HLT-NAACL: Short Papers, pages 73–76, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars. Computational Linguistics, 36(3):505–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of HTL-NAACL,</booktitle>
<pages>975--983</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="24107" citStr="DeNero et al., 2010" startWordPosition="4334" endWordPosition="4337">90 85 80 75 70 65 60 55 50 BLEU SPR 40 30 90 80 70 60 50 20 10 0 Sentence Precision Rate (SPR) terior probabilities extracted from (a) the same generation lattice, and (b) from lattices produced by an Arabic-to-English hierarchical-phrase based MT system developed for the NIST 2012 OpenMT Evaluation. As noted, LMBR relies on a posterior distribution over n-grams as part of its computation or risk. Here, we use LMBR with a posterior of the form α pGYRO + (1–α) pMT. This is effectively performing a system combination between the GYRO generation system and the MT system (de Gispert et al., 2009; DeNero et al., 2010) but restricting the hypothesis space to be that of the GYRO lattice (Blackwood et al., 2010b). Results are reported in the last two rows of Table 2. Relative to 5-gram LM rescoring alone, we see gains in BLEU of 2.3 and 4.4 in MT08-nw and MT09- nw, suggesting that posterior distributions over ngrams provided by SMT systems can give good guidance in generation. These results also suggest that if we knew what words to use, we could generate very good quality translation output. 3.3 Analysis and examples Figure 7 gives GYRO generation examples. These are often fairly fluent, and it is striking h</context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Proceedings of HTL-NAACL, pages 975–983, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Daniel Marcu</author>
</authors>
<title>Hyter: Meaning-equivalent semantics for translation evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>162--171</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="25027" citStr="Dreyer and Marcu, 2012" startWordPosition="4493" endWordPosition="4496">rams provided by SMT systems can give good guidance in generation. These results also suggest that if we knew what words to use, we could generate very good quality translation output. 3.3 Analysis and examples Figure 7 gives GYRO generation examples. These are often fairly fluent, and it is striking how the output can be improved with guidance from the SMT system. The examples also show the harshness of BLEU, e.g. ‘german and turkish officials’ is penalised with respect to ‘ turkish and german officials.’ Metrics based on richer meaning representations, such as HyTER, could be valuable here (Dreyer and Marcu, 2012). Figure 8 shows BLEU and Sentence Precision Rate (SPR), the percentage of exactly reconstructed sentences. As expected, performance is sensitive to length. For bags of up to 10, GYRO reconstructs the reference perfectly in over 65% of the cases. This is a harsh performance metric, and performance falls to less than 10% for bags of size 16-20. For bags of 6-10 words, we find BLEU scores of greater than 85. Performance is Figure 8: GYRO BLEU score and Sentence Precision Rate as a function of the bag of words size. Computed on the concatenation of MT08-nw and MT09-nw. not as good for shorter seg</context>
</contexts>
<marker>Dreyer, Marcu, 2012</marker>
<rawString>Markus Dreyer and Daniel Marcu. 2012. Hyter: Meaning-equivalent semantics for translation evaluation. In Proceedings of NAACL-HLT, pages 162– 171, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Espinosa</author>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
<author>Shoshana Berleant</author>
</authors>
<title>Further meta-evaluation of broad-coverage surface realization.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>564--574</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="18506" citStr="Espinosa et al., 2010" startWordPosition="3334" endWordPosition="3337">ate output from much larger input sets than reported here. We focus here on generation quality for moderate sized problems. 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4- gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM System MT08-nw MT09-nw 4g CCG 48.0 48.8 3g GYRO 59.0 58.4 GYRO +3g 63.0 64.1 4g GYRO +4g 65.5 65.9 100-best oracle 76.1 76.1 lattice oracle 80.4 80.2 Table 1: CCG and GYRO BLEU scores. 4-grams and 5-grams.3 Under these conditions, GYRO finds an output for 91.4% of the bags. For the remainder, we obtain an output either by pruning less or by adding bigram rules (in 7.2% of the bags), or simply by adding all words </context>
</contexts>
<marker>Espinosa, Rajkumar, White, Berleant, 2010</marker>
<rawString>Dominic Espinosa, Rajakrishnan Rajkumar, Michael White, and Shoshana Berleant. 2010. Further meta-evaluation of broad-coverage surface realization. In Proceedings of EMNLP, pages 564–574, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>376--384</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1335" citStr="Genzel, 2010" startWordPosition="196" endWordPosition="197">ge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words f</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of COLING, pages 376–384, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erica Greene</author>
<author>Tugba Bodrumlu</author>
<author>Kevin Knight</author>
</authors>
<title>Automatic analysis of rhythmic poetry with applications to generation and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>524--533</pages>
<location>Cambridge, MA, USA.</location>
<marker>Greene, Bodrumlu, Knight, 2010</marker>
<rawString>Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010. Automatic analysis of rhythmic poetry with applications to generation and translation. In Proceedings of EMNLP, pages 524–533, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Josef Van Genabith</author>
<author>Haifeng Wang</author>
</authors>
<title>Dependency-based n-gram models for general purpose sentence realisation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>04</issue>
<marker>Guo, Van Genabith, Wang, 2011</marker>
<rawString>Yuqing Guo, Josef Van Genabith, and Haifeng Wang. 2011. Dependency-based n-gram models for general purpose sentence realisation. Natural Language Engineering, 17(04):455–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Cyril Allauzen</author>
<author>William Byrne</author>
<author>Adri`a de Gispert</author>
<author>Michael Riley</author>
</authors>
<title>Hierarchical phrase-based translation representations.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1373--1383</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Iglesias, Allauzen, Byrne, de Gispert, Riley, 2011</marker>
<rawString>Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley. 2011. Hierarchical phrase-based translation representations. In Proceedings of EMNLP, pages 1373–1383, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1138" citStr="Knight, 1999" startWordPosition="167" endWordPosition="168">ures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We des</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/COLING,</booktitle>
<pages>704--710</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="26849" citStr="Langkilde and Knight, 1998" startWordPosition="4797" endWordPosition="4800">ach comparison was made by at least 3 judges. With an average selection confidence of 0.754, GYRO was preferred in 45 cases, CCG was preferred in 14 cases, and systems were tied 16 times. This is consistent with the significant difference in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of ACL/COLING, pages 704– 710, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
</authors>
<title>Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="29369" citStr="Lemon, 2011" startWordPosition="5223" endWordPosition="5224">trategy just days before a new confrontation in congress Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To impr</context>
</contexts>
<marker>Lemon, 2011</marker>
<rawString>Oliver Lemon. 2011. Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation. Computer Speech &amp; Language, 25(2):210–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>IDLexpressions: A formalism for representing and parsing finite languages in natural language processing.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>21</volume>
<pages>317</pages>
<contexts>
<context position="27031" citStr="Nederhof and Satta, 2004" startWordPosition="4818" endWordPosition="4821">s. This is consistent with the significant difference in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1 the list of critics of bush ’s policy in iraq a third republican senator joins . 82.9 REF it added that these messages were sent to presid</context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>Mark-Jan Nederhof and Giorgio Satta. 2004. IDLexpressions: A formalism for representing and parsing finite languages in natural language processing. Journal of Artificial Intelligence Research, 21:287– 317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="30186" citStr="Och and Ney, 2002" startWordPosition="5350" endWordPosition="5353"> as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features could be included to define tuneable loglinear rule probabilities (Och and Ney, 2002; Chiang, 2007). The GYRO grammar could be extended using techniques from string-to-tree SMT, in particular by modifying the grammar so that output derivations respect dependencies (Shen et 6Surface Realisation Task, Generation Challenges 2011, www.nltg.brighton.ac.uk/research/ genchal11 al., 2010); this will make it easier to integrate dependency LMs into GYRO. Finally, it would be interesting to couple the GYRO architecture with automata-based models of poetry and rhythmic text (Greene et al., 2010). Acknowledgement The research leading to these results has received funding from the European</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, pages 295–302, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="18482" citStr="Papineni et al., 2002" startWordPosition="3330" endWordPosition="3333">quite readily and generate output from much larger input sets than reported here. We focus here on generation quality for moderate sized problems. 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4- gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM System MT08-nw MT09-nw 4g CCG 48.0 48.8 3g GYRO 59.0 58.4 GYRO +3g 63.0 64.1 4g GYRO +4g 65.5 65.9 100-best oracle 76.1 76.1 lattice oracle 80.4 80.2 Table 1: CCG and GYRO BLEU scores. 4-grams and 5-grams.3 Under these conditions, GYRO finds an output for 91.4% of the bags. For the remainder, we obtain an output either by pruning less or by adding bigram rules (in 7.2% of the bags), or sim</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Pino</author>
<author>Aurelien Waite</author>
<author>William Byrne</author>
</authors>
<title>Simple and efficient model filtering in statistical machine translation.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>98--5</pages>
<contexts>
<context position="16276" citStr="Pino et al., 2012" startWordPosition="2956" endWordPosition="2959"> in this way. For each of these sentences we create a bag. The GYRO system uses a n-gram LM estimated over 1.3 billion words of English text, including the AFP and Xinhua portions of the GigaWord corpus version 4 (1.1 billion words) and the English side of various Arabic-English parallel corpora typically used in MT evaluations (0.2 billion words). Phrases of up to length 5 are extracted for each bag from a text collection containing 10.6 billion words of English news text. We use efficient Hadoop-based look-up techniques to carry out this extraction step and to retrieve rules for generation (Pino et al., 2012). The average number of phrases extracted as a function of the size of the bag is shown in Figure 5. These are the phrasebased rules of our generation grammar. 3.1 Computational Analysis We analyze here the computational requirements of the three alternative GYRO algorithms presented in Sections 2.3 and 2.4. We carry out this analysis on a subset of 200 random subsentences from MT08-nw and MT09-nw chosen to have the same sentence length distribution as the whole data set. For a fixed generation grammar comprised of 3-gram, 4-gram and 5-gram rules only, we run each algorithm with a memory limit</context>
</contexts>
<marker>Pino, Waite, Byrne, 2012</marker>
<rawString>Juan Pino, Aurelien Waite, and William Byrne. 2012. Simple and efficient model filtering in statistical machine translation. The Prague Bulletin of Mathematical Linguistics, 98:5–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>194--201</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="26891" citStr="Ratnaparkhi, 2000" startWordPosition="4804" endWordPosition="4805">an average selection confidence of 0.754, GYRO was preferred in 45 cases, CCG was preferred in 14 cases, and systems were tied 16 times. This is consistent with the significant difference in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of NAACL, pages 194–201, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="2572" citStr="Shen et al., 2010" startWordPosition="401" endWordPosition="404">s are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessments confirm these substantial improvements. 2 Phrase-based Word Ordering We take as input a bag of N words Q = {wi, ... , wN}. The words are sorted, e.g. alphabetically, so that it is possible to refer </context>
<context position="21698" citStr="Shen et al., 2010" startWordPosition="3905" endWordPosition="3908">ore is not 100. 3.2.1 Rescoring GYRO output We now report on rescoring procedures intended to improve the first-pass lattices generated by GYRO. Higher-order language models. The first row in Table 2 reports the result obtained when applying a 5-gram LM to the GYRO lattices generated under a 4-gram. The 5-gram is estimated over the complete 10.6 billion word collection using the uniform backoff strategy of (Brants et al., 2007). We find improvements of 3.0 and 1.9 BLEU with respect to the 4-gram baseline. Dependency language models. We now investigate the benefits of applying a dependency LM (Shen et al., 2010) in a rescoring mode. We run the MALT dependency parser5 on the generation hypotheses and rescore them according to lo9(pLM) + Adlo9(pdepLM), i.e. a weighted combination of the word-based LM and the dependency LM scores. Since it is not possible to run the parser on the entire lattice, we carry out this experiment using the 100-best lists generated from the previous experiment (‘+5g’). The dependency LM is a 3-gram estimated on the entire GigaWord version 5 collection (∼5 billion words). Results are shown in rows 2 and 3 in Table 2, where in each row the performance over the set used to tune t</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Francois Yvon</author>
</authors>
<title>Computing lattice bleu oracle scores for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>120--129</pages>
<location>Avignon, France.</location>
<contexts>
<context position="20631" citStr="Sokolov et al., 2012" startWordPosition="3718" endWordPosition="3721">mutation under the LM with the pruning mechanisms described. The BLEU scores are 54.0 and 54.5 for MT08-nw and MT09 respectively. This indicates that a strong GYRO grammar is very much needed for this type of parsing and generation. Quality of generated lattices. We assess the quality of the lattices output by GYRO under the 4-gram LM by computing the oracle BLEU score of either the 100-best lists or the whole lattices4 in the last two rows of Table 1. In order to compute the latter, we use the linear approximation to BLEU that allows an efficient FST-based implementation of an Oracle search (Sokolov et al., 2012). We draw two conclusions from these results: (a) that there is a significant potential for im3Any word in the bag that does not occur in the large collection of English material is added as a 1-gram rule. 4Obtained by pruning at β = 10 in generation. provement from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Rescoring GYRO output We now report on rescoring procedures intended to improve the first-pass lattices generated by GYRO. Higher</context>
</contexts>
<marker>Sokolov, Wisniewski, Yvon, 2012</marker>
<rawString>Artem Sokolov, Guillaume Wisniewski, and Francois Yvon. 2012. Computing lattice bleu oracle scores for machine translation. In Proceedings of EACL, pages 120–129, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Towards developing generation algorithms for text-to-text applications.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>66--74</pages>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="14235" citStr="Soricut and Marcu (2005" startWordPosition="2604" endWordPosition="2607"> PRUNE-ROW(x) 4 F FSA-REPLACE(FN,l) 5 return L F ~ G 6 function PRUNE-ROW(x) : F � 7 r Fx,r 8 F FSA-REPLACE(F) 9 F F ~ G 10 F FSA-PRUNE(F, β) 11 for each cell y = 1 ... (N) x 12 Fx,r Fx,r · F 13 return Figure 4: Pseudocode for Algorithm 1 (excluding lines 2-3) and Algorithm 2 (including all lines). the LM during parsing. The pseudocode is identical to that of Algorithm 1 except for the following changes: in parsing (Figure 2) we pass G as input and we call the row pruning function of Figure 4 after line 5 if x ≥ r. We note that there is a strong connection between GYRO and the IDL approach of Soricut and Marcu (2005; 2006). Our bag of words parser could be cast in the IDL-formalism, and the FSA ‘Replace’ operation would be expressed by an IDL ‘Unfold’ operation. However, whereas their work applies pruning in the creation of the IDLexpression prior to LM application, GYRO uses unweighted phrase constraints so the LM must be considered for pruning while parsing. 3 Experimental Results We now report various experiments evaluating the performance of the generation approach described above. The system is evaluated using the MT08- nw, and MT09-nw testsets. These correspond to the first English reference of the</context>
<context position="27056" citStr="Soricut and Marcu, 2005" startWordPosition="4822" endWordPosition="4825"> the significant difference in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1 the list of critics of bush ’s policy in iraq a third republican senator joins . 82.9 REF it added that these messages were sent to president bashar al-asad throug</context>
</contexts>
<marker>Soricut, Marcu, 2005</marker>
<rawString>Radu Soricut and Daniel Marcu. 2005. Towards developing generation algorithms for text-to-text applications. In Proceedings of ACL, pages 66–74, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Stochastic Language Generation Using WIDL-Expressions and its Application in Machine Translation and Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1105--1112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="27082" citStr="Soricut and Marcu, 2006" startWordPosition="4826" endWordPosition="4829">ce in BLEU between these systems. 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1 the list of critics of bush ’s policy in iraq a third republican senator joins . 82.9 REF it added that these messages were sent to president bashar al-asad through turkish and german offic</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Stochastic Language Generation Using WIDL-Expressions and its Application in Machine Translation and Summarization. In Proceedings of ACL, pages 1105–1112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="1320" citStr="Tromble and Eisner, 2009" startWordPosition="192" endWordPosition="195">riven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consis</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of EMNLP, pages 1007–1016, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum BayesRisk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--629</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="22615" citStr="Tromble et al., 2008" startWordPosition="4067" endWordPosition="4070">ut this experiment using the 100-best lists generated from the previous experiment (‘+5g’). The dependency LM is a 3-gram estimated on the entire GigaWord version 5 collection (∼5 billion words). Results are shown in rows 2 and 3 in Table 2, where in each row the performance over the set used to tune the parameter Ad is marked with *. In either case, we observe modest but consistent gains across both sets. We find this very promising considering that the parser has been applied to noisy input sentences. Minimum Bayes Risk Decoding. We also use Lattice-based Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008; Blackwood et al., 2010a). Here, the posteriors over n-grams are computed over the output lattices generated by the GYRO system. The result is shown in row labeled ‘+5g +LMBR’, where again we find modest but consistent gains across the two sets with respect to the 5-gram rescored lattices. LMBR with MT posteriors. We investigate LMBR decoding when applying to the generation lattice a linear combination of the n-gram pos5Available at www.maltparser.org 264 4g GYRO rescoring: MT08-nw MT09-nw +5g 68.5 67.8 +5g +depLM Ad = 0.4 68.7 ? 68.1 +5g +depLM Ad = 0.33 68.7 68.2 ? +5g +LMBR 68.6 68.3 +5g +</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum BayesRisk decoding for statistical machine translation. In Proceedings of EMNLP, pages 620–629, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>852--860</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1481" citStr="Wan et al. (2009)" startWordPosition="218" endWordPosition="221">age models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag. Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reorde</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2009</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In Proceedings of EACL, pages 852– 860, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards broad coverage surface realization with ccg.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT).</booktitle>
<contexts>
<context position="27129" citStr="White et al., 2007" startWordPosition="4834" endWordPosition="4837"> Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 Hypothesis SBLEU REF a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . 47.2 critics of bush ’s policy in iraq joins the list of a third republican senator. 69.8 critics of bush ’s iraq policy in a list of republican senator joins the third. 39.1 the list of critics of bush ’s policy in iraq a third republican senator joins . 82.9 REF it added that these messages were sent to president bashar al-asad through turkish and german officials . (a-c) it added that president bashar al-</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with ccg. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT-07),</booktitle>
<pages>172--179</pages>
<contexts>
<context position="29057" citStr="Wong and Mooney, 2007" startWordPosition="5169" endWordPosition="5172">n a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , just days 82.3 before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling 100 for a new strategy just days before a new confrontation in congress Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6. These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT-07), pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntaxbased Grammaticality Improvement using CCG and Guided Search.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1147--1157</pages>
<location>Edinburgh, Scotland, U.K.</location>
<contexts>
<context position="1560" citStr="Zhang and Clark (2011" startWordPosition="231" endWordPosition="234">1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. These techniques are applied to the unconstrained problem of generating a sentence from a multi-set of input words. We describe GYRO (Get Your Order Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag. Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram langua</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntaxbased Grammaticality Improvement using CCG and Guided Search. In Proceedings of EMNLP, pages 1147–1157, Edinburgh, Scotland, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Graeme Blackwood</author>
<author>Stephen Clark</author>
</authors>
<title>Syntax-based word ordering incorporating a large-scale language model.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>736--746</pages>
<location>Avignon, France.</location>
<contexts>
<context position="18158" citStr="Zhang et al., 2012" startWordPosition="3274" endWordPosition="3277">to 20 words), and its memory requirements can be controlled via the beam-width pruning parameter β. Harsher pruning (i.e. lower β) will incur more coverage problems, so it is desirable to use the highest feasible value of β. We emphasise that Algorithm 3, with suitable pruning strategies, can scale up to larger problems quite readily and generate output from much larger input sets than reported here. We focus here on generation quality for moderate sized problems. 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4- gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM System MT08-nw MT09-nw 4g CCG 48.0 48.8 3g GYRO 59.0 58.4 GYRO +3g 6</context>
</contexts>
<marker>Zhang, Blackwood, Clark, 2012</marker>
<rawString>Yue Zhang, Graeme Blackwood, and Stephen Clark. 2012. Syntax-based word ordering incorporating a large-scale language model. In Proceedings of EACL, pages 736–746, Avignon, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>