<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.69083175">
Minimally Supervised Method for Multilingual Paraphrase Extraction
from Definition Sentences on the Web
Yulan Yan∗ Chikara Hashimoto‡ Kentaro Torisawa§
Takao Kawai¶ Jun’ichi Kazamak Stijn De Saeger∗∗
</title>
<author confidence="0.826076">
∗ ‡ § ¶k ∗∗ Information Analysis Laboratory
</author>
<affiliation confidence="0.991326">
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
</affiliation>
<email confidence="0.93975">
{∗ yulan, ‡ ch, § torisawa, ∗∗stijn}@nict.go.jp
</email>
<sectionHeader confidence="0.992205" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804105263158">
We propose a minimally supervised method
for multilingual paraphrase extraction from
definition sentences on the Web. Hashimoto
et al. (2011) extracted paraphrases from
Japanese definition sentences on the Web, as-
suming that definition sentences defining the
same concept tend to contain paraphrases.
However, their method requires manually an-
notated data and is language dependent. We
extend their framework and develop a mini-
mally supervised method applicable to multi-
ple languages. Our experiments show that our
method is comparable to Hashimoto et al.’s
for Japanese and outperforms previous unsu-
pervised methods for English, Japanese, and
Chinese, and that our method extracts 10,000
paraphrases with 92% precision for English,
82.5% precision for Japanese, and 82% preci-
sion for Chinese.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879266666667">
Automatic paraphrasing has been recognized as an
important component for NLP systems, and many
methods have been proposed to acquire paraphrase
knowledge (Lin and Pantel, 2001; Barzilay and
McKeown, 2001; Shinyama et al., 2002; Barzilay
and Lee, 2003; Dolan et al., 2004; Callison-Burch,
2008; Hashimoto et al., 2011; Fujita et al., 2012).
We propose a minimally supervised method for
multilingual paraphrase extraction. Hashimoto et al.
(2011) developed a method to extract paraphrases
from definition sentences on the Web, based on
their observation that definition sentences defining
the same concept tend to contain many paraphrases.
Their method consists of two steps; they extract def-
inition sentences from the Web, and extract phrasal
</bodyText>
<page confidence="0.991012">
63
</page>
<figureCaption confidence="0.57285825">
(1) a. Paraphrasing is the use of your own words to express the au-
thor’s ideas without changing the meaning.
b. Paraphrasing is defined as a process of transforming an expres-
sion into another while keeping its meaning intact.
</figureCaption>
<bodyText confidence="0.68285825">
(2) a. ROM; _(/,�KJ�R�.����R1/4��+·.
J�+�c#zK o . (Paraphrasing refers to
the replacement of an expression into another without changing
the semantic content.)
</bodyText>
<equation confidence="0.923997333333333">
b.o # z ( / , � K o ; J X R &apos; � K
���D��RÈ#�&gt;&gt;� � ; . · . J � + 1/4
�tKZT1&apos;S3K. (Paraphrasing is a process of trans-
</equation>
<bodyText confidence="0.91753">
forming an expression into another of the same language while
preserving the meaning and content as much as possible.)
</bodyText>
<listItem confidence="0.9678025">
(3) a.意i是指i者在不改变原文意思的前提下,完全改变原
文的句子结构。 (Paraphrasing refers to the transformation
of sentence structure by the translator without changing the
meaning of original text.)
</listItem>
<bodyText confidence="0.337149666666667">
b.意i是指只保#原文内容,不保#原文形式的翻i方法。
(Paraphrasing is a translation method of keeping the content of
original text but not keeping the expression.)
</bodyText>
<figureCaption confidence="0.999758">
Figure 1: Multilingual definition pairs on “paraphrasing.”
</figureCaption>
<bodyText confidence="0.999752222222222">
paraphrases from the definition sentences. Both
steps require supervised classifiers trained by manu-
ally annotated data, and heavily depend on their tar-
get language. However, the basic idea is actually
language-independent. Figure 1 gives examples of
definition sentences on the Web that define the same
concept in English, Japanese, and Chinese (with En-
glish translation). As indicated by underlines, each
definition pair has a phrasal paraphrase.
We aim at extending Hashimoto et al.’s method
to a minimally supervised method, thereby enabling
acquisition of phrasal paraphrases within one lan-
guage, but in different languages without manually
annotated data. The first contribution of our work
is to develop a minimally supervised method for
multilingual definition extraction that uses a clas-
sifier distinguishing definition from non-definition.
The classifier is learnt from the first sentences in
</bodyText>
<note confidence="0.551124666666667">
Proceedings of NAACL-HLT 2013, pages 63–73,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
Definition Extraction (Section 2.1) Paraphrase Extraction (Section 2.2)
</note>
<figureCaption confidence="0.998253">
Figure 2: Overall picture of our method.
</figureCaption>
<bodyText confidence="0.999966883333333">
Wikipedia articles, which can be regarded as the def-
inition of the title of Wikipedia article (Kazama and
Torisawa, 2007) and hence can be used as positive
examples. Our method relies on a POS tagger, a de-
pendency parser, a NER tool, noun phrase chunking
rules, and frequency thresholds for each language,
in addition to Wikipedia articles, which can be seen
as a manually annotated knowledge base. How-
ever, our method needs no additional manual anno-
tation particularly for this task and thus we catego-
rize our method as a minimally supervised method.
On the other hand, Hashimoto et al.’s method heav-
ily depends on the properties of Japanese like the
assumption that characteristic expressions of defini-
tion sentences tend to appear at the end of sentence
in Japanese. We show that our method is applica-
ble to English, Japanese, and Chinese, and that its
performance is comparable to state-of-the-art super-
vised methods (Navigli and Velardi, 2010). Since
the three languages are very different we believe that
our definition extraction method is applicable to any
language as long as Wikipedia articles of the lan-
guage exist.
The second contribution of our work is to de-
velop a minimally supervised method for multi-
lingual paraphrase extraction from definition sen-
tences. Again, Hashimoto et al.’s method utilizes
a supervised classifier trained with annotated data
particularly prepared for this task. We eliminate the
need for annotation and instead introduce a method
that uses a novel similarity measure considering
the occurrence of phrase fragments in global con-
texts. Our paraphrase extraction method is mostly
language-independent and, through experiments for
the three languages, we show that it outperforms
unsupervised methods (Pas¸ca and Dienes, 2005;
Koehn et al., 2007) and is comparable to Hashimoto
et al.’s supervised method for Japanese.
Previous methods for paraphrase (and entailment)
extraction can be classified into a distributional sim-
ilarity based approach (Lin and Pantel, 2001; Gef-
fet and Dagan, 2005; Bhagat et al., 2007; Szpek-
tor and Dagan, 2008; Hashimoto et al., 2009) and a
parallel corpus based approach (Barzilay and McK-
eown, 2001; Shinyama et al., 2002; Barzilay and
Lee, 2003; Dolan et al., 2004; Callison-Burch,
2008). The former can exploit large scale monolin-
gual corpora, but is known to be unable to distin-
guish paraphrase pairs from antonymous pairs (Lin
et al., 2003). The latter rarely mistakes antonymous
pairs for paraphrases, but preparing parallel corpora
is expensive. As with Hashimoto et al. (2011), our
method is a kind of parallel corpus approach in that it
uses definition pairs as a parallel corpus. However,
our method does not suffer from a high labor cost
of preparing parallel corpora, since it can automati-
cally collect definition pairs from the Web on a large
scale. The difference between ours and Hashimoto
et al.’s is that our method requires no manual label-
ing of data and is mostly language-independent.
</bodyText>
<sectionHeader confidence="0.970697" genericHeader="method">
2 Proposed Method
</sectionHeader>
<bodyText confidence="0.999944">
Our method first extracts definition sentences from
the Web, and then extracts paraphrases from the def-
inition sentences, as illustrated in Figure 2.
</bodyText>
<subsectionHeader confidence="0.928492">
2.1 Definition Extraction
2.1.1 Automatic Construction of Training Data
</subsectionHeader>
<bodyText confidence="0.999940944444444">
Our method learns a classifier that classifies sen-
tences into definition and non-definition using auto-
matically constructed training data, TrDat. TrDat’s
positive examples, Pos, are the first sentences of
Wikipedia articles and the negative examples, Neg,
are randomly sampled Web sentences. The former
can be seen as definition, while the chance that the
sentences in the latter are definition is quite small.
Our definition extraction not only distinguishes
definition from non-definition but also identities the
defined term of a definition sentence, and in the
paraphrase extraction step our method couples two
definition sentences if their defined terms are identi-
cal. For example, the defined terms of (1a) and (1b)
in Figure 1 are both “Paraphrasing” and thus the two
definition sentences are coupled. For Pos, we mark
up the title of Wikipedia article as the defined term.
For Neg, we randomly select a noun phrase in a sen-
</bodyText>
<figure confidence="0.98526576744186">
Definition
sentences
Paraphrase
candidates
Ranked
paraphrase
candidates
Ranking by Score
Definition
pairs
Web Wikipedia
Automatically constructed training data
Web
Classifier
64
[term] is * which is located
English Japanese Chinese
Type Representation Pos Neg Pos Neg Pos Neg
Surface 120 400 30 100 20 100
N-gram Base 120 400 30 100 — —
POS 2,000 4,000 500 500 100 400
Surface 120 400 30 100 20 40
Subsequence Base 120 400 30 100 — —
POS 2,000 2,000 500 500 200 400
Surface 5 10 5 10 5 5
Subtree Base 5 10 5 10 — —
POS 25 50 25 50 25 50
(A)
Subsequence definition pattern
Subsequence non-definition pattern
[term] isa*in the
youmay* [term]
was [term] *,whois
Subtree definition pattern
Subtree non-definition pattern
[term] is defined as the NP
[term] will not be
N-gram definition pattern
N-gram non-definition pattern
ˆ[term] is the
[term] is a type of
[term] maybe
[term] isnot
</figure>
<tableCaption confidence="0.986858">
Table 1: Examples of English patterns.
</tableCaption>
<bodyText confidence="0.9531005">
tence and mark it up as a (false) defined term. Any
marked term is uniformly replaced with [term].
</bodyText>
<subsectionHeader confidence="0.969499">
2.1.2 Feature Extraction and Learning
</subsectionHeader>
<bodyText confidence="0.999970761904762">
As features, we use patterns that are characteristic
of definition (definition patterns) and those that are
unlikely to be a part of definition (non-definition pat-
terns). Patterns are either N-grams, subsequences, or
dependency subtrees, and are mined automatically
from TrDat. Table 1 shows examples of patterns
mined by our method. In (A) of Table 1, “ˆ” is
a symbol representing the beginning of a sentence.
In (B), “*” represents a wildcard that matches any
number of arbitrary words. Patterns are represented
by either their words’ surface form, base form, or
POS. (Chinese words do not inflect and thus we do
not use the base form for Chinese.)
We assume that definition patterns are fre-
quent in Pos but are infrequent in Neg, and
non-definition patterns are frequent in Neg but
are infrequent in Pos. To see if a given pat-
tern φ is likely to be a definition pattern, we
measure φ’s probability rate Rate(φ). If the
probability rate of φ is large, φ tends to be a
definition pattern. The probability rate of φ is:
</bodyText>
<equation confidence="0.986868666666667">
freq(φ,Pos)/|Pos|
Rate(φ) = if freq(φ, Neg) =6 0.
freq(φ,Neg)/|Neg|
</equation>
<bodyText confidence="0.9536794">
Here, freq(φ,Pos) =|{s ∈ Pos : φ ⊆ s} |and
freq(φ, Neg) = |{s ∈ Neg : φ ⊆ s}|. We write φ ⊆ s
if sentences contains φ. If freq(φ, Neg) = 0,
Rate(φ) is set to the largest value of all the patterns’
Rate values. Only patterns whose Rate is more
than or equal to a Rate threshold ρpos and whose
freq(φ, Pos) is more than or equal to a frequency
threshold are regarded as definition patterns. Simi-
larly, we check if φ is likely to be a non-definition
pattern. Only patterns whose Rate is less or equal
</bodyText>
<tableCaption confidence="0.983571">
Table 2: Values of frequency threshold.
</tableCaption>
<bodyText confidence="0.999711111111111">
to a Rate threshold ρ,,,eg and whose freq(φ, Neg)
is more than or equal to a frequency threshold are
regarded as non-definition patterns. The probability
rate is based on the growth rate (Dong and Li,
1999).
ρpos and ρ,,,eg are set to 2 and 0.5, while the fre-
quency threshold is set differently according to lan-
guages, pattern types (N-gram, subsequence, and
subtree), representation (surface, base, and POS),
and data (Pos and Neg), as in Table 2. The thresholds
in Table 2 were determined manually, but not really
arbitrarily. Basically they were determined accord-
ing to the frequency of each pattern in our data (e.g.
how frequently the surface N-gram of English ap-
pears in English positive training samples (Pos)).
Below, we detail how patterns are acquired. First,
we acquire N-gram patterns. Then, subsequence
patterns are acquired using the N-gram patterns as
input. Finally, subtree patterns are acquired using
the subsequence patterns as input.
N-gram patterns We collect N-gram patterns
from TrDat with N ranging from 2 to 6. We filter
out N-grams using thresholds on the Rate and fre-
quency, and regard those that are kept as definition
or non-definition N-grams.
Subsequence patterns We generate subsequence
patterns as ordered combinations of N-grams with
the wild card “*” inserted between them (we use
two or three N-grams for a subsequence). Then, we
check each of the generated subsequences and keep
it if there exists a sentence in TrDat that contains the
subsequence and whose root node is contained in the
subsequence. For example, subsequence “[term]
is a * in the” is kept if a term-marked sentence like
“[term] is a baseball player in the Dominican Re-
public.” exists in TrDat. Then, patterns are filtered
</bodyText>
<page confidence="0.997886">
65
</page>
<bodyText confidence="0.999905065217392">
out using thresholds on the Rate and frequency as
we did for N-grams.
Subtree patterns For each definition and non-
definition subsequence, we retrieve all the term-
marked sentences that contain the subsequence from
TrDat, and extract a minimal dependency subtree
that covers all the words of the subsequence from
each retrieved sentence. For example, assume that
we retrieve a term-marked sentence “[term] is
usually defined as the way of life of a group of peo-
ple.” for subsequence “[term] is * defined as the”.
Then we extract from the sentence the minimal de-
pendency subtree in the left side of (C) of Table 1.
Note that all the words of the subsequence are con-
tained in the subtree, and that in the subtree a node
(“way”) that is not a part of the subsequence is re-
placed with its dependency label (“NP”) assigned by
the dependency parser. The patterns are filtered out
using thresholds on the Rate and frequency.
We train a SVM classifier1 with a linear kernel,
using binary features that indicate the occurrence of
the patterns described above in a target sentence.
In theory, we could feed all the features to the
SVM classifier and let the classifier pick informa-
tive features. But we restricted the feature set for
practical reasons: the number of features would be-
come tremendously large. There are two reasons for
this. First, the number of sentences in our automati-
cally acquired training data is huge (2,439,257 posi-
tive sentences plus 5,000,000 negative sentences for
English, 703,208 positive sentences plus 1,400,000
negative sentences for Japanese and 310,072 posi-
tive sentences plus 600,000 negative sentences for
Chinese). Second, since each subsequence pattern
is generated as a combination of two or three N-
gram patterns and one subsequence pattern can gen-
erate one or more subtree patterns, using all possi-
ble features leads to a combinatorial explosion of
features. Moreover, since the feature vector will be
highly sparse with a huge number of infrequent fea-
tures, SVM learning becomes very time consuming.
In preliminary experiments we observed that when
using all possible features the learning process took
more than one week for each language. We there-
fore introduced the current feature selection method,
in which the learning process finished in one day but
</bodyText>
<footnote confidence="0.735262">
1http://svmlight.joachims.org.
</footnote>
<bodyText confidence="0.551341">
Original Web sentence: Albert Pujols is a baseball player.
Term-marked sentence 1: [term] is a baseball player.
Term-marked sentence 2: Albert Pujols is a [term].
</bodyText>
<figureCaption confidence="0.996178">
Figure 3: Term-marked sentences from a Web sentence.
</figureCaption>
<bodyText confidence="0.982169">
still obtains good results.
</bodyText>
<subsectionHeader confidence="0.524784">
2.1.3 Definition Extraction from the Web
</subsectionHeader>
<bodyText confidence="0.999993807692308">
We extract a large amount of definition sen-
tences by applying this classifier to sentences in our
Web archive. Because our classifier requires term-
marked sentences (sentences in which the term be-
ing defined is marked) as input, we first have to iden-
tify all such defined term candidates for each sen-
tence. For example, Figure 3 shows a case where a
Web sentence has two NPs (two candidates of de-
fined term). Basically we pick up NPs in a sen-
tence by simple heuristic rules. For English, NPs are
identified using TreeTagger (Schmid, 1995) and two
NPs are merged into one when they are connected by
“for” or “of”. After applying this procedure recur-
sively, the longest NPs are regarded as candidates of
defined terms and term-marked sentences are gener-
ated. For Japanese, we first identify nouns that are
optionally modified by adjectives as NPs, and allow
two NPs connected by “の” (of), if any, to form
a larger NP. For Chinese, nouns that are optionally
modified by adjectives are considered as NPs.
Then, each term-marked sentence is given a fea-
ture vector and classified by the classifier. The term-
marked sentence whose SVM score (the distance
from the hyperplane) is the largest among those from
the same original Web sentence is chosen as the final
classification result for the original Web sentence.
</bodyText>
<subsectionHeader confidence="0.999656">
2.2 Paraphrase Extraction
</subsectionHeader>
<bodyText confidence="0.999989916666667">
We use all the Web sentences classified as defini-
tion and all the sentences in Pos for paraphrase ex-
traction. First, we couple two definition sentences
whose defined term is the same. We filter out defini-
tion sentence pairs whose cosine similarity of con-
tent word vectors is less than or equal to threshold
C, which is set to 0.1. Then, we extract phrases
from each definition sentence, and generate all pos-
sible phrase pairs from the coupled sentences. In
this study, phrases are restricted to predicate phrases
that consist of at least one dependency relation and
in which all the constituents are consecutive in a
</bodyText>
<page confidence="0.646555">
66
</page>
<bodyText confidence="0.983410666666667">
f1 The ratio of the number of words shared between two can-
didate phrases to the number of all of the words in the two
phrases. Words are represented by either their surface form
(f1,1), base form (f1,2) or POS (f1,3).
f2 The identity of the leftmost word (surface form (f2,1), base
form (f2,2) or POS (f2,3)) between two candidate phrases.
f3 The same as f2 except that we use the rightmost word.
There are three corresponding subfunctions (f3,1 to f3,3).
f4 The ratio of the number of words that appear in a candidate
phrase segment of a definition sentence s1 and in a segment
that is NOT a part of the candidate phrase of another def-
inition sentence s2 to the number of all the words of s1’s
</bodyText>
<table confidence="0.548381142857143">
candidate phrase. Words are in their base form (f4,1).
f5 The reversed (s1 &lt;-+ s2) version of f4,1 (f5,1).
f6 The ratio of the number of words (the surface form) of a
shorter candidate phrase to that of a longer one (f6,1).
f7 Cosine similarity between two definition sentences from
which two candidate phrases are extracted. Only content
words in the base form are used (f7,1).
f8 The ratio of the number of parent dependency subtrees that
are shared by two candidate phrases to the number of all the
parent dependency subtrees. The parent dependency sub-
trees are adjacent to the candidate phrases and represented
by their surface form (f8,1), base form (f8,2), or POS (f8,3).
f9 The same as f8 except that we use child dependency sub-
trees. There are 3 subfunctions (f9,1 to f9,3) of f9 type.
f10 The ratio of the number of context N-grams that are shared
by two candidate phrases to the number of all the context N-
grams of both candidate phrases. The context N-grams are
adjacent to the candidate phrases and represented by either
the surface form, the base form, or POS. The N ranges from
1 to 3, and the context is either left-side or right-side. Thus,
there are 18 subfunctions (3 x 3 x 2).
</table>
<tableCaption confidence="0.999373">
Table 3: Local similarity subfunctions, f1,1 to f10,18.
</tableCaption>
<bodyText confidence="0.99975952631579">
sentence. Accordingly, if two definition sentences
that are coupled have three such predicate phrases
respectively, we get nine phrase pairs, for instance.
A phrase pair extracted from a definition pair is a
paraphrase candidate and is given a score that indi-
cates the likelihood of being a paraphrase, Score. It
consists of two similarity measures, local similarity
and global similarity, which are detailed below.
Local similarity Following Hashimoto et al., we
assume that two candidate phrases (p1, p2) tend to
be a paraphrase if they are similar enough and/or
their surrounding contexts are sufficiently similar.
Then, we calculate the local similarity (localSim) of
(p1, p2) as the weighted sum of 37 similarity sub-
functions that are grouped into 10 types (Table 3.)
For example, the f1 type consists of three subfunc-
tions, f1,1, f1,2, and f1,3. The 37 subfunctions are
inspired by Hashimoto et al.’s features. Then, local-
Sim is defined as:
</bodyText>
<equation confidence="0.8362988">
localSim(p1, p2) = Max ls(p1,p2, dl, dam,.).
(dl,dm)EDP(P1,P2)
d7n) _ E10 k; wi,�xfiJ(P1,p2,dl,dm)
Here, ls(p1, p2, dl, z=1 �&amp;quot;=1 ki
DP(p1, p2) is the set of all definition sentence pairs
</equation>
<bodyText confidence="0.999806048780488">
that contain (p1, p2). (dl, dm) is a definition sen-
tence pair containing (p1, p2). ki is the number
of subfunctions of fi type. wi,j is the weight for
fi,j. wi,j is uniformly set to 1 except for f4,1
and f5,1, whose weight is set to −1 since they
indicate the unlikelihood of (p1,p2)’s being a
paraphrase. As the formula indicates, if there is
more than one definition sentence pair that contains
(p1, p2), localSim is calculated from the definition
sentence pair that gives the maximum value of
ls(p1, p2, dl, dm). localSim is local in the sense that
it is calculated based on only one definition pair
from which (p1, p2) are extracted.
Global similarity The global similarity (global-
Sim) is our novel similarity function. We decompose
a candidate phrase pair (p1, p2) into Comm, the com-
mon part between p1 and p2, and Diff, the difference
between the two. For example, Comm and Diff of
(“keep the meaning intact”, “preserve the meaning”)
is (“the meaning”) and (“keep, intact”, “preserve”).
globalSim measures the semantic similarity of
the Diff of a phrase pair. It is proposed based on
the following intuition: phrase pair (p1, p2) tend
to be a paraphrase if their surface difference (i.e.
Diff) have the same meaning. For example, if
“keep, intact” and “preserve” mean the same, then
(“keep the meaning intact”, “preserve the meaning”)
is a paraphrase.
globalSim considers the occurrence of Diff in
global contexts (i.e., all the paraphrase candidates
from all the definition pairs). The globalSim of a
given phrase pair (p1, p2) is measured by basically
counting how many times the Diff of (p1, p2) ap-
pears in all the candidate phrase pairs from all the
definition pairs. The assumption is that Diff tends to
share the same meaning if it appears repeatedly in
paraphrase candidates from all definition sentence
pairs, i.e., our parallel corpus. Each occurrence of
Diff is weighted by the localSim of the phrase pair
in which Diff occurs. Precisely, globalSim is defined
as:
</bodyText>
<page confidence="0.999424">
67
</page>
<tableCaption confidence="0.997893">
Table 4: Language-dependent components.
</tableCaption>
<equation confidence="0.9387195">
localSim(pi, pj) �
M
(pi,pj)EPP(p1,p2)
PP(p1, p2) is the set of candidate phrase pairs
</equation>
<bodyText confidence="0.969518333333333">
whose Diff is the same as (p1,p2).2 M is the num-
ber of similarity subfunction types whose weight is
1, i.e. M = 8 (all the subfunction types except f4
and f5). It is used to normalize the value of each
occurrence of Diff to [0, 1].3 globalSim is global
in the sense that it considers all the definition pairs
that have a phrase pair with the same Diff as a target
candidate phrase pair (p1, p2).
The final score for a candidate phrase pair is:
</bodyText>
<equation confidence="0.931597">
Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2)
</equation>
<bodyText confidence="0.999851166666667">
The way of combining the two similarity functions
has been determined empirically after testing several
other ways of combining them. This ranks all the
candidate phrase pairs.
Finally, we summarize language-dependent com-
ponents that we fix manually in Table 4.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999761">
3.1 Experiments of Definition Extraction
</subsectionHeader>
<bodyText confidence="0.956422214285714">
We show that our unsupervised definition extrac-
tion method is competitive with state-of-the-art su-
pervised methods for English (Navigli and Velardi,
2010), and that it extracts a large number of defini-
tions reasonably accurately for English (3,216,121
definitions with 70% precision), Japanese (651,293
definitions with 62.5% precision), and Chinese
(682,661 definitions with 67% precision).
2If there are more than one (pi, pj) in a definition pair, we
use only one of them that has the largest localSim value.
3Although we claim that our idea of using globalSim is ef-
fective, we do not claim that the above formula for calculating
is the optimal way to implement the idea. Currently we are in-
vestigating a more mathematically well-motivated model.
</bodyText>
<subsectionHeader confidence="0.684232">
3.1.1 Preparing Corpora
</subsectionHeader>
<bodyText confidence="0.999902810810811">
First we describe Pos, Neg, and the Web corpus
from which definition sentences are extracted. As
the source of Pos, we used the English Wikipedia
of April 2011 (3,620,149 articles), the Japanese
Wikipedia of October 2011 (830,417 articles), and
the Chinese Wikipedia of August 2011 (365,545 ar-
ticles). We removed category articles, template ar-
ticles, list articles and so on from them. Then the
number of sentences of Pos was 2,439,257 for En-
glish, 703,208 for Japanese, and 310,072 for Chi-
nese. We verified our assumption that Wikipedia
first sentences can mostly be seen as definition by
manually checking 200 random samples from Pos.
96.5% of English Pos, 100% of Japanese Pos, and
99.5% of Chinese Pos were definitions.
As the source of Neg, we used 600 million
Japanese Web pages (Akamine et al., 2010) and
the ClueWeb09 corpus for English (about 504 mil-
lion pages) and Chinese (about 177 million pages).4
From each Web corpus, we collected the sentences
satisfying following conditions: 1) they contain 5
to 50 words and at least one verb, 2) less than half
of their words are numbers, and 3) they end with a
period. Then we randomly sampled sentences from
the collected sentences as Neg so that INegl was
about twice as large as lPosl: 5,000,000 for English,
1,400,000 for Japanese, and 600,000 for Chinese.
In Section 3.1.3, we use 10% of the Web corpus as
the input to the definition classifier. The number of
sentences are 294,844,141 for English, 245,537,860
for Japanese, and 68,653,130 for Chinese.
All the sentences were POS-tagged and parsed.
We used TreeTagger and MSTParser (McDonald
et al., 2006) for English, JUMAN (Kurohashi and
Kawahara, 2009a) and KNP (Kurohashi and Kawa-
hara, 2009b) for Japanese, MMA (Kruengkrai et al.,
2009) and CNP (Chen et al., 2009) for Chinese.
</bodyText>
<subsectionHeader confidence="0.963671">
3.1.2 Comparison with Previous Methods
</subsectionHeader>
<bodyText confidence="0.999982571428571">
We compared our method with the state-of-the-
art supervised methods proposed by Navigli and Ve-
lardi (2010), using their WCL datasets v1.0 (http:
//lcl.uniroma1.it/wcl/), definition and non-
definition datasets for English (Navigli et al., 2010).
Specifically, we used its training data (TrDat,,,cl,
hereafter), which consisted of 1,908 definition and
</bodyText>
<footnote confidence="0.768253">
4http://lemurproject.org/clueweb09.php/
</footnote>
<figure confidence="0.9306565">
The frequency threshold of Table 2 (Section 2.1.2).
Rules for identifying NPs in sentences (Section 2.1.3).
Threshold
NP rule
POS list
The list of content words’ POS (Section 2.2).
Tagger/parser
POS taggers, dependency parsers and NER tools.
�
globalSim(p1,p2) =
</figure>
<page confidence="0.991108">
68
</page>
<table confidence="0.998765">
Method Precision Recall F1 Accuracy
Proposeddef 86.79 86.97 86.88 89.18
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
</table>
<tableCaption confidence="0.999445">
Table 5: Definition classification results on TrDat,,,,l.
</tableCaption>
<bodyText confidence="0.99985056">
2,711 non-definition sentences, and compared the
following three methods. WCL-1 and WCL-3 are
methods proposed by Navigli and Velardi (2010).
They were trained and tested with 10 fold cross vali-
dation using TrDat,,,,l. Proposeddef is our method,
which used TrDat for acquiring patterns (Section
2.1.2) and training. We tested Proposeddef on each
of TrDat,,,,l’s 10 folds and averaged the results.
Note that, for Proposeddef, we removed sentences
in TrDat,,,,l from TrDat in advance for fairness.
Table 5 shows the results. The numbers for WCL-
1 and WCL-3 are taken from Navigli and Velardi
(2010). Proposeddef outperformed both methods in
terms of recall, F1, and accuracy. Thus, we conclude
that Proposeddef is comparable to WCL-1/WCL-3.
We conducted ablation tests of our method to in-
vestigate the effectiveness of each type of pattern.
When using only N-grams, F1 was 85.41. When
using N-grams and subsequences, F1 was 86.61.
When using N-grams and subtrees, F1 was 86.85.
When using all the features, F1 was 86.88. The re-
sults show that each type of patterns contribute to the
performance, but the contributions of subsequence
patterns and subtree patterns do not seem very sig-
nificant.
</bodyText>
<subsectionHeader confidence="0.910127">
3.1.3 Experiments of Definition Extraction
</subsectionHeader>
<bodyText confidence="0.999899122807018">
We extracted definitions from 10% of the Web
corpus. We applied Proposeddef to the cor-
pus of each language, and the state-of-the-art su-
pervised method for Japanese (Hashimoto et al.,
2011) (Hashidef, hereafter) to the Japanese corpus.
Hashidef was trained on their training data that con-
sisted of 2,911 sentences, 61.1% of which were def-
initions. Note that we removed sentences in TrDat
from 10% of the Web corpus in advance, while we
did not remove Hashimoto et al.’s training data from
the corpus. This means that, for Hashidef, the train-
ing data is included in the test data.
For each method, we filtered out its positive out-
puts whose defined term appeared more than 1,000
times in 10% of the Web corpus, since those terms
tend to be too vague to be a defined term or re-
fer to an entity outside the definition sentence. For
example, if “the college” appears more than 1,000
times in 10% of the corpus, we filter out sen-
tences like “The college is one of three colleges
in the Coast Community College District and was
founded in 1947.” For Proposeddef, the number of
remaining positive outputs is 3,216,121 for English,
651,293 for Japanese, and 682,661 for Chinese. For
Hashidef, the number of positive outputs is 523,882.
For Proposeddef of each language, we randomly
sampled 200 sentences from the remaining positive
outputs. For Hashidef, we first sorted its output by
the SVM score in descending order and then ran-
domly sampled 200 from the top 651,293, i.e., the
same number as the remaining positive outputs of
Proposeddef of Japanese, out of all the remaining
sentences of Hashidef.
For each language, after shuffling all the samples,
two human annotators evaluated each sample. The
annotators for English and Japanese were not the au-
thors, while one of the Chinese annotators was one
of the authors. We regarded a sample as a defini-
tion if it was regarded as a definition by both an-
notators. Cohen’s kappa (Cohen, 1960) was 0.55
for English (moderate agreement (Landis and Koch,
1977)), 0.73 for Japanese (substantial agreement),
and 0.69 for Chinese (substantial agreement).
For English, Proposeddef achieved 70% precision
for the 200 samples. For Japanese, Proposeddef
achieved 62.5% precision for the 200 samples, while
Hashidef achieved 70% precision for the 200 sam-
ples. For Chinese, Proposeddef achieved 67% pre-
cision for the 200 samples. From these results, we
conclude that Proposeddef can extract a large num-
ber of definition sentences from the Web moderately
well for the three languages.
Although the precision is not very high, our ex-
periments in the next section show that we can still
extract a large number of paraphrases with high pre-
cision from these definition sentences, due mainly to
our similarity measures, localSim and globalSim.
</bodyText>
<subsectionHeader confidence="0.999864">
3.2 Experiments of Paraphrase Extraction
</subsectionHeader>
<bodyText confidence="0.99489725">
We show (1) that our paraphrase extraction method
outperforms unsupervised methods for the three lan-
guages, (2) that globalSim is effective, and (3) that
our method is comparable to the state-of-the-art su-
</bodyText>
<page confidence="0.998119">
69
</page>
<bodyText confidence="0.537483095238095">
ProposedScore: Our method. Outputs are ranked by Score.
Proposedlocal: This is the same as ProposedScore except that it ranks
outputs by localSim. The performance drop from ProposedScore
shows globalSim’s effectiveness.
Hashisup: Hashimoto et al.’s supervised method. Training data is the
same as Hashimoto et al. Outputs are ranked by the SVM score
(the distance from the hyperplane). This is for Japanese only.
Hashiuns: The unsupervised version of Hashisup. Outputs are
ranked by the sum of feature values. Japanese only.
SMT: The phrase table construction method of Moses (Koehn et al.,
2007). We assume that Moses should extract a set of two phrases
that are paraphrases of each other, if we input monolingual par-
allel sentence pairs like our definition pairs. We used default
values for all the parameters. Outputs are ranked by the product
of two phrase translation probabilities of both directions.
P&amp;D: The distributional similarity based method by Pas¸ca and Di-
enes (2005) (their “N-gram-Only” method). Outputs are ranked
by the number of contexts two phrases share. Following Pas¸ca
and Dienes (2005), we used the parameters LC = 3 and
MaxP = 4, while MinP, which was 1 in Pas¸ca and Dienes
(2005), was set to 2 since our target was phrasal paraphrases.
</bodyText>
<tableCaption confidence="0.8206555">
Table 6: Evaluated paraphrase extraction methods.
pervised method for Japanese.
</tableCaption>
<subsectionHeader confidence="0.91808">
3.2.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.943773113207547">
We extracted paraphrases from definition sen-
tences in Pos and those extracted by Proposeddef in
Section 3.1.3. First we coupled two definition sen-
tences whose defined term was the same. The num-
ber of definition pairs was 3,208,086 for English,
742,306 for Japanese, and 457,233 for Chinese.
Then we evaluated six methods in Table 6.5 All
the methods except P&amp;D took the same definition
pairs as input, while P&amp;D’s input was 10% of the
Web corpus. The input can be seen as the same for
all the methods, since the definition pairs were de-
rived from that 10% of the Web corpus. In our ex-
periments Exp1 and Exp2 below, all evaluation sam-
ples were shuffled so that human annotators could
not know which sample was from which method.
Annotators were the same as those who conducted
the evaluation in Section 3.1.3. Cohen’s kappa (Co-
hen, 1960) was 0.83 for English, 0.88 for Japanese,
5We filtered out phrase pairs in which one phrase contained a
named entity but the other did not contain the named entity from
the output of ProposedScore, Proposedlocal, SMT, and P&amp;D,
since most of them were not paraphrases. We used Stanford
NER (Finkel et al., 2005) for English named entity recognition
(NER), KNP for Japanese NER, and BaseNER (Zhao and Kit,
2008) for Chinese NER. Hashisup and Hashiuns did the named
entity filtering of the same kind (footnote 3 of Hashimoto et al.
(2011)), and thus we did not apply the filter to them any further.
and 0.85 for Chinese, all of which indicated reason-
ably good (Landis and Koch, 1977). We regarded a
candidate phrase pair as a paraphrase if both annota-
tors regarded it as a paraphrase.
Exp1 We compared the methods that take def-
inition pairs as input, i.e. ProposedScore, Pro-
posedlocal, Hashisup, Hashiuns, and SMT. We ran-
domly sampled 200 phrase pairs from the top 10,000
for each method for evaluation. The evaluation of
each candidate phrase pair (p1, p2) was based on
bidirectional checking of entailment relation, p1 —*
p2 and p2 —* p1, with p1 and p2 embedded in con-
texts, as Hashimoto et al. (2011) did. Entailment
relation of both directions hold if (p1, p2) is a para-
phrase. We used definition pairs from which candi-
date phrase pairs were extracted as contexts.
Exp2 We compared ProposedScore and P&amp;D.
Since P&amp;D restricted its output to phrase pairs in
which each phrase consists of two to four words,
we restricted the output of ProposedScore to 2-to-4-
words phrase pairs, too. We randomly sampled 200
from the top 3,000 phrase pairs from each method
for evaluation, and the annotators checked entail-
ment relation of both directions between two phrases
using Web sentence pairs that contained the two
phrases as contexts.
</bodyText>
<sectionHeader confidence="0.760682" genericHeader="evaluation">
3.2.2 Results
</sectionHeader>
<bodyText confidence="0.999963">
From Exp1, we obtained precision curves in the
upper half of Figure 4. The curves were drawn from
the 200 samples that were sorted in descending order
by their score, and we plotted a dot for every 5 sam-
ples. ProposedScore outperformed Proposedlocal for
the three languages, and thus globalSim was effec-
tive. ProposedScore outperformed Hashisup. How-
ever, we observed that ProposedScore acquired many
candidate phrase pairs (p1, p2) for which p1 and p2
consisted of the same content words like “send a
postcard to the author” and “send the author a post-
card,” while the other methods tended to acquire
more content word variations like “have a chance”
and “have an opportunity.” Then we evaluated all
the methods in terms of how many paraphrases with
content word variations were extracted. We ex-
tracted from the evaluation samples only candidate
phrase pairs whose Diff contained a content word
(content word variation pairs), to see how many
</bodyText>
<page confidence="0.974437">
70
</page>
<figure confidence="0.994997757575758">
1 1 1 ’Proposed_score:
’Proposed_local&apos;
’SMT’
’Hashi_sup’
’Hashi_uns’
0.8 0.8 0.8
0.6
0.4
0.2
0
Precision 0.6 0.6
0.4 0.4
0.2 0.2
0 0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
(A) Top N (#Samples) (B) Top N (#Samples) (C) Top N (#Samples)
1 1 1 ’Proposed_score_cwv’
’Proposed_local_cwv’
’SMT_cwv’
’Hashi_ sup_
cwv’
’Hashi uns cwv’
0.8 0.8 0.8
0.6
0.4
0.2
0
Precision 0.6 0.6
0.4 0.4
0.2 0.2
0 0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
(a) Top N (#Samples) (b) Top N (#Samples) (c) Top N (#Samples)
</figure>
<figureCaption confidence="0.995982">
Figure 4: Precision curves of Exp1: English (A)(a), Chinese (B)(b), and Japanese (C)(c).
</figureCaption>
<figure confidence="0.999871103448276">
’Proposed_score’
’Proposed_score_cwJ
’Pasca’
’Pasca_cwJ
Precision
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
1
0 50 100 150 200
(A) Top N (#Samples)
0 50 100 150 200
(B) Top N (#Samples)
0 50 100 150 200
(C) Top N (#Samples)
</figure>
<figureCaption confidence="0.999964">
Figure 5: Precision curves of Exp2: English (A), Chinese (B), and Japanese (C).
</figureCaption>
<bodyText confidence="0.975910321428571">
of them were paraphrases. The lower half of Fig-
ure 4 shows the results (curves labeled with cwv).
The number of samples for ProposedScore reduced
drastically compared to the others for English and
Japanese, though precision was kept at a high level.
It is due mainly to the globalSim; the Diff of the
non-content word variation pairs appears frequently
in paraphrase candidates, and thus their globalSim
scores are high.
From Exp2, precision curves in Figure 5 were
obtained. P&amp;D acquired more content word varia-
tion pairs as the curves labeled by cwv indicates.
However, ProposedScore’s precision outperformed
P&amp;D’s by a large margin for the three languages.
From all of these results, we conclude (1) that our
paraphrase extraction method outperforms unsuper-
vised methods for the three languages, (2) that glob-
alSim is effective, and (3) that our method is com-
parable to the state-of-the-art supervised method for
Japanese, though our method tends to extract fewer
content word variation pairs than the others.
Table 7 shows examples of English paraphrases
extracted by ProposedScore.
is based in Halifax = is headquartered in Halifax
used for treating HIV = used to treat HIV
is a rare form = is an uncommon type
is a set = is an unordered collection
has an important role = plays a key role
</bodyText>
<tableCaption confidence="0.989741">
Table 7: Examples of extracted English paraphrases.
</tableCaption>
<sectionHeader confidence="0.998136" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99996075">
We proposed a minimally supervised method for
multilingual paraphrase extraction. Our experiments
showed that our paraphrase extraction method out-
performs unsupervised methods (Pas¸ca and Dienes,
2005; Koehn et al., 2007; Hashimoto et al., 2011)
for English, Japanese, and Chinese, and is compara-
ble to the state-of-the-art language dependent super-
vised method for Japanese (Hashimoto et al., 2011).
</bodyText>
<page confidence="0.998128">
71
</page>
<sectionHeader confidence="0.989969" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999357461538461">
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122–129.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16–23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50–57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161–170.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196–205.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’09,
pages 570–579, Singapore. Association for Computa-
tional Linguistics.
Jacob Cohen. 1960. Coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, pages 37–46.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350–
356, Geneva, Switzerland, Aug 23–Aug 27.
Guozhu Dong and Jinyan Li. 1999. Efficient mining of
emerging patterns: discovering trends and differences.
In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’99, pages 43–52, San Diego, California, United
States.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363–370.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn. 2012.
Enlarging paraphrase collections through generaliza-
tion and instantiation. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 631–
642.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107–114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172–1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun’ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1087–1097, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 698–707, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
</reference>
<page confidence="0.971003">
72
</page>
<reference confidence="0.999678048387097">
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513–521, Suntec, Singapore,
August. Association for Computational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2009a.
Japanese morphological analyzer system ju-
man version 6.0 (in japanese). Kyoto University,
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara. 2009b.
Japanese syntax and case analyzer knp version 3.0
(in japanese). Kyoto University, http://nlp.ist.i.kyoto-
u.ac.jp/EN/index.php?KNP.
J. Richard Landis and Gary G. Koch. 1977. Measure-
ment of observer agreement for categorical data. Bio-
metrics, 33(1):159–174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343–360.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492–1493.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ’06, pages 216–220, New
York City, New York.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318–1327, Uppsala, Sweden, July. Association for
Computational Linguistics.
Roberto Navigli, Paola Velardi, and Juana Maria Ruiz-
Martinez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Proceed-
ings of LREC 2010, pages 3716–3722.
Marius Pas¸ca and P´eter Dienes. 2005. Aligning needles
in a haystack: paraphrase acquisition across the web.
In Proceedings of the Second international joint con-
ference on Natural Language Processing, IJCNLP’05,
pages 119–130, Jeju Island, Korea.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, pages 47–50.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313–318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849–856.
Hai Zhao and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character tag-
ging for word segmentation and named entity recog-
nition. In Proceedings of the Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106–111, Hy-
derabad, India.
</reference>
<page confidence="0.999295">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117856">
<title confidence="0.967847">Minimally Supervised Method for Multilingual Paraphrase from Definition Sentences on the Web</title>
<author confidence="0.491621">Chikara</author>
<affiliation confidence="0.78791">De ‡ § ¶k ∗∗ Analysis Universal Communication Research National Institute of Information and Communications Technology</affiliation>
<abstract confidence="0.9181526">We propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the Web. Hashimoto et al. (2011) extracted paraphrases from Japanese definition sentences on the Web, assuming that definition sentences defining the same concept tend to contain paraphrases. However, their method requires manually annotated data and is language dependent. We extend their framework and develop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susumu Akamine</author>
<author>Daisuke Kawahara</author>
<author>Yoshikiyo Kato</author>
<author>Tetsuji Nakagawa</author>
<author>Yutaka I Leon-Suematsu</author>
</authors>
<title>Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara.</title>
<date>2010</date>
<booktitle>In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010),</booktitle>
<pages>122--129</pages>
<contexts>
<context position="24798" citStr="Akamine et al., 2010" startWordPosition="4061" endWordPosition="4064">Japanese Wikipedia of October 2011 (830,417 articles), and the Chinese Wikipedia of August 2011 (365,545 articles). We removed category articles, template articles, list articles and so on from them. Then the number of sentences of Pos was 2,439,257 for English, 703,208 for Japanese, and 310,072 for Chinese. We verified our assumption that Wikipedia first sentences can mostly be seen as definition by manually checking 200 random samples from Pos. 96.5% of English Pos, 100% of Japanese Pos, and 99.5% of Chinese Pos were definitions. As the source of Neg, we used 600 million Japanese Web pages (Akamine et al., 2010) and the ClueWeb09 corpus for English (about 504 million pages) and Chinese (about 177 million pages).4 From each Web corpus, we collected the sentences satisfying following conditions: 1) they contain 5 to 50 words and at least one verb, 2) less than half of their words are numbers, and 3) they end with a period. Then we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. Th</context>
</contexts>
<marker>Akamine, Kawahara, Kato, Nakagawa, Leon-Suematsu, 2010</marker>
<rawString>Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara. 2010. Organizing information on the web to support user judgments on information credibility. In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1474" citStr="Barzilay and Lee, 2003" startWordPosition="207" endWordPosition="210">rvised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to express the author’s ideas without changing the meani</context>
<context position="6312" citStr="Barzilay and Lee, 2003" startWordPosition="955" endWordPosition="958"> extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definition pairs from the Web o</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings of HLT-NAACL 2003, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL joint with the 10th Meeting of the European Chapter of the ACL (ACL/EACL</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1427" citStr="Barzilay and McKeown, 2001" startWordPosition="199" endWordPosition="202">extend their framework and develop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to expres</context>
<context position="6265" citStr="Barzilay and McKeown, 2001" startWordPosition="946" endWordPosition="950">phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automa</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the ACL joint with the 10th Meeting of the European Chapter of the ACL (ACL/EACL 2001), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP2007),</booktitle>
<pages>161--170</pages>
<contexts>
<context position="6149" citStr="Bhagat et al., 2007" startWordPosition="927" endWordPosition="930">annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel c</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP2007), pages 161–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>196--205</pages>
<contexts>
<context position="1516" citStr="Callison-Burch, 2008" startWordPosition="215" endWordPosition="216">es. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to express the author’s ideas without changing the meaning. b. Paraphrasing is defined as a proces</context>
<context position="6355" citStr="Callison-Burch, 2008" startWordPosition="963" endWordPosition="964">ndent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definition pairs from the Web on a large scale. The difference between our</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>570--579</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="25768" citStr="Chen et al., 2009" startWordPosition="4223" endWordPosition="4226">rom the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2.1.2). Rules for identifying NPs in sentences (Section 2.1.3). Threshold NP rule POS list The list of content word</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 570–579, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>In Educational and Psychological Measurement,</booktitle>
<pages>37--46</pages>
<contexts>
<context position="29829" citStr="Cohen, 1960" startWordPosition="4877" endWordPosition="4878">ive outputs. For Hashidef, we first sorted its output by the SVM score in descending order and then randomly sampled 200 from the top 651,293, i.e., the same number as the remaining positive outputs of Proposeddef of Japanese, out of all the remaining sentences of Hashidef. For each language, after shuffling all the samples, two human annotators evaluated each sample. The annotators for English and Japanese were not the authors, while one of the Chinese annotators was one of the authors. We regarded a sample as a definition if it was regarded as a definition by both annotators. Cohen’s kappa (Cohen, 1960) was 0.55 for English (moderate agreement (Landis and Koch, 1977)), 0.73 for Japanese (substantial agreement), and 0.69 for Chinese (substantial agreement). For English, Proposeddef achieved 70% precision for the 200 samples. For Japanese, Proposeddef achieved 62.5% precision for the 200 samples, while Hashidef achieved 70% precision for the 200 samples. For Chinese, Proposeddef achieved 67% precision for the 200 samples. From these results, we conclude that Proposeddef can extract a large number of definition sentences from the Web moderately well for the three languages. Although the precisi</context>
<context position="33120" citStr="Cohen, 1960" startWordPosition="5407" endWordPosition="5409">08,086 for English, 742,306 for Japanese, and 457,233 for Chinese. Then we evaluated six methods in Table 6.5 All the methods except P&amp;D took the same definition pairs as input, while P&amp;D’s input was 10% of the Web corpus. The input can be seen as the same for all the methods, since the definition pairs were derived from that 10% of the Web corpus. In our experiments Exp1 and Exp2 below, all evaluation samples were shuffled so that human annotators could not know which sample was from which method. Annotators were the same as those who conducted the evaluation in Section 3.1.3. Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT, and P&amp;D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. Coefficient of agreement for nominal scales. In Educational and Psychological Measurement, pages 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1494" citStr="Dolan et al., 2004" startWordPosition="211" endWordPosition="214"> to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to express the author’s ideas without changing the meaning. b. Paraphrasing </context>
<context position="6332" citStr="Dolan et al., 2004" startWordPosition="959" endWordPosition="962">stly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definition pairs from the Web on a large scale. The</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics (COLING 2004), pages 350– 356, Geneva, Switzerland, Aug 23–Aug 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guozhu Dong</author>
<author>Jinyan Li</author>
</authors>
<title>Efficient mining of emerging patterns: discovering trends and differences.</title>
<date>1999</date>
<booktitle>In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’99,</booktitle>
<pages>43--52</pages>
<location>San Diego, California, United States.</location>
<contexts>
<context position="11131" citStr="Dong and Li, 1999" startWordPosition="1772" endWordPosition="1775">(φ, Neg) = 0, Rate(φ) is set to the largest value of all the patterns’ Rate values. Only patterns whose Rate is more than or equal to a Rate threshold ρpos and whose freq(φ, Pos) is more than or equal to a frequency threshold are regarded as definition patterns. Similarly, we check if φ is likely to be a non-definition pattern. Only patterns whose Rate is less or equal Table 2: Values of frequency threshold. to a Rate threshold ρ,,,eg and whose freq(φ, Neg) is more than or equal to a frequency threshold are regarded as non-definition patterns. The probability rate is based on the growth rate (Dong and Li, 1999). ρpos and ρ,,,eg are set to 2 and 0.5, while the frequency threshold is set differently according to languages, pattern types (N-gram, subsequence, and subtree), representation (surface, base, and POS), and data (Pos and Neg), as in Table 2. The thresholds in Table 2 were determined manually, but not really arbitrarily. Basically they were determined according to the frequency of each pattern in our data (e.g. how frequently the surface N-gram of English appears in English positive training samples (Pos)). Below, we detail how patterns are acquired. First, we acquire N-gram patterns. Then, su</context>
</contexts>
<marker>Dong, Li, 1999</marker>
<rawString>Guozhu Dong and Jinyan Li. 1999. Efficient mining of emerging patterns: discovering trends and differences. In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’99, pages 43–52, San Diego, California, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363--370</pages>
<contexts>
<context position="33430" citStr="Finkel et al., 2005" startWordPosition="5459" endWordPosition="5462">rs were derived from that 10% of the Web corpus. In our experiments Exp1 and Exp2 below, all evaluation samples were shuffled so that human annotators could not know which sample was from which method. Annotators were the same as those who conducted the evaluation in Section 3.1.3. Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT, and P&amp;D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good (Landis and Koch, 1977). We regarded a candidate phrase pair as a paraphrase if both annotators regarded it as a paraphrase. Exp1 We compared the methods that take definition pairs as input, i.e. ProposedScore, Proposedlocal, Hashisup, Hashiuns, and SMT</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujita</author>
<author>Pierre Isabelle</author>
<author>Roland Kuhn</author>
</authors>
<title>Enlarging paraphrase collections through generalization and instantiation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012),</booktitle>
<pages>631--642</pages>
<contexts>
<context position="1562" citStr="Fujita et al., 2012" startWordPosition="221" endWordPosition="224">mparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to express the author’s ideas without changing the meaning. b. Paraphrasing is defined as a process of transforming an expression into another w</context>
</contexts>
<marker>Fujita, Isabelle, Kuhn, 2012</marker>
<rawString>Atsushi Fujita, Pierre Isabelle, and Roland Kuhn. 2012. Enlarging paraphrase collections through generalization and instantiation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 631– 642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>107--114</pages>
<contexts>
<context position="6128" citStr="Geffet and Dagan, 2005" startWordPosition="922" endWordPosition="926"> eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition </context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Stijn De Saeger</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Large-scale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1172--1181</pages>
<marker>Hashimoto, Torisawa, Kuroda, De Saeger, Murata, Kazama, 2009</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquisition from the web. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 1172–1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jun’ichi Kazama</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Extracting paraphrases from definition sentences on the web.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1087--1097</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Hashimoto, Torisawa, De Saeger, Kazama, Kurohashi, 2011</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jun’ichi Kazama, and Sadao Kurohashi. 2011. Extracting paraphrases from definition sentences on the web. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1087–1097, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>698--707</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4221" citStr="Kazama and Torisawa, 2007" startWordPosition="623" endWordPosition="626">anually annotated data. The first contribution of our work is to develop a minimally supervised method for multilingual definition extraction that uses a classifier distinguishing definition from non-definition. The classifier is learnt from the first sentences in Proceedings of NAACL-HLT 2013, pages 63–73, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Definition Extraction (Section 2.1) Paraphrase Extraction (Section 2.2) Figure 2: Overall picture of our method. Wikipedia articles, which can be regarded as the definition of the title of Wikipedia article (Kazama and Torisawa, 2007) and hence can be used as positive examples. Our method relies on a POS tagger, a dependency parser, a NER tool, noun phrase chunking rules, and frequency thresholds for each language, in addition to Wikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences te</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 698–707, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5884" citStr="Koehn et al., 2007" startWordPosition="886" endWordPosition="889"> to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymo</context>
<context position="31508" citStr="Koehn et al., 2007" startWordPosition="5133" endWordPosition="5136"> state-of-the-art su69 ProposedScore: Our method. Outputs are ranked by Score. Proposedlocal: This is the same as ProposedScore except that it ranks outputs by localSim. The performance drop from ProposedScore shows globalSim’s effectiveness. Hashisup: Hashimoto et al.’s supervised method. Training data is the same as Hashimoto et al. Outputs are ranked by the SVM score (the distance from the hyperplane). This is for Japanese only. Hashiuns: The unsupervised version of Hashisup. Outputs are ranked by the sum of feature values. Japanese only. SMT: The phrase table construction method of Moses (Koehn et al., 2007). We assume that Moses should extract a set of two phrases that are paraphrases of each other, if we input monolingual parallel sentence pairs like our definition pairs. We used default values for all the parameters. Outputs are ranked by the product of two phrase translation probabilities of both directions. P&amp;D: The distributional similarity based method by Pas¸ca and Dienes (2005) (their “N-gram-Only” method). Outputs are ranked by the number of contexts two phrases share. Following Pas¸ca and Dienes (2005), we used the parameters LC = 3 and MaxP = 4, while MinP, which was 1 in Pas¸ca and D</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="25740" citStr="Kruengkrai et al., 2009" startWordPosition="4217" endWordPosition="4220">en we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2.1.2). Rules for identifying NPs in sentences (Section 2.1.3). Threshold NP rule POS l</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Japanese morphological analyzer system juman version 6.0 (in japanese). Kyoto University,</title>
<date>2009</date>
<location>http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.</location>
<contexts>
<context position="25654" citStr="Kurohashi and Kawahara, 2009" startWordPosition="4203" endWordPosition="4206"> one verb, 2) less than half of their words are numbers, and 3) they end with a period. Then we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2</context>
</contexts>
<marker>Kurohashi, Kawahara, 2009</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara. 2009a. Japanese morphological analyzer system juman version 6.0 (in japanese). Kyoto University, http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Japanese syntax and case analyzer knp version 3.0 (in japanese). Kyoto University,</title>
<date>2009</date>
<location>http://nlp.ist.i.kyotou.ac.jp/EN/index.php?KNP.</location>
<contexts>
<context position="25654" citStr="Kurohashi and Kawahara, 2009" startWordPosition="4203" endWordPosition="4206"> one verb, 2) less than half of their words are numbers, and 3) they end with a period. Then we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2</context>
</contexts>
<marker>Kurohashi, Kawahara, 2009</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara. 2009b. Japanese syntax and case analyzer knp version 3.0 (in japanese). Kyoto University, http://nlp.ist.i.kyotou.ac.jp/EN/index.php?KNP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>Measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="29894" citStr="Landis and Koch, 1977" startWordPosition="4885" endWordPosition="4888">y the SVM score in descending order and then randomly sampled 200 from the top 651,293, i.e., the same number as the remaining positive outputs of Proposeddef of Japanese, out of all the remaining sentences of Hashidef. For each language, after shuffling all the samples, two human annotators evaluated each sample. The annotators for English and Japanese were not the authors, while one of the Chinese annotators was one of the authors. We regarded a sample as a definition if it was regarded as a definition by both annotators. Cohen’s kappa (Cohen, 1960) was 0.55 for English (moderate agreement (Landis and Koch, 1977)), 0.73 for Japanese (substantial agreement), and 0.69 for Chinese (substantial agreement). For English, Proposeddef achieved 70% precision for the 200 samples. For Japanese, Proposeddef achieved 62.5% precision for the 200 samples, while Hashidef achieved 70% precision for the 200 samples. For Chinese, Proposeddef achieved 67% precision for the 200 samples. From these results, we conclude that Proposeddef can extract a large number of definition sentences from the Web moderately well for the three languages. Although the precision is not very high, our experiments in the next section show tha</context>
<context position="33800" citStr="Landis and Koch, 1977" startWordPosition="5524" endWordPosition="5527"> phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT, and P&amp;D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good (Landis and Koch, 1977). We regarded a candidate phrase pair as a paraphrase if both annotators regarded it as a paraphrase. Exp1 We compared the methods that take definition pairs as input, i.e. ProposedScore, Proposedlocal, Hashisup, Hashiuns, and SMT. We randomly sampled 200 phrase pairs from the top 10,000 for each method for evaluation. The evaluation of each candidate phrase pair (p1, p2) was based on bidirectional checking of entailment relation, p1 —* p2 and p2 —* p1, with p1 and p2 embedded in contexts, as Hashimoto et al. (2011) did. Entailment relation of both directions hold if (p1, p2) is a paraphrase. </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. Measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1399" citStr="Lin and Pantel, 2001" startWordPosition="195" endWordPosition="198">anguage dependent. We extend their framework and develop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use</context>
<context position="6104" citStr="Lin and Pantel, 2001" startWordPosition="918" endWordPosition="921">ared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<pages>1492--1493</pages>
<contexts>
<context position="6511" citStr="Lin et al., 2003" startWordPosition="988" endWordPosition="991">mparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definition pairs from the Web on a large scale. The difference between ours and Hashimoto et al.’s is that our method requires no manual labeling of data and is mostly language-independent. 2 Proposed Method Our method first extra</context>
</contexts>
<marker>Lin, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03), pages 1492–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>216--220</pages>
<location>New York City, New York.</location>
<contexts>
<context position="25605" citStr="McDonald et al., 2006" startWordPosition="4196" endWordPosition="4199"> 1) they contain 5 to 50 words and at least one verb, 2) less than half of their words are numbers, and 3) they end with a period. Then we randomly sampled sentences from the collected sentences as Neg so that INegl was about twice as large as lPosl: 5,000,000 for English, 1,400,000 for Japanese, and 600,000 for Chinese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.p</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 216–220, New York City, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1318--1327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5048" citStr="Navigli and Velardi, 2010" startWordPosition="760" endWordPosition="763">ikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences tend to appear at the end of sentence in Japanese. We show that our method is applicable to English, Japanese, and Chinese, and that its performance is comparable to state-of-the-art supervised methods (Navigli and Velardi, 2010). Since the three languages are very different we believe that our definition extraction method is applicable to any language as long as Wikipedia articles of the language exist. The second contribution of our work is to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fr</context>
<context position="23371" citStr="Navigli and Velardi, 2010" startWordPosition="3827" endWordPosition="3830">e Diff as a target candidate phrase pair (p1, p2). The final score for a candidate phrase pair is: Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2) The way of combining the two similarity functions has been determined empirically after testing several other ways of combining them. This ranks all the candidate phrase pairs. Finally, we summarize language-dependent components that we fix manually in Table 4. 3 Experiments 3.1 Experiments of Definition Extraction We show that our unsupervised definition extraction method is competitive with state-of-the-art supervised methods for English (Navigli and Velardi, 2010), and that it extracts a large number of definitions reasonably accurately for English (3,216,121 definitions with 70% precision), Japanese (651,293 definitions with 62.5% precision), and Chinese (682,661 definitions with 67% precision). 2If there are more than one (pi, pj) in a definition pair, we use only one of them that has the largest localSim value. 3Although we claim that our idea of using globalSim is effective, we do not claim that the above formula for calculating is the optimal way to implement the idea. Currently we are investigating a more mathematically well-motivated model. 3.1.</context>
<context position="25926" citStr="Navigli and Velardi (2010)" startWordPosition="4246" endWordPosition="4250">nese. In Section 3.1.3, we use 10% of the Web corpus as the input to the definition classifier. The number of sentences are 294,844,141 for English, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2.1.2). Rules for identifying NPs in sentences (Section 2.1.3). Threshold NP rule POS list The list of content words’ POS (Section 2.2). Tagger/parser POS taggers, dependency parsers and NER tools. � globalSim(p1,p2) = 68 Method Precision Recall F1 Accuracy Proposeddef 86.</context>
<context position="27260" citStr="Navigli and Velardi (2010)" startWordPosition="4442" endWordPosition="4445">ication results on TrDat,,,,l. 2,711 non-definition sentences, and compared the following three methods. WCL-1 and WCL-3 are methods proposed by Navigli and Velardi (2010). They were trained and tested with 10 fold cross validation using TrDat,,,,l. Proposeddef is our method, which used TrDat for acquiring patterns (Section 2.1.2) and training. We tested Proposeddef on each of TrDat,,,,l’s 10 folds and averaged the results. Note that, for Proposeddef, we removed sentences in TrDat,,,,l from TrDat in advance for fairness. Table 5 shows the results. The numbers for WCL1 and WCL-3 are taken from Navigli and Velardi (2010). Proposeddef outperformed both methods in terms of recall, F1, and accuracy. Thus, we conclude that Proposeddef is comparable to WCL-1/WCL-3. We conducted ablation tests of our method to investigate the effectiveness of each type of pattern. When using only N-grams, F1 was 85.41. When using N-grams and subsequences, F1 was 86.61. When using N-grams and subtrees, F1 was 86.85. When using all the features, F1 was 86.88. The results show that each type of patterns contribute to the performance, but the contributions of subsequence patterns and subtree patterns do not seem very significant. 3.1.3</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Juana Maria RuizMartinez</author>
</authors>
<title>An annotated dataset for extracting definitions and hypernyms from the web.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>3716--3722</pages>
<contexts>
<context position="26062" citStr="Navigli et al., 2010" startWordPosition="4265" endWordPosition="4268">nglish, 245,537,860 for Japanese, and 68,653,130 for Chinese. All the sentences were POS-tagged and parsed. We used TreeTagger and MSTParser (McDonald et al., 2006) for English, JUMAN (Kurohashi and Kawahara, 2009a) and KNP (Kurohashi and Kawahara, 2009b) for Japanese, MMA (Kruengkrai et al., 2009) and CNP (Chen et al., 2009) for Chinese. 3.1.2 Comparison with Previous Methods We compared our method with the state-of-theart supervised methods proposed by Navigli and Velardi (2010), using their WCL datasets v1.0 (http: //lcl.uniroma1.it/wcl/), definition and nondefinition datasets for English (Navigli et al., 2010). Specifically, we used its training data (TrDat,,,cl, hereafter), which consisted of 1,908 definition and 4http://lemurproject.org/clueweb09.php/ The frequency threshold of Table 2 (Section 2.1.2). Rules for identifying NPs in sentences (Section 2.1.3). Threshold NP rule POS list The list of content words’ POS (Section 2.2). Tagger/parser POS taggers, dependency parsers and NER tools. � globalSim(p1,p2) = 68 Method Precision Recall F1 Accuracy Proposeddef 86.79 86.97 86.88 89.18 WCL-1 99.88 42.09 59.22 76.06 WCL-3 98.81 60.74 75.23 83.48 Table 5: Definition classification results on TrDat,,,,</context>
</contexts>
<marker>Navigli, Velardi, RuizMartinez, 2010</marker>
<rawString>Roberto Navigli, Paola Velardi, and Juana Maria RuizMartinez. 2010. An annotated dataset for extracting definitions and hypernyms from the web. In Proceedings of LREC 2010, pages 3716–3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>P´eter Dienes</author>
</authors>
<title>Aligning needles in a haystack: paraphrase acquisition across the web. In</title>
<date>2005</date>
<booktitle>Proceedings of the Second international joint conference on Natural Language Processing, IJCNLP’05,</booktitle>
<pages>119--130</pages>
<location>Jeju Island,</location>
<marker>Pas¸ca, Dienes, 2005</marker>
<rawString>Marius Pas¸ca and P´eter Dienes. 2005. Aligning needles in a haystack: paraphrase acquisition across the web. In Proceedings of the Second international joint conference on Natural Language Processing, IJCNLP’05, pages 119–130, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to german.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL SIGDAT-Workshop,</booktitle>
<pages>47--50</pages>
<contexts>
<context position="15794" citStr="Schmid, 1995" startWordPosition="2540" endWordPosition="2541">tence. still obtains good results. 2.1.3 Definition Extraction from the Web We extract a large amount of definition sentences by applying this classifier to sentences in our Web archive. Because our classifier requires termmarked sentences (sentences in which the term being defined is marked) as input, we first have to identify all such defined term candidates for each sentence. For example, Figure 3 shows a case where a Web sentence has two NPs (two candidates of defined term). Basically we pick up NPs in a sentence by simple heuristic rules. For English, NPs are identified using TreeTagger (Schmid, 1995) and two NPs are merged into one when they are connected by “for” or “of”. After applying this procedure recursively, the longest NPs are regarded as candidates of defined terms and term-marked sentences are generated. For Japanese, we first identify nouns that are optionally modified by adjectives as NPs, and allow two NPs connected by “の” (of), if any, to form a larger NP. For Chinese, nouns that are optionally modified by adjectives are considered as NPs. Then, each term-marked sentence is given a feature vector and classified by the classifier. The termmarked sentence whose SVM score (the </context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to german. In Proceedings of the ACL SIGDAT-Workshop, pages 47–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd international Conference on Human Language Technology Research (HLT2002),</booktitle>
<pages>313--318</pages>
<contexts>
<context position="1450" citStr="Shinyama et al., 2002" startWordPosition="203" endWordPosition="206">evelop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 1 Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal 63 (1) a. Paraphrasing is the use of your own words to express the author’s ideas wi</context>
<context position="6288" citStr="Shinyama et al., 2002" startWordPosition="951" endWordPosition="954">ontexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automatically collect definit</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of the 2nd international Conference on Human Language Technology Research (HLT2002), pages 313–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary template.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008),</booktitle>
<pages>849--856</pages>
<contexts>
<context position="6175" citStr="Szpektor and Dagan, 2008" startWordPosition="931" endWordPosition="935">d introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary template. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008), pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="33529" citStr="Zhao and Kit, 2008" startWordPosition="5475" endWordPosition="5478">ion samples were shuffled so that human annotators could not know which sample was from which method. Annotators were the same as those who conducted the evaluation in Section 3.1.3. Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT, and P&amp;D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good (Landis and Koch, 1977). We regarded a candidate phrase pair as a paraphrase if both annotators regarded it as a paraphrase. Exp1 We compared the methods that take definition pairs as input, i.e. ProposedScore, Proposedlocal, Hashisup, Hashiuns, and SMT. We randomly sampled 200 phrase pairs from the top 10,000 for each method for evaluation. The eval</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111, Hyderabad, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>