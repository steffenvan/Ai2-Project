<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013486">
<title confidence="0.992646">
Acquisition of Verb Entailment from Text
</title>
<author confidence="0.983181">
Viktor Pekar
</author>
<affiliation confidence="0.986678">
Computational Linguistics Group
University of Wolverhampton
</affiliation>
<address confidence="0.9781675">
MB109 Stafford Street
Wolverhampton WV1 1SB, UK
</address>
<email confidence="0.998458">
v.pekar@wlv.ac.uk
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999682111111111">
The study addresses the problem of auto-
matic acquisition of entailment relations
between verbs. While this task has much
in common with paraphrases acquisition
which aims to discover semantic equiva-
lence between verbs, the main challenge
of entailment acquisition is to capture
asymmetric, or directional, relations. Mo-
tivated by the intuition that it often under-
lies the local structure of coherent text, we
develop a method that discovers verb en-
tailment using evidence about discourse
relations between clauses available in a
parsed corpus. In comparison with earlier
work, the proposed method covers a much
wider range of verb entailment types and
learns the mapping between verbs with
highly varied argument structures.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973155555556">
The entailment relations between verbs are a natural
language counterpart of the commonsense knowl-
edge that certain events and states give rise to other
events and states. For example, there is an entail-
ment relation between the verbs buy and belong,
which reflects the commonsense notion that if some-
one has bought an object, this object belongs to that
person.
A lexical resource encoding entailment can serve
as a useful tool in many tasks where automatic in-
ferencing over natural language text is required. In
Question Answering, it has been used to establish
that a certain sentence found in the corpus can serve
as a suitable, albeit implicit answer to a query (Cur-
tis et al., 2005), (Girju, 2003), (Moldovan and Rus,
2001). In Information Extraction, it can similarly
help to recognize relations between named entities
in cases when the entities in the text are linked by
a linguistic construction that entails a known extrac-
tion pattern, but not by the pattern itself. A lexical
entailment resource can contribute to information re-
trieval tasks via integration into a textual entailment
system that aims to recognize entailment between
two larger text fragments (Dagan et al., 2005).
Since entailment is known to systematically inter-
act with the discourse organization of text (Hobbs,
1985), an entailment resource can be of interest to
tasks that deal with structuring a set of individual
facts into coherent text. In Natural Language Gener-
ation (Reiter and Dale, 2000) and Multi-Document
Summarization (Barzilay et al., 2002) it can be used
to order sentences coming from multiple, possibly
unrelated sources to produce a coherent document.
The knowledge is essential for compiling answers
for procedural questions in a QA system, when sen-
tences containing relevant information are spread
across the corpus (Curtis et al., 2005).
The present paper is concerned with the prob-
lem of automatic acquisition of verb entailment from
text. In the next section we set the background
for the study by describing previous work. We
then define the goal of the study and describe our
method for verb entailment acquisition. After that
we present results of its experimental evaluation. Fi-
nally, we draw conclusions and outline future work.
</bodyText>
<sectionHeader confidence="0.997696" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9994522">
The task of verb entailment acquisition appears to
have much in common with that of paraphrase ac-
quisition (Lin and Pantel, 2001), (Pang et al., 2003),
(Szpektor et al., 2004). In both tasks the goal is
to discover pairs of related verbs and identify map-
</bodyText>
<page confidence="0.994506">
49
</page>
<note confidence="0.9954135">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49–56,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999844886363636">
pings between their argument structures. The impor-
tant distinction is that while in a paraphrase the two
verbs are semantically equivalent, entailment is a di-
rectional, or asymmetric, relation: one verb entails
the other, but the converse does not hold. For ex-
ample, the verbs buy and purchase paraphrase each
other: either of them can substitute its counterpart in
most contexts without altering their meaning. The
verb buy entails own so that buy can be replaced with
own without introducing any contradicting content
into the original sentence. Replacing own with buy,
however, does convey new meaning.
To account for the asymmetric character of entail-
ment, a popular approach has been to use lexico-
syntactic patterns indicative of entailment. In
(Chklovski and Pantel, 2004) different types of se-
mantic relations between verbs are discovered us-
ing surface patterns (like “X-ed by Y-ing” for en-
ablement1, which would match “obtained by bor-
rowing”, for example) and assessing the strength
of asymmetric relations as mutual information be-
tween the two verbs. (Torisawa, 2003) collected
pairs of coordinated verbs, i.e. matching patterns
like “X-ed and Y-ed”, and then estimated the prob-
ability of entailment using corpus counts. (Inui
et al., 2003) used a similar approach exploiting
causative expressions such as because, though, and
so. (Girju, 2003) extracted causal relations between
nouns like “Earthquakes generate tsunami” by first
using lexico-syntactic patterns to collect relevant
data and then using a decision tree classifier to learn
the relations. Although these techniques have been
shown to achieve high precision, their reliance on
surface patterns limits their coverage in that they ad-
dress only those relations that are regularly made
explicit through concrete natural language expres-
sions, and only within sentences.
The method for noun entailment acquisition by
(Geffet and Dagan, 2005) is based on the idea of dis-
tributional inclusion, according to which one noun
is entailed by the other if the set of occurrence con-
texts of the former subsumes that of the latter. How-
ever, this approach is likely to pick only a particular
kind of verb entailment, that of troponymy (such as
</bodyText>
<footnote confidence="0.8548995">
1In (Chklovski and Pantel, 2004) enablement is defined to
be a relation where one event often, but not necessarily always,
gives rise to the other event, which coincides with our definition
of entailment (see Section 3).
</footnote>
<bodyText confidence="0.999676464285715">
march-walk) and overlook pairs where there is little
overlap in the occurrence patterns between the two
verbs.
In tasks involving recognition of relations be-
tween entities such as Question Answering and In-
formation Extraction, it is crucial to encode the
mapping between the argument structures of two
verbs. Pattern-matching often imposes restrictions
on the syntactic configurations in which the verbs
can appear in the corpus: the patterns employed by
(Chklovski and Pantel, 2004) and (Torisawa, 2003)
derive pairs of only those verbs that have identical
argument structures, and often only those that in-
volve a subject and a direct object. The method
for discovery of inference rules by (Lin and Pantel,
2001) obtains pairs of verbs with highly varied argu-
ment structures, which also do not have to be iden-
tical for the two verbs. While the inference rules
the method acquires seem to encompass pairs re-
lated by entailment, these pairs are not distinguished
from paraphrases and the direction of relation in
such pairs is not recognized.
To sum up, a major challenge in entailment ac-
quisition is the need for more generic methods that
would cover an unrestricted range of entailment
types and learn the mapping between verbs with
varied argument structures, eventually yielding re-
sources suitable for robust large-scale applications.
</bodyText>
<sectionHeader confidence="0.993784" genericHeader="method">
3 Verb Entailment
</sectionHeader>
<bodyText confidence="0.997933352941177">
Verb entailment relations have been traditionally at-
tracting a lot of interest from lexical semantics re-
search and their various typologies have been pro-
posed (see, e.g., (Fellbaum, 1998)). In this study,
with the view of potential practical applications, we
adopt an operational definition of entailment. We
define it to be a semantic relation between verbs
where one verb, termed premise P, refers to event
Ep and at the same time implies event EQ, typically
denoted by the other verb, termed consequence Q.
The goal of verb entailment acquisition is then
to find two linguistic templates each consisting of
a verb and slots for its syntactic arguments. In the
pair, (1) the verbs are related in accordance with
our definition of entailment above, (2) there is a
mapping between the slots of the two templates and
(3) the direction of entailment is indicated explic-
</bodyText>
<page confidence="0.975033">
50
</page>
<bodyText confidence="0.999833307692308">
itly. For example, in the template pair “buy(obj:X)
==&gt;. belong(subj:X)” the operator ==&gt;. specifies that the
premise buy entails the consequence belong, and X
indicates a mapping between the object of buy and
the subject of belong, as in The company bought
shares. - The shares belong to the company.
As opposed to logical entailment, we do not re-
quire that verb entailment holds in all conceivable
contexts and view it as a relation that may be more
plausible in some contexts than others. For each
verb pair, we therefore wish to assign a score quan-
tifying the likelihood of its satisfying entailment in
some random context.
</bodyText>
<sectionHeader confidence="0.997298" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999993782608696">
The key assumption behind our approach is that the
ability of a verb to imply an event typically denoted
by a different verb manifests itself in the regular co-
occurrence of the two verbs inside locally coherent
text. This assumption is not arbitrary: as discourse
investigations show (Asher and Lascarides, 2003),
(Hobbs, 1985), lexical entailment plays an impor-
tant role in determining the local structure of dis-
course. We expect this co-occurrence regularity to
be equally characteristic of any pair of verbs related
by entailment, regardless of is type and the syntactic
behavior of verbs.
The method consists of three major steps. First,
it identifies pairs of clauses that are related in the
local discourse. From related clauses, it then cre-
ates templates by extracting pairs of verbs along
with relevant information as to their syntactic be-
havior. Third, the method scores each verb pair
in terms of plausibility of entailment by measuring
how strongly the premise signals the appearance of
the consequence inside the text segment at hand. In
the following sections, we describe these steps in
more detail.
</bodyText>
<subsectionHeader confidence="0.999792">
4.1 Identifying discourse-related clauses
</subsectionHeader>
<bodyText confidence="0.999950051282051">
We attempt to capture local discourse relatedness
between clauses by a combination of several surface
cues. In doing so, we do not build a full discourse
representation of text, nor do we try to identify the
type of particular rhetorical relations between sen-
tences, but rather identify pairs of clauses that are
likely to be discourse-related.
Textual proximity. We start by parsing the cor-
pus with a dependency parser (we use Connexor’s
FDG (Tapanainen and J¨arvinen, 1997)), treating
every verb with its dependent constituents as a
clause. For two clauses to be discourse-related, we
require that they appear close to each other in the
text. Adjacency of sentences has been previously
used to model local coherence (Lapata, 2003). To
capture related clauses within larger text fragments,
we experiment with windows of text of various sizes
around a clause.
Paragraph boundaries. Since locally related
sentences tend to be grouped into paragraphs, we
further require that the two clauses appear within the
same paragraph.
Common event participant. Entity-based theo-
ries of discourse (e.g., (Grosz et al., 1995)) claim
that a coherent text segment tends to focus on a
specific entity. This intuition has been formalized
by (Barzilay and Lapata, 2005), who developed an
entity-based statistical representation of local dis-
course and showed its usefulness for estimating co-
herence between sentences. We also impose this as
a criterion for two clauses to be discourse-related:
their arguments need to refer to the same participant,
henceforth, anchor. We identify the anchor as the
same noun lemma appearing as an argument to the
verbs in both clauses, considering only subject, ob-
ject, and prepositional object arguments. The anchor
must not be a pronoun, since identical pronouns may
refer to different entities and making use of such cor-
respondences is likely to introduce noise.
</bodyText>
<subsectionHeader confidence="0.999151">
4.2 Creating templates
</subsectionHeader>
<bodyText confidence="0.9985">
Once relevant clauses have been identified, we cre-
ate pairs of syntactic templates, each consisting of a
verb and the label specifying the syntactic role the
anchor occupies near the verb. For example, given
a pair of clauses Mary bought a house. and The
house belongs to Mary., the method will extract two
pairs of templates: {buy(obj:X), belong(subj:X)}
and {buy(subj:X), belong(to:X).}
Before templates are constructed, we automati-
cally convert complex sentence parses to simpler,
but semantically equivalent ones so as to increase
the amount of usable data and reduce noise:
</bodyText>
<listItem confidence="0.997929">
• Passive constructions are turned into active
</listItem>
<page confidence="0.929937">
51
</page>
<bodyText confidence="0.552565">
ones: Xwas bought by Y – YboughtX;
</bodyText>
<listItem confidence="0.959451125">
• Phrases with coordinated nouns and verbs are
decomposed: X bought A and B – X bought A,
X bought B; X bought and sold A – X bought A,
X sold A.
• Phrases with past and present participles are
turned into predicate structures: the group led
by A – A leads the group; the group leading the
market – the group leads the market.
</listItem>
<bodyText confidence="0.999847666666667">
The output of this step is V E P x Q, a set of pairs
of templates {p, q}, where p E P is the premise,
consisting of the verb vp and rp – the syntactic re-
lation between vp and the anchor, and q E Q is the
consequence, consisting of the verb vQ and rQ – its
syntactic relation to the anchor.
</bodyText>
<subsectionHeader confidence="0.999309">
4.3 Measuring asymmetric association
</subsectionHeader>
<bodyText confidence="0.999378857142857">
To score the pairs for asymmetric association, we
use a procedure similar to the method by (Resnik,
1993) for learning selectional preferences of verbs.
Each template in a pair is tried as both a premise
and a consequence. We quantify the ’preference’
of the premise p for the consequence q as the con-
tribution of q to the amount of information p con-
tains about its consequences seen in the data. First,
we calculate Kullback-Leibler Divergence (Cover.
and Thomas, 1991) between two probability distrib-
utions, u – the prior distribution of all consequences
in the data and w – their posterior distribution given
p, thus measuring the information p contains about
its consequences:
</bodyText>
<equation confidence="0.997991333333333">
u(x)
u(x) log (1)
w(x)
</equation>
<bodyText confidence="0.9052888">
where u(x) = P(qxjp), w(x) = P(qx), and x ranges
over all consequences in the data. Then, the score for
template {p, q} expressing the association of q with
p is calculated as the proportion of q’s contribution
to Dp(ujjw):
</bodyText>
<equation confidence="0.997996">
P (qjp)
Score(p,q) = P(qjp)log P (p) Dp(ujjw)−&apos; (2)
</equation>
<bodyText confidence="0.975927">
In each pair we compare the scores in both di-
rections, taking the direction with the greater score
to indicate the most likely premise and consequence
and thus the direction of entailment.
</bodyText>
<sectionHeader confidence="0.991793" genericHeader="method">
5 Evaluation Design
</sectionHeader>
<subsectionHeader confidence="0.894533">
5.1 Task
</subsectionHeader>
<bodyText confidence="0.999886">
To evaluate the algorithm, we designed a recognition
task similar to that of pseudo-word disambiguation
(Sch¨utze, 1992), (Dagan et al., 1999). The task was,
given a certain premise, to select its correct conse-
quence out of a pool with several artificially created
incorrect alternatives.
The advantages of this evaluation technique are
twofold. On the one hand, the task mimics many
possible practical applications of the entailment re-
source, such as sentence ordering, where, given a
sentence, it is necessary to identify among several
alternatives another sentence that either entails or is
entailed by the given sentence. On the other hand,
in comparison with manual evaluation of the direct
output of the system, it requires minimal human in-
volvement and makes it possible to conduct large-
scale experiments.
</bodyText>
<subsectionHeader confidence="0.988206">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.999838875">
The experimental material was created from the
BLLIP corpus, a collection of texts from the Wall
Street Journal (years 1987-89). We chose 15 tran-
sitive verbs with the greatest corpus frequency and
used a pilot run of our method to extract 1000
highest-scoring template pairs involving these verbs
as a premise. From them, we manually selected 129
template pairs that satisfied entailment.
For each of the 129 template pairs, four false con-
sequences were created. This was done by randomly
picking verbs with frequency comparable to that of
the verb of the correct consequence. A list of parsed
clauses from the BLLIP corpus was consulted to se-
lect the most typical syntactic configuration of each
of the four false verbs. The resulting five template
pairs, presented in a random order, constituted a test
item. Figure 1 illustrates such a test item.
The entailment acquisition method was evaluated
on entailment templates acquired from the British
National Corpus. Even though the two corpora are
quite different in style, we assume that the evalua-
tion allows conclusions to be drawn as to the relative
quality of performance of the methods under consid-
eration.
</bodyText>
<equation confidence="0.959856666666667">
�
Dp(ujjw) =
n
</equation>
<page confidence="0.976547">
52
</page>
<figure confidence="0.9905978">
1* buy(subj:X,obj:Y)=:&gt;.own(subj:X,obj:Y)
2 buy(subj:X,obj:Y)=:&gt;.approve(subj:X,obj:Y)
3 buy(subj:X,obj:Y)=:&gt;.reach(subj:X,obj:Y)
4 buy(subj:X,obj:Y)=:&gt;.decline(subj:X,obj:Y)
5 buy(subj:X,obj:Y)=:&gt;.compare(obj:X,with:Y)
</figure>
<figureCaption confidence="0.873626666666667">
Figure 1: An item from the test dataset. The tem-
plate pair with the correct consequence is marked
by an asterisk.
</figureCaption>
<subsectionHeader confidence="0.998911">
5.3 Recognition algorithm
</subsectionHeader>
<bodyText confidence="0.999964066666667">
During evaluation, we tested the ability of the
method to select the correct consequence among the
five alternatives. Our entailment acquisition method
generates association scores for one-slot templates.
In order to score the double-slot templates in the
evaluation material, we used the following proce-
dure.
Given a double-slot template, we divide it into
two single-slot ones such that matching arguments
of the two verbs along with the verbs themselves
constitute a separate template. For example, “buy
(subj:X, obj:Y) ==&gt;. own (subj:X, obj:Y)” will be de-
composed into “buy (subj:X) ==�- own (subj:X)” and
“buy (obj:Y) ==�- own (obj:Y)”. The scores of these
two templates are then looked up in the generated
database and averaged. In each test item, the five
alternatives are scored in this manner and the one
with the highest score was chosen as containing the
correct consequence.
The performance was measured in terms of accu-
racy, i.e. as the ratio of correct choices to the total
number of test items. Ties, i.e. cases when the cor-
rect consequence was assigned the same score as one
or more incorrect ones, contributed to the final accu-
racy measure proportionate to the number of tying
alternatives.
This experimental design corresponds to a ran-
dom baseline of 0.2, i.e. the expected accuracy when
selecting a consequence template randomly out of 5
alternatives.
</bodyText>
<sectionHeader confidence="0.999705" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9999625">
We now present the results of the evaluation of the
method. In Section 6.1, we study its parameters and
determine the best configuration. In Section 6.2, we
compare its performance against that of human sub-
jects as well as that of two state-of-the-art lexical re-
sources: the verb entailment knowledge contained in
WordNet2.0 and the inference rules from the DIRT
database (Lin and Pantel, 2001).
</bodyText>
<subsectionHeader confidence="0.999445">
6.1 Model parameters
</subsectionHeader>
<bodyText confidence="0.99997325">
We first examined the following parameters of the
model: the window size, the use of paragraph
boundaries, and the effect of the shared anchor on
the quality of the model.
</bodyText>
<subsectionHeader confidence="0.950684">
6.1.1 Window size and paragraph boundaries
</subsectionHeader>
<bodyText confidence="0.995471466666667">
As was mentioned in Section 4.1, a free parame-
ter in our model is a threshold on the distance be-
tween two clauses, that we take as an indicator that
the clauses are discourse-related. To find an opti-
mal threshold, we experimented with windows of
1, 2 ... 25 clauses around a given clause, taking
clauses appearing within the window as potentially
related to the given one. We also looked at the ef-
fect paragraph boundaries have on the identification
of related clauses. Figure 2 shows two curves de-
picting the accuracy of the method as a function of
the window size: the first one describes performance
when paragraph boundaries are taken into account
(PAR) and the second one when they are ignored
(NO PAR).
</bodyText>
<figureCaption confidence="0.712956666666667">
Figure 2: Accuracy of the algorithm as a function
of window size, with and without paragraph bound-
aries used for delineating coherent text.
</figureCaption>
<bodyText confidence="0.97010225">
One can see that both curves rise fairly steeply up
to window size of around 7, indicating that many en-
tailment pairs are discovered when the two clauses
appear close to each other. The rise is the steepest
</bodyText>
<page confidence="0.99706">
53
</page>
<bodyText confidence="0.999958608695652">
between windows of 1 and 3, suggesting that entail-
ment relations are most often explicated in clauses
appearing very close to each other.
PAR reaches its maximum at the window of 15,
where it levels off. Considering that 88% of para-
graphs in BNC contain 15 clauses or less, we take
this as an indication that a segment of text where
both a premise and its consequence are likely to be
found indeed roughly corresponds to a paragraph.
NO PAR’s maximum is at 10, then the accuracy
starts to decrease, suggesting that evidence found
deeper inside other paragraphs is misleading to our
model.
NO PAR performs consistently better than PAR
until it reaches its peak, i.e. when the window size is
less than 10. This seems to suggest that several ini-
tial and final clauses of adjacent paragraphs are also
likely to contain information useful to the model.
We tested the difference between the maxima
of PAR and NO PAR using the sign test, the non-
parametric equivalent of the paired t-test. The test
did not reveal any significance in the difference be-
tween their accuracies (6-, 7+, 116 ties: p = 1.000).
</bodyText>
<subsectionHeader confidence="0.967893">
6.1.2 Common anchor
</subsectionHeader>
<bodyText confidence="0.996083739130435">
We further examined how the criterion of the
common anchor influenced the quality of the model.
We compared this model (ANCHOR) against the one
that did not require that two clauses share an anchor
(NO ANCHOR), i.e. considering only co-occurrence
of verbs concatenated with specific syntactic role la-
bels. Additionally, we included into the experiment
a model that looked at plain verbs co-occurring in-
side a context window (PLAIN). Figure 3 compares
the performance of these three models (paragraph
boundaries were taken into account in all of them).
Compared with ANCHOR, the other two models
achieve considerably worse accuracy scores. The
differences between the maximum of ANCHOR and
those of the other models are significant according
to the sign test (ANCHOR vs NO ANCHOR: 44+, 8-,
77 ties: p &lt; 0.001; ANCHOR vs PLAIN: 44+, 10-,
75 ties: p &lt; 0.001). Their maxima are also reached
sooner (at the window of 7) and thereafter their per-
formance quickly degrades. This indicates that the
common anchor criterion is very useful, especially
for locating related clauses at larger distances in the
text.
</bodyText>
<figureCaption confidence="0.7679585">
Figure 3: The effect of the common anchor on the
accuracy of the method.
</figureCaption>
<bodyText confidence="0.9999174">
The accuracy scores for NO ANCHOR and PLAIN
are very similar across all the window size settings.
It appears that the consistent co-occurrence of spe-
cific syntactic labels on two verbs gives no addi-
tional evidence about the verbs being related.
</bodyText>
<subsectionHeader confidence="0.999841">
6.2 Human evaluation
</subsectionHeader>
<bodyText confidence="0.999558125">
Once the best parameter settings for the method
were found, we compared its performance against
human judges as well as the DIRT inference rules
and the verb entailment encoded in the WordNet 2.0
database.
Human judges. To elicit human judgments on
the evaluation data, we automatically converted the
templates into a natural language form using a num-
ber of simple rules to arrange words in the correct
grammatical order. In cases where an obligatory
syntactic position near a verb was missing, we sup-
plied the pronouns someone or something in that po-
sition. In each template pair, the premise was turned
into a statement, and the consequence into a ques-
tion. Figure 4 illustrates the result of converting the
test item from the previous example (Figure 1) into
the natural language form.
During the experiment, two judges were asked
to mark those statement-question pairs in each test
item, where, considering the statement, they could
answer the question affirmatively. The judges’ deci-
sions coincided in 95 of 129 test items. The Kappa
statistic is K=0.725, which provides some indication
about the upper bound of performance on this task.
</bodyText>
<page confidence="0.991784">
54
</page>
<figure confidence="0.8643685">
X bought Y. After that:
1* Did X own Y?
2 Did X approve Y?
3 Did X reach Y?
4 Did X decline Y?
5 Did someone compare X with Y?
</figure>
<figureCaption confidence="0.955897">
Figure 4: A test item from the test dataset. The cor-
rect consequence is marked by an asterisk.
</figureCaption>
<bodyText confidence="0.999118684210526">
DIRT. We also experimented with the inference
rules contained in the DIRT database (Lin and Pan-
tel, 2001). According to (Lin and Pantel, 2001), an
inference rule is a relation between two verbs which
are more loosely related than typical paraphrases,
but nonetheless can be useful for performing infer-
ences over natural language texts. We were inter-
ested to see how these inference rules perform on
the entailment recognition task.
For each dependency tree path (a graph linking a
verb with two slots for its arguments), DIRT con-
tains a list of the most similar tree paths along with
the similarity scores. To decide which is the most
likely consequence in each test item, we looked up
the DIRT database for the corresponding two depen-
dency tree paths. The template pair with the greatest
similarity was output as the correct answer.
WordNet. WordNet 2.0 contains manually en-
coded entailment relations between verb synsets,
which are labeled as “cause”, “troponymy”, or “en-
tailment”. To identify the template pair satisfying
entailment in a test item, we checked whether the
two verbs in each pair are linked in WordNet in
terms of one of these three labels. Because Word-
Net does not encode the information as to the rela-
tive plausibility of relations, all template pairs where
verbs were linked in WordNet, were output as cor-
rect answers.
Figure 5 describes the accuracy scores achieved
by our entailment acquisition algorithm, the two hu-
man judges, DIRT and WordNet. For comparison
purposes, the random baseline is also shown.
Our algorithm outperformed WordNet by 0.38
and DIRT by 0.15. The improvement is significant
vs. WordNet (73+, 27-, 29 ties: p&lt;0.001) as well as
vs. DIRT (37+, 20-, 72 ties: p=0.034).
We examined whether the improvement on DIRT
was due to the fact that DIRT had less extensive
</bodyText>
<figureCaption confidence="0.947330666666667">
Figure 5: A comparison of performance of the
proposed algorithm, WordNet, DIRT, two human
judges, and a random baseline.
</figureCaption>
<bodyText confidence="0.999663666666667">
coverage, encoding only verb pairs with similarity
above a certain threshold. We re-computed the ac-
curacy scores for the two methods, ignoring cases
where DIRT did not make any decision, i.e. where
the database contained none of the five verb pairs
of the test item. On the resulting 102 items, our
method was again at an advantage, 0.735 vs. 0.647,
but the significance of the difference could not be
established (21+, 12-, 69 ties: p=0.164).
The difference in the performance between our al-
gorithm and the human judges is quite large (0.103
vs. Judge 1 and 0.088 vs Judge 2), but significance
to the 0.05 level could not be found (vs. Judge 1:
17-, 29+, 83 ties: p=0.105; vs. Judge 2: 15-, 27+,
ties 87: p=0.09).
</bodyText>
<sectionHeader confidence="0.998501" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999021428571429">
In this paper we proposed a novel method for au-
tomatic discovery of verb entailment relations from
text, a problem that is of potential benefit for many
NLP applications. The central assumption behind
the method is that verb entailment relations mani-
fest themselves in the regular co-occurrence of two
verbs inside locally coherent text. Our evaluation
has shown that this assumption provides a promis-
ing approach for discovery of verb entailment. The
method achieves good performance, demonstrating
a closer approximation to the human performance
than inference rules, constructed on the basis of dis-
tributional similarity between paths in parse trees.
A promising direction along which this work
</bodyText>
<page confidence="0.995094">
55
</page>
<bodyText confidence="0.999944357142857">
can be extended is the augmentation of the current
algorithm with techniques for coreference reso-
lution. Coreference, nominal and pronominal, is
an important aspect of the linguistic realization of
local discourse structure, which our model did not
take into account. As the experimental evaluation
suggests, many verbs related by entailment occur
close to one another in the text. It is very likely that
many common event participants appearing in such
proximity are referred to by coreferential expres-
sions, and therefore noticeable improvement can
be expected from applying coreference resolution
to the corpus prior to learning entailment patterns
from it.
</bodyText>
<sectionHeader confidence="0.996673" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99988475">
We are grateful to Nikiforos Karamanis and Mirella Lapata
as well as three anonymous reviewers for valuable comments
and suggestions. We thank Patrick Pantel and Dekang Lin for
making the DIRT database available for this study.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914430379747">
N. Asher and A. Lascarides. 2003. Logics of Conversation.
Cambridge University Press.
R. Barzilay and M. Lapata. 2005. Modeling local coherence:
an entity-based approach. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Linguis-
tics (ACL’05), pages 141–148.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring
strategies for sentence ordering in multidocument summa-
rization. JAIR.
T. Chklovski and P. Pantel. 2004. VERBOCEAN: Mining the
web for fine-grained semantic verb relations. In In Proceed-
ings of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’04).
T.M. Cover. and J.A. Thomas. 1991. Elements ofInformation
Theory. Wiley-Interscience.
J. Curtis, G. Matthews, and D. Baxter. 2005. On the effective
use of cyc in a question answering system. In Proceedings
the IJCAI’05 Workshop on Knowledge and Reasoning for
Answering Questions.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based mod-
els of cooccurrence probabilities. Machine Learning, 34(1-
3):43–69.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pascal
recognising textual entailment challenge. In PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
C. Fellbaum, 1998. WordNet: An Electronic Lexical Database,
chapter Semantic network of English verbs. MIT Press.
M. Geffet and I. Dagan. 2005. The distributional inclusion hy-
potheses and lexical entailment. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL’05), pages 107–114.
R. Girju. 2003. Automatic detection of causal relations for
question answering. In Proceedings of the ACL’03 Work-
shop on ”Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond”.
B. Grosz, A. Joshi, and S.Weinstein. 1995. Centering: a frame-
work for modeling the local coherence of discourse. Com-
putational Linguistics, 21(2):203–225.
J.R. Hobbs. 1985. On the coherence and structure of discourse.
Technical Report CSLI-85-37, Center for the Study of Lan-
guage and Information.
T. Inui, K.Inui, and Y.Matsumoto. 2003. What kinds and
amounts of causal knowledge can be acquired from text by
using connective markers as clues? In Proceedings of the
6th International Conference on Discovery Science, pages
180–193.
M. Lapata. 2003. Probabilistic text structuring: experiments
with sentence ordering. In Proceedings of the 41rd Annual
Meeting of the Association for Computational Linguistics
(ACL’03), pages 545–552.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question answering. Natural Language Engineering,
7(4):343–360.
D. Moldovan and V. Rus. 2001. Logic form transformation
of WordNet and its applicability to question answering. In
Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL’01).
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: extracting paraphrases
and generating new sentences. In Proceedings of HLT-
NAACL’2003.
E. Reiter and R. Dale. 2000. Building Natural Language Gen-
eration Systems. Cambidge University Press.
P. Resnik. 1993. Selection and Information: A Class-Based
Approach to Lexical Relationships. Ph.D. thesis, University
of Pennsylvania.
H. Sch¨utze. 1992. Context space. In Fall Symposium on Prob-
abilistic Approaches to Natural Language, pages 113–120.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling
web-based acquisition of entailment relations. In Proceed-
ings of Empirical Methods in Natural Language Processing
(EMNLP’04).
P. Tapanainen and T. J¨arvinen. 1997. A non-projective depen-
dency parser. In Proceedings of the 5th Conference on Ap-
plied Natural Language Processing, pages 64–71.
K. Torisawa, 2003. Questions and Answers: Theoretical
and Applied Perspectives, chapter An unsupervised learning
method for commonsensical inference rules on events. Uni-
versity of Utrecht.
</reference>
<page confidence="0.998425">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451367">
<title confidence="0.992465">Acquisition of Verb Entailment from Text</title>
<author confidence="0.638583">Viktor</author>
<affiliation confidence="0.9601805">Computational Linguistics University of</affiliation>
<address confidence="0.8477155">MB109 Stafford Wolverhampton WV1 1SB,</address>
<email confidence="0.994075">v.pekar@wlv.ac.uk</email>
<abstract confidence="0.999787210526316">The study addresses the problem of automatic acquisition of entailment relations between verbs. While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations. Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus. In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9219" citStr="Asher and Lascarides, 2003" startWordPosition="1472" endWordPosition="1475">entailment, we do not require that verb entailment holds in all conceivable contexts and view it as a relation that may be more plausible in some contexts than others. For each verb pair, we therefore wish to assign a score quantifying the likelihood of its satisfying entailment in some random context. 4 Approach The key assumption behind our approach is that the ability of a verb to imply an event typically denoted by a different verb manifests itself in the regular cooccurrence of the two verbs inside locally coherent text. This assumption is not arbitrary: as discourse investigations show (Asher and Lascarides, 2003), (Hobbs, 1985), lexical entailment plays an important role in determining the local structure of discourse. We expect this co-occurrence regularity to be equally characteristic of any pair of verbs related by entailment, regardless of is type and the syntactic behavior of verbs. The method consists of three major steps. First, it identifies pairs of clauses that are related in the local discourse. From related clauses, it then creates templates by extracting pairs of verbs along with relevant information as to their syntactic behavior. Third, the method scores each verb pair in terms of plaus</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>141--148</pages>
<contexts>
<context position="11314" citStr="Barzilay and Lapata, 2005" startWordPosition="1803" endWordPosition="1806"> other in the text. Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). To capture related clauses within larger text fragments, we experiment with windows of text of various sizes around a clause. Paragraph boundaries. Since locally related sentences tend to be grouped into paragraphs, we further require that the two clauses appear within the same paragraph. Common event participant. Entity-based theories of discourse (e.g., (Grosz et al., 1995)) claim that a coherent text segment tends to focus on a specific entity. This intuition has been formalized by (Barzilay and Lapata, 2005), who developed an entity-based statistical representation of local discourse and showed its usefulness for estimating coherence between sentences. We also impose this as a criterion for two clauses to be discourse-related: their arguments need to refer to the same participant, henceforth, anchor. We identify the anchor as the same noun lemma appearing as an argument to the verbs in both clauses, considering only subject, object, and prepositional object arguments. The anchor must not be a pronoun, since identical pronouns may refer to different entities and making use of such correspondences </context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>R. Barzilay and M. Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument summarization.</title>
<date>2002</date>
<publisher>JAIR.</publisher>
<contexts>
<context position="2464" citStr="Barzilay et al., 2002" startWordPosition="380" endWordPosition="383">ails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an entailment resource can be of interest to tasks that deal with structuring a set of individual facts into coherent text. In Natural Language Generation (Reiter and Dale, 2000) and Multi-Document Summarization (Barzilay et al., 2002) it can be used to order sentences coming from multiple, possibly unrelated sources to produce a coherent document. The knowledge is essential for compiling answers for procedural questions in a QA system, when sentences containing relevant information are spread across the corpus (Curtis et al., 2005). The present paper is concerned with the problem of automatic acquisition of verb entailment from text. In the next section we set the background for the study by describing previous work. We then define the goal of the study and describe our method for verb entailment acquisition. After that we</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument summarization. JAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>VERBOCEAN: Mining the web for fine-grained semantic verb relations. In</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP’04).</booktitle>
<contexts>
<context position="4399" citStr="Chklovski and Pantel, 2004" startWordPosition="691" endWordPosition="694">is a directional, or asymmetric, relation: one verb entails the other, but the converse does not hold. For example, the verbs buy and purchase paraphrase each other: either of them can substitute its counterpart in most contexts without altering their meaning. The verb buy entails own so that buy can be replaced with own without introducing any contradicting content into the original sentence. Replacing own with buy, however, does convey new meaning. To account for the asymmetric character of entailment, a popular approach has been to use lexicosyntactic patterns indicative of entailment. In (Chklovski and Pantel, 2004) different types of semantic relations between verbs are discovered using surface patterns (like “X-ed by Y-ing” for enablement1, which would match “obtained by borrowing”, for example) and assessing the strength of asymmetric relations as mutual information between the two verbs. (Torisawa, 2003) collected pairs of coordinated verbs, i.e. matching patterns like “X-ed and Y-ed”, and then estimated the probability of entailment using corpus counts. (Inui et al., 2003) used a similar approach exploiting causative expressions such as because, though, and so. (Girju, 2003) extracted causal relatio</context>
<context position="5853" citStr="Chklovski and Pantel, 2004" startWordPosition="920" endWordPosition="923">to achieve high precision, their reliance on surface patterns limits their coverage in that they address only those relations that are regularly made explicit through concrete natural language expressions, and only within sentences. The method for noun entailment acquisition by (Geffet and Dagan, 2005) is based on the idea of distributional inclusion, according to which one noun is entailed by the other if the set of occurrence contexts of the former subsumes that of the latter. However, this approach is likely to pick only a particular kind of verb entailment, that of troponymy (such as 1In (Chklovski and Pantel, 2004) enablement is defined to be a relation where one event often, but not necessarily always, gives rise to the other event, which coincides with our definition of entailment (see Section 3). march-walk) and overlook pairs where there is little overlap in the occurrence patterns between the two verbs. In tasks involving recognition of relations between entities such as Question Answering and Information Extraction, it is crucial to encode the mapping between the argument structures of two verbs. Pattern-matching often imposes restrictions on the syntactic configurations in which the verbs can app</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>T. Chklovski and P. Pantel. 2004. VERBOCEAN: Mining the web for fine-grained semantic verb relations. In In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="13755" citStr="Thomas, 1991" startWordPosition="2226" endWordPosition="2227">r, and q E Q is the consequence, consisting of the verb vQ and rQ – its syntactic relation to the anchor. 4.3 Measuring asymmetric association To score the pairs for asymmetric association, we use a procedure similar to the method by (Resnik, 1993) for learning selectional preferences of verbs. Each template in a pair is tried as both a premise and a consequence. We quantify the ’preference’ of the premise p for the consequence q as the contribution of q to the amount of information p contains about its consequences seen in the data. First, we calculate Kullback-Leibler Divergence (Cover. and Thomas, 1991) between two probability distributions, u – the prior distribution of all consequences in the data and w – their posterior distribution given p, thus measuring the information p contains about its consequences: u(x) u(x) log (1) w(x) where u(x) = P(qxjp), w(x) = P(qx), and x ranges over all consequences in the data. Then, the score for template {p, q} expressing the association of q with p is calculated as the proportion of q’s contribution to Dp(ujjw): P (qjp) Score(p,q) = P(qjp)log P (p) Dp(ujjw)−&apos; (2) In each pair we compare the scores in both directions, taking the direction with the great</context>
</contexts>
<marker>Thomas, 1991</marker>
<rawString>T.M. Cover. and J.A. Thomas. 1991. Elements ofInformation Theory. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curtis</author>
<author>G Matthews</author>
<author>D Baxter</author>
</authors>
<title>On the effective use of cyc in a question answering system.</title>
<date>2005</date>
<booktitle>In Proceedings the IJCAI’05 Workshop on Knowledge and Reasoning for Answering Questions.</booktitle>
<contexts>
<context position="1617" citStr="Curtis et al., 2005" startWordPosition="248" endWordPosition="252">rpart of the commonsense knowledge that certain events and states give rise to other events and states. For example, there is an entailment relation between the verbs buy and belong, which reflects the commonsense notion that if someone has bought an object, this object belongs to that person. A lexical resource encoding entailment can serve as a useful tool in many tasks where automatic inferencing over natural language text is required. In Question Answering, it has been used to establish that a certain sentence found in the corpus can serve as a suitable, albeit implicit answer to a query (Curtis et al., 2005), (Girju, 2003), (Moldovan and Rus, 2001). In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Ho</context>
</contexts>
<marker>Curtis, Matthews, Baxter, 2005</marker>
<rawString>J. Curtis, G. Matthews, and D. Baxter. 2005. On the effective use of cyc in a question answering system. In Proceedings the IJCAI’05 Workshop on Knowledge and Reasoning for Answering Questions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similarity-based models of cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="14625" citStr="Dagan et al., 1999" startWordPosition="2369" endWordPosition="2372">jp), w(x) = P(qx), and x ranges over all consequences in the data. Then, the score for template {p, q} expressing the association of q with p is calculated as the proportion of q’s contribution to Dp(ujjw): P (qjp) Score(p,q) = P(qjp)log P (p) Dp(ujjw)−&apos; (2) In each pair we compare the scores in both directions, taking the direction with the greater score to indicate the most likely premise and consequence and thus the direction of entailment. 5 Evaluation Design 5.1 Task To evaluate the algorithm, we designed a recognition task similar to that of pseudo-word disambiguation (Sch¨utze, 1992), (Dagan et al., 1999). The task was, given a certain premise, to select its correct consequence out of a pool with several artificially created incorrect alternatives. The advantages of this evaluation technique are twofold. On the one hand, the task mimics many possible practical applications of the entailment resource, such as sentence ordering, where, given a sentence, it is necessary to identify among several alternatives another sentence that either entails or is entailed by the given sentence. On the other hand, in comparison with manual evaluation of the direct output of the system, it requires minimal huma</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based models of cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="2119" citStr="Dagan et al., 2005" startWordPosition="327" endWordPosition="330">certain sentence found in the corpus can serve as a suitable, albeit implicit answer to a query (Curtis et al., 2005), (Girju, 2003), (Moldovan and Rus, 2001). In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an entailment resource can be of interest to tasks that deal with structuring a set of individual facts into coherent text. In Natural Language Generation (Reiter and Dale, 2000) and Multi-Document Summarization (Barzilay et al., 2002) it can be used to order sentences coming from multiple, possibly unrelated sources to produce a coherent document. The knowledge is essential for compiling answers for procedural questions in a QA system, when sentences containing relevant information ar</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The pascal recognising textual entailment challenge. In PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database, chapter Semantic network of English verbs.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7585" citStr="Fellbaum, 1998" startWordPosition="1198" endWordPosition="1199">these pairs are not distinguished from paraphrases and the direction of relation in such pairs is not recognized. To sum up, a major challenge in entailment acquisition is the need for more generic methods that would cover an unrestricted range of entailment types and learn the mapping between verbs with varied argument structures, eventually yielding resources suitable for robust large-scale applications. 3 Verb Entailment Verb entailment relations have been traditionally attracting a lot of interest from lexical semantics research and their various typologies have been proposed (see, e.g., (Fellbaum, 1998)). In this study, with the view of potential practical applications, we adopt an operational definition of entailment. We define it to be a semantic relation between verbs where one verb, termed premise P, refers to event Ep and at the same time implies event EQ, typically denoted by the other verb, termed consequence Q. The goal of verb entailment acquisition is then to find two linguistic templates each consisting of a verb and slots for its syntactic arguments. In the pair, (1) the verbs are related in accordance with our definition of entailment above, (2) there is a mapping between the sl</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum, 1998. WordNet: An Electronic Lexical Database, chapter Semantic network of English verbs. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>107--114</pages>
<contexts>
<context position="5529" citStr="Geffet and Dagan, 2005" startWordPosition="861" endWordPosition="864">causative expressions such as because, though, and so. (Girju, 2003) extracted causal relations between nouns like “Earthquakes generate tsunami” by first using lexico-syntactic patterns to collect relevant data and then using a decision tree classifier to learn the relations. Although these techniques have been shown to achieve high precision, their reliance on surface patterns limits their coverage in that they address only those relations that are regularly made explicit through concrete natural language expressions, and only within sentences. The method for noun entailment acquisition by (Geffet and Dagan, 2005) is based on the idea of distributional inclusion, according to which one noun is entailed by the other if the set of occurrence contexts of the former subsumes that of the latter. However, this approach is likely to pick only a particular kind of verb entailment, that of troponymy (such as 1In (Chklovski and Pantel, 2004) enablement is defined to be a relation where one event often, but not necessarily always, gives rise to the other event, which coincides with our definition of entailment (see Section 3). march-walk) and overlook pairs where there is little overlap in the occurrence patterns</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL’03 Workshop on ”Multilingual Summarization and Question Answering - Machine Learning and Beyond”.</booktitle>
<contexts>
<context position="1632" citStr="Girju, 2003" startWordPosition="253" endWordPosition="254">e knowledge that certain events and states give rise to other events and states. For example, there is an entailment relation between the verbs buy and belong, which reflects the commonsense notion that if someone has bought an object, this object belongs to that person. A lexical resource encoding entailment can serve as a useful tool in many tasks where automatic inferencing over natural language text is required. In Question Answering, it has been used to establish that a certain sentence found in the corpus can serve as a suitable, albeit implicit answer to a query (Curtis et al., 2005), (Girju, 2003), (Moldovan and Rus, 2001). In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an </context>
<context position="4974" citStr="Girju, 2003" startWordPosition="782" endWordPosition="783">ment. In (Chklovski and Pantel, 2004) different types of semantic relations between verbs are discovered using surface patterns (like “X-ed by Y-ing” for enablement1, which would match “obtained by borrowing”, for example) and assessing the strength of asymmetric relations as mutual information between the two verbs. (Torisawa, 2003) collected pairs of coordinated verbs, i.e. matching patterns like “X-ed and Y-ed”, and then estimated the probability of entailment using corpus counts. (Inui et al., 2003) used a similar approach exploiting causative expressions such as because, though, and so. (Girju, 2003) extracted causal relations between nouns like “Earthquakes generate tsunami” by first using lexico-syntactic patterns to collect relevant data and then using a decision tree classifier to learn the relations. Although these techniques have been shown to achieve high precision, their reliance on surface patterns limits their coverage in that they address only those relations that are regularly made explicit through concrete natural language expressions, and only within sentences. The method for noun entailment acquisition by (Geffet and Dagan, 2005) is based on the idea of distributional inclu</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for question answering. In Proceedings of the ACL’03 Workshop on ”Multilingual Summarization and Question Answering - Machine Learning and Beyond”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: a framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="11175" citStr="Grosz et al., 1995" startWordPosition="1780" endWordPosition="1783">verb with its dependent constituents as a clause. For two clauses to be discourse-related, we require that they appear close to each other in the text. Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). To capture related clauses within larger text fragments, we experiment with windows of text of various sizes around a clause. Paragraph boundaries. Since locally related sentences tend to be grouped into paragraphs, we further require that the two clauses appear within the same paragraph. Common event participant. Entity-based theories of discourse (e.g., (Grosz et al., 1995)) claim that a coherent text segment tends to focus on a specific entity. This intuition has been formalized by (Barzilay and Lapata, 2005), who developed an entity-based statistical representation of local discourse and showed its usefulness for estimating coherence between sentences. We also impose this as a criterion for two clauses to be discourse-related: their arguments need to refer to the same participant, henceforth, anchor. We identify the anchor as the same noun lemma appearing as an argument to the verbs in both clauses, considering only subject, object, and prepositional object ar</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. Grosz, A. Joshi, and S.Weinstein. 1995. Centering: a framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<tech>Technical Report CSLI-85-37,</tech>
<institution>Center for</institution>
<contexts>
<context position="2227" citStr="Hobbs, 1985" startWordPosition="345" endWordPosition="346">5), (Girju, 2003), (Moldovan and Rus, 2001). In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an entailment resource can be of interest to tasks that deal with structuring a set of individual facts into coherent text. In Natural Language Generation (Reiter and Dale, 2000) and Multi-Document Summarization (Barzilay et al., 2002) it can be used to order sentences coming from multiple, possibly unrelated sources to produce a coherent document. The knowledge is essential for compiling answers for procedural questions in a QA system, when sentences containing relevant information are spread across the corpus (Curtis et al., 2005). The present paper is concerned with the problem of automat</context>
<context position="9234" citStr="Hobbs, 1985" startWordPosition="1476" endWordPosition="1477">that verb entailment holds in all conceivable contexts and view it as a relation that may be more plausible in some contexts than others. For each verb pair, we therefore wish to assign a score quantifying the likelihood of its satisfying entailment in some random context. 4 Approach The key assumption behind our approach is that the ability of a verb to imply an event typically denoted by a different verb manifests itself in the regular cooccurrence of the two verbs inside locally coherent text. This assumption is not arbitrary: as discourse investigations show (Asher and Lascarides, 2003), (Hobbs, 1985), lexical entailment plays an important role in determining the local structure of discourse. We expect this co-occurrence regularity to be equally characteristic of any pair of verbs related by entailment, regardless of is type and the syntactic behavior of verbs. The method consists of three major steps. First, it identifies pairs of clauses that are related in the local discourse. From related clauses, it then creates templates by extracting pairs of verbs along with relevant information as to their syntactic behavior. Third, the method scores each verb pair in terms of plausibility of enta</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>J.R. Hobbs. 1985. On the coherence and structure of discourse. Technical Report CSLI-85-37, Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Inui</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>What kinds and amounts of causal knowledge can be acquired from text by using connective markers as clues?</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th International Conference on Discovery Science,</booktitle>
<pages>180--193</pages>
<contexts>
<context position="4870" citStr="Inui et al., 2003" startWordPosition="765" endWordPosition="768">tric character of entailment, a popular approach has been to use lexicosyntactic patterns indicative of entailment. In (Chklovski and Pantel, 2004) different types of semantic relations between verbs are discovered using surface patterns (like “X-ed by Y-ing” for enablement1, which would match “obtained by borrowing”, for example) and assessing the strength of asymmetric relations as mutual information between the two verbs. (Torisawa, 2003) collected pairs of coordinated verbs, i.e. matching patterns like “X-ed and Y-ed”, and then estimated the probability of entailment using corpus counts. (Inui et al., 2003) used a similar approach exploiting causative expressions such as because, though, and so. (Girju, 2003) extracted causal relations between nouns like “Earthquakes generate tsunami” by first using lexico-syntactic patterns to collect relevant data and then using a decision tree classifier to learn the relations. Although these techniques have been shown to achieve high precision, their reliance on surface patterns limits their coverage in that they address only those relations that are regularly made explicit through concrete natural language expressions, and only within sentences. The method </context>
</contexts>
<marker>Inui, Inui, Matsumoto, 2003</marker>
<rawString>T. Inui, K.Inui, and Y.Matsumoto. 2003. What kinds and amounts of causal knowledge can be acquired from text by using connective markers as clues? In Proceedings of the 6th International Conference on Discovery Science, pages 180–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic text structuring: experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41rd Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<pages>545--552</pages>
<contexts>
<context position="10795" citStr="Lapata, 2003" startWordPosition="1725" endWordPosition="1726">, we do not build a full discourse representation of text, nor do we try to identify the type of particular rhetorical relations between sentences, but rather identify pairs of clauses that are likely to be discourse-related. Textual proximity. We start by parsing the corpus with a dependency parser (we use Connexor’s FDG (Tapanainen and J¨arvinen, 1997)), treating every verb with its dependent constituents as a clause. For two clauses to be discourse-related, we require that they appear close to each other in the text. Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). To capture related clauses within larger text fragments, we experiment with windows of text of various sizes around a clause. Paragraph boundaries. Since locally related sentences tend to be grouped into paragraphs, we further require that the two clauses appear within the same paragraph. Common event participant. Entity-based theories of discourse (e.g., (Grosz et al., 1995)) claim that a coherent text segment tends to focus on a specific entity. This intuition has been formalized by (Barzilay and Lapata, 2005), who developed an entity-based statistical representation of local discourse and</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>M. Lapata. 2003. Probabilistic text structuring: experiments with sentence ordering. In Proceedings of the 41rd Annual Meeting of the Association for Computational Linguistics (ACL’03), pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="3312" citStr="Lin and Pantel, 2001" startWordPosition="518" endWordPosition="521">ning relevant information are spread across the corpus (Curtis et al., 2005). The present paper is concerned with the problem of automatic acquisition of verb entailment from text. In the next section we set the background for the study by describing previous work. We then define the goal of the study and describe our method for verb entailment acquisition. After that we present results of its experimental evaluation. Finally, we draw conclusions and outline future work. 2 Previous Work The task of verb entailment acquisition appears to have much in common with that of paraphrase acquisition (Lin and Pantel, 2001), (Pang et al., 2003), (Szpektor et al., 2004). In both tasks the goal is to discover pairs of related verbs and identify map49 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49–56, New York, June 2006. c�2006 Association for Computational Linguistics pings between their argument structures. The important distinction is that while in a paraphrase the two verbs are semantically equivalent, entailment is a directional, or asymmetric, relation: one verb entails the other, but the converse does not hold. For example, the verbs buy and purcha</context>
<context position="6755" citStr="Lin and Pantel, 2001" startWordPosition="1063" endWordPosition="1066">een the two verbs. In tasks involving recognition of relations between entities such as Question Answering and Information Extraction, it is crucial to encode the mapping between the argument structures of two verbs. Pattern-matching often imposes restrictions on the syntactic configurations in which the verbs can appear in the corpus: the patterns employed by (Chklovski and Pantel, 2004) and (Torisawa, 2003) derive pairs of only those verbs that have identical argument structures, and often only those that involve a subject and a direct object. The method for discovery of inference rules by (Lin and Pantel, 2001) obtains pairs of verbs with highly varied argument structures, which also do not have to be identical for the two verbs. While the inference rules the method acquires seem to encompass pairs related by entailment, these pairs are not distinguished from paraphrases and the direction of relation in such pairs is not recognized. To sum up, a major challenge in entailment acquisition is the need for more generic methods that would cover an unrestricted range of entailment types and learn the mapping between verbs with varied argument structures, eventually yielding resources suitable for robust l</context>
<context position="18635" citStr="Lin and Pantel, 2001" startWordPosition="2996" endWordPosition="2999">e to the number of tying alternatives. This experimental design corresponds to a random baseline of 0.2, i.e. the expected accuracy when selecting a consequence template randomly out of 5 alternatives. 6 Results and Discussion We now present the results of the evaluation of the method. In Section 6.1, we study its parameters and determine the best configuration. In Section 6.2, we compare its performance against that of human subjects as well as that of two state-of-the-art lexical resources: the verb entailment knowledge contained in WordNet2.0 and the inference rules from the DIRT database (Lin and Pantel, 2001). 6.1 Model parameters We first examined the following parameters of the model: the window size, the use of paragraph boundaries, and the effect of the shared anchor on the quality of the model. 6.1.1 Window size and paragraph boundaries As was mentioned in Section 4.1, a free parameter in our model is a threshold on the distance between two clauses, that we take as an indicator that the clauses are discourse-related. To find an optimal threshold, we experimented with windows of 1, 2 ... 25 clauses around a given clause, taking clauses appearing within the window as potentially related to the </context>
<context position="23964" citStr="Lin and Pantel, 2001" startWordPosition="3911" endWordPosition="3915">ement-question pairs in each test item, where, considering the statement, they could answer the question affirmatively. The judges’ decisions coincided in 95 of 129 test items. The Kappa statistic is K=0.725, which provides some indication about the upper bound of performance on this task. 54 X bought Y. After that: 1* Did X own Y? 2 Did X approve Y? 3 Did X reach Y? 4 Did X decline Y? 5 Did someone compare X with Y? Figure 4: A test item from the test dataset. The correct consequence is marked by an asterisk. DIRT. We also experimented with the inference rules contained in the DIRT database (Lin and Pantel, 2001). According to (Lin and Pantel, 2001), an inference rule is a relation between two verbs which are more loosely related than typical paraphrases, but nonetheless can be useful for performing inferences over natural language texts. We were interested to see how these inference rules perform on the entailment recognition task. For each dependency tree path (a graph linking a verb with two slots for its arguments), DIRT contains a list of the most similar tree paths along with the similarity scores. To decide which is the most likely consequence in each test item, we looked up the DIRT database f</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>V Rus</author>
</authors>
<title>Logic form transformation of WordNet and its applicability to question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01).</booktitle>
<contexts>
<context position="1658" citStr="Moldovan and Rus, 2001" startWordPosition="255" endWordPosition="258">t certain events and states give rise to other events and states. For example, there is an entailment relation between the verbs buy and belong, which reflects the commonsense notion that if someone has bought an object, this object belongs to that person. A lexical resource encoding entailment can serve as a useful tool in many tasks where automatic inferencing over natural language text is required. In Question Answering, it has been used to establish that a certain sentence found in the corpus can serve as a suitable, albeit implicit answer to a query (Curtis et al., 2005), (Girju, 2003), (Moldovan and Rus, 2001). In Information Extraction, it can similarly help to recognize relations between named entities in cases when the entities in the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an entailment resource can be</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>D. Moldovan and V. Rus. 2001. Logic form transformation of WordNet and its applicability to question answering. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL’2003.</booktitle>
<contexts>
<context position="3333" citStr="Pang et al., 2003" startWordPosition="522" endWordPosition="525">n are spread across the corpus (Curtis et al., 2005). The present paper is concerned with the problem of automatic acquisition of verb entailment from text. In the next section we set the background for the study by describing previous work. We then define the goal of the study and describe our method for verb entailment acquisition. After that we present results of its experimental evaluation. Finally, we draw conclusions and outline future work. 2 Previous Work The task of verb entailment acquisition appears to have much in common with that of paraphrase acquisition (Lin and Pantel, 2001), (Pang et al., 2003), (Szpektor et al., 2004). In both tasks the goal is to discover pairs of related verbs and identify map49 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49–56, New York, June 2006. c�2006 Association for Computational Linguistics pings between their argument structures. The important distinction is that while in a paraphrase the two verbs are semantically equivalent, entailment is a directional, or asymmetric, relation: one verb entails the other, but the converse does not hold. For example, the verbs buy and purchase paraphrase each ot</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences. In Proceedings of HLTNAACL’2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambidge University Press.</publisher>
<contexts>
<context position="2407" citStr="Reiter and Dale, 2000" startWordPosition="373" endWordPosition="376">the text are linked by a linguistic construction that entails a known extraction pattern, but not by the pattern itself. A lexical entailment resource can contribute to information retrieval tasks via integration into a textual entailment system that aims to recognize entailment between two larger text fragments (Dagan et al., 2005). Since entailment is known to systematically interact with the discourse organization of text (Hobbs, 1985), an entailment resource can be of interest to tasks that deal with structuring a set of individual facts into coherent text. In Natural Language Generation (Reiter and Dale, 2000) and Multi-Document Summarization (Barzilay et al., 2002) it can be used to order sentences coming from multiple, possibly unrelated sources to produce a coherent document. The knowledge is essential for compiling answers for procedural questions in a QA system, when sentences containing relevant information are spread across the corpus (Curtis et al., 2005). The present paper is concerned with the problem of automatic acquisition of verb entailment from text. In the next section we set the background for the study by describing previous work. We then define the goal of the study and describe </context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambidge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="13390" citStr="Resnik, 1993" startWordPosition="2164" endWordPosition="2165">• Phrases with past and present participles are turned into predicate structures: the group led by A – A leads the group; the group leading the market – the group leads the market. The output of this step is V E P x Q, a set of pairs of templates {p, q}, where p E P is the premise, consisting of the verb vp and rp – the syntactic relation between vp and the anchor, and q E Q is the consequence, consisting of the verb vQ and rQ – its syntactic relation to the anchor. 4.3 Measuring asymmetric association To score the pairs for asymmetric association, we use a procedure similar to the method by (Resnik, 1993) for learning selectional preferences of verbs. Each template in a pair is tried as both a premise and a consequence. We quantify the ’preference’ of the premise p for the consequence q as the contribution of q to the amount of information p contains about its consequences seen in the data. First, we calculate Kullback-Leibler Divergence (Cover. and Thomas, 1991) between two probability distributions, u – the prior distribution of all consequences in the data and w – their posterior distribution given p, thus measuring the information p contains about its consequences: u(x) u(x) log (1) w(x) w</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Context space.</title>
<date>1992</date>
<booktitle>In Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>113--120</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>H. Sch¨utze. 1992. Context space. In Fall Symposium on Probabilistic Approaches to Natural Language, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP’04).</booktitle>
<contexts>
<context position="3358" citStr="Szpektor et al., 2004" startWordPosition="526" endWordPosition="529">he corpus (Curtis et al., 2005). The present paper is concerned with the problem of automatic acquisition of verb entailment from text. In the next section we set the background for the study by describing previous work. We then define the goal of the study and describe our method for verb entailment acquisition. After that we present results of its experimental evaluation. Finally, we draw conclusions and outline future work. 2 Previous Work The task of verb entailment acquisition appears to have much in common with that of paraphrase acquisition (Lin and Pantel, 2001), (Pang et al., 2003), (Szpektor et al., 2004). In both tasks the goal is to discover pairs of related verbs and identify map49 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49–56, New York, June 2006. c�2006 Association for Computational Linguistics pings between their argument structures. The important distinction is that while in a paraphrase the two verbs are semantically equivalent, entailment is a directional, or asymmetric, relation: one verb entails the other, but the converse does not hold. For example, the verbs buy and purchase paraphrase each other: either of them can s</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T J¨arvinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>P. Tapanainen and T. J¨arvinen. 1997. A non-projective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
</authors>
<title>Questions and Answers: Theoretical and Applied Perspectives, chapter An unsupervised learning method for commonsensical inference rules on events.</title>
<date>2003</date>
<institution>University of Utrecht.</institution>
<contexts>
<context position="4697" citStr="Torisawa, 2003" startWordPosition="740" endWordPosition="741">ced with own without introducing any contradicting content into the original sentence. Replacing own with buy, however, does convey new meaning. To account for the asymmetric character of entailment, a popular approach has been to use lexicosyntactic patterns indicative of entailment. In (Chklovski and Pantel, 2004) different types of semantic relations between verbs are discovered using surface patterns (like “X-ed by Y-ing” for enablement1, which would match “obtained by borrowing”, for example) and assessing the strength of asymmetric relations as mutual information between the two verbs. (Torisawa, 2003) collected pairs of coordinated verbs, i.e. matching patterns like “X-ed and Y-ed”, and then estimated the probability of entailment using corpus counts. (Inui et al., 2003) used a similar approach exploiting causative expressions such as because, though, and so. (Girju, 2003) extracted causal relations between nouns like “Earthquakes generate tsunami” by first using lexico-syntactic patterns to collect relevant data and then using a decision tree classifier to learn the relations. Although these techniques have been shown to achieve high precision, their reliance on surface patterns limits th</context>
<context position="6546" citStr="Torisawa, 2003" startWordPosition="1029" endWordPosition="1030">rily always, gives rise to the other event, which coincides with our definition of entailment (see Section 3). march-walk) and overlook pairs where there is little overlap in the occurrence patterns between the two verbs. In tasks involving recognition of relations between entities such as Question Answering and Information Extraction, it is crucial to encode the mapping between the argument structures of two verbs. Pattern-matching often imposes restrictions on the syntactic configurations in which the verbs can appear in the corpus: the patterns employed by (Chklovski and Pantel, 2004) and (Torisawa, 2003) derive pairs of only those verbs that have identical argument structures, and often only those that involve a subject and a direct object. The method for discovery of inference rules by (Lin and Pantel, 2001) obtains pairs of verbs with highly varied argument structures, which also do not have to be identical for the two verbs. While the inference rules the method acquires seem to encompass pairs related by entailment, these pairs are not distinguished from paraphrases and the direction of relation in such pairs is not recognized. To sum up, a major challenge in entailment acquisition is the </context>
</contexts>
<marker>Torisawa, 2003</marker>
<rawString>K. Torisawa, 2003. Questions and Answers: Theoretical and Applied Perspectives, chapter An unsupervised learning method for commonsensical inference rules on events. University of Utrecht.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>