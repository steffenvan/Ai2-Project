<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000153">
<title confidence="0.990805">
Training MRF-Based Phrase Translation Models using Gradient Ascent
</title>
<author confidence="0.991438">
Jianfeng Gao Xiaodong He
</author>
<affiliation confidence="0.985296">
Microsoft Research Microsoft Research
</affiliation>
<address confidence="0.973916">
Redmond, WA, USA Redmond, WA, USA
</address>
<email confidence="0.99912">
jfgao@microsoft.com xiaohe@microsoft.com
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988714285714">
This paper presents a general, statistical
framework for modeling phrase translation
via Markov random fields. The model al-
lows for arbituary features extracted from a
phrase pair to be incorporated as evidence.
The parameters of the model are estimated
using a large-scale discriminative training
approach that is based on stochastic gradi-
ent ascent and an N-best list based expected
BLEU as the objective function. The model
is easy to be incoporated into a standard
phrase-based statistical machine translation
system, requiring no code change in the
runtime engine. Evaluation is performed on
two Europarl translation tasks, German-
English and French-English. Results show
that incoporating the Markov random field
model significantly improves the perfor-
mance of a state-of-the-art phrase-based
machine translation system, leading to a
gain of 0.8-1.3 BLEU points.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990730769231">
The phrase translation model, also known as the
phrase table, is one of the core components of a
phrase-based statistical machine translation (SMT)
system. The most common method of constructing
the phrase table takes a two-phase approach. First,
the bilingual phrase pairs are extracted heuristical-
ly from an automatically word-aligned training da-
ta. The second phase is parameter estimation,
where each phrase pair is assigned with some
scores that are estimated based on counting of
words or phrases on the same word-aligned train-
ing data.
There has been a lot of research on improving
the quality of the phrase table using more princi-
pled methods for phrase extraction (e.g., Lamber
and Banchs 2005), parameter estimation (e.g.,
Wuebker et al. 2010; He and Deng 2012), or both
(e.g., Marcu and Wong 2002; Denero et al. 2006).
The focus of this paper is on the parameter estima-
tion phase. We revisit the problem of scoring a
phrase translation pair by developing a new phrase
translation model based on Markov random fields
(MRFs) and large-scale discriminative training.
We strive to address the following three primary
concerns.
First of all, instead of parameterizing a phrase
translation pair using a set of scoring functions that
are learned independently (e.g., phrase translation
probabilities and lexical weights) we use a general,
statistical framework in which arbitrary features
extracted from a phrase pair can be incorporated to
model the translation in a unified way. To this end,
we propose the use of a MRF model.
Second, because the phrase model has to work
with other component models in an SMT system in
order to produce good translations and the quality
of translation is measured via BLEU score, it is de-
sirable to optimize the parameters of the phrase
model jointly with other component models with
respect to an objective function that is closely re-
lated to the evaluation metric under consideration,
i.e., BLEU in this paper. To this end, we resort to a
large-scale discriminative training approach, fol-
lowing the pioneering work of Liang et al. (2006).
Although there are established methods of tuning a
handful of features on small training sets, such as
the MERT method (Och 2003), the development of
discriminative training methods for millions of fea-
tures on millions of sentence pairs is still an ongo-
ing area of research. A recent survey is due to
Koehn (2010). In this paper we show that by using
stochastic gradient ascent and an N-best list based
</bodyText>
<page confidence="0.971737">
450
</page>
<note confidence="0.470879">
Proceedings of NAACL-HLT 2013, pages 450–459,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994395">
expected BLEU as the objective function, large-
scale discriminative training can lead to significant
improvements.
The third primary concern is the ease of adop-
tion of the proposed method. To this end, we use a
simple and well-established learning method, en-
suring that the results can be easily reproduced.
We also develop the features for the MRF model in
such a way that the resulting model is of the same
format as that of a traditional phrase table. Thus,
the model can be easily incorporated into a stand-
ard phrase-based SMT system, requiring no code
change in the runtime engine.
In the rest of the paper, Section 2 presents the
MRF model for phrase translation. Section 3 de-
scribes the way the model parameters are estimated.
Section 4 presents the experimental results on two
Europarl translation tasks. Section 5 reviews pre-
vious work that lays the foundation of this study.
Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.99975825">
The traditional translation models are directional
models that are based on conditional probabilities.
As suggested by the noisy-channel model for SMT
(Brown et al. 1993):
</bodyText>
<equation confidence="0.9794775">
E∗ = argmax P(EJF) =argmaxP(E)P(FJE (1)
E E
</equation>
<bodyText confidence="0.9954125">
The Bayes rule leads us to invert the conditioning
of translation probability from a foreign (source)
sentence F to an English (target) translation E.
However, in practice, the implementation of
state-of-the-art phrase-based SMT systems uses a
weighted log-linear combination of several models
h(F, E, A) including the logarithm of the phrase
probability (and the lexical weight) in source-to-
target and target-to-source directions (Och and Ney
2004)
</bodyText>
<equation confidence="0.955670666666667">
E∗ = argmaxEEm=1,Im.hm(F,E,A) (2)
= argmax Score)(F,E)
E
</equation>
<bodyText confidence="0.999899789473684">
where A in h(F, E, A) is a hidden structure that
best derives E from F, called the Viterbi derivation
afterwards. In phrase-based SMT, A consists of (1)
the segmentation of the source sentence into
phrases, (2) the segmentation of the target sentence
into phrases, and (3) an alignment between the
source and target phrases.
In this paper we use Markov random fields
(MRFs) to model the joint distribution P�(f, e)
over a source-target translation phrase pair (f, e),
parameterized by w. Different from the directional
translation models, as in Equation (1), the MRF
model is undirected, which we believe upholds the
spirit of the use of bi-directional translation proba-
bilities under the log-linear framework. That is, the
agreement or the compatibility of a phrase pair is
more effective to score translation quality than a
directional translation probability which is mod-
eled based on an imagined generative story does.
</bodyText>
<subsectionHeader confidence="0.909032">
2.1 MRF
</subsectionHeader>
<bodyText confidence="0.999924941176471">
MRFs, also known as undirected graphical models,
are widely used in modeling joint distributions of
spatial or contextual dependencies of physical phe-
nomena (Bishop 2006). A Markov random field is
constructed from a graph G . The nodes of the
graph represent random variables, and edges define
the independence semantics between the random
variables. An MRF satisfies the Markov property,
which states that a node is independent of all of its
non-neighbors, defined by the clique configura-
tions of G. In modeling a phrase translation pair,
we define two types of nodes, (1) two phrase nodes
and (2) a set of word nodes, each for a word in the-
se phrases, such as the graph in Figure 1. Let us
denote a clique by c and the set of variables in that
clique by (f, e)c. Then, the joint distribution over
the random variables in G is defined as
</bodyText>
<equation confidence="0.964293666666667">
Pw(f, e) = � R~
~(~) T�((f, e)~; w), (3)
�
</equation>
<bodyText confidence="0.998818555555556">
where e = el,..., e|e |, f = f&amp;quot; ...,fif |and C(G) is
the set of cliques in G, and each Tc((f, e)c; w) is a
non-negative potential function defined over a
clique c that measures the compatibility of the var-
iables in c, w is a set of parameters that are used
within the potential function. Z in Equation (3),
sometimes called the partition function, is a nor-
malization constant
and is given by
</bodyText>
<equation confidence="0.9873715">
Z = EfEe RceC(G) Tc((f, e)c; w) (4)
= Ef Ee Score(f, e),
</equation>
<bodyText confidence="0.9833955">
which ensures that the distribution Pw(f, e) given
by Equation (3) is correctly normalized. The pres-
</bodyText>
<page confidence="0.999058">
451
</page>
<figureCaption confidence="0.889229">
Figure 1: A Markov random field model for phrase
translation of e = el, e2 and f = fl, f2, f3.
</figureCaption>
<bodyText confidence="0.999469681818182">
ence of is one of the major limitations of MRFs
because it is generally not feasible to compute due
to the exponential number of terms in the summa-
tion. However, we notice that Z is a global con-
stant which is independent of a and f. Therefore, in
ranking phrase translation hypotheses, as per-
formed by the decoder in SMT systems, we can
drop Z and simply rank each hypothesis by its
unnormalized joint probability. In our implementa-
tion, we only store in the phrase table for each
translation pair (f, e) its unnormalized probability,
i.e., Score(f, e) as defined in Equation (4).
It is common to define MRF potential functions
of the exponential form as Tc((f, e)c; w) =
exp (wcgi(c)), where q5(c) is a real-valued feature
function over clique c and wc is the weight of the
feature function. In phrase-based SMT systems, the
sentence-level translation probability from F to E
is decomposed as the product of a set of phrase
translation probabilities. By dropping the phrase
segmentation and distortion model components, we
have
</bodyText>
<equation confidence="0.998510666666667">
P(E|F) ≈ maxP(E|A,F) (5)
�
P(E|A,F) = ∏(f,�)∈AP(e|f),
</equation>
<bodyText confidence="0.883239">
where A is the Viterbi derivation. Similarly, the
joint probability P(F, E) can be decomposed as
</bodyText>
<equation confidence="0.973208666666667">
P(F, E) ≈ max �(�, �, �) (6)
�
P(F,A,E) = ∏(f,e)∈APw(f,e)
∝ ∑(f,e)∈A log Pw(f, e)
∝ ∑(f,e)∈A∑C∈C(G(f,e))WCO(C)
= ∑(f,e)∈A W ∙ o(f, e)
</equation>
<bodyText confidence="0.999719857142857">
which is essentially proportional to a weighted lin-
ear combination of a set of features.
To instantiate an MRF model, one needs to de-
fine a graph structure representing the translation
dependencies between source and target phrases,
and a set of potential functions over the cliques of
this graph.
</bodyText>
<subsectionHeader confidence="0.999543">
2.2 Cliques and Potential Functions
</subsectionHeader>
<bodyText confidence="0.999952219512195">
The MRF model studied in this paper is construct-
ed from the graph G in Figure 1. It contains two
types of nodes, including two phrase nodes for the
source and target phrases respectively and word
nodes, each for a word in these phrases. The
cliques and their corresponding potential functions
(or features) attempt to abstract the idea behind
those translation models that have been proved ef-
fective for machine translation in previous work. In
this study we focus on three types of cliques.
First, we consider cliques that contain two
phrase nodes. A potential function over such a
clique captures phrase-to-phrase translation de-
pendencies similar to the use the bi-directional
translation models in phrase-based SMT systems.
The potential is defined as Tp(f, e) = wpq5p(f, e),
where the feature q5p(f, e), called the phrase-pair
feature, is an indicator function whose value is 1 if
e is target phrase and f is source phrase, and 0 oth-
erwise. While the conditional probabilities in a di-
rectional translation model are estimated using rel-
ative frequencies of phrase pairs extracted from
word-aligned parallel sentences, the parameter of
the phrase-pair function wp is learned discrimina-
tively, as we will describe in Section 3.
Second, we consider cliques that contain two
word nodes, one in source phrase and the other in
target phrase. A potential over such a clique cap-
tures word-to-word translation dependencies simi-
lar to the use the IBM Model 1 for lexical
weighting in phrase-based SMT systems (Koehn et
al. 2003). The potential function is defined as
Tt(f, e) = Wtq5t(f, e), where the feature q5t(f, e),
called the word-pair feature, is an indicator func-
tion whose value is 1 if a is a word in target phrase
e and f is a word in source phrase f, and 0 other-
wise.
The third type of cliques contains three word
nodes. Two of them are in one language and the
third in the other language. A potential over such a
clique is intended to capture inter-word dependen-
</bodyText>
<page confidence="0.993467">
452
</page>
<bodyText confidence="0.999937375">
cies for selecting word translations. The potential
function is inspired by the triplet lexicon model
(Hasan et al. 2008) which is based on lexicalized
triplets (e, f, f’) . It can be understood as two
source (or target) words triggering one target (or
source) word. The potential function is defined as
Tt,(f, f&apos;, e) = wt,dit,(f, f&apos;, e), where the feature
dit,(f, f&apos;, e), called the triplet feature, is an indica-
tor function whose value is 1 if e is a word in tar-
get phrase e and f and f’ are two different words
in source phrase f, and 0 otherwise.
For any clique c that contains nodes in only one
language we assume that T(c) = 1 for all setting
of the clique, which has no impact on scoring a
phrase pair. One may wish to define a potential
over cliques containing a phrase node and word
nodes in target language, which could act as a form
of target language model. One may also add edges
in the graph so as to define potentials that capture
more sophisticated translation dependencies. The
optimal potential set could vary among different
language pairs and depend to a large degree upon
the amount and quality of training data. We leave a
comprehensive study of features to future work.
</bodyText>
<sectionHeader confidence="0.994964" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.999436636363637">
This section describes the way the parameters of
the MRF model are estimated. Although MRFs are
by nature generative models, it is not always ap-
propriate to train the parameters using convention-
al likelihood based approaches mainly for two rea-
sons. The first is due to the difficulty in computing
the partition function in Equation (4), especially in
a task of our scale. The second is due to the metric
divergence problem (Morgan et al. 2004). That is,
the maximum likelihood estimation is unlikely to
be optimal for the evaluation metric under consid-
eration, as demonstrated on a variety of tasks in-
cluding machine translation (Och 2003) and infor-
mation retrieval (Metzler and Croft 2005; Gao et
al. 2005). Therefore, we propose a large-scale dis-
criminative training approach that uses stochastic
gradient ascent and an N-best list based expected
BLEU as the objective function.
We cast machine translation as a structured
classification task (Liang et al. 2006). It maps an
input source sentence F to an output pair (E, A)
where E is the output target sentence and A the
Viterbi derivation of E. A is assumed to be con-
structed during the translation process. In phrase-
based SMT, A consists of a segmentation of the
source and target sentences into phrases and an
alignment between source and target phrases.
We also assume that translations are modeled
using a linear model parameterized by a vector 9.
Given a vector h(F, E, A) of feature functions on
(F, E, A), and assuming 9 contains a component
for each feature, the output pair (E, A) for a given
input F are selected using the argmax decision rule
</bodyText>
<equation confidence="0.9870075">
(E∗, A∗) = argmax 9Th(F,E,A) (7)
(E,A)
</equation>
<bodyText confidence="0.999293416666667">
In phrase-based SMT, computing the argmax ex-
actly is intractable, so it is performed approximate-
ly by beam decoding.
In a phrase-based SMT system equipped by a
MRF-based phrase translation model, the parame-
ters we need to learn are 9 = (I, w), where I is a
vector of a handful parameters used in the log-
linear model of Equation (2), with one weight for
each component model; and w is a vector contain-
ing millions of weights, each for one feature func-
tion in the MRF model of Equation (3). Our meth-
od takes three steps to learn 9:
</bodyText>
<listItem confidence="0.998841777777778">
1. Given a baseline phrase-based SMT system
and a pre-set I, we generate for each source
sentence in training data an N-best list of
translation hypotheses.
2. We fix I, and optimize w with respect to an
objective function on training data.
3. We fix w, and optimize I using MERT (Och
2003) to maximize the BLEU score on de-
velopment data.
</listItem>
<bodyText confidence="0.989374">
Now, we describe Steps 1 and 2 in detail.
</bodyText>
<subsectionHeader confidence="0.985778">
3.1 I-Best Generation
</subsectionHeader>
<bodyText confidence="0.9998563">
Given a set of source-target sentence pairs as train-
ing data (Fn, Enr), n = 1 ... N, we use the baseline
phrase-based SMT system to generate for each
source sentence F a list of 100-best candidate
translations, each translation E coupled with its
Viterbi derivation A, according to Equation (7).
We denote the 100-best set by GEN(F). Then, each
output pair (E, A) is labeled by a sentence-level
BLEU score, denoted by sBLEU, which is comput-
ed according to Equation (8) (He and Deng 2012),
</bodyText>
<equation confidence="0.503682">
sBLEU(E, Er) = BP × 4 Zn=11og Pn, (8)
</equation>
<page confidence="0.990484">
453
</page>
<bodyText confidence="0.9999675">
where Er is the reference translation, and pn, n =
1 ... 4, are precisions of n-grams. While precisions
of lower order n-grams, i.e., p1 and p2, are com-
puted directly without any smoothing, matching
counts for higher order n-grams could be sparse at
the sentence level and need to be smoothed as
</bodyText>
<equation confidence="0.997021">
#(matched ngram) + apn°
,for n = 3,4
#(n�r�m) + �
</equation>
<bodyText confidence="0.999982032258064">
where a is a smoothing parameter and is set to 5,
and pn° is the prior value of pn, whose value is
computed as pn° = (pn-1)2/pn-2 for n = 3 and 4.
BP in Equation (8) is the sentence-level brevity
penalty, computed as BP = exp (1 − fl rc), which
differs from its corpus-level counterpart (Papineni
et al. 2002) in two ways. First, we use a non-
clipped BP, which leads to a better approximation
to the corpus-level BLEU computation because the
per-sentence BP might effectively exceed unity in
corpus-level BLEU computation, as discussed in
Chiang et al. (2008). Second, the ratio between the
length of reference sentence r and the length of
translation hypothesis c is scaled by a factor fl such
that the total length of the references on training
data equals that of the 1-best translation hypothe-
ses produced by the baseline SMT system. In our
experiments, the value of fl is computed, on the N-
best training data, as the ratio between the total
length of the references and that of the 1-best
translation hypotheses
In our experiments we find that using sBLEU
defined above leads to a small but consistent im-
provement over other variations of sentence-level
BLEU proposed previously (e.g., Liang et al.
2006). In particular, the use of the scaling factor fl
in computing BP makes BP of the baseline’s 1-
best output close to perfect on training data, and
has an effect of forcing the discriminative training
to improve BLEU by improving n-gram precisions
rather than by improving brevity penalty.
</bodyText>
<subsectionHeader confidence="0.997514">
3.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999961571428571">
We use an N-best list based expected BLEU, a var-
iant of that in Rosti et al. (2011), as the objective
function for parameter optimization. Given the cur-
rent model 0, the expected BLEU, denoted by
xBLEU(0), over one training sample i.e., a labeled
N-best list GEN(F) generated from a pair of source
and target sentences (F, Er), is defined as
</bodyText>
<table confidence="0.8259462">
1 Initialize w, assuming 1 is fixed during training
2 For t = 1...T (T = the total number of iterations)
3 For each training sample (labeled 100-best list)
4 Compute Pe(EIF) for each translation hypothe-
sis E based on the current model 9 = (1, w)
</table>
<bodyText confidence="0.922768333333333">
5 Update the model via w = w + ri ∙ g(w),
where ri is the learning rate and g the gradient
computed according to Equations (12) and (13)
</bodyText>
<figureCaption confidence="0.9920825">
Figure 2: The algorithm of training a MRF-based
phrase translation model.
</figureCaption>
<equation confidence="0.984191">
xBLEU(0)
= GE∈GE N(F) Pe(EIF)sBLEU(E,Er), (9)
</equation>
<bodyText confidence="0.99975625">
where sBLEU is the sentence-level BLEU, defined
in Equation (8), and Pe(EIF) is a normalized trans-
lation probability from F to E computed using
softmax as
</bodyText>
<equation confidence="0.9928055">
exp(scoreo(F,E))
Po (EBF) _ —EE,exp(scoreo(F,E,)), (10)
</equation>
<bodyText confidence="0.9452855">
where Score(.) is the translation score according
to the current model 0
</bodyText>
<equation confidence="0.952231">
Scoreo(F, E) = I ∙ h(F, E, A) (11)
+ G(f,e)∈A W ∙ 4�(f, e).
</equation>
<bodyText confidence="0.999990133333333">
The right hand side of (11) contains two terms. The
first term is the score produced by the baseline sys-
tem, which is fixed during phrase model training.
The second term is the translation score produced
by the MRF model, which is updated after each
training sample during training. Comparing Equa-
tions (2) and (11), we can view the MRF model yet
another component model under the log linear
model framework with its A being set to 1.
Given the objective function, the parameters of
the MRF model are optimized using stochastic
gradient ascent. As shown in Figure 2, we go
through the training set T times, each time is con-
sidered an epoch. For each training sample, we up-
date the model parameters as
</bodyText>
<equation confidence="0.971994">
Wnew = Wold + ri ∙ g(Wold) (12)
</equation>
<bodyText confidence="0.971314">
where ri is the learning rate, and the gradient g is
computed as
</bodyText>
<equation confidence="0.995199">
g(W) = axBLEU(w) (13)
aw
pn =
</equation>
<page confidence="0.980027">
454
</page>
<bodyText confidence="0.999948375">
= E(E,A) U(9, E)P0(E|F)q5(F, E, A),
where U(9, E) = sBLEU(E, Er) − xBLEU(9).
Two considerations regarding the development
of the training method in Figure 2 are worth men-
tioning. They significantly simplify the training
procedure without sacrificing much the quality of
the trained model. First, we do not include a regu-
larization term in the objective function because
we find early stopping and cross valuation more ef-
fective and simpler to implement. In experiments
we produce a MRF model after each epoch, and
test its quality on a development set by first com-
bining the MRF model with other baseline compo-
nent models via MERT and then examining BLEU
score on the development set. We performed train-
ing for T epochs (T = 100 in our experiments) and
then pick the model with the best BLEU score on
the development set. Second, we do not use the
leave-one-out method to generate the N-best lists
(Wuebker et al. 2010). Instead, the models used in
the baseline SMT system are trained on the same
parallel data on which the N-best lists are generat-
ed. One may argue that this could lead to over-
fitting. For example, comparing to the translations
on unseen test data, the generated translation hy-
potheses on the training set are of artificially high
quality with the derivations containing artificially
long phrase pairs. The discrepancy between the
translations on training and test sets could hurt the
training performance. However, we found in our
experiments that the impact of over-fitting on the
quality of the trained MRF models is negligible1.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.956577857142857">
We conducted our experiments on two Europarl
translation tasks, German-to-English (DE-EN) and
French-to-English (FR-EN). The data sets are pub-
lished for the shared task in NAACL 2006 Work-
shop on Statistical Machine Translation (WMT06)
(Koehn and Monz 2006).
For DE-EN, the training set contains 751K sen-
tence pairs, with 21 words per sentence on average.
The official development set used for the shared
1 As pointed out by one of the reviewers, the fact that our
training works fine without leave-one-out is probably due to
the small phrase length limit (i.e., 4) we used. If a longer
phrase limit (e.g., 7) is used the result might be different. We
leave it to future work.
</bodyText>
<table confidence="0.9982404">
Systems DE-EN (TEST2) FR-EN (TEST2)
Rank-1 system 27.3 30.8
Rank-2 system 26.0 30.7
Rank-3 system 25.6 30.5
Our baseline 26.0 31.4
</table>
<tableCaption confidence="0.71668">
Table 1: Baseline results in BLEU. The results of
top ranked systems are reported in Koehn and
Monz (2006)2.
</tableCaption>
<bodyText confidence="0.998487131578947">
task contains 2000 sentences. In our experiments,
we used the first 1000 sentences as a development
set for MERT training and optimizing parameters
for discriminative training, such as learning rate
and the number of iterations. We used the rest
1000 sentences as the first test set (TEST1). We
used the WMT06 test data as the second test set
(TEST2), which contains 2000 sentences.
For FR-EN, the training set contains 688K sen-
tence pairs, with 21 words per sentence on average.
The development set contains 2000 sentences. We
used 2000 sentences from the WMT05 shared task
as TEST1, and the 2000 sentences from the
WMT06 shared task as TEST2.
Two baseline phrase-based SMT systems, each
for one language pair, are developed as follows.
These baseline systems are used in our experi-
ments both for comparison purpose and for gener-
ating N-best lists for discriminative training. First,
we performed word alignment on the training set
using a hidden Markov model with lexicalized dis-
tortion (He 2007), then extracted the phrase table
from the word aligned bilingual texts (Koehn et al.
2003). The maximum phrase length is set to four.
Other models used in a baseline system include a
lexicalized reordering model, word count and
phrase count, and a trigram language model trained
on the English training data provided by the
WMT06 shared task. A fast beam-search phrase-
based decoder (Moore and Quirk 2007) is used and
the distortion limit is set to four. The decoder is
modified so as to output the Viterbi derivation for
each translation hypothesis.
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al. 2002). We also
performed a significance test using the paired t-
test. Differences are considered statistically signif-
icant when the p-value is less than 0.05. Table 1
</bodyText>
<footnote confidence="0.883285">
2 The official results are accessible at
http://www.statmt.org/wmt06/shared-task/results.html
</footnote>
<page confidence="0.995603">
455
</page>
<table confidence="0.999805571428572">
# Systems DE-EN FR-EN
TEST1 TEST2 TEST1 TEST2
1 Baseline 26.0 26.0 31.3 31.4
2 MRFp+t+tp 27.3 α 27.1 α 32.4 α 32.2 α
3 MRFp+t 27.2 α 26.9 α 32.3 α 32.0 α
4 MRFp 26.8 αβ 26.7 αβ 32.2 α 31.8 αβ
5 MRFt 26.8 αβ 26.8 α 32.1 α 31.9 αβ
</table>
<tableCaption confidence="0.7493334">
Table 2: Main results (BLEU scores) of MRF-
based phrase translation models with different
feature classes. The superscripts α and β indicate
statistically significant difference (p &lt; 0.05)
from Baseline and MRFp+t+tp, respectively.
</tableCaption>
<table confidence="0.979528">
Feature classes # of features (weights)
DE-EN FR-EN
phrase-pair features (p) 2.5M 2.3M
word-pair features (t) 12.2M 9.7M
triplet features (tp) 13.4M 13.8M
</table>
<tableCaption confidence="0.967194">
Table 3: Statistics of the features used in build-
ing MRF-based phrase translation models.
</tableCaption>
<bodyText confidence="0.99968275">
presents the baseline results. The performance of
our phrase-based SMT systems compares favora-
bly to the top-ranked systems, thus providing a fair
baseline for our research.
</bodyText>
<subsectionHeader confidence="0.811328">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999841869565217">
Table 2 shows the main results measured in BLEU
evaluated on TEST1 and TEST2.
Row 1 is the baseline system. Rows 2 to 5 are
the systems enhanced by integrating different ver-
sions of the MRF-based phrase translation model.
These versions, labeled as MRFf, are trained using
the method described in Section 3, and differ in the
feature classes (which are specified by the sub-
script f) incorporated in the MRF-based model. In
this study we focused on three classes of features,
as described in Section 2, phrase-pair features (p),
word-pair features (t) and triplet features (tp). The
statistics for these features are given in Table 3.
Table 2 shows that all the MRF models lead to a
substantial improvement over the baseline system
across all test sets, with a statistically significant
margin from 0.8 to 1.3 BLEU points. As expected,
the best phrase model incorporates all of the three
classes of features (MRFp+t+tp in Row 2). We also
find that both MRFp and MRFt, although using
only one class of features, perform quite well. In
TEST2 of DE-EN and TEST1 of FR-EN, they are
in a near statistical tie with MRFp+t and MRFp+t+tp.
</bodyText>
<figure confidence="0.885469">
20 40 60 80 100
</figure>
<figureCaption confidence="0.981663">
Figure 3: BLEU score on development data (y
</figureCaption>
<bodyText confidence="0.953657">
axis) for DE-EN (top) and FR-EN (bottom) as a
function of the number of epochs (x axis).
The result suggests that while the MRF models are
very effective in modeling phrase translations, the
features we used in this study may not fully realize
the potential of the modeling technology.
We also measured the sensitivity of the discrim-
inative training method to different initializations
and training parameters. Results show that our
method is very robust. All the MRF models in Ta-
ble 2 are trained by setting the initial feature vector
to zero, and the learning rate 77=0.01. Figure 3 plots
the BLEU score on development sets as a function
of the number of epochs t. The BLEU score im-
proves quickly in the first 5 epochs, and then either
remains flat, as on the DE-EN data, or keeps in-
creasing but in a much slower pace, as on the FR-
EN data.
</bodyText>
<subsectionHeader confidence="0.999281">
4.2 Comparing Objective Functions
</subsectionHeader>
<bodyText confidence="0.99855025">
This section compares different objective functions
for discriminative training. As shown in Table 4,
xBLEU is compared to three widely used convex
loss functions, i.e., hinge loss, logistic loss, and log
loss. The hinge loss and logistic loss take into ac-
count only two hypotheses among an N-best list
GEN: the one with the best sentence-level BLEU
score with respect to its reference translation, de-
noted by (E*,A*) , called the oracle candidate
henceforth, and the highest scored incorrect candi-
date according to the current model, denoted by
(E&apos;, A&apos;), defined as
</bodyText>
<figure confidence="0.9669930625">
20 40 60 80 100
31.9
31.8
31.7
31.6
27.0
26.8
26.6
26.4
26.2
26.0
25.8
31.5
31.4
31.3
31.2
</figure>
<page confidence="0.996492">
456
</page>
<table confidence="0.99988575">
# Objective DE-EN FR-EN
functions
TEST TEST2 TEST1 TEST2
1
1 xBLEU 27.2 26.9 32.3 32.0
2 hinge loss 26.4α 26.2α 31.8α 31.5α
3 logistic loss 26.3α 26.2α 31.7α 31.5α
4 log loss 26.5α 26.2α 32.1 31.7α
</table>
<tableCaption confidence="0.9296632">
Table 4: BLEU scores of MRF-based phrase trans-
lation models trained using different objective
functions. The MRF models use phrase-pair and
word-pair features. The superscript α indicates
statistically significant difference (p &lt; 0.05) from
</tableCaption>
<equation confidence="0.881123">
xBLUE.
(E&apos;, A&apos;) =
argmax(E,A)EGE (!)\{(E∗,A∗)} Score-(F, E, A),
</equation>
<bodyText confidence="0.999908666666667">
where Score-(.) is defined in Equation (11). Let
x = h(F, E*, A*) − h(F, E&apos;, A&apos;) . The hinge loss
under the N-best re-ranking framework is defined
as max (0,1 − OTx). It is easy to verify that to
train a model using this version of hinge loss, the
update rule of Equation (12) can be rewritten as
</bodyText>
<equation confidence="0.995829">
[W&amp;*+, if E = E* (14)
W&amp;*+ + r7x, otherw1se
</equation>
<bodyText confidence="0.9998655">
where E is the highest scored candidate in GEN.
Following Shalev-Shwartz (2012), by setting 77 =
1, we reach the Perceptron-based training algo-
rithm that has been widely used in previous studies
of discriminative training for SMT (e.g., Liang et
al. 2006; Simianer et al. 2012).
The logistic loss log(1 + exp(−OTx)) leads to
an update rule similar to that of hinge loss
</bodyText>
<equation confidence="0.978892">
new if E = E*W =[W&amp;*+,
W&amp;*+ + r7Pe(x)x, otherw1se (15)
</equation>
<bodyText confidence="0.999923421052632">
where Pe(x) = 1/(1 + exp(OTx)).
The log loss is widely used when a probabilistic
interpretation of the trained model is desired, as in
conditional random fields (CRFs) (Lafferty et al.
2001). Given a training sample, log loss is defined
as log Pe(E*|F), where E* is the oracle translation
hypothesis with respect to its reference translation.
Pe(E*|F) is computed as Equation (10). So, unlike
hinge loss and logistic loss, log loss takes into ac-
count the distribution over all hypotheses in an N-
best list.
The results in Table 4 suggest that the objective
functions that take into account the distribution
over all hypotheses in an N-best list (i.e., xBLEU
and log loss) are more effective than the ones that
do not. xBLEU, although it is a non-concave func-
tion, significantly outperforms the others because it
is more closely coupled with the evaluation metric
under consideration (i.e., BLEU).
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999966897435897">
Among the attempts to learning phrase translation
probabilities that go beyond pure counting of
phrases on word-aligned corpora, Wuebker et al.
(2010) and He and Deng (2012) are most related to
our work. The former find phrase alignment direct-
ly on training data and update the translation prob-
abilities based on this alignment. The latter learn
phrase translation probabilities discriminatively,
which is similar to our approach. But He and
Deng’s method involves multiple stages, and is not
straightforward to implement3. Our method differs
from previous work in its use of a MRF model that
is simple and easy to understand, and a stochastic
gradient ascent based training method that is effi-
cient and easy to implement.
A large portion of previous studies on discrimi-
native training for SMT either use a handful of fea-
tures or use small training sets of a few thousand
sentences (e.g., Och 2003; Shen et al. 2004;
Watanabe et al. 2007; Duh and Kirchhoff 2008;
Chiang et al. 2008; Chiang et al. 2009). Although
there is growing interest in large-scale discrimina-
tive training (e.g., Liang et al. 2006; Tillmann and
Zhang 2006; Blunsom et al. 2008; Hopkins and
May 2011; Zhang et al. 2011), only recently does
some improvement start to be observed (e.g.,
Simianer et al. 2012; He and Deng 2012). It still
remains uncertain if the improvement is attributed
to new features, new training algorithms, objective
functions, or simply large amounts of training data.
We show empirically the importance of objective
functions. Gimple and Smith (2012) also analyze
objective functions, but more from a theoretical
viewpoint.
The proposed MRF-based translation model is
inspired by previous work of applying MRFs for
information retrieval (Metzler and Croft 2005),
query expansion (Metzler et al. 2007; Gao et al.
2012) and POS tagging (Haghighi and Klein 2006).
</bodyText>
<footnote confidence="0.916768">
3 For comparison, the method of He and Deng (2012) also
achieved very similar results to ours using the same experi-
mental setting, as described in Section 4.
</footnote>
<equation confidence="0.822933">
Wn&apos;) =
</equation>
<page confidence="0.989322">
457
</page>
<bodyText confidence="0.999930222222222">
Another undirected graphical model that has been
more widely used for NLP is a CRF (Lafferty et al.
2001). An MRF differs from a CRF in that its par-
tition function is no longer observation dependent.
As a result, learning an MRF is harder than learn-
ing a CRF using maximum likelihood estimation
(Haghighi and Klein 2006). Our work provides an
alternative learning method that is based on dis-
criminative training.
</bodyText>
<sectionHeader confidence="0.999529" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999943684210527">
The contributions of this paper are two-fold. First,
we present a general, statistical framework for
modeling phrase translations via MRFs, where dif-
ferent features can be incorporated in a unified
manner. Second, we demonstrate empirically that
the parameters of the MRF model can be learned
effectively using a large-scale discriminative train-
ing approach which is based on stochastic gradient
ascent and an N-best list based expected BLEU as
the objective function.
In future work we strive to fully realize the po-
tential of the MRF model by developing features
that can capture more sophisticated translation de-
pendencies that those used in this study. We will
also explore the use of MRF-based translation
models for translation systems that go beyond sim-
ple phrases, such as hierarchical phrase based sys-
tems (Chiang 2005) and syntax-based systems
(Galley et al. 2004).
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999204864406779">
Bishop, C. M. 2006. Patten recognition and ma-
chine learning. Springer.
Blunsom, P., Cohn, T., and Osborne, M. 2008. A
discriminative latent variable models for statisti-
cal machine translation. In ACL-HLT.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,
and Mercer, R. L. 1993. The mathematics of sta-
tistical machine translation: parameter estimation.
Computational Linguistics, 19(2): 263-311.
Chiang, D. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL,
pp. 263-270.
Chiang, D., Knight, K., and Wang, W. 2009.
11,001 new features for statistical machine trans-
lation. In NAACL-HLT.
Chiang, D., Marton, Y., and Resnik, P. 2008.
Online large-margin training of syntactic and
structural translation features. In EMNLP.
DeNero, J., Gillick, D., Zhang, J., and Klein, D.
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statisti-
cal Machine Translation, pp. 31-38.
Duh, K., and Kirchhoff, K. 2008. Beyond log-
linear models: boosted minimum error rate train-
ing for n-best ranking. In ACL.
Galley, M., Hopkins, M., Knight, K., Marcu, D.
2004. What&apos;s in a translation rule? In HLT-
NAACL, pp. 273-280.
Gao, J., Xie, S., He, X., and Ali, A. 2012. Learning
lexicon models from search logs for query ex-
pansion. In EMNLP-CoNLL, pp. 666-676.
Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear
discriminant model for information retrieval. In
SIGIR, pp. 290-297.
Gimpel, K., and Smith, N. A. 2012. Structured
ramp loss minimization for machine translation.
In NAACL-HLT.
Haghighi, A., and Klein, D. 2006. Prototype-driven
learning for sequence models. In NAACL.
Hasan, S., Ganitkevitch, J., Ney, H., and Andres-
Fnerre, J. 2008. Triplet lexicon models for statis-
tical machine translation. In EMNLP, pp. 372-
381.
He, X. 2007. Using word-dependent transition
models in HMM based word alignment for sta-
tistical machine translation. In Proc. of the Se-
cond ACL Workshop on Statistical Machine
Translation.
He, X., and Deng, L. 2012. Maximum expected
bleu training of phrase and lexicon translation
models. In ACL, pp. 292-301.
Hopkins, H., and May, J. 2011. Tuning as ranking.
In EMNLP.
Koehn, P. 2010. Statistical machine translation.
Cambridge University Press.
Koehn, P., and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation between
European languages. In Workshop on Statistical
Machine Translation, pp. 102-121.
</reference>
<page confidence="0.983003">
458
</page>
<reference confidence="0.998502194029851">
Koehn, P., Och, F., and Marcu, D. 2003. Statistical
phrase-based translation. In HLT-NAACL, pp.
127-133.
Lafferty, J., McCallum, A., and Pereira, F. 2001.
Conditional random fields: probablistic models
for segmenting and labeling sequence data. In
ICML.
Lambert, P., and Banchs, R.E. 2005. Data inferred
multi-word expressions for statistical machine
translation. In MT Summit X, Phuket, Thailand.
Liang, P., Bouchard-Cote, A. Klein, D., and
Taskar, B. 2006. An end-to-end discriminative
approach to machine translation. In COLING-
ACL.
Marcu, D., and Wong, W. 2002. A phrase-based,
joint probability model for statistical machine
translation. In EMNLP.
Metzler, D., and Croft, B. 2005. A markov random
field model for term dependencies. In SIGIR, pp.
472-479.
Metzler, D., and Croft, B. 2007. Latent concept
expansion using markov random fields. In
SIGIR, pp. 311-318.
Morgan, W., Greiff, W., and Henderson, J. 2004.
Direct maximization of average precision by
hill-climbing with a comparison to a maximum
entropy approach. Technical report. MITRE.
Moore, R., and Quirk, C. 2007. Faster beam-search
decoding for phrasal statistical machine
translation. In MT Summit XI.
Och, F., and Ney, H. 2004. The alignment template
approach to statistical machine translation. Com-
putational Linguistics, 29(1): 19-51.
Och, F. 2003. Minimum error rate training in
statistical machine translation. In ACL, pp. 160-
167.
Papinein, K., Roukos, S., Ward, T., and Zhu W-J.
2002. BLEU: a method for automatic evaluation
of machine translation. In ACL.
Rosti, A-V., Hang, B., Matsoukas, S., and
Schwartz, R. S. 2011. Expected BLEU training
for graphs: bbn system description for WMT
system combination task. In Workshop on
Statistical Machine Translation.
Shalev-Shwartz, Shai. 2012. Online learning and
online convex optimization. Foundations and
Trends in Machine Learning, 4(2):107-194.
Shen, L., Sarkar, A., and Och, F. 2004.
Discriminative reranking for machine
translation. In HLT/NAACL.
Simianer, P., Riezler, S., and Dyer, C. 2012. Joint
feature selection in distributed stochasic learning
for large-scale discriminative training in SMT. In
ACL, pp. 11-21.
Tillmann, C., and Zhang, T. 2006. A
discriminative global training algorithm for
statistical MT. In COLING-ACL.
Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki,
H. 2007. Online large-margin training for
statistical machine translation. In EMNLP.
Wuebker, J., Mauser, A., and Ney, H. 2010.
Training phrase translation models with leaving-
one-out. In ACL, pp. 475-484.
Zhang, Y., Deng, L., He, X., and Acero, A., 2011.
A Novel decision function and the associated
decision-feedback learning for speech
translation, in ICASSP.
</reference>
<page confidence="0.999118">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930717">
<title confidence="0.999331">Training MRF-Based Phrase Translation Models using Gradient Ascent</title>
<author confidence="0.955512">Jianfeng Gao Xiaodong He</author>
<affiliation confidence="0.999636">Microsoft Research Microsoft Research</affiliation>
<address confidence="0.999001">Redmond, WA, USA Redmond, WA, USA</address>
<email confidence="0.999213">jfgao@microsoft.comxiaohe@microsoft.com</email>
<abstract confidence="0.998912181818182">This paper presents a general, statistical framework for modeling phrase translation via Markov random fields. The model allows for arbituary features extracted from a phrase pair to be incorporated as evidence. The parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an N-best list based expected BLEU as the objective function. The model is easy to be incoporated into a standard phrase-based statistical machine translation system, requiring no code change in the runtime engine. Evaluation is performed on two Europarl translation tasks, German- English and French-English. Results show that incoporating the Markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system, leading to a gain of 0.8-1.3 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<title>Patten recognition and machine learning.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6461" citStr="Bishop 2006" startWordPosition="1019" endWordPosition="1020">ifferent from the directional translation models, as in Equation (1), the MRF model is undirected, which we believe upholds the spirit of the use of bi-directional translation probabilities under the log-linear framework. That is, the agreement or the compatibility of a phrase pair is more effective to score translation quality than a directional translation probability which is modeled based on an imagined generative story does. 2.1 MRF MRFs, also known as undirected graphical models, are widely used in modeling joint distributions of spatial or contextual dependencies of physical phenomena (Bishop 2006). A Markov random field is constructed from a graph G . The nodes of the graph represent random variables, and edges define the independence semantics between the random variables. An MRF satisfies the Markov property, which states that a node is independent of all of its non-neighbors, defined by the clique configurations of G. In modeling a phrase translation pair, we define two types of nodes, (1) two phrase nodes and (2) a set of word nodes, each for a word in these phrases, such as the graph in Figure 1. Let us denote a clique by c and the set of variables in that clique by (f, e)c. Then,</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Bishop, C. M. 2006. Patten recognition and machine learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="30964" citStr="Blunsom et al. 2008" startWordPosition="5274" endWordPosition="5277">ethod differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Crof</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Blunsom, P., Cohn, T., and Osborne, M. 2008. A discriminative latent variable models for statistical machine translation. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="4807" citStr="Brown et al. 1993" startWordPosition="759" endWordPosition="762">e easily incorporated into a standard phrase-based SMT system, requiring no code change in the runtime engine. In the rest of the paper, Section 2 presents the MRF model for phrase translation. Section 3 describes the way the model parameters are estimated. Section 4 presents the experimental results on two Europarl translation tasks. Section 5 reviews previous work that lays the foundation of this study. Section 6 concludes the paper. 2 Model The traditional translation models are directional models that are based on conditional probabilities. As suggested by the noisy-channel model for SMT (Brown et al. 1993): E∗ = argmax P(EJF) =argmaxP(E)P(FJE (1) E E The Bayes rule leads us to invert the conditioning of translation probability from a foreign (source) sentence F to an English (target) translation E. However, in practice, the implementation of state-of-the-art phrase-based SMT systems uses a weighted log-linear combination of several models h(F, E, A) including the logarithm of the phrase probability (and the lexical weight) in source-totarget and target-to-source directions (Och and Ney 2004) E∗ = argmaxEEm=1,Im.hm(F,E,A) (2) = argmax Score)(F,E) E where A in h(F, E, A) is a hidden structure tha</context>
</contexts>
<marker>Brown, Pietra, A, Pietra, J, Mercer, 1993</marker>
<rawString>Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer, R. L. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>263--270</pages>
<marker>Chiang, 2005</marker>
<rawString>Chiang, D. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL, pp. 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="30818" citStr="Chiang et al. 2009" startWordPosition="5251" endWordPosition="5254">riminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoret</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>Chiang, D., Knight, K., and Wang, W. 2009. 11,001 new features for statistical machine translation. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16659" citStr="Chiang et al. (2008)" startWordPosition="2829" endWordPosition="2832">smoothed as #(matched ngram) + apn° ,for n = 3,4 #(n�r�m) + � where a is a smoothing parameter and is set to 5, and pn° is the prior value of pn, whose value is computed as pn° = (pn-1)2/pn-2 for n = 3 and 4. BP in Equation (8) is the sentence-level brevity penalty, computed as BP = exp (1 − fl rc), which differs from its corpus-level counterpart (Papineni et al. 2002) in two ways. First, we use a nonclipped BP, which leads to a better approximation to the corpus-level BLEU computation because the per-sentence BP might effectively exceed unity in corpus-level BLEU computation, as discussed in Chiang et al. (2008). Second, the ratio between the length of reference sentence r and the length of translation hypothesis c is scaled by a factor fl such that the total length of the references on training data equals that of the 1-best translation hypotheses produced by the baseline SMT system. In our experiments, the value of fl is computed, on the Nbest training data, as the ratio between the total length of the references and that of the 1-best translation hypotheses In our experiments we find that using sBLEU defined above leads to a small but consistent improvement over other variations of sentence-level </context>
<context position="30797" citStr="Chiang et al. 2008" startWordPosition="5247" endWordPosition="5250">n probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, bu</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>Chiang, D., Marton, Y., and Resnik, P. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>DeNero, J., Gillick, D., Zhang, J., and Klein, D. 2006. Why generative phrase models underperform surface heuristics. In Workshop on Statistical Machine Translation, pp. 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>Beyond loglinear models: boosted minimum error rate training for n-best ranking.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30777" citStr="Duh and Kirchhoff 2008" startWordPosition="5243" endWordPosition="5246"> learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze obj</context>
</contexts>
<marker>Duh, Kirchhoff, 2008</marker>
<rawString>Duh, K., and Kirchhoff, K. 2008. Beyond loglinear models: boosted minimum error rate training for n-best ranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What&apos;s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>273--280</pages>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, M., Hopkins, M., Knight, K., Marcu, D. 2004. What&apos;s in a translation rule? In HLTNAACL, pp. 273-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>S Xie</author>
<author>X He</author>
<author>A Ali</author>
</authors>
<title>Learning lexicon models from search logs for query expansion. In EMNLP-CoNLL,</title>
<date>2012</date>
<pages>666--676</pages>
<contexts>
<context position="31627" citStr="Gao et al. 2012" startWordPosition="5376" endWordPosition="5379">y recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Wn&apos;) = 457 Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on</context>
</contexts>
<marker>Gao, Xie, He, Ali, 2012</marker>
<rawString>Gao, J., Xie, S., He, X., and Ali, A. 2012. Learning lexicon models from search logs for query expansion. In EMNLP-CoNLL, pp. 666-676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>H Qi</author>
<author>X Xia</author>
<author>J-Y Nie</author>
</authors>
<title>Linear discriminant model for information retrieval.</title>
<date>2005</date>
<booktitle>In SIGIR,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="13351" citStr="Gao et al. 2005" startWordPosition="2231" endWordPosition="2234"> are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence F to an output pair (E, A) where E is the output target sentence and A the Viterbi derivation of E. A is assumed to be constructed during the translation process. In phrasebased SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phras</context>
</contexts>
<marker>Gao, Qi, Xia, Nie, 2005</marker>
<rawString>Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear discriminant model for information retrieval. In SIGIR, pp. 290-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In NAACL-HLT.</booktitle>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Gimpel, K., and Smith, N. A. 2012. Structured ramp loss minimization for machine translation. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="31669" citStr="Haghighi and Klein 2006" startWordPosition="5383" endWordPosition="5386">tart to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Wn&apos;) = 457 Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discriminative training. 6 Conclusions Th</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Haghighi, A., and Klein, D. 2006. Prototype-driven learning for sequence models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>J Ganitkevitch</author>
<author>H Ney</author>
<author>J AndresFnerre</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>372--381</pages>
<contexts>
<context position="11561" citStr="Hasan et al. 2008" startWordPosition="1913" endWordPosition="1916">in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as Tt(f, e) = Wtq5t(f, e), where the feature q5t(f, e), called the word-pair feature, is an indicator function whose value is 1 if a is a word in target phrase e and f is a word in source phrase f, and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependen452 cies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalized triplets (e, f, f’) . It can be understood as two source (or target) words triggering one target (or source) word. The potential function is defined as Tt,(f, f&apos;, e) = wt,dit,(f, f&apos;, e), where the feature dit,(f, f&apos;, e), called the triplet feature, is an indicator function whose value is 1 if e is a word in target phrase e and f and f’ are two different words in source phrase f, and 0 otherwise. For any clique c that contains nodes in only one language we assume that T(c) = 1 for all setting of the clique, which has no impact on scoring a phrase pair. One may wis</context>
</contexts>
<marker>Hasan, Ganitkevitch, Ney, AndresFnerre, 2008</marker>
<rawString>Hasan, S., Ganitkevitch, J., Ney, H., and AndresFnerre, J. 2008. Triplet lexicon models for statistical machine translation. In EMNLP, pp. 372-381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
</authors>
<title>Using word-dependent transition models in HMM based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the Second ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="23123" citStr="He 2007" startWordPosition="3941" endWordPosition="3942">EN, the training set contains 688K sentence pairs, with 21 words per sentence on average. The development set contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as TEST1, and the 2000 sentences from the WMT06 shared task as TEST2. Two baseline phrase-based SMT systems, each for one language pair, are developed as follows. These baseline systems are used in our experiments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitiv</context>
</contexts>
<marker>He, 2007</marker>
<rawString>He, X. 2007. Using word-dependent transition models in HMM based word alignment for statistical machine translation. In Proc. of the Second ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>L Deng</author>
</authors>
<title>Maximum expected bleu training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>292--301</pages>
<contexts>
<context position="1868" citStr="He and Deng 2012" startWordPosition="275" endWordPosition="278">. The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in whi</context>
<context position="15711" citStr="He and Deng 2012" startWordPosition="2654" endWordPosition="2657">g MERT (Och 2003) to maximize the BLEU score on development data. Now, we describe Steps 1 and 2 in detail. 3.1 I-Best Generation Given a set of source-target sentence pairs as training data (Fn, Enr), n = 1 ... N, we use the baseline phrase-based SMT system to generate for each source sentence F a list of 100-best candidate translations, each translation E coupled with its Viterbi derivation A, according to Equation (7). We denote the 100-best set by GEN(F). Then, each output pair (E, A) is labeled by a sentence-level BLEU score, denoted by sBLEU, which is computed according to Equation (8) (He and Deng 2012), sBLEU(E, Er) = BP × 4 Zn=11og Pn, (8) 453 where Er is the reference translation, and pn, n = 1 ... 4, are precisions of n-grams. While precisions of lower order n-grams, i.e., p1 and p2, are computed directly without any smoothing, matching counts for higher order n-grams could be sparse at the sentence level and need to be smoothed as #(matched ngram) + apn° ,for n = 3,4 #(n�r�m) + � where a is a smoothing parameter and is set to 5, and pn° is the prior value of pn, whose value is computed as pn° = (pn-1)2/pn-2 for n = 3 and 4. BP in Equation (8) is the sentence-level brevity penalty, compu</context>
<context position="29989" citStr="He and Deng (2012)" startWordPosition="5110" endWordPosition="5113">n over all hypotheses in an Nbest list. The results in Table 4 suggest that the objective functions that take into account the distribution over all hypotheses in an N-best list (i.e., xBLEU and log loss) are more effective than the ones that do not. xBLEU, although it is a non-concave function, significantly outperforms the others because it is more closely coupled with the evaluation metric under consideration (i.e., BLEU). 5 Related Work Among the attempts to learning phrase translation probabilities that go beyond pure counting of phrases on word-aligned corpora, Wuebker et al. (2010) and He and Deng (2012) are most related to our work. The former find phrase alignment directly on training data and update the translation probabilities based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminat</context>
<context position="31721" citStr="He and Deng (2012)" startWordPosition="5393" endWordPosition="5396">ng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Wn&apos;) = 457 Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discriminative training. 6 Conclusions The contributions of this paper are two-fold. First, w</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>He, X., and Deng, L. 2012. Maximum expected bleu training of phrase and lexicon translation models. In ACL, pp. 292-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="30986" citStr="Hopkins and May 2011" startWordPosition="5278" endWordPosition="5281">evious work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansi</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Hopkins, H., and May, J. 2011. Tuning as ranking. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical machine translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3491" citStr="Koehn (2010)" startWordPosition="546" endWordPosition="547"> jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To this end, we use a simple and well-established learning method, ensuring that the results can be easily reproduced. We also develop the features for the MRF model in such a way that the </context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Koehn, P. 2010. Statistical machine translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="21465" citStr="Koehn and Monz 2006" startWordPosition="3659" endWordPosition="3662">es on the training set are of artificially high quality with the derivations containing artificially long phrase pairs. The discrepancy between the translations on training and test sets could hurt the training performance. However, we found in our experiments that the impact of over-fitting on the quality of the trained MRF models is negligible1. 4 Experiments We conducted our experiments on two Europarl translation tasks, German-to-English (DE-EN) and French-to-English (FR-EN). The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) (Koehn and Monz 2006). For DE-EN, the training set contains 751K sentence pairs, with 21 words per sentence on average. The official development set used for the shared 1 As pointed out by one of the reviewers, the fact that our training works fine without leave-one-out is probably due to the small phrase length limit (i.e., 4) we used. If a longer phrase limit (e.g., 7) is used the result might be different. We leave it to future work. Systems DE-EN (TEST2) FR-EN (TEST2) Rank-1 system 27.3 30.8 Rank-2 system 26.0 30.7 Rank-3 system 25.6 30.5 Our baseline 26.0 31.4 Table 1: Baseline results in BLEU. The results of</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Koehn, P., and Monz, C. 2006. Manual and automatic evaluation of machine translation between European languages. In Workshop on Statistical Machine Translation, pp. 102-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="10990" citStr="Koehn et al. 2003" startWordPosition="1807" endWordPosition="1810"> and f is source phrase, and 0 otherwise. While the conditional probabilities in a directional translation model are estimated using relative frequencies of phrase pairs extracted from word-aligned parallel sentences, the parameter of the phrase-pair function wp is learned discriminatively, as we will describe in Section 3. Second, we consider cliques that contain two word nodes, one in source phrase and the other in target phrase. A potential over such a clique captures word-to-word translation dependencies similar to the use the IBM Model 1 for lexical weighting in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as Tt(f, e) = Wtq5t(f, e), where the feature q5t(f, e), called the word-pair feature, is an indicator function whose value is 1 if a is a word in target phrase e and f is a word in source phrase f, and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependen452 cies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalize</context>
<context position="23214" citStr="Koehn et al. 2003" startWordPosition="3954" endWordPosition="3957">average. The development set contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as TEST1, and the 2000 sentences from the WMT06 shared task as TEST2. Two baseline phrase-based SMT systems, each for one language pair, are developed as follows. These baseline systems are used in our experiments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also performed a significance test using the paired</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT-NAACL, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: probablistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="29095" citStr="Lafferty et al. 2001" startWordPosition="4964" endWordPosition="4967">rw1se where E is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting 77 = 1, we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(−OTx)) leads to an update rule similar to that of hinge loss new if E = E*W =[W&amp;*+, W&amp;*+ + r7Pe(x)x, otherw1se (15) where Pe(x) = 1/(1 + exp(OTx)). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log Pe(E*|F), where E* is the oracle translation hypothesis with respect to its reference translation. Pe(E*|F) is computed as Equation (10). So, unlike hinge loss and logistic loss, log loss takes into account the distribution over all hypotheses in an Nbest list. The results in Table 4 suggest that the objective functions that take into account the distribution over all hypotheses in an N-best list (i.e., xBLEU and log loss) are more effective than the ones that do not. xBLEU, although it is a non-concave function, significantly outperforms t</context>
<context position="31945" citStr="Lafferty et al. 2001" startWordPosition="5433" endWordPosition="5436">e functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Wn&apos;) = 457 Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discriminative training. 6 Conclusions The contributions of this paper are two-fold. First, we present a general, statistical framework for modeling phrase translations via MRFs, where different features can be incorporated in a unified manner. Second, we demonstrate empirically that the parameters of the MRF model </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional random fields: probablistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lambert</author>
<author>R E Banchs</author>
</authors>
<title>Data inferred multi-word expressions for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<marker>Lambert, Banchs, 2005</marker>
<rawString>Lambert, P., and Banchs, R.E. 2005. Data inferred multi-word expressions for statistical machine translation. In MT Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Klein Bouchard-Cote</author>
<author>D</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLINGACL.</booktitle>
<contexts>
<context position="3176" citStr="Liang et al. (2006)" startWordPosition="490" endWordPosition="493">lation in a unified way. To this end, we propose the use of a MRF model. Second, because the phrase model has to work with other component models in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discrimina</context>
<context position="13608" citStr="Liang et al. 2006" startWordPosition="2269" endWordPosition="2272">y in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence F to an output pair (E, A) where E is the output target sentence and A the Viterbi derivation of E. A is assumed to be constructed during the translation process. In phrasebased SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. We also assume that translations are modeled using a linear model parameterized by a vector 9. Given a vector h(F, E, A) of feature functions on (F, E, A), and assuming 9 contains a component for each feature, the output pair (E, A) for a given input F </context>
<context position="17309" citStr="Liang et al. 2006" startWordPosition="2940" endWordPosition="2943">length of reference sentence r and the length of translation hypothesis c is scaled by a factor fl such that the total length of the references on training data equals that of the 1-best translation hypotheses produced by the baseline SMT system. In our experiments, the value of fl is computed, on the Nbest training data, as the ratio between the total length of the references and that of the 1-best translation hypotheses In our experiments we find that using sBLEU defined above leads to a small but consistent improvement over other variations of sentence-level BLEU proposed previously (e.g., Liang et al. 2006). In particular, the use of the scaling factor fl in computing BP makes BP of the baseline’s 1- best output close to perfect on training data, and has an effect of forcing the discriminative training to improve BLEU by improving n-gram precisions rather than by improving brevity penalty. 3.2 Parameter Estimation We use an N-best list based expected BLEU, a variant of that in Rosti et al. (2011), as the objective function for parameter optimization. Given the current model 0, the expected BLEU, denoted by xBLEU(0), over one training sample i.e., a labeled N-best list GEN(F) generated from a pai</context>
<context position="28734" citStr="Liang et al. 2006" startWordPosition="4900" endWordPosition="4903">A∗)} Score-(F, E, A), where Score-(.) is defined in Equation (11). Let x = h(F, E*, A*) − h(F, E&apos;, A&apos;) . The hinge loss under the N-best re-ranking framework is defined as max (0,1 − OTx). It is easy to verify that to train a model using this version of hinge loss, the update rule of Equation (12) can be rewritten as [W&amp;*+, if E = E* (14) W&amp;*+ + r7x, otherw1se where E is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting 77 = 1, we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(−OTx)) leads to an update rule similar to that of hinge loss new if E = E*W =[W&amp;*+, W&amp;*+ + r7Pe(x)x, otherw1se (15) where Pe(x) = 1/(1 + exp(OTx)). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log Pe(E*|F), where E* is the oracle translation hypothesis with respect to its reference translation. Pe(E*|F) is computed as Equation (10). So, unlike hinge loss and logistic loss, log lo</context>
<context position="30918" citStr="Liang et al. 2006" startWordPosition="5266" endWordPosition="5269"> is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MR</context>
</contexts>
<marker>Liang, Bouchard-Cote, D, Taskar, 2006</marker>
<rawString>Liang, P., Bouchard-Cote, A. Klein, D., and Taskar, B. 2006. An end-to-end discriminative approach to machine translation. In COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1904" citStr="Marcu and Wong 2002" startWordPosition="282" endWordPosition="285">ucting the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in which arbitrary features extracted from</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, D., and Wong, W. 2002. A phrase-based, joint probability model for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>B Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In SIGIR,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="13333" citStr="Metzler and Croft 2005" startWordPosition="2227" endWordPosition="2230">estimated. Although MRFs are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence F to an output pair (E, A) where E is the output target sentence and A the Viterbi derivation of E. A is assumed to be constructed during the translation process. In phrasebased SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between sourc</context>
<context position="31571" citStr="Metzler and Croft 2005" startWordPosition="5366" endWordPosition="5369">nsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Wn&apos;) = 457 Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work </context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Metzler, D., and Croft, B. 2005. A markov random field model for term dependencies. In SIGIR, pp. 472-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>B Croft</author>
</authors>
<title>Latent concept expansion using markov random fields.</title>
<date>2007</date>
<booktitle>In SIGIR,</booktitle>
<pages>311--318</pages>
<marker>Metzler, Croft, 2007</marker>
<rawString>Metzler, D., and Croft, B. 2007. Latent concept expansion using markov random fields. In SIGIR, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Morgan</author>
<author>W Greiff</author>
<author>J Henderson</author>
</authors>
<title>Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach.</title>
<date>2004</date>
<tech>Technical report. MITRE.</tech>
<contexts>
<context position="13087" citStr="Morgan et al. 2004" startWordPosition="2188" endWordPosition="2191">among different language pairs and depend to a large degree upon the amount and quality of training data. We leave a comprehensive study of features to future work. 3 Training This section describes the way the parameters of the MRF model are estimated. Although MRFs are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence F to an output pair (E, A) where E is the ou</context>
</contexts>
<marker>Morgan, Greiff, Henderson, 2004</marker>
<rawString>Morgan, W., Greiff, W., and Henderson, J. 2004. Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach. Technical report. MITRE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>C Quirk</author>
</authors>
<title>Faster beam-search decoding for phrasal statistical machine translation.</title>
<date>2007</date>
<booktitle>In MT Summit XI.</booktitle>
<contexts>
<context position="23528" citStr="Moore and Quirk 2007" startWordPosition="4006" endWordPosition="4009">iments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also performed a significance test using the paired ttest. Differences are considered statistically significant when the p-value is less than 0.05. Table 1 2 The official results are accessible at http://www.statmt.org/wmt06/shared-task/results.html 455 # Systems DE-EN FR-EN TEST1 TEST2 TEST1 TEST2 1 Baseline 26.0 26.0 31.3 31.4 2 MRFp+t+tp 27.3 α 27.1 α 32.4 α 3</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Moore, R., and Quirk, C. 2007. Faster beam-search decoding for phrasal statistical machine translation. In MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5302" citStr="Och and Ney 2004" startWordPosition="833" endWordPosition="836">al models that are based on conditional probabilities. As suggested by the noisy-channel model for SMT (Brown et al. 1993): E∗ = argmax P(EJF) =argmaxP(E)P(FJE (1) E E The Bayes rule leads us to invert the conditioning of translation probability from a foreign (source) sentence F to an English (target) translation E. However, in practice, the implementation of state-of-the-art phrase-based SMT systems uses a weighted log-linear combination of several models h(F, E, A) including the logarithm of the phrase probability (and the lexical weight) in source-totarget and target-to-source directions (Och and Ney 2004) E∗ = argmaxEEm=1,Im.hm(F,E,A) (2) = argmax Score)(F,E) E where A in h(F, E, A) is a hidden structure that best derives E from F, called the Viterbi derivation afterwards. In phrase-based SMT, A consists of (1) the segmentation of the source sentence into phrases, (2) the segmentation of the target sentence into phrases, and (3) an alignment between the source and target phrases. In this paper we use Markov random fields (MRFs) to model the joint distribution P�(f, e) over a source-target translation phrase pair (f, e), parameterized by w. Different from the directional translation models, as </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, F., and Ney, H. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="3307" citStr="Och 2003" startWordPosition="514" endWordPosition="515">dels in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To th</context>
<context position="13283" citStr="Och 2003" startWordPosition="2221" endWordPosition="2222"> the parameters of the MRF model are estimated. Although MRFs are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence F to an output pair (E, A) where E is the output target sentence and A the Viterbi derivation of E. A is assumed to be constructed during the translation process. In phrasebased SMT, A consists of a segmentation of the source and target sen</context>
<context position="15111" citStr="Och 2003" startWordPosition="2549" endWordPosition="2550">we need to learn are 9 = (I, w), where I is a vector of a handful parameters used in the loglinear model of Equation (2), with one weight for each component model; and w is a vector containing millions of weights, each for one feature function in the MRF model of Equation (3). Our method takes three steps to learn 9: 1. Given a baseline phrase-based SMT system and a pre-set I, we generate for each source sentence in training data an N-best list of translation hypotheses. 2. We fix I, and optimize w with respect to an objective function on training data. 3. We fix w, and optimize I using MERT (Och 2003) to maximize the BLEU score on development data. Now, we describe Steps 1 and 2 in detail. 3.1 I-Best Generation Given a set of source-target sentence pairs as training data (Fn, Enr), n = 1 ... N, we use the baseline phrase-based SMT system to generate for each source sentence F a list of 100-best candidate translations, each translation E coupled with its Viterbi derivation A, according to Equation (7). We denote the 100-best set by GEN(F). Then, each output pair (E, A) is labeled by a sentence-level BLEU score, denoted by sBLEU, which is computed according to Equation (8) (He and Deng 2012)</context>
<context position="30713" citStr="Och 2003" startWordPosition="5233" endWordPosition="5234"> probabilities based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. 2003. Minimum error rate training in statistical machine translation. In ACL, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papinein</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<marker>Papinein, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papinein, K., Roukos, S., Ward, T., and Zhu W-J. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-V Rosti</author>
<author>B Hang</author>
<author>S Matsoukas</author>
<author>R S Schwartz</author>
</authors>
<title>Expected BLEU training for graphs: bbn system description for WMT system combination task.</title>
<date>2011</date>
<booktitle>In Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="17706" citStr="Rosti et al. (2011)" startWordPosition="3009" endWordPosition="3012">1-best translation hypotheses In our experiments we find that using sBLEU defined above leads to a small but consistent improvement over other variations of sentence-level BLEU proposed previously (e.g., Liang et al. 2006). In particular, the use of the scaling factor fl in computing BP makes BP of the baseline’s 1- best output close to perfect on training data, and has an effect of forcing the discriminative training to improve BLEU by improving n-gram precisions rather than by improving brevity penalty. 3.2 Parameter Estimation We use an N-best list based expected BLEU, a variant of that in Rosti et al. (2011), as the objective function for parameter optimization. Given the current model 0, the expected BLEU, denoted by xBLEU(0), over one training sample i.e., a labeled N-best list GEN(F) generated from a pair of source and target sentences (F, Er), is defined as 1 Initialize w, assuming 1 is fixed during training 2 For t = 1...T (T = the total number of iterations) 3 For each training sample (labeled 100-best list) 4 Compute Pe(EIF) for each translation hypothesis E based on the current model 9 = (1, w) 5 Update the model via w = w + ri ∙ g(w), where ri is the learning rate and g the gradient comp</context>
</contexts>
<marker>Rosti, Hang, Matsoukas, Schwartz, 2011</marker>
<rawString>Rosti, A-V., Hang, B., Matsoukas, S., and Schwartz, R. S. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
</authors>
<title>Online learning and online convex optimization. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>4--2</pages>
<contexts>
<context position="28559" citStr="Shalev-Shwartz (2012)" startWordPosition="4872" endWordPosition="4873"> The MRF models use phrase-pair and word-pair features. The superscript α indicates statistically significant difference (p &lt; 0.05) from xBLUE. (E&apos;, A&apos;) = argmax(E,A)EGE (!)\{(E∗,A∗)} Score-(F, E, A), where Score-(.) is defined in Equation (11). Let x = h(F, E*, A*) − h(F, E&apos;, A&apos;) . The hinge loss under the N-best re-ranking framework is defined as max (0,1 − OTx). It is easy to verify that to train a model using this version of hinge loss, the update rule of Equation (12) can be rewritten as [W&amp;*+, if E = E* (14) W&amp;*+ + r7x, otherw1se where E is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting 77 = 1, we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(−OTx)) leads to an update rule similar to that of hinge loss new if E = E*W =[W&amp;*+, W&amp;*+ + r7Pe(x)x, otherw1se (15) where Pe(x) = 1/(1 + exp(OTx)). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log Pe(E*|F), </context>
</contexts>
<marker>Shalev-Shwartz, 2012</marker>
<rawString>Shalev-Shwartz, Shai. 2012. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Sarkar</author>
<author>F Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="30731" citStr="Shen et al. 2004" startWordPosition="5235" endWordPosition="5238">ties based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective funct</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Shen, L., Sarkar, A., and Och, F. 2004. Discriminative reranking for machine translation. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Simianer</author>
<author>S Riezler</author>
<author>C Dyer</author>
</authors>
<title>Joint feature selection in distributed stochasic learning for large-scale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>11--21</pages>
<contexts>
<context position="28757" citStr="Simianer et al. 2012" startWordPosition="4904" endWordPosition="4907">), where Score-(.) is defined in Equation (11). Let x = h(F, E*, A*) − h(F, E&apos;, A&apos;) . The hinge loss under the N-best re-ranking framework is defined as max (0,1 − OTx). It is easy to verify that to train a model using this version of hinge loss, the update rule of Equation (12) can be rewritten as [W&amp;*+, if E = E* (14) W&amp;*+ + r7x, otherw1se where E is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting 77 = 1, we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(−OTx)) leads to an update rule similar to that of hinge loss new if E = E*W =[W&amp;*+, W&amp;*+ + r7Pe(x)x, otherw1se (15) where Pe(x) = 1/(1 + exp(OTx)). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log Pe(E*|F), where E* is the oracle translation hypothesis with respect to its reference translation. Pe(E*|F) is computed as Equation (10). So, unlike hinge loss and logistic loss, log loss takes into account t</context>
<context position="31092" citStr="Simianer et al. 2012" startWordPosition="5296" endWordPosition="5299">ent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Simianer, P., Riezler, S., and Dyer, C. 2012. Joint feature selection in distributed stochasic learning for large-scale discriminative training in SMT. In ACL, pp. 11-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="30943" citStr="Tillmann and Zhang 2006" startWordPosition="5270" endWordPosition="5273">ward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrie</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Tillmann, C., and Zhang, T. 2006. A discriminative global training algorithm for statistical MT. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="30753" citStr="Watanabe et al. 2007" startWordPosition="5239" endWordPosition="5242"> alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki, H. 2007. Online large-margin training for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wuebker</author>
<author>A Mauser</author>
<author>H Ney</author>
</authors>
<title>Training phrase translation models with leavingone-out.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>475--484</pages>
<contexts>
<context position="1849" citStr="Wuebker et al. 2010" startWordPosition="271" endWordPosition="274">nslation (SMT) system. The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistic</context>
<context position="20563" citStr="Wuebker et al. 2010" startWordPosition="3517" endWordPosition="3520">do not include a regularization term in the objective function because we find early stopping and cross valuation more effective and simpler to implement. In experiments we produce a MRF model after each epoch, and test its quality on a development set by first combining the MRF model with other baseline component models via MERT and then examining BLEU score on the development set. We performed training for T epochs (T = 100 in our experiments) and then pick the model with the best BLEU score on the development set. Second, we do not use the leave-one-out method to generate the N-best lists (Wuebker et al. 2010). Instead, the models used in the baseline SMT system are trained on the same parallel data on which the N-best lists are generated. One may argue that this could lead to overfitting. For example, comparing to the translations on unseen test data, the generated translation hypotheses on the training set are of artificially high quality with the derivations containing artificially long phrase pairs. The discrepancy between the translations on training and test sets could hurt the training performance. However, we found in our experiments that the impact of over-fitting on the quality of the tra</context>
<context position="29966" citStr="Wuebker et al. (2010)" startWordPosition="5105" endWordPosition="5108">to account the distribution over all hypotheses in an Nbest list. The results in Table 4 suggest that the objective functions that take into account the distribution over all hypotheses in an N-best list (i.e., xBLEU and log loss) are more effective than the ones that do not. xBLEU, although it is a non-concave function, significantly outperforms the others because it is more closely coupled with the evaluation metric under consideration (i.e., BLEU). 5 Related Work Among the attempts to learning phrase translation probabilities that go beyond pure counting of phrases on word-aligned corpora, Wuebker et al. (2010) and He and Deng (2012) are most related to our work. The former find phrase alignment directly on training data and update the translation probabilities based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Wuebker, J., Mauser, A., and Ney, H. 2010. Training phrase translation models with leavingone-out. In ACL, pp. 475-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>L Deng</author>
<author>X He</author>
<author>A Acero</author>
</authors>
<title>A Novel decision function and the associated decision-feedback learning for speech translation, in ICASSP.</title>
<date>2011</date>
<contexts>
<context position="31006" citStr="Zhang et al. 2011" startWordPosition="5282" endWordPosition="5285"> of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2</context>
</contexts>
<marker>Zhang, Deng, He, Acero, 2011</marker>
<rawString>Zhang, Y., Deng, L., He, X., and Acero, A., 2011. A Novel decision function and the associated decision-feedback learning for speech translation, in ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>