<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004250">
<title confidence="0.9987695">
Combination of Statistical Word Alignments
Based on Multiple Preprocessing Schemes
</title>
<author confidence="0.997733">
Jakob Elming
</author>
<affiliation confidence="0.9890815">
Center for Comp. Modeling of Language
Copenhagen Business School
</affiliation>
<email confidence="0.989693">
je.id@cbs.dk
</email>
<sectionHeader confidence="0.993716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992132">
We present an approach to using multiple
preprocessing schemes to improve statis-
tical word alignments. We show a relative
reduction of alignment error rate of about
38%.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998354">
Word alignments over parallel corpora have be-
come an essential supporting technology to a va-
riety of natural language processing (NLP) appli-
cations most prominent among which is statisti-
cal machine translation (SMT).1 Although phrase-
based approaches to SMT tend to be robust to word-
alignment errors (Lopez and Resnik, 2006), improv-
ing word-alignment is still useful for other NLP re-
search that is more sensitive to alignment quality,
e.g., projection of information across parallel cor-
pora (Yarowsky et al., 2001).
In this paper, we present a novel approach to
using and combining multiple preprocessing (tok-
enization) schemes to improve word alignment. The
intuition here is similar to the combination of dif-
ferent preprocessing schemes for a morphologically
rich language as part of SMT (Sadat and Habash,
2006) except that the focus is on improving the
alignment quality. The language pair we work with
is Arabic-English.
In the following two sections, we present related
work and Arabic preprocessing schemes. Section 4
and 5 present our approach to alignment preprocess-
ing and combination, respectively. Results are pre-
sented in Section 6.
</bodyText>
<footnote confidence="0.999009142857143">
1The second author was supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under Contract No.
HR0011-06-C-0023. Any opinions, findings and conclusions or
recommendations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of DARPA. We
thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah,
Martin Jansche and Owen Rambow for helpful discussions.
</footnote>
<page confidence="0.982001">
25
</page>
<author confidence="0.91275">
Nizar Habash
</author>
<affiliation confidence="0.9584095">
Center for Comp. Learning Systems
Columbia University
</affiliation>
<email confidence="0.967668">
habash@cs.columbia.edu
</email>
<sectionHeader confidence="0.999909" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962">
Recently, several successful attempts have been
made at using supervised machine learning for word
alignment (Liu et al., 2005; Taskar et al., 2005; Itty-
cheriah and Roukos, 2005; Fraser and Marcu, 2006).
In contrast to generative models, this framework is
easier to extend with new features. With the ex-
ception of Fraser and Marcu (2006), these previous
publications do not entirely discard the generative
models in that they integrate IBM model predictions
as features. We extend on this approach by includ-
ing alignment information based on multiple prepro-
cessing schemes in the alignment process.
In other related work, Tillmann et al. (1997) use
several preprocessing strategies on both source and
target language to make them more alike with re-
gards to sentence length and word order. Lee (2004)
only changes the word segmentation of the morpho-
logically complex language (Arabic) to induce mor-
phological and syntactic symmetry between the par-
allel sentences. We differ from these two in that we
do not decide on a certain scheme to make source
and target sentences more symmetrical. Instead, it
is left to the alignment algorithm to decide under
which circumstances alignment information based
on a specific scheme is more likely to be correct than
information based on other schemes.
</bodyText>
<sectionHeader confidence="0.970426" genericHeader="method">
3 Arabic Preprocessing Schemes
</sectionHeader>
<bodyText confidence="0.9990777">
Arabic is a morphologically complex language
with a large set of morphological features. As
such, the set of possible preprocessing schemes
is rather large (Habash and Sadat, 2006). We
focus here on a subset of schemes pertaining to
Arabic attachable clitics. There are three de-
grees of cliticization that apply to a word BASE:
([CONJ+ [PART+ [Al+ BASE +PRON]]]).
At the deepest level, the BASE can have a def-
inite article + (Al+ the)2 or a member of the
</bodyText>
<footnote confidence="0.997302">
2Arabic is transliterated in Buckwalter’s transliteration
scheme.
</footnote>
<note confidence="0.8041255">
Proceedings of NAACL HLT 2007, Companion Volume, pages 25–28,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<tableCaption confidence="0.973176">
Table 1: Arabic preprocessing scheme variants for
’and he will write it’
</tableCaption>
<table confidence="0.997689833333333">
Preprocessing Scheme Example
AR simple wsyktbhA
D1 split CONJ + w+ syktbhA
D2 split CONJ, PART + + w+ s+ yktbhA
TB Arabic Treebank + + w+ syktb +hA
D3 split all clitics + + + w+ s+ yktb +hA
</table>
<bodyText confidence="0.995837833333333">
class of pronominal clitics, +PRON, (e.g., &amp;quot;+
+hA her/it/its). Next comes the class of particles
(PART+), (e.g., + s+ will [future]). Most shallow
is the class of conjunctions (CONJ+), (e.g., + w+
and). We use the following five schemes: AR,
D1, D2, D3 and TB. Definitions and contrastive
examples of these schemes are presented in Ta-
ble 1. To create these schemes, we use MADA,
an off-the-shelf resource for Arabic morphological
disambiguation (Habash and Rambow, 2005), and
TOKAN, a general Arabic tokenizer (Habash and
Sadat, 2006).
</bodyText>
<sectionHeader confidence="0.91244" genericHeader="method">
4 Preprocessing Schemes for Alignment
</sectionHeader>
<bodyText confidence="0.9998613125">
Using a preprocessing scheme for word alignment
breaks the process of applying Giza++ (Och and
Ney, 2003) on some parallel text into three steps:
preprocessing, alignment and remapping. In prepro-
cessing, the words are tokenized into smaller units.
Then, they are passed along to Giza++ for alignment
(default settings). Finally, the Giza++ alignments
are mapped back (remapped) to the original word
form which is AR tokens in this work. For instance,
take the first word in Table 1, wsyktbhA; if the D3
preprocesssing scheme is applied to it before align-
ment, it is turned into four tokens (w+ s+ yktb +hA).
Giza++ will link these tokens to different words on
the English side. In the remapping step, the union
of these links is assigned to the original word wsyk-
tbhA. We refer to such alignments as remappings.
</bodyText>
<sectionHeader confidence="0.982294" genericHeader="method">
5 Alignment Combination
</sectionHeader>
<bodyText confidence="0.999898">
After creating the multiple remappings, we pass
them as features into an alignment combiner. The
combiner is also given a variety of additional fea-
tures, which we discuss later in this section. The
combiner is simply a binary classifier that deter-
mines for each source-target pair whether they are
linked or not. Given the large size of the data used,
we use a simplifying heuristic that allows us to mini-
mize the number of source-target pairs used in train-
ing. Only links evidenced by at least one of the ini-
tial alignments and their immediate neighbors are in-
cluded. All other links are considered non-existent.
The combiner we use here is implemented using a
rule-based classifier, Ripper (Cohen, 1996). The
reasons we use Ripper as opposed other machine
learning approaches are: (a) Ripper produces human
readable rules that allow better understanding of the
kind of decisions being made; and (b) Ripper is rel-
atively fast compared to other machine learning ap-
proaches we examined given the very large nature of
the training data we use. The combiner is trained us-
ing supervised data (human annotated alignments),
which we discuss in Section 6.1.
In the rest of this section we describe the differ-
ent machine learning features given to the combiner.
We break the combination features in two types:
word/sentence level and remapping features.
</bodyText>
<listItem confidence="0.893329454545454">
Word/Sentence Features:
• Word Form: The source and target word forms.
• POS: The source and target part-of-speech tags.
• Location: The source and target relative sentence
position (the ratio of absolute position to sentence
length). We also use the difference between these
values for both source and target.
• Frequency: The source and target word frequency
computed as the number of occurrences of the word
form in training data. We also use the ratio of source
to target frequency.
</listItem>
<bodyText confidence="0.855409428571429">
Similarity: This feature is motivated by the fact that
proper nouns in different languages often resemble
each other, e.g. ’SdAm Hsyn’ and ’sad-
dam hussein’. We use the equivalence classes pro-
posed by Freeman et al. (2006) to normalize Ara-
bic and English word forms. Then, we employ the
longest common substring as a similarity measure.
</bodyText>
<listItem confidence="0.845159666666667">
Remapping Features:
• Link: for each source-target link, we include (a) a
binary value indicating whether the link exists ac-
</listItem>
<bodyText confidence="0.818894285714286">
cording to each remapping; (b) a cumulative sum
of the different remappings supporting this link; and
(c) co-occurrence information for this link. This last
value is calculated for each source-target word pair
as a weighted average of the product of the rela-
tive frequency of co-occurrence in both directions
for each remapping. The weight assigned to each
</bodyText>
<page confidence="0.977598">
26
</page>
<bodyText confidence="0.911973">
remapping is computed empirically.3
</bodyText>
<listItem confidence="0.999786666666667">
• Neighbor: The same information as Link, but for
each of the immediate neighbors of the current link.
• Cross: These include (a) the number of source
</listItem>
<bodyText confidence="0.772092666666667">
words linked to the current target word, the same for
target to source, and the number of words linked to
either of the current words; and (b) the ratio of the
co-occurrence mass placed in this link to the total
mass assigned to the source word, the same for the
target word and the union of both.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999922">
6.1 Experimental Data and Metrics
</subsectionHeader>
<bodyText confidence="0.999908333333333">
The gold standard alignments we use here are part of
the IBM Arabic-English aligned corpus (IBMAC)4
(Ittycheriah and Roukos, 2005). We only use 8.8K
sentences from IBMAC because the rest (smaller
portion) of the corpus uses different normalizations
for numerals that make the two sets incompatible.
We break this data into 6.6K sentences for training
and 2.2K sentences for development. As for test
data, we use the IBMAC’s test set: NIST MTEval
2003 (663 Arabic sentences each human aligned to
four English references).
To get initial Giza++ alignments, we use a larger
parallel corpus together with the annotated set. The
Arabic-English parallel corpus has about 5 million
words.5 The Arabic text in IBMAC is preprocessed
in the AR preprocessing scheme with some ad-
ditional character normalizations. We match the
preprocessing and normalizations on our additional
data to that of IBMAC’s Arabic and English prepro-
cessing (Ittycheriah and Roukos, 2005).
The standard evaluation metric within word align-
ment is the Alignment Error Rate (AER) (Och and
Ney, 2000), which requires gold alignments that are
marked as ’sure’ or ’probable’. Since the IBMAC
gold alignments we use are not marked as such,
AER reduces to 1 - F-score (Ittycheriah and Roukos,
2005):
</bodyText>
<equation confidence="0.9951205">
Pr = |A∩S |Rc = |A∩S |AER = 1 − 2PrRc
|A ||S |Pr+Rc
</equation>
<bodyText confidence="0.982299">
where A links are proposed and S links are gold.
</bodyText>
<footnote confidence="0.995283875">
3We use the AER on the development data normalized so all
weights sum to one. See Section 6.2.
4We thank IBM for making their hand aligned data available
to the research community.
5All of the training data we use is available from the Lin-
guistic Data Consortium (LDC). The parallel text includes Ara-
bic News, eTIRR, English translation of Arabic Treebank, and
Ummah.
</footnote>
<note confidence="0.864943">
NULL links are not included in the evaluation
(Ayan, 2005; Ittycheriah and Roukos, 2005).
</note>
<subsectionHeader confidence="0.745482">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.981978066666667">
We conducted three experiments on our develop-
ment data: (a) to assess the contribution of align-
ment remapping, (b) to assess the contribution of
combination features for a single alignment (i.e., in-
dependent of the combination task) and (c) to deter-
mine the best performing combination of alignment
remappings. Experiments (b) and (c) used only 2.2K
of the gold alignment training data to minimize com-
putation time. As for our test data experiment, we
use our best system with all of the available data. We
also present an error analysis of our best system. The
baseline we measure against in all of these exper-
iments is the state-of-the-art grow-diag-final (gdf)
alignment refinement heuristic commonly used in
phrase-based SMT (Koehn et al., 2003). This heuris-
tic adds links to the intersection of two asymmetrical
statistical alignments in an attempt to assign every
word a link. The AER of this baseline is 24.77%.
The Contribution of Alignment Remapping We
experimented with five alignment remappings in two
directions: dir (Ar-En) and inv (En-Ar). We also
constructed their corresponding gdf alignment. The
more verbose a preprocessing scheme, the lower the
AER for either direction and for gdf of the corre-
sponding remapping. The order of the schemes from
worst to best is AR, D1, D2, TB and D3. The
best result we obtained through remapping is that of
D3gdf which had a 20.45% AER (17.4% relative de-
crease from the baseline).
The Contribution of Combination Features For
each of the basic ten (non gdf) alignment remap-
pings, we trained a version of the combiner that uses
all the relevant features but has access to one align-
ment at a time. We saw a substantial improvement
for all alignment remappings averaging 29.9% rel-
ative decrease in AER against the basic remapped
version. The range of AER values is from 14.5%
(D3dir) to 20.79% (ARinv).
Alignment Combination Experiments To deter-
mine the best subset of alignment remappings to
combine, we ordered the alignments given their
AER performance in the last experiment described
(using combination features). Starting with the best
performer (D3dir), we continued adding alignments
in the order of their performance so long the com-
</bodyText>
<page confidence="0.999349">
27
</page>
<tableCaption confidence="0.998506">
Table 2: Combining the Alignment Remappings
</tableCaption>
<table confidence="0.954912333333333">
Alignment Remapping combination AER
D3dir 14.50
D3dirD2dir 14.12
D3dirD2dirD3inv 12.81
D3dirD2dirD3invD1dir 12.75
D3dirD2dirD3invD1dirARinv 12.69
</table>
<bodyText confidence="0.999613891891892">
bination’s AER score is decreased. Our best com-
bination results are listed in Table 2. All additional
alignments not listed in this table caused an increase
in AER. The best alignment combination used align-
ments from four different schemes which confirms
our intuition that such combination is useful.
Test Set Evaluation We ran our best system
trained on all of the IBMAC data (training &amp; devel-
opment), on all the unseen IBMAC test set. On this
data we achieve a substantial relative improvement
of 38.3% from an AER of 22.99 to 14.19.
Ittycheriah and Roukos (2005) used only the top
50 sentences in IBMAC test data. Our best AER re-
sult on their test set is 14.02% (baseline is 22.48%)
which is higher than their reported result (12.2%
with 20.5% baseline (unrefined GIZA++)). The two
results are not comparable because: (a) Ittycheriah
and Roukos (2005) used additional gold aligned data
that was not released and (b) they use an additional
500K sentences from the LDC UN corpus for Giza
training that was created by adapting to the source
side of the test set – the details of such adaptation
were not provided and thus it is not clear how to
replicate them to compare fairly. Clearly this ad-
ditional data is helpful since even their baseline is
higher than ours.6
Error Analysis We conducted error analysis on
50 sentences from our development set. The ma-
jority of the errors involved high frequency closed-
class words (54%) and complex phrases (non-
compositional or divergent translations) (23%).
Both kinds of errors could be partly addressed by
introducing phrasal constraints which are currently
lacking in our system. Orthogonally, about 18% of
all errors involved gold-standard inconsistencies and
errors. These gold errors are split equally between
closed-class and complex-phrase errors.
</bodyText>
<footnote confidence="0.934673">
6Abraham Ittycheriah, personal communication.
</footnote>
<sectionHeader confidence="0.852447" genericHeader="conclusions">
7 Conclusion and Future Plans
</sectionHeader>
<bodyText confidence="0.999976666666667">
We have presented an approach for using and com-
bining multiple alignments created using different
preprocessing schemes. We have shown a relative
reduction of AER of about 38% on a blind test set.
In the future, we plan to extend our system with ad-
ditional models at the phrase and multi-word levels
for both alignment and alignment combination im-
provement. We plan to use more sophisticated ma-
chine learning models such as support vector ma-
chines for combination and make use of more avail-
able parallel data. We also plan to evaluate the influ-
ence of our alignment improvement on MT quality.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932511627907">
N. Ayan. 2005. Combining Linguistic and Machine Learning
Techniques for Word Alignment Improvement. Ph.D. thesis,
University of Maryland, College Park.
W. Cohen. 1996. Learning trees and rules with set-valued fea-
tures. In Fourteenth Conference of the American Association
ofArtificial Intelligence. AAAI.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL-06.
A. Freeman, S. Condon, and C. Ackerman. 2006. Cross
linguistic name matching in English and Arabic. In HLT-
NAACL-06.
N. Habash and O. Rambow. 2005. Arabic Tokenization, Part-
of-Speech Tagging and Morphological Disambiguation in
One Fell Swoop. In ACL-05.
N. Habash and F. Sadat. 2006. Arabic Preprocessing Schemes
for Statistical Machine Translation. In HLT-NAACL-06.
A. Ittycheriah and S. Roukos. 2005. A maximum entropy word
aligner for arabic-english machine translation. In EMNLP-
05.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical Phrase-based
Translation. In HLT-NAACL-03.
Y. Lee. 2004. Morphological Analysis for Statistical Machine
Translation. In HLT-NAACL-04.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In ACL-05.
A. Lopez and P. Resnik. 2006. Word-based alignment, phrase-
based translation: what’s the link? In AMTA-06.
F. Och and H. Ney. 2000. Improved statistical alignment mod-
els. In ACL-2000.
F. Och and H. Ney. 2003. A Systematic Comparison of Various
Statistical Alignment Models. Computational Linguistics,
29(1):19–52.
F. Sadat and N. Habash. 2006. Combination of Arabic Pre-
processing Schemes for Statistical Machine Translation. In
ACL-06.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimina-
tive matching approach to word alignment. In EMNLP-05.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP-
based search using monotone alignments in statistical trans-
lation. In ACL-97.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection across
aligned corpora. In HLT-01.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789806">
<title confidence="0.998431">Combination of Statistical Word Based on Multiple Preprocessing Schemes</title>
<author confidence="0.995033">Jakob</author>
<affiliation confidence="0.983693">Center for Comp. Modeling of Copenhagen Business</affiliation>
<email confidence="0.966865">je.id@cbs.dk</email>
<abstract confidence="0.975006833333333">We present an approach to using multiple preprocessing schemes to improve statistical word alignments. We show a relative reduction of alignment error rate of about 38%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Ayan</author>
</authors>
<title>Combining Linguistic and Machine Learning Techniques for Word Alignment Improvement.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, College Park.</institution>
<contexts>
<context position="10644" citStr="Ayan, 2005" startWordPosition="1733" endWordPosition="1734"> are not marked as such, AER reduces to 1 - F-score (Ittycheriah and Roukos, 2005): Pr = |A∩S |Rc = |A∩S |AER = 1 − 2PrRc |A ||S |Pr+Rc where A links are proposed and S links are gold. 3We use the AER on the development data normalized so all weights sum to one. See Section 6.2. 4We thank IBM for making their hand aligned data available to the research community. 5All of the training data we use is available from the Linguistic Data Consortium (LDC). The parallel text includes Arabic News, eTIRR, English translation of Arabic Treebank, and Ummah. NULL links are not included in the evaluation (Ayan, 2005; Ittycheriah and Roukos, 2005). 6.2 Results We conducted three experiments on our development data: (a) to assess the contribution of alignment remapping, (b) to assess the contribution of combination features for a single alignment (i.e., independent of the combination task) and (c) to determine the best performing combination of alignment remappings. Experiments (b) and (c) used only 2.2K of the gold alignment training data to minimize computation time. As for our test data experiment, we use our best system with all of the available data. We also present an error analysis of our best syste</context>
</contexts>
<marker>Ayan, 2005</marker>
<rawString>N. Ayan. 2005. Combining Linguistic and Machine Learning Techniques for Word Alignment Improvement. Ph.D. thesis, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning trees and rules with set-valued features.</title>
<date>1996</date>
<booktitle>In Fourteenth Conference of the American Association ofArtificial Intelligence.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="6381" citStr="Cohen, 1996" startWordPosition="1023" endWordPosition="1024">The combiner is also given a variety of additional features, which we discuss later in this section. The combiner is simply a binary classifier that determines for each source-target pair whether they are linked or not. Given the large size of the data used, we use a simplifying heuristic that allows us to minimize the number of source-target pairs used in training. Only links evidenced by at least one of the initial alignments and their immediate neighbors are included. All other links are considered non-existent. The combiner we use here is implemented using a rule-based classifier, Ripper (Cohen, 1996). The reasons we use Ripper as opposed other machine learning approaches are: (a) Ripper produces human readable rules that allow better understanding of the kind of decisions being made; and (b) Ripper is relatively fast compared to other machine learning approaches we examined given the very large nature of the training data we use. The combiner is trained using supervised data (human annotated alignments), which we discuss in Section 6.1. In the rest of this section we describe the different machine learning features given to the combiner. We break the combination features in two types: wor</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>W. Cohen. 1996. Learning trees and rules with set-valued features. In Fourteenth Conference of the American Association ofArtificial Intelligence. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In ACL-06.</booktitle>
<contexts>
<context position="2226" citStr="Fraser and Marcu, 2006" startWordPosition="336" endWordPosition="339">under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. We thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah, Martin Jansche and Owen Rambow for helpful discussions. 25 Nizar Habash Center for Comp. Learning Systems Columbia University habash@cs.columbia.edu 2 Related Work Recently, several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make them more alike with regards to sentence length and word order. Lee (2004) on</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>A. Fraser and D. Marcu. 2006. Semi-supervised training for statistical word alignment. In ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Freeman</author>
<author>S Condon</author>
<author>C Ackerman</author>
</authors>
<title>Cross linguistic name matching in English and Arabic.</title>
<date>2006</date>
<booktitle>In HLTNAACL-06.</booktitle>
<contexts>
<context position="7730" citStr="Freeman et al. (2006)" startWordPosition="1242" endWordPosition="1245">e and target part-of-speech tags. • Location: The source and target relative sentence position (the ratio of absolute position to sentence length). We also use the difference between these values for both source and target. • Frequency: The source and target word frequency computed as the number of occurrences of the word form in training data. We also use the ratio of source to target frequency. Similarity: This feature is motivated by the fact that proper nouns in different languages often resemble each other, e.g. ’SdAm Hsyn’ and ’saddam hussein’. We use the equivalence classes proposed by Freeman et al. (2006) to normalize Arabic and English word forms. Then, we employ the longest common substring as a similarity measure. Remapping Features: • Link: for each source-target link, we include (a) a binary value indicating whether the link exists according to each remapping; (b) a cumulative sum of the different remappings supporting this link; and (c) co-occurrence information for this link. This last value is calculated for each source-target word pair as a weighted average of the product of the relative frequency of co-occurrence in both directions for each remapping. The weight assigned to each 26 r</context>
</contexts>
<marker>Freeman, Condon, Ackerman, 2006</marker>
<rawString>A. Freeman, S. Condon, and C. Ackerman. 2006. Cross linguistic name matching in English and Arabic. In HLTNAACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Tokenization, Partof-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In ACL-05.</booktitle>
<contexts>
<context position="4736" citStr="Habash and Rambow, 2005" startWordPosition="747" endWordPosition="750">le wsyktbhA D1 split CONJ + w+ syktbhA D2 split CONJ, PART + + w+ s+ yktbhA TB Arabic Treebank + + w+ syktb +hA D3 split all clitics + + + w+ s+ yktb +hA class of pronominal clitics, +PRON, (e.g., &amp;quot;+ +hA her/it/its). Next comes the class of particles (PART+), (e.g., + s+ will [future]). Most shallow is the class of conjunctions (CONJ+), (e.g., + w+ and). We use the following five schemes: AR, D1, D2, D3 and TB. Definitions and contrastive examples of these schemes are presented in Table 1. To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). 4 Preprocessing Schemes for Alignment Using a preprocessing scheme for word alignment breaks the process of applying Giza++ (Och and Ney, 2003) on some parallel text into three steps: preprocessing, alignment and remapping. In preprocessing, the words are tokenized into smaller units. Then, they are passed along to Giza++ for alignment (default settings). Finally, the Giza++ alignments are mapped back (remapped) to the original word form which is AR tokens in this work. For instance, take the first word in Table 1, wsyktbhA; if </context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>N. Habash and O. Rambow. 2005. Arabic Tokenization, Partof-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL-06.</booktitle>
<contexts>
<context position="3524" citStr="Habash and Sadat, 2006" startWordPosition="543" endWordPosition="546">rabic) to induce morphological and syntactic symmetry between the parallel sentences. We differ from these two in that we do not decide on a certain scheme to make source and target sentences more symmetrical. Instead, it is left to the alignment algorithm to decide under which circumstances alignment information based on a specific scheme is more likely to be correct than information based on other schemes. 3 Arabic Preprocessing Schemes Arabic is a morphologically complex language with a large set of morphological features. As such, the set of possible preprocessing schemes is rather large (Habash and Sadat, 2006). We focus here on a subset of schemes pertaining to Arabic attachable clitics. There are three degrees of cliticization that apply to a word BASE: ([CONJ+ [PART+ [Al+ BASE +PRON]]]). At the deepest level, the BASE can have a definite article + (Al+ the)2 or a member of the 2Arabic is transliterated in Buckwalter’s transliteration scheme. Proceedings of NAACL HLT 2007, Companion Volume, pages 25–28, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Table 1: Arabic preprocessing scheme variants for ’and he will write it’ Preprocessing Scheme Example AR simple wsyktbhA </context>
<context position="4800" citStr="Habash and Sadat, 2006" startWordPosition="757" endWordPosition="760"> s+ yktbhA TB Arabic Treebank + + w+ syktb +hA D3 split all clitics + + + w+ s+ yktb +hA class of pronominal clitics, +PRON, (e.g., &amp;quot;+ +hA her/it/its). Next comes the class of particles (PART+), (e.g., + s+ will [future]). Most shallow is the class of conjunctions (CONJ+), (e.g., + w+ and). We use the following five schemes: AR, D1, D2, D3 and TB. Definitions and contrastive examples of these schemes are presented in Table 1. To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). 4 Preprocessing Schemes for Alignment Using a preprocessing scheme for word alignment breaks the process of applying Giza++ (Och and Ney, 2003) on some parallel text into three steps: preprocessing, alignment and remapping. In preprocessing, the words are tokenized into smaller units. Then, they are passed along to Giza++ for alignment (default settings). Finally, the Giza++ alignments are mapped back (remapped) to the original word form which is AR tokens in this work. For instance, take the first word in Table 1, wsyktbhA; if the D3 preprocesssing scheme is applied to it before alignment, </context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic Preprocessing Schemes for Statistical Machine Translation. In HLT-NAACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In EMNLP05.</booktitle>
<contexts>
<context position="2201" citStr="Ittycheriah and Roukos, 2005" startWordPosition="331" endWordPosition="335">earch Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. We thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah, Martin Jansche and Owen Rambow for helpful discussions. 25 Nizar Habash Center for Comp. Learning Systems Columbia University habash@cs.columbia.edu 2 Related Work Recently, several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make them more alike with regards to sentence length and </context>
<context position="8991" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1454" endWordPosition="1457">.3 • Neighbor: The same information as Link, but for each of the immediate neighbors of the current link. • Cross: These include (a) the number of source words linked to the current target word, the same for target to source, and the number of words linked to either of the current words; and (b) the ratio of the co-occurrence mass placed in this link to the total mass assigned to the source word, the same for the target word and the union of both. 6 Evaluation 6.1 Experimental Data and Metrics The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC)4 (Ittycheriah and Roukos, 2005). We only use 8.8K sentences from IBMAC because the rest (smaller portion) of the corpus uses different normalizations for numerals that make the two sets incompatible. We break this data into 6.6K sentences for training and 2.2K sentences for development. As for test data, we use the IBMAC’s test set: NIST MTEval 2003 (663 Arabic sentences each human aligned to four English references). To get initial Giza++ alignments, we use a larger parallel corpus together with the annotated set. The Arabic-English parallel corpus has about 5 million words.5 The Arabic text in IBMAC is preprocessed in the</context>
<context position="10675" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1735" endWordPosition="1738">ked as such, AER reduces to 1 - F-score (Ittycheriah and Roukos, 2005): Pr = |A∩S |Rc = |A∩S |AER = 1 − 2PrRc |A ||S |Pr+Rc where A links are proposed and S links are gold. 3We use the AER on the development data normalized so all weights sum to one. See Section 6.2. 4We thank IBM for making their hand aligned data available to the research community. 5All of the training data we use is available from the Linguistic Data Consortium (LDC). The parallel text includes Arabic News, eTIRR, English translation of Arabic Treebank, and Ummah. NULL links are not included in the evaluation (Ayan, 2005; Ittycheriah and Roukos, 2005). 6.2 Results We conducted three experiments on our development data: (a) to assess the contribution of alignment remapping, (b) to assess the contribution of combination features for a single alignment (i.e., independent of the combination task) and (c) to determine the best performing combination of alignment remappings. Experiments (b) and (c) used only 2.2K of the gold alignment training data to minimize computation time. As for our test data experiment, we use our best system with all of the available data. We also present an error analysis of our best system. The baseline we measure agai</context>
<context position="13639" citStr="Ittycheriah and Roukos (2005)" startWordPosition="2213" endWordPosition="2216">.81 D3dirD2dirD3invD1dir 12.75 D3dirD2dirD3invD1dirARinv 12.69 bination’s AER score is decreased. Our best combination results are listed in Table 2. All additional alignments not listed in this table caused an increase in AER. The best alignment combination used alignments from four different schemes which confirms our intuition that such combination is useful. Test Set Evaluation We ran our best system trained on all of the IBMAC data (training &amp; development), on all the unseen IBMAC test set. On this data we achieve a substantial relative improvement of 38.3% from an AER of 22.99 to 14.19. Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data. Our best AER result on their test set is 14.02% (baseline is 22.48%) which is higher than their reported result (12.2% with 20.5% baseline (unrefined GIZA++)). The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set – the details of such adaptation were not provided and thus it is not clear how to replicate them to co</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>A. Ittycheriah and S. Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In EMNLP05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL-03.</booktitle>
<contexts>
<context position="11438" citStr="Koehn et al., 2003" startWordPosition="1859" endWordPosition="1862">ontribution of combination features for a single alignment (i.e., independent of the combination task) and (c) to determine the best performing combination of alignment remappings. Experiments (b) and (c) used only 2.2K of the gold alignment training data to minimize computation time. As for our test data experiment, we use our best system with all of the available data. We also present an error analysis of our best system. The baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final (gdf) alignment refinement heuristic commonly used in phrase-based SMT (Koehn et al., 2003). This heuristic adds links to the intersection of two asymmetrical statistical alignments in an attempt to assign every word a link. The AER of this baseline is 24.77%. The Contribution of Alignment Remapping We experimented with five alignment remappings in two directions: dir (Ar-En) and inv (En-Ar). We also constructed their corresponding gdf alignment. The more verbose a preprocessing scheme, the lower the AER for either direction and for gdf of the corresponding remapping. The order of the schemes from worst to best is AR, D1, D2, TB and D3. The best result we obtained through remapping </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical Phrase-based Translation. In HLT-NAACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
</authors>
<title>Morphological Analysis for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL-04.</booktitle>
<contexts>
<context position="2823" citStr="Lee (2004)" startWordPosition="434" endWordPosition="435"> Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make them more alike with regards to sentence length and word order. Lee (2004) only changes the word segmentation of the morphologically complex language (Arabic) to induce morphological and syntactic symmetry between the parallel sentences. We differ from these two in that we do not decide on a certain scheme to make source and target sentences more symmetrical. Instead, it is left to the alignment algorithm to decide under which circumstances alignment information based on a specific scheme is more likely to be correct than information based on other schemes. 3 Arabic Preprocessing Schemes Arabic is a morphologically complex language with a large set of morphological </context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Y. Lee. 2004. Morphological Analysis for Statistical Machine Translation. In HLT-NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In ACL-05.</booktitle>
<contexts>
<context position="2150" citStr="Liu et al., 2005" startWordPosition="323" endWordPosition="326">s supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. We thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah, Martin Jansche and Owen Rambow for helpful discussions. 25 Nizar Habash Center for Comp. Learning Systems Columbia University habash@cs.columbia.edu 2 Related Work Recently, several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make t</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word alignment. In ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
<author>P Resnik</author>
</authors>
<title>Word-based alignment, phrasebased translation: what’s the link?</title>
<date>2006</date>
<booktitle>In AMTA-06.</booktitle>
<contexts>
<context position="691" citStr="Lopez and Resnik, 2006" startWordPosition="99" endWordPosition="102">essing Schemes Jakob Elming Center for Comp. Modeling of Language Copenhagen Business School je.id@cbs.dk Abstract We present an approach to using multiple preprocessing schemes to improve statistical word alignments. We show a relative reduction of alignment error rate of about 38%. 1 Introduction Word alignments over parallel corpora have become an essential supporting technology to a variety of natural language processing (NLP) applications most prominent among which is statistical machine translation (SMT).1 Although phrasebased approaches to SMT tend to be robust to wordalignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al., 2001). In this paper, we present a novel approach to using and combining multiple preprocessing (tokenization) schemes to improve word alignment. The intuition here is similar to the combination of different preprocessing schemes for a morphologically rich language as part of SMT (Sadat and Habash, 2006) except that the focus is on improving the alignment quality. The language pair we work with is Arabic-English</context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>A. Lopez and P. Resnik. 2006. Word-based alignment, phrasebased translation: what’s the link? In AMTA-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL-2000.</booktitle>
<contexts>
<context position="9922" citStr="Och and Ney, 2000" startWordPosition="1601" endWordPosition="1604">al 2003 (663 Arabic sentences each human aligned to four English references). To get initial Giza++ alignments, we use a larger parallel corpus together with the annotated set. The Arabic-English parallel corpus has about 5 million words.5 The Arabic text in IBMAC is preprocessed in the AR preprocessing scheme with some additional character normalizations. We match the preprocessing and normalizations on our additional data to that of IBMAC’s Arabic and English preprocessing (Ittycheriah and Roukos, 2005). The standard evaluation metric within word alignment is the Alignment Error Rate (AER) (Och and Ney, 2000), which requires gold alignments that are marked as ’sure’ or ’probable’. Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 - F-score (Ittycheriah and Roukos, 2005): Pr = |A∩S |Rc = |A∩S |AER = 1 − 2PrRc |A ||S |Pr+Rc where A links are proposed and S links are gold. 3We use the AER on the development data normalized so all weights sum to one. See Section 6.2. 4We thank IBM for making their hand aligned data available to the research community. 5All of the training data we use is available from the Linguistic Data Consortium (LDC). The parallel text includes Arabic</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. Improved statistical alignment models. In ACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4945" citStr="Och and Ney, 2003" startWordPosition="779" endWordPosition="782">. Next comes the class of particles (PART+), (e.g., + s+ will [future]). Most shallow is the class of conjunctions (CONJ+), (e.g., + w+ and). We use the following five schemes: AR, D1, D2, D3 and TB. Definitions and contrastive examples of these schemes are presented in Table 1. To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). 4 Preprocessing Schemes for Alignment Using a preprocessing scheme for word alignment breaks the process of applying Giza++ (Och and Ney, 2003) on some parallel text into three steps: preprocessing, alignment and remapping. In preprocessing, the words are tokenized into smaller units. Then, they are passed along to Giza++ for alignment (default settings). Finally, the Giza++ alignments are mapped back (remapped) to the original word form which is AR tokens in this work. For instance, take the first word in Table 1, wsyktbhA; if the D3 preprocesssing scheme is applied to it before alignment, it is turned into four tokens (w+ s+ yktb +hA). Giza++ will link these tokens to different words on the English side. In the remapping step, the </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>N Habash</author>
</authors>
<title>Combination of Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In ACL-06.</booktitle>
<contexts>
<context position="1181" citStr="Sadat and Habash, 2006" startWordPosition="176" endWordPosition="179">machine translation (SMT).1 Although phrasebased approaches to SMT tend to be robust to wordalignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al., 2001). In this paper, we present a novel approach to using and combining multiple preprocessing (tokenization) schemes to improve word alignment. The intuition here is similar to the combination of different preprocessing schemes for a morphologically rich language as part of SMT (Sadat and Habash, 2006) except that the focus is on improving the alignment quality. The language pair we work with is Arabic-English. In the following two sections, we present related work and Arabic preprocessing schemes. Section 4 and 5 present our approach to alignment preprocessing and combination, respectively. Results are presented in Section 6. 1The second author was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>F. Sadat and N. Habash. 2006. Combination of Arabic Preprocessing Schemes for Statistical Machine Translation. In ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In EMNLP-05.</booktitle>
<contexts>
<context position="2171" citStr="Taskar et al., 2005" startWordPosition="327" endWordPosition="330"> Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. We thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah, Martin Jansche and Owen Rambow for helpful discussions. 25 Nizar Habash Center for Comp. Learning Systems Columbia University habash@cs.columbia.edu 2 Related Work Recently, several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make them more alike with r</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In EMNLP-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
</authors>
<title>A DPbased search using monotone alignments in statistical translation.</title>
<date>1997</date>
<booktitle>In ACL-97.</booktitle>
<contexts>
<context position="2668" citStr="Tillmann et al. (1997)" startWordPosition="406" endWordPosition="409">essful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2006). In contrast to generative models, this framework is easier to extend with new features. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process. In other related work, Tillmann et al. (1997) use several preprocessing strategies on both source and target language to make them more alike with regards to sentence length and word order. Lee (2004) only changes the word segmentation of the morphologically complex language (Arabic) to induce morphological and syntactic symmetry between the parallel sentences. We differ from these two in that we do not decide on a certain scheme to make source and target sentences more symmetrical. Instead, it is left to the alignment algorithm to decide under which circumstances alignment information based on a specific scheme is more likely to be corr</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DPbased search using monotone alignments in statistical translation. In ACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT-01.</title>
<date>2001</date>
<contexts>
<context position="881" citStr="Yarowsky et al., 2001" startWordPosition="129" endWordPosition="132">atistical word alignments. We show a relative reduction of alignment error rate of about 38%. 1 Introduction Word alignments over parallel corpora have become an essential supporting technology to a variety of natural language processing (NLP) applications most prominent among which is statistical machine translation (SMT).1 Although phrasebased approaches to SMT tend to be robust to wordalignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al., 2001). In this paper, we present a novel approach to using and combining multiple preprocessing (tokenization) schemes to improve word alignment. The intuition here is similar to the combination of different preprocessing schemes for a morphologically rich language as part of SMT (Sadat and Habash, 2006) except that the focus is on improving the alignment quality. The language pair we work with is Arabic-English. In the following two sections, we present related work and Arabic preprocessing schemes. Section 4 and 5 present our approach to alignment preprocessing and combination, respectively. Resu</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT-01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>