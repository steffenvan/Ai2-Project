<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000987">
<title confidence="0.998018">
Training Paradigms for Correcting Errors in Grammar and Usage
</title>
<author confidence="0.996073">
Alla Rozovskaya and Dan Roth
</author>
<affiliation confidence="0.998113">
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.813393">
Urbana, IL 61801
</address>
<email confidence="0.999084">
{rozovska,danr}@illinois.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935310344827">
This paper proposes a novel approach to the
problem of training classifiers to detect and
correct grammar and usage errors in text by
selectively introducing mistakes into the train-
ing data. When training a classifier, we would
like the distribution of examples seen in train-
ing to be as similar as possible to the one seen
in testing. In error correction problems, such
as correcting mistakes made by second lan-
guage learners, a system is generally trained
on correct data, since annotating data for train-
ing is expensive. Error generation methods
avoid expensive data annotation and create
training data that resemble non-native data
with errors.
We apply error generation methods and train
classifiers for detecting and correcting arti-
cle errors in essays written by non-native En-
glish speakers; we show that training on data
that contain errors produces higher accuracy
when compared to a system that is trained on
clean native data. We propose several train-
ing paradigms with error generation and show
that each such paradigm is superior to training
a classifier on native data. We also show that
the most successful error generation methods
are those that use knowledge about the arti-
cle distribution and error patterns observed in
non-native text.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970210526316">
This paper considers the problem of training clas-
sifiers to detect and correct errors in grammar and
word usage in text. Both native and non-native
speakers make a variety of errors that are not always
easy to detect. Consider, for example, the problem
of context-sensitive spelling correction (e.g., (Gold-
ing and Roth, 1996; Golding and Roth, 1999; Carl-
son et al., 2001)). Unlike spelling errors that result in
non-words and are easy to detect, context-sensitive
spelling correction task involves correcting spelling
errors that result in legitimate words, such as confus-
ing peace and piece or your and you’re. The typical
training paradigm for these context-sensitive ambi-
guities is to use text assumed to be error free, replac-
ing each target word occurrence (e.g. peace) with a
confusion set consisting of, say {peace, piece}, thus
generating both positive and negative examples, re-
spectively, from the same context.
This paper proposes a novel error generation ap-
proach to the problem of training classifiers for the
purpose of detecting and correcting grammar and
usage errors in text. Unlike previous work (e.g.,
(Sj¨obergh and Knutsson, 2005; Brockett et al., 2006;
Foster and Andersen, 2009)), we selectively intro-
duce mistakes in an appropriate proportion. In par-
ticular, to create training data that closely resemble
text with naturally occurring errors, we use error fre-
quency information and error distribution statistics
obtained from corrected non-native text. We apply
the method to the problem of detecting and correct-
ing article mistakes made by learners of English as
a Second Language (ESL).
The problem of correcting article errors is gener-
ally viewed as that of article selection, cast as a clas-
sification problem and is trained as described above:
a machine learning algorithm is used to train a clas-
sifier on native English data, where the possible se-
lections are used to generate positive and negative
</bodyText>
<page confidence="0.996377">
154
</page>
<note confidence="0.7781925">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154–162,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.993349542372882">
examples (e.g., (Izumi et al., 2003; Han et al., 2006;
De Felice and Pulman, 2008; Gamon et al., 2008)).
The classifier is then applied to non-native text to
predict the correct article in context. But the article
correction problem differs from the problem of ar-
ticle selection in that we know the original (source)
article that the writer used. When proposing a cor-
rection, we would like to use information about the
original article. One reason for this is that about 90%
of articles are used correctly by ESL learners; this is
higher than the performance of state-of-the-art clas-
sifiers for article selection. Consequently, not us-
ing the writer’s article, when making a prediction,
may result in making more mistakes than there are
in the data. Another reason is that statistics on ar-
ticle errors (e.g., (Han et al., 2006; Lee and Sen-
eff, 2008)) and in the annotation performed for the
present study reveal that non-native English speak-
ers make article mistakes in a consistent manner.
The system can consider the article used by the
writer at evaluation time, by proposing a correction
only when the confidence of the classifier is high
enough, but the article cannot be used in training
if the classifier is trained on clean native data that
do not have errors. Learning Theory says that the
distribution of examples seen in testing should be
as similar as possible to the one seen in training, so
one would like to train on errors similar to those ob-
served in testing. Ideally, we would like to train us-
ing corrected non-native text. In that case, the orig-
inal article of the writer can be used as a feature for
the classifier and the correct article, as judged by
a native English speaker, will be viewed as the la-
bel. However, obtaining annotated data for training
is expensive and, since the native training data do
not contain errors, we cannot use the writer’s article
as a feature for the classifier.
This paper compares the traditional training
paradigm that uses native data to training paradigms
that use data with artificial mistakes. We propose
several methods of generating mistakes in native
training data and demonstrate that they outperform
the traditional training paradigm. We also show that
the most successful error generation methods use
knowledge about the article distribution and error
patterns observed in the ESL data.
The rest of the paper is organized as follows.
First, we discuss the baseline on the error correc-
tion task and show why the baselines used in selec-
tion tasks are not relevant for the error correction
task. Next, we describe prior work in error genera-
tion and show the key difference of our approach.
Section 4 presents the ESL data that we use and
statistics on article errors. Section 5 describes train-
ing paradigms that employ error generation. In Sec-
tions 6 and 7 we present the results and discuss the
results. The key findings are summarized in Table 7
in Section 6. We conclude with a brief discussion of
directions for future work.
</bodyText>
<sectionHeader confidence="0.900754" genericHeader="introduction">
2 Measuring Success in Error Correction
Tasks
</sectionHeader>
<bodyText confidence="0.999945484848485">
The distinction between the selection and the error
correction tasks alluded to earlier is important not
only for training but also in determining an appro-
priate evaluation method.
The standard baseline used in selection tasks is
the relative frequency of the most common class.
For example, in word sense disambiguation, the
baseline is the most frequent sense. In the task
of article selection, the standard baseline used is
to predict the article that occurs most frequently in
the data (usually, it is the zero article, whose fre-
quency is 60-70%). In this context, the performance
of a state-of-the-art classifier (Knight and Chander,
1994; Minnen et al., 2000; Turner and Charniak,
2007; Gamon et al., 2008) whose accuracy is 85-
87% is a significant improvement over the base-
line. The majority has been used as the baseline also
in the context-sensitive spelling task (e.g., (Golding
and Roth, 1999)).
However, in article correction, spelling correc-
tion, and other text correction applications the split
of the classes is not an appropriate baseline since the
majority of the words in the confusion set are used
correctly in the text. Han et al. (2006) report an av-
erage error rate of 13% on article data from TOEFL
essays, which gives a baseline of 87%, versus the
baseline of 60-70% used in the article selection task.
Statistics on article mistakes in our data suggest a
baseline of about 90%, depending on the source lan-
guage of the writer. So the real baseline on the task
is ”do nothing”. Therefore, to determine the base-
line for a correction task, one needs to consider the
error rate in the data.
</bodyText>
<page confidence="0.998504">
155
</page>
<bodyText confidence="0.999978">
Using the definitions of precision and recall and
the “real” baseline, we can also relate the resulting
accuracy of the classifier to the precision and recall
on an error correction task as follows: Let P and R
denote the precision and recall, respectively, of the
system on an error correction task, and Base denote
the error rate in the data. Then the task baseline (i.e.,
accuracy of the data before running the system) is:
</bodyText>
<equation confidence="0.916085">
Baseline = 1 − Base
It can be shown that the error rate after running the
classifier is:
Error = Base * (P + R − 2RP)
P
It follows that the accuracy of the system on the task
is 1 − Error.
</equation>
<bodyText confidence="0.999715">
For example, we can obtain a rough estimate on
the accuracy of the system in Han et al. (2006), us-
ing precision and recall numbers by error type. Ex-
cluding the error type of category other, we can esti-
mate that Base = 0.1, so the baseline is 0.9, average
precision and recall are 0.85 and 0.25, respectively,
and the resulting overall accuracy of the system is
92.2%.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="method">
3 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999971">
3.1 Generating Errors in Text
</subsectionHeader>
<bodyText confidence="0.999992529411765">
In text correction, adding mistakes in training has
been explored before. Although the general ap-
proach has been to produce errors similar to those
observed in the data to be corrected, mistakes were
added in an ad-hoc way, without respecting the er-
ror frequencies and error patterns observed in non-
native text. Izumi et al. (2003) train a maxi-
mum entropy model on error-tagged data from the
Japanese Learners of English corpus (JLE, (Izumi et
al., 2004)) to detect 8 error types in the same cor-
pus. They show improvement when the training set
is enhanced with sentences from the same corpus
to which artificial article mistakes have been added.
Though it is not clear in what proportion mistakes
were added, it is also possible that the improvement
was due to a larger training set. Foster and Ander-
sen (2009) attempt to replicate naturally occurring
learner mistakes in the Cambridge Learner Corpus
(CLC)1, but show a drop in accuracy when the orig-
inal error-tagged data in training are replaced with
corrected CLC sentences containing artificial errors.
Brockett et al. (2006) generate mass noun er-
rors in native English data using relevant exam-
ples found in the Chinese Learners English Cor-
pus (CLEC, (Gui and Yang, 2003)). Training data
consist of an equal number of correct and incor-
rect sentences. Sj¨obergh and Knutsson (2005) in-
troduce split compound and agreement errors into
native Swedish text: agreement errors are added in
every sentence and for compound errors, the train-
ing set consists of an equal number of negative and
positive examples. Their method gives higher recall
at the expense of lower precision compared to rule-
based grammar checkers.
To sum up, although the idea of using data with ar-
tificial mistakes is not new, the advantage of training
on such data has not been investigated. Moreover,
training on error-tagged data is currently unrealistic
in the majority of error correction scenarios, which
suggests that using text with artificial mistakes is the
only alternative to using clean data. However, it has
not been shown whether training on data with artifi-
cial errors is beneficial when compared to utilizing
clean data. More importantly, error statistics have
not been considered for error correction tasks. Lee
and Seneff (2008) examine statistics on article and
preposition mistakes in the JLE corpus. While they
do not suggest a specific approach, they hypothesize
that it might be helpful to incorporate this knowl-
edge into a correction system that targets these two
language phenomena.
</bodyText>
<subsectionHeader confidence="0.999741">
3.2 Approaches to Detecting Article Mistakes
</subsectionHeader>
<bodyText confidence="0.999953090909091">
Automated methods for detecting article mistakes
generally use a machine learning algorithm. Ga-
mon et al. (2008) use a decision tree model and a
5-gram language model trained on the English Giga-
word corpus (LDC2005T12) to correct errors in En-
glish article and preposition usage. Han et al. (2006)
and De Felice and Pulman (2008) train a maximum
entropy classifier. Yi et al. (2008) propose a web
count-based system to correct determiner errors. In
the above approaches, the classifiers are trained on
native data. Therefore the classifiers cannot use the
</bodyText>
<footnote confidence="0.985799">
1http://www.cambridge.org/elt
</footnote>
<page confidence="0.997835">
156
</page>
<bodyText confidence="0.999917">
original article that the writer used as a feature. Han
et al. (2006) use the source article at evaluation time
and propose a correction only when the score of the
classifier is high enough, but the source article is not
used in training.
</bodyText>
<sectionHeader confidence="0.766321" genericHeader="method">
4 Article Errors in ESL Data
</sectionHeader>
<bodyText confidence="0.9999817">
Article errors are one of the most common mistakes
that non-native speakers make, especially those
whose native language does not have an article sys-
tem. For example, Han et al. (2006) report that in
the annotated TOEFL data by Russian, Chinese, and
Japanese speakers 13% of all noun phrases have an
incorrect article. It is interesting to note that article
errors are present even with very advanced speakers.
While the TOEFL data include essays by students of
different proficiency levels, we use data from very
advanced learners and find that error rates on articles
are similar to those reported by Han et al. (2006).
We use data from speakers of three first language
backgrounds: Chinese, Czech, and Russian. None
of these languages has an article system. The Czech
and the Russian data come from the ICLE corpus
(Granger et al., 2002), which is a collection of es-
says written by advanced learners of English. The
Chinese data is a part of the CLEC corpus that con-
tains essays by students of all levels of proficiency.
</bodyText>
<subsectionHeader confidence="0.994583">
4.1 Data Annotation
</subsectionHeader>
<bodyText confidence="0.999892727272727">
A portion of data for each source language was cor-
rected and error-tagged by native speakers. The an-
notation was performed at the sentence level: a sen-
tence was presented to the annotator in the context
of the entire essay. Essay context can become nec-
essary, when an article is acceptable in the context
of a sentence, but is incorrect in the context of the
essay. Our goal was to correct all article errors, in-
cluding those that, while acceptable in the context of
the sentence, were not correct in the context of the
essay. The annotators were also encouraged to pro-
pose more than one correction, as long as all of their
suggestions were consistent with the essay context.
The annotators were asked to correct all mistakes
in the sentence. The annotation schema included
the following error types: mistakes in article and
preposition usage, errors in noun number, spelling,
verb form, and word form2. All other corrections
were marked as word replacement, word deletion,
and word insertion. For details about annotation and
data selection, please refer to the companion paper
(Rozovskaya and Roth, 2010).
</bodyText>
<subsectionHeader confidence="0.993533">
4.2 Statistics on Article Errors
</subsectionHeader>
<bodyText confidence="0.99298225">
Traditionally, three article classes are distinguished:
the, a(an)3 and None (no article). The training and
the test data are thus composed of two types of
events:
</bodyText>
<listItem confidence="0.822932333333333">
1. All articles in the data
2. Spaces in front of a noun phrase if that noun
phrase does not start with an article. To identify
the beginning of a noun phrase, we ran a part-
of-speech tagger and a phrase chunker4 and ex-
cluded all noun phrases not headed5 by a per-
</listItem>
<bodyText confidence="0.988228727272727">
sonal or demonstrative pronoun.
Table 1 shows the size of the test data by source
language, proportion of errors and distribution of ar-
ticle classes before and after annotation and com-
pares these distributions to the distribution of articles
in English Wikipedia. The distribution before anno-
tation shows statistics on article usage by the writers
and the distribution after annotation shows statistics
after the corrections made by the annotators were
applied. As the table shows, the distribution of arti-
cles is quite different for native data (Wikipedia) and
non-native text. In particular, non-native data have a
lower proportion of the.
The annotation statistics also reveal that learn-
ers do not confuse articles randomly. From Table
2, which shows the distribution of article errors by
type, we observe that the majority of mistakes are
omissions and extraneous articles. Table 3 shows
statistics on corrections by source and label, where
source refers to the article used by the writer, and
label refers to the article chosen by the annotator.
Each entry in the table indicates Prob(source =
</bodyText>
<footnote confidence="0.992991">
2Our classification, was inspired by the classification pre-
sented in Tetreault and Chodorow (2008)
3Henceforth, we will use a to refer to both a and an
4The tagger and the chunker are available at http://
L2R.cs.uiuc.edu/˜cogcomp/software.php
5We assume that the last word of the noun phrase is its head.
</footnote>
<page confidence="0.957494">
157
</page>
<table confidence="0.9981818">
Source Number of Proportion of Errors Article Classes
language test examples errors total distribution
a the None
Before annotation 8.5 28.2 63.3
Chinese 1713 9.2% 158 After annotation 9.9 24.9 65.2
Before annotation 9.1 22.9 68.0
Czech 1061 9.6% 102 After annotation 9.9 22.3 67.8
Russian 2146 10.4% 224 Before annotation 10.5 21.7 67.9
After annotation 12.5 20.1 67.4
English Wikipedia 9.6 29.1 61.4
</table>
<tableCaption confidence="0.998241">
Table 1: Statistics on articles in the annotated data before and after annotation.
</tableCaption>
<table confidence="0.997977833333333">
Source Proportion of Errors total Errors by Type
language errors in the data
Extraneous Missing a Missing the Confusion
Chinese 9.2% 158 57.0% 13.3% 22.8% 7.0%
Czech 9.6% 102 45.1% 14.7% 33.3% 6.9%
Russian 10.4% 224 41.5% 20.1% 25.5% 12.3%
</table>
<tableCaption confidence="0.8282585">
Table 2: Distribution of article errors in the annotated data by error type. Extraneous refers to using a or the where
None (no article) is correct. Confusion is using a instead of the or vice versa.
</tableCaption>
<table confidence="0.999915166666667">
Label Source Source
language
a the None
Chinese 81.7% 5.9% 12.4%
a Czech 81.0% 4.8% 14.3%
Russian 75.3% 7.9% 16.9%
Chinese 0.2% 91.3% 8.5%
the Czech 0.9% 84.7% 14.4%
Russian 1.9% 84.9% 13.2%
Chinese 0.6% 7.4%% 92.0%
None Czech 1.3% 5.2% 93.6%
Russian 1.0% 5.4%% 93.6%
</table>
<tableCaption confidence="0.769506">
Table 3: Statistics on article corrections by the original
article (source) and the annotator’s choice (label). Each
entry in the table indicates Prob(source = s|label = l)
for each article pair.
</tableCaption>
<bodyText confidence="0.999444666666667">
s|label = l) for each article pair. We can also ob-
serve specific error patterns. For example, the is
more likely than a to be used superfluously.
</bodyText>
<sectionHeader confidence="0.9412275" genericHeader="method">
5 Introducing Article Errors into Training
Data
</sectionHeader>
<bodyText confidence="0.9999574">
This section describes experiments with error gener-
ation methods. We conduct four sets of experiments.
Each set differs in how article errors are generated in
the training data. We now give a description of error
generation paradigms in each experimental set.
</bodyText>
<subsectionHeader confidence="0.997441">
5.1 Methods of error generation
</subsectionHeader>
<bodyText confidence="0.999185878787879">
We refer to the article that the writer used in the ESL
data as source, and label refers to the article that
the annotator chose. Similarly, when we introduce
errors into the training data, we refer to the original
article as label and to the replacement as source.
This is because the original article is the correct
article choice, and the replacement that the classifier
will see as a feature can be an error. We call this
feature source feature. In other words, both for
training (native data) and test (ESL data), source
denotes the form that the classifier sees as a feature
(which could be an error) and label denotes the
correct article. Below we describe how errors are
generated in each set of experiments.
Method 1: General With probability x each ar-
ticle in the training data is replaced with
a different article uniformly at random, and
with probability (1 − x) it remains un-
changed. We build six classifiers, where x
E {5%,10%,12%,14%,16%,18%}. We call
this method general since it uses no informa-
tion about article distribution in the ESL data.
Method 2: ArticleDistrBeforeAnnot We use the
distribution of articles in the ESL data before
the annotation to change the distribution of ar-
ticles in the training. Specifically, we change
the articles so that their distribution approxi-
mates the distribution of articles in the ESL
data. For example, the relative frequency of
the in English Wikipedia data is 29.1%, while
in the writing by Czech speakers it is 22.3%.
It should be noted that this method changes
the distribution only of source articles, but the
</bodyText>
<page confidence="0.996843">
158
</page>
<bodyText confidence="0.996683227272727">
distribution of labels is not affected. An ad-
ditional constraint that we impose is the mini-
mum error rate r for each article class, so that
Prob(s|l) &gt; r dl E labels. In this fashion, for
each source language we train four classifiers,
where we use article distribution from Chinese,
Czech, and Russian, and where we set the min-
imum error rate r to be E {2%, 3%, 4%, 5%}.
Method 3: ArticleDistrAfterAnnot This method
is similar to the one above but we use the dis-
tribution of articles in the ESL data after the
corrections have been made by the annotators.
Method 4: ErrorDistr This method uses informa-
tion about error patterns in the annotated ESL
data. For example, in the Czech annotated sub-
corpus, label the corresponds to source the in
85% of the cases and corresponds to source
None in 14% of the cases. In other words, in
14% of the cases where the article the should
have been used, the writer used no article at all.
Thus, with probability 14% we change the in
the training data to None.
</bodyText>
<sectionHeader confidence="0.997107" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.99992205">
In this section, we compare the quality of the sys-
tem trained on clean native English data to the qual-
ity of the systems trained on data with errors. The
errors were introduced into the training data using
error generation methods presented in Section 5.
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Al-
gorithm (Freund and Schapire, 1999) implemented
within the Sparse Network of Winnow framework
(Carlson et al., 1999) – we use the regularized
version in Learning Based Java6 (LBJ, (Rizzolo
and Roth, 2007)). While classical Perceptron
comes with generalization bound related to the mar-
gin of the data, Averaged Perceptron also comes
with a PAC-like generalization bound (Freund and
Schapire, 1999). This linear learning algorithm is
known, both theoretically and experimentally, to
be among the best linear learning approaches and
is competitive with SVM and Logistic Regression,
</bodyText>
<footnote confidence="0.971508">
6LBJ code is available at http://L2R.cs.uiuc.edu/
</footnote>
<page confidence="0.443692">
-cogcomp/asoftware.php?skey=LBJ
</page>
<bodyText confidence="0.999837911111111">
while being more efficient in training. It also has
been shown to produce state-of-the-art results on
many natural language applications (Punyakanok et
al., 2008).
Since the methods of error generation described in
Section 5 rely on the distribution of articles and ar-
ticle mistakes and these statistics are specific to the
first language of the writer, we conduct evaluation
separately for each source language. Thus, for each
language group, we train five system types: one sys-
tem is trained on clean English data without errors
(the same classifier for the three language groups)
and four systems are trained on data with errors,
where errors are produced using the four methods
described in Section 5. Training data are extracted
from English Wikipedia.
All of the five systems employ the same set of fea-
tures based on three tokens to the right and to the left
of the target article. For each context word, we use
its relative position, its part-of-speech tag and the
word token itself. We also use the head of the noun
phrase and the conjunctions of the pairs and triples
of the six tokens and their part-of-speech tags7. In
addition to these features, the classifiers trained on
data with errors also use the source article as a fea-
ture. The classifier that is trained on clean English
data cannot use the source feature, since in training
the source always corresponds to the label. By con-
trast, when the training data contain mistakes, the
source is not always the same as the label, the situa-
tion that we also have with the test (ESL) data.
We refer to the classifier trained on clean data
as TrainClean. We refer to the classifiers trained
on data with mistakes as TWE (TrainWithErrors).
There are four types of TWE systems for each lan-
guage group, one for each of the methods of error
generation described in Section 5. All results are the
averaged results of training on three random sam-
ples from Wikipedia with two million training ex-
amples on each round. All five classifiers are trained
on exactly the same set of Wikipedia examples, ex-
cept that we add article mistakes to the data used
by the TWE systems. The TrainClean system
achieves an accuracy of 87.10% on data from En-
glish Wikipedia. This performance is state-of-the-
</bodyText>
<footnote confidence="0.9840775">
7Details about the features are given in the paper’s web page,
accessible from http://L2R.cs.uiuc.edu/-cogcomp/
</footnote>
<page confidence="0.998066">
159
</page>
<bodyText confidence="0.994338914285714">
art compared to other systems reported in the lit-
erature (Knight and Chander, 1994; Minnen et al.,
2000; Turner and Charniak, 2007; Han et al., 2006;
De Felice and Pulman, 2008). The best results
of 92.15% are reported by De Felice and Pulman
(2008). But their system uses sophisticated syntac-
tic features and they observe that the parser does not
perform well on non-native data.
As mentioned in Section 4, the annotation of the
ESL data consisted of correcting all errors in the sen-
tence. We exclude from evaluation examples that
have spelling errors in the 3-word window around
the target article and errors on words that immedi-
ately precede or immediately follow the article, as
such examples would obscure the evaluation of the
training paradigms.
Tables 4, 5 and 6 show performance by language
group. The tables show the accuracy and the er-
ror reduction on the test set. The results of systems
TWE (methods 2 and 3) that use the distribution of
articles before and after annotation are merged and
appear as ArtDistr in the tables, since, as shown
in Table 1, these distributions are very similar and
thus produce similar results. Each table compares
the performance of the TrainClean system to the
performance of the four systems trained on data with
errors.
For all language groups, all classifiers of type
TWE outperform the TrainClean system. The
reduction in error rate is consistent when the TWE
classifiers are compared to the TrainClean system.
Table 7 shows results for all three languages, com-
paring for each language group the TrainClean
classifier to the best performing system of type
TWE.
</bodyText>
<table confidence="0.999777142857143">
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.85% -2.26%
TWE(General) 10.0% 92.57% 6.78%
TWE(ArtDistr) 13.2% 92.67% 8.33%
TWE(ErrorDistr) 9.2% 92.31% 3.51%
Baseline 92.03%
</table>
<tableCaption confidence="0.560114714285714">
Table 4: Chinese speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
</tableCaption>
<table confidence="0.999766285714286">
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.82% 10.31%
TWE(General) 18.0% 92.22% 14.69%
TWE(ArtDistr) 21.6% 92.00% 12.28%
TWE(ErrorDistr) 10.2% 92.15% 13.93%
Baseline 90.88%
</table>
<tableCaption confidence="0.851196857142857">
Table 5: Czech speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
</tableCaption>
<table confidence="0.999872285714286">
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 90.62% 5.92%
TWE(General) 14.0% 91.25% 12.24%
TWE(ArtDistr) 18.8% 91.52% 14.94%
TWE(ErrorDistr) 10.7% 91.63% 16.05%
Baseline 90.03%
</table>
<tableCaption confidence="0.8022715">
Table 6: Russian speakers: Performance of the
TrainClean system (without errors in training) and of
</tableCaption>
<bodyText confidence="0.9758598">
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999975294117647">
As shown in Section 6, training a classifier on
data that contain errors produces better results when
compared to the TrainClean classifier trained on
clean native data. The key results for all language
groups are summarized in Table 7. It should be
noted that the TrainClean system also makes use
of the article chosen by the author through a confi-
dence threshold8; it prefers to keep the article chosen
by the user. The difference is that the TrainClean
system does not consider the author’s article in train-
ing. The results of training with error generation
are better, which shows that training on automati-
cally corrupted data indeed helps. While the per-
formance is different by language group, there is an
observable reduction in error rate for each language
group when TWE systems are used compared to
TrainClean approach. The reduction in error rate
</bodyText>
<footnote confidence="0.840847">
8The decision threshold is found empirically on a subset of
the ESL data set aside for development.
</footnote>
<page confidence="0.994358">
160
</page>
<bodyText confidence="0.999974526315789">
achieved by the best performing TWE system when
compared to the error rate of the TrainClean sys-
tem is 10.06% for Chinese, 4.89% for Czech and
10.77% for Russian, as shown in Table 7. We also
note that the best performing TWE systems for Chi-
nese and Russian speakers are those that rely on the
distribution of articles (Chinese) and the distribution
of errors (Russian), but for Czech it is the General
TWE system that performs the best, maybe because
we had less data for Czech speakers, so their statis-
tics are less reliable.
There are several additional observations to be
made. First, training paradigms that use error gen-
eration methods work better than the training ap-
proach of using clean data. Every system of type
TWE outperforms the TrainClean system, as ev-
idenced by Tables 4, 5, and 6. Second, the propor-
tion of errors in the training data should be similar
to the error rate in the test data. The proportion of
errors in training is shown in Tables 4, 5 and 6 in col-
umn 2. Furthermore, TWE systems ArtDistr and
ErrorDistr that use specific knowledge about arti-
cle and error distributions, respectively, work better
for Russian and Chinese groups than the General
method that adds errors to the data uniformly at ran-
dom. Since ArtDistr and ErrorDistr depend on
the statistics of learner mistakes, the success of the
systems that use these methods for error generation
depends on the accuracy of these statistics, and we
only have between 100 and 250 errors for each lan-
guage group. It would be interesting to see whether
better results can be achieved with these methods if
more annotated data are available. Finally, for the
same reason, there is no significant difference in the
performance of methods ArtDistrBeforeAnnot
and ArtDistrAfterAnnot: With small sizes of an-
notated data there is no difference in article distribu-
tions before and after annotation.
</bodyText>
<sectionHeader confidence="0.986614" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999986285714286">
We have shown that error correction training
paradigms that introduce artificial errors are supe-
rior to training classifiers on clean data. We pro-
posed several methods of error generation that ac-
count for error frequency information and error dis-
tribution statistics from non-native text and demon-
strated that the methods that work best are those that
</bodyText>
<table confidence="0.9957155">
Source Accuracy Error
language Train TWE reduction
Clean
Chinese 91.85% 92.67% 10.06%
Czech 91.82% 92.22% 4.89%
Russian 90.62% 91.63% 10.77%
</table>
<tableCaption confidence="0.906095">
Table 7: Improvement due to training with errors. For
each source language, the last column of the table shows
the reduction in error rate achieved by the best perform-
ing TWE system when compared to the error rate of the
TrainClean system. The error rate for each system is
computed by subtracting the accuracy achieved by the
system, as shown in columns 2 and 3.
</tableCaption>
<bodyText confidence="0.999957416666667">
result in a training corpus that statistically resembles
the non-native text. Adding information about arti-
cle distribution in non-native data and statistics on
specific error types is even more helpful.
We have also argued that the baselines used ear-
lier in the relevant literature – all based on the major-
ity of the most commonly used class – suit selection
tasks, but are inappropriate for error correction. In-
stead, the error rate in the data should be taken into
account when determining the baseline.
The focus of the present study was on training
paradigms. While it is quite possible that the article
correction system presented here can be improved
– we would like to explore improving the system
by using a more sophisticated feature set – we be-
lieve that the performance gap due to the error driven
training paradigms shown here will remain. The rea-
son is that even with better features, some of the fea-
tures that hold in the native data will not be active in
in the ESL writing.
Finally, while this study focused on the problem
of correcting article mistakes, we plan to apply the
proposed training paradigms to similar text correc-
tion problems.
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9903356">
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
</bodyText>
<page confidence="0.99777">
161
</page>
<sectionHeader confidence="0.995892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999489954545454">
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of the 21st COLING and the 44th ACL,
Sydney.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. The
SNoW learning architecture. Technical report.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45–50.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
J. Foster and Ø. Andersen. 2009. GenERRate: Gener-
ating Errors for Use in Grammatical Error Detection.
In Proceedings of the NAACL Workshop on Innovative
Use of NLP for Building Educational Applications.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277-296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings ofIJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182–
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107–130.
S. Granger, E. Dagneaux and F. Meunier 2002. Interna-
tional Corpus of Learner English.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. De-
tecting Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115–129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner’s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119–125.
K. Knight and I. Chander. 1994. Automatic Postediting
of Documents. In Proceedings of the American Asso-
ciation ofArtificial Intelligence, pp 779–784.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop,
Goa.
G. Minnen, F. Bond and A. Copestake 2000. Memory-
Based Learning for Article Generation. In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning and of the Second Learning
Language in Logic Workshop, pp 43–48.
V. Punyakanok, D. Roth, and W. Yih. The importance of
syntactic parsing and inference in semantic role label-
ing. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Interna-
tional Conference on Semantic Computing (ICSC), pp
597–604.
A. Rozovskaya and D. Roth 2010. Annotating ESL Er-
rors: Challenges and Rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLPfor Build-
ing Educational Applications.
J. Sj¨obergh and O. Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Companion Volume, Short Papers, pp 177–180.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal. Special Issue on the 2008
Automatic Analysis of Learner Language CALICO
Workshop.
Y. Xing, J. Gao, and W. Dolan. 2009. A web-based En-
glish proofing system for ESL users. In Proceedings
of IJCNLP.
</reference>
<page confidence="0.997763">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882503">
<title confidence="0.999335">Training Paradigms for Correcting Errors in Grammar and Usage</title>
<author confidence="0.998922">Alla Rozovskaya</author>
<author confidence="0.998922">Dan</author>
<affiliation confidence="0.998555">University of Illinois at</affiliation>
<address confidence="0.921108">Urbana, IL</address>
<abstract confidence="0.998644033333334">This paper proposes a novel approach to the problem of training classifiers to detect and correct grammar and usage errors in text by selectively introducing mistakes into the training data. When training a classifier, we would like the distribution of examples seen in training to be as similar as possible to the one seen in testing. In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive. Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors. We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native English speakers; we show that training on data that contain errors produces higher accuracy when compared to a system that is trained on clean native data. We propose several training paradigms with error generation and show that each such paradigm is superior to training a classifier on native data. We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W B Dolan</author>
<author>M Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and the 44th ACL,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="2630" citStr="Brockett et al., 2006" startWordPosition="411" endWordPosition="414"> words, such as confusing peace and piece or your and you’re. The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a</context>
<context position="10346" citStr="Brockett et al. (2006)" startWordPosition="1724" endWordPosition="1727">o detect 8 error types in the same corpus. They show improvement when the training set is enhanced with sentences from the same corpus to which artificial article mistakes have been added. Though it is not clear in what proportion mistakes were added, it is also possible that the improvement was due to a larger training set. Foster and Andersen (2009) attempt to replicate naturally occurring learner mistakes in the Cambridge Learner Corpus (CLC)1, but show a drop in accuracy when the original error-tagged data in training are replaced with corrected CLC sentences containing artificial errors. Brockett et al. (2006) generate mass noun errors in native English data using relevant examples found in the Chinese Learners English Corpus (CLEC, (Gui and Yang, 2003)). Training data consist of an equal number of correct and incorrect sentences. Sj¨obergh and Knutsson (2005) introduce split compound and agreement errors into native Swedish text: agreement errors are added in every sentence and for compound errors, the training set consists of an equal number of negative and positive examples. Their method gives higher recall at the expense of lower precision compared to rulebased grammar checkers. To sum up, alth</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>C. Brockett, W. B. Dolan, and M. Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. In Proceedings of the 21st COLING and the 44th ACL, Sydney.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Carlson</author>
<author>C Cumby</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>The SNoW learning architecture.</title>
<tech>Technical report.</tech>
<marker>Carlson, Cumby, Rosen, Roth, </marker>
<rawString>A. Carlson, C. Cumby, J. Rosen, and D. Roth. The SNoW learning architecture. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling Up Context Sensitive Text Correction.</title>
<date>2001</date>
<booktitle>IAAI,</booktitle>
<pages>45--50</pages>
<contexts>
<context position="1830" citStr="Carlson et al., 2001" startWordPosition="286" endWordPosition="290">h paradigm is superior to training a classifier on native data. We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text. 1 Introduction This paper considers the problem of training classifiers to detect and correct errors in grammar and word usage in text. Both native and non-native speakers make a variety of errors that are not always easy to detect. Consider, for example, the problem of context-sensitive spelling correction (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)). Unlike spelling errors that result in non-words and are easy to detect, context-sensitive spelling correction task involves correcting spelling errors that result in legitimate words, such as confusing peace and piece or your and you’re. The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context. This paper proposes a novel error generation approa</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling Up Context Sensitive Text Correction. IAAI, 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08.</booktitle>
<marker>De Felice, Pulman, 2008</marker>
<rawString>R. De Felice and S. Pulman. 2008. A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English. In Proceedings of COLING-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
<author>Ø Andersen</author>
</authors>
<title>GenERRate: Generating Errors for Use in Grammatical Error Detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="2658" citStr="Foster and Andersen, 2009" startWordPosition="415" endWordPosition="418">ng peace and piece or your and you’re. The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context. This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text. Unlike previous work (e.g., (Sj¨obergh and Knutsson, 2005; Brockett et al., 2006; Foster and Andersen, 2009)), we selectively introduce mistakes in an appropriate proportion. In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text. We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL). The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a machine learning algorithm </context>
<context position="10077" citStr="Foster and Andersen (2009)" startWordPosition="1683" endWordPosition="1687">, mistakes were added in an ad-hoc way, without respecting the error frequencies and error patterns observed in nonnative text. Izumi et al. (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus. They show improvement when the training set is enhanced with sentences from the same corpus to which artificial article mistakes have been added. Though it is not clear in what proportion mistakes were added, it is also possible that the improvement was due to a larger training set. Foster and Andersen (2009) attempt to replicate naturally occurring learner mistakes in the Cambridge Learner Corpus (CLC)1, but show a drop in accuracy when the original error-tagged data in training are replaced with corrected CLC sentences containing artificial errors. Brockett et al. (2006) generate mass noun errors in native English data using relevant examples found in the Chinese Learners English Corpus (CLEC, (Gui and Yang, 2003)). Training data consist of an equal number of correct and incorrect sentences. Sj¨obergh and Knutsson (2005) introduce split compound and agreement errors into native Swedish text: agr</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>J. Foster and Ø. Andersen. 2009. GenERRate: Generating Errors for Use in Grammatical Error Detection. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="21617" citStr="Freund and Schapire, 1999" startWordPosition="3614" endWordPosition="3617">in 14% of the cases where the article the should have been used, the writer used no article at all. Thus, with probability 14% we change the in the training data to None. 6 Experimental Results In this section, we compare the quality of the system trained on clean native English data to the quality of the systems trained on data with errors. The errors were introduced into the training data using error generation methods presented in Section 5. In each training paradigm, we follow a discriminative approach, using an online learning paradigm and making use of the Averaged Perceptron Algorithm (Freund and Schapire, 1999) implemented within the Sparse Network of Winnow framework (Carlson et al., 1999) – we use the regularized version in Learning Based Java6 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, 6LBJ code is available at http://L2R.cs.uiuc.edu/ -cogco</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="3689" citStr="Gamon et al., 2008" startWordPosition="579" endWordPosition="582">The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a machine learning algorithm is used to train a classifier on native English data, where the possible selections are used to generate positive and negative 154 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154–162, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics examples (e.g., (Izumi et al., 2003; Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008)). The classifier is then applied to non-native text to predict the correct article in context. But the article correction problem differs from the problem of article selection in that we know the original (source) article that the writer used. When proposing a correction, we would like to use information about the original article. One reason for this is that about 90% of articles are used correctly by ESL learners; this is higher than the performance of state-of-the-art classifiers for article selection. Consequently, not using the writer’s article, when making a prediction, may result in ma</context>
<context position="7335" citStr="Gamon et al., 2008" startWordPosition="1193" endWordPosition="1196">portant not only for training but also in determining an appropriate evaluation method. The standard baseline used in selection tasks is the relative frequency of the most common class. For example, in word sense disambiguation, the baseline is the most frequent sense. In the task of article selection, the standard baseline used is to predict the article that occurs most frequently in the data (usually, it is the zero article, whose frequency is 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a baseline of 87%, versus the baseline of 60-70% used in the article s</context>
<context position="11950" citStr="Gamon et al. (2008)" startWordPosition="1979" endWordPosition="1983">aining on data with artificial errors is beneficial when compared to utilizing clean data. More importantly, error statistics have not been considered for error correction tasks. Lee and Seneff (2008) examine statistics on article and preposition mistakes in the JLE corpus. While they do not suggest a specific approach, they hypothesize that it might be helpful to incorporate this knowledge into a correction system that targets these two language phenomena. 3.2 Approaches to Detecting Article Mistakes Automated methods for detecting article mistakes generally use a machine learning algorithm. Gamon et al. (2008) use a decision tree model and a 5-gram language model trained on the English Gigaword corpus (LDC2005T12) to correct errors in English article and preposition usage. Han et al. (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. Yi et al. (2008) propose a web count-based system to correct determiner errors. In the above approaches, the classifiers are trained on native data. Therefore the classifiers cannot use the 1http://www.cambridge.org/elt 156 original article that the writer used as a feature. Han et al. (2006) use the source article at evaluation time and propose</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan, D. Belenko and L. Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying Winnow to Context-Sensitive Spelling Correction.</title>
<date>1996</date>
<journal>ICML,</journal>
<volume>182</volume>
<pages>190</pages>
<contexts>
<context position="1783" citStr="Golding and Roth, 1996" startWordPosition="277" endWordPosition="281">gms with error generation and show that each such paradigm is superior to training a classifier on native data. We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text. 1 Introduction This paper considers the problem of training classifiers to detect and correct errors in grammar and word usage in text. Both native and non-native speakers make a variety of errors that are not always easy to detect. Consider, for example, the problem of context-sensitive spelling correction (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)). Unlike spelling errors that result in non-words and are easy to detect, context-sensitive spelling correction task involves correcting spelling errors that result in legitimate words, such as confusing peace and piece or your and you’re. The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context. This</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>A. R. Golding and D. Roth. 1996. Applying Winnow to Context-Sensitive Spelling Correction. ICML, 182– 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1807" citStr="Golding and Roth, 1999" startWordPosition="282" endWordPosition="285">n and show that each such paradigm is superior to training a classifier on native data. We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text. 1 Introduction This paper considers the problem of training classifiers to detect and correct errors in grammar and word usage in text. Both native and non-native speakers make a variety of errors that are not always easy to detect. Consider, for example, the problem of context-sensitive spelling correction (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)). Unlike spelling errors that result in non-words and are easy to detect, context-sensitive spelling correction task involves correcting spelling errors that result in legitimate words, such as confusing peace and piece or your and you’re. The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context. This paper proposes a novel </context>
<context position="7528" citStr="Golding and Roth, 1999" startWordPosition="1225" endWordPosition="1228"> example, in word sense disambiguation, the baseline is the most frequent sense. In the task of article selection, the standard baseline used is to predict the article that occurs most frequently in the data (usually, it is the zero article, whose frequency is 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a baseline of 87%, versus the baseline of 60-70% used in the article selection task. Statistics on article mistakes in our data suggest a baseline of about 90%, depending on the source language of the writer. So the real baseline on the task is ”do nothing”. Ther</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to Context-Sensitive Spelling Correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<journal>International Corpus of Learner English.</journal>
<contexts>
<context position="13535" citStr="Granger et al., 2002" startWordPosition="2245" endWordPosition="2248"> data by Russian, Chinese, and Japanese speakers 13% of all noun phrases have an incorrect article. It is interesting to note that article errors are present even with very advanced speakers. While the TOEFL data include essays by students of different proficiency levels, we use data from very advanced learners and find that error rates on articles are similar to those reported by Han et al. (2006). We use data from speakers of three first language backgrounds: Chinese, Czech, and Russian. None of these languages has an article system. The Czech and the Russian data come from the ICLE corpus (Granger et al., 2002), which is a collection of essays written by advanced learners of English. The Chinese data is a part of the CLEC corpus that contains essays by students of all levels of proficiency. 4.1 Data Annotation A portion of data for each source language was corrected and error-tagged by native speakers. The annotation was performed at the sentence level: a sentence was presented to the annotator in the context of the entire essay. Essay context can become necessary, when an article is acceptable in the context of a sentence, but is incorrect in the context of the essay. Our goal was to correct all ar</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>S. Granger, E. Dagneaux and F. Meunier 2002. International Corpus of Learner English.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gui</author>
<author>H Yang</author>
</authors>
<title>Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</title>
<date>2003</date>
<contexts>
<context position="10492" citStr="Gui and Yang, 2003" startWordPosition="1750" endWordPosition="1753">icial article mistakes have been added. Though it is not clear in what proportion mistakes were added, it is also possible that the improvement was due to a larger training set. Foster and Andersen (2009) attempt to replicate naturally occurring learner mistakes in the Cambridge Learner Corpus (CLC)1, but show a drop in accuracy when the original error-tagged data in training are replaced with corrected CLC sentences containing artificial errors. Brockett et al. (2006) generate mass noun errors in native English data using relevant examples found in the Chinese Learners English Corpus (CLEC, (Gui and Yang, 2003)). Training data consist of an equal number of correct and incorrect sentences. Sj¨obergh and Knutsson (2005) introduce split compound and agreement errors into native Swedish text: agreement errors are added in every sentence and for compound errors, the training set consists of an equal number of negative and positive examples. Their method gives higher recall at the expense of lower precision compared to rulebased grammar checkers. To sum up, although the idea of using data with artificial mistakes is not new, the advantage of training on such data has not been investigated. Moreover, train</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting Errors in English Article Usage by Non-native Speakers.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="3640" citStr="Han et al., 2006" startWordPosition="570" endWordPosition="573">arners of English as a Second Language (ESL). The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a machine learning algorithm is used to train a classifier on native English data, where the possible selections are used to generate positive and negative 154 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154–162, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics examples (e.g., (Izumi et al., 2003; Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008)). The classifier is then applied to non-native text to predict the correct article in context. But the article correction problem differs from the problem of article selection in that we know the original (source) article that the writer used. When proposing a correction, we would like to use information about the original article. One reason for this is that about 90% of articles are used correctly by ESL learners; this is higher than the performance of state-of-the-art classifiers for article selection. Consequently, not using the writer’s ar</context>
<context position="7781" citStr="Han et al. (2006)" startWordPosition="1267" endWordPosition="1270">s 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a baseline of 87%, versus the baseline of 60-70% used in the article selection task. Statistics on article mistakes in our data suggest a baseline of about 90%, depending on the source language of the writer. So the real baseline on the task is ”do nothing”. Therefore, to determine the baseline for a correction task, one needs to consider the error rate in the data. 155 Using the definitions of precision and recall and the “real” baseline, we can also relate the resulting accuracy of the classifier to the preci</context>
<context position="12134" citStr="Han et al. (2006)" startWordPosition="2012" endWordPosition="2015"> Seneff (2008) examine statistics on article and preposition mistakes in the JLE corpus. While they do not suggest a specific approach, they hypothesize that it might be helpful to incorporate this knowledge into a correction system that targets these two language phenomena. 3.2 Approaches to Detecting Article Mistakes Automated methods for detecting article mistakes generally use a machine learning algorithm. Gamon et al. (2008) use a decision tree model and a 5-gram language model trained on the English Gigaword corpus (LDC2005T12) to correct errors in English article and preposition usage. Han et al. (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. Yi et al. (2008) propose a web count-based system to correct determiner errors. In the above approaches, the classifiers are trained on native data. Therefore the classifiers cannot use the 1http://www.cambridge.org/elt 156 original article that the writer used as a feature. Han et al. (2006) use the source article at evaluation time and propose a correction only when the score of the classifier is high enough, but the source article is not used in training. 4 Article Errors in ESL Data Article errors are one of the most comm</context>
<context position="24746" citStr="Han et al., 2006" startWordPosition="4128" endWordPosition="4131">e random samples from Wikipedia with two million training examples on each round. All five classifiers are trained on exactly the same set of Wikipedia examples, except that we add article mistakes to the data used by the TWE systems. The TrainClean system achieves an accuracy of 87.10% on data from English Wikipedia. This performance is state-of-the7Details about the features are given in the paper’s web page, accessible from http://L2R.cs.uiuc.edu/-cogcomp/ 159 art compared to other systems reported in the literature (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Han et al., 2006; De Felice and Pulman, 2008). The best results of 92.15% are reported by De Felice and Pulman (2008). But their system uses sophisticated syntactic features and they observe that the parser does not perform well on non-native data. As mentioned in Section 4, the annotation of the ESL data consisted of correcting all errors in the sentence. We exclude from evaluation examples that have spelling errors in the 3-word window around the target article and errors on words that immediately precede or immediately follow the article, as such examples would obscure the evaluation of the training paradi</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N. Han, M. Chodorow and C. Leacock. 2006. Detecting Errors in English Article Usage by Non-native Speakers. Journal of Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>H Isahara</author>
</authors>
<title>Automatic Error Detection in the Japanese Leaners English Spoken Data.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="3622" citStr="Izumi et al., 2003" startWordPosition="566" endWordPosition="569"> mistakes made by learners of English as a Second Language (ESL). The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a machine learning algorithm is used to train a classifier on native English data, where the possible selections are used to generate positive and negative 154 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154–162, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics examples (e.g., (Izumi et al., 2003; Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008)). The classifier is then applied to non-native text to predict the correct article in context. But the article correction problem differs from the problem of article selection in that we know the original (source) article that the writer used. When proposing a correction, we would like to use information about the original article. One reason for this is that about 90% of articles are used correctly by ESL learners; this is higher than the performance of state-of-the-art classifiers for article selection. Consequently, not usi</context>
<context position="9598" citStr="Izumi et al. (2003)" startWordPosition="1600" endWordPosition="1603">mbers by error type. Excluding the error type of category other, we can estimate that Base = 0.1, so the baseline is 0.9, average precision and recall are 0.85 and 0.25, respectively, and the resulting overall accuracy of the system is 92.2%. 3 Related Work 3.1 Generating Errors in Text In text correction, adding mistakes in training has been explored before. Although the general approach has been to produce errors similar to those observed in the data to be corrected, mistakes were added in an ad-hoc way, without respecting the error frequencies and error patterns observed in nonnative text. Izumi et al. (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus. They show improvement when the training set is enhanced with sentences from the same corpus to which artificial article mistakes have been added. Though it is not clear in what proportion mistakes were added, it is also possible that the improvement was due to a larger training set. Foster and Andersen (2009) attempt to replicate naturally occurring learner mistakes in the Cambridge Learner Corpus (CLC)1, but show a drop in acc</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003. Automatic Error Detection in the Japanese Leaners English Spoken Data. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>The NICT JLE Corpus: Exploiting the Language Learner’s Speech Database for Research and Education.</title>
<date>2004</date>
<journal>International Journal of the Computer, the Internet and Management,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="9721" citStr="Izumi et al., 2004" startWordPosition="1621" endWordPosition="1624">verage precision and recall are 0.85 and 0.25, respectively, and the resulting overall accuracy of the system is 92.2%. 3 Related Work 3.1 Generating Errors in Text In text correction, adding mistakes in training has been explored before. Although the general approach has been to produce errors similar to those observed in the data to be corrected, mistakes were added in an ad-hoc way, without respecting the error frequencies and error patterns observed in nonnative text. Izumi et al. (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus. They show improvement when the training set is enhanced with sentences from the same corpus to which artificial article mistakes have been added. Though it is not clear in what proportion mistakes were added, it is also possible that the improvement was due to a larger training set. Foster and Andersen (2009) attempt to replicate naturally occurring learner mistakes in the Cambridge Learner Corpus (CLC)1, but show a drop in accuracy when the original error-tagged data in training are replaced with corrected CLC sentences containing artificial error</context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>E. Izumi, K. Uchimoto and H. Isahara. 2004. The NICT JLE Corpus: Exploiting the Language Learner’s Speech Database for Research and Education. International Journal of the Computer, the Internet and Management, 12(2):119–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automatic Postediting of Documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the American Association ofArtificial Intelligence,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="7266" citStr="Knight and Chander, 1994" startWordPosition="1181" endWordPosition="1184">ween the selection and the error correction tasks alluded to earlier is important not only for training but also in determining an appropriate evaluation method. The standard baseline used in selection tasks is the relative frequency of the most common class. For example, in word sense disambiguation, the baseline is the most frequent sense. In the task of article selection, the standard baseline used is to predict the article that occurs most frequently in the data (usually, it is the zero article, whose frequency is 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a</context>
<context position="24680" citStr="Knight and Chander, 1994" startWordPosition="4116" endWordPosition="4119">bed in Section 5. All results are the averaged results of training on three random samples from Wikipedia with two million training examples on each round. All five classifiers are trained on exactly the same set of Wikipedia examples, except that we add article mistakes to the data used by the TWE systems. The TrainClean system achieves an accuracy of 87.10% on data from English Wikipedia. This performance is state-of-the7Details about the features are given in the paper’s web page, accessible from http://L2R.cs.uiuc.edu/-cogcomp/ 159 art compared to other systems reported in the literature (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Han et al., 2006; De Felice and Pulman, 2008). The best results of 92.15% are reported by De Felice and Pulman (2008). But their system uses sophisticated syntactic features and they observe that the parser does not perform well on non-native data. As mentioned in Section 4, the annotation of the ESL data consisted of correcting all errors in the sentence. We exclude from evaluation examples that have spelling errors in the 3-word window around the target article and errors on words that immediately precede or immediately follow the article, as</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automatic Postediting of Documents. In Proceedings of the American Association ofArtificial Intelligence, pp 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>An analysis of grammatical errors in non-native speech in English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Spoken Language Technology Workshop,</booktitle>
<location>Goa.</location>
<contexts>
<context position="4435" citStr="Lee and Seneff, 2008" startWordPosition="704" endWordPosition="708">oblem differs from the problem of article selection in that we know the original (source) article that the writer used. When proposing a correction, we would like to use information about the original article. One reason for this is that about 90% of articles are used correctly by ESL learners; this is higher than the performance of state-of-the-art classifiers for article selection. Consequently, not using the writer’s article, when making a prediction, may result in making more mistakes than there are in the data. Another reason is that statistics on article errors (e.g., (Han et al., 2006; Lee and Seneff, 2008)) and in the annotation performed for the present study reveal that non-native English speakers make article mistakes in a consistent manner. The system can consider the article used by the writer at evaluation time, by proposing a correction only when the confidence of the classifier is high enough, but the article cannot be used in training if the classifier is trained on clean native data that do not have errors. Learning Theory says that the distribution of examples seen in testing should be as similar as possible to the one seen in training, so one would like to train on errors similar to</context>
<context position="11531" citStr="Lee and Seneff (2008)" startWordPosition="1916" endWordPosition="1919">ed grammar checkers. To sum up, although the idea of using data with artificial mistakes is not new, the advantage of training on such data has not been investigated. Moreover, training on error-tagged data is currently unrealistic in the majority of error correction scenarios, which suggests that using text with artificial mistakes is the only alternative to using clean data. However, it has not been shown whether training on data with artificial errors is beneficial when compared to utilizing clean data. More importantly, error statistics have not been considered for error correction tasks. Lee and Seneff (2008) examine statistics on article and preposition mistakes in the JLE corpus. While they do not suggest a specific approach, they hypothesize that it might be helpful to incorporate this knowledge into a correction system that targets these two language phenomena. 3.2 Approaches to Detecting Article Mistakes Automated methods for detecting article mistakes generally use a machine learning algorithm. Gamon et al. (2008) use a decision tree model and a 5-gram language model trained on the English Gigaword corpus (LDC2005T12) to correct errors in English article and preposition usage. Han et al. (20</context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>J. Lee and S. Seneff. 2008. An analysis of grammatical errors in non-native speech in English. In Proceedings of the 2008 Spoken Language Technology Workshop, Goa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>F Bond</author>
<author>A Copestake</author>
</authors>
<title>MemoryBased Learning for Article Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="7287" citStr="Minnen et al., 2000" startWordPosition="1185" endWordPosition="1188"> error correction tasks alluded to earlier is important not only for training but also in determining an appropriate evaluation method. The standard baseline used in selection tasks is the relative frequency of the most common class. For example, in word sense disambiguation, the baseline is the most frequent sense. In the task of article selection, the standard baseline used is to predict the article that occurs most frequently in the data (usually, it is the zero article, whose frequency is 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a baseline of 87%, ver</context>
<context position="24701" citStr="Minnen et al., 2000" startWordPosition="4120" endWordPosition="4123">lts are the averaged results of training on three random samples from Wikipedia with two million training examples on each round. All five classifiers are trained on exactly the same set of Wikipedia examples, except that we add article mistakes to the data used by the TWE systems. The TrainClean system achieves an accuracy of 87.10% on data from English Wikipedia. This performance is state-of-the7Details about the features are given in the paper’s web page, accessible from http://L2R.cs.uiuc.edu/-cogcomp/ 159 art compared to other systems reported in the literature (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Han et al., 2006; De Felice and Pulman, 2008). The best results of 92.15% are reported by De Felice and Pulman (2008). But their system uses sophisticated syntactic features and they observe that the parser does not perform well on non-native data. As mentioned in Section 4, the annotation of the ESL data consisted of correcting all errors in the sentence. We exclude from evaluation examples that have spelling errors in the 3-word window around the target article and errors on words that immediately precede or immediately follow the article, as such examples would </context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>G. Minnen, F. Bond and A. Copestake 2000. MemoryBased Learning for Article Generation. In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop, pp 43–48.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>Punyakanok, Roth, Yih, </marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Modeling Discriminative Global Inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the First International Conference on Semantic Computing (ICSC),</booktitle>
<pages>597--604</pages>
<contexts>
<context position="21786" citStr="Rizzolo and Roth, 2007" startWordPosition="3641" endWordPosition="3644"> Experimental Results In this section, we compare the quality of the system trained on clean native English data to the quality of the systems trained on data with errors. The errors were introduced into the training data using error generation methods presented in Section 5. In each training paradigm, we follow a discriminative approach, using an online learning paradigm and making use of the Averaged Perceptron Algorithm (Freund and Schapire, 1999) implemented within the Sparse Network of Winnow framework (Carlson et al., 1999) – we use the regularized version in Learning Based Java6 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, 6LBJ code is available at http://L2R.cs.uiuc.edu/ -cogcomp/asoftware.php?skey=LBJ while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applications (Punya</context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>N. Rizzolo and D. Roth 2007. Modeling Discriminative Global Inference. In Proceedings of the First International Conference on Semantic Computing (ICSC), pp 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Annotating ESL Errors: Challenges and Rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</booktitle>
<contexts>
<context position="14845" citStr="Rozovskaya and Roth, 2010" startWordPosition="2467" endWordPosition="2470"> were not correct in the context of the essay. The annotators were also encouraged to propose more than one correction, as long as all of their suggestions were consistent with the essay context. The annotators were asked to correct all mistakes in the sentence. The annotation schema included the following error types: mistakes in article and preposition usage, errors in noun number, spelling, verb form, and word form2. All other corrections were marked as word replacement, word deletion, and word insertion. For details about annotation and data selection, please refer to the companion paper (Rozovskaya and Roth, 2010). 4.2 Statistics on Article Errors Traditionally, three article classes are distinguished: the, a(an)3 and None (no article). The training and the test data are thus composed of two types of events: 1. All articles in the data 2. Spaces in front of a noun phrase if that noun phrase does not start with an article. To identify the beginning of a noun phrase, we ran a partof-speech tagger and a phrase chunker4 and excluded all noun phrases not headed5 by a personal or demonstrative pronoun. Table 1 shows the size of the test data by source language, proportion of errors and distribution of articl</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth 2010. Annotating ESL Errors: Challenges and Rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sj¨obergh</author>
<author>O Knutsson</author>
</authors>
<title>Faking errors to avoid making errors.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP 2005,</booktitle>
<location>Borovets.</location>
<marker>Sj¨obergh, Knutsson, 2005</marker>
<rawString>J. Sj¨obergh and O. Knutsson. 2005. Faking errors to avoid making errors. In Proceedings of RANLP 2005, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection.</title>
<date>2008</date>
<booktitle>COLING Workshop on Human Judgments in Computational Linguistics,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="16503" citStr="Tetreault and Chodorow (2008)" startWordPosition="2740" endWordPosition="2743">tive text. In particular, non-native data have a lower proportion of the. The annotation statistics also reveal that learners do not confuse articles randomly. From Table 2, which shows the distribution of article errors by type, we observe that the majority of mistakes are omissions and extraneous articles. Table 3 shows statistics on corrections by source and label, where source refers to the article used by the writer, and label refers to the article chosen by the annotator. Each entry in the table indicates Prob(source = 2Our classification, was inspired by the classification presented in Tetreault and Chodorow (2008) 3Henceforth, we will use a to refer to both a and an 4The tagger and the chunker are available at http:// L2R.cs.uiuc.edu/˜cogcomp/software.php 5We assume that the last word of the noun phrase is its head. 157 Source Number of Proportion of Errors Article Classes language test examples errors total distribution a the None Before annotation 8.5 28.2 63.3 Chinese 1713 9.2% 158 After annotation 9.9 24.9 65.2 Before annotation 9.1 22.9 68.0 Czech 1061 9.6% 102 After annotation 9.9 22.3 67.8 Russian 2146 10.4% 224 Before annotation 10.5 21.7 67.9 After annotation 12.5 20.1 67.4 English Wikipedia 9</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection. COLING Workshop on Human Judgments in Computational Linguistics, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Language Modeling for Determiner Selection. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="7314" citStr="Turner and Charniak, 2007" startWordPosition="1189" endWordPosition="1192">ks alluded to earlier is important not only for training but also in determining an appropriate evaluation method. The standard baseline used in selection tasks is the relative frequency of the most common class. For example, in word sense disambiguation, the baseline is the most frequent sense. In the task of article selection, the standard baseline used is to predict the article that occurs most frequently in the data (usually, it is the zero article, whose frequency is 60-70%). In this context, the performance of a state-of-the-art classifier (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Gamon et al., 2008) whose accuracy is 85- 87% is a significant improvement over the baseline. The majority has been used as the baseline also in the context-sensitive spelling task (e.g., (Golding and Roth, 1999)). However, in article correction, spelling correction, and other text correction applications the split of the classes is not an appropriate baseline since the majority of the words in the confusion set are used correctly in the text. Han et al. (2006) report an average error rate of 13% on article data from TOEFL essays, which gives a baseline of 87%, versus the baseline of 60-70% </context>
<context position="24728" citStr="Turner and Charniak, 2007" startWordPosition="4124" endWordPosition="4127">results of training on three random samples from Wikipedia with two million training examples on each round. All five classifiers are trained on exactly the same set of Wikipedia examples, except that we add article mistakes to the data used by the TWE systems. The TrainClean system achieves an accuracy of 87.10% on data from English Wikipedia. This performance is state-of-the7Details about the features are given in the paper’s web page, accessible from http://L2R.cs.uiuc.edu/-cogcomp/ 159 art compared to other systems reported in the literature (Knight and Chander, 1994; Minnen et al., 2000; Turner and Charniak, 2007; Han et al., 2006; De Felice and Pulman, 2008). The best results of 92.15% are reported by De Felice and Pulman (2008). But their system uses sophisticated syntactic features and they observe that the parser does not perform well on non-native data. As mentioned in Section 4, the annotation of the ESL data consisted of correcting all errors in the sentence. We exclude from evaluation examples that have spelling errors in the 3-word window around the target article and errors on words that immediately precede or immediately follow the article, as such examples would obscure the evaluation of t</context>
</contexts>
<marker>Turner, Charniak, 2007</marker>
<rawString>J. Turner and E. Charniak. 2007. Language Modeling for Determiner Selection. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pp 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wagner</author>
<author>J Foster</author>
<author>J van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal. Special Issue on the</journal>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>J. Wagner, J. Foster, and J. van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal. Special Issue on the 2008 Automatic Analysis of Learner Language CALICO Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Xing</author>
<author>J Gao</author>
<author>W Dolan</author>
</authors>
<title>A web-based English proofing system for ESL users.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Xing, Gao, Dolan, 2009</marker>
<rawString>Y. Xing, J. Gao, and W. Dolan. 2009. A web-based English proofing system for ESL users. In Proceedings of IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>