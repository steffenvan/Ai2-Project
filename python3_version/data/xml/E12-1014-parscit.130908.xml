<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.981549">
Toward Statistical Machine Translation without Parallel Corpora
</title>
<author confidence="0.971462">
Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
</author>
<affiliation confidence="0.9263455">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.977187" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969">
We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901071428572">
The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al., 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a
novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al., 2003) estimated using these mono-
lingual features.
Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:
</bodyText>
<listItem confidence="0.9841392">
• In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).
• In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.
• In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.
• Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.
</listItem>
<page confidence="0.966335">
130
</page>
<note confidence="0.9936155">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130–140,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.967966" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999864833333333">
We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ≈2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.
</bodyText>
<figure confidence="0.99640832">
aufrgund
Wieviel
sollte
man
verdienen
seines
Profils
in
Facebook
How
much
should
you
charge
for
your
Facebook
profile
m
m
m
d
d
a
d
</figure>
<subsectionHeader confidence="0.996094">
2.1 Parameters of phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.999859176470588">
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al., 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al., 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al., 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:
</bodyText>
<listItem confidence="0.993483176470588">
• Phrase pairs. Phrase extraction heuristics
(Venugopal et al., 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.
• Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, 0(e|f) and 0(f|e),
which are typically calculated via maximum
likelihood estimation.
• Lexical weighting. Since MLE overestimates
0 for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-
</listItem>
<figureCaption confidence="0.979920166666667">
Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).
</figureCaption>
<bodyText confidence="0.918107">
age word translation probabilities, w(ez|fj),
are calculated via phrase-pair-internal word
alignments.
</bodyText>
<listItem confidence="0.772527857142857">
• Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.
• Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.
</listItem>
<bodyText confidence="0.999499111111111">
The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.
</bodyText>
<page confidence="0.99694">
131
</page>
<subsectionHeader confidence="0.997635">
2.2 Bilingual lexicon induction for SMT
</subsectionHeader>
<bodyText confidence="0.999872959183674">
Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.
A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al. (2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.
In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100
</bodyText>
<subsectionHeader confidence="0.576604">
Corpus Frequency
</subsectionHeader>
<figureCaption confidence="0.992748333333333">
Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.
</figureCaption>
<bodyText confidence="0.999964142857143">
nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al. (2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.
In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.
Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must
</bodyText>
<footnote confidence="0.964344">
1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.
</footnote>
<figure confidence="0.998853179104477">
0 100 200 300 400 500 600
Accuracy, %
0 10 20 30 40
●
●
●
●
●
●
●
●
● Top 1
Top 10
●
132
Occurrences
Occurrences
J
J
J
J
policy
activity of
para crecer
(projected)
to expand
ES Context
Vector
Projected ES
Context Vector
EN Context
Vectors
J
J
J
J
project
J
J
J
dict.
t1
t2
t3
policy
growth
foreign
tM-1
tM
economic
activity
s1
s2
s3
economico
planeta
tasa
sN-1
sN
extranjero
empleo
para crecer
terrorist (en)
terrorista (es)
terrorist (en)
riqueza (es)
Time
</figure>
<figureCaption confidence="0.9934135">
Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.
</figureCaption>
<bodyText confidence="0.999981222222222">
be compared against the vectors for all words in
the target language vocabulary. The size of the n2
comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.
</bodyText>
<sectionHeader confidence="0.979938" genericHeader="method">
3 Monolingual Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999993857142857">
We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.
</bodyText>
<subsectionHeader confidence="0.999116">
3.1 Phrasal similarity features
</subsectionHeader>
<bodyText confidence="0.925111285714286">
Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, ... sN) and (t1, t2,... tM) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an
</bodyText>
<figureCaption confidence="0.7524475">
Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.
</figureCaption>
<bodyText confidence="0.999859590909091">
N- and target phrase e with an M-dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f.
Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f’s
contextual vector as follows:
</bodyText>
<equation confidence="0.985241">
wk = nf,k x (log(n/nk) + 1)
</equation>
<bodyText confidence="0.9887966">
where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.
Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may
</bodyText>
<page confidence="0.996185">
133
</page>
<bodyText confidence="0.999884960784314">
be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al., 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asi´atico is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al., 2009), but it is scal-
able to full bilingual lexicon induction.
</bodyText>
<subsectionHeader confidence="0.999446">
3.2 Lexical similarity features
</subsectionHeader>
<bodyText confidence="0.99993725">
In addition to the three phrase similarity features
used in our model – c(f, e), t(f, e) and w(f, e) –
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).
Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.
The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities 0 and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).
</bodyText>
<subsectionHeader confidence="0.992511">
3.3 Reordering
</subsectionHeader>
<bodyText confidence="0.999644666666667">
The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating
</bodyText>
<page confidence="0.993665">
134
</page>
<bodyText confidence="0.584081">
Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f(i), e(i))}Ni=1.
Output: Orientation features (pm, ps, pd).
Sf +— sentences containing fin Cf;
Se +— sentences containing e in Ce;
</bodyText>
<equation confidence="0.752079875">
(Bf, �, �) � CollectOccurs(f, UN i=1f(i), Sf);
(Be, Ae, De) i— CollectOccurs(e, UNi=1e(i), Se);
cm = cs = cd = 0;
foreach unique f&apos; in Bf do
foreach translation e&apos; of f&apos; in T do
cm = cm + #BQ(e&apos;);
cs = cs + #AQ(e&apos;);
cd = cd + #DQ(e&apos;);
</equation>
<figure confidence="0.832040714285714">
Das Anlegen Profils in Facebook
eines ist
einfach
What
does
your
Facebook
profile
reveal
s
c — cm + cs + cd;
return (cm c ,cg c ,cdc )
CollectOccurs(r, R, S)
B — (); A — (); D �— ();
foreach sentence s E S do
foreach occurrence ofphrase r in s do
B B + (longest preceding r and in R);
A A + (longest following r and in R);
D D + (longest discontinuous w/ r and in
R);
return (B, A, D);
</figure>
<figureCaption confidence="0.999851">
Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.
</figureCaption>
<bodyText confidence="0.99355988">
po(orientation|f, e) from two monolingual cor-
pora instead a bitext.
Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
“Profils”, e = “profile”), we count its orien-
tation with the previously translated phrase pair
(f&apos; = “in Facebook”, e&apos; = “Facebook”) across
all translated sentence pairs in the bitext.
In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = “Profils”, and at least one other phrase
that we have a translation for, like f&apos; = “in
Facebook”. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
“profile”) and any translation of f&apos;. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f&apos;) and English sentences con-
</bodyText>
<figureCaption confidence="0.72769675">
Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (“profile”, “Profils”)
from non-parallel sentences (the German sentence
translates as “Creating a Facebook profile is easy”).
</figureCaption>
<bodyText confidence="0.999982677419355">
taining their corresponding translations (e, e&apos;), we
are able to increment orientation counts for (f, e)
by looking at whether e and e&apos; are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.
One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2
Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f&apos; preceding f, we look up transla-
tions in the phrase table T. Next, we count3 how
</bodyText>
<footnote confidence="0.996760333333333">
2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.
3#L(x) returns the count of object x in list L.
</footnote>
<page confidence="0.993341">
135
</page>
<table confidence="0.9978198">
Monolingual training corpora Spanish-English phrase table
Europarl Gigaword Wikipedia Phrase pairs 3,093,228
date range 4/96-10/09 5/94-12/08 n/a Spanish phrases 89,386
uniq shared dates 829 5,249 n/a English phrases 926,138
Spanish articles n/a 3,727,954 59,463 Spanish unigrams 13,216
English articles n/a 4,862,876 59,463 Avg # translations 98.7
Spanish lines 1,307,339 22,862,835 2,598,269 Spanish bigrams 41,426
English lines 1,307,339 67,341,030 3,630,041 Avg # translations 31.9
Spanish words 28,248,930 774,813,847 39,738,084 Spanish trigrams 34,744
English words 27,335,006 1,827,065,374 61,656,646 Avg # translations 13.5
</table>
<tableCaption confidence="0.999958">
Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
</tableCaption>
<bodyText confidence="0.999741">
many translations e&apos; of f&apos; appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientationjf, e).
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.994525735294117">
We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.
The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al., 2010).4 MERT
4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.
was re-run for every experiment.
We estimate the parameters of our model from
two sets of monolingual data, detailed in Table 1:
</bodyText>
<listItem confidence="0.93854">
• First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al. (2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.
• Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
</listItem>
<bodyText confidence="0.970897588235294">
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.
To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.
The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the
</bodyText>
<footnote confidence="0.811815">
5We use the afp, apw and xin sections of the corpora.
</footnote>
<page confidence="0.995371">
136
</page>
<figure confidence="0.971139214285714">
1 2 3 4 5 6 7 8 9 10 11
BLEU
0 5 10 15 20 25
B
0 0 5 5 20 20 25
25
21.87 21.54 Estimated Using Europarl
Emate rpora
12.86
4.00
10.52
15.35
14. 02 14.78
16.85 17.50
</figure>
<table confidence="0.968230066666667">
22.92
Exp Phrase scores / orientation scores
1 B/B bilingual / bilingual (Moses)
2 B/- bilingual / distortion
3 -/B none / bilingual
4 -/- none / distortion
5, 12 -/M none / mono
6, 13 t/- temporal mono / distortion
7,14 o/- orthographic mono / distortion
8, 15 c/- contextual mono / distortion
16 w/- Wikipedia topical mono / distorion
9, 17 M/- all mono / distortion
10, 18 M/M all mono / mono
Estim
11, 19 BM/B bilingual + all mono / bilingual
</table>
<page confidence="0.519925">
4
</page>
<figureCaption confidence="0.9100185">
Figure 7: Much of the lossB/in BLEU score when bilingually estimated featuresnare removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Europarl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features,3the second is for reordering probabilities.
</figureCaption>
<figure confidence="0.63575">
EE
5 Experimental Results
</figure>
<figureCaption confidence="0.991437333333333">
Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.
</figureCaption>
<bodyText confidence="0.998941045454545">
phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.
Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/˜anni/papers/lowresmt/
</bodyText>
<figureCaption confidence="0.943431">
Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
</figureCaption>
<bodyText confidence="0.950761222222222">
12 1 14 15 1 17 18 19
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.
</bodyText>
<subsectionHeader confidence="0.997784">
5.1 Lesion experiments
</subsectionHeader>
<bodyText confidence="0.999986125">
Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities 0 and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.
</bodyText>
<subsectionHeader confidence="0.998256">
5.2 Adding equivalent monolingual features
estimated using Europarl
</subsectionHeader>
<bodyText confidence="0.9999265">
Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating
</bodyText>
<figure confidence="0.9927879375">
12 13 14 15 16 17 18 19
BLEU
BLEU
0 0 55 10
10 15
15 20 25
25
1015
10.15
Estimated Using Monolingual Corpora
10 18 M/M all mono /
1
3.13 14.02 14.07
1
7.00 17.92 18.79
23.3623.36
</figure>
<page confidence="0.983416">
137
</page>
<bodyText confidence="0.999965666666667">
reordering probabilities from monolingual data (–
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (–/–) to the
model with bilingual reordering features (–/B).
Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/–), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use “mono-
lingual” corpora which have practically identical
phrasal and temporal distributions.
</bodyText>
<subsectionHeader confidence="0.9040055">
5.3 Estimating features using truly
monolingual corpora
</subsectionHeader>
<bodyText confidence="0.9999665">
Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.
Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).
Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.
</bodyText>
<sectionHeader confidence="0.996791" genericHeader="method">
6 Additional Related Work
</sectionHeader>
<bodyText confidence="0.999938909090909">
Carbonell et al. (2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.
Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
S´anchez-Cartagena et al. (2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.
Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.
Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al. (2005) and Daum´e and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.
A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al., 2010;
Uszkoreit et al., 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al., 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al., 2011).
These are complementary but orthogonal to our
research goals.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997452631579">
This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.
</bodyText>
<page confidence="0.997812">
138
</page>
<sectionHeader confidence="0.990103" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999816207207207">
Enrique Alfonseca, Massimiliano Ciaramita, and
Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.
Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157–160.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Hal Daum´e and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop
on Data-Driven Machine Translation, Toulouse,
France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings ofACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.
Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.
Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284–293, Edinburgh,
Scotland, UK.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
</reference>
<page confidence="0.986985">
139
</page>
<reference confidence="0.999783375">
Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings ofACL.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.
Vctor M. S´anchez-Cartagena, Felipe S´anchez-
Martnez, and Juan Antonio P´erez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.
Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
</reference>
<page confidence="0.997534">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959899">
<title confidence="0.998419">Toward Statistical Machine Translation without Parallel Corpora</title>
<author confidence="0.989516">Alexandre Klementiev Ann Irvine Chris Callison-Burch David</author>
<affiliation confidence="0.9963525">Center for Language and Speech Johns Hopkins University</affiliation>
<abstract confidence="0.999131666666667">We estimate the parameters of a phrasebased statistical machine translation sysfrom instead of a corpus. We extend existing research on bilingual lexicon induction estimate and phrasal translation probabilities for MT-scale phrasetables. We propose a novel algorithm to estimate reordering probabilities from monolingual data. We report translation results for an end-to-end translation system using these monolingual features alone. Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Massimiliano Ciaramita</author>
<author>Keith Hall</author>
</authors>
<title>Gazpacho and summer rash: lexical relationships from temporal patterns of web search queries.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16037" citStr="Alfonseca et al., 2009" startWordPosition="2527" endWordPosition="2530">x (log(n/nk) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calculate the temporal similari</context>
</contexts>
<marker>Alfonseca, Ciaramita, Hall, 2009</marker>
<rawString>Enrique Alfonseca, Massimiliano Ciaramita, and Keith Hall. 2009. Gazpacho and summer rash: lexical relationships from temporal patterns of web search queries. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Simple effective decipherment via combinatorial optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011),</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="19566" citStr="Berg-Kirkpatrick and Klein (2011)" startWordPosition="3108" endWordPosition="3111">lignments across the two phrases. Because individual words are more frequent than multiword phrases, the accuracy of clex, tlex, and wlex tends to be higher than their phrasal equivalents (this is similar to the effect observed in Figure 2). Orthographic / phonetic similarity. The final lexical similarity feature that we incorporate is o(f, e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabil</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011), Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning bilingual lexicons using the visual similarity of labeled web images.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Bergsma, Van Durme, 2011</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2011. Learning bilingual lexicons using the visual similarity of labeled web images. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
<author>Paul Poossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In 12th International Conference on Computational Linguistics (CoLing-1988).</booktitle>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Poossin, 1988</marker>
<rawString>Peter Brown, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Frederick Jelinek, Robert Mercer, and Paul Poossin. 1988. A statistical approach to language translation. In 12th International Conference on Computational Linguistics (CoLing-1988).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1428" citStr="Brown et al., 1993" startWordPosition="202" endWordPosition="205">nd a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features. 1 Introduction The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (Brown et al., 1993). However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system (Germann, 2001). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale</context>
<context position="5048" citStr="Brown et al., 1993" startWordPosition="761" endWordPosition="764">n 3. This approach allows us to replace expensive/rare bilingual parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. aufrgund Wieviel sollte man verdienen seines Profils in Facebook How much should you charge for your Facebook profile m m m d d a d 2.1 Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phra</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="26920" citStr="Callison-Burch et al., 2010" startWordPosition="4313" endWordPosition="4316">ver the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4Specifcially, news-test2008 plus news-syscomb2009 for dev and newstest2009 for test. was re-run for every experiment. We estimate the parameters of our model from two sets of monolingual data, detailed in Table 1: • First, we treat the two sides of the Europarl parallel corpus as independent, monolingual corpora. Haghighi et al. (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. • Next, we estimate the fe</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Steve Klein</author>
<author>David Miller</author>
<author>Michael Steinbaum</author>
<author>Tomer Grassiany</author>
<author>Jochen Frey</author>
</authors>
<title>Context-based machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="34562" citStr="Carbonell et al. (2006)" startWordPosition="5565" endWordPosition="5568"> gain of 14.8 BLEU points, or over 82% of the BLEU point loss that occurred when we dropped all features from the phrase table. This is equivalent to training the standard system on a bitext with roughly 60,000 lines or nearly 2 million words (learning curve omitted for space). Finally, we supplement the standard bilingually estimated model parameters with our monolingual features (BM/B), and we see a 1.5 BLEU point increase over the standard model. Therefore, our monolingually estimated scores capture some novel information not contained in the standard feature set. 6 Additional Related Work Carbonell et al. (2006) described a data-driven MT system that used no parallel text. It produced translation lattices using a bilingual dictionary and scored them using an n-gram language model. Their method has no notion of translation similarity aside from a bilingual dictionary. Similarly, S´anchez-Cartagena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual tex</context>
</contexts>
<marker>Carbonell, Klein, Miller, Steinbaum, Grassiany, Frey, 2006</marker>
<rawString>Jaime Carbonell, Steve Klein, David Miller, Michael Steinbaum, Tomer Grassiany, and Jochen Frey. 2006. Context-based machine translation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Exploiting n-best hypotheses for SMT selfenhancement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>157--160</pages>
<contexts>
<context position="35921" citStr="Chen et al., 2008" startWordPosition="5781" endWordPosition="5784">been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-e</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for SMT selfenhancement. In Proceedings of ACL/HLT, pages 157–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5160" citStr="Chiang, 2005" startWordPosition="779" endWordPosition="780">pora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. aufrgund Wieviel sollte man verdienen seines Profils in Facebook How much should you charge for your Facebook profile m m m d d a d 2.1 Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pai</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/CoLing.</booktitle>
<contexts>
<context position="8974" citStr="Fung and Yee, 1998" startWordPosition="1372" endWordPosition="1375">representing an unknown German word by counting its co-occurrence with all the other words in the German monolingual corpus, (2) projecting this German vector onto the vector space of English using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for English words that were constructed using the English monolingual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning trans</context>
<context position="15290" citStr="Fung and Yee (1998)" startWordPosition="2391" endWordPosition="2394">. After the values have been computed, a contextual vector f is projected onto the English vector space using translations in a seed bilingual dictionary to map the component values into their appropriate English vector positions. This sparse projected vector is compared to the vectors representing all English phrases e. Each phrase pair in the phrase table is assigned a contextual similarity score c(f, e) based on the similarity between e and the projection of f. Various means of computing the component values and vector similarity measures have been proposed in literature (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of f’s contextual vector as follows: wk = nf,k x (log(n/nk) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in tw</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of ACL/CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences.</title>
<date>2009</date>
<booktitle>In Thirteenth Conference On Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="9343" citStr="Garera et al. (2009)" startWordPosition="1434" endWordPosition="1437">gual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel training data, it is surprising that bilingual lexicon induction has not been used for SMT before now. There are several open questions</context>
</contexts>
<marker>Garera, Callison-Burch, Yarowsky, 2009</marker>
<rawString>Nikesh Garera, Chris Callison-Burch, and David Yarowsky. 2009. Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. In Thirteenth Conference On Computational Natural Language Learning (CoNLL-2009), Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Building a statistical machine translation system from scratch: How much bang for the buck can we expect?</title>
<date>2001</date>
<booktitle>In ACL 2001 Workshop on Data-Driven Machine Translation,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1611" citStr="Germann, 2001" startWordPosition="233" endWordPosition="234">y estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features. 1 Introduction The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (Brown et al., 1993). However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system (Germann, 2001). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. We then introduce a novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of a phrase-based statistical model (Koe</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Building a statistical machine translation system from scratch: How much bang for the buck can we expect? In ACL 2001 Workshop on Data-Driven Machine Translation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="10548" citStr="Haghighi et al. (2008)" startWordPosition="1626" endWordPosition="1629">al open questions that make its applicability to SMT uncertain. Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 Corpus Frequency Figure 2: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists. nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al. (2008)). Although previous work reported high translation accuracy, it may be misleading to extrapolate the results to SMT, where it is necessary to translate a much larger set of words and phrases, including many low frequency items. In a preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus. Figure 2 shows the result for translations induced using contextual similarity (defined in Section 3.1). Unsurprisingly, frequent terms have a substantially better chance of being paired with a correct translation, with words that only oc</context>
<context position="27266" citStr="Haghighi et al. (2008)" startWordPosition="4366" endWordPosition="4369">rror rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4Specifcially, news-test2008 plus news-syscomb2009 for dev and newstest2009 for test. was re-run for every experiment. We estimate the parameters of our model from two sets of monolingual data, detailed in Table 1: • First, we treat the two sides of the Europarl parallel corpus as independent, monolingual corpora. Haghighi et al. (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. • Next, we estimate the features from truly monolingual corpora. To estimate the contextual and temporal similarity features, we use the Spanish and English Gigaword corpora.5 These corpora are substantially larger than the Europarl corpora, providing 27x as much Spanish and 67x as much English for contextual similarity, and 6x as many paired dates for temporal similari</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Mining key phrase translations from web corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="35468" citStr="Huang et al. (2005)" startWordPosition="5707" endWordPosition="5710">ena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Concl</context>
</contexts>
<marker>Huang, Zhang, Vogel, 2005</marker>
<rawString>Fei Huang, Ying Zhang, and Stephan Vogel. 2005. Mining key phrase translations from web corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Weakly supervised named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/Coling.</booktitle>
<contexts>
<context position="16012" citStr="Klementiev and Roth, 2006" startWordPosition="2523" endWordPosition="2526">ctor as follows: wk = nf,k x (log(n/nk) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calcul</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the ACL/Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="19900" citStr="Knight and Graehl, 1997" startWordPosition="3161" endWordPosition="3164">e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabilities 0 and lexical weighting probabilities w. Our seven similarity scores are not the only ones that could be incorporated into the translation model. Various other similarity scores can be computed depending on the available monolingual data and its associated metadata (see, e.g. Schafer and Yarowsky (2002)). 3.3 Reordering The re</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Unsupervised Lexical Acquisition.</booktitle>
<contexts>
<context position="9214" citStr="Koehn and Knight (2002)" startWordPosition="1410" endWordPosition="1413">culating the similarity of this sparse projected vector to vectors for English words that were constructed using the English monolingual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel traini</context>
<context position="10492" citStr="Koehn and Knight (2002)" startWordPosition="1615" endWordPosition="1618">ion has not been used for SMT before now. There are several open questions that make its applicability to SMT uncertain. Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 Corpus Frequency Figure 2: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists. nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al. (2008)). Although previous work reported high translation accuracy, it may be misleading to extrapolate the results to SMT, where it is necessary to translate a much larger set of words and phrases, including many low frequency items. In a preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus. Figure 2 shows the result for translations induced using contextual similarity (defined in Section 3.1). Unsurprisingly, frequent terms have a substantially better chance of being pa</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL Workshop on Unsupervised Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2227" citStr="Koehn et al., 2003" startWordPosition="325" endWordPosition="328">01). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. We then introduce a novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of a phrase-based statistical model (Koehn et al., 2003) estimated using these monolingual features. Most of the prior work on lexicon induction is motivated by the idea that it could be applied to machine translation but stops short of actually doing so. Lexicon induction holds the potential to create machine translation systems for languages which do not have extensive parallel corpora. Training would only require two large monolingual corpora and a small bilingual dictionary, if one is available. The idea is that intrinsic properties of monolingual data (possibly along with a handful of bilingual pairs to act as example mappings) can provide ind</context>
<context position="5121" citStr="Koehn et al., 2003" startWordPosition="772" endWordPosition="775"> training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. aufrgund Wieviel sollte man verdienen seines Profils in Facebook How much should you charge for your Facebook profile m m m d d a d 2.1 Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007 Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="5531" citStr="Koehn et al., 2007" startWordPosition="837" endWordPosition="840">es of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, 0</context>
<context position="26282" citStr="Koehn et al., 2007" startWordPosition="4208" endWordPosition="4211">ters of an SMT system from monolingual corpora. This allows us to compare our method against the normal bilingual training procedure. We expect bilingual training to result in higher translation quality because it is a more direct method for learning translation probabilities. We systematically remove different parameters from the standard phrase-based model, and then replace them with our monolingual equivalents. Our goal is to recover as much of the loss as possible for each of the deleted bilingual components. The standard phrase-based model that we use as our top-line is the Moses system (Koehn et al., 2007) trained over the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT sha</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL-2007 Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Machine Translation Summit.</booktitle>
<contexts>
<context position="26346" citStr="Koehn, 2005" startWordPosition="4221" endWordPosition="4222">re our method against the normal bilingual training procedure. We expect bilingual training to result in higher translation quality because it is a more direct method for learning translation probabilities. We systematically remove different parameters from the standard phrase-based model, and then replace them with our monolingual equivalents. Our goal is to recover as much of the loss as possible for each of the deleted bilingual components. The standard phrase-based model that we use as our top-line is the Moses system (Koehn et al., 2007) trained over the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4Specifcially, new</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="7029" citStr="Kumar and Byrne, 2004" startWordPosition="1063" endWordPosition="1066">dels are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ez|fj), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f) also has associated reordering parameters, po(orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Other features. Other typical features are n-gram language model scores and a phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases. These are not bilingually estimated, so we can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Local phrase reordering models for statistical machine translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Holger Schwenk</author>
<author>Christophe Servan</author>
<author>Sadaf Abdul-Rauf</author>
</authors>
<title>Investigations on translation model adaptation using monolingual data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>284--293</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="35997" citStr="Lambert et al., 2011" startWordPosition="5793" endWordPosition="5796">ons from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of BLEU loss that resulted from removi</context>
</contexts>
<marker>Lambert, Schwenk, Servan, Abdul-Rauf, 2011</marker>
<rawString>Patrik Lambert, Holger Schwenk, Christophe Servan, and Sadaf Abdul-Rauf. 2011. Investigations on translation model adaptation using monolingual data. In Proceedings of the Workshop on Statistical Machine Translation, pages 284–293, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<date>2009</date>
<contexts>
<context position="18316" citStr="Mimno et al., 2009" startWordPosition="2912" endWordPosition="2915">normalized topic signatures. In our experiments, we use interlingual links between Wikipedia articles to estimate topic similarity. We treat each linked article pair as a topic and collect counts for each phrase across all articles in its corresponding language. Thus, the size of a phrase topic signature is the number of article pairs with interlingual links in Wikipedia, and each component contains the number of times the phrase appears in (the appropriate side of) the corresponding pair. Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al., 2009), but it is scalable to full bilingual lexicon induction. 3.2 Lexical similarity features In addition to the three phrase similarity features used in our model – c(f, e), t(f, e) and w(f, e) – we include four additional lexical similarity features for each of phrase pair. The first three lexical features clex(f, e), tlex(f, e) and wlex(f, e) are the lexical equivalents of the phrase-level contextual, temporal and wikipedia topic similarity scores. They score the similarity of individual words within the phrases. To compute these lexical similarity features, we average similarity scores over al</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009.</rawString>
</citation>
<citation valid="false">
<title>Polylingual topic models.</title>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker></marker>
<rawString>Polylingual topic models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from non-parallel corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/Coling.</booktitle>
<contexts>
<context position="35690" citStr="Munteanu and Marcu, 2006" startWordPosition="5744" endWordPosition="5747">parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obt</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from non-parallel corpora. In Proceedings of the ACL/Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5242" citStr="Och and Ney, 2003" startWordPosition="790" endWordPosition="793"> set, which are comparatively plentiful/inexpensive. aufrgund Wieviel sollte man verdienen seines Profils in Facebook How much should you charge for your Facebook profile m m m d d a d 2.1 Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f) that are consistent with the word alignments. In this paper we assume th</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="5732" citStr="Och and Ney, 2004" startWordPosition="869" endWordPosition="872">, 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, 0(e|f) and 0(f|e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates 0 for phrase pairs with sparse counts, lexical weighting FFs are used to</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen.</institution>
<contexts>
<context position="5100" citStr="Och, 2002" startWordPosition="770" endWordPosition="771">al parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. aufrgund Wieviel sollte man verdienen seines Profils in Facebook How much should you charge for your Facebook profile m m m d d a d 2.1 Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Ti</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Joseph Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7428" citStr="Och, 2003" startWordPosition="1130" endWordPosition="1131">s, po(orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Other features. Other typical features are n-gram language model scores and a phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases. These are not bilingually estimated, so we can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 2.2 Bilingual lexicon induction for SMT Bilingual lexicon induction describes the class of algorithms that attempt to learn translations from monolingual corpora. Rapp (1995) was the first to propose using non-parallel texts to learn the translations of words. Using large, unrelated English </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1875" citStr="Rapp (1995)" startWordPosition="276" endWordPosition="277">able features. 1 Introduction The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (Brown et al., 1993). However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system (Germann, 2001). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. We then introduce a novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of a phrase-based statistical model (Koehn et al., 2003) estimated using these monolingual features. Most of the prior work on lexicon induction is motivated by the idea that it could be applied to machine translation but stops short of actually doing so. Lexicon induction holds the potential to create </context>
<context position="7910" citStr="Rapp (1995)" startWordPosition="1205" endWordPosition="1206">ation. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 2.2 Bilingual lexicon induction for SMT Bilingual lexicon induction describes the class of algorithms that attempt to learn translations from monolingual corpora. Rapp (1995) was the first to propose using non-parallel texts to learn the translations of words. Using large, unrelated English and German corpora (with 163m and 135m words) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. His algorithm worked by (1) building a context vector representing an unknown German word by counting its co-occurrence with all the other words in the German monolingual corpus, (2) projecting this German vec</context>
<context position="10438" citStr="Rapp (1995)" startWordPosition="1608" endWordPosition="1609">s surprising that bilingual lexicon induction has not been used for SMT before now. There are several open questions that make its applicability to SMT uncertain. Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 Corpus Frequency Figure 2: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists. nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al. (2008)). Although previous work reported high translation accuracy, it may be misleading to extrapolate the results to SMT, where it is necessary to translate a much larger set of words and phrases, including many low frequency items. In a preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus. Figure 2 shows the result for translations induced using contextual similarity (defined in Section 3.1). Unsurprisingly, frequen</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8153" citStr="Rapp (1999)" startWordPosition="1244" endWordPosition="1245">for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 2.2 Bilingual lexicon induction for SMT Bilingual lexicon induction describes the class of algorithms that attempt to learn translations from monolingual corpora. Rapp (1995) was the first to propose using non-parallel texts to learn the translations of words. Using large, unrelated English and German corpora (with 163m and 135m words) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. His algorithm worked by (1) building a context vector representing an unknown German word by counting its co-occurrence with all the other words in the German monolingual corpus, (2) projecting this German vector onto the vector space of English using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for English words that were constructed using the English monolingual corpus, and (4) outputtin</context>
<context position="13816" citStr="Rapp (1999)" startWordPosition="2159" endWordPosition="2160">be used to replace bilingual estimation of phrase- and lexical-translation probabilities, making a significant step towards SMT without parallel corpora. 3 Monolingual Parameter Estimation We use bilingual lexicon induction methods to estimate the parameters of a phrase-based translation model from monolingual data. Instead of scores estimated from bilingual parallel data, we make use of cues present in monolingual data to provide multiple orthogonal estimates of similarity between a pair of phrases. 3.1 Phrasal similarity features Contextual similarity. We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. More formally, assume that (s1, s2, ... sN) and (t1, t2,... tM) are (arbitrarily indexed) source and target vocabularies, respectively. A source phrase f is represented with an Figure 4: Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature. N- and target phrase e with an M-dimensional </context>
<context position="15269" citStr="Rapp (1999)" startWordPosition="2389" endWordPosition="2390">ngual corpora. After the values have been computed, a contextual vector f is projected onto the English vector space using translations in a seed bilingual dictionary to map the component values into their appropriate English vector positions. This sparse projected vector is compared to the vectors representing all English phrases e. Each phrase pair in the phrase table is assigned a contextual similarity score c(f, e) based on the similarity between e and the projection of f. Various means of computing the component values and vector similarity measures have been proposed in literature (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of f’s contextual vector as follows: wk = nf,k x (log(n/nk) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual simi</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadaf Abdul Rauf</author>
<author>Holger Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve SMT performance.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="35974" citStr="Rauf and Schwenk, 2009" startWordPosition="5789" endWordPosition="5792">rk on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of BLEU loss th</context>
</contexts>
<marker>Rauf, Schwenk, 2009</marker>
<rawString>Sadaf Abdul Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="35048" citStr="Ravi and Knight (2011)" startWordPosition="5639" endWordPosition="5642">mated scores capture some novel information not contained in the standard feature set. 6 Additional Related Work Carbonell et al. (2006) described a data-driven MT system that used no parallel text. It produced translation lattices using a bilingual dictionary and scored them using an n-gram language model. Their method has no notion of translation similarity aside from a bilingual dictionary. Similarly, S´anchez-Cartagena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corp</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vctor M S´anchez-Cartagena</author>
<author>Felipe S´anchezMartnez</author>
<author>Juan Antonio P´erez-Ortiz</author>
</authors>
<title>Integrating shallow-transfer rules into phrase-based statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the XIII Machine Translation Summit.</booktitle>
<marker>S´anchez-Cartagena, S´anchezMartnez, P´erez-Ortiz, 2011</marker>
<rawString>Vctor M. S´anchez-Cartagena, Felipe S´anchezMartnez, and Juan Antonio P´erez-Ortiz. 2011. Integrating shallow-transfer rules into phrase-based statistical machine translation. In Proceedings of the XIII Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="9112" citStr="Schafer and Yarowsky (2002)" startWordPosition="1394" endWordPosition="1397">rojecting this German vector onto the vector space of English using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for English words that were constructed using the English monolingual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical mac</context>
<context position="15985" citStr="Schafer and Yarowsky, 2002" startWordPosition="2519" endWordPosition="2522">mponent of f’s contextual vector as follows: wk = nf,k x (log(n/nk) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other</context>
<context position="20476" citStr="Schafer and Yarowsky (2002)" startWordPosition="3245" endWordPosition="3248">tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabilities 0 and lexical weighting probabilities w. Our seven similarity scores are not the only ones that could be incorporated into the translation model. Various other similarity scores can be computed depending on the available monolingual data and its associated metadata (see, e.g. Schafer and Yarowsky (2002)). 3.3 Reordering The remaining component of the phrase-based SMT model is the reordering model. We introduce a novel algorithm for estimating 134 Input: Source and target phrases f and e, Source and target monolingual corpora Cf and Ce, Phrase table pairs T = {(f(i), e(i))}Ni=1. Output: Orientation features (pm, ps, pd). Sf +— sentences containing fin Cf; Se +— sentences containing e in Ce; (Bf, �, �) � CollectOccurs(f, UN i=1f(i), Sf); (Be, Ae, De) i— CollectOccurs(e, UNi=1e(i), Se); cm = cs = cd = 0; foreach unique f&apos; in Bf do foreach translation e&apos; of f&apos; in T do cm = cm + #BQ(e&apos;); cs = cs </context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean Senellart</author>
</authors>
<title>Translation model adaptation for an Arabic/French news translation system by lightly-supervised training.</title>
<date>2009</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="35950" citStr="Schwenk and Senellart, 2009" startWordPosition="5785" endWordPosition="5788">ader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover </context>
</contexts>
<marker>Schwenk, Senellart, 2009</marker>
<rawString>Holger Schwenk and Jean Senellart. 2009. Translation model adaptation for an Arabic/French news translation system by lightly-supervised training. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on large-scale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="35902" citStr="Schwenk, 2008" startWordPosition="5779" endWordPosition="5780">method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize </context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on large-scale lightly-supervised training for statistical machine translation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="35710" citStr="Smith et al., 2010" startWordPosition="5748" endWordPosition="5751">a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new language</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="7005" citStr="Tillman, 2004" startWordPosition="1061" endWordPosition="1062"> phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ez|fj), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f) also has associated reordering parameters, po(orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Other features. Other typical features are n-gram language model scores and a phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases. These are not bilingually estimated, so we can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A projection extension algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5712" citStr="Tillmann, 2003" startWordPosition="867" endWordPosition="868">02; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, 0(e|f) and 0(f|e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates 0 for phrase pairs with sparse counts, lexical weigh</context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A projection extension algorithm for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<contexts>
<context position="35735" citStr="Uszkoreit et al., 2010" startWordPosition="5752" endWordPosition="5755">and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluat</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Effective phrase translation extraction from alignment models.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5696" citStr="Venugopal et al., 2003" startWordPosition="863" endWordPosition="866">ng phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, 0(e|f) and 0(f|e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates 0 for phrase pairs with sparse count</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2003. Effective phrase translation extraction from alignment models. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>