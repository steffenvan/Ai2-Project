<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000181">
<title confidence="0.99107">
Guiding Semi-Supervision with Constraint-Driven Learning
</title>
<author confidence="0.983834">
Ming-Wei Chang Lev Ratinov Dan Roth
</author>
<affiliation confidence="0.9907375">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.73253">
Urbana, IL 61801
</address>
<email confidence="0.996683">
{mchang21, ratinov2, danr}@uiuc.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999731">
Over the last few years, two of the main
research directions in machine learning of
natural language processing have been the
study of semi-supervised learning algo-
rithms as a way to train classifiers when the
labeled data is scarce, and the study of ways
to exploit knowledge and global information
in structured learning tasks. In this paper,
we suggest a method for incorporating do-
main knowledge in semi-supervised learn-
ing algorithms. Our novel framework unifies
and can exploit several kinds of task specific
constraints. The experimental results pre-
sented in the information extraction domain
demonstrate that applying constraints helps
the model to generate better feedback during
learning, and hence the framework allows
for high performance learning with signif-
icantly less training data than was possible
before on these tasks.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999643295454546">
Natural Language Processing (NLP) systems typi-
cally require large amounts of knowledge to achieve
good performance. Acquiring labeled data is a dif-
ficult and expensive task. Therefore, an increasing
attention has been recently given to semi-supervised
learning, where large amounts of unlabeled data are
used to improve the models learned from a small
training set (Collins and Singer, 1999; Thelen and
Riloff, 2002). The hope is that semi-supervised or
even unsupervised approaches, when given enough
knowledge about the structure of the problem, will
be competitive with the supervised models trained
on large training sets. However, in the general
case, semi-supervised approaches give mixed re-
sults, and sometimes even degrade the model per-
formance (Nigam et al., 2000). In many cases, im-
proving semi-supervised models was done by seed-
ing these models with domain information taken
from dictionaries or ontology (Cohen and Sarawagi,
2004; Collins and Singer, 1999; Haghighi and Klein,
2006; Thelen and Riloff, 2002). On the other hand,
in the supervised setting, it has been shown that
incorporating domain and problem specific struc-
tured information can result in substantial improve-
ments (Toutanova et al., 2005; Roth and Yih, 2005).
This paper proposes a novel constraints-based
learning protocol for guiding semi-supervised learn-
ing. We develop a formalism for constraints-based
learning that unifies several kinds of constraints:
unary, dictionary based and n-ary constraints, which
encode structural information and interdependencies
among possible labels. One advantage of our for-
malism is that it allows capturing different levels of
constraint violation. Our protocol can be used in
the presence of any learning model, including those
that acquire additional statistical constraints from
observed data while learning (see Section 5. In the
experimental part of this paper we use HMMs as the
underlying model, and exhibit significant reduction
in the number of training examples required in two
information extraction problems.
As is often the case in semi-supervised learning,
the algorithm can be viewed as a process that im-
proves the model by generating feedback through
</bodyText>
<page confidence="0.952044">
280
</page>
<note confidence="0.925613">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280–287,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.994882625">
labeling unlabeled examples. Our algorithm pushes
this intuition further, in that the use of constraints
allows us to better exploit domain information as a
way to label, along with the current learned model,
unlabeled examples. Given a small amount of la-
beled data and a large unlabeled pool, our frame-
work initializes the model with the labeled data and
then repeatedly:
</bodyText>
<listItem confidence="0.84322">
(1) Uses constraints and the learned model to label
the instances in the pool.
(2) Updates the model by newly labeled data.
</listItem>
<bodyText confidence="0.999871111111111">
This way, we can generate better “training” ex-
amples during the semi-supervised learning process.
The core of our approach, (1), is described in Sec-
tion 5. The task is described in Section 3 and the
Experimental study in Section 6. It is shown there
that the improvement on the training examples via
the constraints indeed boosts the learned model and
the proposed method significantly outperforms the
traditional semi-supervised framework.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999830396226415">
In the semi-supervised domain there are two main
approaches for injecting domain specific knowledge.
One is using the prior knowledge to accurately tailor
the generative model so that it captures the domain
structure. For example, (Grenager et al., 2005) pro-
poses Diagonal Transition Models for sequential la-
beling tasks where neighboring words tend to have
the same labels. This is done by constraining the
HMM transition matrix, which can be done also for
other models, such as CRF. However (Roth and Yih,
2005) showed that reasoning with more expressive,
non-sequential constraints can improve the perfor-
mance for the supervised protocol.
A second approach has been to use a small high-
accuracy set of labeled tokens as a way to seed and
bootstrap the semi-supervised learning. This was
used, for example, by (Thelen and Riloff, 2002;
Collins and Singer, 1999) in information extraction,
and by (Smith and Eisner, 2005) in POS tagging.
(Haghighi and Klein, 2006) extends the dictionary-
based approach to sequential labeling tasks by prop-
agating the information given in the seeds with con-
textual word similarity. This follows a conceptually
similar approach by (Cohen and Sarawagi, 2004)
that uses a large named-entity dictionary, where the
similarity between the candidate named-entity and
its matching prototype in the dictionary is encoded
as a feature in a supervised classifier.
In our framework, dictionary lookup approaches
are viewed as unary constraints on the output states.
We extend these kinds of constraints and allow for
more general, n-ary constraints.
In the supervised learning setting it has been es-
tablished that incorporating global information can
significantly improve performance on several NLP
tasks, including information extraction and semantic
role labeling. (Punyakanok et al., 2005; Toutanova
et al., 2005; Roth and Yih, 2005). Our formalism
is most related to this last work. But, we develop a
semi-supervised learning protocol based on this for-
malism. We also make use of soft constraints and,
furthermore, extend the notion of soft constraints to
account for multiple levels of constraints’ violation.
Conceptually, although not technically, the most re-
lated work to ours is (Shen et al., 2005) that, in
a somewhat ad-hoc manner uses soft constraints to
guide an unsupervised model that was crafted for
mention tracking. To the best of our knowledge,
we are the first to suggest a general semi-supervised
protocol that is driven by soft constraints.
We propose learning with constraints - a frame-
work that combines the approaches described above
in a unified and intuitive way.
</bodyText>
<sectionHeader confidence="0.858455" genericHeader="method">
3 Tasks, Examples and Datasets
</sectionHeader>
<bodyText confidence="0.999713823529412">
In Section 4 we will develop a general framework
for semi-supervised learning with constraints. How-
ever, it is useful to illustrate the ideas on concrete
problems. Therefore, in this section, we give a brief
introduction to the two domains on which we tested
our algorithms. We study two information extrac-
tion problems in each of which, given text, a set of
pre-defined fields is to be identified. Since the fields
are typically related and interdependent, these kinds
of applications provide a good test case for an ap-
proach like ours.1
The first task is to identify fields from citations
(McCallum et al., 2000) . The data originally in-
cluded 500 labeled references, and was later ex-
tended with 5,000 unannotated citations collected
from papers found on the Internet (Grenager et al.,
2005). Given a citation, the task is to extract the
</bodyText>
<footnote confidence="0.9584135">
&apos;The data for both problems is available at:
http://www.stanford.edu/ grenager/data/unsupie.tgz
</footnote>
<page confidence="0.996934">
281
</page>
<figure confidence="0.9968914">
(a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [
TECH-REPORT PhD thesis, ] [ INSTITUTION DIKU , University of Copenhagen, ] [ DATE May 1994 . ]
(b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE
Programming language ] [ TECH-REPORT. PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE
1994 . ]
</figure>
<figureCaption confidence="0.994315">
Figure 1: Error analysis of a HMM model. The labels are annotated by underline and are to the right of
</figureCaption>
<bodyText confidence="0.979227575">
each open bracket. The correct assignment was shown in (a). While the predicted label assignment (b) is
generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues
for state transitions. The constraint “Fields cannot end with stop words (such as “the”)” may be also good.
fields that appear in the given reference. See Fig. 1.
There are 13 possible fields including author, title,
location, etc.
To gain an insight to how the constraints can guide
semi-supervised learning, assume that the sentence
shown in Figure 1 appears in the unlabeled data
pool. Part (a) of the figure shows the correct la-
beled assignment and part (b) shows the assignment
labeled by a HMM trained on 30 labels. However,
if we apply the constraint that state transition can
occur only on punctuation marks, the same HMM
model parameters will result in the correct labeling
(a). Therefore, by adding the improved labeled as-
signment we can generate better training samples
during semi-supervised learning. In fact, the punc-
tuation marks are only some of the constraints that
can be applied to this problem. The set of constraints
we used in our experiments appears in Table 1. Note
that some of the constraints are non-local and are
very intuitive for people, yet it is very difficult to
inject this knowledge into most models.
The second problem we consider is extracting
fields from advertisements (Grenager et al., 2005).
The dataset consists of 8,767 advertisements for
apartment rentals in the San Francisco Bay Area
downloaded in June 2004 from the Craigslist web-
site. In the dataset, only 302 entries have been la-
beled with 12 fields, including size, rent, neighbor-
hood, features, and so on. The data was prepro-
cessed using regular expressions for phone numbers,
email addresses and URLs. The list of the con-
straints for this domain is given in Table 1. We im-
plement some global constraints and include unary
constraints which were largely imported from the
list of seed words used in (Haghighi and Klein,
2006). We slightly modified the seedwords due to
difference in preprocessing.
</bodyText>
<page confidence="0.987546">
282
</page>
<sectionHeader confidence="0.986941" genericHeader="method">
4 Notation and Definitions
</sectionHeader>
<subsectionHeader confidence="0.641149">
Consider a structured classification problem, where
</subsectionHeader>
<bodyText confidence="0.998300076923077">
given an input sequence x = (x1,... , xN), the task
is to find the best assignment to the output variables
y = (y1, ... , yM). We denote X to be the space of
the possible input sequences and Y to be the set of
possible output sequences.
We define a structured output classifier as a func-
tion h : X  Y that uses a global scoring function
f : X × Y  R to assign scores to each possible in-
put/output pair. Given an input x, a desired function
f will assign the correct output y the highest score
among all the possible outputs. The global scoring
function is often decomposed as a weighted sum of
feature functions,
</bodyText>
<equation confidence="0.997766666666667">
M
f(x, y) = Aifi(x, y) = A · F(x, y).
i=1
</equation>
<bodyText confidence="0.999869333333333">
This decomposition applies both to discriminative
linear models and to generative models such as
HMMs and CRFs, in which case the linear sum
corresponds to log likelihood assigned to the in-
put/output pair by the model (for details see (Roth,
1999) for the classification case and (Collins, 2002)
for the structured case). Even when not dictated by
the model, the feature functions fi(x, y) used are
local to allow inference tractability. Local feature
function can capture some context for each input or
output variable, yet it is very limited to allow dy-
namic programming decoding during inference.
Now, consider a scenario where we have a set
of constraints C1, ... , CK. We define a constraint
C : X × Y  {0, 1} as a function that indicates
whether the input/output sequence violates some de-
sired properties. When the constraints are hard, the
solution is given by
</bodyText>
<figure confidence="0.954434818181818">
argmax A · F(x, y),
yE1C(.)
(a)-Citations
1) Each field must be a consecutive list of words, and can
appear at most once in a citation.
2) State transitions must occur on punctuation marks.
3) The citation can only start with author or editor.
4) The words pp., pages correspond to PAGE.
5) Four digits starting with 20xx and 19xx are DATE.
6) Quotations can appear only in titles.
7) The words note, submitted, appear are NOTE.
8) The words CA, Australia, NY are LOCATION.
9) The words tech, technical are TECH REPORT.
10) The words proc, journal, proceedings, ACM are JOUR-
NAL or BOOKTITLE.
11) The words ed, editors correspond to EDITOR.
(b)-Advertisements
1) State transitions can occur only on punctuation marks or
the newline symbol.
2) Each field must be at least 3 words long.
3) The words laundry, kitchen, parking are FEATURES.
4) The words sq, ft, bdrm are SIZE.
5) The word $, *MONEY* are RENT.
6) The words close, near, shopping are NEIGHBORHOOD.
7) The words laundry kitchen, parking are FEATURES.
8) The (normalized) words phone, email are CONTACT.
9) The words immediately, begin, cheaper are AVAILABLE.
10) The words roommates, respectful, drama are ROOM-
MATES.
11) The words smoking, dogs, cats are RESTRICTIONS.
12) The word http, image, link are PHOTOS.
13) The words address, carlmont, st, cross are ADDRESS.
14) The words utilities, pays, electricity are UTILITIES.
</figure>
<tableCaption confidence="0.989202">
Table 1: The list of constraints for extracting fields
</tableCaption>
<bodyText confidence="0.981202045454545">
from citations and advertisements. Some constraints
(represented in the first block of each domain) are
global and are relatively difficult to inject into tradi-
tional models. While all the constraints hold for the
vast majority of the data, some of them are violated
by some correct labeled assignments.
where 1C(x) is a subset of Y for which all Ci as-
sign the value 1 for the given (x, y).
When the constraints are soft, we want to in-
cur some penalty for their violation. Moreover, we
want to incorporate into our cost function a mea-
sure for the amount of violation incurred by vi-
olating the constraint. A generic way to capture
this intuition is to introduce a distance function
d(y, 1Ci(x)) between the space of outputs that re-
spect the constraint,1Ci(x), and the given output se-
quence y. One possible way to implement this dis-
tance function is as the minimal Hamming distance
to a sequence that respects the constraint Ci, that is:
d(y, 1Ci(x)) = min(y&apos;E1C(.)) H(y, y&apos;). If the penalty
for violating the soft constraint Ci is pi, we write the
score function as:
</bodyText>
<equation confidence="0.998353666666667">
K
argmax A · F(x, y) − pid(y, 1Ci(x)) (1)
y i=1
</equation>
<bodyText confidence="0.999957777777778">
We refer to d(y, 1C(x)) as the valuation of the
constraint C on (x, y). The intuition behind (1) is as
follows. Instead of merely maximizing the model’s
likelihood, we also want to bias the model using
some knowledge. The first term of (1) is used to
learn from data. The second term biases the mode
by using the knowledge encoded in the constraints.
Note that we do not normalize our objective function
to be a true probability distribution.
</bodyText>
<sectionHeader confidence="0.56889" genericHeader="method">
5 Learning and Inference with Constraints
</sectionHeader>
<bodyText confidence="0.999974205882353">
In this section we present a new constraint-driven
learning algorithm (CODL) for using constraints to
guide semi-supervised learning. The task is to learn
the parameter vector A by using the new objective
function (1). While our formulation allows us to
train also the coefficients of the constraints valua-
tion, pi, we choose not to do it, since we view this as
a way to bias (or enforce) the prior knowledge into
the learned model, rather than allowing the data to
brush it away. Our experiments demonstrate that the
proposed approach is robust to inaccurate approxi-
mation of the prior knowledge (assigning the same
penalty to all the pi ).
We note that in the presence of constraints, the
inference procedure (for finding the output y that
maximizes the cost function) is usually done with
search techniques (rather than Viterbi decoding,
see (Toutanova et al., 2005; Roth and Yih, 2005) for
a discussion), we chose beamsearch decoding.
The semi-supervised learning with constraints is
done with an EM-like procedure. We initialize the
model with traditional supervised learning (ignoring
the constraints) on a small labeled set. Given an un-
labeled set U, in the estimation step, the traditional
EM algorithm assigns a distribution over labeled as-
signments Y of each x E U, and in the maximization
step, the set of model parameters is learned from the
distributions assigned in the estimation step.
However, in the presence of constraints, assigning
the complete distributions in the estimation step is
infeasible since the constraints reshape the distribu-
tion in an arbitrary way. As in existing methods for
training a model by maximizing a linear cost func-
tion (maximize likelihood or discriminative maxi-
</bodyText>
<page confidence="0.992014">
283
</page>
<bodyText confidence="0.999968895833333">
mization), the distribution over y is represented as
the set of scores assigned to it; rather than consid-
ering the score assigned to all y&apos;s, we truncate the
distribution to the top K assignments as returned
by the search. Given a set of K top assignments
yi, , yK, we approximate the estimation step by
assigning uniform probability to the top K candi-
dates, and zero to the other output sequences. We
denote this algorithm top-K hard EM. In this pa-
per, we use beamsearch to generate K candidates
according to (1).
Our training algorithm is summarized in Figure 2.
Several things about the algorithm should be clari-
fied: the Top-K-Inference procedure in line 7, the
learning procedure in line 9, and the new parameter
estimation in line 9.
The Top-K-Inference is a procedure that returns
the K labeled assignments that maximize the new
objective function (1). In our case we used the top-
K elements in the beam, but this could be applied
to any other inference procedure. The fact that the
constraints are used in the inference procedure (in
particular, for generating new training examples) al-
lows us to use a learning algorithm that ignores the
constraints, which is a lot more efficient (although
algorithms that do take the constraints into account
can be used too). We used maximum likelihood es-
timation of A but, in general, perceptron or quasi-
Newton can also be used.
It is known that traditional semi-supervised train-
ing can degrade the learned model’s performance.
(Nigam et al., 2000) has suggested to balance the
contribution of labeled and unlabeled data to the pa-
rameters. The intuition is that when iteratively esti-
mating the parameters with EM, we disallow the pa-
rameters to drift too far from the supervised model.
The parameter re-estimation in line 9, uses a similar
intuition, but instead of weighting data instances, we
introduced a smoothing parameter -y which controls
the convex combination of models induced by the la-
beled and the unlabeled data. Unlike the technique
mentioned above which focuses on naive Bayes, our
method allows us to weight linear models generated
by different learning algorithms.
Another way to look the algorithm is from the
self-training perspective (McClosky et al., 2006).
Similarly to self-training, we use the current model
to generate new training examples from the unla-
</bodyText>
<figure confidence="0.46069625">
Input:
Cycles: learning cycles
Tr = {x, y}: labeled training set.
U: unlabeled dataset
F: set of feature functions.
{ρi}: set of penalties.
{Ci}: set of constraints.
γ: balancing parameter with the supervised model.
learn(Tr, F): supervised learning algorithm
Top-K-Inference:
returns top-K labeled scored by the cost function (1)
CODL:
</figure>
<listItem confidence="0.914593888888889">
1. Initialize A0 = learn(Tr, F).
2. A = A0.
3. For Cycles iterations do:
4. T = φ
5. For each x ∈ U
6. {(x, y1), . . . , (x, yK)} =
7. Top-K-Inference(x, A, F, {Ci}, {ρi})
8. T = T ∪ {(x, y1), ... , (x, yK)}
9. A = γA0 + (1 − γ)learn(T, F)
</listItem>
<figureCaption confidence="0.574544">
Figure 2: COnstraint Driven Learning (CODL). In
</figureCaption>
<bodyText confidence="0.998701035714285">
Top-K-Inference, we use beamsearch to find the K-
best solution according to Eq. (1).
beled set. However, there are two important differ-
ences. One is that in self-training, once an unlabeled
sample was labeled, it is never labeled again. In
our case all the samples are relabeled in each iter-
ation. In self-training it is often the case that only
high-confidence samples are added to the labeled
data pool. While we include all the samples in the
training pool, we could also limit ourselves to the
high-confidence samples. The second difference is
that each unlabeled example generates K labeled in-
stances. The case of one iteration of top-1 hard EM
is equivalent to self training, where all the unlabeled
samples are added to the labeled pool.
There are several possible benefits to using K &gt; 1
samples. (1) It effectively increases the training set
by a factor of K (albeit by somewhat noisy exam-
ples). In the structured scenario, each of the top-K
assignments is likely to have some good components
so generating top-K assignments helps leveraging
the noise. (2) Given an assignment that does not sat-
isfy some constraints, using top-K allows for mul-
tiple ways to correct it. For example, consider the
output 11101000 with the constraint that it should
belong to the language 1*0*. If the two top scoring
corrections are 11111000 and 11100000, consider-
ing only one of those can negatively bias the model.
</bodyText>
<page confidence="0.998024">
284
</page>
<sectionHeader confidence="0.994815" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999989414634147">
In this section, we present empirical results of our
algorithms on two domains: citations and adver-
tisements. Both problems are modeled with a sim-
ple token-based HMM. We stress that token-based
HMM cannot represent many of our constraints. The
function d(y, 1C(x)) used is an approximation of a
Hamming distance function, discussed in Section 7.
For both domains, and all the experiments, -y was
set to 0.1. The constraints violation penalty p is set
to − log 10−4 and − log 10−1 for citations and ad-
vertisements, resp.2 Note that all constraints share
the same penalty. The number of semi-supervised
training cycles (line 3 of Figure 2) was set to 5. The
constraints for the two domains are listed in Table 1.
We trained models on training sets of size vary-
ing from 5 to 300 for the citations and from 5 to
100 for the advertisements. Additionally, in all the
semi-supervised experiments, 1000 unlabeled exam-
ples are used. We report token-based3 accuracy on
100 held-out examples (which do not overlap neither
with the training nor with the unlabeled data). We
ran 5 experiments in each setting, randomly choos-
ing the training set. The results reported below are
the averages over these 5 runs.
To verify our claims we implemented several
baselines. The first baseline is the supervised learn-
ing protocol denoted by sup. The second baseline
was a traditional top-1 Hard EM also known as
truncated EM4 (denoted by H for Hard). In the third
baseline, denoted H&amp;W, we balanced the weight
of the supervised and unsupervised models as de-
scribed in line 9 of Figure 2. We compare these base-
lines to our proposed protocol, H&amp;W&amp;C, where we
added the constraints to guide the H&amp;W protocol.
We experimented with two flavors of the algorithm:
the top-1 and the top-K version. In the top-K ver-
sion, the algorithm uses K-best predictions (K=50)
for each instance in order to update the model as de-
scribed in Figure 2.
The experimental results for both domains are in
given Table 2. As hypothesized, hard EM sometimes
</bodyText>
<footnote confidence="0.660179285714286">
2The guiding intuition is that AF(x, y) corresponds to a log-
likelihood of a HMM model and p to a crude estimation of the
log probability that a constraint does not hold. p was tuned on
a development set and kept fixed in all experiments.
3Each token (word or punctuation mark) is assigned a state.
4We also experimented with (soft) EM without constraints,
but the results were generally worse.
</footnote>
<table confidence="0.991154266666667">
(a)- Citations
N Inf. sup. H H&amp;W H&amp;W&amp;C H&amp;W&amp;C
(Top-1) (Top-K)
5 no I 55.1 60.9 63.6 70.6 71.0
I 66.6 69.0 72.5 76.0 77.8
10 no I 64.6 66.8 69.8 76.5 76.7
I 78.1 78.1 81.0 83.4 83.8
15 no I 68.7 70.6 73.7 78.6 79.4
I 81.3 81.9 84.1 85.5 86.2
20 no I 70.1 72.4 75.0 79.6 79.4
I 81.1 82.4 84.0 86.1 86.1
25 no I 72.7 73.2 77.0 81.6 82.0
I 84.3 84.2 86.2 87.4 87.6
300 no I 86.1 80.7 87.1 88.2 88.2
I 92.5 89.6 93.4 93.6 93.5
(b)-Advertisements
N Inf. sup. H H&amp;W H&amp;W&amp;C H&amp;W&amp;C
(Top-1) (Top-K)
5 no I 55.2 61.8 60.5 66.0 66.0
I 59.4 65.2 63.6 69.3 69.6
10 no I 61.6 69.2 67.0 70.8 70.9
I 66.6 73.2 71.6 74.7 74.7
15 no I 66.3 71.7 70.1 73.0 73.0
I 70.4 75.6 74.5 76.6 76.9
20 no I 68.1 72.8 72.0 74.5 74.6
I 71.9 76.7 75.7 77.9 78.1
25 no I 70.0 73.8 73.0 74.9 74.8
I 73.7 77.7 76.6 78.4 78.5
100 no I 76.3 76.2 77.6 78.5 78.6
I 80.4 80.5 81.2 81.8 81.7
</table>
<tableCaption confidence="0.990696">
Table 2: Experimental results for extracting fields
</tableCaption>
<bodyText confidence="0.986050454545455">
from citations and advertisements. N is the number
of labeled samples. H is the traditional hard-EM and
H&amp;W weighs labeled and unlabeled data as men-
tioned in Sec. 5. Our proposed model is H&amp;W&amp;C,
which uses constraints in the learning procedure. I
refers to using constraints during inference at eval-
uation time. Note that adding constraints improves
the accuracy during both learning and inference.
degrade the performance. Indeed, with 300 labeled
examples in the citations domain, the performance
decreases from 86.1 to 80.7. The usefulness of in-
jecting constraints in semi-supervised learning is ex-
hibited in the two right most columns: using con-
straints H&amp;W&amp;C improves the performance over
H&amp;W quite significantly.
We carefully examined the contribution of us-
ing constraints to the learning stage and the testing
stage, and two separate results are presented: test-
ing with constraints (denoted I for inference) and
without constraints (no I). The I results are consis-
tently better. And, it is also clear from Table 2,
that using constraints in training always improves
</bodyText>
<page confidence="0.995105">
285
</page>
<bodyText confidence="0.998072911111111">
the model and the amount of improvement depends
on the amount of labeled data.
Figure 3 compares two protocols on the adver-
tisements domain: H&amp;W+I, where we first run the
H&amp;W protocol and then apply the constraints dur-
ing testing stage, and H&amp;W&amp;C+I, which uses con-
straints to guide the model during learning and uses
it also in testing. Although injecting constraints in
the learning process helps, testing with constraints is
more important than using constraints during learn-
ing, especially when the labeled data size is large.
This confirms results reported for the supervised
learning case in (Punyakanok et al., 2005; Roth and
Yih, 2005). However, as shown, our proposed al-
gorithm H&amp;W&amp;C for training with constraints is
critical when the amount labeled data is small.
Figure 4 further strengthens this point. In the cita-
tions domain, H&amp;W&amp;C+I achieves with 20 labeled
samples similar performance to the supervised ver-
sion without constraints with 300 labeled samples.
(Grenager et al., 2005) and (Haghighi and Klein,
2006) also report results for semi-supervised learn-
ing for these domains. However, due to differ-
ent preprocessing, the comparison is not straight-
forward. For the citation domain, when 20 labeled
and 300 unlabeled samples are available, (Grenager
et al., 2005) observed an increase from 65.2% to
71.3%. Our improvement is from 70.1% to 79.4%.
For the advertisement domain, they observed no im-
provement, while our model improves from 68.1%
to 74.6% with 20 labeled samples. Moreover, we
successfully use out-of-domain data (web data) to
improve our model, while they report that this data
did not improve their unsupervised model.
(Haghighi and Klein, 2006) also worked on one of
our data sets. Their underlying model, Markov Ran-
dom Fields, allows more expressive features. Nev-
ertheless, when they use only unary constraints they
get 53.75%. When they use their final model, along
with a mechanism for extending the prototypes to
other tokens, they get results that are comparable to
our model with 10 labeled examples. Additionally,
in their framework, it is not clear how to use small
amounts of labeled data when available. Our model
outperforms theirs once we add 10 more examples.
</bodyText>
<figureCaption confidence="0.758578666666667">
Figure 3: Comparison between H&amp;W+I and
H&amp;W&amp;C+I on the advertisements domain. When
there is a lot of labeled data, inference with con-
straints is more important than using constraints dur-
ing learning. However, it is important to train with
constraints when the amount of labeled data is small.
Figure 4: With 20 labeled citations, our algorithm
performs competitively to the supervised version
trained on 300 samples.
</figureCaption>
<sectionHeader confidence="0.991511" genericHeader="method">
7 Soft Constraints
</sectionHeader>
<bodyText confidence="0.999536">
This section discusses the importance of using soft
constraints rather than hard constraints, the choice
of Hamming distance for d(y, 1C(x)) and how we
approximate it. We use two constraints to illustrate
the ideas. (C1): “state transitions can only occur on
punctuation marks or newlines”, and (C2): “the field
TITLE must appear”.
First, we claim that defining d(y, 1C(x)) to be
the Hamming distance is superior to using a binary
value, d(y, 1C(x)) = 0 if y E 1C(x) and 1 other-
wise. Consider, for example, the constraint C1 in
the advertisements domain. While the vast majority
of the instances satisfy the constraint, some violate
it in more than one place. Therefore, once the binary
distance is set to 1, the algorithm looses the ability to
discriminate constraint violations in other locations
</bodyText>
<figure confidence="0.999823411764706">
5 10 15 20 25 100
0.85
0.8
0.75
0.7
0.65
H+N+I
H+N+C+I
0.95
0.85
0.75
0.9
0.8
0.7
sup. (300)
H+N+C+I
5 10 15 20 25 100
</figure>
<page confidence="0.995469">
286
</page>
<bodyText confidence="0.997869454545455">
of the same instance. This may hurt the performance
in both the inference and the learning stage.
Computing the Hamming distance exactly can
be a computationally hard problem. Further-
more, it is unreasonable to implement the ex-
act computation for each constraint. Therefore,
we implemented a generic approximation for the
hamming distance assuming only that we are
given a boolean function OC(yN) that returns
whether labeling the token xN with state yN vio-
lates constraint with respect to an already labeled
</bodyText>
<equation confidence="0.834918">
sequence (x1, ... , xN−1, y1, ... , yN−1). Then
N
d(y, 1C(x)) _ EN i�1 OC(yi). For example,
</equation>
<bodyText confidence="0.99868408">
consider the prefix x1, x2, x3, x4, which con-
tains no punctuation or newlines and was labeled
AUTH, AUTH, DATE, DATE. This labeling
violates C1, the minimal hamming distance is 2, and
our approximation gives 1, (since there is only one
transition that violates the constraint.)
For constraints which cannot be validated based
on prefix information, our approximation resorts to
binary violation count. For instance, the constraint
C2 cannot be implemented with prefix information
when the assignment is not complete. Otherwise, it
would mean that the field TITLE should appear as
early as possible in the assignment.
While (Roth and Yih, 2005) showed the signif-
icance of using hard constraints, our experiments
show that using soft constraints is a superior op-
tion. For example, in the advertisements domain,
C1 holds for the large majority of the gold-labeled
instances, but is sometimes violated. In supervised
training with 100 labeled examples on this domain,
sup gave 76.3% accuracy. When the constraint vio-
lation penalty p was infinity (equivalent to hard con-
straint), the accuracy improved to 78.7%, but when
the penalty was set to −log(0.1), the accuracy of the
model jumped to 80.6%.
</bodyText>
<sectionHeader confidence="0.997986" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.992776333333333">
We proposed to use constraints as a way to guide
semi-supervised learning. The framework devel-
oped is general both in terms of the representation
and expressiveness of the constraints, and in terms
of the underlying model being learned – HMM in
the current implementation. Moreover, our frame-
work is a useful tool when the domain knowledge
cannot be expressed by the model.
The results show that constraints improve not
only the performance of the final inference stage but
also propagate useful information during the semi-
supervised learning process and that training with
the constraints is especially significant when the
number of labeled training data is small.
Acknowledgments: This work is supported by NSF SoD-
HCER-0613885 and by a grant from Boeing. Part of this work
was done while Dan Roth visited the Technion, Israel, sup-
ported by a Lady Davis Fellowship.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999327538461538">
W. Cohen and S. Sarawagi. 2004. Exploiting dictionaries in
named entity extraction: Combining semi-markov extraction
processes and data integration methods. In Proc. of the ACM
SIGKDD.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised
learning of field segmentation models for information extrac-
tion. In Proc. of the Annual Meeting of the ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proc. of HTL-NAACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy markov models for information extraction and seg-
mentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proceedings of HLT-NAACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text
classification from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103–134.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learn-
ing and inference over constrained output. In Proc. of IJCAI.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML.
D. Roth. 1999. Learning in natural language. In Proc. ofIJCAI,
pages 898–904.
W. Shen, X. Li, and A. Doan. 2005. Constraint-based entity
matching. In Proc. of AAAI).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proc. of the Annual
Meeting of the ACL.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern contexts.
In Proc. of EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of the
Annual Meeting of the ACL.
</reference>
<page confidence="0.997246">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928197">
<title confidence="0.99993">Guiding Semi-Supervision with Constraint-Driven Learning</title>
<author confidence="0.999951">Ming-Wei Chang Lev Ratinov Dan Roth</author>
<affiliation confidence="0.9999365">Department of Computer Science University of Illinois at Urbana-Champaign</affiliation>
<address confidence="0.99066">Urbana, IL 61801</address>
<email confidence="0.936381">ratinov2,</email>
<abstract confidence="0.999816666666667">Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>S Sarawagi</author>
</authors>
<title>Exploiting dictionaries in named entity extraction: Combining semi-markov extraction processes and data integration methods.</title>
<date>2004</date>
<booktitle>In Proc. of the ACM SIGKDD.</booktitle>
<contexts>
<context position="2024" citStr="Cohen and Sarawagi, 2004" startWordPosition="297" endWordPosition="300">ve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependenci</context>
<context position="5601" citStr="Cohen and Sarawagi, 2004" startWordPosition="852" endWordPosition="855">equential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraints and allow for more general, n-ary constraints. In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling. </context>
</contexts>
<marker>Cohen, Sarawagi, 2004</marker>
<rawString>W. Cohen and S. Sarawagi. 2004. Exploiting dictionaries in named entity extraction: Combining semi-markov extraction processes and data integration methods. In Proc. of the ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1473" citStr="Collins and Singer, 1999" startWordPosition="213" endWordPosition="216">lying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance. Acquiring labeled data is a difficult and expensive task. Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2</context>
<context position="5275" citStr="Collins and Singer, 1999" startWordPosition="802" endWordPosition="805">ger et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraint</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="11688" citStr="Collins, 2002" startWordPosition="1877" endWordPosition="1878">f : X × Y  R to assign scores to each possible input/output pair. Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs. The global scoring function is often decomposed as a weighted sum of feature functions, M f(x, y) = Aifi(x, y) = A · F(x, y). i=1 This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case). Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability. Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference. Now, consider a scenario where we have a set of constraints C1, ... , CK. We define a constraint C : X × Y  {0, 1} as a function that indicates whether the input/output sequence violates some desired properties. When the constraints are hard, the solution is given by argmax A · F(x, y), yE1C(.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Unsupervised learning of field segmentation models for information extraction.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4667" citStr="Grenager et al., 2005" startWordPosition="702" endWordPosition="705"> process. The core of our approach, (1), is described in Section 5. The task is described in Section 3 and the Experimental study in Section 6. It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework. 2 Related Work In the semi-supervised domain there are two main approaches for injecting domain specific knowledge. One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure. For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singe</context>
<context position="7849" citStr="Grenager et al., 2005" startWordPosition="1210" endWordPosition="1213">re, in this section, we give a brief introduction to the two domains on which we tested our algorithms. We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified. Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) . The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005). Given a citation, the task is to extract the &apos;The data for both problems is available at: http://www.stanford.edu/ grenager/data/unsupie.tgz 281 (a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [ TECH-REPORT PhD thesis, ] [ INSTITUTION DIKU , University of Copenhagen, ] [ DATE May 1994 . ] (b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE Programming language ] [ TECH-REPORT. PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE 1994 . ] Figure </context>
<context position="9985" citStr="Grenager et al., 2005" startWordPosition="1571" endWordPosition="1574"> marks, the same HMM model parameters will result in the correct labeling (a). Therefore, by adding the improved labeled assignment we can generate better training samples during semi-supervised learning. In fact, the punctuation marks are only some of the constraints that can be applied to this problem. The set of constraints we used in our experiments appears in Table 1. Note that some of the constraints are non-local and are very intuitive for people, yet it is very difficult to inject this knowledge into most models. The second problem we consider is extracting fields from advertisements (Grenager et al., 2005). The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website. In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on. The data was preprocessed using regular expressions for phone numbers, email addresses and URLs. The list of the constraints for this domain is given in Table 1. We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006). We </context>
<context position="26767" citStr="Grenager et al., 2005" startWordPosition="4468" endWordPosition="4471">e learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large. This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005). However, as shown, our proposed algorithm H&amp;W&amp;C for training with constraints is critical when the amount labeled data is small. Figure 4 further strengthens this point. In the citations domain, H&amp;W&amp;C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples. (Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains. However, due to different preprocessing, the comparison is not straightforward. For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al., 2005) observed an increase from 65.2% to 71.3%. Our improvement is from 70.1% to 79.4%. For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples. Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report th</context>
</contexts>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proc. of HTL-NAACL.</booktitle>
<contexts>
<context position="2076" citStr="Haghighi and Klein, 2006" startWordPosition="305" endWordPosition="308">lins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our forma</context>
<context position="5377" citStr="Haghighi and Klein, 2006" startWordPosition="818" endWordPosition="821">words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraints and allow for more general, n-ary constra</context>
<context position="10580" citStr="Haghighi and Klein, 2006" startWordPosition="1672" endWordPosition="1675">ements (Grenager et al., 2005). The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website. In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on. The data was preprocessed using regular expressions for phone numbers, email addresses and URLs. The list of the constraints for this domain is given in Table 1. We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006). We slightly modified the seedwords due to difference in preprocessing. 282 4 Notation and Definitions Consider a structured classification problem, where given an input sequence x = (x1,... , xN), the task is to find the best assignment to the output variables y = (y1, ... , yM). We denote X to be the space of the possible input sequences and Y to be the set of possible output sequences. We define a structured output classifier as a function h : X  Y that uses a global scoring function f : X × Y  R to assign scores to each possible input/output pair. Given an input x, a desired function f </context>
<context position="26798" citStr="Haghighi and Klein, 2006" startWordPosition="4473" endWordPosition="4476">sting with constraints is more important than using constraints during learning, especially when the labeled data size is large. This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005). However, as shown, our proposed algorithm H&amp;W&amp;C for training with constraints is critical when the amount labeled data is small. Figure 4 further strengthens this point. In the citations domain, H&amp;W&amp;C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples. (Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains. However, due to different preprocessing, the comparison is not straightforward. For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al., 2005) observed an increase from 65.2% to 71.3%. Our improvement is from 70.1% to 79.4%. For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples. Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report that this data did not improve th</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proc. of HTL-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="7670" citStr="McCallum et al., 2000" startWordPosition="1181" endWordPosition="1184">atasets In Section 4 we will develop a general framework for semi-supervised learning with constraints. However, it is useful to illustrate the ideas on concrete problems. Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms. We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified. Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) . The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005). Given a citation, the task is to extract the &apos;The data for both problems is available at: http://www.stanford.edu/ grenager/data/unsupie.tgz 281 (a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [ TECH-REPORT PhD thesis, ] [ INSTITUTION DIKU , University of Copenhagen, ] [ DATE May 1994 . ] (b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="19235" citStr="McClosky et al., 2006" startWordPosition="3151" endWordPosition="3154">is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model. The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data. Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms. Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006). Similarly to self-training, we use the current model to generate new training examples from the unlaInput: Cycles: learning cycles Tr = {x, y}: labeled training set. U: unlabeled dataset F: set of feature functions. {ρi}: set of penalties. {Ci}: set of constraints. γ: balancing parameter with the supervised model. learn(Tr, F): supervised learning algorithm Top-K-Inference: returns top-K labeled scored by the cost function (1) CODL: 1. Initialize A0 = learn(Tr, F). 2. A = A0. 3. For Cycles iterations do: 4. T = φ 5. For each x ∈ U 6. {(x, y1), . . . , (x, yK)} = 7. Top-K-Inference(x, A, F, {</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1856" citStr="Nigam et al., 2000" startWordPosition="271" endWordPosition="274">and expensive task. Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for cons</context>
<context position="18507" citStr="Nigam et al., 2000" startWordPosition="3035" endWordPosition="3038"> used the topK elements in the beam, but this could be applied to any other inference procedure. The fact that the constraints are used in the inference procedure (in particular, for generating new training examples) allows us to use a learning algorithm that ignores the constraints, which is a lot more efficient (although algorithms that do take the constraints into account can be used too). We used maximum likelihood estimation of A but, in general, perceptron or quasiNewton can also be used. It is known that traditional semi-supervised training can degrade the learned model’s performance. (Nigam et al., 2000) has suggested to balance the contribution of labeled and unlabeled data to the parameters. The intuition is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model. The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data. Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated b</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI. D. Roth</booktitle>
<contexts>
<context position="6225" citStr="Punyakanok et al., 2005" startWordPosition="942" endWordPosition="945">that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraints and allow for more general, n-ary constraints. In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling. (Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005). Our formalism is most related to this last work. But, we develop a semi-supervised learning protocol based on this formalism. We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints’ violation. Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking. To the best of our knowledge, we are the first to suggest</context>
<context position="26393" citStr="Punyakanok et al., 2005" startWordPosition="4409" endWordPosition="4412">5 the model and the amount of improvement depends on the amount of labeled data. Figure 3 compares two protocols on the advertisements domain: H&amp;W+I, where we first run the H&amp;W protocol and then apply the constraints during testing stage, and H&amp;W&amp;C+I, which uses constraints to guide the model during learning and uses it also in testing. Although injecting constraints in the learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large. This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005). However, as shown, our proposed algorithm H&amp;W&amp;C for training with constraints is critical when the amount labeled data is small. Figure 4 further strengthens this point. In the citations domain, H&amp;W&amp;C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples. (Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains. However, due to different preprocessing, the comparison is not straightforward. For the citation domain, when 20 labeled and 3</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learning and inference over constrained output. In Proc. of IJCAI. D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning in natural language.</title>
<date>1999</date>
<booktitle>In Proc. ofIJCAI,</booktitle>
<pages>898--904</pages>
<contexts>
<context position="11640" citStr="Roth, 1999" startWordPosition="1870" endWordPosition="1871"> : X  Y that uses a global scoring function f : X × Y  R to assign scores to each possible input/output pair. Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs. The global scoring function is often decomposed as a weighted sum of feature functions, M f(x, y) = Aifi(x, y) = A · F(x, y). i=1 This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case). Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability. Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference. Now, consider a scenario where we have a set of constraints C1, ... , CK. We define a constraint C : X × Y  {0, 1} as a function that indicates whether the input/output sequence violates some desired properties. When the constraints are hard, the</context>
</contexts>
<marker>Roth, 1999</marker>
<rawString>D. Roth. 1999. Learning in natural language. In Proc. ofIJCAI, pages 898–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shen</author>
<author>X Li</author>
<author>A Doan</author>
</authors>
<title>Constraint-based entity matching.</title>
<date>2005</date>
<booktitle>In Proc. of AAAI).</booktitle>
<contexts>
<context position="6641" citStr="Shen et al., 2005" startWordPosition="1010" endWordPosition="1013">s been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling. (Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005). Our formalism is most related to this last work. But, we develop a semi-supervised learning protocol based on this formalism. We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints’ violation. Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking. To the best of our knowledge, we are the first to suggest a general semi-supervised protocol that is driven by soft constraints. We propose learning with constraints - a framework that combines the approaches described above in a unified and intuitive way. 3 Tasks, Examples and Datasets In Section 4 we will develop a general framework for semi-supervised learning with constraints. However, it is useful to illustrate the ideas on concrete problems. Therefore, in this se</context>
</contexts>
<marker>Shen, Li, Doan, 2005</marker>
<rawString>W. Shen, X. Li, and A. Doan. 2005. Constraint-based entity matching. In Proc. of AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5334" citStr="Smith and Eisner, 2005" startWordPosition="811" endWordPosition="814">uential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraint</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1499" citStr="Thelen and Riloff, 2002" startWordPosition="217" endWordPosition="220">e model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. 1 Introduction Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance. Acquiring labeled data is a difficult and expensive task. Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002). The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 20</context>
<context position="5248" citStr="Thelen and Riloff, 2002" startWordPosition="798" endWordPosition="801">ture. For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels. This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF. However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol. A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning. This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging. (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity. This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2300" citStr="Toutanova et al., 2005" startWordPosition="340" endWordPosition="343">trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000). In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002). On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005). This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our formalism is that it allows capturing different levels of constraint violation. Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data wh</context>
<context position="6249" citStr="Toutanova et al., 2005" startWordPosition="946" endWordPosition="949">ntity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier. In our framework, dictionary lookup approaches are viewed as unary constraints on the output states. We extend these kinds of constraints and allow for more general, n-ary constraints. In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling. (Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005). Our formalism is most related to this last work. But, we develop a semi-supervised learning protocol based on this formalism. We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints’ violation. Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking. To the best of our knowledge, we are the first to suggest a general semi-supervis</context>
<context position="16170" citStr="Toutanova et al., 2005" startWordPosition="2647" endWordPosition="2650">s us to train also the coefficients of the constraints valuation, pi, we choose not to do it, since we view this as a way to bias (or enforce) the prior knowledge into the learned model, rather than allowing the data to brush it away. Our experiments demonstrate that the proposed approach is robust to inaccurate approximation of the prior knowledge (assigning the same penalty to all the pi ). We note that in the presence of constraints, the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding, see (Toutanova et al., 2005; Roth and Yih, 2005) for a discussion), we chose beamsearch decoding. The semi-supervised learning with constraints is done with an EM-like procedure. We initialize the model with traditional supervised learning (ignoring the constraints) on a small labeled set. Given an unlabeled set U, in the estimation step, the traditional EM algorithm assigns a distribution over labeled assignments Y of each x E U, and in the maximization step, the set of model parameters is learned from the distributions assigned in the estimation step. However, in the presence of constraints, assigning the complete dis</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint learning improves semantic role labeling. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>