<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027928">
<title confidence="0.964798">
Improving Coreference Resolution by Using Conversational Metadata
</title>
<author confidence="0.840583">
Xiaoqiang Luo and Radu Florian and Todd Ward
</author>
<affiliation confidence="0.668692">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.76459">
Yorktown Heights, NY 10598
</address>
<email confidence="0.998546">
{xiaoluo,raduf,toddward}@us.ibm.com
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999687222222222">
In this paper, we propose the use of metadata
contained in documents to improve corefer-
ence resolution. Specifically, we quantify the
impact of speaker and turn information on the
performance of our coreference system, and
show that the metadata can be effectively en-
coded as features of a statistical resolution sys-
tem, which leads to a statistically significant
improvement in performance.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973541666667">
Coreference resolution aims to find the set of lin-
guistic expressions that refer to a common entity. It
is a discourse-level task given that the ambiguity of
many referential relationships among linguistic ex-
pressions can only be correctly resolved by examin-
ing information extracted from the entire document.
In this paper, we focus on exploiting the struc-
tural information (e.g., speaker and turn in conversa-
tional documents) represented in the metadata of an
input document. Such metadata often coincides with
the discourse structure, and is presumably useful to
coreference resolution. The goal of this study is to
quantify the effect metadata. To this end, informa-
tion contained in metadata is encoded as features in
our coreference resolution system, and statistically
significant improvement is observed.
The rest of the paper is organized as follows.
In Section 2 we describe the data set on which
this study is based. In Section 3 we first show
how to incorporate information carried by metadata
into a statistical coreference resolution system. We
also quantify the impact of metadata when they are
treated as extraneous data. Results and discussions
of the results are also presented in that section.
</bodyText>
<sectionHeader confidence="0.992665" genericHeader="method">
2 Data Set
</sectionHeader>
<bodyText confidence="0.888222135135135">
This study uses the 2007 ACE data. In the ACE
program, a mention is textual reference to an
object of interest while the set of mentions in a
document referring to the same object is called
entity. Each mention is of one of 7 entity
types: FAC(cility), GPE (Geo-Political Entity),
LOC(ation), ORG(anization), PER(son), VEH(icle),
and WEA(pon). Every entity type has a prede-
fined set of subtypes. For example, ORG sub-
types include commercial, governmental and
educational etc, which reflect different sub-
groups of organizations. Mentions referring to the
same entity share the same type and subtype. A
mention can also be assigned with one of 3 men-
tion types: either NAM(e), NOM(inal), or PRO(noun).
Accordingly, entities have “levels:” if an entity con-
tains at least one NAM mention, its level is NAM; or
if it does not contain any NAM mention, but contains
at least one NOM mention, then the entity is of level
NOM; if an entity has only PRO mention(s), then its
level is PRO. More information about ACE entity
annotation can be found in the official annotation
guideline (Linguistic Data Consortium, 2008).
The ACE 2007 documents come from a variety of
sources, namely newswire, broadcast conversation,
broadcast news, Usenet, web log and telephone con-
versation. Some of them contain rich metadata, as
illustrated in the following excerpt of one broadcast
conversation document:
&lt;DOC&gt;
&lt;DOCID&gt;CNN_CF_20030303.1900.00&lt;/DOCID&gt;
&lt;TEXT&gt;
&lt;TURN&gt;
&lt;SPEAKER&gt; Begala &lt;/SPEAKER&gt;
Well, we’ll debate that later on in the
show. We’ll have a couple of experts
come out, ...
</bodyText>
<page confidence="0.967765">
201
</page>
<note confidence="0.449767">
Proceedings of NAACL HLT 2009: Short Papers, pages 201–204,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.993174727272727">
&lt;/TURN&gt;
&lt;TURN&gt;
&lt;SPEAKER&gt; Novak &lt;/SPEAKER&gt;
Paul, as I understand your definition
of a political -- of a professional
politician based on that is somebody
who is elected to public office. ...
&lt;/TURN&gt;
...
&lt;/TEXT&gt;
&lt;/DOC&gt;
</figure>
<bodyText confidence="0.999935894736842">
In this example, SPEAKER and TURN informa-
tion are marked by their corresponding SGML tags.
Such metadata provides structural information: for
instance, the metadata implies that Begala is the
speaker of the utterance “Well, we’ll debate ..., ”
and Novak the speaker of the utterance “Paul, as
I understand your definition ...” Intuitively, knowing
the speakers of the previous and current turn would
make it a lot easier to find the right antecedent of
pronominal mentions I and your in the sentence:
“Paul, as I understand your definition ...”
Documents in non-conversational genres (e.g.
newswire documents) also contain speaker and quo-
tation, which resemble conversational utterance, but
they are not annotated. For these documents, we
use heuristics (e.g., existence of double or single
quote, a short list of communication verb lemmas
such as “say,” “tell” and “speak” etc) to determine
the speaker of a direct quotation if necessary.
</bodyText>
<sectionHeader confidence="0.864787" genericHeader="method">
3 Impact of Metadata
</sectionHeader>
<bodyText confidence="0.999866666666667">
In this section we describe how metadata is used to
improve our statistical coreference resolution sys-
tem.
</bodyText>
<subsectionHeader confidence="0.999409">
3.1 Resolution System
</subsectionHeader>
<bodyText confidence="0.999987159090909">
The coreference system used in our study is a data-
driven, machine-learning-based system. Mentions
in a document are processed sequentially by men-
tion type: NAM mentions are processed first, fol-
lowed by NOM mentions and then PRO mentions.
The first mention is used to create an initial entity
with a deterministic score 1. The second mention
can be either linked to the first entity, or used to cre-
ate a new entity, and the two actions are assigned a
score computed from a log linear model. This pro-
cess is repeated until all mentions in a document are
processed. During training time, the process is ap-
plied to the training data and training instances (both
positive and negative) are generated. At testing time,
the same process is applied to an input document
and the hypothesis with the highest score is selected
as the final coreference result. At the core of the
coreference system is a conditional log linear model
P (l|e, m) which measures how likely a mention m
is or is not coreferential with an existing entity e.
The modeling framework provides us with the flexi-
bility to integrate metadata information by encoding
it as features.
The coreference resolution system employs a va-
riety of lexical, semantic, distance and syntactic
features(Luo et al., 2004; Luo and Zitouni, 2005).
The full-blown system achieves an 56.2% ACE-
value score on the official 2007 ACE test data,
which is about the same as the best-performing sys-
tem in the Entity Detection and Recognition (EDR)
task (NIST, 2007). So we believe that the resolution
system is fairly solid.
The aforementioned 56.2% score includes men-
tion detection (i.e., finding mention boundaries and
predicting mention attributes) and coreference res-
olution. Since this study is about coreference res-
olution only, the subsequent experiments, are thus
performed on gold-standard mentions. We split the
ACE 2007 data into a training set consisting of 499
documents, and a test set of 100 documents. The
training and test split ratio is roughly the same across
genres. The performance numbers reported in the
subsequent subsections are on the 100-document de-
velopment test set.
</bodyText>
<subsectionHeader confidence="0.998899">
3.2 Metadata Features
</subsectionHeader>
<bodyText confidence="0.9605162">
For conversational documents with speaker and turn
information, we compute a group of binary features
for a candidate referent r and the current mention
m. Feature values are 1 if the conditions described
below hold:
</bodyText>
<listItem confidence="0.99945675">
• if r is a speaker, m is a pronominal mention and
r utters the sentence containing m.
• if r is a speaker, m is pronoun and r utters the
sentence one turn before the one containing m.
• if mention r and mention m are seen in the
same turn.
• if mention r and mention m are in two consec-
utive turns.
</listItem>
<bodyText confidence="0.9994725">
Note that the first feature is not subsumed by the
third one since a turn may contain multiple sen-
tences. For the same reason, the last feature does not
subsume the second one. For the sample document
in Section 2, the first feature fires if r = Novak and
m = I; the second features fires if r = Begala
</bodyText>
<page confidence="0.990964">
202
</page>
<bodyText confidence="0.999919366666667">
and m = I; the third feature fires if r = Paul
and m = I; and lastly, the fourth feature fires if
r = We and m = I. For ACE documents that
do not carry turn and speaker information such as
newswire, we use heuristic rules to empirically de-
termine the speaker and the corresponding quota-
tions before computing these features.
To test the effect of the feature group, we trained
two models: a baseline system without speaker and
turn features, and a contrast system by adding the
speaker and turn features to the baseline system. The
contrast results are tabulated in Table 1. We observe
an overall 0.7 point ACE-value improvement. We
also compute the ACE-values at document level for
the two systems, and a paired Wilcoxon (Wilcoxon,
1945) rank-sum test is conducted, which indicates
that the difference between the two systems is statis-
tically significant at level p ≤ 0.002.
Note that the features often help link pronouns
with their antecedents in conversational documents.
But ACE-value is a weighted metric which heav-
ily discounts pronominal mentions and entities. We
suspect that the effect of speaker and turn informa-
tion could be larger if we weigh all mention types
equally. This is confirmed when we looked at the un-
weighted B3 (Bagga and Baldwin, 1998) numbers
reported by the official ACE08 scorer (column B3
in Table 1): the overall B3 score is improved from
73.8% to 76.4% – a 2.6 point improvement, which
is almost 4 times as large as the ACE-value change.
</bodyText>
<table confidence="0.997656333333333">
System ACE-Value B3
baseline 78.7 73.8
+ Spkr/Turn 79.4 76.4
</table>
<tableCaption confidence="0.9867165">
Table 1: Coreference performance: baseline vs. system
with speaker and turn features.
</tableCaption>
<subsectionHeader confidence="0.993771">
3.3 Metadata: To Use Or Not to Use?
</subsectionHeader>
<bodyText confidence="0.999323666666667">
In the ACE evaluations prior to 2008, mentions in-
side metadata (such as speaker and poster) are anno-
tated and scored as normal mentions, although such
metadata is not part of the actual content of a doc-
ument. An interesting question is: how large an ef-
fect do mentions inside metadata have on the system
performance? If metadata are not annotated as men-
tions, is it still useful to look into them? To answer
this question, we remove speaker mentions in con-
versational documents (i.e., broadcast conversation
and telephone conversation) from both the training
and test data. Then we train two systems:
</bodyText>
<listItem confidence="0.7829116">
• System A: the system totally disregards meta-
data.
• System B: the system first recovers speaker
metadata using a very simple rule: all to-
kens within the &lt;SPEAKER&gt; tags are treated
as one PER mention. This rule recovers most
speaker mentions, but it can occasionally re-
sult in errors. For instance, the speaker “CNN
correspondent John Smith” includes affilia-
tion and profession information and ought to
be tagged as three mentions: “CNN” as an
ORG(anization) mention, “correspondent” and
“John Smith” as two PER mentions. With re-
covered speaker mentions, we train a model
and resolve coreference as normal.
</listItem>
<bodyText confidence="0.99377975">
After mentions in the test data are chained in Sys-
tem B, speaker mentions are then removed from sys-
tem output so that the coreference result is directly
comparable with that of System A.
The ACE-value comparison between System A
and System B is shown in Table 2. As can be
seen, System B works much better than System A,
which ignores SPEAKER tags. For telephone con-
versations (cts), ACE-value improves as much as 4.6
points. A paired Wilcoxon test on document-level
ACE-values indicates that the difference is statisti-
cally significant at p &lt; 0.016.
</bodyText>
<table confidence="0.99601425">
System bc cts
A 75.2 66.8
B 76.6 71.4
Abs. Change 1.4 4.6
</table>
<tableCaption confidence="0.983833333333333">
Table 2: Metadata improves the ACE-value for broadcast
conversation (bc) and telephone conversation (cts) docu-
ments.
</tableCaption>
<bodyText confidence="0.999941">
The reason why metadata helps is that speaker
mention can be used to localize the coreference pro-
cess and therefore improves the performance. For
example, in the sentences uttered by “Novak” (cf.
the sample document in Section 2), it is intuitively
straightforward to link mention I with Novak, and
your with Begala – when speaker mentions are
made present in the coreference system B. On the
other hand, in System A, “I” is likely to be linked
with “Paul” because of its proximity of “Paul” in the
absence of speaker information.
The result of this experiment suggests that, unsur-
prisingly, speaker and turn metadata carry structural
</bodyText>
<page confidence="0.997406">
203
</page>
<bodyText confidence="0.9996855">
information helpful for coreference resolution. Even
if speaker mentions are not annotated (as in System
A), it is still beneficial to make use of it, e.g., by first
identifying them automatically as in System B.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999976586206896">
There is a large body of literature for coreference
resolution based on machine learning (Kehler, 1997;
Soon et al., 2001; Ng and Cardie, 2002; Yang et al.,
2008; Luo et al., 2004) approach. Strube and Muller
(2003) presented a machine-learning based pronoun
resolution system for spoken dialogue (Switchboard
corpus). The document genre in their study is simi-
lar to the ACE telephony conversation documents,
and they did include some dialogue-specific fea-
tures, such as an anaphora’s preference for S, VP
or NP, in their system, but they did not use speaker
or turn information. Gupta et al. (2007) presents
an algorithm disambiguating generic and referential
“you.”
Cristea et al. (1999) attempted to improve coref-
erence resolution by first analyzing the discourse
structure of a document with rhetoric structure the-
ory (RST) (Mann and Thompson, 1987) and then
using the resulted discourse structure in coreference
resolution. Since obtaining reliably the discourse
structure itself is a challenge, they got mixed results
compared with a linear structure baseline.
Our work presented in this paper concentrates on
the structural information represented in metadata,
such as turn or speaker information. Such metadata
provides reliable discourse structure, especially for
conversational documents, which is proven benefi-
cial for enhancing the performance of our corefer-
ence resolution system.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99990825">
This work is partially supported by DARPA GALE
program under the contract number HR0011-06-02-
0001. We’d also like to thank 3 reviewers for their
helpful comments.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999598355932203">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Eval-
uation (LREC’98), pages 563–566.
Dan Cristea, Nancy lde, Daniel Marcu, Valentin Tablan-
livia Polanyi, and Martin van den Berg. 1999. Dis-
course structure and co-reference: An empirical study.
In Proceedings of ACL Workshop ”The Relation of
Discourse/Dialogue Structure and Reference”. Asso-
ciation for Computational Linguistics.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007.
Disambiguating between generic and referential ”you”
in dialog. In Proceedings of the 45th ACL(the Demo
and Poster Sessions), pages 105–108, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Andrew Kehler. 1997. Probabilistic coreference in infor-
mation extraction. In Proc. ofEMNLP.
Linguistic Data Consortium. 2008. ACE (Automatic
Content Extraction) English annotation guidelines
for entities. http://projects.ldc.upenn.edu/ace/docs/
English-Entities-Guidelines v6.5.pdf.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proc. of Human Language Technology
(HLT)/Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. ofACL.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text organiza-
tion. Technical Report RS-87-190, USC/Information
Sciences Institute.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. ofACL, pages 104–111.
NIST. 2007. 2007 automatic con-
tent extraction evaluation official results.
http://www.nist.gov/speech/tests/ace/2007/doc/
ace07 eval official results 20070402.html.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521–544.
Michael Strube and Christoph Muller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics, I:80–83.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Proceedings of ACL-08: HLT, pages 843–
851, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998892">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915748">
<title confidence="0.999244">Improving Coreference Resolution by Using Conversational Metadata</title>
<author confidence="0.97519">Xiaoqiang Luo</author>
<author confidence="0.97519">Radu Florian</author>
<author confidence="0.97519">Todd</author>
<affiliation confidence="0.997772">IBM T.J. Watson Research</affiliation>
<address confidence="0.956894">Yorktown Heights, NY 10598</address>
<abstract confidence="0.9981076">In this paper, we propose the use of metadata contained in documents to improve coreference resolution. Specifically, we quantify the impact of speaker and turn information on the performance of our coreference system, and show that the metadata can be effectively encoded as features of a statistical resolution system, which leads to a statistically significant improvement in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC’98),</booktitle>
<pages>563--566</pages>
<contexts>
<context position="9091" citStr="Bagga and Baldwin, 1998" startWordPosition="1490" endWordPosition="1493">compute the ACE-values at document level for the two systems, and a paired Wilcoxon (Wilcoxon, 1945) rank-sum test is conducted, which indicates that the difference between the two systems is statistically significant at level p ≤ 0.002. Note that the features often help link pronouns with their antecedents in conversational documents. But ACE-value is a weighted metric which heavily discounts pronominal mentions and entities. We suspect that the effect of speaker and turn information could be larger if we weigh all mention types equally. This is confirmed when we looked at the unweighted B3 (Bagga and Baldwin, 1998) numbers reported by the official ACE08 scorer (column B3 in Table 1): the overall B3 score is improved from 73.8% to 76.4% – a 2.6 point improvement, which is almost 4 times as large as the ACE-value change. System ACE-Value B3 baseline 78.7 73.8 + Spkr/Turn 79.4 76.4 Table 1: Coreference performance: baseline vs. system with speaker and turn features. 3.3 Metadata: To Use Or Not to Use? In the ACE evaluations prior to 2008, mentions inside metadata (such as speaker and poster) are annotated and scored as normal mentions, although such metadata is not part of the actual content of a document.</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC’98), pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cristea</author>
<author>Nancy lde</author>
<author>Daniel Marcu</author>
<author>Valentin Tablanlivia Polanyi</author>
<author>Martin van den Berg</author>
</authors>
<title>Discourse structure and co-reference: An empirical study.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL Workshop ”The Relation of Discourse/Dialogue Structure and Reference”. Association for Computational Linguistics.</booktitle>
<marker>Cristea, lde, Marcu, Polanyi, van den Berg, 1999</marker>
<rawString>Dan Cristea, Nancy lde, Daniel Marcu, Valentin Tablanlivia Polanyi, and Martin van den Berg. 1999. Discourse structure and co-reference: An empirical study. In Proceedings of ACL Workshop ”The Relation of Discourse/Dialogue Structure and Reference”. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Matthew Purver</author>
<author>Dan Jurafsky</author>
</authors>
<title>Disambiguating between generic and referential ”you” in dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL(the Demo and Poster Sessions),</booktitle>
<pages>105--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12884" citStr="Gupta et al. (2007)" startWordPosition="2126" endWordPosition="2129">m B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of a document with rhetoric structure theory (RST) (Mann and Thompson, 1987) and then using the resulted discourse structure in coreference resolution. Since obtaining reliably the discourse structure itself is a challenge, they got mixed results compared with a linear structure baseline. Our work presented in this paper concentrates on the structural information represented in metadata, such as turn or speaker informa</context>
</contexts>
<marker>Gupta, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007. Disambiguating between generic and referential ”you” in dialog. In Proceedings of the 45th ACL(the Demo and Poster Sessions), pages 105–108, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Probabilistic coreference in information extraction.</title>
<date>1997</date>
<booktitle>In Proc. ofEMNLP.</booktitle>
<contexts>
<context position="12387" citStr="Kehler, 1997" startWordPosition="2046" endWordPosition="2047">in the coreference system B. On the other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Andrew Kehler. 1997. Probabilistic coreference in information extraction. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>ACE (Automatic Content Extraction) English annotation guidelines for entities. http://projects.ldc.upenn.edu/ace/docs/ English-Entities-Guidelines v6.5.pdf.</title>
<date>2008</date>
<contexts>
<context position="2952" citStr="Consortium, 2008" startWordPosition="467" endWordPosition="468">different subgroups of organizations. Mentions referring to the same entity share the same type and subtype. A mention can also be assigned with one of 3 mention types: either NAM(e), NOM(inal), or PRO(noun). Accordingly, entities have “levels:” if an entity contains at least one NAM mention, its level is NAM; or if it does not contain any NAM mention, but contains at least one NOM mention, then the entity is of level NOM; if an entity has only PRO mention(s), then its level is PRO. More information about ACE entity annotation can be found in the official annotation guideline (Linguistic Data Consortium, 2008). The ACE 2007 documents come from a variety of sources, namely newswire, broadcast conversation, broadcast news, Usenet, web log and telephone conversation. Some of them contain rich metadata, as illustrated in the following excerpt of one broadcast conversation document: &lt;DOC&gt; &lt;DOCID&gt;CNN_CF_20030303.1900.00&lt;/DOCID&gt; &lt;TEXT&gt; &lt;TURN&gt; &lt;SPEAKER&gt; Begala &lt;/SPEAKER&gt; Well, we’ll debate that later on in the show. We’ll have a couple of experts come out, ... 201 Proceedings of NAACL HLT 2009: Short Papers, pages 201–204, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics &lt;/TUR</context>
</contexts>
<marker>Consortium, 2008</marker>
<rawString>Linguistic Data Consortium. 2008. ACE (Automatic Content Extraction) English annotation guidelines for entities. http://projects.ldc.upenn.edu/ace/docs/ English-Entities-Guidelines v6.5.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Imed Zitouni</author>
</authors>
<title>Multilingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In Proc. of Human Language Technology (HLT)/Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6142" citStr="Luo and Zitouni, 2005" startWordPosition="975" endWordPosition="978"> and negative) are generated. At testing time, the same process is applied to an input document and the hypothesis with the highest score is selected as the final coreference result. At the core of the coreference system is a conditional log linear model P (l|e, m) which measures how likely a mention m is or is not coreferential with an existing entity e. The modeling framework provides us with the flexibility to integrate metadata information by encoding it as features. The coreference resolution system employs a variety of lexical, semantic, distance and syntactic features(Luo et al., 2004; Luo and Zitouni, 2005). The full-blown system achieves an 56.2% ACEvalue score on the official 2007 ACE test data, which is about the same as the best-performing system in the Entity Detection and Recognition (EDR) task (NIST, 2007). So we believe that the resolution system is fairly solid. The aforementioned 56.2% score includes mention detection (i.e., finding mention boundaries and predicting mention attributes) and coreference resolution. Since this study is about coreference resolution only, the subsequent experiments, are thus performed on gold-standard mentions. We split the ACE 2007 data into a training set</context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>Xiaoqiang Luo and Imed Zitouni. 2005. Multilingual coreference resolution with syntactic features. In Proc. of Human Language Technology (HLT)/Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="6118" citStr="Luo et al., 2004" startWordPosition="971" endWordPosition="974">ces (both positive and negative) are generated. At testing time, the same process is applied to an input document and the hypothesis with the highest score is selected as the final coreference result. At the core of the coreference system is a conditional log linear model P (l|e, m) which measures how likely a mention m is or is not coreferential with an existing entity e. The modeling framework provides us with the flexibility to integrate metadata information by encoding it as features. The coreference resolution system employs a variety of lexical, semantic, distance and syntactic features(Luo et al., 2004; Luo and Zitouni, 2005). The full-blown system achieves an 56.2% ACEvalue score on the official 2007 ACE test data, which is about the same as the best-performing system in the Entity Detection and Recognition (EDR) task (NIST, 2007). So we believe that the resolution system is fairly solid. The aforementioned 56.2% score includes mention detection (i.e., finding mention boundaries and predicting mention attributes) and coreference resolution. Since this study is about coreference resolution only, the subsequent experiments, are thus performed on gold-standard mentions. We split the ACE 2007 </context>
<context position="12465" citStr="Luo et al., 2004" startWordPosition="2060" endWordPosition="2063">y to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report RS-87-190,</tech>
<institution>USC/Information Sciences Institute.</institution>
<contexts>
<context position="13138" citStr="Mann and Thompson, 1987" startWordPosition="2163" endWordPosition="2166">achine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of a document with rhetoric structure theory (RST) (Mann and Thompson, 1987) and then using the resulted discourse structure in coreference resolution. Since obtaining reliably the discourse structure itself is a challenge, they got mixed results compared with a linear structure baseline. Our work presented in this paper concentrates on the structural information represented in metadata, such as turn or speaker information. Such metadata provides reliable discourse structure, especially for conversational documents, which is proven beneficial for enhancing the performance of our coreference resolution system. Acknowledgments This work is partially supported by DARPA G</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report RS-87-190, USC/Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="12427" citStr="Ng and Cardie, 2002" startWordPosition="2052" endWordPosition="2055">he other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. ofACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>automatic content extraction evaluation official results. http://www.nist.gov/speech/tests/ace/2007/doc/ ace07 eval official results 20070402.html.</title>
<date>2007</date>
<contexts>
<context position="6352" citStr="NIST, 2007" startWordPosition="1013" endWordPosition="1014">is a conditional log linear model P (l|e, m) which measures how likely a mention m is or is not coreferential with an existing entity e. The modeling framework provides us with the flexibility to integrate metadata information by encoding it as features. The coreference resolution system employs a variety of lexical, semantic, distance and syntactic features(Luo et al., 2004; Luo and Zitouni, 2005). The full-blown system achieves an 56.2% ACEvalue score on the official 2007 ACE test data, which is about the same as the best-performing system in the Entity Detection and Recognition (EDR) task (NIST, 2007). So we believe that the resolution system is fairly solid. The aforementioned 56.2% score includes mention detection (i.e., finding mention boundaries and predicting mention attributes) and coreference resolution. Since this study is about coreference resolution only, the subsequent experiments, are thus performed on gold-standard mentions. We split the ACE 2007 data into a training set consisting of 499 documents, and a test set of 100 documents. The training and test split ratio is roughly the same across genres. The performance numbers reported in the subsequent subsections are on the 100-</context>
</contexts>
<marker>NIST, 2007</marker>
<rawString>NIST. 2007. 2007 automatic content extraction evaluation official results. http://www.nist.gov/speech/tests/ace/2007/doc/ ace07 eval official results 20070402.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="12406" citStr="Soon et al., 2001" startWordPosition="2048" endWordPosition="2051">ence system B. On the other hand, in System A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreferenc</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Christoph Muller</author>
</authors>
<title>A machine learning approach to pronoun resolution in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12500" citStr="Strube and Muller (2003)" startWordPosition="2065" endWordPosition="2068">ecause of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the discourse structure of a document with rhetoric structure </context>
</contexts>
<marker>Strube, Muller, 2003</marker>
<rawString>Michael Strube and Christoph Muller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wilcoxon</author>
</authors>
<title>Individual comparisons by ranking methods.</title>
<date>1945</date>
<journal>Biometrics,</journal>
<pages>80--83</pages>
<contexts>
<context position="8567" citStr="Wilcoxon, 1945" startWordPosition="1406" endWordPosition="1407"> documents that do not carry turn and speaker information such as newswire, we use heuristic rules to empirically determine the speaker and the corresponding quotations before computing these features. To test the effect of the feature group, we trained two models: a baseline system without speaker and turn features, and a contrast system by adding the speaker and turn features to the baseline system. The contrast results are tabulated in Table 1. We observe an overall 0.7 point ACE-value improvement. We also compute the ACE-values at document level for the two systems, and a paired Wilcoxon (Wilcoxon, 1945) rank-sum test is conducted, which indicates that the difference between the two systems is statistically significant at level p ≤ 0.002. Note that the features often help link pronouns with their antecedents in conversational documents. But ACE-value is a weighted metric which heavily discounts pronominal mentions and entities. We suspect that the effect of speaker and turn information could be larger if we weigh all mention types equally. This is confirmed when we looked at the unweighted B3 (Bagga and Baldwin, 1998) numbers reported by the official ACE08 scorer (column B3 in Table 1): the o</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics, I:80–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Jun Lang</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>843--851</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="12446" citStr="Yang et al., 2008" startWordPosition="2056" endWordPosition="2059">tem A, “I” is likely to be linked with “Paul” because of its proximity of “Paul” in the absence of speaker information. The result of this experiment suggests that, unsurprisingly, speaker and turn metadata carry structural 203 information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. 4 Related Work There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is similar to the ACE telephony conversation documents, and they did include some dialogue-specific features, such as an anaphora’s preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential “you.” Cristea et al. (1999) attempted to improve coreference resolution by first analyzing the disc</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Proceedings of ACL-08: HLT, pages 843– 851, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>