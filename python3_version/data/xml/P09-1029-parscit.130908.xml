<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995139">
Discovering the Discriminative Views: Measuring Term Weights for
Sentiment Analysis
</title>
<author confidence="0.997551">
Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee
</author>
<affiliation confidence="0.999723">
Division of Electrical and Computer Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
</affiliation>
<email confidence="0.989419">
{yangpa,ljj,jhlee}@postech.ac.kr
</email>
<sectionHeader confidence="0.994822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941777777778">
This paper describes an approach to uti-
lizing term weights for sentiment analysis
tasks and shows how various term weight-
ing schemes improve the performance of
sentiment analysis systems. Previously,
sentiment analysis was mostly studied un-
der data-driven and lexicon-based frame-
works. Such work generally exploits tex-
tual features for fact-based analysis tasks
or lexical indicators from a sentiment lexi-
con. We propose to model term weighting
into a sentiment analysis system utilizing
collection statistics, contextual and topic-
related characteristics as well as opinion-
related properties. Experiments carried
out on various datasets show that our
approach effectively improves previous
methods.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963018181819">
With the explosion in the amount of commentaries
on current issues and personal views expressed in
weblogs on the Internet, the field of studying how
to analyze such remarks and sentiments has been
increasing as well. The field of opinion mining
and sentiment analysis involves extracting opin-
ionated pieces of text, determining the polarities
and strengths, and extracting holders and targets
of the opinions.
Much research has focused on creating testbeds
for sentiment analysis tasks. Most notable
and widely used are Multi-Perspective Question
Answering (MPQA) and Movie-review datasets.
MPQA is a collection of newspaper articles anno-
tated with opinions and private states at the sub-
sentence level (Wiebe et al., 2003). Movie-review
dataset consists of positive and negative reviews
from the Internet Movie Database (IMDb) archive
(Pang et al., 2002).
Evaluation workshops such as TREC and NT-
CIR have recently joined in this new trend of re-
search and organized a number of successful meet-
ings. At the TREC Blog Track meetings, re-
searchers have dealt with the problem of retriev-
ing topically-relevant blog posts and identifying
documents with opinionated contents (Ounis et
al., 2008). NTCIR Multilingual Opinion Analy-
sis Task (MOAT) shared a similar mission, where
participants are provided with a number of topics
and a set of relevant newspaper articles for each
topic, and asked to extract opinion-related proper-
ties from enclosed sentences (Seki et al., 2008).
Previous studies for sentiment analysis belong
to either the data-driven approach where an anno-
tated corpus is used to train a machine learning
(ML) classifier, or to the lexicon-based approach
where a pre-compiled list of sentiment terms is uti-
lized to build a sentiment score function.
This paper introduces an approach to the senti-
ment analysis tasks with an emphasis on how to
represent and evaluate the weights of sentiment
terms. We propose a number of characteristics of
good sentiment terms from the perspectives of in-
formativeness, prominence, topic–relevance, and
semantic aspects using collection statistics, con-
textual information, semantic associations as well
as opinion–related properties of terms. These term
weighting features constitute the sentiment analy-
sis model in our opinion retrieval system. We test
our opinion retrieval system with TREC and NT-
CIR datasets to validate the effectiveness of our
term weighting features. We also verify the ef-
fectiveness of the statistical features used in data-
driven approaches by evaluating an ML classifier
with labeled corpora.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999403666666667">
Representing text with salient features is an im-
portant part of a text processing task, and there ex-
ists many works that explore various features for
</bodyText>
<page confidence="0.48429">
253
</page>
<note confidence="0.9998825">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99955187037037">
text analysis systems (Sebastiani, 2002; Forman,
2003). Sentiment analysis task have also been us-
ing various lexical, syntactic, and statistical fea-
tures (Pang and Lee, 2008). Pang et al. (2002)
employed n-gram and POS features for ML meth-
ods to classify movie-review data. Also, syntac-
tic features such as the dependency relationship of
words and subtrees have been shown to effectively
improve the performances of sentiment analysis
(Kudo and Matsumoto, 2004; Gamon, 2004; Mat-
sumoto et al., 2005; Ng et al., 2006).
While these features are usually employed by
data-driven approaches, there are unsupervised ap-
proaches for sentiment analysis that make use of a
set of terms that are semantically oriented toward
expressing subjective statements (Yu and Hatzi-
vassiloglou, 2003). Accordingly, much research
has focused on recognizing terms’ semantic ori-
entations and strength, and compiling sentiment
lexicons (Hatzivassiloglou and Mckeown, 1997;
Turney and Littman, 2003; Kamps et al., 2004;
Whitelaw et al., 2005; Esuli and Sebastiani, 2006).
Interestingly, there are conflicting conclusions
about the usefulness of the statistical features in
sentiment analysis tasks (Pang and Lee, 2008).
Pang et al. (2002) presents empirical results in-
dicating that using term presence over term fre-
quency is more effective in a data-driven sentiment
classification task. Such a finding suggests that
sentiment analysis may exploit different types of
characteristics from the topical tasks, that, unlike
fact-based text analysis tasks, repetition of terms
does not imply a significance on the overall senti-
ment. On the other hand, Wiebe et al. (2004) have
noted that hapax legomena (terms that only appear
once in a collection of texts) are good signs for
detecting subjectivity. Other works have also ex-
ploited rarely occurring terms for sentiment anal-
ysis tasks (Dave et al., 2003; Yang et al., 2006).
The opinion retrieval task is a relatively recent
issue that draws both the attention of IR and NLP
communities. Its task is to find relevant documents
that also contain sentiments about a given topic.
Generally, the opinion retrieval task has been ap-
proached as a two–stage task: first, retrieving top-
ically relevant documents, then reranking the doc-
uments by the opinion scores (Ounis et al., 2006).
This approach is also appropriate for evaluation
systems such as NTCIR MOAT that assumes that
the set of topically relevant documents are already
known in advance. On the other hand, there are
also some interesting works on modeling the topic
and sentiment of documents in a unified way (Mei
et al., 2007; Zhang and Ye, 2008).
</bodyText>
<sectionHeader confidence="0.721316" genericHeader="method">
3 Term Weighting and Sentiment
Analysis
</sectionHeader>
<bodyText confidence="0.999886">
In this section, we describe the characteristics of
terms that are useful in sentiment analysis, and
present our sentiment analysis model as part of
an opinion retrieval system and an ML sentiment
classifier.
</bodyText>
<subsectionHeader confidence="0.999758">
3.1 Characteristics of Good Sentiment Terms
</subsectionHeader>
<bodyText confidence="0.9985175">
This section examines the qualities of useful terms
for sentiment analysis tasks and corresponding
features. For the sake of organization, we cate-
gorize the sources of features into either global or
local knowledge, and either topic-independent or
topic-dependent knowledge.
Topic-independently speaking, a good senti-
ment term is discriminative and prominent, such
that the appearance of the term imposes greater
influence on the judgment of the analysis system.
The rare occurrence of terms in document collec-
tions has been regarded as a very important feature
in IR methods, and effective IR models of today,
either explicitly or implicitly, accommodate this
feature as an Inverse Document Frequency (IDF)
heuristic (Fang et al., 2004). Similarly, promi-
nence of a term is recognized by the frequency of
the term in its local context, formulated as Term
Frequency (TF) in IR.
If a topic of the text is known, terms that are rel-
evant and descriptive of the subject should be re-
garded to be more useful than topically-irrelevant
and extraneous terms. One way of measuring this
is using associations between the query and terms.
Statistical measures of associations between terms
include estimations by the co-occurrence in the
whole collection, such as Point-wise Mutual In-
formation (PMI) and Latent Semantic Analysis
(LSA). Another method is to use proximal infor-
mation of the query and the word, using syntactic
structure such as dependency relations of words
that provide the graphical representation of the
text (Mullen and Collier, 2004). The minimum
spans of words in such graph may represent their
associations in the text. Also, the distance between
words in the local context or in the thesaurus-
like dictionaries such as WordNet may be approx-
imated as such measure.
</bodyText>
<page confidence="0.826354">
254
</page>
<subsectionHeader confidence="0.99688">
3.2 Opinion Retrieval Model
</subsectionHeader>
<bodyText confidence="0.999984166666667">
The goal of an opinion retrieval system is to find a
set of opinionated documents that are relevant to a
given topic. We decompose the opinion retrieval
system into two tasks: the topical retrieval task
and the sentiment analysis task. This two-stage
approach for opinion retrieval has been taken by
many systems and has been shown to perform well
(Ounis et al., 2006). The topic and the sentiment
aspects of the opinion retrieval task are modeled
separately, and linearly combined together to pro-
duce a list of topically-relevant and opinionated
documents as below.
</bodyText>
<equation confidence="0.637472">
ScoreOpRet(D, Q) = a-Scorerel(D, Q)+(1−a)-Scoreop(D, Q)
</equation>
<bodyText confidence="0.999789722222222">
The topic-relevance model 5coreTel may be sub-
stituted by any IR system that retrieves relevant
documents for the query Q. For tasks such as
NTCIR MOAT, relevant documents are already
known in advance and it becomes unnecessary to
estimate the relevance degree of the documents.
We focus on modeling the sentiment aspect of
the opinion retrieval task, assuming that the topic-
relevance of documents is provided in some way.
To assign documents with sentiment degrees,
we estimate the probability of a document D to
generate a query Q and to possess opinions as in-
dicated by a random variable Op.1 Assuming uni-
form prior probabilities of documents D, query Q,
and Op, and conditional independence between Q
and Op, the opinion score function reduces to es-
timating the generative probability of Q and Op
given D.
</bodyText>
<equation confidence="0.602864">
Scoreop(D, Q) = p(D  |Op, Q) a p(Op, Q  |D)
</equation>
<bodyText confidence="0.999600125">
If we regard that the document D is represented
as a bag of words and that the words are uniformly
distributed, then
words from the document to have sentiment mean-
ings and associations with the given query.
In the following sections, we assess the three
factors of the sentiment models from the perspec-
tives of term weighting.
</bodyText>
<subsectionHeader confidence="0.904174">
3.2.1 Word Sentiment Model
</subsectionHeader>
<bodyText confidence="0.999982903225806">
Modeling the sentiment of a word has been a pop-
ular approach in sentiment analysis. There are
many publicly available lexicon resources. The
size, format, specificity, and reliability differ in all
these lexicons. For example, lexicon sizes range
from a few hundred to several hundred thousand.
Some lexicons assign real number scores to in-
dicate sentiment orientations and strengths (i.e.
probabilities of having positive and negative sen-
timents) (Esuli and Sebastiani, 2006) while other
lexicons assign discrete classes (weak/strong, pos-
itive/negative) (Wilson et al., 2005). There are
manually compiled lexicons (Stone et al., 1966)
while some are created semi-automatically by ex-
panding a set of seed terms (Esuli and Sebastiani,
2006).
The goal of this paper is not to create or choose
an appropriate sentiment lexicon, but rather it is
to discover useful term features other than the
sentiment properties. For this reason, one sen-
timent lexicon, namely SentiWordNet, is utilized
throughout the whole experiment.
SentiWordNet is an automatically generated
sentiment lexicon using a semi-supervised method
(Esuli and Sebastiani, 2006). It consists of Word-
Net synsets, where each synset is assigned three
probability scores that add up to 1: positive, nega-
tive, and objective.
These scores are assigned at sense level (synsets
in WordNet), and we use the following equations
to assess the sentiment scores at the word level.
</bodyText>
<equation confidence="0.997298571428571">
p(Pos  |w) = max SWNP os(s)
sEsynset(w)
Y_ p(Op, Q  |w) - p(w  |D) p(Ne9  |w) = max SWNNeg(s)
p(Op, Q  |D) = sEsynset(w)
wED
Y_ = p(Op  |w) - p(Q  |w) - p(w  |D) (1)
wED
</equation>
<bodyText confidence="0.999852714285714">
Equation 1 consists of three factors: the proba-
bility of a word to be opinionated (P(Oplw)), the
likelihood of a query given a word (P(QIw)), and
the probability of a document generating a word
(P(wID)). Intuitively speaking, the probability of
a document embodying topically related opinion is
estimated by accumulating the probabilities of all
</bodyText>
<equation confidence="0.7789435">
1Throughout this paper, Op indicates Op = 1.
p(Op  |w) = max (p(Pos  |w), p(Ne9  |w))
</equation>
<bodyText confidence="0.999821222222222">
where synset(w) is the set of synsets of w and
5WNPo3(s), 5WNNeg(s) are positive and neg-
ative scores of a synset in SentiWordNet. We as-
sess the subjective score of a word as the maxi-
mum value of the positive and the negative scores,
because a word has either a positive or a negative
sentiment in a given context.
The word sentiment model can also make use
of other types of sentiment lexicons. The sub-
</bodyText>
<page confidence="0.574914">
255
</page>
<bodyText confidence="0.999861166666667">
jectivity lexicon used in OpinionFinder2 is com-
piled from several manually and automatically
built resources. Each word in the lexicon is tagged
with the strength (strong/weak) and polarity (Pos-
itive/Negative/Neutral). The word sentiment can
be modeled as below.
</bodyText>
<equation confidence="0.999365">
P(Pos|w) = I
P(Op  |w) = max (p(Pos  |w), p(Neg  |w))
</equation>
<subsectionHeader confidence="0.492001">
3.2.2 Topic Association Model
</subsectionHeader>
<bodyText confidence="0.999983555555556">
If a topic is given in the sentiment analysis, terms
that are closely associated with the topic should
be assigned heavy weighting. For example, sen-
timent words such as scary and funny are more
likely to be associated with topic words such as
book and movie than grocery or refrigerator.
In the topic association model, p(Q I w) is es-
timated from the associations between the word w
and a set of query terms Q.
</bodyText>
<equation confidence="0.996663666666667">
EqEQ Asc-Score(q, w)
p(Q  |w) = a r Asc-Score(q, w)
 |Q  |qEQ
</equation>
<bodyText confidence="0.976639970588235">
Asc-Score(q, w) is the association score between
q and w, and I Q I is the number of query words.
To measure associations between words, we
employ statistical approaches using document col-
lections such as LSA and PMI, and local proximity
features using the distance in dependency trees or
texts.
Latent Semantic Analysis (LSA) (Landauer and
Dumais, 1997) creates a semantic space from a
collection of documents to measure the semantic
relatedness of words. Point-wise Mutual Informa-
tion (PMI) is a measure of associations used in in-
formation theory, where the association between
two words is evaluated with the joint and individ-
ual distributions of the two words. PMI-IR (Tur-
ney, 2001) uses an IR system and its search op-
erators to estimate the probabilities of two terms
and their conditional probabilities. Equations for
association scores using LSA and PMI are given
below.
2http://www.cs.pitt.edu/mpqa/
For the experimental purpose, we used publicly
available online demonstrations for LSA and PMI.
For LSA, we used the online demonstration mode
from the Latent Semantic Analysis page from the
University of Colorado at Boulder.3 For PMI, we
used the online API provided by the CogWorks
Lab at the Rensselaer Polytechnic Institute.4
Word associations between two terms may also
be evaluated in the local context where the terms
appear together. One way of measuring the prox-
imity of terms is using the syntactic structures.
Given the dependency tree of the text, we model
the association between two terms as below.
</bodyText>
<equation confidence="0.9349845">
r 1.0 min. span in dep. tree &lt; DgVn
Asc-ScoreDT P (w1, w2) =510.5 otherwise
</equation>
<bodyText confidence="0.958614888888889">
where, Dsy,,, is arbitrarily set to 3.
Another way is to use co-occurrence statistics
as below.
�
1.0 if distance betweenw1andw2 &lt; K
Asc-Score�P(w1,w2) = 0.5 otherwise
where K is the maximum window size for the
co-occurrence and is arbitrarily set to 3 in our ex-
periments.
The statistical approaches may suffer from data
sparseness problems especially for named entity
terms used in the query, and the proximal clues
cannot sufficiently cover all term–query associa-
tions. To avoid assigning zero probabilities, our
topic association models assign 0.5 to word pairs
with no association and 1.0 to words with perfect
association.
Note that proximal features using co-occurrence
and dependency relationships were used in pre-
vious work. For opinion retrieval tasks, Yang et
al. (2006) and Zhang and Ye (2008) used the co-
occurrence of a query word and a sentiment word
within a certain window size. Mullen and Collier
(2004) manually annotated named entities in their
dataset (i.e. title of the record and name of the
artist for music record reviews), and utilized pres-
ence and position features in their ML approach.
</bodyText>
<subsectionHeader confidence="0.84697">
3.2.3 Word Generation Model
</subsectionHeader>
<bodyText confidence="0.9970195">
Our word generation model p(w I d) evaluates the
prominence and the discriminativeness of a word
</bodyText>
<table confidence="0.424698933333333">
3http://lsa.colorado.edu/, default parameter settings for
the semantic space (TASA, 1st year college level) and num-
ber of factors (300).
4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with the
Google Search Engine.
1.0 if w is Positive and Strong
0.5 if w is Positive and Weak
0.0 otherwise
Asc-ScoreLSa(w1, w2) = 1 + LSA(w1, w2)2
1 + PMI-IR(w1, w2)
Asc-ScorePmI(w1, w2) = 2
256
w in a document d. These issues correspond to the F: normalized idf as in BM25
core issues of traditional IR tasks. IR models, such
as Vector Space (VS), probabilistic models such
as BM25, and Language Modeling (LM), albeit in
different forms of approach and measure, employ
heuristics and formal modeling approaches to ef-
fectively evaluate the relevance of a term to a doc-
ument (Fang et al., 2004). Therefore, we estimate
the word generation model with popular IR mod-
els’ the relevance scores of a document d given w
as a query.5
p(w  |d) =_ IR-SCORE(w, d)
In our experiments, we use the Vector Space
model with Pivoted Normalization (VS), Proba-
bilistic model (BM25), and Language modeling
with Dirichlet Smoothing (LM).
VSPN(w, d) = 1 + ln(1 + ln(c(w, d))) •!n N+1
df (w)
 |d |
(1 − s) + s ·
avgdl
BM25(w, d) = ln N − df(w) + 0.5 · (k1 + 1) · c(w, d) t to the topic.
df(w) + 0.5 k1((1 − b) + bavg|dl + c(w, d)
1 +
LMDI(w, d) = ln
µ
µ ��+µ
4.1 Opinion Retieval Task – TREC Blog
Track
�
c(w, d)
• c(w, C)
+ ln
</table>
<bodyText confidence="0.8333086">
c(w, d) is the frequency of w in d,
d
is the
number of unique terms in d, avgdl is the average
d
of all documents, N is the number of doc-
uments in the collection,
is the number of
documents with w, C is the entire collection, and
ki and b are constants 2.0 an
</bodyText>
<figure confidence="0.593347588235294">
|
|
|
|
df(w)
d 0.75.
sented as a vector of features. We test vari
ous
combinations of the term weighting schemes listed
below.
proper assumptions and derivations,
d) can
be derived to language modeling approaches. Refer to (Zhai
and Laffert
5With
p(w|
y, 2004).
</figure>
<subsectionHeader confidence="0.991233">
3.3 Data-driven Approach
</subsectionHeader>
<bodyText confidence="0.979497555555555">
To verify the effectiveness of our term weight-
ing schemes in experimental settings of the data-
driven approach, we carry out a set of simple ex-
periments with ML classifiers. Specifically, we
explore the statistical term weighting features of
the word generation model with Support Vector
machine (SVM), faithfully reproducing previous
work as closely as possible (Pang et al., 2002).
Each instance of train and test data is repre-
</bodyText>
<listItem confidence="0.984587888888889">
•PRESENCE: binary indicator for the pres-
ence of a term
• TF: term fr
equency
257 • VS.TF: normalized tf as in VS
• BM25.TF: normalized tf as in BM25
• IDF: inverse document frequency
• VS.IDF: normalized idf as in VS
• BM25.ID
</listItem>
<sectionHeader confidence="0.849895" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999935266666667">
Our experiments consist of an opinion retrieval
task and a sentiment classification task. We use
MPQA and movie-review corpora in our experi-
ments with an ML classifier. For the opinion re-
trieval task, we use the two datasets used by TREC
blog track and NTCIR MOAT evaluation work-
shops.
The opinion retrieval task at TREC Blog Track
consists of three subtasks: topic retrieval, opinion
retrieval, and polarity retrieval. Opinion and polar-
ity retrieval subtasks use the relevant documents
retrieved at the topic retrieval stage. On the other
hand, the NTCIR MOAT task aims to find opin-
ionated sentences given a set of documents that are
already hand-assessed to be relevan
</bodyText>
<subsubsectionHeader confidence="0.483124">
4.1.1 Experimental Setting
</subsubsectionHeader>
<bodyText confidence="0.988320233333333">
TREC Blog Track uses the TREC
corpus
(Macdonald and Ounis, 2006). It is a collection
of RSS feeds (38.6 GB), permalink documents
(88.8GB), and homepages (28.8GB) crawled on
the Internet over an eleven week period from De-
cember 2005 to February 2006.
Non-relevant content of blog posts such as
HTML tags, advertisement, site description, and
menu are removed with an effective internal spam
removal algorithm (Nam et al., 2009). While our
sentiment analysis model uses the entire relevant
portion of the blog posts, further stopword re-
moval and stemming is done for the blog retrieval
system.
For the relevance retrieval model, we faithfully
reproduce the passage-based language model with
pseudo-relevance feedback (Lee et al., 2008).
We use in total 100 topics from TREC 2007 and
2008 blog opinion retrieval tasks (07:901-950 and
08:1001-1050). We use the topics from Blog 07
to optimize the parameter for linearly combining
the retrieval and opinion models, and use Blog 08
topics as our test data. Topics are extracted only
from the Title field, using the Porter
an
Blog06
stemmer
d
a stopword list.
</bodyText>
<tableCaption confidence="0.518261">
Table 1: Performance of opinion retrieval models
using Blog 08 topics. The linear combination pa-
rameter A is optimized on Blog 07 topics. † indi-
cates statistical significance at the 1% level over
the baseline.
</tableCaption>
<table confidence="0.978142866666667">
Model MAP R-prec P@10
TOPIC REL. 0.4052 0.4366 0.6440
BASELINE 0.4141 0.4534 0.6440
VS 0.4196 0.4542 0.6600
BM25 0.4235† 0.4579 0.6600
LM 0.4158 0.4520 0.6560
PMI 0.4177 0.4538 0.6620
LSA 0.4155 0.4526 0.6480
WP 0.4165 0.4533 0.6640
BM25·PMI 0.4238† 0.4575 0.6600
BM25·LSA 0.4237† 0.4578 0.6600
BM25·WP 0.4237† 0.4579 0.6600
BM25·PMI·WP 0.4242† 0.4574 0.6620
BM25·LSA·WP 0.4238† 0.4576 0.6580
4.1.2 Experimental Result
</table>
<tableCaption confidence="0.706456">
Retrieval performances using different combina-
tions of term weighting features are presented in
Table 1. Using only the word sentiment model is
</tableCaption>
<bodyText confidence="0.995515655172414">
set as our baseline.
First, each feature of the word generation and
topic association models are tested; all features of
the models improve over the baseline. We observe
that the features of our word generation model is
more effective than those of the topic association
model. Among the features of the word generation
model, the most improvement was achieved with
BM25, improving the MAP by 2.27%.
Features of the topic association model show
only moderate improvements over the baseline.
We observe that these features generally improve
P@10 performance, indicating that they increase
the accuracy of the sentiment analysis system.
PMI out-performed LSA for all evaluation mea-
sures. Among the topic association models, PMI
performs the best in MAP and R-prec, while WP
achieved the biggest improvement in P@10.
Since BM25 performs the best among the word
generation models, its combination with other fea-
tures was investigated. Combinations of BM25
with the topic association models all improve the
performance of the baseline and BM25. This
demonstrates that the word generation model and
the topic association model are complementary to
each other.
The best MAP was achieved with BM25, PMI,
and WP (+2.44% over the baseline). We observe
that PMI and WP also complement each other.
</bodyText>
<subsectionHeader confidence="0.588407666666667">
4.2 Sentiment Analysis Task – NTCIR
MOAT
4.2.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999988868421053">
Another set of experiments for our opinion analy-
sis model was carried out on the NTCIR-7 MOAT
English corpus. The English opinion corpus
for NTCIR MOAT consists of newspaper articles
from the Mainichi Daily News, Korea Times, Xin-
hua News, Hong Kong Standard, and the Straits
Times. It is a collection of documents manu-
ally assessed for relevance to a set of queries
from NTCIR-7 Advanced Cross-lingual Informa-
tion Access (ACLIA) task. The corpus consists of
167 documents, or 4,711 sentences for 14 test top-
ics. Each sentence is manually tagged with opin-
ionatedness, polarity, and relevance to the topic by
three annotators from a pool of six annotators.
For preprocessing, no removal or stemming is
performed on the data. Each sentence was pro-
cessed with the Stanford English parser6 to pro-
duce a dependency parse tree. Only the Title fields
of the topics were used.
For performance evaluations of opinion and po-
larity detection, we use precision, recall, and F-
measure, the same measure used to report the offi-
cial results at the NTCIR MOAT workshop. There
are lenient and strict evaluations depending on the
agreement of the annotators; if two out of three an-
notators agreed upon an opinion or polarity anno-
tation then it is used during the lenient evaluation,
similarly three out of three agreements are used
during the strict evaluation. We present the perfor-
mances using the lenient evaluation only, for the
two evaluations generally do not show much dif-
ference in relative performance changes.
Since MOAT is a classification task, we use a
threshold parameter to draw a boundary between
opinionated and non-opinionated sentences. We
report the performance of our system using the
NTCIR-7 dataset, where the threshold parameter
is optimized using the NTCIR-6 dataset.
</bodyText>
<subsectionHeader confidence="0.552618">
4.2.2 Experimental Result
</subsectionHeader>
<bodyText confidence="0.9981175">
We present the performance of our sentiment anal-
ysis system in Table 2. As in the experiments with
</bodyText>
<footnote confidence="0.844383">
6http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.801065">
258
</page>
<tableCaption confidence="0.980404">
Table 2: Performance of the Sentiment Analy-
</tableCaption>
<table confidence="0.958503411764706">
sis System on NTCIR7 dataset. System parame-
ters are optimized for F-measure using NTCIR6
dataset with lenient evaluations.
Opinionated
Model Precision Recall F-Measure
BASELINE 0.305 0.866 0.451
VS 0.331 0.807 0.470
BM25 0.327 0.795 0.464
LM 0.325 0.794 0.461
LSA 0.315 0.806 0.453
PMI 0.342 0.603 0.436
DTP 0.322 0.778 0.455
VS·LSA 0.335 0.769 0.466
VS·PMI 0.311 0.833 0.453
VS·DTP 0.342 0.745 0.469
VS·LSA·DTP 0.349 0.719 0.470
VS·PMI·DTP 0.328 0.773 0.461
</table>
<bodyText confidence="0.997767958333333">
the TREC dataset, using only the word sentiment
model is used as our baseline.
Similarly to the TREC experiments, the features
of the word generation model perform exception-
ally better than that of the topic association model.
The best performing feature of the word genera-
tion model is VS, achieving a 4.21% improvement
over the baseline’s f-measure. Interestingly, this is
the tied top performing f-measure over all combi-
nations of our features.
While LSA and DTP show mild improvements,
PMI performed worse than baseline, with higher
precision but a drop in recall. DTP was the best
performing topic association model.
When combining the best performing feature
of the word generation model (VS) with the fea-
tures of the topic association model, LSA, PMI
and DTP all performed worse than or as well as
the VS in f-measure evaluation. LSA and DTP im-
proves precision slightly, but with a drop in recall.
PMI shows the opposite tendency.
The best performing system was achieved using
VS, LSA and DTP at both precision and f-measure
evaluations.
</bodyText>
<subsectionHeader confidence="0.9955325">
4.3 Classification task – SVM
4.3.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999645">
To test our SVM classifier, we perform the classi-
fication task. Movie Review polarity dataset7 was
</bodyText>
<note confidence="0.5343935">
7http://www.cs.cornell.edu/people/pabo/movie-review-
data/
</note>
<tableCaption confidence="0.9496385">
Table 3: Average ten-fold cross-validation accura-
cies of polarity classification task with SVM.
</tableCaption>
<table confidence="0.999281166666667">
Accuracy
Features Movie-review MPQA
PRESENCE 82.6 76.8
TF 71.1 76.5
VS.TF 81.3 76.7
BM25.TF 81.4 77.9
IDF 61.6 61.8
VS.IDF 83.6 77.9
BM25.IDF 83.6 77.8
VS.TF·VS.IDF 83.8 77.9
BM25.TF·BM25.IDF 84.1 77.7
BM25.TF·VS.IDF 85.1 77.7
</table>
<bodyText confidence="0.999758642857143">
first introduced by Pang et al. (2002) to test various
ML-based methods for sentiment classification. It
is a balanced dataset of 700 positive and 700 neg-
ative reviews, collected from the Internet Movie
Database (IMDb) archive. MPQA Corpus8 con-
tains 535 newspaper articles manually annotated
at sentence and subsentence level for opinions and
other private states (Wiebe et al., 2005).
To closely reproduce the experiment with the
best performance carried out in (Pang et al., 2002)
using SVM, we use unigram with the presence
feature. We test various combinations of our fea-
tures applicable to the task. For evaluation, we use
ten-fold cross-validation accuracy.
</bodyText>
<subsectionHeader confidence="0.867915">
4.3.2 Experimental Result
</subsectionHeader>
<bodyText confidence="0.999918466666667">
We present the sentiment classification perfor-
mances in Table 3.
As observed by Pang et al. (2002), using the raw
tf drops the accuracy of the sentiment classifica-
tion (-13.92%) of movie-review data. Using the
raw idf feature worsens the accuracy even more
(-25.42%). Normalized tf-variants show improve-
ments over tf but are worse than presence. Nor-
malized idf features produce slightly better accu-
racy results than the baseline. Finally, combining
any normalized tf and idf features improved the
baseline (high 83% ∼ low 85%). The best combi-
nation was BM25.TF·VS.IDF.
MPQA corpus reveals similar but somewhat un-
certain tendency.
</bodyText>
<footnote confidence="0.73646">
8http://www.cs.pitt.edu/mpqa/databaserelease/
</footnote>
<page confidence="0.695813">
259
</page>
<subsectionHeader confidence="0.880578">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999986489795918">
Overall, the opinion retrieval and the sentiment
analysis models achieve improvements using our
proposed features. Especially, the features of the
word generation model improve the overall per-
formances drastically. Its effectiveness is also ver-
ified with a data-driven approach; the accuracy of
a sentiment classifier trained on a polarity dataset
was improved by various combinations of normal-
ized tf and idf statistics.
Differences in effectiveness of VS, BM25, and
LM come from parameter tuning and corpus dif-
ferences. For the TREC dataset, BM25 performed
better than the other models, and for the NTCIR
dataset, VS performed better.
Our features of the topic association model
show mild improvement over the baseline perfor-
mance in general. PMI and LSA, both modeling
the semantic associations between words, show
different behaviors on the datasets. For the NT-
CIR dataset, LSA performed better, while PMI
is more effective for the TREC dataset. We be-
lieve that the explanation lies in the differences
between the topics for each dataset. In general,
the NTCIR topics are general descriptive words
such as “regenerative medicine”, “American econ-
omy after the 911 terrorist attacks”, and “law-
suit brought against Microsoft for monopolistic
practices.” The TREC topics are more named-
entity-like terms such as “Carmax”, “Wikipedia
primary source”, “Jiffy Lube”, “Starbucks”, and
“Windows Vista.” We have experimentally shown
that LSA is more suited to finding associations
between general terms because its training docu-
ments are from a general domain.9 Our PMI mea-
sure utilizes a web search engine, which covers a
variety of named entity terms.
Though the features of our topic association
model, WP and DTP, were evaluated on different
datasets, we try our best to conjecture the differ-
ences. WP on TREC dataset shows a small im-
provement of MAP compared to other topic asso-
ciation features, while the precision is improved
the most when this feature is used alone. The DTP
feature displays similar behavior with precision. It
also achieves the best f-measure over other topic
association features. DTP achieves higher rela-
tive improvement (3.99% F-measure verse 2.32%
MAP), and is more effective for improving the per-
formance in combination with LSA and PMI.
</bodyText>
<footnote confidence="0.556571">
9TASA Corpus, http://lsa.colorado.edu/spaces.html
</footnote>
<sectionHeader confidence="0.998149" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999921117647059">
In this paper, we proposed various term weighting
schemes and how such features are modeled in the
sentiment analysis task. Our proposed features in-
clude corpus statistics, association measures using
semantic and local-context proximities. We have
empirically shown the effectiveness of the features
with our proposed opinion retrieval and sentiment
analysis models.
There exists much room for improvement with
further experiments with various term weighting
methods and datasets. Such methods include,
but by no means limited to, semantic similarities
between word pairs using lexical resources such
as WordNet (Miller, 1995) and data-driven meth-
ods with various topic-dependent term weighting
schemes on labeled corpus with topics such as
MPQA.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999452333333333">
This work was supported in part by MKE &amp; IITA
through IT Leading R&amp;D Support Project and in
part by the BK 21 Project in 2009.
</bodyText>
<sectionHeader confidence="0.995472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9879340625">
Kushal Dave, Steve Lawrence, and David M. Pennock. 2003.
Mining the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519–528.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Language
Resources and Evaluation (LREC’06), pages 417–422,
Geneva, IT.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal
study of information retrieval heuristics. In SIGIR ’04:
Proceedings of the 27th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 49–56, New York, NY, USA. ACM.
George Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal of
Machine Learning Research, 3:1289–1305.
Michael Gamon. 2004. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and the
role of linguistic analysis. In Proceedings of the Inter-
national Conference on Computational Linguistics (COL-
ING).
Vasileios Hatzivassiloglou and Kathleen R. Mckeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL’97), pages 174–181,
madrid, ES.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure se-
mantic orientation of adjectives. In Proceedings of the
4th International Conference on Language Resources and
Evaluation (LREC’04), pages 1115–1118, Lisbon, PT.
</reference>
<page confidence="0.564565">
260
</page>
<reference confidence="0.997620766666667">
Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato’s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211–240, April.
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob Nam,
Hun young Jung, and Jong-Hyeok Lee. 2008. Kle at trec
2008 blog track: Blog post and feed retrieval. In Proceed-
ings of TREC-08.
Craig Macdonald and Iadh Ounis. 2006. The TREC Blogs06
collection: creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Shotaro Matsumoto, Hiroya Takamura, and Manabu Oku-
mura. 2005. Sentiment classification using word sub-
sequences and dependency sub-trees. In Proceedings of
PAKDD’05, the 9th Pacific-Asia Conference on Advances
in Knowledge Discovery and Data Mining.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture: Mod-
eling facets and opinions in weblogs. In Proceedings of
WWW, pages 171–180, New York, NY, USA. ACM Press.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39–41.
Tony Mullen and Nigel Collier. 2004. Sentiment analysis
using support vector machines with diverse information
sources. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 412–418, July. Poster paper.
Sang-Hyob Nam, Seung-Hoon Na, Yeha Lee, and Jong-
Hyeok Lee. 2009. Diffpost: Filtering non-relevant con-
tent based on content difference between two consecutive
blog posts. In ECIR.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in the
automatic identification and classification of reviews. In
Proceedings of the COLING/ACL Main Conference Poster
Sessions, pages 611–618, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and
I. Soboroff. 2006. Overview of the trec-2006 blog track.
In Proceedings of TREC-06, pages 15–27, November.
I. Ounis, C. Macdonald, and I. Soboroff. 2008. Overview
of the trec-2008 blog track. In Proceedings of TREC-08,
pages 15–27, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 79–86.
Fabrizio Sebastiani. 2002. Machine learning in automated
text categorization. ACM Computing Surveys, 34(1):1–47.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-
Hsi Chen, and Noriko Kando. 2008. Overview of mul-
tilingual opinion analysis task at ntcir-7. In Proceedings
of The 7th NTCIR Workshop (2007/2008) - Evaluation of
Information Access Technologies: Information Retrieval,
Question Answering and Cross-Lingual Information Ac-
cess.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, USA.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315–346.
Peter D. Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL ’01: Proceedings of the
12th European Conference on Machine Learning, pages
491–502, London, UK. Springer-Verlag.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis. In
Proceedings of the 14th ACM international conference
on Information and knowledge management (CIKM’05),
pages 625–631, Bremen, DE.
Janyce Wiebe, E. Breck, Christopher Buckley, Claire Cardie,
P. Davis, B. Fraser, Diane Litman, D. Pierce, Ellen Riloff,
Theresa Wilson, D. Day, and Mark Maybury. 2003. Rec-
ognizing and organizing opinions expressed in the world
press. In Proceedings of the 2003 AAAI Spring Sympo-
sium on New Directions in Question Answering.
Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computational Linguistics, 30(3):277–308,
September.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2/3):164–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP’05), pages 347–354,
Vancouver, CA.
Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.
2006. WIDIT in TREC-2006 Blog track. In Proceedings
of TREC.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of 2003 Conference on the Empirical Methods in
Natural Language Processing (EMNLP’03), pages 129–
136, Sapporo, JP.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to infor-
mation retrieval. ACM Trans. Inf. Syst., 22(2):179–214.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment for
opinion retrieval. In SIGIR ’08: Proceedings of the 31st
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 411–418,
New York, NY, USA. ACM.
</reference>
<page confidence="0.91252">
261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951869">
<title confidence="0.999378">Discovering the Discriminative Views: Measuring Term Weights for Sentiment Analysis</title>
<author confidence="0.99779">Jungi Kim</author>
<author confidence="0.99779">Jin-Ji Li</author>
<author confidence="0.99779">Jong-Hyeok Lee</author>
<affiliation confidence="0.9949525">Division of Electrical and Computer Engineering Pohang University of Science and Technology, Pohang, Republic of Korea</affiliation>
<abstract confidence="0.998137105263158">This paper describes an approach to utilizing term weights for sentiment analysis tasks and shows how various term weighting schemes improve the performance of sentiment analysis systems. Previously, sentiment analysis was mostly studied under data-driven and lexicon-based frameworks. Such work generally exploits textual features for fact-based analysis tasks or lexical indicators from a sentiment lexicon. We propose to model term weighting into a sentiment analysis system utilizing collection statistics, contextual and topicrelated characteristics as well as opinionrelated properties. Experiments carried out on various datasets show that our approach effectively improves previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="5752" citStr="Dave et al., 2003" startWordPosition="871" endWordPosition="874"> using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other hand, Wiebe et al. (2004) have noted that hapax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Other works have also exploited rarely occurring terms for sentiment analysis tasks (Dave et al., 2003; Yang et al., 2006). The opinion retrieval task is a relatively recent issue that draws both the attention of IR and NLP communities. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking the documents by the opinion scores (Ounis et al., 2006). This approach is also appropriate for evaluation systems such as NTCIR MOAT that assumes that the set of topically relevant documents are already known in advance. On the other</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC’06),</booktitle>
<pages>417--422</pages>
<location>Geneva, IT.</location>
<contexts>
<context position="4924" citStr="Esuli and Sebastiani, 2006" startWordPosition="742" endWordPosition="745"> sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other hand, Wiebe et al. (2004) hav</context>
<context position="10906" citStr="Esuli and Sebastiani, 2006" startWordPosition="1707" endWordPosition="1710">ry. In the following sections, we assess the three factors of the sentiment models from the perspectives of term weighting. 3.2.1 Word Sentiment Model Modeling the sentiment of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al., 2005). There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006). The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties. For this reason, one sentiment lexicon, namely SentiWordNet, is utilized throughout the whole experiment. SentiWordNet is an automatically generated sentiment lexic</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC’06), pages 417–422, Geneva, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Fang</author>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A formal study of information retrieval heuristics.</title>
<date>2004</date>
<booktitle>In SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>49--56</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7533" citStr="Fang et al., 2004" startWordPosition="1153" endWordPosition="1156">ke of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-independently speaking, a good sentiment term is discriminative and prominent, such that the appearance of the term imposes greater influence on the judgment of the analysis system. The rare occurrence of terms in document collections has been regarded as a very important feature in IR methods, and effective IR models of today, either explicitly or implicitly, accommodate this feature as an Inverse Document Frequency (IDF) heuristic (Fang et al., 2004). Similarly, prominence of a term is recognized by the frequency of the term in its local context, formulated as Term Frequency (TF) in IR. If a topic of the text is known, terms that are relevant and descriptive of the subject should be regarded to be more useful than topically-irrelevant and extraneous terms. One way of measuring this is using associations between the query and terms. Statistical measures of associations between terms include estimations by the co-occurrence in the whole collection, such as Point-wise Mutual Information (PMI) and Latent Semantic Analysis (LSA). Another metho</context>
<context position="17283" citStr="Fang et al., 2004" startWordPosition="2764" endWordPosition="2767">sci.rpi.edu/msr/, PMI-IR with the Google Search Engine. 1.0 if w is Positive and Strong 0.5 if w is Positive and Weak 0.0 otherwise Asc-ScoreLSa(w1, w2) = 1 + LSA(w1, w2)2 1 + PMI-IR(w1, w2) Asc-ScorePmI(w1, w2) = 2 256 w in a document d. These issues correspond to the F: normalized idf as in BM25 core issues of traditional IR tasks. IR models, such as Vector Space (VS), probabilistic models such as BM25, and Language Modeling (LM), albeit in different forms of approach and measure, employ heuristics and formal modeling approaches to effectively evaluate the relevance of a term to a document (Fang et al., 2004). Therefore, we estimate the word generation model with popular IR models’ the relevance scores of a document d given w as a query.5 p(w |d) =_ IR-SCORE(w, d) In our experiments, we use the Vector Space model with Pivoted Normalization (VS), Probabilistic model (BM25), and Language modeling with Dirichlet Smoothing (LM). VSPN(w, d) = 1 + ln(1 + ln(c(w, d))) •!n N+1 df (w) |d | (1 − s) + s · avgdl BM25(w, d) = ln N − df(w) + 0.5 · (k1 + 1) · c(w, d) t to the topic. df(w) + 0.5 k1((1 − b) + bavg|dl + c(w, d) 1 + LMDI(w, d) = ln µ µ ��+µ 4.1 Opinion Retieval Task – TREC Blog Track � c(w, d) • c(w</context>
</contexts>
<marker>Fang, Tao, Zhai, 2004</marker>
<rawString>Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49–56, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="3936" citStr="Forman, 2003" startWordPosition="594" endWordPosition="595">with TREC and NTCIR datasets to validate the effectiveness of our term weighting features. We also verify the effectiveness of the statistical features used in datadriven approaches by evaluating an ML classifier with labeled corpora. 2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make u</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="4355" citStr="Gamon, 2004" startWordPosition="660" endWordPosition="661">e 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conf</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R Mckeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97),</booktitle>
<pages>174--181</pages>
<location>madrid, ES.</location>
<contexts>
<context position="4826" citStr="Hatzivassiloglou and Mckeown, 1997" startWordPosition="726" endWordPosition="729"> dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms </context>
</contexts>
<marker>Hatzivassiloglou, Mckeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. Mckeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97), pages 174–181, madrid, ES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten De Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<pages>1115--1118</pages>
<location>Lisbon, PT.</location>
<marker>Kamps, Marx, Mokken, De Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten De Rijke. 2004. Using wordnet to measure semantic orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), pages 1115–1118, Lisbon, PT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4342" citStr="Kudo and Matsumoto, 2004" startWordPosition="656" endWordPosition="659"> for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, t</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="14023" citStr="Landauer and Dumais, 1997" startWordPosition="2235" endWordPosition="2238">ly to be associated with topic words such as book and movie than grocery or refrigerator. In the topic association model, p(Q I w) is estimated from the associations between the word w and a set of query terms Q. EqEQ Asc-Score(q, w) p(Q |w) = a r Asc-Score(q, w) |Q |qEQ Asc-Score(q, w) is the association score between q and w, and I Q I is the number of query words. To measure associations between words, we employ statistical approaches using document collections such as LSA and PMI, and local proximity features using the distance in dependency trees or texts. Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) creates a semantic space from a collection of documents to measure the semantic relatedness of words. Point-wise Mutual Information (PMI) is a measure of associations used in information theory, where the association between two words is evaluated with the joint and individual distributions of the two words. PMI-IR (Turney, 2001) uses an IR system and its search operators to estimate the probabilities of two terms and their conditional probabilities. Equations for association scores using LSA and PMI are given below. 2http://www.cs.pitt.edu/mpqa/ For the experimental purpose, we used publicly</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeha Lee</author>
<author>Seung-Hoon Na</author>
<author>Jungi Kim</author>
<author>Sang-Hyob Nam</author>
<author>Hun young Jung</author>
<author>Jong-Hyeok Lee</author>
</authors>
<title>Kle at trec</title>
<date>2008</date>
<booktitle>In Proceedings of TREC-08.</booktitle>
<contexts>
<context position="20540" citStr="Lee et al., 2008" startWordPosition="3343" endWordPosition="3346">nk documents (88.8GB), and homepages (28.8GB) crawled on the Internet over an eleven week period from December 2005 to February 2006. Non-relevant content of blog posts such as HTML tags, advertisement, site description, and menu are removed with an effective internal spam removal algorithm (Nam et al., 2009). While our sentiment analysis model uses the entire relevant portion of the blog posts, further stopword removal and stemming is done for the blog retrieval system. For the relevance retrieval model, we faithfully reproduce the passage-based language model with pseudo-relevance feedback (Lee et al., 2008). We use in total 100 topics from TREC 2007 and 2008 blog opinion retrieval tasks (07:901-950 and 08:1001-1050). We use the topics from Blog 07 to optimize the parameter for linearly combining the retrieval and opinion models, and use Blog 08 topics as our test data. Topics are extracted only from the Title field, using the Porter an Blog06 stemmer d a stopword list. Table 1: Performance of opinion retrieval models using Blog 08 topics. The linear combination parameter A is optimized on Blog 07 topics. † indicates statistical significance at the 1% level over the baseline. Model MAP R-prec P@1</context>
</contexts>
<marker>Lee, Na, Kim, Nam, Jung, Lee, 2008</marker>
<rawString>Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob Nam, Hun young Jung, and Jong-Hyeok Lee. 2008. Kle at trec 2008 blog track: Blog post and feed retrieval. In Proceedings of TREC-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Macdonald</author>
<author>Iadh Ounis</author>
</authors>
<title>The TREC Blogs06 collection: creating and analysing a blog test collection.</title>
<date>2006</date>
<tech>Technical Report TR-2006-224,</tech>
<institution>Department of Computer Science, University of Glasgow.</institution>
<contexts>
<context position="19871" citStr="Macdonald and Ounis, 2006" startWordPosition="3239" endWordPosition="3242">periments with an ML classifier. For the opinion retrieval task, we use the two datasets used by TREC blog track and NTCIR MOAT evaluation workshops. The opinion retrieval task at TREC Blog Track consists of three subtasks: topic retrieval, opinion retrieval, and polarity retrieval. Opinion and polarity retrieval subtasks use the relevant documents retrieved at the topic retrieval stage. On the other hand, the NTCIR MOAT task aims to find opinionated sentences given a set of documents that are already hand-assessed to be relevan 4.1.1 Experimental Setting TREC Blog Track uses the TREC corpus (Macdonald and Ounis, 2006). It is a collection of RSS feeds (38.6 GB), permalink documents (88.8GB), and homepages (28.8GB) crawled on the Internet over an eleven week period from December 2005 to February 2006. Non-relevant content of blog posts such as HTML tags, advertisement, site description, and menu are removed with an effective internal spam removal algorithm (Nam et al., 2009). While our sentiment analysis model uses the entire relevant portion of the blog posts, further stopword removal and stemming is done for the blog retrieval system. For the relevance retrieval model, we faithfully reproduce the passage-b</context>
</contexts>
<marker>Macdonald, Ounis, 2006</marker>
<rawString>Craig Macdonald and Iadh Ounis. 2006. The TREC Blogs06 collection: creating and analysing a blog test collection. Technical Report TR-2006-224, Department of Computer Science, University of Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word subsequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>In Proceedings of PAKDD’05, the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="4379" citStr="Matsumoto et al., 2005" startWordPosition="662" endWordPosition="666"> Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions abou</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word subsequences and dependency sub-trees. In Proceedings of PAKDD’05, the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: Modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>171--180</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6480" citStr="Mei et al., 2007" startWordPosition="993" endWordPosition="996">R and NLP communities. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking the documents by the opinion scores (Ounis et al., 2006). This approach is also appropriate for evaluation systems such as NTCIR MOAT that assumes that the set of topically relevant documents are already known in advance. On the other hand, there are also some interesting works on modeling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008). 3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. 3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features. For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-ind</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: Modeling facets and opinions in weblogs. In Proceedings of WWW, pages 171–180, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Commun. ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>412--418</pages>
<note>Poster paper.</note>
<contexts>
<context position="8337" citStr="Mullen and Collier, 2004" startWordPosition="1284" endWordPosition="1287">that are relevant and descriptive of the subject should be regarded to be more useful than topically-irrelevant and extraneous terms. One way of measuring this is using associations between the query and terms. Statistical measures of associations between terms include estimations by the co-occurrence in the whole collection, such as Point-wise Mutual Information (PMI) and Latent Semantic Analysis (LSA). Another method is to use proximal information of the query and the word, using syntactic structure such as dependency relations of words that provide the graphical representation of the text (Mullen and Collier, 2004). The minimum spans of words in such graph may represent their associations in the text. Also, the distance between words in the local context or in the thesauruslike dictionaries such as WordNet may be approximated as such measure. 254 3.2 Opinion Retrieval Model The goal of an opinion retrieval system is to find a set of opinionated documents that are relevant to a given topic. We decompose the opinion retrieval system into two tasks: the topical retrieval task and the sentiment analysis task. This two-stage approach for opinion retrieval has been taken by many systems and has been shown to </context>
<context position="16185" citStr="Mullen and Collier (2004)" startWordPosition="2585" endWordPosition="2588">aches may suffer from data sparseness problems especially for named entity terms used in the query, and the proximal clues cannot sufficiently cover all term–query associations. To avoid assigning zero probabilities, our topic association models assign 0.5 to word pairs with no association and 1.0 to words with perfect association. Note that proximal features using co-occurrence and dependency relationships were used in previous work. For opinion retrieval tasks, Yang et al. (2006) and Zhang and Ye (2008) used the cooccurrence of a query word and a sentiment word within a certain window size. Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach. 3.2.3 Word Generation Model Our word generation model p(w I d) evaluates the prominence and the discriminativeness of a word 3http://lsa.colorado.edu/, default parameter settings for the semantic space (TASA, 1st year college level) and number of factors (300). 4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with the Google Search Engine. 1.0 if w is Positive and Strong 0.5 if w is Positive and Weak 0.</context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 412–418, July. Poster paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sang-Hyob Nam</author>
<author>Seung-Hoon Na</author>
<author>Yeha Lee</author>
<author>JongHyeok Lee</author>
</authors>
<title>Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts.</title>
<date>2009</date>
<booktitle>In ECIR.</booktitle>
<contexts>
<context position="20233" citStr="Nam et al., 2009" startWordPosition="3297" endWordPosition="3300">ic retrieval stage. On the other hand, the NTCIR MOAT task aims to find opinionated sentences given a set of documents that are already hand-assessed to be relevan 4.1.1 Experimental Setting TREC Blog Track uses the TREC corpus (Macdonald and Ounis, 2006). It is a collection of RSS feeds (38.6 GB), permalink documents (88.8GB), and homepages (28.8GB) crawled on the Internet over an eleven week period from December 2005 to February 2006. Non-relevant content of blog posts such as HTML tags, advertisement, site description, and menu are removed with an effective internal spam removal algorithm (Nam et al., 2009). While our sentiment analysis model uses the entire relevant portion of the blog posts, further stopword removal and stemming is done for the blog retrieval system. For the relevance retrieval model, we faithfully reproduce the passage-based language model with pseudo-relevance feedback (Lee et al., 2008). We use in total 100 topics from TREC 2007 and 2008 blog opinion retrieval tasks (07:901-950 and 08:1001-1050). We use the topics from Blog 07 to optimize the parameter for linearly combining the retrieval and opinion models, and use Blog 08 topics as our test data. Topics are extracted only</context>
</contexts>
<marker>Nam, Na, Lee, Lee, 2009</marker>
<rawString>Sang-Hyob Nam, Seung-Hoon Na, Yeha Lee, and JongHyeok Lee. 2009. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL Main Conference Poster Sessions,</booktitle>
<pages>611--618</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4397" citStr="Ng et al., 2006" startWordPosition="667" endWordPosition="670">the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness o</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL Main Conference Poster Sessions, pages 611–618, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Ounis</author>
<author>M de Rijke</author>
<author>C Macdonald</author>
<author>G A Mishne</author>
<author>I Soboroff</author>
</authors>
<title>Overview of the trec-2006 blog track.</title>
<date>2006</date>
<booktitle>In Proceedings of TREC-06,</booktitle>
<pages>15--27</pages>
<marker>Ounis, de Rijke, Macdonald, Mishne, Soboroff, 2006</marker>
<rawString>I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and I. Soboroff. 2006. Overview of the trec-2006 blog track. In Proceedings of TREC-06, pages 15–27, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Ounis</author>
<author>C Macdonald</author>
<author>I Soboroff</author>
</authors>
<title>Overview of the trec-2008 blog track.</title>
<date>2008</date>
<booktitle>In Proceedings of TREC-08,</booktitle>
<pages>15--27</pages>
<contexts>
<context position="2187" citStr="Ounis et al., 2008" startWordPosition="318" endWordPosition="321">Movie-review datasets. MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (Wiebe et al., 2003). Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002). Evaluation workshops such as TREC and NTCIR have recently joined in this new trend of research and organized a number of successful meetings. At the TREC Blog Track meetings, researchers have dealt with the problem of retrieving topically-relevant blog posts and identifying documents with opinionated contents (Ounis et al., 2008). NTCIR Multilingual Opinion Analysis Task (MOAT) shared a similar mission, where participants are provided with a number of topics and a set of relevant newspaper articles for each topic, and asked to extract opinion-related properties from enclosed sentences (Seki et al., 2008). Previous studies for sentiment analysis belong to either the data-driven approach where an annotated corpus is used to train a machine learning (ML) classifier, or to the lexicon-based approach where a pre-compiled list of sentiment terms is utilized to build a sentiment score function. This paper introduces an appro</context>
</contexts>
<marker>Ounis, Macdonald, Soboroff, 2008</marker>
<rawString>I. Ounis, C. Macdonald, and I. Soboroff. 2008. Overview of the trec-2008 blog track. In Proceedings of TREC-08, pages 15–27, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="4056" citStr="Pang and Lee, 2008" startWordPosition="611" endWordPosition="614">ctiveness of the statistical features used in datadriven approaches by evaluating an ML classifier with labeled corpora. 2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 20</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1854" citStr="Pang et al., 2002" startWordPosition="263" endWordPosition="266">n mining and sentiment analysis involves extracting opinionated pieces of text, determining the polarities and strengths, and extracting holders and targets of the opinions. Much research has focused on creating testbeds for sentiment analysis tasks. Most notable and widely used are Multi-Perspective Question Answering (MPQA) and Movie-review datasets. MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (Wiebe et al., 2003). Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002). Evaluation workshops such as TREC and NTCIR have recently joined in this new trend of research and organized a number of successful meetings. At the TREC Blog Track meetings, researchers have dealt with the problem of retrieving topically-relevant blog posts and identifying documents with opinionated contents (Ounis et al., 2008). NTCIR Multilingual Opinion Analysis Task (MOAT) shared a similar mission, where participants are provided with a number of topics and a set of relevant newspaper articles for each topic, and asked to extract opinion-related properties from enclosed sentences (Seki </context>
<context position="4076" citStr="Pang et al. (2002)" startWordPosition="615" endWordPosition="618">istical features used in datadriven approaches by evaluating an ML classifier with labeled corpora. 2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, mu</context>
<context position="18822" citStr="Pang et al., 2002" startWordPosition="3060" endWordPosition="3063">s. We test vari ous combinations of the term weighting schemes listed below. proper assumptions and derivations, d) can be derived to language modeling approaches. Refer to (Zhai and Laffert 5With p(w| y, 2004). 3.3 Data-driven Approach To verify the effectiveness of our term weighting schemes in experimental settings of the datadriven approach, we carry out a set of simple experiments with ML classifiers. Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM), faithfully reproducing previous work as closely as possible (Pang et al., 2002). Each instance of train and test data is repre•PRESENCE: binary indicator for the presence of a term • TF: term fr equency 257 • VS.TF: normalized tf as in VS • BM25.TF: normalized tf as in BM25 • IDF: inverse document frequency • VS.IDF: normalized idf as in VS • BM25.ID 4 Experiment Our experiments consist of an opinion retrieval task and a sentiment classification task. We use MPQA and movie-review corpora in our experiments with an ML classifier. For the opinion retrieval task, we use the two datasets used by TREC blog track and NTCIR MOAT evaluation workshops. The opinion retrieval task </context>
<context position="27111" citStr="Pang et al. (2002)" startWordPosition="4383" endWordPosition="4386">t both precision and f-measure evaluations. 4.3 Classification task – SVM 4.3.1 Experimental Setting To test our SVM classifier, we perform the classification task. Movie Review polarity dataset7 was 7http://www.cs.cornell.edu/people/pabo/movie-reviewdata/ Table 3: Average ten-fold cross-validation accuracies of polarity classification task with SVM. Accuracy Features Movie-review MPQA PRESENCE 82.6 76.8 TF 71.1 76.5 VS.TF 81.3 76.7 BM25.TF 81.4 77.9 IDF 61.6 61.8 VS.IDF 83.6 77.9 BM25.IDF 83.6 77.8 VS.TF·VS.IDF 83.8 77.9 BM25.TF·BM25.IDF 84.1 77.7 BM25.TF·VS.IDF 85.1 77.7 first introduced by Pang et al. (2002) to test various ML-based methods for sentiment classification. It is a balanced dataset of 700 positive and 700 negative reviews, collected from the Internet Movie Database (IMDb) archive. MPQA Corpus8 contains 535 newspaper articles manually annotated at sentence and subsentence level for opinions and other private states (Wiebe et al., 2005). To closely reproduce the experiment with the best performance carried out in (Pang et al., 2002) using SVM, we use unigram with the presence feature. We test various combinations of our features applicable to the task. For evaluation, we use ten-fold c</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3921" citStr="Sebastiani, 2002" startWordPosition="592" endWordPosition="593"> retrieval system with TREC and NTCIR datasets to validate the effectiveness of our term weighting features. We also verify the effectiveness of the statistical features used in datadriven approaches by evaluating an ML classifier with labeled corpora. 2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analy</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>HsinHsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at ntcir-7.</title>
<date>2008</date>
<booktitle>In Proceedings of The 7th NTCIR Workshop</booktitle>
<marker>Seki, Evans, Ku, Le Sun, Kando, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, HsinHsi Chen, and Noriko Kando. 2008. Overview of multilingual opinion analysis task at ntcir-7. In Proceedings of The 7th NTCIR Workshop (2007/2008) - Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<contexts>
<context position="11065" citStr="Stone et al., 1966" startWordPosition="1729" endWordPosition="1732">timent of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al., 2005). There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006). The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties. For this reason, one sentiment lexicon, namely SentiWordNet, is utilized throughout the whole experiment. SentiWordNet is an automatically generated sentiment lexicon using a semi-supervised method (Esuli and Sebastiani, 2006). It consists of WordNet synsets, where each synset is assigned three probability scores that add</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4852" citStr="Turney and Littman, 2003" startWordPosition="730" endWordPosition="733">d subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significa</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmiir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="14355" citStr="Turney, 2001" startWordPosition="2290" endWordPosition="2292">e number of query words. To measure associations between words, we employ statistical approaches using document collections such as LSA and PMI, and local proximity features using the distance in dependency trees or texts. Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) creates a semantic space from a collection of documents to measure the semantic relatedness of words. Point-wise Mutual Information (PMI) is a measure of associations used in information theory, where the association between two words is evaluated with the joint and individual distributions of the two words. PMI-IR (Turney, 2001) uses an IR system and its search operators to estimate the probabilities of two terms and their conditional probabilities. Equations for association scores using LSA and PMI are given below. 2http://www.cs.pitt.edu/mpqa/ For the experimental purpose, we used publicly available online demonstrations for LSA and PMI. For LSA, we used the online demonstration mode from the Latent Semantic Analysis page from the University of Colorado at Boulder.3 For PMI, we used the online API provided by the CogWorks Lab at the Rensselaer Polytechnic Institute.4 Word associations between two terms may also be </context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmiir versus lsa on toefl. In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning, pages 491–502, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05),</booktitle>
<pages>625--631</pages>
<location>Bremen, DE.</location>
<contexts>
<context position="4895" citStr="Whitelaw et al., 2005" startWordPosition="738" endWordPosition="741">ove the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other </context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05), pages 625–631, Bremen, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>E Breck</author>
<author>Christopher Buckley</author>
<author>Claire Cardie</author>
<author>P Davis</author>
<author>B Fraser</author>
<author>Diane Litman</author>
<author>D Pierce</author>
<author>Ellen Riloff</author>
<author>Theresa Wilson</author>
<author>D Day</author>
<author>Mark Maybury</author>
</authors>
<title>Recognizing and organizing opinions expressed in the world press.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 AAAI Spring Symposium on New Directions in Question Answering.</booktitle>
<contexts>
<context position="1722" citStr="Wiebe et al., 2003" startWordPosition="244" endWordPosition="247">gs on the Internet, the field of studying how to analyze such remarks and sentiments has been increasing as well. The field of opinion mining and sentiment analysis involves extracting opinionated pieces of text, determining the polarities and strengths, and extracting holders and targets of the opinions. Much research has focused on creating testbeds for sentiment analysis tasks. Most notable and widely used are Multi-Perspective Question Answering (MPQA) and Movie-review datasets. MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (Wiebe et al., 2003). Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002). Evaluation workshops such as TREC and NTCIR have recently joined in this new trend of research and organized a number of successful meetings. At the TREC Blog Track meetings, researchers have dealt with the problem of retrieving topically-relevant blog posts and identifying documents with opinionated contents (Ounis et al., 2008). NTCIR Multilingual Opinion Analysis Task (MOAT) shared a similar mission, where participants are provided with a number of topics and</context>
</contexts>
<marker>Wiebe, Breck, Buckley, Cardie, Davis, Fraser, Litman, Pierce, Riloff, Wilson, Day, Maybury, 2003</marker>
<rawString>Janyce Wiebe, E. Breck, Christopher Buckley, Claire Cardie, P. Davis, B. Fraser, Diane Litman, D. Pierce, Ellen Riloff, Theresa Wilson, D. Day, and Mark Maybury. 2003. Recognizing and organizing opinions expressed in the world press. In Proceedings of the 2003 AAAI Spring Symposium on New Directions in Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="5520" citStr="Wiebe et al. (2004)" startWordPosition="832" endWordPosition="835">i and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other hand, Wiebe et al. (2004) have noted that hapax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Other works have also exploited rarely occurring terms for sentiment analysis tasks (Dave et al., 2003; Yang et al., 2006). The opinion retrieval task is a relatively recent issue that draws both the attention of IR and NLP communities. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking th</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3):277–308, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="27457" citStr="Wiebe et al., 2005" startWordPosition="4436" endWordPosition="4439">th SVM. Accuracy Features Movie-review MPQA PRESENCE 82.6 76.8 TF 71.1 76.5 VS.TF 81.3 76.7 BM25.TF 81.4 77.9 IDF 61.6 61.8 VS.IDF 83.6 77.9 BM25.IDF 83.6 77.8 VS.TF·VS.IDF 83.8 77.9 BM25.TF·BM25.IDF 84.1 77.7 BM25.TF·VS.IDF 85.1 77.7 first introduced by Pang et al. (2002) to test various ML-based methods for sentiment classification. It is a balanced dataset of 700 positive and 700 negative reviews, collected from the Internet Movie Database (IMDb) archive. MPQA Corpus8 contains 535 newspaper articles manually annotated at sentence and subsentence level for opinions and other private states (Wiebe et al., 2005). To closely reproduce the experiment with the best performance carried out in (Pang et al., 2002) using SVM, we use unigram with the presence feature. We test various combinations of our features applicable to the task. For evaluation, we use ten-fold cross-validation accuracy. 4.3.2 Experimental Result We present the sentiment classification performances in Table 3. As observed by Pang et al. (2002), using the raw tf drops the accuracy of the sentiment classification (-13.92%) of movie-review data. Using the raw idf feature worsens the accuracy even more (-25.42%). Normalized tf-variants sho</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP’05),</booktitle>
<pages>347--354</pages>
<location>Vancouver, CA.</location>
<contexts>
<context position="11006" citStr="Wilson et al., 2005" startWordPosition="1720" endWordPosition="1723"> term weighting. 3.2.1 Word Sentiment Model Modeling the sentiment of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al., 2005). There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006). The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties. For this reason, one sentiment lexicon, namely SentiWordNet, is utilized throughout the whole experiment. SentiWordNet is an automatically generated sentiment lexicon using a semi-supervised method (Esuli and Sebastiani, 2006). It consists of WordNet synsets, wher</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP’05), pages 347–354, Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiduk Yang</author>
<author>Ning Yu</author>
<author>Alejandro Valerio</author>
<author>Hui Zhang</author>
</authors>
<date>2006</date>
<booktitle>WIDIT in TREC-2006 Blog track. In Proceedings of TREC.</booktitle>
<contexts>
<context position="5772" citStr="Yang et al., 2006" startWordPosition="875" endWordPosition="878">e over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other hand, Wiebe et al. (2004) have noted that hapax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Other works have also exploited rarely occurring terms for sentiment analysis tasks (Dave et al., 2003; Yang et al., 2006). The opinion retrieval task is a relatively recent issue that draws both the attention of IR and NLP communities. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking the documents by the opinion scores (Ounis et al., 2006). This approach is also appropriate for evaluation systems such as NTCIR MOAT that assumes that the set of topically relevant documents are already known in advance. On the other hand, there are als</context>
<context position="16046" citStr="Yang et al. (2006)" startWordPosition="2559" endWordPosition="2562">rwise where K is the maximum window size for the co-occurrence and is arbitrarily set to 3 in our experiments. The statistical approaches may suffer from data sparseness problems especially for named entity terms used in the query, and the proximal clues cannot sufficiently cover all term–query associations. To avoid assigning zero probabilities, our topic association models assign 0.5 to word pairs with no association and 1.0 to words with perfect association. Note that proximal features using co-occurrence and dependency relationships were used in previous work. For opinion retrieval tasks, Yang et al. (2006) and Zhang and Ye (2008) used the cooccurrence of a query word and a sentiment word within a certain window size. Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach. 3.2.3 Word Generation Model Our word generation model p(w I d) evaluates the prominence and the discriminativeness of a word 3http://lsa.colorado.edu/, default parameter settings for the semantic space (TASA, 1st year college level) and number of factors (300). 4http</context>
</contexts>
<marker>Yang, Yu, Valerio, Zhang, 2006</marker>
<rawString>Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang. 2006. WIDIT in TREC-2006 Blog track. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of 2003 Conference on the Empirical Methods in Natural Language Processing (EMNLP’03),</booktitle>
<pages>129--136</pages>
<location>Sapporo, JP.</location>
<contexts>
<context position="4659" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="704" endWordPosition="708">atures (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding su</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of 2003 Conference on the Empirical Methods in Natural Language Processing (EMNLP’03), pages 129– 136, Sapporo, JP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>22</volume>
<issue>2</issue>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Xingyao Ye</author>
</authors>
<title>A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval.</title>
<date>2008</date>
<booktitle>In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>411--418</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6501" citStr="Zhang and Ye, 2008" startWordPosition="997" endWordPosition="1000">ies. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking the documents by the opinion scores (Ounis et al., 2006). This approach is also appropriate for evaluation systems such as NTCIR MOAT that assumes that the set of topically relevant documents are already known in advance. On the other hand, there are also some interesting works on modeling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008). 3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. 3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features. For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-independently speaking, </context>
<context position="16070" citStr="Zhang and Ye (2008)" startWordPosition="2564" endWordPosition="2567">ximum window size for the co-occurrence and is arbitrarily set to 3 in our experiments. The statistical approaches may suffer from data sparseness problems especially for named entity terms used in the query, and the proximal clues cannot sufficiently cover all term–query associations. To avoid assigning zero probabilities, our topic association models assign 0.5 to word pairs with no association and 1.0 to words with perfect association. Note that proximal features using co-occurrence and dependency relationships were used in previous work. For opinion retrieval tasks, Yang et al. (2006) and Zhang and Ye (2008) used the cooccurrence of a query word and a sentiment word within a certain window size. Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach. 3.2.3 Word Generation Model Our word generation model p(w I d) evaluates the prominence and the discriminativeness of a word 3http://lsa.colorado.edu/, default parameter settings for the semantic space (TASA, 1st year college level) and number of factors (300). 4http://cwl-projects.cogsci.r</context>
</contexts>
<marker>Zhang, Ye, 2008</marker>
<rawString>Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 411–418, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>