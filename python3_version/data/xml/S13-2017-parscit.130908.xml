<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012602">
<title confidence="0.9977875">
MELODI: Semantic Similarity of Words and Compositional Phrases
using Latent Vector Weighting
</title>
<author confidence="0.963085">
Tim Van de Cruys Stergos Afantenos Philippe Muller
</author>
<affiliation confidence="0.881297">
IRIT, CNRS IRIT, Toulouse University IRIT, Toulouse University
</affiliation>
<email confidence="0.977759">
tim.vandecruys@irit.fr stergos.afantenos@irit.fr philippe.muller@irit.fr
</email>
<sectionHeader confidence="0.995404" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994025">
In this paper we present our system for the
SemEval 2013 Task 5a on semantic similar-
ity of words and compositional phrases. Our
system uses a dependency-based vector space
model, in combination with a technique called
latent vector weighting. The system computes
the similarity between a particular noun in-
stance and the head noun of a particular noun
phrase, which was weighted according to the
semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the
similarity threshold, was tuned using the train-
ing data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999395833333333">
In the course of the last two decades, vector space
models have gained considerable momentum for se-
mantic processing. Initially, these models only dealt
with individual words, ignoring the context in which
these words appear. More recently, two different but
related approaches emerged that take into account
the interaction between different words within a par-
ticular context. The first approach aims at building a
joint, compositional representation for larger units
beyond the individual word level (e.g., the com-
posed, semantic representation of the noun phrase
crispy chips). The second approach, different but re-
lated to the first one, computes the specific meaning
of a word within a particular context (e.g. the mean-
ing of the noun bank in the context of the adjective
bankrupt).
In this paper, we describe our system for the Sem-
Eval 2013 Task 5a: semantic similarity of words and
</bodyText>
<page confidence="0.97165">
98
</page>
<bodyText confidence="0.9991931">
compositional phrases – which follows the latter ap-
proach. Our system uses a dependency-based vector
space model, in combination with a technique called
latent vector weighting (Van de Cruys et al., 2011).
The system computes the similarity between a par-
ticular noun instance and the head noun of a par-
ticular noun phrase, which was weighted according
to the semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the simi-
larity threshold, was tuned using the training data.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99725935">
In recent years, a number of methods have been de-
veloped that try to capture the compositional mean-
ing of units beyond the individual word level within
a distributional framework. One of the first ap-
proaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata’s (2008) ap-
proach. They explore a number of different mod-
els for vector composition, of which vector addition
(the sum of each feature) and vector multiplication
(the elementwise multiplication of each feature) are
the most important. Baroni and Zamparelli (2010)
present a method for the composition of adjectives
and nouns. In their model, an adjective is a linear
function of one vector (the noun vector) to another
vector (the vector for the adjective-noun pair). The
linear transformation for a particular adjective is rep-
resented by a matrix, and is learned automatically
from a corpus, using partial least-squares regression.
Coecke et al. (2010) present an abstract theoretical
framework in which a sentence vector is a function
of the Kronecker product of its word vectors, which
allows for greater interaction between the different
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
word features. And Socher et al. (2012) present a
model for compositionality based on recursive neu-
ral networks.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pad´o (2008, 2009) make use of
selectional preferences to express the meaning of
a word in context. And Dinu and Lapata (2010)
propose a probabilistic framework that models the
meaning of words as a probability distribution over
latent factors. This allows them to model contex-
tualized meaning as a change in the original sense
distribution.
Our work takes the latter approach of computing
word meaning in context, and is described in detail
below.
</bodyText>
<sectionHeader confidence="0.999033" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999982086956522">
Our method uses latent vector weighting (Van de
Cruys et al., 2011) in order to compute a se-
mantic representation for the meaning of a word
within a particular context. The method relies
upon a factorization model in which words, together
with their window-based context features and their
dependency-based context features, are linked to la-
tent dimensions. The factorization model allows us
to determine which dimensions are important for a
particular context, and adapt the dependency-based
feature vector of the word accordingly. The mod-
ified feature vector is then compared to the target
noun feature vector with the cosine similarity func-
tion.
This following sections describe our model in
more detail. In section 3.1, we describe non-
negative matrix factorization – the factorization
technique that our model uses. Section 3.2 describes
our way of combining dependency-based context
features and window-based context features within
the same factorization model. Section 3.3, then, de-
scribes our method of computing the meaning of a
word within a particular context.
</bodyText>
<subsectionHeader confidence="0.997562">
3.1 Non-negative Matrix Factorization
</subsectionHeader>
<bodyText confidence="0.9716732">
Our latent model uses a factorization technique
called non-negative matrix factorization (Lee and
Seung, 2000) in order to find latent dimensions. The
key idea is that a non-negative matrix A is factorized
into two other non-negative matrices, W and H
</bodyText>
<equation confidence="0.781853">
Aixj ^ WixkHkxj (1)
</equation>
<bodyText confidence="0.996334235294118">
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler
divergence as an objective function, we want to find
the matrices W and H for which the divergence
between A and WH (the multiplication of W and
H) is the smallest. This factorization is carried
out through the iterative application of update rules.
Matrices W and H are randomly initialized, and the
rules in 2 and 3 are iteratively applied – alternating
between them. In each iteration, each vector is ade-
quately normalized, so that all dimension values sum
to 1.
</bodyText>
<equation confidence="0.5578398">
Aiµ
Ei Wia (WH)iµ
(2)
Aiµ
Eµ Haµ (WH)iµ (3)
</equation>
<subsectionHeader confidence="0.999923">
3.2 Combining syntax and context words
</subsectionHeader>
<bodyText confidence="0.999995631578948">
Using an extension of non-negative matrix fac-
torization (Van de Cruys, 2008), it is possible
to jointly induce latent factors for three different
modes: nouns, their window-based context words,
and their dependency-based context features. The
intuition is that the window-based context words
inform us about broad, topical similarity, whereas
the dependency-based features get at a tighter,
synonym-like similarity. As input to the algo-
rithm, two matrices are constructed that capture the
pairwise co-occurrence frequencies for the different
modes. The first matrix contains co-occurrence fre-
quencies of words cross-classified by dependency-
based features, and the second matrix contains co-
occurrence frequencies of words cross-classified by
words that appear in the word’s context window.
NMF is then applied to the two matrices, and the
separate factorizations are interleaved (i.e. matrix
W, which contains the nouns by latent dimensions,
</bodyText>
<figure confidence="0.83980125">
Haµ +— Haµ
Ek Wka
Wia +— Wia
Ev Hav
</figure>
<page confidence="0.983022">
99
</page>
<bodyText confidence="0.99978775">
is shared between both factorizations). A graphical
representation of the interleaved factorization algo-
rithm is given in figure 1. The numbered arrows in-
dicate the sequence of the updates.
</bodyText>
<figureCaption confidence="0.998497">
Figure 1: A graphical representation of the interleaved
</figureCaption>
<bodyText confidence="0.955111090909091">
NMF
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based context features) are all rep-
resented according to a limited number of latent fac-
tors.
The factorization that comes out of the NMF
model can be interpreted probabilistically (Gaussier
and Goutte, 2005; Ding et al., 2008). More specifi-
cally, we can transform the factorization into a stan-
dard latent variable model of the form
</bodyText>
<equation confidence="0.997567">
K
p(wi,dj) = E p(z)p(wi|z)p(dj|z) (4)
z=1
</equation>
<bodyText confidence="0.999798428571429">
by introducing two K x K diagonal scaling matrices
X and Y, such that Xkk = Ei Wik and Ykk = Ej Hkj.
The factorization WH can then be rewritten as
such that WX−1 represents p(wi|z), (Y−1H)T rep-
resents p(dj|z), and XY represents p(z). Using
Bayes’ theorem, it is now straightforward to deter-
mine p(z|dj).
</bodyText>
<equation confidence="0.978646">
p(z|dj) = p(dj|z)p(z) (6)
p(dj)
</equation>
<subsectionHeader confidence="0.970843">
3.3 Meaning in Context
3.3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999992533333333">
Using the results of the factorization model de-
scribed above, we can now adapt a word’s feature
vector according to the context in which it appears.
Intuitively, the context of the word (in our case,
the dependency-based context feature that acts as an
adjectival modifier to the head noun) pinpoint the
important semantic dimensions of the particular in-
stance, creating a probability distribution over latent
factors. The required probability vector, p(z|dj), is
yielded by our factorization model. This probabil-
ity distribution over latent factors can be interpreted
as a semantic fingerprint of the passage in which the
target word appears. Using this fingerprint, we can
now determine a new probability distribution over
dependency features given the context.
</bodyText>
<equation confidence="0.999442">
p(d|dj) = p(z|dj)p(d|z) (7)
</equation>
<bodyText confidence="0.9998288">
The last step is to weight the original probability
vector of the word according to the probability vec-
tor of the dependency features given the word’s con-
text, by taking the pointwise multiplication of prob-
ability vectors p(d|wi) and p(d|dj).
</bodyText>
<equation confidence="0.998711">
p(d|wi,dj) = p(d|wi) - p(d|dj) (8)
</equation>
<bodyText confidence="0.999989571428571">
Note that this final step is a crucial one in our
approach. We do not just build a model based on
latent factors, but we use the latent factors to de-
termine which of the features in the original word
vector are the salient ones given a particular context.
This allows us to compute an accurate adaptation of
the original word vector in context.
</bodyText>
<subsectionHeader confidence="0.726877">
3.3.2 Example
</subsectionHeader>
<bodyText confidence="0.99980125">
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun instrument within the phrases (1)
and (2), taken from the task’s test set:
</bodyText>
<listItem confidence="0.966266">
(1) musical instrument
(2) optical instrument
</listItem>
<bodyText confidence="0.995979666666667">
First, we extract the context feature for both in-
stances, in this case C1 = {musicaladj} for phrase
(1), and C2 = {opticaladj} for phrase (2). Next, we
</bodyText>
<figure confidence="0.935769">
U
U
=
K
I
K
W
I
3
4
I
K
=
A
nouns x
dependencies
V
B
nouns x
context words
H
V
G
1
2
(5)
</figure>
<equation confidence="0.7107595">
= (WX−1)(XY)(Y−1H)
WH = (WX−1X)(YY−1H)
</equation>
<page confidence="0.898231">
100
</page>
<bodyText confidence="0.9998874">
look up p(z|C1) and p(z|C2) – the probability distri-
butions over latent factors given the context – which
are yielded by our factorization model. Using these
probability distributions over latent factors, we can
now determine the probability of each dependency
feature given the different contexts – p(d|C1) and
p(d|C2) (equation 7).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears (equation 8).
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of instrument given the different contexts,
which yields the results presented below.
</bodyText>
<listItem confidence="0.995127">
1. instrumentN, C1: percussion, flute, violin,
melody, harp
2. instrumentN, C2: sensor, detector, amplifier,
device, microscope
</listItem>
<subsectionHeader confidence="0.993581">
3.4 Implementational details
</subsectionHeader>
<bodyText confidence="0.999857352941177">
Our model has been trained on the UKWaC cor-
pus (Baroni et al., 2009). The corpus has been
part of speech tagged and lemmatized with Stan-
ford Part-Of-Speech Tagger (Toutanova and Man-
ning, 2000; Toutanova et al., 2003), and parsed with
MaltParser (Nivre et al., 2006) trained on sections
2-21 of the Wall Street Journal section of the Penn
Treebank extended with about 4000 questions from
the QuestionBank1, so that dependency triples could
be extracted.
The matrices needed for our interleaved NMF fac-
torization are extracted from the corpus. Our model
was built using 5K nouns, 80K dependency relations,
and 2K context words2 (excluding stop words) with
highest frequency in the training set, which yields
matrices of 5K nouns × 80K dependency relations,
and 5K nouns × 2K context words.
</bodyText>
<footnote confidence="0.99695825">
1http://maltparser.org/mco/english_parser/
engmalt.html
2We used a fairly large, paragraph-like window of four sen-
tences.
</footnote>
<table confidence="0.995186666666667">
model accuracy precision recall F1
dist .69 .83 .48 .61
lvw .75 .84 .61 .71
</table>
<tableCaption confidence="0.989118">
Table 1: Results of the distributional model (dist) and la-
tent vector weighting model (lvw) on the SemEval task
5a
</tableCaption>
<bodyText confidence="0.999917222222222">
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in the
model), and applying 100 iterations. The interleaved
NMF algorithm was implemented in Matlab; the pre-
processing scripts and scripts for vector computation
in context were written in Python.
The model is entirely unsupervised. The only pa-
rameter to set, the cosine similarity threshold 0, is
induced from the training set. We set 0 = .049.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999505">
Table 1 shows the evaluation results of the simple
distributional model (which only takes into account
the head noun) and our model that uses latent vector
weighting. The results indicate that our model based
on latent vector weighting performs quite a bit bet-
ter than a standard dependency-based distributional
model. The lvw model attains an accuracy of .75 –
a 6% improvement over the distributional model –
and an F-measure of .71 – a 10% improvement over
the distributional model.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998862166666667">
In this paper we presented an entirely unsuper-
vised system for the assessment of the similarity of
words and compositional phrases. Our system uses a
dependency-based vector space model, in combina-
tion with latent vector weighting. The system com-
putes the similarity between a particular noun in-
stance and the head noun of a particular noun phrase,
which was weighted according to the semantics of
the modifier. Using our system yields a substantial
improvement over a simple dependency-based dis-
tributional model, which only takes the head noun
into account.
</bodyText>
<page confidence="0.998532">
101
</page>
<sectionHeader confidence="0.989665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999551898734177">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October. Association for
Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift,
Linguistic Analysis, vol. 36, 36.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics &amp; Data Analysis, 52(8):3913–3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–906,
Waikiki, Hawaii, USA.
Katrin Erk and Sebastian Pad´o. 2009. Paraphrase as-
sessment in structured vector space: Exploring param-
eters and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics,
pages 57–65, Athens, Greece.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601–602, Salvador, Brazil.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In Ad-
vances in Neural Information Processing Systems 13,
pages 556–562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings ofACL-
08: HLT, pages 236–244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216–
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201–
1211, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63–70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252–259.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word meaning
in context. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1012–1022, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929–936, Manchester.
</reference>
<page confidence="0.998612">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650180">
<title confidence="0.997801">MELODI: Semantic Similarity of Words and Compositional using Latent Vector Weighting</title>
<author confidence="0.999181">Tim Van_de_Cruys Stergos Afantenos Philippe Muller</author>
<affiliation confidence="0.714015">IRIT, CNRS IRIT, Toulouse University IRIT, Toulouse</affiliation>
<email confidence="0.934435">tim.vandecruys@irit.frstergos.afantenos@irit.frphilippe.muller@irit.fr</email>
<abstract confidence="0.997974928571429">In this paper we present our system for the SemEval 2013 Task 5a on semantic similarity of words and compositional phrases. Our system uses a dependency-based vector space model, in combination with a technique called latent vector weighting. The system computes the similarity between a particular noun instance and the head noun of a particular noun phrase, which was weighted according to the semantics of the modifier. The system is entirely unsupervised; one single parameter, the similarity threshold, was tuned using the training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2808" citStr="Baroni and Zamparelli (2010)" startWordPosition="435" endWordPosition="438">er, the similarity threshold, was tuned using the training data. 2 Related work In recent years, a number of methods have been developed that try to capture the compositional meaning of units beyond the individual word level within a distributional framework. One of the first approaches to tackle compositional phenomena in a systematic way is Mitchell and Lapata’s (2008) approach. They explore a number of different models for vector composition, of which vector addition (the sum of each feature) and vector multiplication (the elementwise multiplication of each feature) are the most important. Baroni and Zamparelli (2010) present a method for the composition of adjectives and nouns. In their model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different Second Joint Conferen</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="11836" citStr="Baroni et al., 2009" startWordPosition="1887" endWordPosition="1890">he target word (the aggregate of dependency-based context features over all contexts of the target word) according to the new distribution given the context in which the target word appears (equation 8). We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN, C1: percussion, flute, violin, melody, harp 2. instrumentN, C2: sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1, so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest freque</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis,</title>
<date>2010</date>
<volume>36</volume>
<pages>36</pages>
<contexts>
<context position="3198" citStr="Coecke et al. (2010)" startWordPosition="497" endWordPosition="500"> number of different models for vector composition, of which vector addition (the sum of each feature) and vector multiplication (the elementwise multiplication of each feature) are the most important. Baroni and Zamparelli (2010) present a method for the composition of adjectives and nouns. In their model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics word features. And Socher et al. (2012) present a model for compositionality based on recursive neural networks. Closely related to the work on compositionality </context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis, vol. 36, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
</authors>
<title>On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing.</title>
<date>2008</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>52</volume>
<issue>8</issue>
<contexts>
<context position="8147" citStr="Ding et al., 2008" startWordPosition="1270" endWordPosition="1273">µ Ek Wka Wia +— Wia Ev Hav 99 is shared between both factorizations). A graphical representation of the interleaved factorization algorithm is given in figure 1. The numbered arrows indicate the sequence of the updates. Figure 1: A graphical representation of the interleaved NMF When the factorization is finished, the three different modes (words, window-based context words and dependency-based context features) are all represented according to a limited number of latent factors. The factorization that comes out of the NMF model can be interpreted probabilistically (Gaussier and Goutte, 2005; Ding et al., 2008). More specifically, we can transform the factorization into a standard latent variable model of the form K p(wi,dj) = E p(z)p(wi|z)p(dj|z) (4) z=1 by introducing two K x K diagonal scaling matrices X and Y, such that Xkk = Ei Wik and Ykk = Ej Hkj. The factorization WH can then be rewritten as such that WX−1 represents p(wi|z), (Y−1H)T represents p(dj|z), and XY represents p(z). Using Bayes’ theorem, it is now straightforward to determine p(z|dj). p(z|dj) = p(dj|z)p(z) (6) p(dj) 3.3 Meaning in Context 3.3.1 Overview Using the results of the factorization model described above, we can now adapt</context>
</contexts>
<marker>Ding, Li, Peng, 2008</marker>
<rawString>Chris Ding, Tao Li, and Wei Peng. 2008. On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. Computational Statistics &amp; Data Analysis, 52(8):3913–3927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="3991" citStr="Dinu and Lapata (2010)" startWordPosition="616" endWordPosition="619">een the different Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics word features. And Socher et al. (2012) present a model for compositionality based on recursive neural networks. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Our work takes the latter approach of computing word meaning in context, and is described in detail below. 3 Methodology Our method uses latent vector weighting (Van de Cruys et al., 2011) in order to compute a semantic representation for the meaning of a word within a particular context. The method relies upon a factorization model in which words, together with their window-based </context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Waikiki, Hawaii, USA.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897–906, Waikiki, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>57--65</pages>
<location>Athens, Greece.</location>
<marker>Erk, Pad´o, 2009</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 57–65, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
</authors>
<title>Relation between PLSA and NMF and implications.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>601--602</pages>
<location>Salvador, Brazil.</location>
<contexts>
<context position="8127" citStr="Gaussier and Goutte, 2005" startWordPosition="1266" endWordPosition="1269">atent dimensions, Haµ +— Haµ Ek Wka Wia +— Wia Ev Hav 99 is shared between both factorizations). A graphical representation of the interleaved factorization algorithm is given in figure 1. The numbered arrows indicate the sequence of the updates. Figure 1: A graphical representation of the interleaved NMF When the factorization is finished, the three different modes (words, window-based context words and dependency-based context features) are all represented according to a limited number of latent factors. The factorization that comes out of the NMF model can be interpreted probabilistically (Gaussier and Goutte, 2005; Ding et al., 2008). More specifically, we can transform the factorization into a standard latent variable model of the form K p(wi,dj) = E p(z)p(wi|z)p(dj|z) (4) z=1 by introducing two K x K diagonal scaling matrices X and Y, such that Xkk = Ei Wik and Ykk = Ej Hkj. The factorization WH can then be rewritten as such that WX−1 represents p(wi|z), (Y−1H)T represents p(dj|z), and XY represents p(z). Using Bayes’ theorem, it is now straightforward to determine p(z|dj). p(z|dj) = p(dj|z)p(z) (6) p(dj) 3.3 Meaning in Context 3.3.1 Overview Using the results of the factorization model described abo</context>
</contexts>
<marker>Gaussier, Goutte, 2005</marker>
<rawString>Eric Gaussier and Cyril Goutte. 2005. Relation between PLSA and NMF and implications. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 601–602, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="5548" citStr="Lee and Seung, 2000" startWordPosition="853" endWordPosition="856">ture vector with the cosine similarity function. This following sections describe our model in more detail. In section 3.1, we describe nonnegative matrix factorization – the factorization technique that our model uses. Section 3.2 describes our way of combining dependency-based context features and window-based context features within the same factorization model. Section 3.3, then, describes our method of computing the meaning of a word within a particular context. 3.1 Non-negative Matrix Factorization Our latent model uses a factorization technique called non-negative matrix factorization (Lee and Seung, 2000) in order to find latent dimensions. The key idea is that a non-negative matrix A is factorized into two other non-negative matrices, W and H Aixj ^ WixkHkxj (1) where k is much smaller than i, j so that both instances and features are expressed in terms of a few components. Non-negative matrix factorization enforces the constraint that all three matrices must be non-negative, so all elements must be greater than or equal to zero. Using the minimization of the Kullback-Leibler divergence as an objective function, we want to find the matrices W and H for which the divergence between A and WH (t</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition. proceedings ofACL08: HLT,</title>
<date>2008</date>
<pages>236--244</pages>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. proceedings ofACL08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="12033" citStr="Nivre et al., 2006" startWordPosition="1919" endWordPosition="1922">n 8). We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN, C1: percussion, flute, violin, melody, harp 2. instrumentN, C2: sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1, so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest frequency in the training set, which yields matrices of 5K nouns × 80K dependency relations, and 5K nouns × 2K context words. 1http://maltparser.org/mco/english_parser/ engmalt.html 2We used a fairly lar</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216– 2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3676" citStr="Socher et al. (2012)" startWordPosition="564" endWordPosition="567">adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics word features. And Socher et al. (2012) present a model for compositionality based on recursive neural networks. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Our work takes the latter approach of computing word meaning in conte</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201– 1211, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="11959" citStr="Toutanova and Manning, 2000" startWordPosition="1906" endWordPosition="1910">o the new distribution given the context in which the target word appears (equation 8). We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN, C1: percussion, flute, violin, melody, harp 2. instrumentN, C2: sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1, so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest frequency in the training set, which yields matrices of 5K nouns × 80K dependency relations, and 5K nouns × 2K context words. 1ht</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="11984" citStr="Toutanova et al., 2003" startWordPosition="1911" endWordPosition="1914">the context in which the target word appears (equation 8). We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN, C1: percussion, flute, violin, melody, harp 2. instrumentN, C2: sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1, so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest frequency in the training set, which yields matrices of 5K nouns × 80K dependency relations, and 5K nouns × 2K context words. 1http://maltparser.org/mco/e</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>Latent vector weighting for word meaning in context.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1012--1022</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Van de Cruys, Poibeau, Korhonen, 2011</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in context. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>Using three way data for word sense discrimination.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>929--936</pages>
<location>Manchester.</location>
<marker>Van de Cruys, 2008</marker>
<rawString>Tim Van de Cruys. 2008. Using three way data for word sense discrimination. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 929–936, Manchester.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>