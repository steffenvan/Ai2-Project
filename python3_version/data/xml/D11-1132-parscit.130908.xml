<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990216">
Relation Extraction with Relation Topics
</title>
<author confidence="0.865901">
Chang Wang James Fan Aditya Kalyanpur David Gondek
</author>
<note confidence="0.789073">
IBM T. J. Watson Research Lab
</note>
<address confidence="0.732494">
19 Skyline Drive, Hawthorne, New York 10532
</address>
<email confidence="0.983857">
{wangchan, fanj, adityakal, dgondek}@us.ibm.com
</email>
<sectionHeader confidence="0.995568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922777777778">
This paper describes a novel approach to the
semantic relation detection problem. Instead
of relying only on the training instances for
a new relation, we leverage the knowledge
learned from previously trained relation detec-
tors. Specifically, we detect a new semantic
relation by projecting the new relation’s train-
ing instances onto a lower dimension topic
space constructed from existing relation de-
tectors through a three step process. First, we
construct a large relation repository of more
than 7,000 relations from Wikipedia. Second,
we construct a set of non-redundant relation
topics defined at multiple scales from the re-
lation repository to characterize the existing
relations. Similar to the topics defined over
words, each relation topic is an interpretable
multinomial distribution over the existing re-
lations. Third, we integrate the relation topics
in a kernel function, and use it together with
SVM to construct detectors for new relations.
The experimental results on Wikipedia and
ACE data have confirmed that background-
knowledge-based topics generated from the
Wikipedia relation repository can significantly
improve the performance over the state-of-the-
art relation detection approaches.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999689275">
Detecting semantic relations in text is very useful
in both information retrieval and question answer-
ing because it enables knowledge bases to be lever-
aged to score passages and retrieve candidate an-
swers. To extract semantic relations from text, three
types of approaches have been applied. Rule-based
methods (Miller et al., 2000) employ a number of
linguistic rules to capture relation patterns. Feature-
based methods (Kambhatla, 2004; Zhao and Grish-
man, 2005) transform relation instances into a large
amount of linguistic features like lexical, syntactic
and semantic features, and capture the similarity be-
tween these feature vectors. Recent results mainly
rely on kernel-based approaches. Many of them fo-
cus on using tree kernels to learn parse tree struc-
ture related features (Collins and Duffy, 2001; Cu-
lotta and Sorensen, 2004; Bunescu and Mooney,
2005). Other researchers study how different ap-
proaches can be combined to improve the extraction
performance. For example, by combining tree ker-
nels and convolution string kernels, (Zhang et al.,
2006) achieved the state of the art performance on
ACE (ACE, 2004), which is a benchmark dataset for
relation extraction.
Although a large set of relations have been iden-
tified, adapting the knowledge extracted from these
relations for new semantic relations is still a chal-
lenging task. Most of the work on domain adapta-
tion of relation detection has focused on how to cre-
ate detectors from ground up with as little training
data as possible through techniques such as boot-
strapping (Etzioni et al., 2005). We take a differ-
ent approach, focusing on how the knowledge ex-
tracted from the existing relations can be reused to
help build detectors for new relations. We believe by
reusing knowledge one can build a more cost effec-
tive relation detector, but there are several challenges
associated with reusing knowledge.
The first challenge to address in this approach is
how to construct a relation repository that has suffi-
</bodyText>
<page confidence="0.934253">
1426
</page>
<note confidence="0.957906">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1426–1436,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999968328358209">
cient coverage. In this paper, we introduce a method
that automatically extracts the knowledge charac-
terizing more than 7,000 relations from Wikipedia.
Wikipedia is comprehensive, containing a diverse
body of content with significant depth and grows
rapidly. Wikipedia’s infoboxes are particularly in-
teresting for relation extraction. They are short,
manually-created, and often have a relational sum-
mary of an article: a set of attribute/value pairs de-
scribing the article’s subject.
Another challenge is how to deal with overlap of
relations in the repository. For example, Wikipedia
authors may make up a name when a new relation
is needed without checking if a similar relation has
already been created. This leads to relation duplica-
tion. We refine the relation repository based on an
unsupervised multiscale analysis of the correlations
between existing relations. This method is parame-
ter free, and able to produce a set of non-redundant
relation topics defined at multiple scales. Similar to
the topics defined over words (Blei et al., 2003), we
define relation topics as multinomial distributions
over the existing relations. The relation topics ex-
tracted in our approach are interpretable, orthonor-
mal to each other, and can be used as basis relations
to re-represent the new relation instances.
The third challenge is how to use the relation top-
ics for a relation detector. We map relation instances
in the new domains to the relation topic space, re-
sulting in a set of new features characterizing the
relationship between the relation instances and ex-
isting relations. By doing so, background knowl-
edge from the existing relations can be introduced
into the new relations, which overcomes the limi-
tations of the existing approaches when the training
data is not sufficient. Our work fits in to a class of re-
lation extraction research based on “distant supervi-
sion”, which studies how knowledge and resources
external to the target domain can be used to im-
prove relation extraction. (Mintz et al., 2009; Jiang,
2009; Chan and Roth, 2010). One distinction be-
tween our approach and other existing approaches is
that we represent the knowledge from distant super-
vision using automatically constructed topics. When
we test on new instances, we do not need to search
against the knowledge base. In addition, our top-
ics also model the indirect relationship between re-
lations. Such information cannot be directly found
from the knowledge base.
The contributions of this paper are three-fold.
Firstly, we extract a large amount of training
data for more than 7,000 semantic relations from
Wikipedia (Wikipedia, 2011) and DBpedia (Auer
et al., 2007). A key part of this step is how we
handle noisy data with little human effort. Sec-
ondly, we present an unsupervised way to con-
struct a set of relation topics at multiple scales.
This step is parameter free, and results in a non-
redundant, multiscale relation topic space. Thirdly,
we design a new kernel for relation detection by
integrating the relation topics into the relation de-
tector construction. The experimental results on
Wikipedia and ACE data (ACE, 2004) have con-
firmed that background-knowledge-based features
generated from the Wikipedia relation repository
can significantly improve the performance over the
state-of-the-art relation detection approaches.
</bodyText>
<sectionHeader confidence="0.81732" genericHeader="method">
2 Extracting Relations from Wikipedia
</sectionHeader>
<bodyText confidence="0.99967125">
Our training data is from two parts: relation in-
stances from DBpedia (extracted from Wikipedia
infoboxes), and sentences describing the relations
from the corresponding Wikipedia pages.
</bodyText>
<subsectionHeader confidence="0.996709">
2.1 Collecting the Training Data
</subsectionHeader>
<bodyText confidence="0.99996545">
Since our relations correspond to Wikipedia infobox
properties, we use an approach similar to that de-
scribed in (Hoffmann et al., 2010) to collect positive
training data instances. We assume that a Wikipedia
page containing a particular infobox property is
likely to express the same relation in the text of
the page. We further assume that the relation is
most likely expressed in the first sentence on the
page which mentions the arguments of the relation.
For example, the Wikipedia page for “Albert Ein-
stein” contains an infobox property “alma mater”
with value “University of Zurich”, and the first sen-
tence mentioning the arguments is the following:
“Einstein was awarded a PhD by the University of
Zurich”, which expresses the relation. When look-
ing for relation arguments on the page, we go be-
yond (sub)string matching, and use link information
to match entities which may have different surface
forms. Using this technique, we are able to collect a
large amount of positive training instances of DBpe-
</bodyText>
<page confidence="0.989838">
1427
</page>
<bodyText confidence="0.99936452173913">
dia relations.
To get precise type information for the argu-
ments of a DBpedia relation, we use the DBpedia
knowledge base (Auer et al., 2007) and the asso-
ciated YAGO type system (Suchanek et al., 2007).
Note that for every Wikipedia page, there is a cor-
responding DBpedia entry which has captured the
infobox-properties as RDF triples. Some of the
triples include type information, where the subject
of the triple is a Wikipedia entity, and the object
is a YAGO type for the entity. For example, the
DBpedia entry for the entity “Albert Einstein” in-
cludes YAGO types such as Scientist, Philosopher,
Violinist etc. These YAGO types are also linked
to appropriate WordNet concepts, providing for ac-
curate sense disambiguation. Thus, for any en-
tity argument of a relation we are learning, we ob-
tain sense-disambiguated type information (includ-
ing super-types, sub-types, siblings etc.), which be-
come useful generalization features in the relation
detection model. Given a common noun, we can
also retrieve its type information by checking against
WordNet (Fellbaum, 1998).
</bodyText>
<subsectionHeader confidence="0.999897">
2.2 Extracting Rules from the Training Data
</subsectionHeader>
<bodyText confidence="0.979894826086957">
We use a set of rules together with their popular-
ities (occurrence count) to characterize a relation.
A rule representing the relations between two ar-
guments has five components (ordered): arguments
type, argument2 type, noun, preposition and verb. A
rule example of ActiveYearsEndDate relation (about
the year that a person retired) is:
person100007846|year115203791|-|in|retire.
In this example, arguments type is per-
son100007846, argument2 type is year115203791,
both of which are from YAGO type system. The
key words connecting these two arguments are in
(preposition) and retire (verb). This rule does not
have a noun, so we use a ‘-’ to take the position of
noun. The same relation can be represented in many
different ways. Another rule example characterizing
the same relation is
person100007846|year115203791|retirement|-|announce.
This paper only considers three types of words:
noun, verb and preposition. It is straightforward to
expand or simplify the rules by including more or
removing some word types. The keywords are ex-
tracted from the shortest path on the dependency
</bodyText>
<figureCaption confidence="0.998212">
Figure 1: A dependency tree example.
</figureCaption>
<bodyText confidence="0.999945785714286">
tree between the two arguments. A dependency
tree (Figure 1) represents grammatical relations be-
tween words in a sentence. We used a slot grammar
parser (McCord, 1995) to generate the parse tree of
each sentence. Note that there could be multiple
paths between two arguments in the tree. We only
take the shortest path into consideration. The pop-
ularity value corresponding to each rule represents
how many times this rule applies to the given rela-
tion in the given data. Multiple rules can be con-
structed from one relation instance, if multiple argu-
ment types are associated with the instance, or mul-
tiple nouns, prepositions or verbs are in the depen-
dency path.
</bodyText>
<subsectionHeader confidence="0.999056">
2.3 Cleaning the Training Data
</subsectionHeader>
<bodyText confidence="0.999980235294118">
To find a sentence on the Wikipedia page that is
likely to express a relation in its infobox, we con-
sider the first sentence on the page that mentions
both arguments of the relation. This heuristic ap-
proach returns reasonably good results, but brings in
about 20% noise in the form of false positives, which
is a concern when building an accurate statistical re-
lation detector. To address this issue, we have devel-
oped a two-step technique to automatically remove
some of the noisy data. In the first step, we extract
popular argument types and keywords for each DB-
pedia relation from the given data, and then use the
combinations of those types and words to create ini-
tial rules. Many of the argument types and keywords
introduced by the noisy data are often not very pop-
ular, so they can be filtered out in the first step. Not
all initial rules make sense. In the second step, we
</bodyText>
<page confidence="0.960291">
1428
</page>
<bodyText confidence="0.999843142857143">
check each rule against the training data to see if that
rule really exists in the training data or not. If it does
not exist, we filter it out. If a sentence does not have
a single rule passing the above procedure, that sen-
tence will be removed. Using the above techniques,
we collect examples characterizing 7,628 DBpedia
relations.
</bodyText>
<sectionHeader confidence="0.930475" genericHeader="method">
3 Learning Multiscale Relation Topics
</sectionHeader>
<bodyText confidence="0.999919318181818">
An extra step extracting knowledge from the raw
data is needed for two reasons: Firstly, many DB-
pedia relations are inter-related. For example, some
DBpedia relations have a subclass relationship, e.g.
“AcademyAward” and “Award”; others overlap in
their scope and use, e.g., “Composer” and “Artist”;
while some are equivalent, e.g., “DateOfBirth” and
“BirthDate”. Secondly, a fairly large amount of the
noisy labels are still in the training data.
To reveal the intrinsic structure of the current DB-
pedia relation space and filter out noise, we car-
ried out a correlation analysis of relations in the
training data, resulting in a relation topic space.
Each relation topic is a multinomial distribution
over the existing relations. We adapted diffusion
wavelets (Coifman and Maggioni, 2006) for this
task. Compared to the other well-known topic ex-
traction methods like LDA (Blei et al., 2003) and
LSI (Deerwester et al., 1990), diffusion wavelets can
efficiently extract a hierarchy of interpretable topics
without any user input parameter (Wang and Ma-
hadevan, 2009).
</bodyText>
<subsectionHeader confidence="0.999818">
3.1 An Overview of Diffusion Wavelets
</subsectionHeader>
<bodyText confidence="0.999913">
The diffusion wavelets algorithm constructs a com-
pressed representation of the dyadic powers of a
square matrix by representing the associated matri-
ces at each scale not in terms of the original (unit
vector) basis, but rather using a set of custom gener-
ated bases (Coifman and Maggioni, 2006). Figure
2 summarizes the procedure to generate diffusion
wavelets. Given a matrix T, the QR (a modified
QR decomposition) subroutine decomposes T into
an orthogonal matrix Q and a triangular matrix R
such that T Pz� QR, where |Ti,k − (QR)i,k |&lt; e
for any i and k. Columns in Q are orthonormal ba-
sis functions spanning the column space of T at the
finest scale. RQ is the new representation of T with
</bodyText>
<figure confidence="0.537339444444444">
{[j]φ0} = DWT (T, , J)
//INPUT:
//T: The input matrix.
//: Desired precision, which can be set to a small
number or simply machine precision.
//J: Number of levels (optional).
//OUTPUT:
//[j]φ0: extended diffusion scaling functions at
scale j.
</figure>
<equation confidence="0.9195075">
0 = I;
For j = 0 to J − 1 {
([j+1]φj, [T2j]φj+1
φj ) — QR([T2j]φj
φj, );
[j+1]φ0 = [j+1]φj[j]φ0;
[T 2j+1]φj+1 φj+1 = ([T 2j]φj+1 φj[j+1]φj)2;
}
</equation>
<figureCaption confidence="0.931854">
Figure 2: Diffusion Wavelets construct multiscale repre-
</figureCaption>
<bodyText confidence="0.958013291666667">
sentations of the input matrix at different scales. QR is a
modified QR decomposition. J is the max step number
(this is optional, since the algorithm automatically ter-
minates when it reaches a matrix of size 1 x 1). The
notation [T]φb
φa denotes matrix T whose column space is
represented using basis b at scale b, and row space is
represented using basis a at scale a. The notation [b]φa
denotes basis b represented on the basis a. At an arbi-
trary scale j, we have pj basis functions, and length of
each function is lj. The number of pj is determined by
the intrinsic structure of the given dataset in QR routine.
[T]φb is a pb x la matrix, and [b]φa is an la x pb matrix.
φa
respect to the space spanned by the columns of Q
(this result is based on the matrix invariant subspace
theory). At an arbitrary level j, DWT learns the ba-
sis functions from T2&apos; using QR. Compared to the
number of basis functions spanning T2&apos;’s original
column space, we usually get fewer basis functions,
since some high frequency information (correspond-
ing to the “noise” at that level) can be filtered out.
DWT then computes T2&apos;+1 using the low frequency
representation of T23 and the procedure repeats.
</bodyText>
<subsectionHeader confidence="0.999525">
3.2 Constructing Multiscale Relation Topics
Learning Relation Correlations
</subsectionHeader>
<bodyText confidence="0.9998982">
Assume we have M relations, and the ith of them
is characterized by mi &lt;rule, popularity&gt; pairs. We
use s(a, b) to represent the similarity between the
ath and bth relations. To compute s(a, b), we first
normalize the popularities for each relation, and then
</bodyText>
<page confidence="0.976511">
1429
</page>
<bodyText confidence="0.998550285714286">
look for the rules that are shared by both relation a
and b. We use the product of corresponding pop-
ularity values to represent the similarity score be-
tween two relations with respect to each common
rule. s(a, b) is set to the sum of such scores over
all common rules. The relation-relation correlation
matrix S is constructed as follows:
</bodyText>
<equation confidence="0.999692666666667">
8(1,1) · · · 8(1, M)
S = [ · · · · · · · · · ]
8(M,1) · · · 8(M, M)
</equation>
<bodyText confidence="0.9999938">
We have more than 200, 000 argument types, tens
of thousands of distinct nouns, prepositions, and
verbs, so we potentially have trillions of distinct
rules. One rule may appear in multiple relations.
The more rules two relations share, the more related
two relations should be. The rules shared across dif-
ferent relations offer us a novel way to model the
correlations between different relations, and further
allow us to create relation topics. The rules can also
be simplified. For example, we may treat argument1,
argument2, noun, preposition and verb separately.
This results in simple rules that only involve in one
argument type or word. The correlations between
relations are then computed only based on one par-
ticular component like argument1, noun, etc.
</bodyText>
<subsectionHeader confidence="0.937801">
Theoretical Analysis
</subsectionHeader>
<bodyText confidence="0.988831139534884">
Matrix S models the correlations between rela-
tions in the training data. Once S is constructed, we
adapt diffusion wavelets (Coifman and Maggioni,
2006) to automatically extract the basis functions
spanning the original column space of S at multi-
ple scales. The key strength of the approach is that
it is data-driven, largely parameter-free and can au-
tomatically determine the number of levels of the
topical hierarchy, as well as the topics at each level.
However, to apply diffusion wavelets to S, we first
need to show that S is a positive semi-definite ma-
trix. This property guarantees that all eigenvalues
of S are &gt; 0. Depending on the way we formal-
ize the rules, the methods to validate this property
are slightly different. When we treat argument1,
argument2, noun, preposition and verb separately, it
is straightforward to see the property holds. In The-
orem 1, we show the property also holds when we
use more complicated rules (using the 5-tuple rule
in Section 2.2 as an example in the proof).
Theorem 1. S is a Positive Semi-Definite matrix.
Proof: An arbitrary rule ri is uniquely characterized
by a five tuple: argument1 type |argument2 type|
noun |preposition |verb. Since the number of dis-
tinct argument types and words are constants, the
number of all possible rules is also a constant: R.
If we treat each rule as a feature, then the set of
rules characterizing an arbitrary relation ri can be
represented as a point [p1i , · · · , pRi ] in a latent R di-
mensional rule space, where pji represents the popu-
larity of rule j in relation ri in the given data.
We can verify that the way to compute s(a, b) is
the same as s(a, b) = &lt; [p1a · · · pRa ], [p1b · · · pRb ] &gt;,
where &lt; ·, · &gt; is the cosine similarity (kernel). It
follows directly from the definition of positive semi-
definite matrix (PSD) that S is PSD (Sch¨olkopf and
Smola, 2002).
In our approach, we construct multiscale re-
lation topics by applying DWT to decompose
S/Amax(S), where Amax(S) represents the largest
eigenvalue of S. Theorem 2 shows that this decom-
position will converge, resulting in a relation topic
hierarchy with one single topic at the top level.
</bodyText>
<construct confidence="0.6692162">
Theorem 2. Let Amax(S) represent the largest
eigenvalue of matrix S, then DWT(S/Amax(S), E)
produces a set of nested subspaces of the column
space of S, and the highest level of the resulting sub-
space hierarchy is spanned by one basis function.
</construct>
<listItem confidence="0.830591">
Proof: From Theorem 1, we know that S is a PSD
matrix. This means Amax(S) E [0,+oo) (all eigen-
values of S are non-negative). This further implies
that A(S)/ Amax(S) E [0, 1], where A(S) represents
any eigenvalue of S.
</listItem>
<bodyText confidence="0.999809375">
The idea underlying diffusion wavelets is based
on decomposing the spectrum of an input matrix
into various spectral bands, spanned by basis func-
tions (Coifman and Maggioni, 2006). Let T =
S/Amax(S). In Figure 2, we construct spectral
bands of eigenvalues, whose associated eigenvectors
span the corresponding subspaces. Define dyadic
spatial scales tj as
</bodyText>
<equation confidence="0.999087">
2t = 2j+1 − 1, j &gt; 0 .
At each spatial scale, the spectral band is defined as:
Qj(T) = {A E A(T), Atj &gt; E},
� j
t=0
tj =
</equation>
<page confidence="0.750229">
1430
</page>
<bodyText confidence="0.971527714285714">
where A(T) represents any eigenvalue of T, and E E
(0, 1) is a pre-defined threshold in Figure 2. We can
now associate with each of the spectral bands a vec-
tor subspace spanned by the corresponding eigen-
vectors:
Vj = (f�a : A E A(T), Atj &gt; E}), j &gt; 0 .
In the limit, we obtain
</bodyText>
<equation confidence="0.570415">
Vj = (f�a : A = 1})
</equation>
<bodyText confidence="0.999916428571429">
That is, the highest level of the resulting subspace
hierarchy is spanned by the eigenvector associated
with the largest eigenvalue of T.
This result shows that the multiscale analysis of
the relation space will automatically terminate at the
level spanned by one basis, which is the most popu-
lar relation topic in the training data.
</bodyText>
<subsectionHeader confidence="0.995042">
3.3 High Level Explanation
</subsectionHeader>
<bodyText confidence="0.999917875">
We first create a set of rules to characterize each in-
put relation. Since these rules may occur in multi-
ple relations, they provide a way to model the co-
occurrence relationship between different relations.
Our algorithm starts with the relation co-occurrence
matrix and then repeatedly applies QR decomposi-
tion to learn the topics at the current level while at
the same time modifying the matrix to focus more on
low-frequency indirect co-occurrences (between re-
lations) for the next level. Running DWT is equiv-
alent to running a Markov chain on the input data
forward in time, integrating the local geometry and
therefore revealing the relevant geometric structures
of the whole data set at different scales. At scale
j, the representation of T2&apos;+1 is compressed based
on the amount of remaining information and the de-
sired precision. This procedure is illustrated in Fig-
ure 3. In the resulting topic space, instances with
related relations will be grouped together. This ap-
proach may significantly help us detect new rela-
tions, since it potentially expands the information
brought in by new relation instances from making
use of the knowledge extracted from the existing re-
lation repository.
</bodyText>
<subsectionHeader confidence="0.855957">
3.4 Benefits
</subsectionHeader>
<bodyText confidence="0.999418">
As shown in Figure 3, the topic spaces at different
levels are spanned by a different number of basis
</bodyText>
<figureCaption confidence="0.984368">
Figure 3: Learning Relation Topics at Multiple Scales.
</figureCaption>
<bodyText confidence="0.959165833333333">
functions. These numbers reveal the dimensions of
the relevant geometric structures of data at different
levels. These numbers are completely data-driven:
the diffusion wavelets approach can automatically
find the number of levels and simultaneously gen-
erate the topics at each level. Experiments show that
most multiscale topics are interpretable (due to the
sparsity of the scaling functions), such that we can
interpret the topics at different scales and select the
best scale for embedding. Compared to bootstrap-
ping approach, our approach is accumulative; that
is as the system learns more relations, it gets bet-
ter at learning new relations. Because our approach
takes advantage of the previously learned relations,
and the topic space is enriched as we learn more and
more relations.
We use diffusion wavelets (DWT) rather than
other hierarchy topic models like hLDA (Blei et
al., 2004) to extract relation topics for two rea-
sons. First, DWT is parameter free while other
models need some user-input parameters like hier-
archy level. Second, DWT is more efficient than the
other models. After the relation correlation matrix
is constructed, DWT only needs a couple of min-
utes to extract multiscale topics on a regular com-
puter. A direct experimental comparison between
DWT and hLDA can be found in (Wang and Ma-
hadevan, 2009).
lim
j→∞
</bodyText>
<page confidence="0.988622">
1431
</page>
<sectionHeader confidence="0.986018" genericHeader="method">
4 Constructing Relation Detectors with
Multiscale Relation Topics
</sectionHeader>
<subsectionHeader confidence="0.999415">
4.1 Project Relation Instances onto Topics
</subsectionHeader>
<bodyText confidence="0.999643789473684">
When we design detectors for new relations, we
treat arg1, arg2, noun, and verb separately to
get stronger correlations between relations. We
do not directly use preposition. Any DBpe-
dia relation r  {1, · · · , M} is represented with
4 vectors rt = [rt(1), · · · , rt(Nt)], where t 
{arg1, arg2, noun, verb}, Nt represents the size of
the vocabulary set of the type t component in the
Wikipedia training data, and rt(j) represents the oc-
currence count of type t component in relation r. For
example, Nverb is the size of the verb vocabulary set
in the training data and rverb(j) represents the occur-
rence count of the jth verb in relation r. When a new
relation instance x is given, we extract the depen-
dency path between two arguments, and create four
vectors xt, where t  {arg1, arg2, noun, verb},
following the same format as rt. The projection re-
sult of xt onto the DBpedia relation space Xt is as
follows:
</bodyText>
<equation confidence="0.541621">
Xt = [&lt; rt(1), xt(1) &gt;, · · · , &lt; rt(M), xt(M) &gt;],
</equation>
<bodyText confidence="0.990300666666667">
where &lt; ·, · &gt; is the cosine similarity of two vec-
tors. At level k, the embedding of x is Ekx =
[EkX , EX , EkX , EkX b], where Ek =
</bodyText>
<subsectionHeader confidence="0.901953">
4.2 Design New Kernel Using Topic Features
</subsectionHeader>
<bodyText confidence="0.99995">
We combine Exk with 3 existing kernels (KArgument,
KPath and KBOW) to create a new kernel for rela-
tion detection.
</bodyText>
<listItem confidence="0.9934545">
(1) KArgument matches two arguments, it returns the
number of common argument types that the input ar-
guments share.
(2) KPath matches two dependency paths. This
kernel is formally defined in (Zhao and Grishman,
2005). We extended this kernel by also matching
the common nouns, prepositions and verbs in the de-
pendency paths. We assign weight 1 to verbs, 0.5 to
nouns and prepositions.
(3) KBOW models the number of common nouns,
</listItem>
<bodyText confidence="0.677626888888889">
prepositions and verbs in the given sentences but
not in the dependency paths. Since these words are
not as important as the words inside the dependency
path, we assign weight 0.25 to them.
(4) KTFk(x, y) =&lt; Exk, Eky &gt;, where x, y are two
input relation instances, and &lt; ·, · &gt; models the co-
sine similarity of two vectors. TF stands for topic
feature.
(5) The final kernel used in this paper is
</bodyText>
<equation confidence="0.751959">
a1KArgument + a2KPath + a3KBOW + a4KTF,,,
</equation>
<bodyText confidence="0.9719895">
where αz can be tuned for each individual domain.
In this paper, we set αz = 1 for i  {1, 2, 3, 4}.
</bodyText>
<subsectionHeader confidence="0.886332">
4.3 Algorithm to Construct Relation Detectors
</subsectionHeader>
<figure confidence="0.8605397">
1. Construct a relation repository from Wikipedia.
(a) Collect training data from Wikipedia and DB-
pedia (Section 2.1);
(b) Clean the data representing each input relation
(Section 2.2 and 2.3);
(c) Create relation correlation matrix S following
the approach described in Section 3.2, result-
ing in an M x M matrix.
2. Create multiscale relation topics.
[Ok]o0 = DWT (S/Amax(S), e), where DWT () is
</figure>
<bodyText confidence="0.839368571428571">
the diffusion wavelets implementation described in
Section 3.1. [Ok]oo are the scaling function bases
at level k represented as an M x pk matrix, k =
1, · · · , h represents the level in the topic hierarchy.
The value of pk is determined in DWT () based on
the intrinsic structure of the given dataset. Columns
of [Ok]oo are used as relation topics at level k.
</bodyText>
<sectionHeader confidence="0.334453" genericHeader="method">
3. Construct relation detectors for new relations.
</sectionHeader>
<bodyText confidence="0.996723">
Given the training data from a new relation, project
the data onto level k of the multiscale topic hierar-
chy, where k is chosen by users (Section 4.1). Ap-
ply SVM classifiers together with our kernel (Sec-
tion 4.2) to create detectors for new relations.
</bodyText>
<sectionHeader confidence="0.987415" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.98929675">
We used SVMLight (Joachims, 1999) together with
the user defined kernel setting in our approach. The
trade-off parameter between training error and mar-
gin c is 1 for all experiments. Our approach to
learn multiscale relation topics is largely parameter
free. The only parameter to be set is the precision
ε = 10−5, which is also the default value in the dif-
fusion wavelets implementation.
</bodyText>
<subsectionHeader confidence="0.998331">
5.1 Learning Multiscale Relation Topics
</subsectionHeader>
<bodyText confidence="0.8005046">
Following the approach discussed in Section 2.1,
we collect more than 620,000 training instances for
arg1 arg
2 noun ver
([0k]00)TXt, and [�k]00 is defined in Figure 2.
</bodyText>
<page confidence="0.994367">
1432
</page>
<tableCaption confidence="0.98588725">
Table 1: Number of topics at different levels (DBpe-
dia Relations) under 5 different settings: use args, noun,
preposition and verb; arg1 only; arg2 only; noun only and
verb only.
</tableCaption>
<table confidence="0.977413625">
Level args &amp; words arg1 arg2 noun verb
1 7628 7628 7628 7628 7628
2 269 119 155 249 210
3 32 17 19 25 35
4 7 5 5 7 10
5 3 2 3 4 4
6 2 1 2 2 2
7 1 1 1 1
</table>
<bodyText confidence="0.990680666666667">
7,628 DBpedia relations. For any given topic vec-
tor v, we know it is a column vector of length M,
where M is the size of the DBpedia relation set
and �vl = 1. The entry v[i] represents the contri-
bution of relation i to this topic. To explain the
main concept of topic v, we sort the entries on
v and print out the relations corresponding to the
top entries. These relations summarize the top-
ics in the relation repository. One topic exam-
ple is as follows: [doctoraladvisor (0.683366), doc-
toralstudents (0.113201), candidate (0.014662), academ-
icadvisors (0.008623), notablestudents (0.003829), col-
lege (0.003021), operatingsystem (0.002964), combatant
(0.002826), influences (0.002285), training (0.002148),
· · · ], where doctoraladvisor is a DBpedia relation
and 0.683366 is its contribution to the topic. The
length of this relation vector is 7,628. We only list
the top 10 relations here.
Our approach identifies 5 different topic hierar-
chies under different settings (use args, noun, prepo-
sition and verb; arg1 only; arg2 only; noun only and
verb only). The number of the topics at each level is
shown in Table 1. At the first level, each input rela-
tion is treated as a topic. At the second level, num-
bers of topics go down to reasonable numbers like
269. Finally at the top level, the number of topic is
down to 1 (Theorem 2 also proves this). We show
some topic examples under the first setting. The 3
topics at level 5 are shown in Table 2. They represent
the most popular DBpedia relation topics. Almost
all 269 topics at level 5 look semantically meaning-
ful. They nicely capture the related relations. Some
examples are in Table 3.
</bodyText>
<tableCaption confidence="0.992197">
Table 2: 3 topics at level 5 (all word types and args).
</tableCaption>
<table confidence="0.665208">
Top 4 Relations and Their Contributions
starring 86.6%, writer 3.8%, producer 3.2%, director 1.6%
birthplace 75.3%, clubs 6.1%, deathplace 5.1%, location 4.1%
clubs 55.3%, teams 9.3%, nationalteam 6.3% college 6.0%
</table>
<tableCaption confidence="0.993284">
Table 3: Some topics at level 2 (all word types and args).
</tableCaption>
<table confidence="0.9921055">
Top Relations
activeyearsenddate, careerend, finalyear, retired
commands, partof, battles, notablecommanders
occupation, shortdescription, profession, dates
influenced, schooltradition, notableideas, maininterests
destinations, end, through, posttown
prizes, award, academyawards, highlights
inflow, outflow, length, maxdepth
after, successor, endingterminus
college, almamater, education
</table>
<subsectionHeader confidence="0.992392">
5.2 Relation Detection on Wikipedia Data
</subsectionHeader>
<bodyText confidence="0.999937862068966">
In previous experiment, 20,000 relation instances
were held and not used to construct the topic space.
These instances are randomly selected from 100 re-
lations (200 instances from each relation). This set
is used as a benchmark to compare different rela-
tion detection approaches. In this experiment, 100
instances from each relation are used for training,
and the other 100 are for testing. In training, we try
three different settings: n = 5, 20 and 100, where n
is the size of the training set for each relation. When
we train a model for one relation, we use the train-
ing positive instances from the other 99 relations as
training negatives. For example, we use 5 training
positive instances and 5*99=495 training negatives
to train a detector for each relation.
We compare our approach against the regular
rule-based approach (Lin and Pantel, 2001) and two
other kernel-based approaches (presented in Sec-
tion 4.2) for relation detection task. The comparison
results are summarized in Table 4. The approach
using relation topics (level 2) consistently outper-
forms the other three approaches in all three settings.
When n = 5, it achieves the largest improvement
over the other three. This indicates that using re-
lation topics that integrate the knowledge extracted
from the existing relations, can significantly benefit
us when the training data is insufficient. This is rea-
sonable, since the prior knowledge becomes more
valuable in this scenario.
</bodyText>
<page confidence="0.694004">
1433
</page>
<bodyText confidence="0.9909435">
The users can select the level that is the most ap- heuristic rules were applied. We are aware of some
propriate for their applications. In this example, we methods that could stack on our approach to further
only have alignment results at 7 levels. Choosing the improve the performance on ACE test. The Com-
space at level 2 spanned by a couple of hundreds of posite kernel result in Table 5 is based on a linear
basis functions is a natural choice, since the levels combination of the Argument kernel and Convolu-
below and above this have too many or too few fea- tion Tree kernel. (Zhang et al., 2006) showed that
tures, respectively. A user can also select the most by carefully choosing the weight of each compo-
appropriate level by checking if the related relation nent and using a polynomial expansion, they could
topics are meaningful for their applications. achieve the best performance on this data: 72.1% F-
5.3 Relation Detection on ACE Data measure. (Nguyen et al., 2009) further showed that
In this experiment, we use the news domain docu- the performance can be improved by taking syntac-
ments of the ACE 2004 corpus (ACE, 2004) to com- tic and semantic structures into consideration. They
pare our approaches against the state-of-the-art ap- used several types of syntactic information includ-
proaches. This dataset includes 348 documents and ing constituent and dependency syntactic parse trees
around 4400 relation instances. 7 relation types, to improve the state of the art approaches to 71.5%
7 entity types, numerous relation sub-types, entity on F-measure. Heuristic rules extracted from the
sub-types, and mention types are defined on this target data can also help improve the performance.
set. The task is to classify the relation instances (Jiang and Zhai, 2007) reported that by taking sev-
into one of the 7 relation types or “NONE”, which eral heuristic rules they can improve the F-measure
means there is no relation. For comparison, we use of Composite Kernel to 70.4%. They also showed
the same setting as (Zhang et al., 2006), by apply- that using maximum entropy classifier rather than
ing a 5-fold cross-validation. The scores reported SVM achieved the best performance on this task:
here are the average of all 5 folds. This is also how 72.9% F-measure. To the best of our knowledge, the
the other approaches are evaluated. In this test, we most recent result was reported by (Zhou and Zhu,
treat entity types, entity sub-types and mention types 2011), who extended their previous work in (Zhou
equally as argument types. Table 5 summarizes et al., 2007). By using several heuristics to define
the performance after applying the kernels presented an effective portion of constituent trees, and training
in Section 4.2 incrementally, showing the improve- the classifiers using ACE relation sub-types (rather
ment from each individual kernel. We also com- than on types), they achieved an impressive 75.8%
pare our approaches to the other state-of-the-art ap- F-measure. However, as pointed out in (Nguyen et
proaches including Convolution Tree kernel (Collins al., 2009), such heuristics are tuned on the target re-
and Duffy, 2001), Syntactic kernel (Zhao and Grish- lation extraction task and might not be appropriate to
man, 2005), Composite kernel (linear) (Zhang et al., compare against the automatic learning approaches.
2006) and the best kernel in (Nguyen et al., 2009). Even though we have not done any domain specific
Our approach with relation topics at level 2 has the parameter tuning or applied any heuristics, our ap-
best performance, achieving a 73.24% F-measure. proach still achieve significant improvements over
The impact of the relation topics is huge. They im- all approaches mentioned above except one, which
prove the F-measure from 61.15% to 73.24%. We is based on heuristics extracted from the target do-
also test our approach using the topics at level 3. main. This also implies that by combining some of
The performance is slightly worse than using level the above ideas with relation topics, the performance
2, but still better than the others. on ACE data may be further improved.
This paper studies how relation topics extracted 6 Conclusions
from Wikipedia relation repository can help improve This paper proposes a novel approach to create de-
relation detection performance. We do not want to tectors for new relations integrating the knowledge
tune our approach to one particular relation detec- extracted from the existing relations. The contribu-
tion task, like ACE 2004. In our experiments, no tions of this paper are three-fold. Firstly, we pro-
parameter tuning was taken and no domain specific
1434
</bodyText>
<tableCaption confidence="0.784083">
Table 4: F-measure comparison of different approaches
over 100 DBpedia relations with 5, 20 and 100 posi-
tive examples per relation. AG: KArgument, DP: KPath,
</tableCaption>
<table confidence="0.973516166666667">
BOW: KBOW, TFk: KTFk.
Approaches 100 20 5
Rule Based 37.70% 27.45% 13.20%
AG+ DP 73.64% 51.85% 22.95%
AG+ DP+ BOW 78.74% 62.76% 31.98%
AG+ DP+ BOW+ TF2 81.18% 68.03% 41.60%
</table>
<tableCaption confidence="0.976639">
Table 5: Performance comparison of different approaches
with SVM over the ACE 2004 data. P: Precision, R: Re-
call, F: F-measure, AG: KArgument, DP: KPath, BOW:
</tableCaption>
<table confidence="0.954798545454545">
KBOW, TFk: KTFk.
Approaches P(%) R(%) F(%)
Convolution Tree Kernel 72.5 56.7 63.6
Composite Kernel (linear) 73.50 67.00 70.10
Syntactic Kernel 69.23 70.50 69.86
Nguyen, et al. (2009) 76.60 67.00 71.50
AG 59.56 46.22 52.02
AG + DP 64.44 54.93 59.28
AG + DP + BOW 62.00 61.19 61.15
AG + DP + BOW + TF3 69.63 76.51 72.90
AG + DP + BOW + TF2 69.15 77.88 73.24
</table>
<bodyText confidence="0.999763941176471">
vide an automatic way to collect training data for
more than 7,000 relations from Wikipedia and DB-
pedia. Secondly, we present an unsupervised way to
construct a set of relation topics at multiple scales.
Different from the topics defined over words, rela-
tion topics are defined over the existing relations.
Thirdly, we design a new kernel for relation detec-
tion by integrating the relation topics in the repre-
sentation of the relation instances. By leveraging
the knowledge extracted from the Wikipedia rela-
tion repository, our approach significantly improves
the performance over the state-of-the-art approaches
on ACE data. This paper makes use of all DBpedia
relations to create relation topics. It is possible that
using a subset of them (more related to the target
relations) might improve the performance. We will
explore this in future work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996635">
We thank the reviewers for their helpful comments.
This material is based upon work supported in part
by the IBM DeepQA (Watson) project. We also
gratefully acknowledge the support of Defense Ad-
vanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment.
</bodyText>
<sectionHeader confidence="0.998241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993982">
ACE. 2004. The automatic content extraction projects,
http://projects.ldc.upenn.edu/ace/.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
DBpedia: A nucleus for a web of open data. In Pro-
ceedings of the 6th International Semantic Web Con-
ference, Busan, Korea, pages 11–15. Springer.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let allocation. Journal of Machine Learning Research,
3:993–1022.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 2004.
Hierarchical topic models and the nested Chinese
restaurant process. In Proceedings of the Advances in
Neural Information Processing Systems (NIPS).
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics, pages 152–160.
R. Coifman and M. Maggioni. 2006. Diffusion
wavelets. Applied and Computational Harmonic
Analysis, 21:53–94.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of the
Advances in Neural Information Processing Systems
(NIPS), pages 625–632.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 423–429.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391–407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
</reference>
<page confidence="0.825078">
1435
</page>
<reference confidence="0.99920461627907">
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An ex-
perimental study. Artificial Intelligence, 165:91–134.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 286–295.
Jing Jiang and Chengxiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 113–120.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics (ACL)
and the 4th International Joint Conference on Natural
Language Processing (IJCNLP), pages 1012–1020.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. MIT Press.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323–328.
Michael McCord. 1995. Slot grammar: A system
for simpler construction of practical natural language
grammars. Communications of the ACM, 38(11).
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical pars-
ing to extract information from text. In Proceedings
of the 1st North American Chapter of the Association
for Computational Linguistics Conference.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), pages 1003–1011.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
B. Sch¨olkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A large ontology from
Wikipedia and WordNet. Web Semantics: Science,
Services and Agents on the World Wide Web, 6(3):203–
217.
C. Wang and S. Mahadevan. 2009. Multiscale analysis
of document corpora based on diffusion models. In
Proceedings of the International Joint Conference on
Artificial Intelligence (IJCAI), pages 1592–1597.
Wikipedia. 2011. http://www.wikipedia.org/.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations between
entities with both flat and structured features. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (ACL).
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 419–426.
G. Zhou and Q. Zhu. 2011. Kernel-based semantic rela-
tion detection and classification via enriched parse tree
structure. Journal of Computer Science and Technol-
ogy, 26:45–56.
G. Zhou, M. Zhang, D. Ji, and Q. Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP).
</reference>
<page confidence="0.992871">
1436
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958456">
<title confidence="0.999736">Relation Extraction with Relation Topics</title>
<author confidence="0.997922">Chang Wang James Fan Aditya Kalyanpur David</author>
<affiliation confidence="0.996472">IBM T. J. Watson Research</affiliation>
<address confidence="0.984069">19 Skyline Drive, Hawthorne, New York</address>
<email confidence="0.992564">fanj,adityakal,</email>
<abstract confidence="0.9994745">This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation’s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>The automatic content extraction projects,</title>
<date>2004</date>
<location>http://projects.ldc.upenn.edu/ace/.</location>
<contexts>
<context position="2567" citStr="ACE, 2004" startWordPosition="384" endWordPosition="385">t of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has focused on how to create detectors from ground up with as little training data as possible through techniques such as bootstrapping (Etzioni et al., 2005). We take a different approach, focusing on how the knowledge extracted from the existing relations can be reused to help build detectors for new relations. We </context>
<context position="6741" citStr="ACE, 2004" startWordPosition="1054" endWordPosition="1055">e extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DBpedia (Auer et al., 2007). A key part of this step is how we handle noisy data with little human effort. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. This step is parameter free, and results in a nonredundant, multiscale relation topic space. Thirdly, we design a new kernel for relation detection by integrating the relation topics into the relation detector construction. The experimental results on Wikipedia and ACE data (ACE, 2004) have confirmed that background-knowledge-based features generated from the Wikipedia relation repository can significantly improve the performance over the state-of-the-art relation detection approaches. 2 Extracting Relations from Wikipedia Our training data is from two parts: relation instances from DBpedia (extracted from Wikipedia infoboxes), and sentences describing the relations from the corresponding Wikipedia pages. 2.1 Collecting the Training Data Since our relations correspond to Wikipedia infobox properties, we use an approach similar to that described in (Hoffmann et al., 2010) to</context>
<context position="33208" citStr="ACE 2004" startWordPosition="5558" endWordPosition="5559">olubelow and above this have too many or too few fea- tion Tree kernel. (Zhang et al., 2006) showed that tures, respectively. A user can also select the most by carefully choosing the weight of each compoappropriate level by checking if the related relation nent and using a polynomial expansion, they could topics are meaningful for their applications. achieve the best performance on this data: 72.1% F5.3 Relation Detection on ACE Data measure. (Nguyen et al., 2009) further showed that In this experiment, we use the news domain docu- the performance can be improved by taking syntacments of the ACE 2004 corpus (ACE, 2004) to com- tic and semantic structures into consideration. They pare our approaches against the state-of-the-art ap- used several types of syntactic information includproaches. This dataset includes 348 documents and ing constituent and dependency syntactic parse trees around 4400 relation instances. 7 relation types, to improve the state of the art approaches to 71.5% 7 entity types, numerous relation sub-types, entity on F-measure. Heuristic rules extracted from the sub-types, and mention types are defined on this target data can also help improve the performance. set. The t</context>
<context position="36617" citStr="ACE 2004" startWordPosition="6101" endWordPosition="6102">o implies that by combining some of The performance is slightly worse than using level the above ideas with relation topics, the performance 2, but still better than the others. on ACE data may be further improved. This paper studies how relation topics extracted 6 Conclusions from Wikipedia relation repository can help improve This paper proposes a novel approach to create derelation detection performance. We do not want to tectors for new relations integrating the knowledge tune our approach to one particular relation detec- extracted from the existing relations. The contribution task, like ACE 2004. In our experiments, no tions of this paper are three-fold. Firstly, we proparameter tuning was taken and no domain specific 1434 Table 4: F-measure comparison of different approaches over 100 DBpedia relations with 5, 20 and 100 positive examples per relation. AG: KArgument, DP: KPath, BOW: KBOW, TFk: KTFk. Approaches 100 20 5 Rule Based 37.70% 27.45% 13.20% AG+ DP 73.64% 51.85% 22.95% AG+ DP+ BOW 78.74% 62.76% 31.98% AG+ DP+ BOW+ TF2 81.18% 68.03% 41.60% Table 5: Performance comparison of different approaches with SVM over the ACE 2004 data. P: Precision, R: Recall, F: F-measure, AG: KArgum</context>
</contexts>
<marker>ACE, 2004</marker>
<rawString>ACE. 2004. The automatic content extraction projects, http://projects.ldc.upenn.edu/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>DBpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference, Busan, Korea,</booktitle>
<pages>11--15</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6276" citStr="Auer et al., 2007" startWordPosition="973" endWordPosition="976">and Roth, 2010). One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base. The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DBpedia (Auer et al., 2007). A key part of this step is how we handle noisy data with little human effort. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. This step is parameter free, and results in a nonredundant, multiscale relation topic space. Thirdly, we design a new kernel for relation detection by integrating the relation topics into the relation detector construction. The experimental results on Wikipedia and ACE data (ACE, 2004) have confirmed that background-knowledge-based features generated from the Wikipedia relation repository can significantly improve the</context>
<context position="8358" citStr="Auer et al., 2007" startWordPosition="1303" endWordPosition="1306">erty “alma mater” with value “University of Zurich”, and the first sentence mentioning the arguments is the following: “Einstein was awarded a PhD by the University of Zurich”, which expresses the relation. When looking for relation arguments on the page, we go beyond (sub)string matching, and use link information to match entities which may have different surface forms. Using this technique, we are able to collect a large amount of positive training instances of DBpe1427 dia relations. To get precise type information for the arguments of a DBpedia relation, we use the DBpedia knowledge base (Auer et al., 2007) and the associated YAGO type system (Suchanek et al., 2007). Note that for every Wikipedia page, there is a corresponding DBpedia entry which has captured the infobox-properties as RDF triples. Some of the triples include type information, where the subject of the triple is a Wikipedia entity, and the object is a YAGO type for the entity. For example, the DBpedia entry for the entity “Albert Einstein” includes YAGO types such as Scientist, Philosopher, Violinist etc. These YAGO types are also linked to appropriate WordNet concepts, providing for accurate sense disambiguation. Thus, for any en</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In Proceedings of the 6th International Semantic Web Conference, Busan, Korea, pages 11–15. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4674" citStr="Blei et al., 2003" startWordPosition="713" endWordPosition="716">tribute/value pairs describing the article’s subject. Another challenge is how to deal with overlap of relations in the repository. For example, Wikipedia authors may make up a name when a new relation is needed without checking if a similar relation has already been created. This leads to relation duplication. We refine the relation repository based on an unsupervised multiscale analysis of the correlations between existing relations. This method is parameter free, and able to produce a set of non-redundant relation topics defined at multiple scales. Similar to the topics defined over words (Blei et al., 2003), we define relation topics as multinomial distributions over the existing relations. The relation topics extracted in our approach are interpretable, orthonormal to each other, and can be used as basis relations to re-represent the new relation instances. The third challenge is how to use the relation topics for a relation detector. We map relation instances in the new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be</context>
<context position="13300" citStr="Blei et al., 2003" startWordPosition="2109" endWordPosition="2112">, “Composer” and “Artist”; while some are equivalent, e.g., “DateOfBirth” and “BirthDate”. Secondly, a fairly large amount of the noisy labels are still in the training data. To reveal the intrinsic structure of the current DBpedia relation space and filter out noise, we carried out a correlation analysis of relations in the training data, resulting in a relation topic space. Each relation topic is a multinomial distribution over the existing relations. We adapted diffusion wavelets (Coifman and Maggioni, 2006) for this task. Compared to the other well-known topic extraction methods like LDA (Blei et al., 2003) and LSI (Deerwester et al., 1990), diffusion wavelets can efficiently extract a hierarchy of interpretable topics without any user input parameter (Wang and Mahadevan, 2009). 3.1 An Overview of Diffusion Wavelets The diffusion wavelets algorithm constructs a compressed representation of the dyadic powers of a square matrix by representing the associated matrices at each scale not in terms of the original (unit vector) basis, but rather using a set of custom generated bases (Coifman and Maggioni, 2006). Figure 2 summarizes the procedure to generate diffusion wavelets. Given a matrix T, the QR </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese restaurant process.</title>
<date>2004</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="23386" citStr="Blei et al., 2004" startWordPosition="3859" endWordPosition="3862">ch level. Experiments show that most multiscale topics are interpretable (due to the sparsity of the scaling functions), such that we can interpret the topics at different scales and select the best scale for embedding. Compared to bootstrapping approach, our approach is accumulative; that is as the system learns more relations, it gets better at learning new relations. Because our approach takes advantage of the previously learned relations, and the topic space is enriched as we learn more and more relations. We use diffusion wavelets (DWT) rather than other hierarchy topic models like hLDA (Blei et al., 2004) to extract relation topics for two reasons. First, DWT is parameter free while other models need some user-input parameters like hierarchy level. Second, DWT is more efficient than the other models. After the relation correlation matrix is constructed, DWT only needs a couple of minutes to extract multiscale topics on a regular computer. A direct experimental comparison between DWT and hLDA can be found in (Wang and Mahadevan, 2009). lim j→∞ 1431 4 Constructing Relation Detectors with Multiscale Relation Topics 4.1 Project Relation Instances onto Topics When we design detectors for new relati</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 2004. Hierarchical topic models and the nested Chinese restaurant process. In Proceedings of the Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2309" citStr="Bunescu and Mooney, 2005" startWordPosition="341" endWordPosition="344">rom text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has focused on how to create detectors from ground up with a</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>152--160</pages>
<contexts>
<context position="5673" citStr="Chan and Roth, 2010" startWordPosition="878" endWordPosition="881">s to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient. Our work fits in to a class of relation extraction research based on “distant supervision”, which studies how knowledge and resources external to the target domain can be used to improve relation extraction. (Mintz et al., 2009; Jiang, 2009; Chan and Roth, 2010). One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base. The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DBpedia (Auer et al., 20</context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Coifman</author>
<author>M Maggioni</author>
</authors>
<date>2006</date>
<booktitle>Diffusion wavelets. Applied and Computational Harmonic Analysis,</booktitle>
<pages>21--53</pages>
<contexts>
<context position="13198" citStr="Coifman and Maggioni, 2006" startWordPosition="2091" endWordPosition="2094">ions have a subclass relationship, e.g. “AcademyAward” and “Award”; others overlap in their scope and use, e.g., “Composer” and “Artist”; while some are equivalent, e.g., “DateOfBirth” and “BirthDate”. Secondly, a fairly large amount of the noisy labels are still in the training data. To reveal the intrinsic structure of the current DBpedia relation space and filter out noise, we carried out a correlation analysis of relations in the training data, resulting in a relation topic space. Each relation topic is a multinomial distribution over the existing relations. We adapted diffusion wavelets (Coifman and Maggioni, 2006) for this task. Compared to the other well-known topic extraction methods like LDA (Blei et al., 2003) and LSI (Deerwester et al., 1990), diffusion wavelets can efficiently extract a hierarchy of interpretable topics without any user input parameter (Wang and Mahadevan, 2009). 3.1 An Overview of Diffusion Wavelets The diffusion wavelets algorithm constructs a compressed representation of the dyadic powers of a square matrix by representing the associated matrices at each scale not in terms of the original (unit vector) basis, but rather using a set of custom generated bases (Coifman and Maggio</context>
<context position="17535" citStr="Coifman and Maggioni, 2006" startWordPosition="2844" endWordPosition="2847">nt relations offer us a novel way to model the correlations between different relations, and further allow us to create relation topics. The rules can also be simplified. For example, we may treat argument1, argument2, noun, preposition and verb separately. This results in simple rules that only involve in one argument type or word. The correlations between relations are then computed only based on one particular component like argument1, noun, etc. Theoretical Analysis Matrix S models the correlations between relations in the training data. Once S is constructed, we adapt diffusion wavelets (Coifman and Maggioni, 2006) to automatically extract the basis functions spanning the original column space of S at multiple scales. The key strength of the approach is that it is data-driven, largely parameter-free and can automatically determine the number of levels of the topical hierarchy, as well as the topics at each level. However, to apply diffusion wavelets to S, we first need to show that S is a positive semi-definite matrix. This property guarantees that all eigenvalues of S are &gt; 0. Depending on the way we formalize the rules, the methods to validate this property are slightly different. When we treat argume</context>
<context position="20168" citStr="Coifman and Maggioni, 2006" startWordPosition="3314" endWordPosition="3317">Amax(S) represent the largest eigenvalue of matrix S, then DWT(S/Amax(S), E) produces a set of nested subspaces of the column space of S, and the highest level of the resulting subspace hierarchy is spanned by one basis function. Proof: From Theorem 1, we know that S is a PSD matrix. This means Amax(S) E [0,+oo) (all eigenvalues of S are non-negative). This further implies that A(S)/ Amax(S) E [0, 1], where A(S) represents any eigenvalue of S. The idea underlying diffusion wavelets is based on decomposing the spectrum of an input matrix into various spectral bands, spanned by basis functions (Coifman and Maggioni, 2006). Let T = S/Amax(S). In Figure 2, we construct spectral bands of eigenvalues, whose associated eigenvectors span the corresponding subspaces. Define dyadic spatial scales tj as 2t = 2j+1 − 1, j &gt; 0 . At each spatial scale, the spectral band is defined as: Qj(T) = {A E A(T), Atj &gt; E}, � j t=0 tj = 1430 where A(T) represents any eigenvalue of T, and E E (0, 1) is a pre-defined threshold in Figure 2. We can now associate with each of the spectral bands a vector subspace spanned by the corresponding eigenvectors: Vj = (f�a : A E A(T), Atj &gt; E}), j &gt; 0 . In the limit, we obtain Vj = (f�a : A = 1}) </context>
</contexts>
<marker>Coifman, Maggioni, 2006</marker>
<rawString>R. Coifman and M. Maggioni. 2006. Diffusion wavelets. Applied and Computational Harmonic Analysis, 21:53–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="2254" citStr="Collins and Duffy, 2001" startWordPosition="332" endWordPosition="335">ve candidate answers. To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has f</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>423--429</pages>
<contexts>
<context position="2282" citStr="Culotta and Sorensen, 2004" startWordPosition="336" endWordPosition="340">extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has focused on how to create dete</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="13334" citStr="Deerwester et al., 1990" startWordPosition="2115" endWordPosition="2118">hile some are equivalent, e.g., “DateOfBirth” and “BirthDate”. Secondly, a fairly large amount of the noisy labels are still in the training data. To reveal the intrinsic structure of the current DBpedia relation space and filter out noise, we carried out a correlation analysis of relations in the training data, resulting in a relation topic space. Each relation topic is a multinomial distribution over the existing relations. We adapted diffusion wavelets (Coifman and Maggioni, 2006) for this task. Compared to the other well-known topic extraction methods like LDA (Blei et al., 2003) and LSI (Deerwester et al., 1990), diffusion wavelets can efficiently extract a hierarchy of interpretable topics without any user input parameter (Wang and Mahadevan, 2009). 3.1 An Overview of Diffusion Wavelets The diffusion wavelets algorithm constructs a compressed representation of the dyadic powers of a square matrix by representing the associated matrices at each scale not in terms of the original (unit vector) basis, but rather using a set of custom generated bases (Coifman and Maggioni, 2006). Figure 2 summarizes the procedure to generate diffusion wavelets. Given a matrix T, the QR (a modified QR decomposition) subr</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
</authors>
<title>AnaMaria Popescu, Tal Shaked,</title>
<location>Stephen Soderland,</location>
<marker>Etzioni, Cafarella, Downey, </marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>165--91</pages>
<marker>Weld, Yates, 2005</marker>
<rawString>Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165:91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9285" citStr="Fellbaum, 1998" startWordPosition="1452" endWordPosition="1453">s a YAGO type for the entity. For example, the DBpedia entry for the entity “Albert Einstein” includes YAGO types such as Scientist, Philosopher, Violinist etc. These YAGO types are also linked to appropriate WordNet concepts, providing for accurate sense disambiguation. Thus, for any entity argument of a relation we are learning, we obtain sense-disambiguated type information (including super-types, sub-types, siblings etc.), which become useful generalization features in the relation detection model. Given a common noun, we can also retrieve its type information by checking against WordNet (Fellbaum, 1998). 2.2 Extracting Rules from the Training Data We use a set of rules together with their popularities (occurrence count) to characterize a relation. A rule representing the relations between two arguments has five components (ordered): arguments type, argument2 type, noun, preposition and verb. A rule example of ActiveYearsEndDate relation (about the year that a person retired) is: person100007846|year115203791|-|in|retire. In this example, arguments type is person100007846, argument2 type is year115203791, both of which are from YAGO type system. The key words connecting these two arguments ar</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>286--295</pages>
<contexts>
<context position="7338" citStr="Hoffmann et al., 2010" startWordPosition="1133" endWordPosition="1136"> and ACE data (ACE, 2004) have confirmed that background-knowledge-based features generated from the Wikipedia relation repository can significantly improve the performance over the state-of-the-art relation detection approaches. 2 Extracting Relations from Wikipedia Our training data is from two parts: relation instances from DBpedia (extracted from Wikipedia infoboxes), and sentences describing the relations from the corresponding Wikipedia pages. 2.1 Collecting the Training Data Since our relations correspond to Wikipedia infobox properties, we use an approach similar to that described in (Hoffmann et al., 2010) to collect positive training data instances. We assume that a Wikipedia page containing a particular infobox property is likely to express the same relation in the text of the page. We further assume that the relation is most likely expressed in the first sentence on the page which mentions the arguments of the relation. For example, the Wikipedia page for “Albert Einstein” contains an infobox property “alma mater” with value “University of Zurich”, and the first sentence mentioning the arguments is the following: “Einstein was awarded a PhD by the University of Zurich”, which expresses the r</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="33872" citStr="Jiang and Zhai, 2007" startWordPosition="5656" endWordPosition="5659">c structures into consideration. They pare our approaches against the state-of-the-art ap- used several types of syntactic information includproaches. This dataset includes 348 documents and ing constituent and dependency syntactic parse trees around 4400 relation instances. 7 relation types, to improve the state of the art approaches to 71.5% 7 entity types, numerous relation sub-types, entity on F-measure. Heuristic rules extracted from the sub-types, and mention types are defined on this target data can also help improve the performance. set. The task is to classify the relation instances (Jiang and Zhai, 2007) reported that by taking sevinto one of the 7 relation types or “NONE”, which eral heuristic rules they can improve the F-measure means there is no relation. For comparison, we use of Composite Kernel to 70.4%. They also showed the same setting as (Zhang et al., 2006), by apply- that using maximum entropy classifier rather than ing a 5-fold cross-validation. The scores reported SVM achieved the best performance on this task: here are the average of all 5 folds. This is also how 72.9% F-measure. To the best of our knowledge, the the other approaches are evaluated. In this test, we most recent r</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and Chengxiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
</authors>
<title>Multi-task transfer learning for weakly-supervised relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="5651" citStr="Jiang, 2009" startWordPosition="876" endWordPosition="877">he new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient. Our work fits in to a class of relation extraction research based on “distant supervision”, which studies how knowledge and resources external to the target domain can be used to improve relation extraction. (Mintz et al., 2009; Jiang, 2009; Chan and Roth, 2010). One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base. The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DB</context>
</contexts>
<marker>Jiang, 2009</marker>
<rawString>Jing Jiang. 2009. Multi-task transfer learning for weakly-supervised relation extraction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP), pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27332" citStr="Joachims, 1999" startWordPosition="4574" endWordPosition="4575">level k represented as an M x pk matrix, k = 1, · · · , h represents the level in the topic hierarchy. The value of pk is determined in DWT () based on the intrinsic structure of the given dataset. Columns of [Ok]oo are used as relation topics at level k. 3. Construct relation detectors for new relations. Given the training data from a new relation, project the data onto level k of the multiscale topic hierarchy, where k is chosen by users (Section 4.1). Apply SVM classifiers together with our kernel (Section 4.2) to create detectors for new relations. 5 Experimental Results We used SVMLight (Joachims, 1999) together with the user defined kernel setting in our approach. The trade-off parameter between training error and margin c is 1 for all experiments. Our approach to learn multiscale relation topics is largely parameter free. The only parameter to be set is the precision ε = 10−5, which is also the default value in the diffusion wavelets implementation. 5.1 Learning Multiscale Relation Topics Following the approach discussed in Section 2.1, we collect more than 620,000 training instances for arg1 arg 2 noun ver ([0k]00)TXt, and [�k]00 is defined in Figure 2. 1432 Table 1: Number of topics at d</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making Large-Scale SVM Learning Practical. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="1883" citStr="Kambhatla, 2004" startWordPosition="276" endWordPosition="277">dge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches. 1 Introduction Detecting semantic relations in text is very useful in both information retrieval and question answering because it enables knowledge bases to be leveraged to score passages and retrieve candidate answers. To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernel</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="31486" citStr="Lin and Pantel, 2001" startWordPosition="5266" endWordPosition="5269">compare different relation detection approaches. In this experiment, 100 instances from each relation are used for training, and the other 100 are for testing. In training, we try three different settings: n = 5, 20 and 100, where n is the size of the training set for each relation. When we train a model for one relation, we use the training positive instances from the other 99 relations as training negatives. For example, we use 5 training positive instances and 5*99=495 training negatives to train a detector for each relation. We compare our approach against the regular rule-based approach (Lin and Pantel, 2001) and two other kernel-based approaches (presented in Section 4.2) for relation detection task. The comparison results are summarized in Table 4. The approach using relation topics (level 2) consistently outperforms the other three approaches in all three settings. When n = 5, it achieves the largest improvement over the other three. This indicates that using relation topics that integrate the knowledge extracted from the existing relations, can significantly benefit us when the training data is insufficient. This is reasonable, since the prior knowledge becomes more valuable in this scenario. </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="10621" citStr="McCord, 1995" startWordPosition="1656" endWordPosition="1657">e relation can be represented in many different ways. Another rule example characterizing the same relation is person100007846|year115203791|retirement|-|announce. This paper only considers three types of words: noun, verb and preposition. It is straightforward to expand or simplify the rules by including more or removing some word types. The keywords are extracted from the shortest path on the dependency Figure 1: A dependency tree example. tree between the two arguments. A dependency tree (Figure 1) represents grammatical relations between words in a sentence. We used a slot grammar parser (McCord, 1995) to generate the parse tree of each sentence. Note that there could be multiple paths between two arguments in the tree. We only take the shortest path into consideration. The popularity value corresponding to each rule represents how many times this rule applies to the given relation in the given data. Multiple rules can be constructed from one relation instance, if multiple argument types are associated with the instance, or multiple nouns, prepositions or verbs are in the dependency path. 2.3 Cleaning the Training Data To find a sentence on the Wikipedia page that is likely to express a rel</context>
</contexts>
<marker>McCord, 1995</marker>
<rawString>Michael McCord. 1995. Slot grammar: A system for simpler construction of practical natural language grammars. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference.</booktitle>
<contexts>
<context position="1779" citStr="Miller et al., 2000" startWordPosition="259" endWordPosition="262">rs for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches. 1 Introduction Detecting semantic relations in text is very useful in both information retrieval and question answering because it enables knowledge bases to be leveraged to score passages and retrieve candidate answers. To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to </context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="5638" citStr="Mintz et al., 2009" startWordPosition="872" endWordPosition="875">ation instances in the new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient. Our work fits in to a class of relation extraction research based on “distant supervision”, which studies how knowledge and resources external to the target domain can be used to improve relation extraction. (Mintz et al., 2009; Jiang, 2009; Chan and Roth, 2010). One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base. The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia,</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="33069" citStr="Nguyen et al., 2009" startWordPosition="5531" endWordPosition="5534"> posite kernel result in Table 5 is based on a linear basis functions is a natural choice, since the levels combination of the Argument kernel and Convolubelow and above this have too many or too few fea- tion Tree kernel. (Zhang et al., 2006) showed that tures, respectively. A user can also select the most by carefully choosing the weight of each compoappropriate level by checking if the related relation nent and using a polynomial expansion, they could topics are meaningful for their applications. achieve the best performance on this data: 72.1% F5.3 Relation Detection on ACE Data measure. (Nguyen et al., 2009) further showed that In this experiment, we use the news domain docu- the performance can be improved by taking syntacments of the ACE 2004 corpus (ACE, 2004) to com- tic and semantic structures into consideration. They pare our approaches against the state-of-the-art ap- used several types of syntactic information includproaches. This dataset includes 348 documents and ing constituent and dependency syntactic parse trees around 4400 relation instances. 7 relation types, to improve the state of the art approaches to 71.5% 7 entity types, numerous relation sub-types, entity on F-measure. Heuris</context>
<context position="35492" citStr="Nguyen et al., 2009" startWordPosition="5917" endWordPosition="5920">the classifiers using ACE relation sub-types (rather ment from each individual kernel. We also com- than on types), they achieved an impressive 75.8% pare our approaches to the other state-of-the-art ap- F-measure. However, as pointed out in (Nguyen et proaches including Convolution Tree kernel (Collins al., 2009), such heuristics are tuned on the target reand Duffy, 2001), Syntactic kernel (Zhao and Grish- lation extraction task and might not be appropriate to man, 2005), Composite kernel (linear) (Zhang et al., compare against the automatic learning approaches. 2006) and the best kernel in (Nguyen et al., 2009). Even though we have not done any domain specific Our approach with relation topics at level 2 has the parameter tuning or applied any heuristics, our apbest performance, achieving a 73.24% F-measure. proach still achieve significant improvements over The impact of the relation topics is huge. They im- all approaches mentioned above except one, which prove the F-measure from 61.15% to 73.24%. We is based on heuristics extracted from the target doalso test our approach using the topics at level 3. main. This also implies that by combining some of The performance is slightly worse than using le</context>
<context position="37420" citStr="Nguyen, et al. (2009)" startWordPosition="6231" endWordPosition="6234">es over 100 DBpedia relations with 5, 20 and 100 positive examples per relation. AG: KArgument, DP: KPath, BOW: KBOW, TFk: KTFk. Approaches 100 20 5 Rule Based 37.70% 27.45% 13.20% AG+ DP 73.64% 51.85% 22.95% AG+ DP+ BOW 78.74% 62.76% 31.98% AG+ DP+ BOW+ TF2 81.18% 68.03% 41.60% Table 5: Performance comparison of different approaches with SVM over the ACE 2004 data. P: Precision, R: Recall, F: F-measure, AG: KArgument, DP: KPath, BOW: KBOW, TFk: KTFk. Approaches P(%) R(%) F(%) Convolution Tree Kernel 72.5 56.7 63.6 Composite Kernel (linear) 73.50 67.00 70.10 Syntactic Kernel 69.23 70.50 69.86 Nguyen, et al. (2009) 76.60 67.00 71.50 AG 59.56 46.22 52.02 AG + DP 64.44 54.93 59.28 AG + DP + BOW 62.00 61.19 61.15 AG + DP + BOW + TF3 69.63 76.51 72.90 AG + DP + BOW + TF2 69.15 77.88 73.24 vide an automatic way to collect training data for more than 7,000 relations from Wikipedia and DBpedia. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. Different from the topics defined over words, relation topics are defined over the existing relations. Thirdly, we design a new kernel for relation detection by integrating the relation topics in the representation of the </context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2002</date>
<publisher>MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>B. Sch¨olkopf and A. J. Smola. 2002. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A large ontology from Wikipedia and WordNet. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2007</date>
<volume>6</volume>
<issue>3</issue>
<pages>217</pages>
<contexts>
<context position="8418" citStr="Suchanek et al., 2007" startWordPosition="1314" endWordPosition="1317"> the first sentence mentioning the arguments is the following: “Einstein was awarded a PhD by the University of Zurich”, which expresses the relation. When looking for relation arguments on the page, we go beyond (sub)string matching, and use link information to match entities which may have different surface forms. Using this technique, we are able to collect a large amount of positive training instances of DBpe1427 dia relations. To get precise type information for the arguments of a DBpedia relation, we use the DBpedia knowledge base (Auer et al., 2007) and the associated YAGO type system (Suchanek et al., 2007). Note that for every Wikipedia page, there is a corresponding DBpedia entry which has captured the infobox-properties as RDF triples. Some of the triples include type information, where the subject of the triple is a Wikipedia entity, and the object is a YAGO type for the entity. For example, the DBpedia entry for the entity “Albert Einstein” includes YAGO types such as Scientist, Philosopher, Violinist etc. These YAGO types are also linked to appropriate WordNet concepts, providing for accurate sense disambiguation. Thus, for any entity argument of a relation we are learning, we obtain sense</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A large ontology from Wikipedia and WordNet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203– 217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>S Mahadevan</author>
</authors>
<title>Multiscale analysis of document corpora based on diffusion models.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1592--1597</pages>
<contexts>
<context position="13474" citStr="Wang and Mahadevan, 2009" startWordPosition="2134" endWordPosition="2138">g data. To reveal the intrinsic structure of the current DBpedia relation space and filter out noise, we carried out a correlation analysis of relations in the training data, resulting in a relation topic space. Each relation topic is a multinomial distribution over the existing relations. We adapted diffusion wavelets (Coifman and Maggioni, 2006) for this task. Compared to the other well-known topic extraction methods like LDA (Blei et al., 2003) and LSI (Deerwester et al., 1990), diffusion wavelets can efficiently extract a hierarchy of interpretable topics without any user input parameter (Wang and Mahadevan, 2009). 3.1 An Overview of Diffusion Wavelets The diffusion wavelets algorithm constructs a compressed representation of the dyadic powers of a square matrix by representing the associated matrices at each scale not in terms of the original (unit vector) basis, but rather using a set of custom generated bases (Coifman and Maggioni, 2006). Figure 2 summarizes the procedure to generate diffusion wavelets. Given a matrix T, the QR (a modified QR decomposition) subroutine decomposes T into an orthogonal matrix Q and a triangular matrix R such that T Pz� QR, where |Ti,k − (QR)i,k |&lt; e for any i and k. Co</context>
<context position="23823" citStr="Wang and Mahadevan, 2009" startWordPosition="3932" endWordPosition="3936">arned relations, and the topic space is enriched as we learn more and more relations. We use diffusion wavelets (DWT) rather than other hierarchy topic models like hLDA (Blei et al., 2004) to extract relation topics for two reasons. First, DWT is parameter free while other models need some user-input parameters like hierarchy level. Second, DWT is more efficient than the other models. After the relation correlation matrix is constructed, DWT only needs a couple of minutes to extract multiscale topics on a regular computer. A direct experimental comparison between DWT and hLDA can be found in (Wang and Mahadevan, 2009). lim j→∞ 1431 4 Constructing Relation Detectors with Multiscale Relation Topics 4.1 Project Relation Instances onto Topics When we design detectors for new relations, we treat arg1, arg2, noun, and verb separately to get stronger correlations between relations. We do not directly use preposition. Any DBpedia relation r  {1, · · · , M} is represented with 4 vectors rt = [rt(1), · · · , rt(Nt)], where t  {arg1, arg2, noun, verb}, Nt represents the size of the vocabulary set of the type t component in the Wikipedia training data, and rt(j) represents the occurrence count of type t component in</context>
</contexts>
<marker>Wang, Mahadevan, 2009</marker>
<rawString>C. Wang and S. Mahadevan. 2009. Multiscale analysis of document corpora based on diffusion models. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1592–1597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2011</date>
<note>http://www.wikipedia.org/.</note>
<contexts>
<context position="6244" citStr="Wikipedia, 2011" startWordPosition="969" endWordPosition="970"> al., 2009; Jiang, 2009; Chan and Roth, 2010). One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base. The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DBpedia (Auer et al., 2007). A key part of this step is how we handle noisy data with little human effort. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. This step is parameter free, and results in a nonredundant, multiscale relation topic space. Thirdly, we design a new kernel for relation detection by integrating the relation topics into the relation detector construction. The experimental results on Wikipedia and ACE data (ACE, 2004) have confirmed that background-knowledge-based features generated from the Wikipedia relation reposito</context>
</contexts>
<marker>Wikipedia, 2011</marker>
<rawString>Wikipedia. 2011. http://www.wikipedia.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2506" citStr="Zhang et al., 2006" startWordPosition="371" endWordPosition="374">ao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has focused on how to create detectors from ground up with as little training data as possible through techniques such as bootstrapping (Etzioni et al., 2005). We take a different approach, focusing on how the knowledge extracted from the existing relations</context>
<context position="32692" citStr="Zhang et al., 2006" startWordPosition="5470" endWordPosition="5473">is scenario. 1433 The users can select the level that is the most ap- heuristic rules were applied. We are aware of some propriate for their applications. In this example, we methods that could stack on our approach to further only have alignment results at 7 levels. Choosing the improve the performance on ACE test. The Comspace at level 2 spanned by a couple of hundreds of posite kernel result in Table 5 is based on a linear basis functions is a natural choice, since the levels combination of the Argument kernel and Convolubelow and above this have too many or too few fea- tion Tree kernel. (Zhang et al., 2006) showed that tures, respectively. A user can also select the most by carefully choosing the weight of each compoappropriate level by checking if the related relation nent and using a polynomial expansion, they could topics are meaningful for their applications. achieve the best performance on this data: 72.1% F5.3 Relation Detection on ACE Data measure. (Nguyen et al., 2009) further showed that In this experiment, we use the news domain docu- the performance can be improved by taking syntacments of the ACE 2004 corpus (ACE, 2004) to com- tic and semantic structures into consideration. They par</context>
<context position="34140" citStr="Zhang et al., 2006" startWordPosition="5704" endWordPosition="5707">nces. 7 relation types, to improve the state of the art approaches to 71.5% 7 entity types, numerous relation sub-types, entity on F-measure. Heuristic rules extracted from the sub-types, and mention types are defined on this target data can also help improve the performance. set. The task is to classify the relation instances (Jiang and Zhai, 2007) reported that by taking sevinto one of the 7 relation types or “NONE”, which eral heuristic rules they can improve the F-measure means there is no relation. For comparison, we use of Composite Kernel to 70.4%. They also showed the same setting as (Zhang et al., 2006), by apply- that using maximum entropy classifier rather than ing a 5-fold cross-validation. The scores reported SVM achieved the best performance on this task: here are the average of all 5 folds. This is also how 72.9% F-measure. To the best of our knowledge, the the other approaches are evaluated. In this test, we most recent result was reported by (Zhou and Zhu, treat entity types, entity sub-types and mention types 2011), who extended their previous work in (Zhou equally as argument types. Table 5 summarizes et al., 2007). By using several heuristics to define the performance after applyi</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>419--426</pages>
<contexts>
<context position="1909" citStr="Zhao and Grishman, 2005" startWordPosition="278" endWordPosition="282">generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches. 1 Introduction Detecting semantic relations in text is very useful in both information retrieval and question answering because it enables knowledge bases to be leveraged to score passages and retrieve candidate answers. To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005). Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) ac</context>
<context position="25418" citStr="Zhao and Grishman, 2005" startWordPosition="4230" endWordPosition="4233">ion result of xt onto the DBpedia relation space Xt is as follows: Xt = [&lt; rt(1), xt(1) &gt;, · · · , &lt; rt(M), xt(M) &gt;], where &lt; ·, · &gt; is the cosine similarity of two vectors. At level k, the embedding of x is Ekx = [EkX , EX , EkX , EkX b], where Ek = 4.2 Design New Kernel Using Topic Features We combine Exk with 3 existing kernels (KArgument, KPath and KBOW) to create a new kernel for relation detection. (1) KArgument matches two arguments, it returns the number of common argument types that the input arguments share. (2) KPath matches two dependency paths. This kernel is formally defined in (Zhao and Grishman, 2005). We extended this kernel by also matching the common nouns, prepositions and verbs in the dependency paths. We assign weight 1 to verbs, 0.5 to nouns and prepositions. (3) KBOW models the number of common nouns, prepositions and verbs in the given sentences but not in the dependency paths. Since these words are not as important as the words inside the dependency path, we assign weight 0.25 to them. (4) KTFk(x, y) =&lt; Exk, Eky &gt;, where x, y are two input relation instances, and &lt; ·, · &gt; models the cosine similarity of two vectors. TF stands for topic feature. (5) The final kernel used in this p</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>Q Zhu</author>
</authors>
<title>Kernel-based semantic relation detection and classification via enriched parse tree structure.</title>
<date>2011</date>
<journal>Journal of Computer Science and Technology,</journal>
<pages>26--45</pages>
<marker>Zhou, Zhu, 2011</marker>
<rawString>G. Zhou and Q. Zhu. 2011. Kernel-based semantic relation detection and classification via enriched parse tree structure. Journal of Computer Science and Technology, 26:45–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>M Zhang</author>
<author>D Ji</author>
<author>Q Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP).</booktitle>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>G. Zhou, M. Zhang, D. Ji, and Q. Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>