<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992825">
Discovering Sociolinguistic Associations with Structured Sparsity
</title>
<author confidence="0.998717">
Jacob Eisenstein Noah A. Smith Eric P. Xing
</author>
<affiliation confidence="0.879489333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998894">
{jacobeis,nasmith,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993365">
We present a method to discover robust and
interpretable sociolinguistic associations from
raw geotagged text data. Using aggregate de-
mographic statistics about the authors’ geo-
graphic communities, we solve a multi-output
regression problem between demographics
and lexical frequencies. By imposing a com-
posite Bl regularizer, we obtain structured
sparsity, driving entire rows of coefficients
to zero. We perform two regression studies.
First, we use term frequencies to predict de-
mographic attributes; our method identifies a
compact set of words that are strongly asso-
ciated with author demographics. Next, we
conjoin demographic attributes into features,
which we use to predict term frequencies. The
composite regularizer identifies a small num-
ber of features, which correspond to com-
munities of authors united by shared demo-
graphic and linguistic properties.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961326086957">
How is language influenced by the speaker’s so-
ciocultural identity? Quantitative sociolinguistics
usually addresses this question through carefully
crafted studies that correlate individual demographic
attributes and linguistic variables—for example, the
interaction between income and the “dropped r” fea-
ture of the New York accent (Labov, 1966). But
such studies require the knowledge to select the
“dropped r” and the speaker’s income, from thou-
sands of other possibilities. In this paper, we present
a method to acquire such patterns from raw data. Us-
ing multi-output regression with structured sparsity,
our method identifies a small subset of lexical items
that are most influenced by demographics, and dis-
covers conjunctions of demographic attributes that
are especially salient for lexical variation.
Sociolinguistic associations are difficult to model,
because the space of potentially relevant interactions
is large and complex. On the linguistic side there
are thousands of possible variables, even if we limit
ourselves to unigram lexical features. On the demo-
graphic side, the interaction between demographic
attributes is often non-linear: for example, gender
may negate or amplify class-based language differ-
ences (Zhang, 2005). Thus, additive models which
assume that each demographic attribute makes a lin-
ear contribution are inadequate.
In this paper, we explore the large space of po-
tential sociolinguistic associations using structured
sparsity. We treat the relationship between language
and demographics as a set of multi-input, multi-
output regression problems. The regression coeffi-
cients are arranged in a matrix, with rows indicating
predictors and columns indicating outputs. We ap-
ply a composite regularizer that drives entire rows
of the coefficient matrix to zero, yielding compact,
interpretable models that reuse features across dif-
ferent outputs. If we treat the lexical frequencies
as inputs and the author’s demographics as outputs,
the induced sparsity pattern reveals the set of lexi-
cal items that is most closely tied to demographics.
If we treat the demographic attributes as inputs and
build a model to predict the text, we can incremen-
tally construct a conjunctive feature space of demo-
graphic attributes, capturing key non-linear interac-
tions.
</bodyText>
<page confidence="0.934011">
1365
</page>
<note confidence="0.979068">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1365–1374,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998655625">
The primary purpose of this research is ex-
ploratory data analysis to identify both the most
linguistic-salient demographic features, and the
most demographically-salient words. However, this
model also enables predictions about demographic
features by analyzing raw text, potentially support-
ing applications in targeted information extraction
or advertising. On the task of predicting demo-
graphics from text, we find that our sparse model
yields performance that is statistically indistinguish-
able from the full vocabulary, even with a reduction
in the model complexity an order of magnitude. On
the task of predicting text from author demograph-
ics, we find that our incrementally constructed fea-
ture set obtains significantly better perplexity than a
linear model of demographic attributes.
</bodyText>
<sectionHeader confidence="0.996342" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999885785714286">
Our dataset is derived from prior work in which
we gathered the text and geographical locations of
9,250 microbloggers on the website twitter.
com (Eisenstein et al., 2010). Bloggers were se-
lected from a pool of frequent posters whose mes-
sages include metadata indicating a geographical lo-
cation within a bounding box around the continen-
tal United States. We limit the vocabulary to the
5,418 terms which are used by at least 40 authors; no
stoplists are applied, as the use of standard or non-
standard orthography for stopwords (e.g., to vs. 2)
may convey important information about the author.
The dataset includes messages during the first week
of March 2010.
O’Connor et al. (2010) obtained aggregate demo-
graphic statistics for these data by mapping geoloca-
tions to publicly-available data from the U. S. Cen-
sus ZIP Code Tabulation Areas (ZCTA).1 There
are 33,178 such areas in the USA (the 9,250 mi-
crobloggers in our dataset occupy 3,458 unique ZC-
TAs), and they are designed to contain roughly
equal numbers of inhabitants and demographically-
homogeneous populations. The demographic at-
tributes that we consider in this paper are shown
in Table 1. All attributes are based on self-reports.
The race and ethnicity attributes are not mutually
exclusive—individuals can indicate any number of
races or ethnicities. The “other language” attribute
</bodyText>
<footnote confidence="0.9938655">
1http://www.census.gov/support/cen2000.
html
</footnote>
<table confidence="0.997695428571429">
mean std. dev.
race &amp; ethnicity
% white 52.1 29.0
% African American 32.2 29.1
% Hispanic 15.7 18.3
language
% English speakers 73.7 18.4
% Spanish speakers 14.6 15.6
% other language speakers 11.7 9.2
socioeconomic
% urban 95.1 14.3
% with family 64.1 14.4
% renters 48.9 23.4
median income ($) 42,500 18,100
</table>
<tableCaption confidence="0.999971">
Table 1: The demographic attributes used in this research.
</tableCaption>
<bodyText confidence="0.999585592592593">
aggregates all languages besides English and Span-
ish. “Urban areas” refer to sets of census tracts or
census blocks which contain at least 2,500 residents;
our “% urban” attribute is the percentage of individ-
uals in each ZCTA who are listed as living in an ur-
ban area. We also consider the percentage of indi-
viduals who live with their families, the percentage
who live in rented housing, and the median reported
income in each ZCTA.
While geographical aggregate statistics are fre-
quently used to proxy for individual socioeconomic
status in research areas such as public health (e.g.,
Rushton, 2008), it is clear that interpretation must
proceed with caution. Consider an author from a ZIP
code in which 60% of the residents are Hispanic:2
we do not know the likelihood that the author is His-
panic, because the set of Twitter users is not a rep-
resentative sample of the overall population. Polling
research suggests that users of both Twitter (Smith
and Rainie, 2010) and geolocation services (Zick-
uhr and Smith, 2010) are much more diverse with
respect to age, gender, race and ethnicity than the
general population of Internet users. Nonetheless,
at present we can only use aggregate statistics to
make inferences about the geographic communities
in which our authors live, and not the authors them-
selves.
</bodyText>
<footnote confidence="0.954645">
2In the U.S. Census, the official ethnonym is Hispanic or
Latino; for brevity we will use Hispanic in the rest of this paper.
</footnote>
<page confidence="0.995712">
1366
</page>
<sectionHeader confidence="0.995418" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9999225">
The selection of both words and demographic fea-
tures can be framed in terms of multi-output regres-
sion with structured sparsity. To select the lexical
indicators that best predict demographics, we con-
struct a regression problem in which term frequen-
cies are the predictors and demographic attributes
are the outputs; to select the demographic features
that predict word use, this arrangement is reversed.
Through structured sparsity, we learn models in
which entire sets of coefficients are driven to zero;
this tells us which words and demographic features
can safely be ignored.
This section describes the model and implemen-
tation for output-regression with structured sparsity;
in Section 4 and 5 we give the details of its applica-
tion to select terms and demographic features. For-
mally, we consider the linear equation Y = XB+c,
where,
</bodyText>
<listItem confidence="0.969123">
1367 correlations across both the vocabulary and the de-
• Y is the dependent variable matrix, with di-
mensions N x T, where N is the number of
samples and T is the number of output dimen-
sions (or tasks);
• X is the independent variable matrix, with di-
mensions N x P, where P is the number of
input dimensions (or predictors);
• B is the matrix of regression coefficients, with
dimensions P x T;
• c is a N x T matrix in which each element is
noise from a zero-mean Gaussian distribution.
</listItem>
<bodyText confidence="0.94571775">
We would like to solve the unconstrained opti-
mization problem,
EminimizeB ||Y − XB||2 F + AR(B), (1)
where ||A||2 F indicates the squared Frobenius norm
Ej a2 ij, and the function R(B) defines a norm
i
on the regression coefficients B. Ridge regres-
sion applies the E2 norm R(B) = Et1 EP b2
</bodyText>
<equation confidence="0.960738666666667">
pp
,
t
</equation>
<bodyText confidence="0.815917">
and lasso regression applies the
</bodyText>
<equation confidence="0.842598">
norm R(B)
EP
bpt
; in both cases, it is possible to de-
compose the multi-output regression problem, treat-
ing each output dimension separately. However, our
working hypothesis is that there will be substan
E1
=
1
|
|
tial
</equation>
<bodyText confidence="0.977898">
mographic features—for example, a demographic
feature such as the percentage of Spanish speakers
will predict a large set of words. Our goal is to select
a small set of predictors yielding good performance
across all output dimensions. Thus, we desire struc-
tured sparsity, in which entire rows of the coefficient
matrix B are driven to zero.
Structured sparsity is not achieved by the
norm. The lasso gives element-wise sparsity, in
which many entries of B are driven to zero, but each
predictor may have anon-zero value for some output
dimension. To drive entire rows of B to zero, we re-
quire acomposite regularizer. We consider the
norm, which is the sum of E� norms across output
dimensions: R(B)
(Turlach et al.,
2005). This norm, which corresponds to a multi-
output lasso regression, has the desired property of
dri
</bodyText>
<equation confidence="0.939548333333333">
lasso’sE1
E1,�
=ETtmaxpbpt
</equation>
<bodyText confidence="0.6329535">
ving entire rows of B to zero.
Et
</bodyText>
<subsectionHeader confidence="0.993958">
3.1 Optimization
</subsectionHeader>
<bodyText confidence="0.98410432">
There are several techniques for solving the
normalized regression, including interior point
methods (Turlach et al., 2005) and projected gradi-
ent (Duchi et al., 2008; Quattoni et al., 2009). We
choose the blockwise coordinate descent approach
of Liu et al. (2009) because it is easy to implement
and efficient: the time complexity of each iteration
is independent of the number of samples.3
Due to space limitations, we defer to Liu et al.
(2009) for a complete description of the algorithm.
However, we note two aspects of our implementa-
tion which are important for natural language pro-
cessing applications. The
efficiency is
accomplished by precomputing the matrices C =
XTY and D = XTX, where X and Y are the stan-
dardized versions of X and Y, obtained by subtract-
ing the mean and scaling by the variance. Explicit
mean correction would destroy the sparse term fre-
quency data representation and render us unable to
store the data in memory; however, we can achieve
the same effect by computing C =XTY
x
y
where x and y are row vectors indicating the mean
</bodyText>
<equation confidence="0.993810272727273">
E1,�
algorithm’s
−N
T
,
s
implementation is available at
3Our
http://sailing.cs.cmu.edu/sociolinguistic.html.
of X and Y respectively.4 We can similarly compute
D = XTX − N xTx.
</equation>
<bodyText confidence="0.9470808">
If the number of predictors is too large, it may
not be possible to store the dense matrix D in mem-
ory. We have found that approximation based on the
truncated singular value decomposition provides an
effective trade-off of time for space. Specifically, we
compute XTX �
USVT (USVT \ 1T = U (SVTVSTUT \ 1 = UM.
Lower truncation levels are less accurate, but are
faster and require less space: for K singular val-
ues, the storage cost is O(KP), instead of O(P2);
the time cost increases by a factor of K. This ap-
proximation was not necessary in the experiments
presented here, although we have found that it per-
forms well as long as the regularizer is not too close
to zero.
</bodyText>
<subsectionHeader confidence="0.997975">
3.2 Regularization
</subsectionHeader>
<bodyText confidence="0.999993052631579">
The regularization constant A can be computed us-
ing cross-validation. As A increases, we reuse the
previous solution of B for initialization; this “warm
start” trick can greatly accelerate the computation
of the overall regularization path (Friedman et al.,
2010). At each Ai, we solve the sparse multi-output
regression; the solution Bi defines a sparse set of
predictors for all tasks.
We then use this limited set of predictors to con-
struct a new input matrix Xi, which serves as the
input in a standard ridge regression, thus refitting
the model. The tuning set performance of this re-
gression is the score for Ai. Such post hoc refitting
is often used in tandem with the lasso and related
sparse methods; the effectiveness of this procedure
has been demonstrated in both theory (Wasserman
and Roeder, 2009) and practice (Wu et al., 2010).
The regularization parameter of the ridge regression
is determined by internal cross-validation.
</bodyText>
<sectionHeader confidence="0.914581" genericHeader="method">
4 Predicting Demographics from Text
</sectionHeader>
<bodyText confidence="0.959019333333333">
Sparse multi-output regression can be used to select
a subset of vocabulary items that are especially in-
dicative of demographic and geographic differences.
4Assume without loss of generality that X and Y are scaled
to have variance 1, because this scaling does not affect the spar-
sity pattern.
Starting from the regression problem (1), the predic-
tors X are set to the term frequencies, with one col-
umn for each word type and one row for each author
in the dataset. The outputs Y are set to the ten demo-
graphic attributes described in Table 1 (we consider
much larger demographic feature spaces in the next
section) The E1, regularizer will drive entire rows
of the coefficient matrix B to zero, eliminating all
demographic effects for many words.
</bodyText>
<subsectionHeader confidence="0.997851">
4.1 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.994876081081081">
We evaluate the ability of lexical features to predict
the demographic attributes of their authors (as prox-
ied by the census data from the author’s geograph-
ical area). The purpose of this evaluation is to as-
sess the predictive ability of the compact subset of
lexical items identified by the multi-output lasso, as
compared with the full vocabulary. In addition, this
evaluation establishes a baseline for performance on
the demographic prediction task.
We perform five-fold cross-validation, using the
multi-output lasso to identify a sparse feature set
in the training data. We compare against several
other dimensionality reduction techniques, match-
ing the number of features obtained by the multi-
output lasso at each fold. First, we compare against
a truncated singular value decomposition, with the
truncation level set to the number of terms selected
by the multi-output lasso; this is similar in spirit to
vector-based lexical semantic techniques (Sch¨utze
and Pedersen, 1993). We also compare against sim-
ply selecting the N most frequent terms, and the N
terms with the greatest variance in frequency across
authors. Finally, we compare against the complete
set of all 5,418 terms. As before, we perform post
hoc refitting on the training data using a standard
ridge regression. The regularization constant for the
ridge regression is identified using nested five-fold
cross validation within the training set.
We evaluate on the refit models on the heldout
test folds. The scoring metric is Pearson’s correla-
tion coefficient between the predicted and true de-
mographics: p(y, y) = cov(y,�y)
σyσ�y , with cov(y, y) in-
dicating the covariance and Qy indicating the stan-
dard deviation. On this metric, a perfect predictor
will score 1 and a random predictor will score 0. We
report the average correlation across all ten demo-
</bodyText>
<page confidence="0.958736">
1368
</page>
<figure confidence="0.662833">
number of features
</figure>
<figureCaption confidence="0.9986475">
Figure 1: Average correlation plotted against the number
of active features (on a logarithmic scale).
</figureCaption>
<bodyText confidence="0.999651622222222">
graphic attributes, as well as the individual correla-
tions.
Results Table 2 shows the correlations obtained
by regressions performed on a range of different vo-
cabularies, averaged across all five folds. Linguistic
features are best at predicting race, ethnicity, lan-
guage, and the proportion of renters; the other de-
mographic attributes are more difficult to predict.
Among feature sets, the highest average correlation
is obtained by the full vocabulary, but the multi-
output lasso obtains nearly identical performance
using a feature set that is an order of magnitude
smaller. Applying the Fischer transformation, we
find that all correlations are statistically significant
at p &lt; .001.
The Fischer transformation can also be used to
estimate 95% confidence intervals around the cor-
relations. The extent of the confidence intervals
varies slightly across attributes, but all are tighter
than ±0.02. We find that the multi-output lasso and
the full vocabulary regression are not significantly
different on any of the attributes. Thus, the multi-
output lasso achieves a 93% compression of the fea-
ture set without a significant decrease in predictive
performance. The multi-output lasso yields higher
correlations than the other dimensionality reduction
techniques on all of the attributes; these differences
are statistically significant in many—but not all—
cases. The correlations for each attribute are clearly
not independent, so we do not compare the average
across attributes.
Recall that the regularization coefficient was cho-
sen by nested cross-validation within the training
set; the average number of features selected is
394.6. Figure 1 shows the performance of each
dimensionality-reduction technique across the reg-
ularization path for the first of five cross-validation
folds. Computing the truncated SVD of a sparse ma-
trix at very large truncation levels is computationally
expensive, so we cannot draw the complete perfor-
mance curve for this method. The multi-output lasso
dominates the alternatives, obtaining a particularly
strong advantage with very small feature sets. This
demonstrates its utility for identifying interpretable
models which permit qualitative analysis.
</bodyText>
<subsectionHeader confidence="0.98478">
4.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.99974753125">
For a qualitative analysis, we retrain the model on
the full dataset, and tune the regularization to iden-
tify a compact set of 69 features. For each identified
term, we apply a significance test on the relationship
between the presence of each term and the demo-
graphic indicators shown in the columns of the ta-
ble. Specifically, we apply the Wald test for compar-
ing the means of independent samples, while mak-
ing the Bonferroni correction for multiple compar-
isons (Wasserman, 2003). The use of sparse multi-
output regression for variable selection increases the
power of post hoc significance testing, because the
Bonferroni correction bases the threshold for sta-
tistical significance on the total number of compar-
isons. We find 275 associations at the p &lt; .05 level;
at the higher threshold required by a Bonferroni cor-
rection for comparisons among all terms in the vo-
cabulary, 69 of these associations would have been
missed.
Table 3 shows the terms identified by our model
which have a significant correlation with at least one
of the demographic indicators. We divide words in
the list into categories, which order alphabetically
by the first word in each category: emoticons; stan-
dard English, defined as words with Wordnet entries;
proper names; abbreviations; non-English words;
non-standard words used with English. The cate-
gorization was based on the most frequent sense in
an informal analysis of our data. A glossary of non-
standard terms is given in Table 4.
Some patterns emerge from Table 3. Standard
English words tend to appear in areas with more
</bodyText>
<figure confidence="0.999103615384615">
0.16
102 103
0.28
0.26
0.24
0.22
0.2
0.18
multi−output lasso
SVD
highest variance
most frequent
average correlation
</figure>
<page confidence="0.985216">
1369
</page>
<table confidence="0.997653222222222">
vocabulary average white Eng. lang. urban
Afr. Am. Span. lang. family
Hisp. other lang. renter
med. inc.
full 5418 0.260 0.337 0.318 0.296 0.384 0.296 0.256 0.155 0.113 0.295 0.152
multi-output lasso 0.260 0.326 0.308 0.304 0.383 0.303 0.249 0.153 0.113 0.302 0.156
SVD 0.237 0.321 0.299 0.269 0.352 0.272 0.226 0.138 0.081 0.278 0.136
highest variance 394.6 0.220 0.309 0.287 0.245 0.315 0.248 0.199 0.132 0.085 0.250 0.135
most frequent 0.204 0.294 0.264 0.222 0.293 0.229 0.178 0.129 0.073 0.228 0.126
</table>
<tableCaption confidence="0.999839">
Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.
</tableCaption>
<bodyText confidence="0.999795476190476">
English speakers; predictably, Spanish words tend
to appear in areas with Spanish speakers and His-
panics. Emoticons tend to be used in areas with
many Hispanics and few African Americans. Ab-
breviations (e.g., lmaoo) have a nearly uniform
demographic profile, displaying negative correla-
tions with whites and English speakers, and posi-
tive correlations with African Americans, Hispanics,
renters, Spanish speakers, and areas classified as ur-
ban.
Many non-standard English words (e.g., dats)
appear in areas with high proportions of renters,
African Americans, and non-English speakers,
though a subset (haha, hahaha, and yep) display
the opposite demographic pattern. Many of these
non-standard words are phonetic transcriptions of
standard words or phrases: that’s-*dats, what’s
up-*wassup, I’m going to-*ima. The relationship
between these transcriptions and the phonological
characteristics of dialects such as African-American
Vernacular English is a topic for future work.
</bodyText>
<sectionHeader confidence="0.992672" genericHeader="conclusions">
5 Conjunctive Demographic Features
</sectionHeader>
<bodyText confidence="0.999737857142857">
Next, we demonstrate how to select conjunctions of
demographic features that predict text. Again, we
apply multi-output regression, but now we reverse
the direction of inference: the predictors are demo-
graphic features, and the outputs are term frequen-
cies. The sparsity-inducing E1� norm will select a
subset of demographic features that explain the term
frequencies.
We create an initial feature set f(0)(X) by bin-
ning each demographic attribute, using five equal-
frequency bins. We then constructive conjunctive
features by applying a procedure inspired by related
work in computational biology, called “Screen and
Clean” (Wu et al., 2010). On iteration i:
</bodyText>
<listItem confidence="0.911485090909091">
• Solve the sparse multi-output regression prob-
lem Y = f(z)(X)B(z) + c.
• Select a subset of features S(z) such that m E
S(z) iff maxi |0) |&gt; 0. These are the row
MJ
indices of the predictors with non-zero coeffi-
cients.
• Create a new feature set f(z+1)(X), including
the conjunction of each feature (and its nega-
tion) in S(z) with each feature in the initial set
f(0)(X).
</listItem>
<bodyText confidence="0.999812384615385">
We iterate this process to create features that con-
join as many as three attributes. In addition to the
binned versions of the demographic attributes de-
scribed in Table 1, we include geographical infor-
mation. We built Gaussian mixture models over the
locations, with 3, 5, 8, 12, 17, and 23 components.
For each author we include the most likely cluster
assignment in each of the six mixture models. For
efficiency, the outputs Y are not set to the raw term
frequencies; instead we compute a truncated sin-
gular value decomposition of the term frequencies
W Pz� UVDT, and use the basis U. We set the trun-
cation level to 100.
</bodyText>
<subsectionHeader confidence="0.996454">
5.1 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999976363636364">
The ability of the induced demographic features to
predict text is evaluated using a traditional perplex-
ity metric. The same test and training split is used
from the vocabulary experiments. We construct a
language model from the induced demographic fea-
tures by training a multi-output ridge regression,
which gives a matrix B� that maps from demographic
features to term frequencies across the entire vocab-
ulary. For each document in the test set, the “raw”
predicted language model is yd = f(xd)B, which
is then normalized. The probability mass assigned
</bodyText>
<page confidence="0.942033">
1370
</page>
<figure confidence="0.9995959375">
model
perplexity
definition
on my way
over
sister
school
shake my (fuck-
ing) head
with
what’s up
what
your, you
you plural
yes
you
</figure>
<figureCaption confidence="0.89691075">
Table 4: A glossary of non-standard terms from Ta-
ble 3. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
</figureCaption>
<bodyText confidence="0.884052210526316">
induced demographic features
raw demographic attributes
baseline (no demographics)
Table 5: Word perplexity on test documents, using
language models estimated from induced demographic
features, raw demographic attributes, and a relative-
frequency baseline. Lower scores are better.
to unseen words is determined through nested cross-
validation. We compare against a baseline language
model obtained from the training set, again using
nested cross-validation to set the probability of un-
seen terms.
Results are shown in Table 5. The language mod-
els induced from demographic data yield small but
statistically significant improvements over the base-
line (Wilcoxon signed-rank test, p &lt; .001). More-
over, the model based on conjunctive features signif-
icantly outperforms the model constructed from raw
attributes (p &lt; .001).
</bodyText>
<subsectionHeader confidence="0.991647">
5.2 Features Discovered
</subsectionHeader>
<bodyText confidence="0.999724888888889">
Our approach discovers 37 conjunctive features,
yielding the results shown in Table 5. We sort all
features by frequency, and manually select a sub-
set to display in Table 6. Alongside each feature,
we show the words with the highest and lowest log-
odds ratios with respect to the feature. Many of these
terms are non-standard; while space does not permit
a complete glossary, some are defined in Table 4 or
in our earlier work (Eisenstein et al., 2010).
</bodyText>
<figure confidence="0.983004595238095">
333.9
335.4
337.1
term
bbm
dats
dead(ass)
famu
ima
lls
lm(f)ao+
lml
madd
nah
odee
definition
Blackberry Messenger
that’s
very
Florida Agricultural
and Mechanical Univ.
I’m going to
laughing like shit
laughing my (fucking)
ass off
love my life
very, lots
no
very
term
omw
ova
sis
skool
sm(f)h
w|
wassup
wat
ya
yall
yep
yoo+
</figure>
<figureCaption confidence="0.3277123">
white
Afr. Am.
Hisp.
Eng. lang.
Span. lang.
other lang.
urban
family
renter
med. inc.
</figureCaption>
<equation confidence="0.964785596153846">
- - - + - + + +
;) - + - +
:( -
:) -
:d + - + - +
as - + -
awesome + - - - +
break - + - -
campus - + - -
dead - + - + + +
hell - + - -
shit - +
train - + +
will - + -
would + -
atlanta - + - -
famu + - + - - -
harlem - +
bbm - + - + + +
lls + - + - -
lmaoo - + + - + + + +
lmaooo - + + - + + + +
lmaoooo - + + - + + +
lmfaoo - + - + + +
lmfaooo - + - + + +
lml - + + - + + + + -
odee - + - + + +
omw - + + - + + + +
smfh - + + - + + + +
smh - + + +
w |- + - + + + +
con + - + +
la - + - +
si - + - +
dats - + - + -
deadass - + + - + + + +
haha + - -
hahah + -
hahaha + - - +
ima - + - + +
madd - - + +
nah - + - + + +
ova - + - +
sis - + +
skool - + - + + + -
wassup - + + - + + + + -
wat - + + - + + + + -
ya - + +
yall - +
yep - + - - - -
yoo - + + - + + + +
yooo - + - + +
</equation>
<tableCaption confidence="0.614906">
Table 3: Demographically-indicative terms discovered by
</tableCaption>
<table confidence="0.940684208333333">
multi-output sparse regression. Statistically significant
(p &lt; .05) associations are marked with a + or -.
1371
feature positive terms negative terms
1 geo: Northeast m2 brib mangoville soho odeee fasho #ilovefamu foo coo fina
2 geo: NYC mangoville lolss m2 brib wordd bahaha fasho goofy #ilovefamu
tacos
4 geo: South+Midwest renter ≤ 0.615 white ≤ 0.823 hme muthafucka bae charlotte tx odeee m2 lolss diner mangoville
7 Afr. Am. &gt; 0.101 renter &gt; 0.615 Span. lang. &gt; 0.063 dhat brib odeee lolss wassupp bahaha charlotte california ikr en-
ter
8 Afr. Am. ≤ 0.207 Hispanic &gt; 0.119 Span. lang. &gt; 0.063 les ahah para san donde bmore ohio #lowkey #twitterjail
nahhh
9 geo: NYC Span. lang. ≤ 0.213 mangoville thatt odeee lolss landed rodney jawn wiz golf
buzzin
12 Afr. Am. &gt; 0.442 geo: South+Midwest white ≤ 0.823 #ilovefamu panama midterms knoe esta pero odeee hii
willies #lowkey
15 geo: West Coast other lang. &gt; 0.110 ahah fasho san koo diego granted pride adore phat pressure
17 Afr. Am. &gt; 0.442 geo: NYC other lang. ≤ 0.110 lolss iim buzzin qonna qood foo tender celebs pages pandora
20 Afr. Am. ≤ 0.207 Span. lang. &gt; 0.063 white &gt; 0.823 del bby cuando estoy muscle knicks becoming uncomfortable
large granted
23 Afr. Am. ≤ 0.050 geo: West Span. lang. ≤ 0.106 leno it’d 15th hacked government knicks liquor uu hunn homee
33 Afr. Am. &gt; 0.101 geo: SF Bay Span. lang. &gt; 0.063 hella aha california bay o.o aj everywhere phones shift re-
gardless
36 Afr. Am. ≤ 0.050 geo: DC/Philadelphia Span. lang. ≤ 0.106 deh opens stuffed yaa bmore hmmmmm dyin tea cousin hella
</table>
<tableCaption confidence="0.949152">
Table 6: Conjunctive features discovered by our method with a strong sparsity-inducing prior, ordered by frequency.
</tableCaption>
<bodyText confidence="0.991026461538462">
We also show the words with high log-odds for each feature (postive terms) and its negation (negative terms).
In general, geography was a strong predictor, ap-
pearing in 25 of the 37 conjunctions. Features 1
and 2 (F1 and F2) are purely geographical, captur-
ing the northeastern United States and the New York
City area. The geographical area of F2 is completely
contained by F1; the associated terms are thus very
similar, but by having both features, the model can
distinguish terms which are used in northeastern ar-
eas outside New York City, as well as terms which
are especially likely in New York.5
Several features conjoin geography with demo-
graphic attributes. For example, F9 further refines
the New York City area by focusing on communities
that have relatively low numbers of Spanish speak-
ers; F17 emphasizes New York neighborhoods that
have very high numbers of African Americans and
few speakers of languages other than English and
Spanish. The regression model can use these fea-
tures in combination to make fine-grained distinc-
tions about the differences between such neighbor-
hoods. Outside New York, we see that F4 combines
a broad geographic area with attributes that select at
least moderate levels of minorities and fewer renters
(a proxy for areas that are less urban), while F15
identifies West Coast communities with large num-
</bodyText>
<footnote confidence="0.670357">
5Mangoville and M2 are clubs in New York; fasho and coo
were previously found to be strongly associated with the West
Coast (Eisenstein et al., 2010).
</footnote>
<bodyText confidence="0.999854857142857">
bers of speakers of languages other than English and
Spanish.
Race and ethnicity appear in 28 of the 37 con-
junctions. The attribute indicating the proportion of
African Americans appeared in 22 of these features,
strongly suggesting that African American Vernac-
ular English (Rickford, 1999) plays an important
role in social media text. Many of these features
conjoined the proportion of African Americans with
geographical features, identifying local linguistic
styles used predominantly in either African Amer-
ican or white communities. Among features which
focus on minority communities, F17 emphasizes the
New York area, F33 focuses on the San Francisco
Bay area, and F12 selects a broad area in the Mid-
west and South. Conversely, F23 selects areas with
very few African Americans and Spanish-speakers
in the western part of the United States, and F36 se-
lects for similar demographics in the area of Wash-
ington and Philadelphia.
Other features conjoined the proportion of
African Americans with the proportion of Hispan-
ics and/or Spanish speakers. In some cases, features
selected for high proportions of both African Amer-
icans and Hispanics; for example, F7 seems to iden-
tify a general “urban minority” group, emphasizing
renters, African Americans, and Spanish speakers.
Other features differentiate between African Ameri-
</bodyText>
<page confidence="0.832742">
1372
</page>
<bodyText confidence="0.995324854166666">
cans and Hispanics: F8 identifies regions with many Census statistics to estimate the age, gender, and
Spanish speakers and Hispanics, but few African racial distributions of various lexical items.6 Eisen-
Americans; F20 identifies regions with both Span- stein et al. (2010) infer geographic clusters that are
ish speakers and whites, but few African Americans. coherent with respect to both location and lexical
F8 and F20 tend to emphasize more Spanish words distributions; follow-up work by O’Connor et al.
than features which select for both African Ameri- (2010) applies a similar generative model to demo-
cans and Hispanics. graphic data. The model presented here differs in
While race, geography, and language predom- two key ways: first, we use sparsity-inducing regu-
inate, the socioeconomic attributes appear in far larization to perform variable selection; second, we
fewer features. The most prevalent attribute is the eschew high-dimensional mixture models in favor of
proportion of renters, which appears in F4 and F7, a bottom-up approach of building conjunctions of
and in three other features not shown here. This at- demographic and geographic attributes. In a mix-
tribute may be a better indicator of the urban/rural ture model, each component must define a distribu-
divide than the “% urban” attribute, which has a tion over all demographic variables, which may be
very low threshold for what counts as urban (see difficult to estimate in a high-dimensional setting.
Table 1). It may also be a better proxy for wealth Early examples of the use of sparsity in natu-
than median income, which appears in only one of ral language processing include maximum entropy
the thirty-seven selected features. Overall, the se- classification (Kazama and Tsujii, 2003), language
lected features tend to include attributes that are easy modeling (Goodman, 2004), and incremental pars-
to predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all
6 Related Work apply the standard lasso, obtaining sparsity for a sin-
Sociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely
and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010)
been used to identify relationships between demo- reformulated the problem of reranking N-best lists
graphic features and linguistic variables since the as multi-task learning with structured sparsity.
1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion
cent developments include the use of mixed factor This paper demonstrates how regression with struc-
models to account for idiosyncrasies of individual tured sparsity can be applied to select words and
speakers (Johnson, 2009), as well as clustering and conjunctive demographic features that reveal soci-
multidimensional scaling (Nerbonne, 2009) to en- olinguistic associations. The resulting models are
able aggregate inference across multiple linguistic compact and interpretable, with little cost in accu-
variables. However, all of these approaches assume racy. In the future we hope to consider richer lin-
that both the linguistic indicators and demographic guistic models capable of identifying multi-word ex-
attributes have already been identified by the re- pressions and syntactic variation.
searcher. In contrast, our approach focuses on iden- Acknowledgments We received helpful feedback
tifying these indicators automatically from data. We from Moira Burke, Scott Kiesling, Seyoung Kim, Andr´e
view our approach as an exploratory complement to Martins, Kriti Puniyani, and the anonymous reviewers.
more traditional analysis. Brendan O’Connor provided the data for this research,
There is relatively little computational work on and Seunghak Lee shared a Matlab implementation of
identifying speaker demographics. Chang et al. the multi-output lasso, which was the basis for our C
(2010) use U.S. Census statistics about the ethnic implementation. This research was enabled by AFOSR
distribution of last names as an anchor in a latent- FA9550010247, ONR N0001140910758, NSF CAREER
variable model that infers the ethnicity of Facebook DBI-0546594, NSF CAREER IIS-1054319, NSF IIS-
users; however, their paper analyzes social behav- 0713379, an Alfred P. Sloan Fellowship, and Google’s
ior rather than language use. In unpublished work, support of the Worldly Knowledge project at CMU.
David Bamman uses geotagged Twitter text and U.S.
1373
6http://www.lexicalist.com
</bodyText>
<sectionHeader confidence="0.990117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752082352942">
Henrietta J. Cedergren and David Sankoff. 1974. Vari-
able rules: Performance as a statistical reflection of
competence. Language, 50(2):333–355.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. ePluribus: Ethnicity on so-
cial networks. In Proceedings of ICWSM.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
Bl-ball for learning in high dimensions. In Proceed-
ings of ICML.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. n-best rerank-
ing by multitask learning. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
Metrics.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model of ge-
ographic lexical variation. In Proceedings of EMNLP.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical
Software, 33(1):1–22.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proceedings of NAACL-HLT.
Daniel E. Johnson. 2009. Getting off the GoldVarb
standard: Introducing Rbrul for mixed-effects variable
rule analysis. Language and Linguistics Compass,
3(1):359–383.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP.
William Labov. 1966. The Social Stratification of En-
glish in New York City. Center for Applied Linguis-
tics.
Han Liu, Mark Palatucci, and Jian Zhang. 2009. Block-
wise coordinate descent procedures for the multi-task
lasso, with applications to neural semantic basis dis-
covery. In Proceedings of ICML.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175–198.
Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science.
Ariadna Quattoni, Xavier Carreras, Michael Collins, and
Trevor Darrell. 2009. An efficient projection for Bl,�
regularization. In Proceedings of ICML.
John R. Rickford. 1999. African American Vernacular
English. Blackwell.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and Bl regularization for re-
laxed maximum-entropy modeling. In Proceedings of
EMNLP.
Gerard Rushton, Marc P. Armstrong, Josephine Gittler,
Barry R. Greene, Claire E. Pavlik, Michele M. West,
and Dale L. Zimmerman, editors. 2008. Geocoding
Health Data: The Use of Geographic Codes in Cancer
Prevention and Control, Research, and Practice. CRC
Press.
Hinrich Sch¨utze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Aaron Smith and Lee Rainie. 2010. Who tweets? Tech-
nical report, Pew Research Center, December.
Berwin A. Turlach, William N. Venables, and Stephen J.
Wright. 2005. Simultaneous variable selection. Tech-
nometrics, 47(3):349–363.
Larry Wasserman and Kathryn Roeder. 2009. High-
dimensional variable selection. Annals of Statistics,
37(5A):2178–2201.
Larry Wasserman. 2003. All of Statistics: A Concise
Course in Statistical Inference. Springer.
Jing Wu, Bernie Devlin, Steven Ringquist, Massimo
Trucco, and Kathryn Roeder. 2010. Screen and clean:
A tool for identifying interactions in genome-wide as-
sociation studies. Genetic Epidemiology, 34(3):275–
285.
Qing Zhang. 2005. A Chinese yuppie in Beijing: Phono-
logical variation and the construction of a new profes-
sional identity. Language in Society, 34:431–466.
Kathryn Zickuhr and Aaron Smith. 2010. 4% of online
Americans use location-based services. Technical re-
port, Pew Research Center, November.
</reference>
<page confidence="0.994958">
1374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931938">
<title confidence="0.999796">Discovering Sociolinguistic Associations with Structured Sparsity</title>
<author confidence="0.999993">Jacob Eisenstein Noah A Smith Eric P Xing</author>
<affiliation confidence="0.9999505">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.99676880952381">We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a comwe obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we demographic attributes into which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Henrietta J Cedergren</author>
<author>David Sankoff</author>
</authors>
<title>Variable rules: Performance as a statistical reflection of competence.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>2</issue>
<contexts>
<context position="33636" citStr="Cedergren and Sankoff, 1974" startWordPosition="5561" endWordPosition="5564">emental parsto predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all 6 Related Work apply the standard lasso, obtaining sparsity for a sinSociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsity. 1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion cent developments include the use of mixed factor This paper demonstrates how regression with strucmodels to account for idiosyncrasies of individual tured sparsity can be applied to select words and speakers (Johnson, 2009), as well as clustering and conjunctive demographic features that reveal socimultidimensional scaling (Nerbonne, 2009) to en- olinguistic associations. The resulting models are able aggregate inference across multiple linguistic compact and interpretable, with little cost in accuvariables. However, all of these approaches assume racy. In the future w</context>
</contexts>
<marker>Cedergren, Sankoff, 1974</marker>
<rawString>Henrietta J. Cedergren and David Sankoff. 1974. Variable rules: Performance as a statistical reflection of competence. Language, 50(2):333–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Itamar Rosenn</author>
<author>Lars Backstrom</author>
<author>Cameron Marlow</author>
</authors>
<title>ePluribus: Ethnicity on social networks.</title>
<date>2010</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<marker>Chang, Rosenn, Backstrom, Marlow, 2010</marker>
<rawString>Jonathan Chang, Itamar Rosenn, Lars Backstrom, and Cameron Marlow. 2010. ePluribus: Ethnicity on social networks. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Tushar Chandra</author>
</authors>
<title>Efficient projections onto the Bl-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10546" citStr="Duchi et al., 2008" startWordPosition="1666" endWordPosition="1669">tries of B are driven to zero, but each predictor may have anon-zero value for some output dimension. To drive entire rows of B to zero, we require acomposite regularizer. We consider the norm, which is the sum of E� norms across output dimensions: R(B) (Turlach et al., 2005). This norm, which corresponds to a multioutput lasso regression, has the desired property of dri lasso’sE1 E1,� =ETtmaxpbpt ving entire rows of B to zero. Et 3.1 Optimization There are several techniques for solving the normalized regression, including interior point methods (Turlach et al., 2005) and projected gradient (Duchi et al., 2008; Quattoni et al., 2009). We choose the blockwise coordinate descent approach of Liu et al. (2009) because it is easy to implement and efficient: the time complexity of each iteration is independent of the number of samples.3 Due to space limitations, we defer to Liu et al. (2009) for a complete description of the algorithm. However, we note two aspects of our implementation which are important for natural language processing applications. The efficiency is accomplished by precomputing the matrices C = XTY and D = XTX, where X and Y are the standardized versions of X and Y, obtained by subtrac</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. 2008. Efficient projections onto the Bl-ball for learning in high dimensions. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
<author>Masaaki Nagata</author>
</authors>
<title>n-best reranking by multitask learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics.</booktitle>
<contexts>
<context position="33398" citStr="Duh et al. (2010)" startWordPosition="5529" endWordPosition="5532">cessing include maximum entropy the thirty-seven selected features. Overall, the se- classification (Kazama and Tsujii, 2003), language lected features tend to include attributes that are easy modeling (Goodman, 2004), and incremental parsto predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all 6 Related Work apply the standard lasso, obtaining sparsity for a sinSociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsity. 1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion cent developments include the use of mixed factor This paper demonstrates how regression with strucmodels to account for idiosyncrasies of individual tured sparsity can be applied to select words and speakers (Johnson, 2009), as well as clustering and conjunctive demographic features that reveal socimultidimensional scaling (Nerbonne, 2</context>
</contexts>
<marker>Duh, Sudoh, Tsukada, Isozaki, Nagata, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki Isozaki, and Masaaki Nagata. 2010. n-best reranking by multitask learning. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model of geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model of geographic lexical variation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Rob Tibshirani</author>
</authors>
<title>Regularization paths for generalized linear models via coordinate descent.</title>
<date>2010</date>
<journal>Journal of Statistical Software,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="12571" citStr="Friedman et al., 2010" startWordPosition="2016" endWordPosition="2019">s are less accurate, but are faster and require less space: for K singular values, the storage cost is O(KP), instead of O(P2); the time cost increases by a factor of K. This approximation was not necessary in the experiments presented here, although we have found that it performs well as long as the regularizer is not too close to zero. 3.2 Regularization The regularization constant A can be computed using cross-validation. As A increases, we reuse the previous solution of B for initialization; this “warm start” trick can greatly accelerate the computation of the overall regularization path (Friedman et al., 2010). At each Ai, we solve the sparse multi-output regression; the solution Bi defines a sparse set of predictors for all tasks. We then use this limited set of predictors to construct a new input matrix Xi, which serves as the input in a standard ridge regression, thus refitting the model. The tuning set performance of this regression is the score for Ai. Such post hoc refitting is often used in tandem with the lasso and related sparse methods; the effectiveness of this procedure has been demonstrated in both theory (Wasserman and Roeder, 2009) and practice (Wu et al., 2010). The regularization p</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2010</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2010. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="32998" citStr="Goodman, 2004" startWordPosition="5470" endWordPosition="5471">ch component must define a distribudivide than the “% urban” attribute, which has a tion over all demographic variables, which may be very low threshold for what counts as urban (see difficult to estimate in a high-dimensional setting. Table 1). It may also be a better proxy for wealth Early examples of the use of sparsity in natuthan median income, which appears in only one of ral language processing include maximum entropy the thirty-seven selected features. Overall, the se- classification (Kazama and Tsujii, 2003), language lected features tend to include attributes that are easy modeling (Goodman, 2004), and incremental parsto predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all 6 Related Work apply the standard lasso, obtaining sparsity for a sinSociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsit</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel E Johnson</author>
</authors>
<title>Getting off the GoldVarb standard: Introducing Rbrul for mixed-effects variable rule analysis.</title>
<date>2009</date>
<journal>Language and Linguistics Compass,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="33884" citStr="Johnson, 2009" startWordPosition="5601" endWordPosition="5602">ed sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsity. 1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion cent developments include the use of mixed factor This paper demonstrates how regression with strucmodels to account for idiosyncrasies of individual tured sparsity can be applied to select words and speakers (Johnson, 2009), as well as clustering and conjunctive demographic features that reveal socimultidimensional scaling (Nerbonne, 2009) to en- olinguistic associations. The resulting models are able aggregate inference across multiple linguistic compact and interpretable, with little cost in accuvariables. However, all of these approaches assume racy. In the future we hope to consider richer linthat both the linguistic indicators and demographic guistic models capable of identifying multi-word exattributes have already been identified by the re- pressions and syntactic variation. searcher. In contrast, our app</context>
</contexts>
<marker>Johnson, 2009</marker>
<rawString>Daniel E. Johnson. 2009. Getting off the GoldVarb standard: Introducing Rbrul for mixed-effects variable rule analysis. Language and Linguistics Compass, 3(1):359–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="32906" citStr="Kazama and Tsujii, 2003" startWordPosition="5455" endWordPosition="5458">and geographic attributes. In a mixtribute may be a better indicator of the urban/rural ture model, each component must define a distribudivide than the “% urban” attribute, which has a tion over all demographic variables, which may be very low threshold for what counts as urban (see difficult to estimate in a high-dimensional setting. Table 1). It may also be a better proxy for wealth Early examples of the use of sparsity in natuthan median income, which appears in only one of ral language processing include maximum entropy the thirty-seven selected features. Overall, the se- classification (Kazama and Tsujii, 2003), language lected features tend to include attributes that are easy modeling (Goodman, 2004), and incremental parsto predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all 6 Related Work apply the standard lasso, obtaining sparsity for a sinSociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphi</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>The Social Stratification of English in</title>
<date>1966</date>
<institution>City. Center for Applied Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="1462" citStr="Labov, 1966" startWordPosition="199" endWordPosition="200">onjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties. 1 Introduction How is language influenced by the speaker’s sociocultural identity? Quantitative sociolinguistics usually addresses this question through carefully crafted studies that correlate individual demographic attributes and linguistic variables—for example, the interaction between income and the “dropped r” feature of the New York accent (Labov, 1966). But such studies require the knowledge to select the “dropped r” and the speaker’s income, from thousands of other possibilities. In this paper, we present a method to acquire such patterns from raw data. Using multi-output regression with structured sparsity, our method identifies a small subset of lexical items that are most influenced by demographics, and discovers conjunctions of demographic attributes that are especially salient for lexical variation. Sociolinguistic associations are difficult to model, because the space of potentially relevant interactions is large and complex. On the </context>
</contexts>
<marker>Labov, 1966</marker>
<rawString>William Labov. 1966. The Social Stratification of English in New York City. Center for Applied Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Liu</author>
<author>Mark Palatucci</author>
<author>Jian Zhang</author>
</authors>
<title>Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10644" citStr="Liu et al. (2009)" startWordPosition="1682" endWordPosition="1685">n. To drive entire rows of B to zero, we require acomposite regularizer. We consider the norm, which is the sum of E� norms across output dimensions: R(B) (Turlach et al., 2005). This norm, which corresponds to a multioutput lasso regression, has the desired property of dri lasso’sE1 E1,� =ETtmaxpbpt ving entire rows of B to zero. Et 3.1 Optimization There are several techniques for solving the normalized regression, including interior point methods (Turlach et al., 2005) and projected gradient (Duchi et al., 2008; Quattoni et al., 2009). We choose the blockwise coordinate descent approach of Liu et al. (2009) because it is easy to implement and efficient: the time complexity of each iteration is independent of the number of samples.3 Due to space limitations, we defer to Liu et al. (2009) for a complete description of the algorithm. However, we note two aspects of our implementation which are important for natural language processing applications. The efficiency is accomplished by precomputing the matrices C = XTY and D = XTX, where X and Y are the standardized versions of X and Y, obtained by subtracting the mean and scaling by the variance. Explicit mean correction would destroy the sparse term </context>
</contexts>
<marker>Liu, Palatucci, Zhang, 2009</marker>
<rawString>Han Liu, Mark Palatucci, and Jian Zhang. 2009. Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<title>Data-driven dialectology.</title>
<date>2009</date>
<journal>Language and Linguistics Compass,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="34002" citStr="Nerbonne, 2009" startWordPosition="5616" endWordPosition="5617"> al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsity. 1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion cent developments include the use of mixed factor This paper demonstrates how regression with strucmodels to account for idiosyncrasies of individual tured sparsity can be applied to select words and speakers (Johnson, 2009), as well as clustering and conjunctive demographic features that reveal socimultidimensional scaling (Nerbonne, 2009) to en- olinguistic associations. The resulting models are able aggregate inference across multiple linguistic compact and interpretable, with little cost in accuvariables. However, all of these approaches assume racy. In the future we hope to consider richer linthat both the linguistic indicators and demographic guistic models capable of identifying multi-word exattributes have already been identified by the re- pressions and syntactic variation. searcher. In contrast, our approach focuses on iden- Acknowledgments We received helpful feedback tifying these indicators automatically from data. </context>
</contexts>
<marker>Nerbonne, 2009</marker>
<rawString>John Nerbonne. 2009. Data-driven dialectology. Language and Linguistics Compass, 3(1):175–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Jacob Eisenstein</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>A mixture model of demographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of NIPS Workshop on Machine Learning in Computational Social Science.</booktitle>
<marker>O’Connor, Eisenstein, Xing, Smith, 2010</marker>
<rawString>Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and Noah A. Smith. 2010. A mixture model of demographic lexical variation. In Proceedings of NIPS Workshop on Machine Learning in Computational Social Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>An efficient projection for Bl,� regularization.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10570" citStr="Quattoni et al., 2009" startWordPosition="1670" endWordPosition="1673">n to zero, but each predictor may have anon-zero value for some output dimension. To drive entire rows of B to zero, we require acomposite regularizer. We consider the norm, which is the sum of E� norms across output dimensions: R(B) (Turlach et al., 2005). This norm, which corresponds to a multioutput lasso regression, has the desired property of dri lasso’sE1 E1,� =ETtmaxpbpt ving entire rows of B to zero. Et 3.1 Optimization There are several techniques for solving the normalized regression, including interior point methods (Turlach et al., 2005) and projected gradient (Duchi et al., 2008; Quattoni et al., 2009). We choose the blockwise coordinate descent approach of Liu et al. (2009) because it is easy to implement and efficient: the time complexity of each iteration is independent of the number of samples.3 Due to space limitations, we defer to Liu et al. (2009) for a complete description of the algorithm. However, we note two aspects of our implementation which are important for natural language processing applications. The efficiency is accomplished by precomputing the matrices C = XTY and D = XTX, where X and Y are the standardized versions of X and Y, obtained by subtracting the mean and scalin</context>
</contexts>
<marker>Quattoni, Carreras, Collins, Darrell, 2009</marker>
<rawString>Ariadna Quattoni, Xavier Carreras, Michael Collins, and Trevor Darrell. 2009. An efficient projection for Bl,� regularization. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Rickford</author>
</authors>
<date>1999</date>
<publisher>African American Vernacular English. Blackwell.</publisher>
<contexts>
<context position="30097" citStr="Rickford, 1999" startWordPosition="5019" endWordPosition="5020">butes that select at least moderate levels of minorities and fewer renters (a proxy for areas that are less urban), while F15 identifies West Coast communities with large num5Mangoville and M2 are clubs in New York; fasho and coo were previously found to be strongly associated with the West Coast (Eisenstein et al., 2010). bers of speakers of languages other than English and Spanish. Race and ethnicity appear in 28 of the 37 conjunctions. The attribute indicating the proportion of African Americans appeared in 22 of these features, strongly suggesting that African American Vernacular English (Rickford, 1999) plays an important role in social media text. Many of these features conjoined the proportion of African Americans with geographical features, identifying local linguistic styles used predominantly in either African American or white communities. Among features which focus on minority communities, F17 emphasizes the New York area, F33 focuses on the San Francisco Bay area, and F12 selects a broad area in the Midwest and South. Conversely, F23 selects areas with very few African Americans and Spanish-speakers in the western part of the United States, and F36 selects for similar demographics in</context>
</contexts>
<marker>Rickford, 1999</marker>
<rawString>John R. Rickford. 1999. African American Vernacular English. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Incremental feature selection and Bl regularization for relaxed maximum-entropy modeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33098" citStr="Riezler and Vasserman, 2004" startWordPosition="5484" endWordPosition="5487">on over all demographic variables, which may be very low threshold for what counts as urban (see difficult to estimate in a high-dimensional setting. Table 1). It may also be a better proxy for wealth Early examples of the use of sparsity in natuthan median income, which appears in only one of ral language processing include maximum entropy the thirty-seven selected features. Overall, the se- classification (Kazama and Tsujii, 2003), language lected features tend to include attributes that are easy modeling (Goodman, 2004), and incremental parsto predict from text (compare with Table 2). ing (Riezler and Vasserman, 2004). These papers all 6 Related Work apply the standard lasso, obtaining sparsity for a sinSociolinguistics has a long tradition of quantitative gle output dimension. Structured sparsity has rarely and computational research. Logistic regression has been applied to language tasks, but Duh et al. (2010) been used to identify relationships between demo- reformulated the problem of reranking N-best lists graphic features and linguistic variables since the as multi-task learning with structured sparsity. 1970s (Cedergren and Sankoff, 1974). More re- 7 Conclusion cent developments include the use of m</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Stefan Riezler and Alexander Vasserman. 2004. Incremental feature selection and Bl regularization for relaxed maximum-entropy modeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Rushton</author>
<author>Marc P Armstrong</author>
<author>Josephine Gittler</author>
<author>Barry R Greene</author>
</authors>
<date>2008</date>
<booktitle>Geocoding Health Data: The Use of Geographic Codes in Cancer Prevention and Control, Research, and Practice.</booktitle>
<editor>Claire E. Pavlik, Michele M. West, and Dale L. Zimmerman, editors.</editor>
<publisher>CRC Press.</publisher>
<marker>Rushton, Armstrong, Gittler, Greene, 2008</marker>
<rawString>Gerard Rushton, Marc P. Armstrong, Josephine Gittler, Barry R. Greene, Claire E. Pavlik, Michele M. West, and Dale L. Zimmerman, editors. 2008. Geocoding Health Data: The Use of Geographic Codes in Cancer Prevention and Control, Research, and Practice. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Jan Pedersen</author>
</authors>
<title>A vector model for syntagmatic and paradigmatic relatedness.</title>
<date>1993</date>
<booktitle>In Proceedings of the 9th Annual Conference of the UW Centre for the New OED and Text Research.</booktitle>
<marker>Sch¨utze, Pedersen, 1993</marker>
<rawString>Hinrich Sch¨utze and Jan Pedersen. 1993. A vector model for syntagmatic and paradigmatic relatedness. In Proceedings of the 9th Annual Conference of the UW Centre for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Smith</author>
<author>Lee Rainie</author>
</authors>
<title>Who tweets?</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Pew Research Center,</institution>
<contexts>
<context position="7125" citStr="Smith and Rainie, 2010" startWordPosition="1079" endWordPosition="1082"> the percentage who live in rented housing, and the median reported income in each ZCTA. While geographical aggregate statistics are frequently used to proxy for individual socioeconomic status in research areas such as public health (e.g., Rushton, 2008), it is clear that interpretation must proceed with caution. Consider an author from a ZIP code in which 60% of the residents are Hispanic:2 we do not know the likelihood that the author is Hispanic, because the set of Twitter users is not a representative sample of the overall population. Polling research suggests that users of both Twitter (Smith and Rainie, 2010) and geolocation services (Zickuhr and Smith, 2010) are much more diverse with respect to age, gender, race and ethnicity than the general population of Internet users. Nonetheless, at present we can only use aggregate statistics to make inferences about the geographic communities in which our authors live, and not the authors themselves. 2In the U.S. Census, the official ethnonym is Hispanic or Latino; for brevity we will use Hispanic in the rest of this paper. 1366 3 Models The selection of both words and demographic features can be framed in terms of multi-output regression with structured </context>
</contexts>
<marker>Smith, Rainie, 2010</marker>
<rawString>Aaron Smith and Lee Rainie. 2010. Who tweets? Technical report, Pew Research Center, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berwin A Turlach</author>
<author>William N Venables</author>
<author>Stephen J Wright</author>
</authors>
<title>Simultaneous variable selection.</title>
<date>2005</date>
<journal>Technometrics,</journal>
<volume>47</volume>
<issue>3</issue>
<contexts>
<context position="10204" citStr="Turlach et al., 2005" startWordPosition="1612" endWordPosition="1615"> predict a large set of words. Our goal is to select a small set of predictors yielding good performance across all output dimensions. Thus, we desire structured sparsity, in which entire rows of the coefficient matrix B are driven to zero. Structured sparsity is not achieved by the norm. The lasso gives element-wise sparsity, in which many entries of B are driven to zero, but each predictor may have anon-zero value for some output dimension. To drive entire rows of B to zero, we require acomposite regularizer. We consider the norm, which is the sum of E� norms across output dimensions: R(B) (Turlach et al., 2005). This norm, which corresponds to a multioutput lasso regression, has the desired property of dri lasso’sE1 E1,� =ETtmaxpbpt ving entire rows of B to zero. Et 3.1 Optimization There are several techniques for solving the normalized regression, including interior point methods (Turlach et al., 2005) and projected gradient (Duchi et al., 2008; Quattoni et al., 2009). We choose the blockwise coordinate descent approach of Liu et al. (2009) because it is easy to implement and efficient: the time complexity of each iteration is independent of the number of samples.3 Due to space limitations, we def</context>
</contexts>
<marker>Turlach, Venables, Wright, 2005</marker>
<rawString>Berwin A. Turlach, William N. Venables, and Stephen J. Wright. 2005. Simultaneous variable selection. Technometrics, 47(3):349–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Wasserman</author>
<author>Kathryn Roeder</author>
</authors>
<title>Highdimensional variable selection.</title>
<date>2009</date>
<journal>Annals of Statistics,</journal>
<pages>37--5</pages>
<contexts>
<context position="13118" citStr="Wasserman and Roeder, 2009" startWordPosition="2111" endWordPosition="2114">erate the computation of the overall regularization path (Friedman et al., 2010). At each Ai, we solve the sparse multi-output regression; the solution Bi defines a sparse set of predictors for all tasks. We then use this limited set of predictors to construct a new input matrix Xi, which serves as the input in a standard ridge regression, thus refitting the model. The tuning set performance of this regression is the score for Ai. Such post hoc refitting is often used in tandem with the lasso and related sparse methods; the effectiveness of this procedure has been demonstrated in both theory (Wasserman and Roeder, 2009) and practice (Wu et al., 2010). The regularization parameter of the ridge regression is determined by internal cross-validation. 4 Predicting Demographics from Text Sparse multi-output regression can be used to select a subset of vocabulary items that are especially indicative of demographic and geographic differences. 4Assume without loss of generality that X and Y are scaled to have variance 1, because this scaling does not affect the sparsity pattern. Starting from the regression problem (1), the predictors X are set to the term frequencies, with one column for each word type and one row f</context>
</contexts>
<marker>Wasserman, Roeder, 2009</marker>
<rawString>Larry Wasserman and Kathryn Roeder. 2009. Highdimensional variable selection. Annals of Statistics, 37(5A):2178–2201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Wasserman</author>
</authors>
<title>All of Statistics: A Concise Course in Statistical Inference.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<contexts>
<context position="18708" citStr="Wasserman, 2003" startWordPosition="2986" endWordPosition="2987">e sets. This demonstrates its utility for identifying interpretable models which permit qualitative analysis. 4.2 Qualitative Analysis For a qualitative analysis, we retrain the model on the full dataset, and tune the regularization to identify a compact set of 69 features. For each identified term, we apply a significance test on the relationship between the presence of each term and the demographic indicators shown in the columns of the table. Specifically, we apply the Wald test for comparing the means of independent samples, while making the Bonferroni correction for multiple comparisons (Wasserman, 2003). The use of sparse multioutput regression for variable selection increases the power of post hoc significance testing, because the Bonferroni correction bases the threshold for statistical significance on the total number of comparisons. We find 275 associations at the p &lt; .05 level; at the higher threshold required by a Bonferroni correction for comparisons among all terms in the vocabulary, 69 of these associations would have been missed. Table 3 shows the terms identified by our model which have a significant correlation with at least one of the demographic indicators. We divide words in t</context>
</contexts>
<marker>Wasserman, 2003</marker>
<rawString>Larry Wasserman. 2003. All of Statistics: A Concise Course in Statistical Inference. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Wu</author>
<author>Bernie Devlin</author>
<author>Steven Ringquist</author>
<author>Massimo Trucco</author>
<author>Kathryn Roeder</author>
</authors>
<title>Screen and clean: A tool for identifying interactions in genome-wide association studies.</title>
<date>2010</date>
<journal>Genetic Epidemiology,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>285</pages>
<contexts>
<context position="13149" citStr="Wu et al., 2010" startWordPosition="2117" endWordPosition="2120">arization path (Friedman et al., 2010). At each Ai, we solve the sparse multi-output regression; the solution Bi defines a sparse set of predictors for all tasks. We then use this limited set of predictors to construct a new input matrix Xi, which serves as the input in a standard ridge regression, thus refitting the model. The tuning set performance of this regression is the score for Ai. Such post hoc refitting is often used in tandem with the lasso and related sparse methods; the effectiveness of this procedure has been demonstrated in both theory (Wasserman and Roeder, 2009) and practice (Wu et al., 2010). The regularization parameter of the ridge regression is determined by internal cross-validation. 4 Predicting Demographics from Text Sparse multi-output regression can be used to select a subset of vocabulary items that are especially indicative of demographic and geographic differences. 4Assume without loss of generality that X and Y are scaled to have variance 1, because this scaling does not affect the sparsity pattern. Starting from the regression problem (1), the predictors X are set to the term frequencies, with one column for each word type and one row for each author in the dataset. </context>
<context position="22184" citStr="Wu et al., 2010" startWordPosition="3515" endWordPosition="3518">to select conjunctions of demographic features that predict text. Again, we apply multi-output regression, but now we reverse the direction of inference: the predictors are demographic features, and the outputs are term frequencies. The sparsity-inducing E1� norm will select a subset of demographic features that explain the term frequencies. We create an initial feature set f(0)(X) by binning each demographic attribute, using five equalfrequency bins. We then constructive conjunctive features by applying a procedure inspired by related work in computational biology, called “Screen and Clean” (Wu et al., 2010). On iteration i: • Solve the sparse multi-output regression problem Y = f(z)(X)B(z) + c. • Select a subset of features S(z) such that m E S(z) iff maxi |0) |&gt; 0. These are the row MJ indices of the predictors with non-zero coefficients. • Create a new feature set f(z+1)(X), including the conjunction of each feature (and its negation) in S(z) with each feature in the initial set f(0)(X). We iterate this process to create features that conjoin as many as three attributes. In addition to the binned versions of the demographic attributes described in Table 1, we include geographical information. </context>
</contexts>
<marker>Wu, Devlin, Ringquist, Trucco, Roeder, 2010</marker>
<rawString>Jing Wu, Bernie Devlin, Steven Ringquist, Massimo Trucco, and Kathryn Roeder. 2010. Screen and clean: A tool for identifying interactions in genome-wide association studies. Genetic Epidemiology, 34(3):275– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Zhang</author>
</authors>
<title>A Chinese yuppie in Beijing: Phonological variation and the construction of a new professional identity. Language in Society,</title>
<date>2005</date>
<pages>34--431</pages>
<contexts>
<context position="2358" citStr="Zhang, 2005" startWordPosition="332" endWordPosition="333"> a small subset of lexical items that are most influenced by demographics, and discovers conjunctions of demographic attributes that are especially salient for lexical variation. Sociolinguistic associations are difficult to model, because the space of potentially relevant interactions is large and complex. On the linguistic side there are thousands of possible variables, even if we limit ourselves to unigram lexical features. On the demographic side, the interaction between demographic attributes is often non-linear: for example, gender may negate or amplify class-based language differences (Zhang, 2005). Thus, additive models which assume that each demographic attribute makes a linear contribution are inadequate. In this paper, we explore the large space of potential sociolinguistic associations using structured sparsity. We treat the relationship between language and demographics as a set of multi-input, multioutput regression problems. The regression coefficients are arranged in a matrix, with rows indicating predictors and columns indicating outputs. We apply a composite regularizer that drives entire rows of the coefficient matrix to zero, yielding compact, interpretable models that reus</context>
</contexts>
<marker>Zhang, 2005</marker>
<rawString>Qing Zhang. 2005. A Chinese yuppie in Beijing: Phonological variation and the construction of a new professional identity. Language in Society, 34:431–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn Zickuhr</author>
<author>Aaron Smith</author>
</authors>
<title>4% of online Americans use location-based services.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Pew Research Center,</institution>
<contexts>
<context position="7176" citStr="Zickuhr and Smith, 2010" startWordPosition="1086" endWordPosition="1090">e median reported income in each ZCTA. While geographical aggregate statistics are frequently used to proxy for individual socioeconomic status in research areas such as public health (e.g., Rushton, 2008), it is clear that interpretation must proceed with caution. Consider an author from a ZIP code in which 60% of the residents are Hispanic:2 we do not know the likelihood that the author is Hispanic, because the set of Twitter users is not a representative sample of the overall population. Polling research suggests that users of both Twitter (Smith and Rainie, 2010) and geolocation services (Zickuhr and Smith, 2010) are much more diverse with respect to age, gender, race and ethnicity than the general population of Internet users. Nonetheless, at present we can only use aggregate statistics to make inferences about the geographic communities in which our authors live, and not the authors themselves. 2In the U.S. Census, the official ethnonym is Hispanic or Latino; for brevity we will use Hispanic in the rest of this paper. 1366 3 Models The selection of both words and demographic features can be framed in terms of multi-output regression with structured sparsity. To select the lexical indicators that bes</context>
</contexts>
<marker>Zickuhr, Smith, 2010</marker>
<rawString>Kathryn Zickuhr and Aaron Smith. 2010. 4% of online Americans use location-based services. Technical report, Pew Research Center, November.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>