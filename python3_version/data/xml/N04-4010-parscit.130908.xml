<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975458">
Using N-best Lists for Named Entity Recognition from Chinese Speech
</title>
<author confidence="0.676825">
Lufeng ZHAI*, Pascale FUNG*, Richard SCHWARTZ†, Marine CARPUAT$, Dekai WU$
</author>
<affiliation confidence="0.415196">
† BBN Technologies
</affiliation>
<address confidence="0.640875666666667">
9861 Broken Land Parkway
Columbia, MD 21046
U.S.A
</address>
<email confidence="0.933643">
schwartz@bbn.com
</email>
<author confidence="0.261874">
* HKUST
</author>
<affiliation confidence="0.87853025">
Human Language Technology Center
Electrical &amp; Electronic Engineering
University of Science and Technology
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.834532">
{lfzhai,pascale}@ee.ust.hk
</email>
<author confidence="0.289755">
$ HKUST
</author>
<affiliation confidence="0.925775">
Human Language Technology Center
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.991654">
{marine,dekai}@cs.ust.hk
</email>
<sectionHeader confidence="0.997322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924636363636">
We present the first known result for named
entity recognition (NER) in realistic large-
vocabulary spoken Chinese. We establish this
result by applying a maximum entropy model,
currently the single best known approach for
textual Chinese NER, to the recognition
output of the BBN LVCSR system on Chinese
Broadcast News utterances. Our results
support the claim that transferring NER
approaches from text to spoken language is a
significantly more difficult task for Chinese
than for English. We propose re-segmenting
the ASR hypotheses as well as applying post-
classification to improve the performance.
Finally, we introduce a method of using n-best
hypotheses that yields a small but nevertheless
useful improvement NER accuracy. We use
acoustic, phonetic, language model, NER and
other scores as confidence measure.
Experimental results show an average of 6.7%
relative improvement in precision and 1.7%
relative improvement in F-measure.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.994635653061224">
Named Entity Recognition (NER) is the first step for
many tasks in the fields of natural language processing
and information retrieval. It is a designated task in a
number of conferences, including the Message
Understanding Conference (MUC), the Information
Retrieval and Extraction Conference (IREX), the
Conferences on Natural Language Learning (CoNLL)
and the recent Automatic Content Extraction
Conference (ACE).
There has been a considerable amount of work on
English NER yielding good performance (Tjong Kim
Sang et al. 2002, 2003; Cucerzan &amp; Yarowsky 1999;
Wu et al. 2003). However, Chinese NER is more
difficult, especially on speech output, due to two
reasons. First, Chinese has a large number of homonyms
and the vocabulary used in Chinese person names is an
open set so more characters/words are unseen in the
training data. Second, there is no standard definition of
Chinese words. Word segmentation errors made by
recognizers may lead to NER errors. Previous work on
Chinese textual NER includes Jing et al. (2003) and Sun
et al. (2003) but there has been no published work on
NER in spoken Chinese.
Named Entity Recognition for speech is more difficult
than for text, since the most reliable features for textual
NER (punctuation, capitalization, and syntactic
patterns) are often not available in speech output. NER
on automatically recognized broadcast news was first
conducted by MITRE in 1997, and was subsequently
added to Hub-4 evaluation as a task. Palmer et al.
(1999) used error modeling, and Horlock &amp; King (2003)
proposed discriminative training to handle NER errors;
both used a hidden Markov model (HMM). Miller et al.
(1999) also reported results in English speech NER
using an HMM model. In a NIST 1999 evaluation, it
was found that NER errors on speech arise from a
combination of ASR errors and errors of the underlying
NER system.
In this work, we investigate whether the NIST finding
holds for Chinese speech NER as well. We present the
first known result for recognizing named entities in
realistic large-vocabulary spoken Chinese. We propose
to use the best-known model for Chinese textual NER—
a maximum entropy model—on Chinese speech NER.
We also propose using re-segmentation and post-
classification to improve this model. Finally, we
propose to integrate the ASR and NER components to
optimize NER performance by making use of the n-best
ASR output.
</bodyText>
<sectionHeader confidence="0.944125" genericHeader="method">
2. A Spoken Chinese NER Model
</sectionHeader>
<subsectionHeader confidence="0.94861">
2.1 LVCSR output
</subsectionHeader>
<bodyText confidence="0.966208571428571">
We use the ASR output from BBN’s Byblos system on
broadcast news data from the Xinhua News Agency,
y
which has 1046 sentences. This system has a character
error rate of 7%. We had manually annotated them with
named entities as an evaluation set according to the PFR
corpus annotation guideline (PFR 2001).
</bodyText>
<subsectionHeader confidence="0.901565">
2.2 A maximum-entropy NER model with post-
classification
</subsectionHeader>
<bodyText confidence="0.999883777777778">
To establish a baseline spoken Chinese NER model, we
selected a maximum entropy (MaxEnt) approach since
this is currently the single most accurate approach
known for recognizing named entities in text (Tjong
Kim Sang et al., 2002, 2003, Jing et al., 2003)1. In the
CoNLL 2003 NER evaluation, 5 out of 16 systems use
MaxEnt models and the top 3 results for English and top
2 results for German were obtained by systems that use
MaxEnt.
Natural language can be viewed as a stochastic process.
We can use p(y|x) to denote the probability distribution
of what we try to predict y (.e.g. part-of-speech tag,
Named Entity tag) conditioned on what we observe x
(e.g. previous POS or the actual word). The Maximum
Entropy principle can be stated as follows: given some
set of constrains from observations, find the most
uniform probability distribution (Maximum Entropy)
p(y|x) that satisfies these constrains:
</bodyText>
<equation confidence="0.948629636363636">
* = arg maxyi P(yi  |xi)
1
P y x
(  |) = exp(∑ λ ⋅ f x y
( , ))
i i j j i i
Z x
( )
i j
0
=
</equation>
<bodyText confidence="0.882738392857143">
In the above equations, fj(xi,yk) is a binary valued feature
function, and λj is a weight that indicates how important
feature fj is for the model. Z(xi) is a normalization factor.
We estimate the weights using the improved iterative
scaling (IIS) algorithm.
We use two annotated corpora for training. One is a
corpus of
Daily newspaper from January 1998,
annotated by the Institute of Computational Linguistics
of Beijing University (the
corpus). This corpus
consists of about 20k sentences, annotated with word
segmentation, part-of-speech tags and three named-
entity tags including person (PER), location (LOC) and
organization (ORG) . We use the first 6k sentences to
train our NER system. Our system is then evaluated on
2k sentences from
Daily and
sentences from
the BBN ASR output. The results are shown in Tables 1
People’s
“PFR”
People’s
1k
d 3.
the only publicly available corpus is
transcription of broadcast news, provided by NIST for
the Information Extraction
</bodyText>
<subsectionHeader confidence="0.44506">
Entity Recognition
</subsectionHeader>
<bodyText confidence="0.890646">
evaluation (the
corpus). This corpus consists of
10 hours of training data and 1 hour of test data. Ten
categories of NEs were annotated, including person
names, location, organization, date, duration, an
a human-generated
–
“IEER”
d
measure. A comparison of results is shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.868636">
2.3 Experimental setup
2.4 Results and discussion
</subsectionHeader>
<bodyText confidence="0.841955">
= λ ( , ))
</bodyText>
<equation confidence="0.992353833333333">
⋅ f x y
l m
∑ exp(∑
k= 0 j=0
i j j i k
m
</equation>
<bodyText confidence="0.994355">
an approach of one-pass
to a
two-pass approach where the identified NE candidates
are classified later. In addition, we propose a hybrid
approach of using one-pass identification/classification
results, discarding the extracted NE tags, an
identification/classification
d re-
classifying the extracted NE in a second pass.
We exclude from the pre
</bodyText>
<page confidence="0.852439">
1
</page>
<bodyText confidence="0.9050676">
sent focus the slight improvements
that are usually possible to obtain by combination of multiple
models, usually through ad hoc methods such as voting.
word-based model. These two models
achieved almost the same results, which show that our
</bodyText>
<note confidence="0.508645">
NER system based on MaxEnt is
</note>
<bodyText confidence="0.977593666666666">
IBM’sHMM
state-of-the-art.
For our task, we first compare a character-based
MaxEnt model to a word-based model. Since
recognition errors also lead to segmentation errors
which in turn have an adverse effect on the NER
performance, we experiment with disregarding the word
boundaries in the ASR hypothesis and instead re-
segment using a MaxEnt segmenter. We also compare
an
To compare our system to the IBM baseline described
in (Jing et al. 2003), we need to evaluate our system on
the same corpus as they used. Among the data they used,
From text to speech
Table 1 compares the NER performances of the same
MaxEnt model on the Chinese textual PFR test data and
the one-best BBN ASR hypotheses. We can see a
significant drop in performance in the latter. These
results support the claim that transferring NER
approaches from text to spoken language is a
significantly more difficult task for Chinese than for
English. We argue that this is due to the combination of
different factors specific to spoken Chinese. First,
Chinese has a large number of homonyms that leads to a
degradation in speech recognition accuracy which in
turn leads to low NER accuracy. Second, the vocabulary
used in Chinese person names is an open set so man
y
characters/words are unseen in the training data.
Comparison to IBM baseline
Table 2 compares results on IEER data from our
baseline word-based MaxEnt model compared with that
of
</bodyText>
<sectionHeader confidence="0.841825" genericHeader="method">
Re-segmentation effect
</sectionHeader>
<bodyText confidence="0.9989287">
Table 3 shows that by discarding word boundaries from
the ASR hypothesis, and then re-segmenting using our
MaxEnt segmenter, we obtained a better performance in
most cases. We believe that some re
duction in
segmentation errors due to recognition errors is obtained
this way; for example, in the ASR output, two words
“ 4 ” in “ VA N T M Vq + 4 ” are
misrecognized as one word “4 ”, which can be
corrected by re-segmentation.
</bodyText>
<subsectionHeader confidence="0.530265">
Post-classification effect
</subsectionHeader>
<bodyText confidence="0.998205857142857">
Table 3 also shows that the one-pass
identification/classification method yields better results
than the two-pass method. However, there are still
errors in the one-pass output where the bracketing is
correct, but the NE classification is wrong. In particular,
the type ORG is easily confusable with LOC in Chinese.
Both types of NEs tend to be rather long. We propose a
hybrid approach by first using the one-pass method to
extract NEs, and then removing all type information,
combining words of one NE to a whole NE-word and
post-classifying all the NE-words again. Our results in
Figure 1 show that the post-classification combined
with the one-pass approach performs much better on all
types.
</bodyText>
<table confidence="0.998387166666667">
PER LOC ORG
P R F P R F P R F
Newspaper .86 .76 .81 .87 .75 .81 .83 .83 .83
text
1-best ASR 22 .18 .20 .75 .79 .77 .43 .35 .39
hypothesis
</table>
<tableCaption confidence="0.9957905">
Table 1. NER results on Chinese speech data are worse
than on Chinese text data.
</tableCaption>
<table confidence="0.999902666666667">
Model Precision Recall F-measure
IBM HMM 77.51% 65.22% 70.83%
MaxEnt 77.3% 65.4% 70.9%
</table>
<tableCaption confidence="0.980953">
Table 2. Our NER baseline is comparable to the IBM
baseline.
</tableCaption>
<bodyText confidence="0.99996925">
possible to find a way to make use of the n-best ASR
output, in order to improve the NER performance.
However, we can expect it to be difficult to get
significant improvement since the same figure (Figure 1)
shows that precision drops much more quickly than
recall. This is because the nth hypothesis tends to have
more character errors than the (n-1)th hypothesis, which
may lead to more NER errors. Therefore the question is,
given n NE-tagged hypotheses, what is the best way to
use them to obtain a better NER overall performance
than by using the one-best hypothesis alone?
One simple approach is to allow all the hypotheses to
vote on a possible NE output. In simple voting, a
recognized named-entity is considered correct only
when it appears in more than 30 percent of the total
number of all the hypotheses for one utterance. The
result of this simple voting is shown in Table 4. Next,
we propose a mechanism of weighted voting using
confidence measure for each hypothesis. In one
experiment, we use the MaxEnt NER score as
confidence measure. In another experiment, we use all
the six scores (acoustic, language model, number of
words, number of phones, number of silence, or NER
score) provided by the BBN ASR system as confidence
measure. During implementation, an optimizer based on
Powell’s algorithm is used to find the 6 weights (ωk) for
each score (Sk). For any given hypothesis, confidence
measure is given by:
</bodyText>
<equation confidence="0.952282666666667">
6
W = LSk-COk
k=
</equation>
<bodyText confidence="0.975427">
The above confidence measure is then normalized into a
final confidence measure for each hypothesis:
</bodyText>
<table confidence="0.879005692307692">
1
1
i=
PER LOC ORG
P R F P R F P R F
2-pass, .23 .18 .20 .75 .79 .77 .43 .35 .39
word
1-pass, .25 .20 .21 .76 .84 .80 .70 .25 .36
word
2-pass, .53 .43 .48 .67 .70 .68 .75 .59 .66
character
1-pass, .60 .45 .52 .56 .69 .62 .55 .35 .43
character
</table>
<figure confidence="0.9715239375">
W=
iN
ˆ
exp( )
W i
exp( )
W l
Finally, an NE output is considered valid if
N
LW*8i(NE)&gt;0.3
NE occurs in the i th hypothesis
�
Otherwise
L
l=1
8i (NE) = r 1,
</figure>
<tableCaption confidence="0.992925">
Table 3. The character model is better than the word
model, and one-pass NER is better than two-pass.
</tableCaption>
<sectionHeader confidence="0.896158" genericHeader="method">
3. Using N-Best Lists to Improve NER
</sectionHeader>
<bodyText confidence="0.999696625">
Miller et al. (1999) performed NER on the one-best
hypothesis of English Broadcast News data. Palmer &amp;
Ostendorf (2001) and Horlock &amp; King (2003) carried
out English NER on word lattices. We are interested in
investigating how to best utilize the n-best hypothesis
from the ASR system to improve NER performances.
From Figure 1, we can see that recall increases as the
number of hypotheses increases. Thus it would appear
</bodyText>
<subsectionHeader confidence="0.984089">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999865">
We use the n-best hypothesis of 1,046 Broadcast News
Chinese utterances from the BBN LVCSR system. n
ranges from one to 300, averaging at 68. Each utterance
has a reference transcription with no recognition error.
</bodyText>
<subsectionHeader confidence="0.863478">
3.2 Results and discussion
</subsectionHeader>
<bodyText confidence="0.978870933333333">
Table 4 presents the NER results for the reference
sentence, one best hypothesis, and different n-best
voting methods. Results for the reference sentences
show the upper bound performance (68% F-measure) of
applying a MaxEnt NER system trained from the
Chinese text corpus (e.g., PFR) to Chinese speech
output (e.g., Broadcast News). From Table 4, we can
conclude that it is possible to improve NER precision by
using n-best hypothesis by finding the optimized
combination of different acoustic, language model,
NER, and other scores. In particular, since most errors
in Chinese ASR seem to be for person names, using
NER score on the n-best hypotheses can improve
recognition results by a relative 6.7% in precision and
1.7% in F-measure.
</bodyText>
<table confidence="0.994282846153846">
Results PER LOC ORG
F P F P F P
Reference 0.71 0.75 0.78 0.77 0.56 0.72
sentence
One best 0.46 0.50 0.75 0.74 0.54 0.69
n-best 0.45 0.59 0.76 0.75 0.56 0.71
simple vote
n-best 0.46 0.59 0.77 0.76 0.55 0.71
weighted vote
(NE score)
n-best 0.48 0.53 0.75 0.73 0.55 0.69
weighted vote
(all scores)
</table>
<tableCaption confidence="0.99171">
Table 4. n-best weighted voting with NE score gives the
best performance.
</tableCaption>
<sectionHeader confidence="0.996104" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.986934526315789">
We present the first known result for named entity
recognition (NER) in realistic large-vocabulary spoken
Chinese. We apply a maximum entropy (MaxEnt)
based system to the n-best output of the BBN LVCSR
system on Chinese Broadcast News utterances. Our
results support the claim that transferring NER
approaches from text to spoken language is a
significantly more difficult task for Chinese than for
English. We show that re-segmenting the ASR
hypotheses improves the NER performance by 24%. We
also show that applying post-classification improves the
NER performance by 13%. Finally, we introduce a
method of using n-best hypotheses that yields a useful
6.7% relative improvement in NER precision, and 1.7%
relative improvement in F-measure, by weighted voting.
Acknowledgements. We would like to thank the Hong Kong
Research Grants Council (RGC) for supporting this research
in part via grants HKUST6206/03E, HKUST6256/00E,
HKUST6083/99E, DAG03/04.EG30, and DAG03/04.EG09.
</bodyText>
<figure confidence="0.36886">
Recall
</figure>
<figureCaption confidence="0.985213">
Figure 1. Post-classification improves NER performance.
</figureCaption>
<reference confidence="0.992665142857143">
Institute of Computational Linguistics, Beijing University. 2001. The PFR
corpus. http://icl.pku.edu.cn/research/corpus/shengming.htm.
Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and Abraham
ITTYCHERIAH. 2003. HowtogetaChineseName(Entity): Segmentation and
combination issues. Proceedings of EMNLP. Sapporo, Japan: July 2003.
David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE.
1999. Named entity extraction from broadcast news. Proceedings of the
DARPA Broadcast News Workshop. Herndon, Virginia: 1999. 37-40.
David D. PALMER, Mari OSTENDORF and John D. BURGER. 1999. Robust
information extraction from spoken language data. Proceedings of
Eurospeech 1999. Sep 1999.
Jian SUN, Ming ZHOU and Jianfeng GAO. 2003. A class-based language model
approach to Chinese named entity identification. Computational Linguistics
and Chinese Language Processing. 2003.
</reference>
<figure confidence="0.702157666666667">
Precision
F-measure
Erik F. TJONG KIM SANG. 2002. Introduction to the CoNLL-2002 Shared Task:
</figure>
<sectionHeader confidence="0.986396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9975252">
Language-independent named entity recognition. Proceedings of CoNLL-
2002. Taipei, Taiwan: 2002. 155-158.
Erik F. TJONG KIM SANG and Fien DE MEULDER. 2003. Introduction to the
CoNLL-2003 Shared Task: Language-Independent Named Entity
Recognition. Proceedings of CoNLL-2003. Edmonton, Canada. 142-147.
Silviu CUCERZAN and David YAROWSKY. 1999. Language independent named
entity recognition combining morphological and contextual evidence.
Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC.
University of Maryland, MD.
James HORLOCK and Simon KING. 2003. Discriminative Methods for Improving
Named Entity Extraction on Speech Data. Proceedings of Eurospeech 2003.
Geneva.
Dekai WU, Grace NGAI and Marine CARPUAT. 2003. A Stacked, Voted, Stacked
Model for Named Entity Recognition. Proceedings of CoNLL-2003.
Edmonton, Canada: 2003. 200-203.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.135137">
<title confidence="0.999974">Using N-best Lists for Named Entity Recognition from Chinese Speech</title>
<author confidence="0.992895">Pascale Richard Marine Dekai</author>
<affiliation confidence="0.847305333333333">BBN Land Columbia, MD</affiliation>
<email confidence="0.996229">schwartz@bbn.com</email>
<affiliation confidence="0.8779695">Human Language Technology Electrical &amp; Electronic University of Science and</affiliation>
<address confidence="0.97532">Clear Water Bay, Hong</address>
<email confidence="0.967552">lfzhai@ee.ust.hk</email>
<email confidence="0.967552">pascale@ee.ust.hk</email>
<affiliation confidence="0.7107005">Human Language Technology Department of Computer University of Science and</affiliation>
<address confidence="0.976636">Clear Water Bay, Hong</address>
<email confidence="0.988129">marine@cs.ust.hk</email>
<email confidence="0.988129">dekai@cs.ust.hk</email>
<abstract confidence="0.999158521739131">We present the first known result for named entity recognition (NER) in realistic largevocabulary spoken Chinese. We establish this result by applying a maximum entropy model, currently the single best known approach for textual Chinese NER, to the recognition output of the BBN LVCSR system on Chinese Broadcast News utterances. Our results support the claim that transferring NER approaches from text to spoken language is a significantly more difficult task for Chinese than for English. We propose re-segmenting the ASR hypotheses as well as applying postclassification to improve the performance. we introduce a method of using hypotheses that yields a small but nevertheless useful improvement NER accuracy. We use acoustic, phonetic, language model, NER and other scores as confidence measure. Experimental results show an average of 6.7% relative improvement in precision and 1.7% relative improvement in F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>2001</date>
<institution>Institute of Computational Linguistics, Beijing University.</institution>
<note>The PFR corpus. http://icl.pku.edu.cn/research/corpus/shengming.htm.</note>
<contexts>
<context position="12473" citStr="(2001)" startWordPosition="2095" endWordPosition="2095"> .20 .75 .79 .77 .43 .35 .39 word 1-pass, .25 .20 .21 .76 .84 .80 .70 .25 .36 word 2-pass, .53 .43 .48 .67 .70 .68 .75 .59 .66 character 1-pass, .60 .45 .52 .56 .69 .62 .55 .35 .43 character W= iN ˆ exp( ) W i exp( ) W l Finally, an NE output is considered valid if N LW*8i(NE)&gt;0.3 NE occurs in the i th hypothesis � Otherwise L l=1 8i (NE) = r 1, Table 3. The character model is better than the word model, and one-pass NER is better than two-pass. 3. Using N-Best Lists to Improve NER Miller et al. (1999) performed NER on the one-best hypothesis of English Broadcast News data. Palmer &amp; Ostendorf (2001) and Horlock &amp; King (2003) carried out English NER on word lattices. We are interested in investigating how to best utilize the n-best hypothesis from the ASR system to improve NER performances. From Figure 1, we can see that recall increases as the number of hypotheses increases. Thus it would appear 3.1 Experimental setup We use the n-best hypothesis of 1,046 Broadcast News Chinese utterances from the BBN LVCSR system. n ranges from one to 300, averaging at 68. Each utterance has a reference transcription with no recognition error. 3.2 Results and discussion Table 4 presents the NER results </context>
</contexts>
<marker>2001</marker>
<rawString>Institute of Computational Linguistics, Beijing University. 2001. The PFR corpus. http://icl.pku.edu.cn/research/corpus/shengming.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan JING</author>
<author>Radu FLORIAN</author>
<author>Xiaoqiang LUO</author>
<author>Tong ZHANG</author>
<author>Abraham ITTYCHERIAH</author>
</authors>
<title>HowtogetaChineseName(Entity): Segmentation and combination issues.</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<location>Sapporo, Japan:</location>
<marker>JING, FLORIAN, LUO, ZHANG, ITTYCHERIAH, 2003</marker>
<rawString>Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and Abraham ITTYCHERIAH. 2003. HowtogetaChineseName(Entity): Segmentation and combination issues. Proceedings of EMNLP. Sapporo, Japan: July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David MILLER</author>
<author>Richard SCHWARTZ</author>
<author>Ralph WEISCHEDEL</author>
<author>Rebecca STONE</author>
</authors>
<title>Named entity extraction from broadcast news.</title>
<date>1999</date>
<booktitle>Proceedings of the DARPA Broadcast News Workshop.</booktitle>
<pages>37--40</pages>
<location>Herndon, Virginia:</location>
<marker>MILLER, SCHWARTZ, WEISCHEDEL, STONE, 1999</marker>
<rawString>David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE. 1999. Named entity extraction from broadcast news. Proceedings of the DARPA Broadcast News Workshop. Herndon, Virginia: 1999. 37-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D PALMER</author>
<author>Mari OSTENDORF</author>
<author>John D BURGER</author>
</authors>
<title>Robust information extraction from spoken language data.</title>
<date>1999</date>
<booktitle>Proceedings of Eurospeech</booktitle>
<marker>PALMER, OSTENDORF, BURGER, 1999</marker>
<rawString>David D. PALMER, Mari OSTENDORF and John D. BURGER. 1999. Robust information extraction from spoken language data. Proceedings of Eurospeech 1999. Sep 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian SUN</author>
<author>Ming ZHOU</author>
<author>Jianfeng GAO</author>
</authors>
<title>A class-based language model approach to Chinese named entity identification. Computational Linguistics and Chinese Language Processing.</title>
<date>2003</date>
<marker>SUN, ZHOU, GAO, 2003</marker>
<rawString>Jian SUN, Ming ZHOU and Jianfeng GAO. 2003. A class-based language model approach to Chinese named entity identification. Computational Linguistics and Chinese Language Processing. 2003.</rawString>
</citation>
<citation valid="true">
<title>Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>Proceedings of CoNLL2002.</booktitle>
<pages>155--158</pages>
<location>Taipei, Taiwan:</location>
<marker>2002</marker>
<rawString>Language-independent named entity recognition. Proceedings of CoNLL2002. Taipei, Taiwan: 2002. 155-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F TJONG KIM SANG</author>
<author>Fien DE MEULDER</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-2003.</booktitle>
<pages>142--147</pages>
<location>Edmonton,</location>
<marker>SANG, DE MEULDER, 2003</marker>
<rawString>Erik F. TJONG KIM SANG and Fien DE MEULDER. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. Proceedings of CoNLL-2003. Edmonton, Canada. 142-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu CUCERZAN</author>
<author>David YAROWSKY</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and</booktitle>
<institution>VLC. University of Maryland, MD.</institution>
<marker>CUCERZAN, YAROWSKY, 1999</marker>
<rawString>Silviu CUCERZAN and David YAROWSKY. 1999. Language independent named entity recognition combining morphological and contextual evidence. Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James HORLOCK</author>
<author>Simon KING</author>
</authors>
<title>Discriminative Methods for Improving Named Entity Extraction on Speech Data.</title>
<date>2003</date>
<booktitle>Proceedings of Eurospeech 2003.</booktitle>
<location>Geneva.</location>
<marker>HORLOCK, KING, 2003</marker>
<rawString>James HORLOCK and Simon KING. 2003. Discriminative Methods for Improving Named Entity Extraction on Speech Data. Proceedings of Eurospeech 2003. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
<author>Grace NGAI</author>
<author>Marine CARPUAT</author>
</authors>
<title>A Stacked, Voted, Stacked Model for Named Entity Recognition.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-2003.</booktitle>
<pages>200--203</pages>
<location>Edmonton, Canada:</location>
<marker>WU, NGAI, CARPUAT, 2003</marker>
<rawString>Dekai WU, Grace NGAI and Marine CARPUAT. 2003. A Stacked, Voted, Stacked Model for Named Entity Recognition. Proceedings of CoNLL-2003. Edmonton, Canada: 2003. 200-203.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>