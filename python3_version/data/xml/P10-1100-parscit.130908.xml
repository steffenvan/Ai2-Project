<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003234">
<title confidence="0.989066">
Learning Script Knowledge with Web Experiments
</title>
<author confidence="0.996074">
Michaela Regneri Alexander Koller Manfred Pinkal
</author>
<affiliation confidence="0.97199">
Department of Computational Linguistics and Cluster of Excellence
Saarland University, Saarbr¨ucken
</affiliation>
<email confidence="0.987624">
{regneri|koller|pinkal}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.993665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994525">
We describe a novel approach to unsuper-
vised learning of the events that make up
a script, along with constraints on their
temporal ordering. We collect natural-
language descriptions of script-specific
event sequences from volunteers over the
Internet. Then we compute a graph rep-
resentation of the script’s temporal struc-
ture using a multiple sequence alignment
algorithm. The evaluation of our system
shows that we outperform two informed
baselines.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974787878788">
A script is “a standardized sequence of events that
describes some stereotypical human activity such
as going to a restaurant or visiting a doctor” (Barr
and Feigenbaum, 1981). Scripts are fundamental
pieces of commonsense knowledge that are shared
between the different members of the same cul-
ture, and thus a speaker assumes them to be tac-
itly understood by a hearer when a scenario re-
lated to a script is evoked: When one person says
“I’m going shopping”, it is an acceptable reply
to say “did you bring enough money?”, because
the SHOPPING script involves a ‘payment’ event,
which again involves the transfer of money.
It has long been recognized that text under-
standing systems would benefit from the implicit
information represented by a script (Cullingford,
1977; Mueller, 2004; Miikkulainen, 1995). There
are many other potential applications, includ-
ing automated storytelling (Swanson and Gordon,
2008), anaphora resolution (McTear, 1987), and
information extraction (Rau et al., 1989).
However, it is also commonly accepted that the
large-scale manual formalization of scripts is in-
feasible. While there have been a few attempts at
doing this (Mueller, 1998; Gordon, 2001), efforts
in which expert annotators create script knowledge
bases clearly don’t scale. The same holds true of
the script-like structures called “scenario frames”
in FrameNet (Baker et al., 1998).
There has recently been a surge of interest in
automatically learning script-like knowledge re-
sources from corpora (Chambers and Jurafsky,
2008b; Manshadi et al., 2008); but while these
efforts have achieved impressive results, they are
limited by the very fact that a lot of scripts – such
as SHOPPING – are shared implicit knowledge, and
their events are therefore rarely elaborated in text.
In this paper, we propose a different approach
to the unsupervised learning of script-like knowl-
edge. We focus on the temporal event structure of
scripts; that is, we aim to learn what phrases can
describe the same event in a script, and what con-
straints must hold on the temporal order in which
these events occur. We approach this problem by
asking non-experts to describe typical event se-
quences in a given scenario over the Internet. This
allows us to assemble large and varied collections
of event sequence descriptions (ESDs), which are
focused on a single scenario. We then compute a
temporal script graph for the scenario by identify-
ing corresponding event descriptions using a Mul-
tiple Sequence Alignment algorithm from bioin-
formatics, and converting the alignment into a
graph. This graph makes statements about what
phrases can describe the same event of a scenario,
and in what order these events can take place. Cru-
cially, our algorithm exploits the sequential struc-
ture of the ESDs to distinguish event descriptions
that occur at different points in the script storyline,
even when they are semantically similar. We eval-
uate our script graph algorithm on ten unseen sce-
narios, and show that it significantly outperforms
a clustering-based baseline.
The paper is structured as follows. We will
first position our research in the landscape of re-
lated work in Section 2. We will then define how
</bodyText>
<page confidence="0.980474">
979
</page>
<note confidence="0.9436725">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999304666666667">
we understand scripts, and what aspect of scripts
we model here, in Section 3. Section 4 describes
our data collection method, and Section 5 explains
how we use Multiple Sequence Alignment to com-
pute a temporal script graph. We evaluate our sys-
tem in Section 6 and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999862508474576">
Approaches to learning script-like knowledge are
not new. For instance, Mooney (1990) describes
an early attempt to acquire causal chains, and
Smith and Arnold (2009) use a graph-based algo-
rithm to learn temporal script structures. However,
to our knowledge, such approaches have never
been shown to generalize sufficiently for wide
coverage application, and none of them was rig-
orously evaluated.
More recently, there have been a number of ap-
proaches to automatically learning event chains
from corpora (Chambers and Jurafsky, 2008b;
Chambers and Jurafsky, 2009; Manshadi et al.,
2008). These systems typically employ a method
for classifying temporal relations between given
event descriptions (Chambers et al., 2007; Cham-
bers and Jurafsky, 2008a; Mani et al., 2006).
They achieve impressive performance at extract-
ing high-level descriptions of procedures such as
a CRIMINAL PROCESS. Because our approach in-
volves directly asking people for event sequence
descriptions, it can focus on acquiring specific
scripts from arbitrary domains, and we can con-
trol the level of granularity at which scripts are
described. Furthermore, we believe that much
information about scripts is usually left implicit
in texts and is therefore easier to learn from our
more explicit data. Finally, our system automat-
ically learns different phrases which describe the
same event together with the temporal ordering
constraints.
Jones and Thompson (2003) describe an ap-
proach to identifying different natural language re-
alizations for the same event considering the tem-
poral structure of a scenario. However, they don’t
aim to acquire or represent the temporal structure
of the whole script in the end.
In its ability to learn paraphrases using Mul-
tiple Sequence Alignment, our system is related
to Barzilay and Lee (2003). Unlike Barzilay and
Lee, we do not tackle the general paraphrase prob-
lem, but only consider whether two phrases de-
scribe the same event in the context of the same
script. Furthermore, the atomic units of our align-
ment process are entire phrases, while in Barzilay
and Lee’s setting, the atomic units are words.
Finally, it is worth pointing out that our work
is placed in the growing landscape of research
that attempts to learn linguistic information out of
data directly collected from users over the Inter-
net. Some examples are the general acquisition of
commonsense knowledge (Singh et al., 2002), the
use of browser games for that purpose (von Ahn
and Dabbish, 2008), and the collaborative anno-
tation of anaphoric reference (Chamberlain et al.,
2009). In particular, the use of the Amazon Me-
chanical Turk, which we use here, has been evalu-
ated and shown to be useful for language process-
ing tasks (Snow et al., 2008).
</bodyText>
<sectionHeader confidence="0.998473" genericHeader="method">
3 Scripts
</sectionHeader>
<bodyText confidence="0.998149848484849">
Before we delve into the technical details, let us
establish some terminology. In this paper, we dis-
tinguish scenarios, as classes of human activities,
from scripts, which are stereotypical models of the
internal structure of these activities. Where EAT-
ING IN A RESTAURANT is a scenario, the script
describes a number of events, such as ordering and
leaving, that must occur in a certain order in order
to constitute an EATING IN A RESTAURANT activ-
ity. The classical perspective on scripts (Schank
and Abelson, 1977) has been that next to defin-
ing some events with temporal constraints, a script
also defines their participants and their causal con-
nections.
Here we focus on the narrower task of learning
the events that a script consists of, and of model-
ing and learning the temporal ordering constraints
that hold between them. Formally, we will spec-
ify a script (in this simplified sense) in terms of a
directed graph G3 = (E37 T3), where E3 is a set
of nodes representing the events of a scenario s,
and T3 is a set of edges (ez7 ek) indicating that the
event ez typically happens before ek in s. We call
G3 the temporal script graph (TSG) for s.
Each event in a TSG can usually be expressed
with many different natural-language phrases. As
the TSG in Fig. 3 illustrates, the first event in the
script for EATING IN A FAST FOOD RESTAURANT
can be equivalently described as ‘walk to the
counter’ or ‘walk up to the counter’; even phrases
like ‘walk into restaurant’, which would not usu-
ally be taken as paraphrases of these, can be ac-
cepted as describing the same event in the context
</bodyText>
<page confidence="0.981715">
980
</page>
<listItem confidence="0.999679458333333">
1. look at menu 1. walk into restaurant
2. decide what you want 2. find the end of the line
3. order at counter 3. stand in line
4. pay at counter 4. look at menu board
5. receive food at counter 5. decide on food and drink
6. take food to table 6. tell cashier your order
7. eat food 7. listen to cashier repeat order
8. listen for total price
9. swipe credit card in scanner
10. put up credit card
11. take receipt
12. look at order number
13. take your cup
14. stand off to the side
15. wait for number to be called
16. get your drink
1. walk to the counter
2. place an order
3. pay the bill
4. wait for the ordered food
5. get the food
6. move to a table
7. eat food
8. exit the place
</listItem>
<figureCaption confidence="0.998585">
Figure 1: Three event sequence descriptions
</figureCaption>
<bodyText confidence="0.999962842105263">
of this scenario. We call a natural-language real-
ization of an individual event in the script an event
description, and we call a sequence of event de-
scriptions that form one particular instance of the
script an event sequence description (ESD). Ex-
amples of ESDs for the FAST FOOD RESTAURANT
script are shown in Fig. 1.
One way to look at a TSG is thus that its nodes
are equivalence classes of different phrases that
describe the same event; another is that valid ESDs
can be generated from a TSG by randomly select-
ing phrases from some nodes and arranging them
in an order that respects the temporal precedence
constraints in T3. Our goal in this paper is to take
a set of ESDs for a given scenario as our input
and then compute a TSG that clusters different de-
scriptions of the same event into the same node,
and contains edges that generalize the temporal in-
formation encoded in the ESDs.
</bodyText>
<sectionHeader confidence="0.988087" genericHeader="method">
4 Data Acquisition
</sectionHeader>
<bodyText confidence="0.999939">
In order to automatically learn TSGs, we selected
22 scenarios for which we collect ESDs. We de-
liberately included scenarios of varying complex-
ity, including some that we considered hard to
describe (CHILDHOOD, CREATE A HOMEPAGE),
scenarios with highly variable orderings between
events (MAKING SCRAMBLED EGGS), and sce-
narios for which we expected cultural differences
(WEDDING).
We used the Amazon Mechanical Turk1 to col-
lect the data. For every scenario, we asked 25 peo-
ple to enter a typical sequence of events in this sce-
nario, in temporal order and in “bullet point style”.
</bodyText>
<footnote confidence="0.940753">
1http://www.mturk.com/
</footnote>
<bodyText confidence="0.999965736842105">
We required the annotators to enter at least 5 and
at most 16 events. Participants were allowed to
skip a scenario if they felt unable to enter events
for it, but had to indicate why. We did not restrict
the participants (e.g. to native speakers).
In this way, we collected 493 ESDs for the 22
scenarios. People used the possibility to skip a
form 57 times. The most frequent explanation for
this was that they didn’t know how a certain sce-
nario works: The scenario with the highest pro-
portion of skipped forms was CREATE A HOME-
PAGE, whereas MAKING SCRAMBLED EGGS was
the only one in which nobody skipped a form. Be-
cause we did not restrict the participants’ inputs,
the data was fairly noisy. For the purpose of this
study, we manually corrected the data for orthog-
raphy and filtered out forms that were written in
broken English or did not comply with the task
(e.g. when users misunderstood the scenario, or
did not list the event descriptions in temporal or-
der). Overall we discarded 15% of the ESDs.
Fig. 1 shows three of the ESDs we collected
for EATING IN A FAST-FOOD RESTAURANT. As
the example illustrates, descriptions differ in their
starting points (‘walk into restaurant’ vs. ‘walk to
counter’), the granularity of the descriptions (‘pay
the bill’ vs. event descriptions 8–11 in the third
sequence), and the events that are mentioned in
the sequence (not even ‘eat food’ is mentioned in
all ESDs). Overall, the ESDs we collected con-
sisted of 9 events on average, but their lengths var-
ied widely: For most scenarios, there were sig-
nificant numbers of ESDs both with the minimum
length of 5 and the maximum length of 16 and ev-
erything in between. Combined with the fact that
93% of all individual event descriptions occurred
only once, this makes it challenging to align the
different ESDs with each other.
</bodyText>
<sectionHeader confidence="0.98673" genericHeader="method">
5 Temporal Script Graphs
</sectionHeader>
<bodyText confidence="0.999951777777778">
We will now describe how we compute a temporal
script graph out of the collected data. We proceed
in two steps. First, we identify phrases from dif-
ferent ESDs that describe the same event by com-
puting a Multiple Sequence Alignment (MSA) of
all ESDs for the same scenario. Then we postpro-
cess the MSA and convert it into a temporal script
graph, which encodes and generalizes the tempo-
ral information contained in the original ESDs.
</bodyText>
<page confidence="0.949994">
981
</page>
<bodyText confidence="0.993404458333333">
row s1 s2 s3 s4
1 0 walk into restaurant 0 enter restaurant
2 0 0 walk to the counter go to counter
3 0 find the end of the line 0 0
4 0 stand in line 0 0
5 look at menu look at menu board 0 0
6 decide what you want decide on food and drink 0 make selection
7 order at counter tell cashier your order place an order place order
8 0 listen to cashier repeat order 0 0
9 pay at counter 0 pay the bill pay for food
10 0 listen for total price 0 0
11 0 swipe credit card in scanner 0 0
12 0 put up credit card 0 0
13 0 take receipt 0 0
14 0 look at order number 0 0
15 0 take your cup 0 0
16 0 stand off to the side 0 0
17 0 wait for number to be called wait for the ordered food 0
18 receive food at counter get your drink get the food pick up order
19 0 0 0 pick up condiments
20 take food to table 0 move to a table go to table
21 eat food 0 eat food consume food
22 0 0 0 clear tray
22 0 0 exit the place 0
</bodyText>
<figureCaption confidence="0.99447">
Figure 2: A MSA of four event sequence descriptions
</figureCaption>
<subsectionHeader confidence="0.988942">
5.1 Multiple Sequence Alignment
</subsectionHeader>
<bodyText confidence="0.984289913043478">
The problem of computing Multiple Sequence
Alignments comes from bioinformatics, where it
is typically used to find corresponding elements in
proteins or DNA (Durbin et al., 1998).
A sequence alignment algorithm takes as its in-
put some sequences s1, ... , sn E E* over some al-
phabet E, along with a cost function cm : ExE —*
R for substitutions and gap costs cgap E R for in-
sertions and deletions. In bioinformatics, the ele-
ments of E could be nucleotides and a sequence
could be a DNA sequence; in our case, E contains
the individual event descriptions in our data, and
the sequences are the ESDs.
A Multiple Sequence Alignment A of these se-
quences is then a matrix as in Fig. 2: The i-th col-
umn of A is the sequence si, possibly with some
gaps (“0”) interspersed between the symbols of
si, such that each row contains at least one non-
gap. If a row contains two non-gaps, we take these
symbols to be aligned; aligning a non-gap with a
gap can be thought of as an insertion or deletion.
Each sequence alignment A can be assigned a
cost c(A) in the following way:
</bodyText>
<equation confidence="0.995816333333333">
c(A) = cgap · Em + n �m �m cm(aji, aki)
i=1 j=1� k=j+1�
ajii4O akiOO
</equation>
<bodyText confidence="0.9999925">
where Em is the number of gaps in A, n is the
number of rows and m the number of sequences.
In other words, we sum up the alignment cost for
any two symbols from E that are aligned with
each other, and add the gap cost for each gap.
There is an algorithm that computes cheapest pair-
wise alignments (i.e. n = 2) in polynomial time
(Needleman and Wunsch, 1970). For n &gt; 2, the
problem is NP-complete, but there are efficient al-
gorithms that approximate the cheapest MSAs by
aligning two sequences first, considering the result
as a single sequence whose elements are pairs, and
repeating this process until all sequences are incor-
porated in the MSA (Higgins and Sharp, 1988).
</bodyText>
<subsectionHeader confidence="0.999436">
5.2 Semantic similarity
</subsectionHeader>
<bodyText confidence="0.999953909090909">
In order to apply MSA to the problem of aligning
ESDs, we choose E to be the set of all individ-
ual event descriptions in a given scenario. Intu-
itively, we want the MSA to prefer the alignment
of two phrases if they are semantically similar, i.e.
it should cost more to align ‘exit’ with ‘eat’ than
‘exit’ with ‘leave’. Thus we take a measure of se-
mantic (dis)similarity as the cost function cm.
The phrases to be compared are written in
bullet-point style. They are typically short and
elliptic (no overt subject), they lack determiners
and use infinitive or present progressive form for
the main verb. Also, the lexicon differs consider-
ably from usual newspaper corpora. For these rea-
sons, standard methods for similarity assessment
are not straightforwardly applicable: Simple bag-
of-words approaches do not provide sufficiently
good results, and standard taggers and parsers can-
not process our descriptions with sufficient accu-
racy.
We therefore employ a simple, robust heuristics,
which is tailored to our data and provides very
</bodyText>
<page confidence="0.98563">
982
</page>
<figureCaption confidence="0.997588">
wait in line
look at menu board
wait in line to order my food
examine menu board
look at the menu
look at menu
</figureCaption>
<bodyText confidence="0.867485181818182">
walk into the reasturant
walk up to the counter
walk into restaurant
go to restaurant
walk to the counter
get in line
enter restaurant
stand in line
i decide what i want
decide what to eat
decide on food and drink
decide on what to order
make selection
decide what you want
go to cashier
go to ordering counter
go to counter
order food
i order it
tell cashier your order
order items from wall menu
order my food
place an order
order at counter
place order
pay at counter
pay for the food
pay for food
give order to the employee
pay the bill
pay
pay for the food and drinks
pay for order
make payment
keep my receipt
take receipt
collect utensils
pay for order
pick up order
wait for my order
look at prices
wait
look at order number
wait for order to be done
wait for food to be ready
wait for order
wait for the ordered food
expect order
wait for food
move to a table
sit down
wait for number to be called
seat at a table
sit down at table
leave
pick up condiments
take your cup
receive food
take food to table
receive tray with order
get condiments
get the food
receive food at counter
pick up food when ready
get my order
get food
</bodyText>
<figureCaption confidence="0.993392">
Figure 3: An extract from the graph computed for EATING IN A FAST FOOD RESTAURANT
</figureCaption>
<bodyText confidence="0.9997332">
shallow dependency-style syntactic information.
We identify the first potential verb of the phrase
(according to the POS information provided by
WordNet) as the predicate, the preceding noun (if
any) as subject, and all following potential nouns
as objects. (With this fairly crude tagging method,
we also count nouns in prepositional phrases as
“objects”.)
On the basis of this pseudo-parse, we compute
the similarity measure sim:
</bodyText>
<equation confidence="0.725462">
sim = α · pred + 0 · subj + -y · obj
</equation>
<bodyText confidence="0.999988608695652">
where pred, subj, and obj are the similarity val-
ues for predicates, subjects and objects respec-
tively, and α, 0, -y are weights. If a constituent
is not present in one of the phrases to compare,
we set its weight to zero and redistribute it over
the other weights. We fix the individual simi-
larity scores pred, subj, and obj depending on
the WordNet relation between the most similar
WordNet senses of the respective lemmas (100 for
synonyms, 0 for lemmas without any relation, and
intermediate numbers for different kind of Word-
Net links).
We optimized the values for pred, subj, and
obj as well as the weights α, 0 and -y using a
held-out development set of scenarios. Our exper-
iments showed that in most cases, the verb con-
tributes the largest part to the similarity (accord-
ingly, α needs to be higher than the other factors).
We achieved improved accuracy by distinguishing
a class of verbs that contribute little to the meaning
of the phrase (i.e., support verbs, verbs of move-
ment, and the verb “get”), and assigning them a
separate, lower α.
</bodyText>
<subsectionHeader confidence="0.998809">
5.3 Building Temporal Script Graphs
</subsectionHeader>
<bodyText confidence="0.999980722222222">
We can now compute a low-cost MSA for each
scenario out of the ESDs. From this alignment, we
extract a temporal script graph, in the following
way. First, we construct an initial graph which has
one node for each row of the MSA as in Fig. 2. We
interpret each node of the graph as representing
a single event in the script, and the phrases that
are collected in the node as different descriptions
of this event; that is, we claim that these phrases
are paraphrases in the context of this scenario. We
then add an edge (u, v) to the graph iff (1) u =�
v, (2) there was at least one ESD in the original
data in which some phrase in u directly preceded
some phrase in v, and (3) if a single ESD contains
a phrase from u and from v, the phrase from u
directly precedes the one from v. In terms of the
MSA, this means that if a phrase from u comes
from the same column as a phrase from v, there
are at most some gaps between them. This initial
graph represents exactly the same information as
the MSA, in a different notation.
The graph is automatically post-processed in
a second step to simplify it and eliminate noise
that caused MSA errors. At first we prune spu-
rious nodes which contain only one event descrip-
tion. Then we refine the graph by merging nodes
whose elements should have been aligned in the
first place but were missed by the MSA. We merge
two nodes if they satisfy certain structural and se-
mantic constraints.
The semantic constraints check whether the
event descriptions of the merged node would be
sufficiently consistent according to the similarity
measure from Section 5.2. To check whether we
can merge two nodes u and v, we use an unsuper-
vised clustering algorithm (Flake et al., 2004) to
</bodyText>
<page confidence="0.998392">
983
</page>
<bodyText confidence="0.999932888888889">
first cluster the event descriptions in u and v sep-
arately. Then we combine the event descriptions
from u and v and cluster the resulting set. If the
union has more clusters than either u or v, we as-
sume the nodes to be too dissimilar for merging.
The structural constraints depend on the graph
structure. We only merge two nodes u and v if
their event descriptions come from different se-
quences and one of the following conditions holds:
</bodyText>
<listItem confidence="0.995494833333333">
• u and v have the same parent;
• u has only one parent, v is its only child;
• v has only one child and is the only child of
u;
• all children of u (except for v) are also chil-
dren of v.
</listItem>
<bodyText confidence="0.9997065">
These structural constraints prevent the merg-
ing algorithm from introducing new temporal re-
lations that are not supported by the input ESDs.
We take the output of this post-processing step
as the temporal script graph. An excerpt of the
graph we obtain for our running example is shown
in Fig. 3. One node created by the node merg-
ing step was the top left one, which combines one
original node containing ‘walk into restaurant’ and
another with ‘go to restaurant’. The graph mostly
groups phrases together into event nodes quite
well, although there are some exceptions, such as
the ‘collect utensils’ node. Similarly, the tempo-
ral information in the graph is pretty accurate. But
perhaps most importantly, our MSA-based algo-
rithm manages to keep similar phrases like ‘wait
in line’ and ‘wait for my order’ apart by exploiting
the sequential structure of the input ESDs.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999905">
We evaluated the two core aspects of our sys-
tem: its ability to recognize descriptions of the
same event (paraphrases) and the resulting tem-
poral constraints it defines on the event descrip-
tions (happens-before relation). We compare our
approach to two baseline systems and show that
our system outperforms both baselines and some-
times even comes close to our upper bound.
</bodyText>
<subsectionHeader confidence="0.997197">
6.1 Method
</subsectionHeader>
<bodyText confidence="0.999990390243902">
We selected ten scenarios which we did not use
for development purposes, five of them taken from
the corpus described in Section 4, the other five
from the OMICS corpus.2 The OMICS corpus is a
freely available, web-collected corpus by the Open
Mind Initiative (Singh et al., 2002). It contains
several stories (Pz� scenarios) consisting of multi-
ple ESDs. The corpus strongly resembles ours in
language style and information provided, but is re-
stricted to “indoor activities” and contains much
more data than our collection (175 scenarios with
more than 40 ESDs each).
For each scenario, we created a paraphrase set
out of 30 randomly selected pairs of event de-
scriptions which the system classified as para-
phrases and 30 completely random pairs. The
happens-before set consisted of 30 pairs classified
as happens-before, 30 random pairs and addition-
ally all 60 pairs in reverse order. We added the
reversed pairs to check whether the raters really
prefer one direction or whether they accept both
and were biased by the order of presentation.
We presented each pair to 5 non-experts, all
US residents, via Mechanical Turk. For the para-
phrase set, an exemplary question we asked the
rater looks as follows, instantiating the Scenario
and the two descriptions to compare appropriately:
Imagine two people, both telling a story
about SCENARIO. Could the first one
say event2 to describe the same part of
the story that the second one describes
with events ?
For the happens-before task, the question template
was the following:
Imagine somebody telling a story about
SCENARIO in which the events events
and event2 occur. Would events nor-
mally happen before event2?
We constructed a gold standard by a majority deci-
sion of the raters. An expert rater adjudicated the
pairs with a 3:2 vote ratio.
</bodyText>
<subsectionHeader confidence="0.999785">
6.2 Upper Bound and Baselines
</subsectionHeader>
<bodyText confidence="0.999272428571429">
To show the contributions of the different system
components, we implemented two baselines:
Clustering Baseline: We employed an unsu-
pervised clustering algorithm (Flake et al., 2004)
and fed it all event descriptions of a scenario. We
first created a similarity graph with one node per
event description. Each pair of nodes is connected
</bodyText>
<footnote confidence="0.978595">
2http://openmind.hri-us.com/
</footnote>
<page confidence="0.989191">
984
</page>
<table confidence="0.934156526315789">
sys PRECISION RECALL F-SCORE
basecl baselev sys basecl baselev sys basecl baselev upper
0.52 0.43 0.50 0.84 0.89 0.11 0.64 0.58 9 0.17 0.60
0.70 0.42 0.75 0.88 1.00 0.25 0.78 9 0.59 9 0.38 9 0.92
0.52 0.32 1.00 0.94 1.00 0.12 0.67 9 0.48 9 0.21 9 0.82
0.58 0.34 0.50 0.86 0.95 0.10 0.69 9 0.50 9 0.16 9 0.91
0.65 0.42 0.40 0.87 1.00 0.09 0.74 9 0.59 9 0.14 9 0.88
0.93 0.45 0.70 0.85 1.00 0.21 0.89 9 0.71 9 0.33 0.79
0.59 0.43 0.59 0.83 1.00 0.54 0.69 0.60 0.57 0.80
0.57 0.30 0.33 0.94 1.00 0.22 0.71 9 0.46 9 0.27 0.77
0.50 0.27 0.56 0.94 1.00 0.31 0.65 9 0.42 o 0.40 9 0.82
0.75 0.54 0.67 0.92 0.96 0.23 0.83 9 0.69 9 0.34 0.85
0.63 0.40 0.60 0.89 0.98 0.22 0.73 0.56 0.30 0.82
SCENARIO
MTUnx
pay with credit card
eat in restaurant
iron clothes I
cook scrambled eggs
</table>
<figure confidence="0.85052275">
take a bus
OMICS
buy from vending machine
iron clothes II
make coffee
make omelette
answer the phone
AVERAGE
</figure>
<figureCaption confidence="0.999619">
Figure 4: Results for paraphrasing task; significance of difference to sys: • : p G 0.01, o : p G 0.1
</figureCaption>
<bodyText confidence="0.999803727272727">
with a weighted edge; the weight reflects the se-
mantic similarity of the nodes’ event descriptions
as described in Section 5.2. To include all input in-
formation on inequality of events, we did not allow
for edges between nodes containing two descrip-
tions occurring together in one ESD. The underly-
ing assumption here is that two different event de-
scriptions of the same ESD always represent dis-
tinct events.
The clustering algorithm uses a parameter
which influences the cluster granularity, without
determining the exact number of clusters before-
hand. We optimized this parameter automatically
for each scenario: The system picks the value that
yields the optimal result with respect to density
and distance of the clusters (Flake et al., 2004),
i.e. the elements of each cluster are as similar as
possible to each other, and as dissimilar as possi-
ble to the elements of all other clusters.
The clustering baseline considers two phrases
as paraphrases if they are in the same cluster. It
claims a happens-before relation between phrases
e and f if some phrase in e’s cluster precedes
some phrase in f’s cluster in the original ESDs.
With this baseline, we can show the contribution
of MSA.
Levenshtein Baseline: This system follows the
same steps as our system, but using Levenshtein
distance as the measure of semantic similarity for
MSA and for node merging (cf. Section 5.3). This
lets us measure the contribution of the more fine-
grained similarity function. We computed Leven-
shtein distance as the character-wise edit distance
on the phrases, divided by the phrases’ character
length so as to get comparable values for shorter
and longer phrases. The gap costs for MSA with
Levenshtein were optimized on our development
set so as to produce the best possible alignment.
Upper bound: We also compared our system
to a human-performance upper bound. Because no
single annotator rated all pairs of ESDs, we con-
structed a “virtual annotator” as a point of com-
parison, by randomly selecting one of the human
annotations for each pair.
</bodyText>
<subsectionHeader confidence="0.896392">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999668285714286">
We calculated precision, recall, and f-score for our
system, the baselines, and the upper bound as fol-
lows, with allsystem being the number of pairs la-
belled as paraphrase or happens-before, allgold as
the respective number of pairs in the gold standard
and correct as the number of pairs labeled cor-
rectly by the system.
</bodyText>
<equation confidence="0.773953833333333">
correct correct
precision = recall =
allsystem allgold
2 *precision * recall
f-score =
precision + recall
</equation>
<bodyText confidence="0.999891588235294">
The tables in Fig. 4 and 5 show the results of our
system and the reference values; Fig. 4 describes
the paraphrasing task and Fig. 5 the happens-
before task. The upper half of the tables describes
the test sets from our own corpus, the remainder
refers to OMICS data. The columns labelled sys
contain the results of our system, based describes
the clustering baseline and basele, the Levenshtein
baseline. The f-score for the upper bound is in the
column upper. For the f-score values, we calcu-
lated the significance for the difference between
our system and the baselines as well as the upper
bound, using a resampling test (Edgington, 1986).
The values marked with • differ from our system
significantly at a level of p G 0.01, o marks a level
of p G 0.1. The remaining values are not signifi-
cant with p G 0.1. (For the average values, no sig-
</bodyText>
<page confidence="0.995478">
985
</page>
<table confidence="0.921365823529412">
sys PRECISION RECALL F-SCORE
basecl baselev sys basecl baselev sys basecl baselev upper
0.86 0.49 0.65 0.84 0.74 0.45 0.85 9 0.59 9 0.53 0.92
0.78 0.48 0.68 0.84 0.98 0.75 0.81 9 0.64 0.71 9 0.95
0.78 0.54 0.75 0.72 0.95 0.53 0.75 0.69 9 0.62 9 0.92
0.67 0.54 0.55 0.64 0.98 0.69 0.66 0.70 0.61 9 0.88
0.80 0.49 0.68 0.80 1.00 0.37 0.80 9 0.66 9 0.48 9 0.96
0.83 0.48 0.79 0.86 1.00 0.96 0.84 9 0.64 0.87 0.90
0.84 0.51 0.69 0.85 0.90 0.75 0.84 9 0.66 o 0.71 0.83
0.78 0.48 0.75 0.80 0.96 0.66 0.79 9 0.64 0.70 0.84
0.70 0.55 0.50 0.78 1.00 0.55 0.74 0.71 o 0.53 o 0.83
0.70 0.55 0.79 0.83 0.93 0.82 0.76 o 0.69 0.81 9 0.92
0.77 0.51 0.68 0.80 0.95 0.65 0.78 0.66 0.66 0.90
SCENARIO
MTUnx
pay with credit card
eat in restaurant
</table>
<figure confidence="0.9022346">
iron clothes I
cook scrambled eggs
take a bus
OMICS
buy from vending machine
iron clothes II
make coffee
make omelette
answer the phone
AVERAGE
</figure>
<figureCaption confidence="0.99987">
Figure 5: Results for happens-before task; significance of difference to sys: • : p:5 0.01, o : p:5 0.1
</figureCaption>
<bodyText confidence="0.999949532258065">
nificance is calculated because this does not make
sense for scenario-wise evaluation.)
Paraphrase task: Our system outperforms
both baselines clearly, reaching significantly
higher f-scores in 17 of 20 cases. Moreover, for
five scenarios, the upper bound does not differ sig-
nificantly from our system. For judging the pre-
cision, consider that the test set is slightly biased:
Labeling all pairs with the majority category (no
paraphrase) would result in a precision of 0.64.
However, recall and f-score for this trivial lower
bound would be 0.
The only scenario in which our system doesn’t
score very well is BUY FROM A VENDING MA-
CHINE, where the upper bound is not significantly
better either. The clustering system, which can’t
exploit the sequential information from the ESDs,
has trouble distinguishing semantically similar
phrases (high recall, low precision). The Leven-
shtein similarity measure, on the other hand, is too
restrictive and thus results in comparatively high
precisions, but very low recall.
Happens-before task: In most cases, and on
average, our system is superior to both base-
lines. Where a baseline system performs better
than ours, the differences are not significant. In
four cases, our system does not differ significantly
from the upper bound. Regarding precision, our
system outperforms both baselines in all scenarios
except one (MAKE OMELETTE).
Again the clustering baseline is not fine-grained
enough and suffers from poor precision, only
slightly better than the majority baseline. The Lev-
enshtein baseline gets mostly poor recall, except
for ANSWER THE PHONE: to describe this sce-
nario, people used very similar wording. In such a
scenario, adding lexical knowledge to the sequen-
tial information makes less of a difference.
On average, the baselines do much better here
than for the paraphrase task. This is because once
a system decides on paraphrase clusters that are
essentially correct, it can retrieve correct informa-
tion about the temporal order directly from the
original ESDs.
Both tables illustrate that the task complexity
strongly depends on the scenario: Scripts that al-
low for a lot of variation with respect to ordering
(such as COOK SCRAMBLED EGGS) are particu-
larly challenging for our system. This is due to the
fact that our current system can neither represent
nor find out that two events can happen in arbitrary
order (e.g., ‘take out pan’ and ‘take out bowl’).
One striking difference between the perfor-
mance of our system on the OMICS data and on
our own dataset is the relation to the upper bound:
On our own data, the upper bound is almost al-
ways significantly better than our system, whereas
significant differences are rare on OMICS. This
difference bears further analysis; we speculate it
might be caused either by the increased amount of
training data in OMICS or by differences in lan-
guage (e.g., fewer anaphoric references).
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999979333333333">
We conclude with a summary of this paper and
some discussion along with hints to future work
in the last part.
</bodyText>
<subsectionHeader confidence="0.955446">
7.1 Summary
</subsectionHeader>
<bodyText confidence="0.9985064">
In this paper, we have described a novel approach
to the unsupervised learning of temporal script in-
formation. Our approach differs from previous
work in that we collect training data by directly
asking non-expert users to describe a scenario, and
</bodyText>
<page confidence="0.995649">
986
</page>
<bodyText confidence="0.999966375">
then apply a Multiple Sequence Alignment algo-
rithm to extract scenario-specific paraphrase and
temporal ordering information. We showed that
our system outperforms two baselines and some-
times approaches human-level performance, espe-
cially because it can exploit the sequential struc-
ture of the script descriptions to separate clusters
of semantically similar events.
</bodyText>
<subsectionHeader confidence="0.940951">
7.2 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.999984527777778">
We believe that we can scale this approach to
model a large numbers of scenarios represent-
ing implicit shared knowledge. To realize this
goal, we are going to automatize several process-
ing steps that were done manually for the cur-
rent study. We will restrict the user input to lex-
icon words to avoid manual orthography correc-
tion. Further, we will implement some heuristics
to filter unusable instances by matching them with
the remaining data. As far as the data collection is
concerned, we plan to replace the web form with a
browser game, following the example of von Ahn
and Dabbish (2008). This game will feature an
algorithm that can generate new candidate scenar-
ios without any supervision, for instance by identi-
fying suitable sub-events of collected scripts (e.g.
inducing data collection for PAY as sub-event se-
quence of GO SHOPPING)
On the technical side, we intend to address the
question of detecting participants of the scripts and
integrating them into the graphs, Further, we plan
to move on to more elaborate data structures than
our current TSGs, and then identify and repre-
sent script elements like optional events, alterna-
tive events for the same step, and events that can
occur in arbitrary order.
Because our approach gathers information from
volunteers on the Web, it is limited by the knowl-
edge of these volunteers. We expect it will per-
form best for general commonsense knowledge;
culture-specific knowledge or domain-specific ex-
pert knowledge will be hard for it to learn. This
limitation could be addressed by targeting spe-
cific groups of online users, or by complementing
our approach with corpus-based methods, which
might perform well exactly where ours does not.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998423">
We want to thank Dustin Smith for the OMICS
data, Alexis Palmer for her support with Amazon
Mechanical Turk, Nils Bendfeldt for the creation
of all web forms and Ines Rehbein for her effort
with several parsing experiments. In particular, we
thank the anonymous reviewers for their helpful
comments. – This work was funded by the Cluster
of Excellence “Multimodal Computing and Inter-
action” in the German Excellence Initiative.
</bodyText>
<sectionHeader confidence="0.996429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870465116279">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, pages 86–90, Morristown, NJ,
USA. Association for Computational Linguistics.
Avron Barr and Edward Feigenbaum. 1981. The
Handbook of Artificial Intelligence, Volume 1.
William Kaufman Inc., Los Altos, CA.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003.
Jon Chamberlain, Massimo Poesio, and Udo Kru-
schwitz. 2009. A demonstration of human compu-
tation using the phrase detectives annotation game.
In KDD Workshop on Human Computation. ACM.
Nathanael Chambers and Dan Jurafsky. 2008a. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of EMNLP 2008.
Nathanael Chambers and Dan Jurafsky. 2008b. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings ofACL-IJCNLP 2009.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of ACL-07: Interactive
Poster and Demonstration Sessions.
Richard Edward Cullingford. 1977. Script applica-
tion: computer understanding of newspaper stories.
Ph.D. thesis, Yale University, New Haven, CT, USA.
Richard Durbin, Sean Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis. Cambridge University Press.
Eugene S Edgington. 1986. Randomization tests.
Marcel Dekker, Inc., New York, NY, USA.
Gary W. Flake, Robert E. Tarjan, and Kostas Tsiout-
siouliklis. 2004. Graph clustering and minimum cut
trees. Internet Mathematics, 1(4).
Andrew S. Gordon. 2001. Browsing image collec-
tions with representations of common-sense activi-
ties. JASIST, 52(11).
</reference>
<page confidence="0.978176">
987
</page>
<reference confidence="0.999624526315789">
Desmond G. Higgins and Paul M. Sharp. 1988.
Clustal: a package for performing multiple sequence
alignment on a microcomputer. Gene, 73(1).
Dominic R. Jones and Cynthia A. Thompson. 2003.
Identifying events using similarity and context. In
Proceedings of CoNNL-2003.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In
COLING/ACL-2006.
Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proceed-
ings of the 21st FLAIRS Conference.
Michael McTear. 1987. The Articulate Computer.
Blackwell Publishers, Inc., Cambridge, MA, USA.
Risto Miikkulainen. 1995. Script-based inference and
memory retrieval in subsymbolic story processing.
Applied Intelligence, 5(2), 04.
Raymond J. Mooney. 1990. Learning plan schemata
from observation: Explanation-based learning for
plan recognition. Cognitive Science, 14(4).
Erik T. Mueller. 1998. Natural Language Processing
with Thought Treasure. Signiform.
Erik T. Mueller. 2004. Understanding script-based sto-
ries using commonsense reasoning. Cognitive Sys-
tems Research, 5(4).
Saul B. Needleman and Christian D. Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3), March.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using
linguistic knowledge acquisition. Information Pro-
cessing and Management, 25(4):419 – 428.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding. Lawrence Erl-
baum, Hillsdale, NJ.
Push Singh, Thomas Lin, Erik T. Mueller, Grace Lim,
Travell Perkins, and Wan L. Zhu. 2002. Open
mind common sense: Knowledge acquisition from
the general public. In On the Move to Meaningful
Internet Systems - DOA, CoopIS and ODBASE 2002,
London, UK. Springer-Verlag.
Dustin Smith and Kenneth C. Arnold. 2009. Learning
hierarchical plans by reading simple english narra-
tives. In Proceedings of the Commonsense Work-
shop at IUI-09.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP 2008.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of ICIDS 2008.
Luis von Ahn and Laura Dabbish. 2008. Designing
games with a purpose. Commun. ACM, 51(8).
</reference>
<page confidence="0.997613">
988
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914899">
<title confidence="0.99992">Learning Script Knowledge with Web Experiments</title>
<author confidence="0.999839">Michaela Regneri Alexander Koller Manfred Pinkal</author>
<affiliation confidence="0.997758">Department of Computational Linguistics and Cluster of Excellence Saarland University, Saarbr¨ucken</affiliation>
<email confidence="0.97956">{regneri|koller|pinkal}@coli.uni-saarland.de</email>
<abstract confidence="0.995237769230769">We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect naturallanguage descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script’s temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2093" citStr="Baker et al., 1998" startWordPosition="307" endWordPosition="310">gford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in text. In this paper, we propose a different approach to the unsupervised learning of script-like knowledge. We focus on the temporal event structure of scripts; that is, we aim to learn what phrases can describ</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, pages 86–90, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avron Barr</author>
<author>Edward Feigenbaum</author>
</authors>
<date>1981</date>
<booktitle>The Handbook of Artificial Intelligence,</booktitle>
<volume>1</volume>
<publisher>William Kaufman Inc.,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="891" citStr="Barr and Feigenbaum, 1981" startWordPosition="120" endWordPosition="123">ovel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect naturallanguage descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script’s temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines. 1 Introduction A script is “a standardized sequence of events that describes some stereotypical human activity such as going to a restaurant or visiting a doctor” (Barr and Feigenbaum, 1981). Scripts are fundamental pieces of commonsense knowledge that are shared between the different members of the same culture, and thus a speaker assumes them to be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Muel</context>
</contexts>
<marker>Barr, Feigenbaum, 1981</marker>
<rawString>Avron Barr and Edward Feigenbaum. 1981. The Handbook of Artificial Intelligence, Volume 1. William Kaufman Inc., Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="6191" citStr="Barzilay and Lee (2003)" startWordPosition="959" endWordPosition="962">sually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together with the temporal ordering constraints. Jones and Thompson (2003) describe an approach to identifying different natural language realizations for the same event considering the temporal structure of a scenario. However, they don’t aim to acquire or represent the temporal structure of the whole script in the end. In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Unlike Barzilay and Lee, we do not tackle the general paraphrase problem, but only consider whether two phrases describe the same event in the context of the same script. Furthermore, the atomic units of our alignment process are entire phrases, while in Barzilay and Lee’s setting, the atomic units are words. Finally, it is worth pointing out that our work is placed in the growing landscape of research that attempts to learn linguistic information out of data directly collected from users over the Internet. Some examples are the general acquisition of commonsense knowledge (Singh et al., 200</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Chamberlain</author>
<author>Massimo Poesio</author>
<author>Udo Kruschwitz</author>
</authors>
<title>A demonstration of human computation using the phrase detectives annotation game.</title>
<date>2009</date>
<booktitle>In KDD Workshop on Human Computation.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="6948" citStr="Chamberlain et al., 2009" startWordPosition="1085" endWordPosition="1088">event in the context of the same script. Furthermore, the atomic units of our alignment process are entire phrases, while in Barzilay and Lee’s setting, the atomic units are words. Finally, it is worth pointing out that our work is placed in the growing landscape of research that attempts to learn linguistic information out of data directly collected from users over the Internet. Some examples are the general acquisition of commonsense knowledge (Singh et al., 2002), the use of browser games for that purpose (von Ahn and Dabbish, 2008), and the collaborative annotation of anaphoric reference (Chamberlain et al., 2009). In particular, the use of the Amazon Mechanical Turk, which we use here, has been evaluated and shown to be useful for language processing tasks (Snow et al., 2008). 3 Scripts Before we delve into the technical details, let us establish some terminology. In this paper, we distinguish scenarios, as classes of human activities, from scripts, which are stereotypical models of the internal structure of these activities. Where EATING IN A RESTAURANT is a scenario, the script describes a number of events, such as ordering and leaving, that must occur in a certain order in order to constitute an EA</context>
</contexts>
<marker>Chamberlain, Poesio, Kruschwitz, 2009</marker>
<rawString>Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz. 2009. A demonstration of human computation using the phrase detectives annotation game. In KDD Workshop on Human Computation. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="2238" citStr="Chambers and Jurafsky, 2008" startWordPosition="328" endWordPosition="331">nd Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in text. In this paper, we propose a different approach to the unsupervised learning of script-like knowledge. We focus on the temporal event structure of scripts; that is, we aim to learn what phrases can describe the same event in a script, and what constraints must hold on the temporal order in which these events occur. We approach this problem by askin</context>
<context position="4921" citStr="Chambers and Jurafsky, 2008" startWordPosition="763" endWordPosition="766">al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008a. Jointly combining implicit constraints improves temporal ordering. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="2238" citStr="Chambers and Jurafsky, 2008" startWordPosition="328" endWordPosition="331">nd Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in text. In this paper, we propose a different approach to the unsupervised learning of script-like knowledge. We focus on the temporal event structure of scripts; that is, we aim to learn what phrases can describe the same event in a script, and what constraints must hold on the temporal order in which these events occur. We approach this problem by askin</context>
<context position="4921" citStr="Chambers and Jurafsky, 2008" startWordPosition="763" endWordPosition="766">al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008b. Unsupervised learning of narrative event chains. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP</booktitle>
<contexts>
<context position="4951" citStr="Chambers and Jurafsky, 2009" startWordPosition="767" endWordPosition="770">ur system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information a</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings ofACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Shan Wang</author>
<author>Dan Jurafsky</author>
</authors>
<title>Classifying temporal relations between events.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07: Interactive Poster and Demonstration Sessions.</booktitle>
<contexts>
<context position="5107" citStr="Chambers et al., 2007" startWordPosition="789" endWordPosition="792">n early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns differ</context>
</contexts>
<marker>Chambers, Wang, Jurafsky, 2007</marker>
<rawString>Nathanael Chambers, Shan Wang, and Dan Jurafsky. 2007. Classifying temporal relations between events. In Proceedings of ACL-07: Interactive Poster and Demonstration Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Edward Cullingford</author>
</authors>
<title>Script application: computer understanding of newspaper stories.</title>
<date>1977</date>
<tech>Ph.D. thesis,</tech>
<institution>Yale University,</institution>
<location>New Haven, CT, USA.</location>
<contexts>
<context position="1485" citStr="Cullingford, 1977" startWordPosition="221" endWordPosition="222">r and Feigenbaum, 1981). Scripts are fundamental pieces of commonsense knowledge that are shared between the different members of the same culture, and thus a speaker assumes them to be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al</context>
</contexts>
<marker>Cullingford, 1977</marker>
<rawString>Richard Edward Cullingford. 1977. Script application: computer understanding of newspaper stories. Ph.D. thesis, Yale University, New Haven, CT, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological Sequence Analysis.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14412" citStr="Durbin et al., 1998" startWordPosition="2474" endWordPosition="2477"> number 0 0 15 0 take your cup 0 0 16 0 stand off to the side 0 0 17 0 wait for number to be called wait for the ordered food 0 18 receive food at counter get your drink get the food pick up order 19 0 0 0 pick up condiments 20 take food to table 0 move to a table go to table 21 eat food 0 eat food consume food 22 0 0 0 clear tray 22 0 0 exit the place 0 Figure 2: A MSA of four event sequence descriptions 5.1 Multiple Sequence Alignment The problem of computing Multiple Sequence Alignments comes from bioinformatics, where it is typically used to find corresponding elements in proteins or DNA (Durbin et al., 1998). A sequence alignment algorithm takes as its input some sequences s1, ... , sn E E* over some alphabet E, along with a cost function cm : ExE —* R for substitutions and gap costs cgap E R for insertions and deletions. In bioinformatics, the elements of E could be nucleotides and a sequence could be a DNA sequence; in our case, E contains the individual event descriptions in our data, and the sequences are the ESDs. A Multiple Sequence Alignment A of these sequences is then a matrix as in Fig. 2: The i-th column of A is the sequence si, possibly with some gaps (“0”) interspersed between the sy</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene S Edgington</author>
</authors>
<title>Randomization tests.</title>
<date>1986</date>
<publisher>Marcel Dekker, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="29878" citStr="Edgington, 1986" startWordPosition="5222" endWordPosition="5223"> show the results of our system and the reference values; Fig. 4 describes the paraphrasing task and Fig. 5 the happensbefore task. The upper half of the tables describes the test sets from our own corpus, the remainder refers to OMICS data. The columns labelled sys contain the results of our system, based describes the clustering baseline and basele, the Levenshtein baseline. The f-score for the upper bound is in the column upper. For the f-score values, we calculated the significance for the difference between our system and the baselines as well as the upper bound, using a resampling test (Edgington, 1986). The values marked with • differ from our system significantly at a level of p G 0.01, o marks a level of p G 0.1. The remaining values are not significant with p G 0.1. (For the average values, no sig985 sys PRECISION RECALL F-SCORE basecl baselev sys basecl baselev sys basecl baselev upper 0.86 0.49 0.65 0.84 0.74 0.45 0.85 9 0.59 9 0.53 0.92 0.78 0.48 0.68 0.84 0.98 0.75 0.81 9 0.64 0.71 9 0.95 0.78 0.54 0.75 0.72 0.95 0.53 0.75 0.69 9 0.62 9 0.92 0.67 0.54 0.55 0.64 0.98 0.69 0.66 0.70 0.61 9 0.88 0.80 0.49 0.68 0.80 1.00 0.37 0.80 9 0.66 9 0.48 9 0.96 0.83 0.48 0.79 0.86 1.00 0.96 0.84 9</context>
</contexts>
<marker>Edgington, 1986</marker>
<rawString>Eugene S Edgington. 1986. Randomization tests. Marcel Dekker, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary W Flake</author>
<author>Robert E Tarjan</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>Graph clustering and minimum cut trees.</title>
<date>2004</date>
<journal>Internet Mathematics,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="21685" citStr="Flake et al., 2004" startWordPosition="3804" endWordPosition="3807">mplify it and eliminate noise that caused MSA errors. At first we prune spurious nodes which contain only one event description. Then we refine the graph by merging nodes whose elements should have been aligned in the first place but were missed by the MSA. We merge two nodes if they satisfy certain structural and semantic constraints. The semantic constraints check whether the event descriptions of the merged node would be sufficiently consistent according to the similarity measure from Section 5.2. To check whether we can merge two nodes u and v, we use an unsupervised clustering algorithm (Flake et al., 2004) to 983 first cluster the event descriptions in u and v separately. Then we combine the event descriptions from u and v and cluster the resulting set. If the union has more clusters than either u or v, we assume the nodes to be too dissimilar for merging. The structural constraints depend on the graph structure. We only merge two nodes u and v if their event descriptions come from different sequences and one of the following conditions holds: • u and v have the same parent; • u has only one parent, v is its only child; • v has only one child and is the only child of u; • all children of u (exc</context>
<context position="25592" citStr="Flake et al., 2004" startWordPosition="4470" endWordPosition="4473">he same part of the story that the second one describes with events ? For the happens-before task, the question template was the following: Imagine somebody telling a story about SCENARIO in which the events events and event2 occur. Would events normally happen before event2? We constructed a gold standard by a majority decision of the raters. An expert rater adjudicated the pairs with a 3:2 vote ratio. 6.2 Upper Bound and Baselines To show the contributions of the different system components, we implemented two baselines: Clustering Baseline: We employed an unsupervised clustering algorithm (Flake et al., 2004) and fed it all event descriptions of a scenario. We first created a similarity graph with one node per event description. Each pair of nodes is connected 2http://openmind.hri-us.com/ 984 sys PRECISION RECALL F-SCORE basecl baselev sys basecl baselev sys basecl baselev upper 0.52 0.43 0.50 0.84 0.89 0.11 0.64 0.58 9 0.17 0.60 0.70 0.42 0.75 0.88 1.00 0.25 0.78 9 0.59 9 0.38 9 0.92 0.52 0.32 1.00 0.94 1.00 0.12 0.67 9 0.48 9 0.21 9 0.82 0.58 0.34 0.50 0.86 0.95 0.10 0.69 9 0.50 9 0.16 9 0.91 0.65 0.42 0.40 0.87 1.00 0.09 0.74 9 0.59 9 0.14 9 0.88 0.93 0.45 0.70 0.85 1.00 0.21 0.89 9 0.71 9 0.33</context>
<context position="27507" citStr="Flake et al., 2004" startWordPosition="4819" endWordPosition="4822">. To include all input information on inequality of events, we did not allow for edges between nodes containing two descriptions occurring together in one ESD. The underlying assumption here is that two different event descriptions of the same ESD always represent distinct events. The clustering algorithm uses a parameter which influences the cluster granularity, without determining the exact number of clusters beforehand. We optimized this parameter automatically for each scenario: The system picks the value that yields the optimal result with respect to density and distance of the clusters (Flake et al., 2004), i.e. the elements of each cluster are as similar as possible to each other, and as dissimilar as possible to the elements of all other clusters. The clustering baseline considers two phrases as paraphrases if they are in the same cluster. It claims a happens-before relation between phrases e and f if some phrase in e’s cluster precedes some phrase in f’s cluster in the original ESDs. With this baseline, we can show the contribution of MSA. Levenshtein Baseline: This system follows the same steps as our system, but using Levenshtein distance as the measure of semantic similarity for MSA and f</context>
</contexts>
<marker>Flake, Tarjan, Tsioutsiouliklis, 2004</marker>
<rawString>Gary W. Flake, Robert E. Tarjan, and Kostas Tsioutsiouliklis. 2004. Graph clustering and minimum cut trees. Internet Mathematics, 1(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Gordon</author>
</authors>
<title>Browsing image collections with representations of common-sense activities.</title>
<date>2001</date>
<journal>JASIST,</journal>
<volume>52</volume>
<issue>11</issue>
<contexts>
<context position="1898" citStr="Gordon, 2001" startWordPosition="280" endWordPosition="281">t’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in text. In this p</context>
</contexts>
<marker>Gordon, 2001</marker>
<rawString>Andrew S. Gordon. 2001. Browsing image collections with representations of common-sense activities. JASIST, 52(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond G Higgins</author>
<author>Paul M Sharp</author>
</authors>
<title>Clustal: a package for performing multiple sequence alignment on a microcomputer.</title>
<date>1988</date>
<journal>Gene,</journal>
<volume>73</volume>
<issue>1</issue>
<contexts>
<context position="16037" citStr="Higgins and Sharp, 1988" startWordPosition="2784" endWordPosition="2787">mber of rows and m the number of sequences. In other words, we sum up the alignment cost for any two symbols from E that are aligned with each other, and add the gap cost for each gap. There is an algorithm that computes cheapest pairwise alignments (i.e. n = 2) in polynomial time (Needleman and Wunsch, 1970). For n &gt; 2, the problem is NP-complete, but there are efficient algorithms that approximate the cheapest MSAs by aligning two sequences first, considering the result as a single sequence whose elements are pairs, and repeating this process until all sequences are incorporated in the MSA (Higgins and Sharp, 1988). 5.2 Semantic similarity In order to apply MSA to the problem of aligning ESDs, we choose E to be the set of all individual event descriptions in a given scenario. Intuitively, we want the MSA to prefer the alignment of two phrases if they are semantically similar, i.e. it should cost more to align ‘exit’ with ‘eat’ than ‘exit’ with ‘leave’. Thus we take a measure of semantic (dis)similarity as the cost function cm. The phrases to be compared are written in bullet-point style. They are typically short and elliptic (no overt subject), they lack determiners and use infinitive or present progres</context>
</contexts>
<marker>Higgins, Sharp, 1988</marker>
<rawString>Desmond G. Higgins and Paul M. Sharp. 1988. Clustal: a package for performing multiple sequence alignment on a microcomputer. Gene, 73(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic R Jones</author>
<author>Cynthia A Thompson</author>
</authors>
<title>Identifying events using similarity and context.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNNL-2003.</booktitle>
<contexts>
<context position="5823" citStr="Jones and Thompson (2003)" startWordPosition="898" endWordPosition="901">extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together with the temporal ordering constraints. Jones and Thompson (2003) describe an approach to identifying different natural language realizations for the same event considering the temporal structure of a scenario. However, they don’t aim to acquire or represent the temporal structure of the whole script in the end. In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Unlike Barzilay and Lee, we do not tackle the general paraphrase problem, but only consider whether two phrases describe the same event in the context of the same script. Furthermore, the atomic units of our alignment process are </context>
</contexts>
<marker>Jones, Thompson, 2003</marker>
<rawString>Dominic R. Jones and Cynthia A. Thompson. 2003. Identifying events using similarity and context. In Proceedings of CoNNL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Marc Verhagen</author>
<author>Ben Wellner</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In COLING/ACL-2006.</booktitle>
<contexts>
<context position="5157" citStr="Mani et al., 2006" startWordPosition="798" endWordPosition="801">nd Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky. 2006. Machine learning of temporal relations. In COLING/ACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>Learning a probabilistic model of event sequences from internet weblog stories.</title>
<date>2008</date>
<booktitle>In Proceedings of the 21st FLAIRS Conference.</booktitle>
<contexts>
<context position="2263" citStr="Manshadi et al., 2008" startWordPosition="332" endWordPosition="335">olution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in text. In this paper, we propose a different approach to the unsupervised learning of script-like knowledge. We focus on the temporal event structure of scripts; that is, we aim to learn what phrases can describe the same event in a script, and what constraints must hold on the temporal order in which these events occur. We approach this problem by asking non-experts to describe</context>
<context position="4975" citStr="Manshadi et al., 2008" startWordPosition="771" endWordPosition="774">nclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually </context>
</contexts>
<marker>Manshadi, Swanson, Gordon, 2008</marker>
<rawString>Mehdi Manshadi, Reid Swanson, and Andrew S. Gordon. 2008. Learning a probabilistic model of event sequences from internet weblog stories. In Proceedings of the 21st FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McTear</author>
</authors>
<title>The Articulate Computer.</title>
<date>1987</date>
<publisher>Blackwell Publishers, Inc.,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1663" citStr="McTear, 1987" startWordPosition="243" endWordPosition="244"> be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008)</context>
</contexts>
<marker>McTear, 1987</marker>
<rawString>Michael McTear. 1987. The Articulate Computer. Blackwell Publishers, Inc., Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
</authors>
<title>Script-based inference and memory retrieval in subsymbolic story processing.</title>
<date>1995</date>
<journal>Applied Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>04</pages>
<contexts>
<context position="1521" citStr="Miikkulainen, 1995" startWordPosition="225" endWordPosition="226">re fundamental pieces of commonsense knowledge that are shared between the different members of the same culture, and thus a speaker assumes them to be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a </context>
</contexts>
<marker>Miikkulainen, 1995</marker>
<rawString>Risto Miikkulainen. 1995. Script-based inference and memory retrieval in subsymbolic story processing. Applied Intelligence, 5(2), 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning plan schemata from observation: Explanation-based learning for plan recognition.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="4474" citStr="Mooney (1990)" startWordPosition="696" endWordPosition="697">ection 2. We will then define how 979 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics we understand scripts, and what aspect of scripts we model here, in Section 3. Section 4 describes our data collection method, and Section 5 explains how we use Multiple Sequence Alignment to compute a temporal script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event de</context>
</contexts>
<marker>Mooney, 1990</marker>
<rawString>Raymond J. Mooney. 1990. Learning plan schemata from observation: Explanation-based learning for plan recognition. Cognitive Science, 14(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik T Mueller</author>
</authors>
<title>Natural Language Processing with Thought Treasure.</title>
<date>1998</date>
<publisher>Signiform.</publisher>
<contexts>
<context position="1883" citStr="Mueller, 1998" startWordPosition="278" endWordPosition="279">olves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impressive results, they are limited by the very fact that a lot of scripts – such as SHOPPING – are shared implicit knowledge, and their events are therefore rarely elaborated in </context>
</contexts>
<marker>Mueller, 1998</marker>
<rawString>Erik T. Mueller. 1998. Natural Language Processing with Thought Treasure. Signiform.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik T Mueller</author>
</authors>
<title>Understanding script-based stories using commonsense reasoning.</title>
<date>2004</date>
<journal>Cognitive Systems Research,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="1500" citStr="Mueller, 2004" startWordPosition="223" endWordPosition="224">981). Scripts are fundamental pieces of commonsense knowledge that are shared between the different members of the same culture, and thus a speaker assumes them to be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There</context>
</contexts>
<marker>Mueller, 2004</marker>
<rawString>Erik T. Mueller. 2004. Understanding script-based stories using commonsense reasoning. Cognitive Systems Research, 5(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of molecular biology,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="15723" citStr="Needleman and Wunsch, 1970" startWordPosition="2732" endWordPosition="2735">non-gaps, we take these symbols to be aligned; aligning a non-gap with a gap can be thought of as an insertion or deletion. Each sequence alignment A can be assigned a cost c(A) in the following way: c(A) = cgap · Em + n �m �m cm(aji, aki) i=1 j=1� k=j+1� ajii4O akiOO where Em is the number of gaps in A, n is the number of rows and m the number of sequences. In other words, we sum up the alignment cost for any two symbols from E that are aligned with each other, and add the gap cost for each gap. There is an algorithm that computes cheapest pairwise alignments (i.e. n = 2) in polynomial time (Needleman and Wunsch, 1970). For n &gt; 2, the problem is NP-complete, but there are efficient algorithms that approximate the cheapest MSAs by aligning two sequences first, considering the result as a single sequence whose elements are pairs, and repeating this process until all sequences are incorporated in the MSA (Higgins and Sharp, 1988). 5.2 Semantic similarity In order to apply MSA to the problem of aligning ESDs, we choose E to be the set of all individual event descriptions in a given scenario. Intuitively, we want the MSA to prefer the alignment of two phrases if they are semantically similar, i.e. it should cost</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa F Rau</author>
<author>Paul S Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Information extraction and text summarization using linguistic knowledge acquisition.</title>
<date>1989</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>25</volume>
<issue>4</issue>
<pages>428</pages>
<contexts>
<context position="1710" citStr="Rau et al., 1989" startWordPosition="248" endWordPosition="251">cenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jurafsky, 2008b; Manshadi et al., 2008); but while these efforts have achieved impress</context>
</contexts>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. Information extraction and text summarization using linguistic knowledge acquisition. Information Processing and Management, 25(4):419 – 428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, Plans, Goals and Understanding. Lawrence Erlbaum,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="7642" citStr="Schank and Abelson, 1977" startWordPosition="1204" endWordPosition="1207">here, has been evaluated and shown to be useful for language processing tasks (Snow et al., 2008). 3 Scripts Before we delve into the technical details, let us establish some terminology. In this paper, we distinguish scenarios, as classes of human activities, from scripts, which are stereotypical models of the internal structure of these activities. Where EATING IN A RESTAURANT is a scenario, the script describes a number of events, such as ordering and leaving, that must occur in a certain order in order to constitute an EATING IN A RESTAURANT activity. The classical perspective on scripts (Schank and Abelson, 1977) has been that next to defining some events with temporal constraints, a script also defines their participants and their causal connections. Here we focus on the narrower task of learning the events that a script consists of, and of modeling and learning the temporal ordering constraints that hold between them. Formally, we will specify a script (in this simplified sense) in terms of a directed graph G3 = (E37 T3), where E3 is a set of nodes representing the events of a scenario s, and T3 is a set of edges (ez7 ek) indicating that the event ez typically happens before ek in s. We call G3 the </context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Push Singh</author>
<author>Thomas Lin</author>
<author>Erik T Mueller</author>
<author>Grace Lim</author>
<author>Travell Perkins</author>
<author>Wan L Zhu</author>
</authors>
<title>Open mind common sense: Knowledge acquisition from the general public.</title>
<date>2002</date>
<booktitle>In On the Move to Meaningful Internet Systems - DOA, CoopIS and ODBASE 2002,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="6793" citStr="Singh et al., 2002" startWordPosition="1060" endWordPosition="1063">ay and Lee (2003). Unlike Barzilay and Lee, we do not tackle the general paraphrase problem, but only consider whether two phrases describe the same event in the context of the same script. Furthermore, the atomic units of our alignment process are entire phrases, while in Barzilay and Lee’s setting, the atomic units are words. Finally, it is worth pointing out that our work is placed in the growing landscape of research that attempts to learn linguistic information out of data directly collected from users over the Internet. Some examples are the general acquisition of commonsense knowledge (Singh et al., 2002), the use of browser games for that purpose (von Ahn and Dabbish, 2008), and the collaborative annotation of anaphoric reference (Chamberlain et al., 2009). In particular, the use of the Amazon Mechanical Turk, which we use here, has been evaluated and shown to be useful for language processing tasks (Snow et al., 2008). 3 Scripts Before we delve into the technical details, let us establish some terminology. In this paper, we distinguish scenarios, as classes of human activities, from scripts, which are stereotypical models of the internal structure of these activities. Where EATING IN A RESTA</context>
<context position="23868" citStr="Singh et al., 2002" startWordPosition="4190" endWordPosition="4193">em: its ability to recognize descriptions of the same event (paraphrases) and the resulting temporal constraints it defines on the event descriptions (happens-before relation). We compare our approach to two baseline systems and show that our system outperforms both baselines and sometimes even comes close to our upper bound. 6.1 Method We selected ten scenarios which we did not use for development purposes, five of them taken from the corpus described in Section 4, the other five from the OMICS corpus.2 The OMICS corpus is a freely available, web-collected corpus by the Open Mind Initiative (Singh et al., 2002). It contains several stories (Pz� scenarios) consisting of multiple ESDs. The corpus strongly resembles ours in language style and information provided, but is restricted to “indoor activities” and contains much more data than our collection (175 scenarios with more than 40 ESDs each). For each scenario, we created a paraphrase set out of 30 randomly selected pairs of event descriptions which the system classified as paraphrases and 30 completely random pairs. The happens-before set consisted of 30 pairs classified as happens-before, 30 random pairs and additionally all 60 pairs in reverse or</context>
</contexts>
<marker>Singh, Lin, Mueller, Lim, Perkins, Zhu, 2002</marker>
<rawString>Push Singh, Thomas Lin, Erik T. Mueller, Grace Lim, Travell Perkins, and Wan L. Zhu. 2002. Open mind common sense: Knowledge acquisition from the general public. In On the Move to Meaningful Internet Systems - DOA, CoopIS and ODBASE 2002, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Smith</author>
<author>Kenneth C Arnold</author>
</authors>
<title>Learning hierarchical plans by reading simple english narratives.</title>
<date>2009</date>
<booktitle>In Proceedings of the Commonsense Workshop at IUI-09.</booktitle>
<contexts>
<context position="4555" citStr="Smith and Arnold (2009)" startWordPosition="707" endWordPosition="710">eting of the Association for Computational Linguistics, pages 979–988, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics we understand scripts, and what aspect of scripts we model here, in Section 3. Section 4 describes our data collection method, and Section 5 explains how we use Multiple Sequence Alignment to compute a temporal script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 200</context>
</contexts>
<marker>Smith, Arnold, 2009</marker>
<rawString>Dustin Smith and Kenneth C. Arnold. 2009. Learning hierarchical plans by reading simple english narratives. In Proceedings of the Commonsense Workshop at IUI-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>Say anything: A massively collaborative open domain story writing companion.</title>
<date>2008</date>
<booktitle>In Proceedings of ICIDS</booktitle>
<contexts>
<context position="1627" citStr="Swanson and Gordon, 2008" startWordPosition="237" endWordPosition="240">same culture, and thus a speaker assumes them to be tacitly understood by a hearer when a scenario related to a script is evoked: When one person says “I’m going shopping”, it is an acceptable reply to say “did you bring enough money?”, because the SHOPPING script involves a ‘payment’ event, which again involves the transfer of money. It has long been recognized that text understanding systems would benefit from the implicit information represented by a script (Cullingford, 1977; Mueller, 2004; Miikkulainen, 1995). There are many other potential applications, including automated storytelling (Swanson and Gordon, 2008), anaphora resolution (McTear, 1987), and information extraction (Rau et al., 1989). However, it is also commonly accepted that the large-scale manual formalization of scripts is infeasible. While there have been a few attempts at doing this (Mueller, 1998; Gordon, 2001), efforts in which expert annotators create script knowledge bases clearly don’t scale. The same holds true of the script-like structures called “scenario frames” in FrameNet (Baker et al., 1998). There has recently been a surge of interest in automatically learning script-like knowledge resources from corpora (Chambers and Jur</context>
</contexts>
<marker>Swanson, Gordon, 2008</marker>
<rawString>Reid Swanson and Andrew S. Gordon. 2008. Say anything: A massively collaborative open domain story writing companion. In Proceedings of ICIDS 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Designing games with a purpose.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>8</issue>
<marker>von Ahn, Dabbish, 2008</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2008. Designing games with a purpose. Commun. ACM, 51(8).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>