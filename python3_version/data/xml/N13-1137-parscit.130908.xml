<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.990517">
Generating Expressions that Refer to Visible Objects
</title>
<author confidence="0.91326">
Margaret Mitchell Kees van Deemter Ehud Reiter
</author>
<affiliation confidence="0.889201">
Johns Hopkins HLTCOE University of Aberdeen University of Aberdeen
</affiliation>
<email confidence="0.997537">
m.mitchell@jhu.edu k.vdeemter@abdn.ac.uk e.reiter@abdn.ac.uk
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997462368421053">
We introduce a novel algorithm for generat-
ing referring expressions, informed by human
and computer vision and designed to refer to
visible objects. Our method separates abso-
lute properties like color from relative proper-
ties like size to stochastically generate a di-
verse set of outputs. Expressions generated
using this method are often overspecified and
may be underspecified, akin to expressions
produced by people. We call such expressions
identifying descriptions. The algorithm out-
performs the well-known Incremental Algo-
rithm (Dale and Reiter, 1995) and the Graph-
Based Algorithm (Krahmer et al., 2003; Vi-
ethen et al., 2008) across a variety of images
in two domains. We additionally motivate
an evaluation method for referring expression
generation that takes the proposed algorithm’s
non-determinism into account.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918966666666">
Referring expression generation (REG) is the task
of generating an expression that can identify a ref-
erent to a listener. These expressions generally take
the form of a definite noun phrase such as “the large
orange plate” or “the furry running dog”. Research
in REG primarily focuses on the subtask of select-
ing a set of properties that may be used to construct
the final surface expression, e.g., (color:orange,
size:large, type:plate). This property selection task
is optimized to meet different goals: for example,
to be identical to those a person would generate in
the same situation, or to be unique to the intended
referent and no other item in the discourse.
We focus on the task of generating referring ex-
pressions for visible objects, specifically with the
goal of generating descriptive, human-like referring
expressions. We are motivated by the desire to con-
nect this algorithm to input from a computer vision
system, and discuss how this may work through-
out the paper. Computer vision (CV) does not yet
reliably provide features for some of the most fre-
quent properties that people use in visual descrip-
tion (in particular, size-based features), and so we
use a gold-standard visual input, evaluating purely
on REG. The proposed algorithm, which we call
the Visible Objects Algorithm, is designed to ap-
proximate human variation identifying an object in
a group of visible, real world objects.
Our primary contributions are the following.
Background for each issue is provided in Section 2:
</bodyText>
<listItem confidence="0.999216857142857">
1. An approach accounting for overspecification,
underspecification, and some of the known ef-
fects of vision on reference.
2. A function to approximate the stochastic nature
of reference. This reflects that people will pro-
duce different references to the same object.
3. A separation between absolute properties like
</listItem>
<bodyText confidence="0.898796428571429">
color, which may be detected directly by CV,
from relative properties like size and loca-
tion, which require reasoning over visual fea-
tures to determine an appropriate form (e.g.,
height/width and distance features between
pixels are available from a visual input; saying
an object is “tall” requires further reasoning).
</bodyText>
<listItem confidence="0.98072">
4. An evaluation method for non-deterministic
REG that aligns generated and observed data
and calculates accuracy over alignments.
</listItem>
<page confidence="0.959634">
1174
</page>
<note confidence="0.51326">
Proceedings of NAACL-HLT 2013, pages 1174–1184,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.90887" genericHeader="method">
2 Motivation &amp; Overview
</sectionHeader>
<bodyText confidence="0.999913837209303">
Most implemented algorithms for referring expres-
sion generation focus on unique identification of a
referent, determining the set of properties that dis-
tinguish a particular target object from the other ob-
jects in the scene (the contrast set) (Dale, 1989; Re-
iter and Dale, 1992; Dale and Reiter, 1995; Krahmer
et al., 2003; Areces et al., 2008). This view of refer-
ence was first outlined by Olson (1970), “the spec-
ification of an intended referent relative to a set of
alternatives”. A substantial body of evidence now
shows that contrastive value relative to alternatives
is not the only factor motivating speakers’ property
choices, specifically in visual domains. The phe-
nomena of overspecification and redundancy, where
speakers select properties that have little or no con-
trastive value, was observed in early developmen-
tal studies in visual domains (Ford and Olson, 1975;
Whitehurst, 1976; Sonnenschein, 1985) as well as
later studies on adult speakers in visual domains
(Pechmann, 1989; Engelhardt et al., 2006; Koolen et
al., 2011). The related phenomenon of underspecifi-
cation, where speakers select a set of properties that
do not linguistically specify the referent, has also re-
ceived some attention, particularly in visual domains
(Clark et al., 1983; Kelleher et al., 2005).
These findings make sense in light of visual ev-
idence that some properties “pop out” in the scene
(Treisman and Gelade, 1980), and speakers may be-
gin referring before scanning the full set of scene ob-
jects (Pechmann, 1989), selecting those properties
that are salient for them (Horton and Keysar, 1996;
Bard et al., 2009) without spending a great amount
of cognitive effort considering the perception of a
hearer (Keysar and Henly, 2002).
We take this evidence to suggest an approach for
a visual reference algorithm that generates natural,
human-like reference by generating visual proper-
ties that are salient for a speaker.1 We can under-
stand what is salient visually (what does the visual
system first respond to, what guides attention?), lin-
guistically (what do people tend to mention in visual
scenes?), and cognitively, which we will not have
room to discuss in this paper (what is atypical for
</bodyText>
<footnote confidence="0.999264666666667">
1We can also add functionality to ensure that a referent is
uniquely identified against the contrast set (whether or not that
reflects what a person would do), which we will describe.
</footnote>
<figureCaption confidence="0.9749744">
Figure 1: Relative properties, like size and location, are
difficult to obtain from a two-dimensional image. We find
it easy to perceive the background object as larger than
the one in the front; but they are technically the same size
in the image (from Murray et al. (2006)).
</figureCaption>
<bodyText confidence="0.998955916666667">
this object?); as well as in terms of broader notions
of salience, e.g., discourse salience (Krahmer and
Theune, 2002).
This suggests a paradigm shift in the generation
task when referring to visible objects, if the goal is
to produce human-like reference. In particular, this
suggests moving from selecting properties that rule
out other scene objects to selecting properties that
are salient for the speaker (visually, conversation-
ally, based on previous experiences, etc.). This mir-
rors related research on the tradeoff between audi-
ence design and egocentrism in language production
(Clark and Murphy, 1982; Horton and Keysar, 1996;
Bard et al., 2009; Gann and Barr, 2013). Under-
and overspecification naturally fall out from such an
approach, with no need to specifically model either
phenomenon.
Perhaps unsurprisingly, the set of properties that
are visually salient and the set of properties that are
linguistically salient largely overlap. Color is the
first property our visual system processes, followed
soon after by size (Murray et al., 2006; Fang et al.,
2008; Schwarzkopf et al., 2010); and people tend
to use color (Pechmann, 1989; Viethen et al., 2012)
and size when identifying objects, with size com-
mon when there is another object of the same type
in the scene (Brown-Schmidt and Tanenhaus, 2006).
Following this, our algorithm gives a privileged
position to these properties, processing them first.
Using computer vision techniques to determine an
object’s color works reasonably well (Berg et al.,
2011), and the relevant visual features for this task
may be useful in future work to return several pos-
sible color labels that capture differences in lexical
choice (cf. Reiter and Sripada (2002)).
Detecting size does not work well (Forsyth,
</bodyText>
<page confidence="0.98488">
1175
</page>
<bodyText confidence="0.999966541666667">
2011); and when it does, it will likely not take the
form supposed in recent generation work. Most
REG algorithms use a predefined single-featured
value, such as “big”; however, given an image-based
input, obtaining such a value requires (1) determin-
ing how the object is situated in a three-dimensional
space, difficult to obtain from a two-dimensional im-
age (see Figure 1); and (2) determining what the
value should be: object detectors currently can pro-
vide the height and width of the location where an
object is likely to exist (its bounding box), as well as
the x- and y-axis locations of the pixels within the
object detection; but a value from these features like
“big”, “tall”, or “long” requires further reasoning.
As such, we incorporate the top-performing size al-
gorithm introduced in Mitchell et al. (2011), which
takes as input the height and widths of objects in the
image and outputs a size value or NONE, indicating
that size should not be used to describe the object.
In addition to color and size, location and orien-
tation begin to be processed early on in the visual
system (Treisman, 1985; Itti and Koch, 2001), with
our first perception of location corresponding to ba-
sic cues of where an object is relative to our focus
of attention. For an input image, this simple type of
location corresponds to surface forms such as, e.g.,
“on the right of the image” or “at the top of the im-
age”. Along with size, location and orientation make
up the three primary relative properties that we aim
to generate language for.
After the simple forms for color, size, location,
and orientation properties are processed, our visual
system feeds forward to two parallel pathways, the
so-called “what” and “where” pathways (Ungerlei-
der and Mishkin, 1982), which process properties
with growing complexity. The “what” pathway in-
cludes absolute properties like shape and material,
which computer vision has had some success de-
tecting (Ferrari and Zisserman, 2007; Farhadi et al.,
2009) while the “where” pathway corresponds to
more complex spatial orientation and location infor-
mation, such as where objects are relative to one an-
other and which way they are facing.
To begin connecting this process to the genera-
tion of human-like descriptions of visible objects,
we start with the following simplification: Color and
size have a privileged status, the first properties pro-
cessed. These are followed by the relative properties
</bodyText>
<figureCaption confidence="0.977363">
Figure 2: Initial model for generating visual reference.
</figureCaption>
<bodyText confidence="0.951284948717949">
of location and orientation, which may feed forward
to more complex location and orientation properties
in one pathway; and absolute properties following
color, like material and shape, which may be pro-
cessed in another pathway.
This gives us the basic model for generating ref-
erence to visible objects shown in Figure 2. To gen-
erate reference in this model, nodes correspond to
general visual attributes and may generate forms for
visual properties (attribute:value pairs). That is, a
property such as color:red is generated from the at-
tribute node color and a property such as size:tall is
generated from the attribute node size. We are lim-
ited by existing REG corpora in which properties we
can evaluate; in this paper, we examine the effect of
the independent selection of color and size, followed
by location and orientation.2
Generating human-like expressions in this setting
begins to be possible by adopting recent propos-
als that REG handle speaker variation (Viethen and
Dale, 2010) and the non-deterministic nature of ref-
erence (van Gompel et al., 2012; van Deemter et
al., 2012b). We can capture such variation simply
by estimating aatt, the likelihood that an attribute
att generates a corresponding visual property. Dur-
ing generation, the algorithm passes through each at-
tribute node, and uses this estimate to stochastically
add each property to the output property set.
Such a non-deterministic process means that the
algorithm will not return the same output every time,
which offers new challenges for evaluation. If we
run the algorithm 1,000 times, we have a distribu-
tion over several possible output property sets. From
this we can obtain the majority set and check if it
matches the majority observed set. Similarly, we can
2We have also built an algorithm and corpus with more com-
plex properties in order to tease out further details of visual ref-
erence, but must leave these details for follow up work; for now,
we focus on the properties common to REG corpora.
</bodyText>
<page confidence="0.976284">
1176
</page>
<bodyText confidence="0.9999595">
run the algorithm for as many instances as we have
in our test data, and see how well the property sets
it produces aligns to the observed property sets. We
discuss evaluation using both methods in Section 6.
</bodyText>
<sectionHeader confidence="0.929401" genericHeader="method">
3 The State of the Art in REG
</sectionHeader>
<subsectionHeader confidence="0.997701">
3.1 Algorithms
</subsectionHeader>
<bodyText confidence="0.999981263157895">
In order to understand how this approach compares
to the state of the art in REG, we evaluate against
two of the most well-known algorithms, the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Graph-Based Algorithm (Krahmer et. al, 2003, as
implemented in Viethen et al., 2008). Details on
these algorithms are available in their corresponding
papers. As a brief summary, both algorithms formal-
ize the objects in the discourse as a set of properties
(attribute:�alue pairs). For example, one object may
be represented as (type:box, color:red, size:large).
The task is to find the set of properties that uniquely
specify the referent. This is known as a content se-
lection problem, and the set of properties chosen by
the algorithm is called a distinguishing description.
The Incremental Algorithm (IA) proceeds by it-
erating through attributes in a predefined order (a
preference order), and for each attribute, it checks
whether specifying a value would rule out at least
one item in the contrast set that has not already been
ruled out. If it will, the attribute:�alue is added to
the distinguishing description. This process contin-
ues until all contrast items (distractors) are ruled out
or all available properties have been checked. We
use the implementation of the IA available from the
NLTK (Bird et al., 2009).3
In the Graph-Based Algorithm (GB), the objects
in the discourse are represented within a labeled di-
rected graph, and content selection is a subgraph
construction problem. Each object is represented as
a vertex, with properties for an object represented as
self-edges on the object vertex, and spatial relations
between objects represented as edges between ver-
tices. The algorithm seeks to find the cheapest sub-
graph, calculated from the edge costs. We use the
implementation available from Viethen et al. (2008),
which adds a preference order to decide between
edges with the same cost during search. This has
</bodyText>
<footnote confidence="0.9250595">
3https://github.com/nltk/nltk contrib/blob/master/
nltk contrib/referring.py retrieved 1.Aug.2012.
</footnote>
<bodyText confidence="0.999711608695652">
been one of the best-performing systems in recent
generation challenges (Gatt and Belz, 2008; Gatt et
al., 2009).
An important commonality between these algo-
rithms, and much of the work on REG that they
have influenced, is the focus on unique identifica-
tion and operating deterministically. Both produce
one property set (and only one), and stop once a tar-
get item has been uniquely identified (or else fail).
Their driving goal is to rule out distractor objects.
In the approach introduced here, the algorithm
produces a distribution over several possible out-
puts, and the initial driving mechanism is based on
likelihood estimates for each attribute independent
of the other objects in the scene, rather than ruling
out all distractors. This offers a way to capture some
aspects of human-like reference, including under-
and overspecification and speaker variation. Due to
the fundamentally different objective of this algo-
rithm, we will call the kind of expression it generates
an identifying description, following Searle (1969).
This is a description that the system finds (1) useful
to describe the referent and (2) true of the referent.
</bodyText>
<sectionHeader confidence="0.992958" genericHeader="method">
4 The Algorithm
</sectionHeader>
<bodyText confidence="0.999913380952381">
The Visible Objects Algorithm iterates through lists
of visible attributes, stochastically adding properties
to the property set it will generate. After this initial
search, the algorithm then scans through the objects
in the scene, roughly corresponding to how people
scan a scene when referring (Pechmann, 1989). The
target referent type, corresponding to the head noun
in the final generated description, is added to the
property set at the end of the algorithm.
We represent the basic components of the algo-
rithm graphically in Figure 3. Full code is available
online.4 After START, the algorithm proceeds in par-
allel through a list of absolute attributes and a list
of relative attributes. The likelihood of generating a
property is a function of the prior likelihood αatt and
&apos;y, a penalty on the length of the constructed prop-
erty set up to that point. This ensures that only a few
properties are generated for a referent, and the ex-
pression will not be too complex. This is also in line
with recent research suggesting that there are rarely
more than three adjectives in a visual noun phrase
</bodyText>
<footnote confidence="0.955805">
4https://github.com/itallow/VisibleObjectsAlgorithm.
</footnote>
<page confidence="0.992967">
1177
</page>
<bodyText confidence="0.999801444444444">
(Berg et al., 2011). Once the algorithm hits END,
it scans through the objects in the scene. If it finds
an object that is the same type as the referent object,
the algorithm checks through the attributes again in
a preference order akin to the IA, comparing the ob-
ject’s properties against the referent’s and generating
properties as a function of the length penalty alone.
If the algorithm does not find an object that it is the
same type, no further properties are added.
</bodyText>
<subsectionHeader confidence="0.979015">
4.1 Requirements
</subsectionHeader>
<bodyText confidence="0.999226">
The algorithm requires the following:
</bodyText>
<listItem confidence="0.936389384615384">
1. Prior likelihood estimates on the inclusion of
different attributes. Represented as aatt.
2. Ordered list of absolute attributes beyond color.
Represented as AP.
3. Ordered list of relative attributes beyond size.
Represented as RP.
4. Ordered list of all attributes. Represented as P.
5. Ordered list specifying the order in which to
scan through other scene objects. The current
implementation uses the order in which the ob-
jects are listed in the corpora it is run on.
(1) is similar to the cost functions for GB, but
attributes are selected non-deterministically using
</listItem>
<bodyText confidence="0.998947555555556">
prior likelihoods. (2), (3), and (4) are similar to
the IA’s and GB’s preference order. For our eval-
uation corpora, AP is empty and RP contains loca-
tion and orientation. (5) is novel to this algorithm,
defining an order in which to compare the target ob-
ject against other objects in the scene. This is in-
spired by the process of incremental speech produc-
tion (Pechmann, 1989), where speakers scan objects
during naming, incrementally producing properties.
</bodyText>
<subsectionHeader confidence="0.996299">
4.2 The Stochastic Process
</subsectionHeader>
<bodyText confidence="0.9996677">
Generally speaking, we want to penalize longer de-
scriptions and encourage the attributes that we know
people are likely to use. We can encourage a likely
attribute by using its prior likelihood as an estimate
of whether to include it. We can penalize longer de-
scriptions with a penalty proportional to the length
of the property set under construction. In other
words, given a prior likelihood estimate for includ-
ing an attribute att, aatt, and the property set con-
structed so far A, we compute whether to add a prop-
</bodyText>
<figure confidence="0.810604">
a. b.
TUNA corpus GRE3D3 corpus
</figure>
<figureCaption confidence="0.998869">
Figure 4: Example scenes from corpora.
</figureCaption>
<bodyText confidence="0.9756875">
erty for att to A as a function of aatt and the length-
based penalty Y:
</bodyText>
<equation confidence="0.81958">
f(A ∪ {x}) = Yaatt
</equation>
<bodyText confidence="0.851376">
where
</bodyText>
<figure confidence="0.826530818181818">
� 1
�|� |if |A |&gt; 0
Y=
1 otherwise
algorithm then chooses a random number n, 0
1. If n &lt;
{x}), it adds the propert
≤
≤
f(A∪
y.
</figure>
<bodyText confidence="0.9785282">
ter performance than an algori
thm that did not con-
tain this step.
and A is an empirically determined weight. The
n
</bodyText>
<subsectionHeader confidence="0.999586">
4.3 Scanning Through Objects
</subsectionHeader>
<bodyText confidence="0.999922666666667">
After the initial pass through the properties, the al-
gorithm compares each object in the scene that is
the same type as the target. If the values for an
attribute are different, then the corresponding prop-
erty is added to the property set based on the length
penalty alone; when the goal is unique identification,
the algorithm can use no penalty. In development,
we found that incrementally scanning through ob-
jects after initially adding properties resulted in bet-
</bodyText>
<subsectionHeader confidence="0.986764">
4.4 Worked Example
</subsectionHeader>
<bodyText confidence="0.973144833333333">
Suppose the input in Figure 6 (visualized in Figure
4a), with the goal of referring to objl by producing
a property set A. First, the algorithm scans through
color and size in parallel. For color, it finds the cor-
responding value grey; with a computer vision in-
put, this would be possible using the object pixels
as features. There is no length penalty at this point
so it adds the property color:grey to A with
likelihood
For our evaluation domains, acolor
is around .90 across folds, and so a color property is
usually added.
For size, the algorithm finds an appropriate value
using the Size Algorithm from Mitchell et al. (2011).
The Size Algorithm uses the average height an
(|A|=0),
acolor.
d
</bodyText>
<page confidence="0.976781">
1178
</page>
<figureCaption confidence="0.997042">
Figure 3: Basic model for generating visual reference.
</figureCaption>
<bodyText confidence="0.987939">
width of all objects that are the same type as the ref-
erent object; in this case, obj2, obj3, obj4. This re-
turns a size value large, and so the property size:large
is added to A with likelihood αsZZe (around .40 to .70
across folds, depending on the domain).
The most likely property set at this point is sim-
ply (color:grey). The next most likely is (color:grey,
size:large), then (size:large). There are no fur-
ther absolute properties in this example, but there
are values for the relative attributes loc (location)
and ori (orientation). Assuming RP=(location,
orientation), the algorithm first analyzes location,
then orientation. A location property is added to A
with likelihood αlo, multiplied by the length penalty
�= 1
</bodyText>
<equation confidence="0.9713195">
(a�1) if A=(color:grey); �= 1
(a�2) if A=(color:grey,
</equation>
<bodyText confidence="0.968740333333333">
size:large), etc.; and an orientation property is added
to A with likelihood αoTZ multiplied by the length
penalty �= 1
(a�1) if the property set is (color:grey),
etc. At this point, the likelihood of adding further
properties quickly diminishes.
Once all properties have been analyzed, the algo-
rithm scans through the objects in the scene. For
each object obj2... obj,,, if the object is the same
type as the target object obj1, then any different
property of the target referent is added to A with
a likelihood based on the length penalty alone &apos;y.
(type:desk) is added at the end.
For this example scene, the algorithm will gen-
erate the property sets (color:grey, type:desk),
(color:grey, size:large, type:desk), (size:large,
type:desk), (color:grey, ori:front, type:desk),
(color:grey, loc:(3,1), type:desk), etc., with dif-
ferent frequencies. Due to the length penalty,
generated property sets will almost never have more
than 3 properties.
</bodyText>
<figure confidence="0.927430666666667">
tg color:yellow size:(63,63) type:ball loc:right-hand
lm color:red size:(345,345) type:cube loc:right-hand
obj3 color:yellow size:(70,70) type:cube loc:left-hand
</figure>
<figureCaption confidence="0.975207666666667">
Figure 5: Example input scene: GRE3D3 corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
</figureCaption>
<figure confidence="0.997988428571429">
obj1 colour:grey size:(454,454) type:desk loc:(3,1) ori:front
obj2 colour:blue size:(454,454) type:desk loc:(2,1) ori:front
obj3 colour:red size:(454,454) type:desk loc:(3,2) ori:back
obj4 colour:green size:(254,254) type:desk loc:(4,1) ori:left
obj5 colour:blue size:(454,454) type:fan loc:(1,1) ori:front
obj6 colour:red size:(454,454) type:fan loc:(5,1) ori:back
obj7 colour:green size:(254,254) type:fan loc:(2,2) ori:left
</figure>
<figureCaption confidence="0.991064666666667">
Figure 6: Example input scene: TUNA corpus. For IA
And GB, gold-standard size values are provided rather
than measurements (small, large).
</figureCaption>
<bodyText confidence="0.997538333333333">
As such, although (color:grey, type:desk) would
sufficiently distinguish the intended referent, we
instead produce a variety of sets, overspecify-
ing in some instances (e.g., (color:grey, ori:front,
type:desk)), and with a small chance of underspec-
ifying in others (e.g., (size:large, type:desk)).
</bodyText>
<sectionHeader confidence="0.996896" genericHeader="method">
5 Evaluation Algorithms &amp; Corpora
</sectionHeader>
<subsectionHeader confidence="0.964253">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999910818181818">
We evaluate on two well-known REG corpora, the
GRE3D3 corpus (Viethen and Dale, 2008) and the
singular furniture section of the TUNA corpus (van
Deemter et al., 2006). Both corpora contain expres-
sions elicited to computer-generated objects, and so
provide a reasonable starting point for evaluating
reference to visible objects. For all algorithms, we
evaluate on the selection of referent attributes. Lex-
ical choice and word order are not taken into ac-
count. Example images from GRE3D3 and TUNA
are shown in Figure 4, and example algorithm input
</bodyText>
<page confidence="0.99234">
1179
</page>
<bodyText confidence="0.994485428571429">
from these corpora are shown in Figures 5 and 6.
In GRE3D3, we evaluate on the selection of type,
color, size, and location, but leave aside proper-
ties of relatum objects, which are not currently ad-
dressed by this algorithm or the IA. In TUNA, we
evaluate on the selection of type, color, size and
orientation.5
</bodyText>
<subsectionHeader confidence="0.925234">
5.2 Algorithms
5.2.1 The Incremental Algorithm
</subsectionHeader>
<bodyText confidence="0.999971833333333">
The Incremental Algorithm requires a preference
order list (PO) specifying the order to iterate through
scene attributes. We determine the preference or-
der from corpus frequencies using cross-validation
to hold out a test scene and list attributes from the
training scenes in descending order. We find that
color precedes size in the preference orders, in line
with recent research showing that this allows the al-
gorithm to perform optimally on the TUNA corpus
(van Deemter et al., 2012a). In development, we find
that IA performs best with type as the last attribute
in the PO, and report on numbers with this approach.
</bodyText>
<subsectionHeader confidence="0.756661">
5.2.2 The Graph-Based Algorithm
</subsectionHeader>
<bodyText confidence="0.99999123076923">
The version of the Graph-Based Algorithm that
we use is available from Viethen et al. (2008). This
algorithm requires (1) a set of cost functions for each
edge, and (2) a PO for deciding between properties
in the case of a tie. For (1), we use the method from
Theune et al. (2011) to assign two costs (0, 1) to
the edges. We first determine the relative frequency
with which each property is mentioned for a target
object, and then create costs for each property using
k-means clustering (k=2) in the Weka toolkit (Hall
et al., 2009). We refer interested readers to the The-
une et al. paper for further details. For (2), we follow
the same method as for the Incremental Algorithm.
</bodyText>
<subsectionHeader confidence="0.555299">
5.2.3 The Visual Objects Algorithm
</subsectionHeader>
<bodyText confidence="0.914335">
The proposed algorithm requires αatt, which we
estimate as the relative frequency of each attribute
att in the training data. The ordered attribute lists for
the algorithm (AP, RP and P) are built in the same
way as the preference order list for the IA and GB,
listing attributes from the training data in order of
5We remove location from evaluation in this corpus. Lo-
cation is not annotated directly, but split such that only x-
dimension or y-dimension may be marked for a reference.
descending frequency. For these corpora, there are
not absolute properties beyond color, so AP is empty.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999206">
Previous evaluation of REG algorithms have used
measurements such as Uniqueness, Minimality,
Dice (Belz and Gatt, 2008), and Accuracy (Gatt et
al., 2009; Reiter and Belz, 2009). Uniqueness is
the proportion of outputs that identify the referent
uniquely, and Minimality is the proportion of out-
puts that are both minimal and unique. As our goal
is to mimic human reference, these metrics are not
as useful for the evaluations as the others.
The Dice metric provides a value for the similar-
ity between a generated description and a human-
produced description, and therefore serves as a rea-
sonable objective measure for how human-like the
produced sets are. Given the generated property set
(DS) and the human-produced property set (DH),
Dice is calculated as:
</bodyText>
<equation confidence="0.996616">
2 x |DS n DH|
|DS |+ |DH|
</equation>
<bodyText confidence="0.994753666666667">
For each input domain, we evaluate over boolean
values (included or excluded) for the attributes D
(see Table 1). Note that this means the specific val-
ues for the attributes are not compared. In this for-
mulation based on boolean values, |DS|=|DH|=|D|
and Dice reduces to:
</bodyText>
<equation confidence="0.794187">
|DS n DH|
|D|
</equation>
<bodyText confidence="0.9998855">
Calculating Dice over the same number of at-
tributes for both the observed and generated data
has the nice mathematical property of making Dice
equal to other common metrics for evaluating a
model, including Accuracy, Precision, and Recall.6
Since the proposed algorithm is stochastic, this in-
troduces a problem in using a metric that compares
single expressions. We therefore seek to find the
best alignment between the set of expressions pro-
duced by the algorithm and the set of expressions
produced by people. We formulate this alignment as
an assignment problem weighted by Dice. For the
corpus of observed property sets H and the corpus
of generated property sets 5, we find the best align-
</bodyText>
<footnote confidence="0.945577">
6A false positive is a false negative, and there are no true
negatives, so all four metrics are equivalent.
</footnote>
<page confidence="0.956474">
1180
</page>
<table confidence="0.995063">
Example Corresponding Evaluated
Expression Property Set Property Set
the red ball (color:red, type:ball) type:1 color:1
size:0 loc:0
</table>
<tableCaption confidence="0.974713">
Table 1: Example human expression and corresponding
boolean-valued property set for evaluation in GRE3D3,
with D={type, color, size, and location}.
</tableCaption>
<bodyText confidence="0.7023715">
ment x out of all possible alignments X between the
corpora:
</bodyText>
<equation confidence="0.6033025">
�arg maxxEX Dice(DS, DH)
(S,H)Ex
</equation>
<bodyText confidence="0.997997652173913">
This may be solved in polynomial time using the
Hungarian method (Kuhn, 1955; Munkres, 1957).
Note that because IA and GB are deterministic, find-
ing an optimal alignment is trivial. We call this
method ALIGNED DICE.
It is an open question whether an alignment-based
evaluation is fair: the proposed algorithm has more
than one chance to match the human descriptions.
In the second evaluation method (MAJORITY) we
address this issue, comparing how often the most
frequent generated set compares with the most fre-
quent observed set. We run the proposed algorithm
1,000 times, and the generated property sets are or-
dered by frequency. The most frequent generated
set is compared against the most frequent human-
produced set. The majority score is the percentage
of folds where these two sets match. For IA and FB,
the most frequent generated set is the only gener-
ated set. This is a simple way to fairly compare the
output of deterministic and non-deterministic algo-
rithms. There are no ties in the generated sets, but
in the case of a tie in the observed data, we count a
match if any match the most frequent generated set.
</bodyText>
<subsectionHeader confidence="0.99833">
6.1 GRE3D3
</subsectionHeader>
<bodyText confidence="0.9530916">
We randomly select two scenes (7, 9) from Set 1
and their mirrored counterparts in Set 2 (17, 19) for
development. We empirically determine A=5 for the
length-based penalty -y in the proposed algorithm.
We use the eight remaining scenes in each Set
for eight-fold cross-validation, estimating parame-
ters for the algorithms on the seven training scenes
in each fold, as discussed in Section 5.2.
For ALIGNED DICE, we run the proposed algo-
rithm five times in each fold and report the average
</bodyText>
<table confidence="0.9994106">
Algorithm ALIGNED DICE MAJORITY
Set 1 Set 2 Set 1 Set 2
Proposed Alg. 88.23 90.06 62.50 50.00
IA 87.71 85.13 62.50 25.00
GB 87.71 88.73 62.50 50.00
</table>
<tableCaption confidence="0.891369">
Table 2: GRE3D3: Results (in %).
</tableCaption>
<table confidence="0.9999446">
Algorithm ALIGNED DICE MAJORITY
+LOC -LOC +LOC -LOC
Proposed Alg. 88.75 86.07 40.00 40.00
IA 81.79 81.55 0.00 100.00
GB 75.36 66.04 20.00 20.00
</table>
<tableCaption confidence="0.999732">
Table 3: TUNA: Results (in %).
</tableCaption>
<bodyText confidence="0.994054111111111">
score. Results are shown in Table 2.7
The proposed Visible Objects Algorithm achieves
higher accuracy than either version of the Incremen-
tal Algorithm or the Graph-Based Algorithm using
ALIGNED DICE. In MAJORITY, the Graph-Based
and the Visible Objects Algorithm both predict the
majority property set in this evaluation at least 50%
of the time. The algorithm is competitive with the
state of the art on this corpus.
</bodyText>
<subsectionHeader confidence="0.991029">
6.2 TUNA
</subsectionHeader>
<bodyText confidence="0.999962882352941">
TUNA is split into two conditions: subjects discour-
aged to use location (-LOC) or not (+LOC). We ran-
domly hold out two scenes from both conditions (1
and 2), and find a value of A=5 again works well on
the development data.
As in the GRE3D3 corpus, we use the TUNA
scenes in five-fold cross-validation, estimating pa-
rameters on the four training scenes in each fold. For
ALIGNED DICE, we average over five runs of the al-
gorithm, and for MAJORITY, we run the proposed
algorithm 1,000 times for each test scene.
Results are shown in Table 3. Again we see that
the proposed Visible Objects Algorithm is compet-
itive with the IA and GB for both ALIGNED DICE
and MAJORITY. GB performs poorly here, and this
may be due to the data sparsity issue that arises when
requiring the algorithm to train on properties.8 In
</bodyText>
<footnote confidence="0.803068375">
7We do not report statistical significance; the proposed algo-
rithm produces several possible outputs for one input, while the
IA and GB produce only one.
8The original property-based weighting approach (Theune
et al., 2011; Koolen et al., 2012, see Section 5.2) trained on ob-
ject collections that were identical to their test data in all proper-
ties except x- and y-dimension, and so this was less of an issue.
We hope to explore whether basing weights on attributes alone
</footnote>
<page confidence="0.990586">
1181
</page>
<bodyText confidence="0.9997475">
MAJORITY, the Visible Objects Algorithm is rela-
tively stable across conditions, generating the ma-
jority property set in 40% of the test scenes. It does
not outperform the IA in the -LOC condition, but the
IA has a large range across the two conditions (0%
and 100%).
</bodyText>
<sectionHeader confidence="0.985954" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.957040795454546">
We have introduced a new algorithm for generating
referring expressions, inspired by human and com-
puter vision and aiming to refer in a human-like way
to visible objects. The algorithm successfully gener-
ates the most common attributes that people choose
for different objects, and offers a varied output to
capture speaker variation. In contrast to most algo-
rithms for the generation of referring expressions,
which have aimed to produce distinguishing descrip-
tions when these exist (Krahmer and van Deemter,
2012), the core idea behind this algorithm is to gen-
erate what is likely for a speaker in a visual domain.
Since the driving mechanism behind the algorithm
is not to uniquely identify the object, but rather to
pipeline the analysis of properties in a way similar
to human visual processing, the generated expres-
sion may be overspecified or underspecified.
We are limited by available REG corpora to re-
liably assess methods for generating more com-
plex absolute properties like shape and material, but
adding such properties would help advance the gen-
eration of human-like reference in visual scenes and
offers further points of connection between the gen-
eration process and computer vision property detec-
tion. Models for generating more complex spatial
relations are currently available, and are a natural
extension to this framework (e.g., those of Kelleher
and Costello (2009)) as object detection becomes
more robust.
We may also be able to build more sophisticated
graphical models as larger corpora become avail-
able. For example, modeling the conditional proba-
bility of generating reference for a property vn given
the previously generated context p(vn|v1 ... vn−1)
may bring us closer to human-like output.
There are several additional issues that do not
arise in this evaluation, but we expect must be ac-
counted for when referring to naturalistic objects in
improves performance.
visual domains. These include:
• The interconnected nature of properties, where
some properties entail others; for example, a
wooden object is likely to be called wooden, re-
ferring to its material, rather than tan or brown.
</bodyText>
<listItem confidence="0.992988666666667">
• The role of typicality, where properties are se-
lected because they are atypical for the object.
• Referring to more complex properties, e.g., ma-
terial, texture, etc., and object parts.
• Better methods for determining the length
penalty and attribute likelihoods.
</listItem>
<bodyText confidence="0.999236">
We hope to discuss extensions to this algorithm
covering these aspects of reference in future work.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999124">
Funding for this research has been provided by
SICSA and ORSAS. We thank the anonymous re-
viewers for useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.998915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992358642857143">
Carlos Areces, Alexander Koller, and Kristina Striegnitz.
2008. Referring expressions as formulas of descrip-
tion logic. Proceedings of the 5th International Nat-
ural Language Generation Conference (INLG 2008),
pages 42–29.
Ellen Gurman Bard, Robin Hill, Manabu Arai, and
Mary Ellen Foster. 2009. Accessibility and attention
in situated dialogue: Roles and regulations. Proceed-
ings of the Workshop on the Production of Referring
Expressions (PRE-CogSci 2009).
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrin-
sic evaluation measures for referring expression gen-
eration. Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL
2008), pages 197–200.
Alexander C. Berg, Tamara L. Berg, Hal Daum´e III,
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, and Kota Yam-
aguchi. 2011. An exploration of how to learn from
visually descriptive text. JHU-CLSP Summer Work-
shop Whitepaper.
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O’Reilly Me-
dia Inc., Sebastopol, CA.
Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006.
Watching the eyes when talking about size: An investi-
gation of message formulation and utterance planning.
Journal of Memory and Language, 54:592–609.
</reference>
<page confidence="0.962676">
1182
</page>
<reference confidence="0.998834132075472">
Herbert H. Clark and Gregory L. Murphy. 1982. Audi-
ence Design in Meaning and Reference. In J. F. LeNy
and W. Kintsch, editors, Language and Comprehen-
sion, volume 9 of Advances in Psychology, pages 287–
299. North-Holland, Amsterdam.
Herbert H. Clark, Robert Schreuder, and Samuel But-
trick. 1983. Common ground and the understanding
of demonstrative reference. Journal of Verbal Learn-
ing and Verbal Behavior, 22:245–258.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the gricean maxims in the generation
of referring expressions. Cognitive Science, 19:233–
263.
Robert Dale. 1989. Cooking up referring expressions.
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 1989).
P. E. Engelhardt, K. Bailey, and F. Ferreira. 2006. Do
speakers and listeners observe the gricean maxim of
quantity? Journal of Memory and Language, 54:554–
573.
Fang Fang, Huseyin Boyaci, Daniel Kersten, and Scott O.
Murray. 2008. Attention-dependent representation
of a size illusion in human V1. Current biology,
18(21):1707–1712.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR 2009).
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. Advances in Neural Information Processing
Systems (NIPS 2007).
William Ford and David Olson. 1975. The elabora-
tion of the noun phrase in children’s description of ob-
jects. The Journal of Experimental Child Psychology,
19:371–382.
David A. Forsyth. 2011. Personal communica-
tion. Video clip of communication available from:
http://vimeo.com/40553150. At 1:06:46.
T. M. Gann and D. J. Barr. 2013. Speaking from expe-
rience: Audience design as expert performance. Lan-
guage and Cognitive Processes. In press.
Albert Gatt and Anja Belz. 2008. Attribute selec-
tion for referring expression generation: New algo-
rithms and evaluation methods. Proceedings of 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 50–58.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA
REG challenge 2009: Overview and evaluation results.
Proceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 174–
182.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
William S. Horton and Boaz Keysar. 1996. When do
speakers take into account common ground? Cogni-
tion, 59(1):91–117.
Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature Reviews Neuro-
science, 2:194–203.
John Kelleher and Fintan Costello. 2009. Applying
computational models of spatial prepositions to vi-
sually situated dialog. Computational Linguistics,
35(2):271–306.
John Kelleher, Fintan Costello, and Josef van Genabith.
2005. Dynamically structuring, updating and interre-
lating representations of visual and linguistic discourse
context. Artificial Intelligence, 167:62–102.
Boaz Keysar and Anne S. Henly. 2002. Speakers’ over-
estimation of their effectiveness. Psychological Sci-
ence, 13(3):207–212.
Ruud Koolen, Martijn Goudbeek, and Emiel Krahmer.
2011. Effects of scene variation on referential over-
specification. Proceedings of the 33rd Annual Meeting
of the Cognitive Science Society (CogSci 2011).
Ruud Koolen, Emiel Krahmer, and Mari¨et Theune. 2012.
Learning preferences for referring expression genera-
tion: Effects of domain, language and algorithm. Pro-
ceedings of the 7th International Workshop on Natural
Language Generation (INLG 2012).
Emiel Krahmer and Mari¨et Theune. 2002. Efficient
context-sensitive generation of referring expressions.
Information Sharing: Reference and Presupposition
in Language Generation and Interpretation, 143:223–
263.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173–218.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly,
2:83–97.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
Proceedings of the 13th European Workshop on Natu-
ral Language Generation (ENLG 2011).
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of Industrial and
Applied Mathematics, 5(1):32–38.
Scott O. Murray, Huseyin Boyaci, and Daniel Kersten.
2006. The representation of perceived angular size in
human primary visual cortex. Nature Neuroscience,
9(3):429–434.
</reference>
<page confidence="0.646098">
1183
</page>
<reference confidence="0.999876435294118">
David R. Olson. 1970. Language and thought: Aspects
of a cognitive theory of semantics. Psychological Re-
view, 77:257–273.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:89–110.
Ehud Reiter and Anja Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Computa-
tional Linguistics, 35(4):529–558.
Ehud Reiter and Robert Dale. 1992. A fast algorithm
for the generation of referring expressions. Proceed-
ings of the 14th International Conference on Compu-
tational Linguistics (COLING 1992), 1:232–238.
Ehud Reiter and Somayajulu Sripada. 2002. Human
variation and lexical choice. Computational Linguis-
tics, 28:545–553.
D. Samuel Schwarzkopf, Chen Song, and Geraint Rees.
2010. The surface area of human V1 predicts the
subjective experience of object size. Nature Neuro-
science, 14(1):28–30.
J. R. Searle. 1969. Speech Acts: An Essay in the Philos-
ophy of Language. Cambridge University Press, Cam-
bridge.
Susan Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489–508.
Mari¨et Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter – how much
data is required to train a REG algorithm? Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL 2011).
Anne M. Treisman and Garry Gelade. 1980. A feature
integration theory of attention. Cognitive Psychology,
12:97–13.
Anne Treisman. 1985. Preattentive processing in vi-
sion. Computer Vision, Graphics, and Image Process-
ing, 31:156177.
L. G. Ungerleider and M. Mishkin. 1982. Two Corti-
cal Visual Systems. In D. J. Ingle, M. Goodale, and
R. J. W. Mansfield, editors, Analysis of Visual Be-
haviour, chapter 18, pages 549–586. The MIT Press.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. Proceedings
of the 4th International Conference on Natural Lan-
guage Generation (INLG 2006).
Kees van Deemter, Albert Gatt, Ielka van der Sluis, and
Richard Power. 2012a. Generation of referring ex-
pressions: Assessing the incremental algorithm. Cog-
nitive Science, 36(5):799–836.
Kees van Deemter, Emiel Krahmer, Roger van Gompel,
and Albert Gatt. 2012b. Towards a computational psy-
cholinguistics of reference production. TopiCS: Pro-
duction of Referring Expressions - Bridging the Gap
between Computational and Empirical Approaches to
Reference.
Roger P. G. van Gompel, Albert Gatt, Emiel Krahmer,
and Kees van Deemter. 2012. PRO: A computational
model of referential overspecification. Architectures
and Mechanisms for Language Processing (AMLaP
2012).
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 59–67.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. Proceedings of the 8th Australasian Lan-
guage Technology Workshop (ALTW 2010), pages 81–
89.
Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et The-
une, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008).
Jette Viethen, Martijn Goudbeek, and Emiel Krahmer.
2012. The impact of colour difference and coloure
codability on reference production. Proceedings of the
34th Annual Meeting of the Cognitive Science Society
(CogSci 2012).
G. J. Whitehurst. 1976. The development of communi-
cation: Changes with age and modeling. Child Devel-
opment, 47:473–482.
</reference>
<page confidence="0.995257">
1184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819203">
<title confidence="0.999933">Generating Expressions that Refer to Visible Objects</title>
<author confidence="0.998838">Margaret Mitchell Kees van_Deemter Ehud Reiter</author>
<affiliation confidence="0.907068">Johns Hopkins HLTCOE University of Aberdeen University of</affiliation>
<email confidence="0.923426">m.mitchell@jhu.eduk.vdeemter@abdn.ac.uke.reiter@abdn.ac.uk</email>
<abstract confidence="0.99859445">We introduce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. We call such expressions The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the Graph- Based Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carlos Areces</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>Referring expressions as formulas of description logic.</title>
<date>2008</date>
<booktitle>Proceedings of the 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>42--29</pages>
<contexts>
<context position="3851" citStr="Areces et al., 2008" startWordPosition="586" endWordPosition="589">n method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Motivation &amp; Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on </context>
</contexts>
<marker>Areces, Koller, Striegnitz, 2008</marker>
<rawString>Carlos Areces, Alexander Koller, and Kristina Striegnitz. 2008. Referring expressions as formulas of description logic. Proceedings of the 5th International Natural Language Generation Conference (INLG 2008), pages 42–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Gurman Bard</author>
<author>Robin Hill</author>
<author>Manabu Arai</author>
<author>Mary Ellen Foster</author>
</authors>
<title>Accessibility and attention in situated dialogue: Roles and regulations.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on the Production of Referring Expressions</booktitle>
<contexts>
<context position="5116" citStr="Bard et al., 2009" startWordPosition="787" endWordPosition="790">9; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amount of cognitive effort considering the perception of a hearer (Keysar and Henly, 2002). We take this evidence to suggest an approach for a visual reference algorithm that generates natural, human-like reference by generating visual properties that are salient for a speaker.1 We can understand what is salient visually (what does the visual system first respond to, what guides attention?), linguistically (what do people tend to mention in visual scenes?), and cognitively, which we will not have room to discuss in this paper (what is atypical for 1We can also add fun</context>
<context position="6810" citStr="Bard et al., 2009" startWordPosition="1061" endWordPosition="1064">f broader notions of salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of t</context>
</contexts>
<marker>Bard, Hill, Arai, Foster, 2009</marker>
<rawString>Ellen Gurman Bard, Robin Hill, Manabu Arai, and Mary Ellen Foster. 2009. Accessibility and attention in situated dialogue: Roles and regulations. Proceedings of the Workshop on the Production of Referring Expressions (PRE-CogSci 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Albert Gatt</author>
</authors>
<title>Intrinsic vs. extrinsic evaluation measures for referring expression generation.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>197--200</pages>
<contexts>
<context position="26728" citStr="Belz and Gatt, 2008" startWordPosition="4294" endWordPosition="4297">t in the training data. The ordered attribute lists for the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS) and the human-produced pro</context>
</contexts>
<marker>Belz, Gatt, 2008</marker>
<rawString>Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic evaluation measures for referring expression generation. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL 2008), pages 197–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daum´e Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Karl Stratos</author>
<author>Kota Yamaguchi</author>
</authors>
<title>An exploration of how to learn from visually descriptive text. JHU-CLSP Summer Workshop Whitepaper.</title>
<date>2011</date>
<contexts>
<context position="7680" citStr="Berg et al., 2011" startWordPosition="1197" endWordPosition="1200">e linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter and Sripada (2002)). Detecting size does not work well (Forsyth, 1175 2011); and when it does, it will likely not take the form supposed in recent generation work. Most REG algorithms use a predefined single-featured value, such as “big”; however, given an image-based input, obtaining such a value requires (1) determining how the object is situated in a three-dimensional space, difficult to obtain from a two-dimensional ima</context>
<context position="17021" citStr="Berg et al., 2011" startWordPosition="2716" endWordPosition="2719">lable online.4 After START, the algorithm proceeds in parallel through a list of absolute attributes and a list of relative attributes. The likelihood of generating a property is a function of the prior likelihood αatt and &apos;y, a penalty on the length of the constructed property set up to that point. This ensures that only a few properties are generated for a referent, and the expression will not be too complex. This is also in line with recent research suggesting that there are rarely more than three adjectives in a visual noun phrase 4https://github.com/itallow/VisibleObjectsAlgorithm. 1177 (Berg et al., 2011). Once the algorithm hits END, it scans through the objects in the scene. If it finds an object that is the same type as the referent object, the algorithm checks through the attributes again in a preference order akin to the IA, comparing the object’s properties against the referent’s and generating properties as a function of the length penalty alone. If the algorithm does not find an object that it is the same type, no further properties are added. 4.1 Requirements The algorithm requires the following: 1. Prior likelihood estimates on the inclusion of different attributes. Represented as aa</context>
</contexts>
<marker>Berg, Berg, Dodge, Goyal, Han, Mensch, Mitchell, Stratos, Yamaguchi, 2011</marker>
<rawString>Alexander C. Berg, Tamara L. Berg, Hal Daum´e III, Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, and Kota Yamaguchi. 2011. An exploration of how to learn from visually descriptive text. JHU-CLSP Summer Workshop Whitepaper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.,</booktitle>
<location>Sebastopol, CA.</location>
<contexts>
<context position="13974" citStr="Bird et al., 2009" startWordPosition="2238" endWordPosition="2241">operties chosen by the algorithm is called a distinguishing description. The Incremental Algorithm (IA) proceeds by iterating through attributes in a predefined order (a preference order), and for each attribute, it checks whether specifying a value would rule out at least one item in the contrast set that has not already been ruled out. If it will, the attribute:�alue is added to the distinguishing description. This process continues until all contrast items (distractors) are ruled out or all available properties have been checked. We use the implementation of the IA available from the NLTK (Bird et al., 2009).3 In the Graph-Based Algorithm (GB), the objects in the discourse are represented within a labeled directed graph, and content selection is a subgraph construction problem. Each object is represented as a vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during sear</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc., Sebastopol, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Brown-Schmidt</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Watching the eyes when talking about size: An investigation of message formulation and utterance planning.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>54--592</pages>
<contexts>
<context position="7471" citStr="Brown-Schmidt and Tanenhaus, 2006" startWordPosition="1167" endWordPosition="1170">erand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter and Sripada (2002)). Detecting size does not work well (Forsyth, 1175 2011); and when it does, it will likely not take the form supposed in recent generation work. Most REG algorithms use a predefined single-featured v</context>
</contexts>
<marker>Brown-Schmidt, Tanenhaus, 2006</marker>
<rawString>Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006. Watching the eyes when talking about size: An investigation of message formulation and utterance planning. Journal of Memory and Language, 54:592–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Gregory L Murphy</author>
</authors>
<title>Audience Design in Meaning and Reference.</title>
<date>1982</date>
<journal>Language and Comprehension,</journal>
<booktitle>of Advances in Psychology,</booktitle>
<volume>9</volume>
<pages>287--299</pages>
<editor>In J. F. LeNy and W. Kintsch, editors,</editor>
<publisher>North-Holland,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="6766" citStr="Clark and Murphy, 1982" startWordPosition="1053" endWordPosition="1056">al. (2006)). this object?); as well as in terms of broader notions of salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with s</context>
</contexts>
<marker>Clark, Murphy, 1982</marker>
<rawString>Herbert H. Clark and Gregory L. Murphy. 1982. Audience Design in Meaning and Reference. In J. F. LeNy and W. Kintsch, editors, Language and Comprehension, volume 9 of Advances in Psychology, pages 287– 299. North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Robert Schreuder</author>
<author>Samuel Buttrick</author>
</authors>
<title>Common ground and the understanding of demonstrative reference.</title>
<date>1983</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>22--245</pages>
<contexts>
<context position="4769" citStr="Clark et al., 1983" startWordPosition="728" endWordPosition="731">lly in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amount of cognitive effort considering the perception of a hearer (Keysar and Henly, 2002). We take this evidence to suggest an approach for a visual reference algorithm that generates natural, human-like reference by generatin</context>
</contexts>
<marker>Clark, Schreuder, Buttrick, 1983</marker>
<rawString>Herbert H. Clark, Robert Schreuder, and Samuel Buttrick. 1983. Common ground and the understanding of demonstrative reference. Journal of Verbal Learning and Verbal Behavior, 22:245–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<pages>263</pages>
<contexts>
<context position="794" citStr="Dale and Reiter, 1995" startWordPosition="106" endWordPosition="109">chell@jhu.edu k.vdeemter@abdn.ac.uk e.reiter@abdn.ac.uk Abstract We introduce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. We call such expressions identifying descriptions. The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the GraphBased Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account. 1 Introduction Referring expression generation (REG) is the task of generating an expression that can identify a referent to a listener. These expressions generally take the form of a definite noun phrase such as “the large orange plate” or “the furry running dog”. Research in REG primarily focuses on the subtask of selecting a set </context>
<context position="3807" citStr="Dale and Reiter, 1995" startWordPosition="578" endWordPosition="581"> requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Motivation &amp; Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonn</context>
<context position="12843" citStr="Dale and Reiter, 1995" startWordPosition="2055" endWordPosition="2058">rties in order to tease out further details of visual reference, but must leave these details for follow up work; for now, we focus on the properties common to REG corpora. 1176 run the algorithm for as many instances as we have in our test data, and see how well the property sets it produces aligns to the observed property sets. We discuss evaluation using both methods in Section 6. 3 The State of the Art in REG 3.1 Algorithms In order to understand how this approach compares to the state of the art in REG, we evaluate against two of the most well-known algorithms, the Incremental Algorithm (Dale and Reiter, 1995) and the Graph-Based Algorithm (Krahmer et. al, 2003, as implemented in Viethen et al., 2008). Details on these algorithms are available in their corresponding papers. As a brief summary, both algorithms formalize the objects in the discourse as a set of properties (attribute:�alue pairs). For example, one object may be represented as (type:box, color:red, size:large). The task is to find the set of properties that uniquely specify the referent. This is known as a content selection problem, and the set of properties chosen by the algorithm is called a distinguishing description. The Incrementa</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19:233– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3761" citStr="Dale, 1989" startWordPosition="571" endWordPosition="572">l input; saying an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Motivation &amp; Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Robert Dale. 1989. Cooking up referring expressions. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL 1989).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Engelhardt</author>
<author>K Bailey</author>
<author>F Ferreira</author>
</authors>
<title>Do speakers and listeners observe the gricean maxim of quantity?</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<volume>54</volume>
<pages>573</pages>
<contexts>
<context position="4524" citStr="Engelhardt et al., 2006" startWordPosition="690" endWordPosition="693">on (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without</context>
</contexts>
<marker>Engelhardt, Bailey, Ferreira, 2006</marker>
<rawString>P. E. Engelhardt, K. Bailey, and F. Ferreira. 2006. Do speakers and listeners observe the gricean maxim of quantity? Journal of Memory and Language, 54:554– 573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Fang</author>
<author>Huseyin Boyaci</author>
<author>Daniel Kersten</author>
<author>Scott O Murray</author>
</authors>
<title>Attention-dependent representation of a size illusion in human V1. Current biology,</title>
<date>2008</date>
<pages>18--21</pages>
<contexts>
<context position="7228" citStr="Fang et al., 2008" startWordPosition="1126" endWordPosition="1129">us experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in le</context>
</contexts>
<marker>Fang, Boyaci, Kersten, Murray, 2008</marker>
<rawString>Fang Fang, Huseyin Boyaci, Daniel Kersten, and Scott O. Murray. 2008. Attention-dependent representation of a size illusion in human V1. Current biology, 18(21):1707–1712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</booktitle>
<contexts>
<context position="9909" citStr="Farhadi et al., 2009" startWordPosition="1569" endWordPosition="1572">f the image” or “at the top of the image”. Along with size, location and orientation make up the three primary relative properties that we aim to generate language for. After the simple forms for color, size, location, and orientation properties are processed, our visual system feeds forward to two parallel pathways, the so-called “what” and “where” pathways (Ungerleider and Mishkin, 1982), which process properties with growing complexity. The “what” pathway includes absolute properties like shape and material, which computer vision has had some success detecting (Ferrari and Zisserman, 2007; Farhadi et al., 2009) while the “where” pathway corresponds to more complex spatial orientation and location information, such as where objects are relative to one another and which way they are facing. To begin connecting this process to the generation of human-like descriptions of visible objects, we start with the following simplification: Color and size have a privileged status, the first properties processed. These are followed by the relative properties Figure 2: Initial model for generating visual reference. of location and orientation, which may feed forward to more complex location and orientation propert</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ferrari</author>
<author>A Zisserman</author>
</authors>
<title>Learning visual attributes.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="9886" citStr="Ferrari and Zisserman, 2007" startWordPosition="1565" endWordPosition="1568">uch as, e.g., “on the right of the image” or “at the top of the image”. Along with size, location and orientation make up the three primary relative properties that we aim to generate language for. After the simple forms for color, size, location, and orientation properties are processed, our visual system feeds forward to two parallel pathways, the so-called “what” and “where” pathways (Ungerleider and Mishkin, 1982), which process properties with growing complexity. The “what” pathway includes absolute properties like shape and material, which computer vision has had some success detecting (Ferrari and Zisserman, 2007; Farhadi et al., 2009) while the “where” pathway corresponds to more complex spatial orientation and location information, such as where objects are relative to one another and which way they are facing. To begin connecting this process to the generation of human-like descriptions of visible objects, we start with the following simplification: Color and size have a privileged status, the first properties processed. These are followed by the relative properties Figure 2: Initial model for generating visual reference. of location and orientation, which may feed forward to more complex location </context>
</contexts>
<marker>Ferrari, Zisserman, 2007</marker>
<rawString>V. Ferrari and A. Zisserman. 2007. Learning visual attributes. Advances in Neural Information Processing Systems (NIPS 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Ford</author>
<author>David Olson</author>
</authors>
<title>The elaboration of the noun phrase in children’s description of objects.</title>
<date>1975</date>
<journal>The Journal of Experimental Child Psychology,</journal>
<pages>19--371</pages>
<contexts>
<context position="4383" citStr="Ford and Olson, 1975" startWordPosition="669" endWordPosition="672">Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set</context>
</contexts>
<marker>Ford, Olson, 1975</marker>
<rawString>William Ford and David Olson. 1975. The elaboration of the noun phrase in children’s description of objects. The Journal of Experimental Child Psychology, 19:371–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Forsyth</author>
</authors>
<title>Personal communication. Video clip of communication available from: http://vimeo.com/40553150. At 1:06:46.</title>
<date>2011</date>
<marker>Forsyth, 2011</marker>
<rawString>David A. Forsyth. 2011. Personal communication. Video clip of communication available from: http://vimeo.com/40553150. At 1:06:46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Gann</author>
<author>D J Barr</author>
</authors>
<title>Speaking from experience: Audience design as expert performance. Language and Cognitive Processes.</title>
<date>2013</date>
<publisher>In press.</publisher>
<contexts>
<context position="6832" citStr="Gann and Barr, 2013" startWordPosition="1065" endWordPosition="1068">f salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the sc</context>
</contexts>
<marker>Gann, Barr, 2013</marker>
<rawString>T. M. Gann and D. J. Barr. 2013. Speaking from experience: Audience design as expert performance. Language and Cognitive Processes. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
</authors>
<title>Attribute selection for referring expression generation: New algorithms and evaluation methods.</title>
<date>2008</date>
<booktitle>Proceedings of 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>50--58</pages>
<contexts>
<context position="14778" citStr="Gatt and Belz, 2008" startWordPosition="2355" endWordPosition="2358"> is represented as a vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the </context>
</contexts>
<marker>Gatt, Belz, 2008</marker>
<rawString>Albert Gatt and Anja Belz. 2008. Attribute selection for referring expression generation: New algorithms and evaluation methods. Proceedings of 5th International Natural Language Generation Conference (INLG 2008), pages 50–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNA REG challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>Proceedings of the 12th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>174--182</pages>
<contexts>
<context position="14798" citStr="Gatt et al., 2009" startWordPosition="2359" endWordPosition="2362">vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the scene, rather than r</context>
<context position="26761" citStr="Gatt et al., 2009" startWordPosition="4300" endWordPosition="4303">attribute lists for the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS) and the human-produced property set (DH), Dice is calculate</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA REG challenge 2009: Overview and evaluation results. Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 174– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="25824" citStr="Hall et al., 2009" startWordPosition="4143" endWordPosition="4146">in the PO, and report on numbers with this approach. 5.2.2 The Graph-Based Algorithm The version of the Graph-Based Algorithm that we use is available from Viethen et al. (2008). This algorithm requires (1) a set of cost functions for each edge, and (2) a PO for deciding between properties in the case of a tie. For (1), we use the method from Theune et al. (2011) to assign two costs (0, 1) to the edges. We first determine the relative frequency with which each property is mentioned for a target object, and then create costs for each property using k-means clustering (k=2) in the Weka toolkit (Hall et al., 2009). We refer interested readers to the Theune et al. paper for further details. For (2), we follow the same method as for the Incremental Algorithm. 5.2.3 The Visual Objects Algorithm The proposed algorithm requires αatt, which we estimate as the relative frequency of each attribute att in the training data. The ordered attribute lists for the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5We remove location from evaluation in this corpus. Location is not annotated directly, but split such </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William S Horton</author>
<author>Boaz Keysar</author>
</authors>
<title>When do speakers take into account common ground?</title>
<date>1996</date>
<journal>Cognition,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="5096" citStr="Horton and Keysar, 1996" startWordPosition="783" endWordPosition="786">al domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amount of cognitive effort considering the perception of a hearer (Keysar and Henly, 2002). We take this evidence to suggest an approach for a visual reference algorithm that generates natural, human-like reference by generating visual properties that are salient for a speaker.1 We can understand what is salient visually (what does the visual system first respond to, what guides attention?), linguistically (what do people tend to mention in visual scenes?), and cognitively, which we will not have room to discuss in this paper (what is atypical for </context>
<context position="6791" citStr="Horton and Keysar, 1996" startWordPosition="1057" endWordPosition="1060">?); as well as in terms of broader notions of salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is </context>
</contexts>
<marker>Horton, Keysar, 1996</marker>
<rawString>William S. Horton and Boaz Keysar. 1996. When do speakers take into account common ground? Cognition, 59(1):91–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Itti</author>
<author>Christof Koch</author>
</authors>
<title>Computational modelling of visual attention.</title>
<date>2001</date>
<journal>Nature Reviews Neuroscience,</journal>
<pages>2--194</pages>
<contexts>
<context position="9054" citStr="Itti and Koch, 2001" startWordPosition="1430" endWordPosition="1433">ct is likely to exist (its bounding box), as well as the x- and y-axis locations of the pixels within the object detection; but a value from these features like “big”, “tall”, or “long” requires further reasoning. As such, we incorporate the top-performing size algorithm introduced in Mitchell et al. (2011), which takes as input the height and widths of objects in the image and outputs a size value or NONE, indicating that size should not be used to describe the object. In addition to color and size, location and orientation begin to be processed early on in the visual system (Treisman, 1985; Itti and Koch, 2001), with our first perception of location corresponding to basic cues of where an object is relative to our focus of attention. For an input image, this simple type of location corresponds to surface forms such as, e.g., “on the right of the image” or “at the top of the image”. Along with size, location and orientation make up the three primary relative properties that we aim to generate language for. After the simple forms for color, size, location, and orientation properties are processed, our visual system feeds forward to two parallel pathways, the so-called “what” and “where” pathways (Unge</context>
</contexts>
<marker>Itti, Koch, 2001</marker>
<rawString>Laurent Itti and Christof Koch. 2001. Computational modelling of visual attention. Nature Reviews Neuroscience, 2:194–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kelleher</author>
<author>Fintan Costello</author>
</authors>
<title>Applying computational models of spatial prepositions to visually situated dialog.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="34233" citStr="Kelleher and Costello (2009)" startWordPosition="5554" endWordPosition="5557">a way similar to human visual processing, the generated expression may be overspecified or underspecified. We are limited by available REG corpora to reliably assess methods for generating more complex absolute properties like shape and material, but adding such properties would help advance the generation of human-like reference in visual scenes and offers further points of connection between the generation process and computer vision property detection. Models for generating more complex spatial relations are currently available, and are a natural extension to this framework (e.g., those of Kelleher and Costello (2009)) as object detection becomes more robust. We may also be able to build more sophisticated graphical models as larger corpora become available. For example, modeling the conditional probability of generating reference for a property vn given the previously generated context p(vn|v1 ... vn−1) may bring us closer to human-like output. There are several additional issues that do not arise in this evaluation, but we expect must be accounted for when referring to naturalistic objects in improves performance. visual domains. These include: • The interconnected nature of properties, where some proper</context>
</contexts>
<marker>Kelleher, Costello, 2009</marker>
<rawString>John Kelleher and Fintan Costello. 2009. Applying computational models of spatial prepositions to visually situated dialog. Computational Linguistics, 35(2):271–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kelleher</author>
<author>Fintan Costello</author>
<author>Josef van Genabith</author>
</authors>
<title>Dynamically structuring, updating and interrelating representations of visual and linguistic discourse context.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>167--62</pages>
<marker>Kelleher, Costello, van Genabith, 2005</marker>
<rawString>John Kelleher, Fintan Costello, and Josef van Genabith. 2005. Dynamically structuring, updating and interrelating representations of visual and linguistic discourse context. Artificial Intelligence, 167:62–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boaz Keysar</author>
<author>Anne S Henly</author>
</authors>
<title>Speakers’ overestimation of their effectiveness.</title>
<date>2002</date>
<journal>Psychological Science,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="5232" citStr="Keysar and Henly, 2002" startWordPosition="805" endWordPosition="808">select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amount of cognitive effort considering the perception of a hearer (Keysar and Henly, 2002). We take this evidence to suggest an approach for a visual reference algorithm that generates natural, human-like reference by generating visual properties that are salient for a speaker.1 We can understand what is salient visually (what does the visual system first respond to, what guides attention?), linguistically (what do people tend to mention in visual scenes?), and cognitively, which we will not have room to discuss in this paper (what is atypical for 1We can also add functionality to ensure that a referent is uniquely identified against the contrast set (whether or not that reflects w</context>
</contexts>
<marker>Keysar, Henly, 2002</marker>
<rawString>Boaz Keysar and Anne S. Henly. 2002. Speakers’ overestimation of their effectiveness. Psychological Science, 13(3):207–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruud Koolen</author>
<author>Martijn Goudbeek</author>
<author>Emiel Krahmer</author>
</authors>
<title>Effects of scene variation on referential overspecification.</title>
<date>2011</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (CogSci</booktitle>
<contexts>
<context position="4546" citStr="Koolen et al., 2011" startWordPosition="694" endWordPosition="697">tion of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amou</context>
</contexts>
<marker>Koolen, Goudbeek, Krahmer, 2011</marker>
<rawString>Ruud Koolen, Martijn Goudbeek, and Emiel Krahmer. 2011. Effects of scene variation on referential overspecification. Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (CogSci 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruud Koolen</author>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Learning preferences for referring expression generation: Effects of domain, language and algorithm.</title>
<date>2012</date>
<booktitle>Proceedings of the 7th International Workshop on Natural Language Generation (INLG</booktitle>
<contexts>
<context position="32316" citStr="Koolen et al., 2012" startWordPosition="5240" endWordPosition="5243"> for MAJORITY, we run the proposed algorithm 1,000 times for each test scene. Results are shown in Table 3. Again we see that the proposed Visible Objects Algorithm is competitive with the IA and GB for both ALIGNED DICE and MAJORITY. GB performs poorly here, and this may be due to the data sparsity issue that arises when requiring the algorithm to train on properties.8 In 7We do not report statistical significance; the proposed algorithm produces several possible outputs for one input, while the IA and GB produce only one. 8The original property-based weighting approach (Theune et al., 2011; Koolen et al., 2012, see Section 5.2) trained on object collections that were identical to their test data in all properties except x- and y-dimension, and so this was less of an issue. We hope to explore whether basing weights on attributes alone 1181 MAJORITY, the Visible Objects Algorithm is relatively stable across conditions, generating the majority property set in 40% of the test scenes. It does not outperform the IA in the -LOC condition, but the IA has a large range across the two conditions (0% and 100%). 7 Conclusions and Future Work We have introduced a new algorithm for generating referring expressio</context>
</contexts>
<marker>Koolen, Krahmer, Theune, 2012</marker>
<rawString>Ruud Koolen, Emiel Krahmer, and Mari¨et Theune. 2012. Learning preferences for referring expression generation: Effects of domain, language and algorithm. Proceedings of the 7th International Workshop on Natural Language Generation (INLG 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Efficient context-sensitive generation of referring expressions. Information Sharing: Reference and Presupposition in Language Generation and Interpretation,</title>
<date>2002</date>
<pages>143--223</pages>
<contexts>
<context position="6275" citStr="Krahmer and Theune, 2002" startWordPosition="979" endWordPosition="982">o discuss in this paper (what is atypical for 1We can also add functionality to ensure that a referent is uniquely identified against the contrast set (whether or not that reflects what a person would do), which we will describe. Figure 1: Relative properties, like size and location, are difficult to obtain from a two-dimensional image. We find it easy to perceive the background object as larger than the one in the front; but they are technically the same size in the image (from Murray et al. (2006)). this object?); as well as in terms of broader notions of salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>Emiel Krahmer and Mari¨et Theune. 2002. Efficient context-sensitive generation of referring expressions. Information Sharing: Reference and Presupposition in Language Generation and Interpretation, 143:223– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<pages>38--173</pages>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38:173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="28936" citStr="Kuhn, 1955" startWordPosition="4661" endWordPosition="4662">erated property sets 5, we find the best align6A false positive is a false negative, and there are no true negatives, so all four metrics are equivalent. 1180 Example Corresponding Evaluated Expression Property Set Property Set the red ball (color:red, type:ball) type:1 color:1 size:0 loc:0 Table 1: Example human expression and corresponding boolean-valued property set for evaluation in GRE3D3, with D={type, color, size, and location}. ment x out of all possible alignments X between the corpora: �arg maxxEX Dice(DS, DH) (S,H)Ex This may be solved in polynomial time using the Hungarian method (Kuhn, 1955; Munkres, 1957). Note that because IA and GB are deterministic, finding an optimal alignment is trivial. We call this method ALIGNED DICE. It is an open question whether an alignment-based evaluation is fair: the proposed algorithm has more than one chance to match the human descriptions. In the second evaluation method (MAJORITY) we address this issue, comparing how often the most frequent generated set compares with the most frequent observed set. We run the proposed algorithm 1,000 times, and the generated property sets are ordered by frequency. The most frequent generated set is compared </context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H. W. Kuhn. 1955. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Two approaches for generating size modifiers.</title>
<date>2011</date>
<booktitle>Proceedings of the 13th European Workshop on Natural Language Generation (ENLG</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2011</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2011. Two approaches for generating size modifiers. Proceedings of the 13th European Workshop on Natural Language Generation (ENLG 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of Industrial and Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="28952" citStr="Munkres, 1957" startWordPosition="4663" endWordPosition="4664">rty sets 5, we find the best align6A false positive is a false negative, and there are no true negatives, so all four metrics are equivalent. 1180 Example Corresponding Evaluated Expression Property Set Property Set the red ball (color:red, type:ball) type:1 color:1 size:0 loc:0 Table 1: Example human expression and corresponding boolean-valued property set for evaluation in GRE3D3, with D={type, color, size, and location}. ment x out of all possible alignments X between the corpora: �arg maxxEX Dice(DS, DH) (S,H)Ex This may be solved in polynomial time using the Hungarian method (Kuhn, 1955; Munkres, 1957). Note that because IA and GB are deterministic, finding an optimal alignment is trivial. We call this method ALIGNED DICE. It is an open question whether an alignment-based evaluation is fair: the proposed algorithm has more than one chance to match the human descriptions. In the second evaluation method (MAJORITY) we address this issue, comparing how often the most frequent generated set compares with the most frequent observed set. We run the proposed algorithm 1,000 times, and the generated property sets are ordered by frequency. The most frequent generated set is compared against the most</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of Industrial and Applied Mathematics, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott O Murray</author>
<author>Huseyin Boyaci</author>
<author>Daniel Kersten</author>
</authors>
<title>The representation of perceived angular size in human primary visual cortex.</title>
<date>2006</date>
<journal>Nature Neuroscience,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="6154" citStr="Murray et al. (2006)" startWordPosition="960" endWordPosition="963">), linguistically (what do people tend to mention in visual scenes?), and cognitively, which we will not have room to discuss in this paper (what is atypical for 1We can also add functionality to ensure that a referent is uniquely identified against the contrast set (whether or not that reflects what a person would do), which we will describe. Figure 1: Relative properties, like size and location, are difficult to obtain from a two-dimensional image. We find it easy to perceive the background object as larger than the one in the front; but they are technically the same size in the image (from Murray et al. (2006)). this object?); as well as in terms of broader notions of salience, e.g., discourse salience (Krahmer and Theune, 2002). This suggests a paradigm shift in the generation task when referring to visible objects, if the goal is to produce human-like reference. In particular, this suggests moving from selecting properties that rule out other scene objects to selecting properties that are salient for the speaker (visually, conversationally, based on previous experiences, etc.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and </context>
</contexts>
<marker>Murray, Boyaci, Kersten, 2006</marker>
<rawString>Scott O. Murray, Huseyin Boyaci, and Daniel Kersten. 2006. The representation of perceived angular size in human primary visual cortex. Nature Neuroscience, 9(3):429–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Olson</author>
</authors>
<title>Language and thought: Aspects of a cognitive theory of semantics.</title>
<date>1970</date>
<journal>Psychological Review,</journal>
<pages>77--257</pages>
<contexts>
<context position="3910" citStr="Olson (1970)" startWordPosition="599" endWordPosition="600">ed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Motivation &amp; Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhard</context>
</contexts>
<marker>Olson, 1970</marker>
<rawString>David R. Olson. 1970. Language and thought: Aspects of a cognitive theory of semantics. Psychological Review, 77:257–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Pechmann</author>
</authors>
<title>Incremental speech production and referential overspecification.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<pages>27--89</pages>
<contexts>
<context position="4499" citStr="Pechmann, 1989" startWordPosition="688" endWordPosition="689"> outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; B</context>
<context position="7301" citStr="Pechmann, 1989" startWordPosition="1140" endWordPosition="1141"> audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter and Sripada (2002)). Detecting size does not wor</context>
<context position="16157" citStr="Pechmann, 1989" startWordPosition="2574" endWordPosition="2575">ariation. Due to the fundamentally different objective of this algorithm, we will call the kind of expression it generates an identifying description, following Searle (1969). This is a description that the system finds (1) useful to describe the referent and (2) true of the referent. 4 The Algorithm The Visible Objects Algorithm iterates through lists of visible attributes, stochastically adding properties to the property set it will generate. After this initial search, the algorithm then scans through the objects in the scene, roughly corresponding to how people scan a scene when referring (Pechmann, 1989). The target referent type, corresponding to the head noun in the final generated description, is added to the property set at the end of the algorithm. We represent the basic components of the algorithm graphically in Figure 3. Full code is available online.4 After START, the algorithm proceeds in parallel through a list of absolute attributes and a list of relative attributes. The likelihood of generating a property is a function of the prior likelihood αatt and &apos;y, a penalty on the length of the constructed property set up to that point. This ensures that only a few properties are generated</context>
<context position="18483" citStr="Pechmann, 1989" startWordPosition="2965" endWordPosition="2966">o scan through other scene objects. The current implementation uses the order in which the objects are listed in the corpora it is run on. (1) is similar to the cost functions for GB, but attributes are selected non-deterministically using prior likelihoods. (2), (3), and (4) are similar to the IA’s and GB’s preference order. For our evaluation corpora, AP is empty and RP contains location and orientation. (5) is novel to this algorithm, defining an order in which to compare the target object against other objects in the scene. This is inspired by the process of incremental speech production (Pechmann, 1989), where speakers scan objects during naming, incrementally producing properties. 4.2 The Stochastic Process Generally speaking, we want to penalize longer descriptions and encourage the attributes that we know people are likely to use. We can encourage a likely attribute by using its prior likelihood as an estimate of whether to include it. We can penalize longer descriptions with a penalty proportional to the length of the property set under construction. In other words, given a prior likelihood estimate for including an attribute att, aatt, and the property set constructed so far A, we compu</context>
</contexts>
<marker>Pechmann, 1989</marker>
<rawString>Thomas Pechmann. 1989. Incremental speech production and referential overspecification. Linguistics, 27:89–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Anja Belz</author>
</authors>
<title>An investigation into the validity of some metrics for automatically evaluating natural language generation systems.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="26785" citStr="Reiter and Belz, 2009" startWordPosition="4304" endWordPosition="4307"> the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS) and the human-produced property set (DH), Dice is calculated as: 2 x |DS n DH| |DS </context>
</contexts>
<marker>Reiter, Belz, 2009</marker>
<rawString>Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th International Conference on Computational Linguistics (COLING</booktitle>
<pages>1--232</pages>
<contexts>
<context position="3784" citStr="Reiter and Dale, 1992" startWordPosition="573" endWordPosition="577">ing an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Motivation &amp; Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975;</context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>Ehud Reiter and Robert Dale. 1992. A fast algorithm for the generation of referring expressions. Proceedings of the 14th International Conference on Computational Linguistics (COLING 1992), 1:232–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Human variation and lexical choice.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--545</pages>
<contexts>
<context position="7871" citStr="Reiter and Sripada (2002)" startWordPosition="1229" endWordPosition="1232">l., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter and Sripada (2002)). Detecting size does not work well (Forsyth, 1175 2011); and when it does, it will likely not take the form supposed in recent generation work. Most REG algorithms use a predefined single-featured value, such as “big”; however, given an image-based input, obtaining such a value requires (1) determining how the object is situated in a three-dimensional space, difficult to obtain from a two-dimensional image (see Figure 1); and (2) determining what the value should be: object detectors currently can provide the height and width of the location where an object is likely to exist (its bounding b</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002. Human variation and lexical choice. Computational Linguistics, 28:545–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Samuel Schwarzkopf</author>
<author>Chen Song</author>
<author>Geraint Rees</author>
</authors>
<title>The surface area of human V1 predicts the subjective experience of object size.</title>
<date>2010</date>
<journal>Nature Neuroscience,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="7255" citStr="Schwarzkopf et al., 2010" startWordPosition="1130" endWordPosition="1133">.). This mirrors related research on the tradeoff between audience design and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter an</context>
</contexts>
<marker>Schwarzkopf, Song, Rees, 2010</marker>
<rawString>D. Samuel Schwarzkopf, Chen Song, and Geraint Rees. 2010. The surface area of human V1 predicts the subjective experience of object size. Nature Neuroscience, 14(1):28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="15716" citStr="Searle (1969)" startWordPosition="2505" endWordPosition="2506">riving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the scene, rather than ruling out all distractors. This offers a way to capture some aspects of human-like reference, including underand overspecification and speaker variation. Due to the fundamentally different objective of this algorithm, we will call the kind of expression it generates an identifying description, following Searle (1969). This is a description that the system finds (1) useful to describe the referent and (2) true of the referent. 4 The Algorithm The Visible Objects Algorithm iterates through lists of visible attributes, stochastically adding properties to the property set it will generate. After this initial search, the algorithm then scans through the objects in the scene, roughly corresponding to how people scan a scene when referring (Pechmann, 1989). The target referent type, corresponding to the head noun in the final generated description, is added to the property set at the end of the algorithm. We rep</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>J. R. Searle. 1969. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Sonnenschein</author>
</authors>
<title>The development of referential communication skills: Some situations in which speakers give redundant messages.</title>
<date>1985</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>14--489</pages>
<contexts>
<context position="4422" citStr="Sonnenschein, 1985" startWordPosition="675" endWordPosition="676">1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), sel</context>
</contexts>
<marker>Sonnenschein, 1985</marker>
<rawString>Susan Sonnenschein. 1985. The development of referential communication skills: Some situations in which speakers give redundant messages. Journal of Psycholinguistic Research, 14:489–508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
<author>Ruud Koolen</author>
<author>Emiel Krahmer</author>
<author>Sander Wubben</author>
</authors>
<title>Does size matter – how much data is required to train a REG algorithm?</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="25571" citStr="Theune et al. (2011)" startWordPosition="4099" endWordPosition="4102">precedes size in the preference orders, in line with recent research showing that this allows the algorithm to perform optimally on the TUNA corpus (van Deemter et al., 2012a). In development, we find that IA performs best with type as the last attribute in the PO, and report on numbers with this approach. 5.2.2 The Graph-Based Algorithm The version of the Graph-Based Algorithm that we use is available from Viethen et al. (2008). This algorithm requires (1) a set of cost functions for each edge, and (2) a PO for deciding between properties in the case of a tie. For (1), we use the method from Theune et al. (2011) to assign two costs (0, 1) to the edges. We first determine the relative frequency with which each property is mentioned for a target object, and then create costs for each property using k-means clustering (k=2) in the Weka toolkit (Hall et al., 2009). We refer interested readers to the Theune et al. paper for further details. For (2), we follow the same method as for the Incremental Algorithm. 5.2.3 The Visual Objects Algorithm The proposed algorithm requires αatt, which we estimate as the relative frequency of each attribute att in the training data. The ordered attribute lists for the alg</context>
<context position="32295" citStr="Theune et al., 2011" startWordPosition="5236" endWordPosition="5239">of the algorithm, and for MAJORITY, we run the proposed algorithm 1,000 times for each test scene. Results are shown in Table 3. Again we see that the proposed Visible Objects Algorithm is competitive with the IA and GB for both ALIGNED DICE and MAJORITY. GB performs poorly here, and this may be due to the data sparsity issue that arises when requiring the algorithm to train on properties.8 In 7We do not report statistical significance; the proposed algorithm produces several possible outputs for one input, while the IA and GB produce only one. 8The original property-based weighting approach (Theune et al., 2011; Koolen et al., 2012, see Section 5.2) trained on object collections that were identical to their test data in all properties except x- and y-dimension, and so this was less of an issue. We hope to explore whether basing weights on attributes alone 1181 MAJORITY, the Visible Objects Algorithm is relatively stable across conditions, generating the majority property set in 40% of the test scenes. It does not outperform the IA in the -LOC condition, but the IA has a large range across the two conditions (0% and 100%). 7 Conclusions and Future Work We have introduced a new algorithm for generatin</context>
</contexts>
<marker>Theune, Koolen, Krahmer, Wubben, 2011</marker>
<rawString>Mari¨et Theune, Ruud Koolen, Emiel Krahmer, and Sander Wubben. 2011. Does size matter – how much data is required to train a REG algorithm? Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne M Treisman</author>
<author>Garry Gelade</author>
</authors>
<title>A feature integration theory of attention.</title>
<date>1980</date>
<pages>12--97</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="4920" citStr="Treisman and Gelade, 1980" startWordPosition="754" endWordPosition="757">alue, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects (Pechmann, 1989), selecting those properties that are salient for them (Horton and Keysar, 1996; Bard et al., 2009) without spending a great amount of cognitive effort considering the perception of a hearer (Keysar and Henly, 2002). We take this evidence to suggest an approach for a visual reference algorithm that generates natural, human-like reference by generating visual properties that are salient for a speaker.1 We can understand what is salient visually (what does the visual system first respond to, what gui</context>
</contexts>
<marker>Treisman, Gelade, 1980</marker>
<rawString>Anne M. Treisman and Garry Gelade. 1980. A feature integration theory of attention. Cognitive Psychology, 12:97–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Treisman</author>
</authors>
<title>Preattentive processing in vision.</title>
<date>1985</date>
<booktitle>Computer Vision, Graphics, and Image Processing,</booktitle>
<pages>31--156177</pages>
<contexts>
<context position="9032" citStr="Treisman, 1985" startWordPosition="1428" endWordPosition="1429">on where an object is likely to exist (its bounding box), as well as the x- and y-axis locations of the pixels within the object detection; but a value from these features like “big”, “tall”, or “long” requires further reasoning. As such, we incorporate the top-performing size algorithm introduced in Mitchell et al. (2011), which takes as input the height and widths of objects in the image and outputs a size value or NONE, indicating that size should not be used to describe the object. In addition to color and size, location and orientation begin to be processed early on in the visual system (Treisman, 1985; Itti and Koch, 2001), with our first perception of location corresponding to basic cues of where an object is relative to our focus of attention. For an input image, this simple type of location corresponds to surface forms such as, e.g., “on the right of the image” or “at the top of the image”. Along with size, location and orientation make up the three primary relative properties that we aim to generate language for. After the simple forms for color, size, location, and orientation properties are processed, our visual system feeds forward to two parallel pathways, the so-called “what” and </context>
</contexts>
<marker>Treisman, 1985</marker>
<rawString>Anne Treisman. 1985. Preattentive processing in vision. Computer Vision, Graphics, and Image Processing, 31:156177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L G Ungerleider</author>
<author>M Mishkin</author>
</authors>
<title>Two Cortical Visual Systems. In</title>
<date>1982</date>
<booktitle>Analysis of Visual Behaviour, chapter 18,</booktitle>
<pages>549--586</pages>
<editor>D. J. Ingle, M. Goodale, and R. J. W. Mansfield, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9680" citStr="Ungerleider and Mishkin, 1982" startWordPosition="1534" endWordPosition="1538">001), with our first perception of location corresponding to basic cues of where an object is relative to our focus of attention. For an input image, this simple type of location corresponds to surface forms such as, e.g., “on the right of the image” or “at the top of the image”. Along with size, location and orientation make up the three primary relative properties that we aim to generate language for. After the simple forms for color, size, location, and orientation properties are processed, our visual system feeds forward to two parallel pathways, the so-called “what” and “where” pathways (Ungerleider and Mishkin, 1982), which process properties with growing complexity. The “what” pathway includes absolute properties like shape and material, which computer vision has had some success detecting (Ferrari and Zisserman, 2007; Farhadi et al., 2009) while the “where” pathway corresponds to more complex spatial orientation and location information, such as where objects are relative to one another and which way they are facing. To begin connecting this process to the generation of human-like descriptions of visible objects, we start with the following simplification: Color and size have a privileged status, the fi</context>
</contexts>
<marker>Ungerleider, Mishkin, 1982</marker>
<rawString>L. G. Ungerleider and M. Mishkin. 1982. Two Cortical Visual Systems. In D. J. Ingle, M. Goodale, and R. J. W. Mansfield, editors, Analysis of Visual Behaviour, chapter 18, pages 549–586. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Ielka van der Sluis</author>
<author>Albert Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>Proceedings of the 4th International Conference on Natural Language Generation (INLG</booktitle>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>Kees van Deemter, Ielka van der Sluis, and Albert Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. Proceedings of the 4th International Conference on Natural Language Generation (INLG 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Albert Gatt</author>
<author>Ielka van der Sluis</author>
<author>Richard Power</author>
</authors>
<title>Generation of referring expressions: Assessing the incremental algorithm.</title>
<date>2012</date>
<journal>Cognitive Science,</journal>
<volume>36</volume>
<issue>5</issue>
<marker>van Deemter, Gatt, van der Sluis, Power, 2012</marker>
<rawString>Kees van Deemter, Albert Gatt, Ielka van der Sluis, and Richard Power. 2012a. Generation of referring expressions: Assessing the incremental algorithm. Cognitive Science, 36(5):799–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Emiel Krahmer</author>
<author>Roger van Gompel</author>
<author>Albert Gatt</author>
</authors>
<title>Towards a computational psycholinguistics of reference production. TopiCS: Production of Referring Expressions - Bridging the Gap between Computational and Empirical Approaches to Reference.</title>
<date>2012</date>
<marker>van Deemter, Krahmer, van Gompel, Gatt, 2012</marker>
<rawString>Kees van Deemter, Emiel Krahmer, Roger van Gompel, and Albert Gatt. 2012b. Towards a computational psycholinguistics of reference production. TopiCS: Production of Referring Expressions - Bridging the Gap between Computational and Empirical Approaches to Reference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger P G van Gompel</author>
<author>Albert Gatt</author>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>PRO: A computational model of referential overspecification. Architectures and Mechanisms for Language Processing</title>
<date>2012</date>
<marker>van Gompel, Gatt, Krahmer, van Deemter, 2012</marker>
<rawString>Roger P. G. van Gompel, Albert Gatt, Emiel Krahmer, and Kees van Deemter. 2012. PRO: A computational model of referential overspecification. Architectures and Mechanisms for Language Processing (AMLaP 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>The use of spatial relations in referring expression generation.</title>
<date>2008</date>
<booktitle>Proceedings of the 5th International Natural Language Generation Conference (INLG</booktitle>
<pages>59--67</pages>
<contexts>
<context position="23812" citStr="Viethen and Dale, 2008" startWordPosition="3803" endWordPosition="3806">colour:green size:(254,254) type:fan loc:(2,2) ori:left Figure 6: Example input scene: TUNA corpus. For IA And GB, gold-standard size values are provided rather than measurements (small, large). As such, although (color:grey, type:desk) would sufficiently distinguish the intended referent, we instead produce a variety of sets, overspecifying in some instances (e.g., (color:grey, ori:front, type:desk)), and with a small chance of underspecifying in others (e.g., (size:large, type:desk)). 5 Evaluation Algorithms &amp; Corpora 5.1 Corpora We evaluate on two well-known REG corpora, the GRE3D3 corpus (Viethen and Dale, 2008) and the singular furniture section of the TUNA corpus (van Deemter et al., 2006). Both corpora contain expressions elicited to computer-generated objects, and so provide a reasonable starting point for evaluating reference to visible objects. For all algorithms, we evaluate on the selection of referent attributes. Lexical choice and word order are not taken into account. Example images from GRE3D3 and TUNA are shown in Figure 4, and example algorithm input 1179 from these corpora are shown in Figures 5 and 6. In GRE3D3, we evaluate on the selection of type, color, size, and location, but leav</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008. The use of spatial relations in referring expression generation. Proceedings of the 5th International Natural Language Generation Conference (INLG 2008), pages 59–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>Speaker-dependent variation in content selection for referring expression generation.</title>
<date>2010</date>
<booktitle>Proceedings of the 8th Australasian Language Technology Workshop (ALTW 2010),</booktitle>
<pages>81--89</pages>
<contexts>
<context position="11400" citStr="Viethen and Dale, 2010" startWordPosition="1807" endWordPosition="1810">respond to general visual attributes and may generate forms for visual properties (attribute:value pairs). That is, a property such as color:red is generated from the attribute node color and a property such as size:tall is generated from the attribute node size. We are limited by existing REG corpora in which properties we can evaluate; in this paper, we examine the effect of the independent selection of color and size, followed by location and orientation.2 Generating human-like expressions in this setting begins to be possible by adopting recent proposals that REG handle speaker variation (Viethen and Dale, 2010) and the non-deterministic nature of reference (van Gompel et al., 2012; van Deemter et al., 2012b). We can capture such variation simply by estimating aatt, the likelihood that an attribute att generates a corresponding visual property. During generation, the algorithm passes through each attribute node, and uses this estimate to stochastically add each property to the output property set. Such a non-deterministic process means that the algorithm will not return the same output every time, which offers new challenges for evaluation. If we run the algorithm 1,000 times, we have a distribution </context>
</contexts>
<marker>Viethen, Dale, 2010</marker>
<rawString>Jette Viethen and Robert Dale. 2010. Speaker-dependent variation in content selection for referring expression generation. Proceedings of the 8th Australasian Language Technology Workshop (ALTW 2010), pages 81– 89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
<author>Pascal Touset</author>
</authors>
<title>Controlling redundancy in referring expressions.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="868" citStr="Viethen et al., 2008" startWordPosition="119" endWordPosition="123">ce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. We call such expressions identifying descriptions. The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the GraphBased Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account. 1 Introduction Referring expression generation (REG) is the task of generating an expression that can identify a referent to a listener. These expressions generally take the form of a definite noun phrase such as “the large orange plate” or “the furry running dog”. Research in REG primarily focuses on the subtask of selecting a set of properties that may be used to construct the final surface expression, </context>
<context position="12936" citStr="Viethen et al., 2008" startWordPosition="2070" endWordPosition="2073">for follow up work; for now, we focus on the properties common to REG corpora. 1176 run the algorithm for as many instances as we have in our test data, and see how well the property sets it produces aligns to the observed property sets. We discuss evaluation using both methods in Section 6. 3 The State of the Art in REG 3.1 Algorithms In order to understand how this approach compares to the state of the art in REG, we evaluate against two of the most well-known algorithms, the Incremental Algorithm (Dale and Reiter, 1995) and the Graph-Based Algorithm (Krahmer et. al, 2003, as implemented in Viethen et al., 2008). Details on these algorithms are available in their corresponding papers. As a brief summary, both algorithms formalize the objects in the discourse as a set of properties (attribute:�alue pairs). For example, one object may be represented as (type:box, color:red, size:large). The task is to find the set of properties that uniquely specify the referent. This is known as a content selection problem, and the set of properties chosen by the algorithm is called a distinguishing description. The Incremental Algorithm (IA) proceeds by iterating through attributes in a predefined order (a preference</context>
<context position="14488" citStr="Viethen et al. (2008)" startWordPosition="2318" endWordPosition="2321">operties have been checked. We use the implementation of the IA available from the NLTK (Bird et al., 2009).3 In the Graph-Based Algorithm (GB), the objects in the discourse are represented within a labeled directed graph, and content selection is a subgraph construction problem. Each object is represented as a vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else</context>
<context position="25383" citStr="Viethen et al. (2008)" startWordPosition="4062" endWordPosition="4065"> determine the preference order from corpus frequencies using cross-validation to hold out a test scene and list attributes from the training scenes in descending order. We find that color precedes size in the preference orders, in line with recent research showing that this allows the algorithm to perform optimally on the TUNA corpus (van Deemter et al., 2012a). In development, we find that IA performs best with type as the last attribute in the PO, and report on numbers with this approach. 5.2.2 The Graph-Based Algorithm The version of the Graph-Based Algorithm that we use is available from Viethen et al. (2008). This algorithm requires (1) a set of cost functions for each edge, and (2) a PO for deciding between properties in the case of a tie. For (1), we use the method from Theune et al. (2011) to assign two costs (0, 1) to the edges. We first determine the relative frequency with which each property is mentioned for a target object, and then create costs for each property using k-means clustering (k=2) in the Weka toolkit (Hall et al., 2009). We refer interested readers to the Theune et al. paper for further details. For (2), we follow the same method as for the Incremental Algorithm. 5.2.3 The Vi</context>
</contexts>
<marker>Viethen, Dale, Krahmer, Theune, Touset, 2008</marker>
<rawString>Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et Theune, and Pascal Touset. 2008. Controlling redundancy in referring expressions. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Martijn Goudbeek</author>
<author>Emiel Krahmer</author>
</authors>
<title>The impact of colour difference and coloure codability on reference production.</title>
<date>2012</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci</booktitle>
<contexts>
<context position="7324" citStr="Viethen et al., 2012" startWordPosition="1142" endWordPosition="1145"> and egocentrism in language production (Clark and Murphy, 1982; Horton and Keysar, 1996; Bard et al., 2009; Gann and Barr, 2013). Underand overspecification naturally fall out from such an approach, with no need to specifically model either phenomenon. Perhaps unsurprisingly, the set of properties that are visually salient and the set of properties that are linguistically salient largely overlap. Color is the first property our visual system processes, followed soon after by size (Murray et al., 2006; Fang et al., 2008; Schwarzkopf et al., 2010); and people tend to use color (Pechmann, 1989; Viethen et al., 2012) and size when identifying objects, with size common when there is another object of the same type in the scene (Brown-Schmidt and Tanenhaus, 2006). Following this, our algorithm gives a privileged position to these properties, processing them first. Using computer vision techniques to determine an object’s color works reasonably well (Berg et al., 2011), and the relevant visual features for this task may be useful in future work to return several possible color labels that capture differences in lexical choice (cf. Reiter and Sripada (2002)). Detecting size does not work well (Forsyth, 1175 2</context>
</contexts>
<marker>Viethen, Goudbeek, Krahmer, 2012</marker>
<rawString>Jette Viethen, Martijn Goudbeek, and Emiel Krahmer. 2012. The impact of colour difference and coloure codability on reference production. Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Whitehurst</author>
</authors>
<title>The development of communication: Changes with age and modeling. Child Development,</title>
<date>1976</date>
<pages>47--473</pages>
<contexts>
<context position="4401" citStr="Whitehurst, 1976" startWordPosition="673" endWordPosition="674"> Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on adult speakers in visual domains (Pechmann, 1989; Engelhardt et al., 2006; Koolen et al., 2011). The related phenomenon of underspecification, where speakers select a set of properties that do not linguistically specify the referent, has also received some attention, particularly in visual domains (Clark et al., 1983; Kelleher et al., 2005). These findings make sense in light of visual evidence that some properties “pop out” in the scene (Treisman and Gelade, 1980), and speakers may begin referring before scanning the full set of scene objects </context>
</contexts>
<marker>Whitehurst, 1976</marker>
<rawString>G. J. Whitehurst. 1976. The development of communication: Changes with age and modeling. Child Development, 47:473–482.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>