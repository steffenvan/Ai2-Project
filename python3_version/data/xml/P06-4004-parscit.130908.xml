<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011231">
<title confidence="0.952312">
Valido: a Visual Tool for Validating Sense Annotations
</title>
<author confidence="0.918526">
Roberto Navigli
</author>
<affiliation confidence="0.565919">
Dipartimento di Informatica
Universit`a di Roma “La Sapienza”
Roma, Italy
</affiliation>
<email confidence="0.992451">
navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.993731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994516">
In this paper we present Valido, a tool
that supports the difficult task of validating
sense choices produced by a set of annota-
tors. The validator can analyse the seman-
tic graphs resulting from each sense choice
and decide which sense is more coherent
with respect to the structure of the adopted
lexicon. We describe the interface and re-
port an evaluation of the tool in the valida-
tion of manual sense annotations.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977189189189">
The task of sense annotation consists in the assign-
ment of the appropriate senses to words in context.
For each word, the senses are chosen with respect
to a sense inventory encoded by a reference dic-
tionary. The free availability and, as a result, the
massive adoption of WordNet (Fellbaum, 1998)
largely contributed to its status of de facto standard
in the NLP community. Unfortunately, WordNet
is a fine-grained resource, which encodes possibly
subtle sense distictions.
Several studies report an inter-annotator agree-
ment around 70% when using WordNet as a refer-
ence sense inventory. For instance, the agreement
in the Open Mind Word Expert project (Chklovski
and Mihalcea, 2002) was 67.3%. Such a low
agreement is only in part due to the inexperience
of sense annotators (e.g. volunteers on the web).
Rather, to a large part it is due to the difficulty in
making clear which are the real distinctions be-
tween close word senses in the WordNet inventory.
Adjudicating sense choices, i.e. the task of vali-
dating word senses, is therefore critical in building
a high-quality data set. The validation task can be
defined as follows: let w be a word in a sentence
u, previously annotated by a set of annotators
A = {a1, a2, ..., anJ each providing a sense for
w, and let SA = {s1, s2, ..., smJ C_ Senses(w)
be the set of senses chosen for w by the annotators
in A, where Senses(w) is the set of senses of w
in the reference inventory (e.g. WordNet). A val-
idator is asked to validate, that is to adjudicate a
sense s E Senses(w) for a word w over the oth-
ers. Notice that s is a word sense for w in the sense
inventory, but is not necessarily in SA, although it
is likely to be. Also note that the annotators in A
can be either human or automatic, depending upon
the purpose of the exercise.
</bodyText>
<sectionHeader confidence="0.983744" genericHeader="method">
2 Semantic Interconnections
</sectionHeader>
<bodyText confidence="0.999708857142857">
Semantic graphs are a notation developed to rep-
resent knowledge explicitly as a set of conceptual
entities and their interrelationships. Fields like the
analysis of the lexical text cohesion (Morris and
Hirst, 1991), word sense disambiguation (Agirre
and Rigau, 1996; Mihalcea and Moldovan, 2001),
ontology learning (Navigli and Velardi, 2005), etc.
have certainly benefited from the availability of
wide-coverage computational lexicons like Word-
Net (Fellbaum, 1998), as well as semantically an-
notated corpora like SemCor (Miller et al., 1993).
Recently, a knowledge-based algorithm for
Word Sense Disambiguation, called Structural Se-
mantic Interconnections1 (SSI) (Navigli and Ve-
lardi, 2004), has been shown to provide interest-
ing insights into the choice of word senses by pro-
viding structural justifications in terms of semantic
graphs.
SSI exploits an extensive lexical knowledge
base, built upon the WordNet lexicon and enriched
with collocation information representing seman-
</bodyText>
<footnote confidence="0.999113">
1SSI is available online at http://lcl.di.uniroma1.it/ssi.
</footnote>
<page confidence="0.994056">
13
</page>
<bodyText confidence="0.940889727272727">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13–16,
Sydney, July 2006. c�2006 Association for Computational Linguistics
tic relatedness between sense pairs. Collocations
are acquired from existing resources (like the Ox-
ford Collocations, the Longman Language Acti-
vator, collocation web sites, etc.). Each colloca-
tion is mapped to the WordNet sense inventory in
a semi-automatic manner and transformed into a
relatedness edge (Navigli and Velardi, 2005).
Given a word context C = {w1, ..., wk}, SSI
builds a graph G = (V, E) such that V =
</bodyText>
<equation confidence="0.996295666666667">
k
U SensesWN(wi) and (s, s&apos;) E E if there is at
i=1
</equation>
<bodyText confidence="0.999813714285714">
least one semantic interconnection between s and
s&apos; in the lexical knowledge base. A semantic inter-
connection pattern is a relevant sequence of edges
selected according to a manually-created context-
free grammar, i.e. a path connecting a pair of word
senses, possibly including a number of interme-
diate concepts. The grammar consists of a small
number of rules, inspired by the notion of lexi-
cal chains (Morris and Hirst, 1991). An excerpt
of the context-free grammar encoding semantic in-
terconnection patterns for the WordNet lexicon is
reported in Table 1. For the full set of interconnec-
tions the reader can refer to Navigli and Velardi
(2004).
SSI performs disambiguation in an iterative
fashion, by maintaining a set C of senses as a se-
mantic context. Initially, C = V (the entire set
of senses of words in C). At each step, for each
sense s in C, the algorithm calculates a score of
the degree of connectivity between s and the other
senses in C:
</bodyText>
<equation confidence="0.991445">
E E 1
s&apos;EC\{s1 iEIC(s,s&apos;) length(i)
E |IC(s,s&apos;)|
s&apos;EC\{s1
</equation>
<bodyText confidence="0.999963454545454">
where IC(s, s&apos;) is the set of interconnections be-
tween senses s and s&apos;. The contribution of a sin-
gle interconnection is given by the reciprocal of its
length, calculated as the number of edges connect-
ing its ends. The overall degree of connectivity
is then normalized by the number of contributing
interconnections. The highest ranking sense s of
word w is chosen and the senses of w are removed
from the semantic context C. The algorithm termi-
nates when either C = ∅ or there is no sense such
that its score exceeds a fixed threshold.
</bodyText>
<sectionHeader confidence="0.975346" genericHeader="method">
3 The Tool: Valido
</sectionHeader>
<footnote confidence="0.676160666666667">
Based on SSI, we developed a visual tool, Valido2,
to visually support the validator in the difficult task
2Valido is available at http://lcl.di.uniroma1.it/valido.
</footnote>
<equation confidence="0.9897453">
S → S&apos;S1|S&apos;S2|S&apos;S3
(start rule)
S&apos; → enominalization|epertainymy|E
(part-of-speech jump)
S1 → ekind−of S1|epart−of S1|ekind−of |epart−of
(hyperonymy/meronymy)
S2 → ekind−of S2|erelatednessS2|ekind−of |erelatedness
(hypernymy/relatedness)
S3 → esimilarityS3|eantonymyS3|esimilarity|eantonymy
(adjectives)
</equation>
<tableCaption confidence="0.7616035">
Table 1: An excerpt of the context-free grammar
for the recognition of semantic interconnections.
</tableCaption>
<bodyText confidence="0.980070608695652">
of assessing the quality and suitability of sense an-
notations. The tool takes as input a corpus of doc-
uments whose sentences were previously tagged
by one or more annotators with word senses from
the WordNet inventory. The corpus can be input
in xml format, as specified in the initial page.
The user can browse the sentences, and adjudi-
cate a choice over the others in case of disagree-
ment among the annotators. To the end of assist-
ing the user in the validation task, the tool high-
lights each word in a sentence with different col-
ors, namely: green for words having a full agree-
ment, red for words where no agreement can be
found, orange for those words on which a valida-
tion policy can be applied.
A validation policy is a strategy for suggesting a
default sense choice to the validator in case of dis-
agreement. Initially, the validator can choose one
of four validation policies to be applied to those
words with disagreement on which sense to as-
sign:
(α) majority voting: if there exists a sense s E
SA (the set of senses chosen by the annotators
</bodyText>
<equation confidence="0.98891025">
in A) such that |{aEA  |a annotated w with s} |≥
|A|
2, s is proposed as the preferred sense for w;
1
</equation>
<bodyText confidence="0.97894">
(β) majority voting + SSI: the same as the pre-
vious policy, with the addition that if there
exists no sense chosen by a majority of an-
notators, SSI is applied to w, and the sense
chosen by the algorithm, if any, is proposed
to the validator;
</bodyText>
<listItem confidence="0.7561495">
(y) SSI: the SSI algorithm is applied to w, and
the chosen sense, if any, is proposed to the
validator;
(δ) no validation: w is left untagged.
</listItem>
<bodyText confidence="0.89418">
Notice that for policies (β) and (y) Valido ap-
plies the SSI algorithm to w in the context of its
</bodyText>
<equation confidence="0.945691">
ScoreSSI(s,C) =
</equation>
<page confidence="0.985447">
14
</page>
<bodyText confidence="0.999929176470588">
sentence σ by taking into account for disambigua-
tion only the senses in s (i.e. the set of senses cho-
sen by the annotators). In general, given a set of
words with disagreement W ⊆ σ, SSI is applied
to W using as a fixed context the agreed senses
chosen for the words in σ \ W.
Also note that the suggestion of a sense choice,
marked in orange based on the validation policy,
is just a proposal and can freely modified by the
validator, as explained hereafter.
Before starting the interface, the validator can
also choose whether to add a virtual annotator
aSSt to the set of annotators A. This virtual an-
notator tags each word w E σ with the sense
chosen by the application of the SSI algorithm
to σ. As a result, the selected validation pol-
icy will be applied to the new set of annotators
A&apos; = A U {aSSt}. This is useful especially when
|A |= 1 (e.g. in the automatic application of a
single word sense disambiguation system), that is
when validation policies are of no use.
Figure 1 illustrates the interface of the tool:
in the top pane the sentence at hand is shown,
marked with colors as explained above. The
main pane shows the semantic interconnections
between senses for which either there is a full
agreement or the chosen validation policy can be
applied. When the user clicks on a word w, the
left pane reports the sense inventory for w, in-
cluding information about the hypernym, defini-
tion and usage for each sense of w. The validator
can then click on a sense and see how the seman-
tic graph shown in the main pane changes after the
selection, possibly resulting in a different number
and strength of semantic interconnection patterns
supporting that sense choice. For each sense in the
left pane, the annotators in A who favoured that
choice are listed (for instance, in the figure anno-
tator #1 chose sense #1 of street, while annotator
#2 as well as SSI chose sense #2).
If the validator decides that a certain word sense
is more convincing based on its semantic graph,
(s)he can select that sense as a final choice by
clicking on the validate button on top of the left
pane. In case the validator wants to validate
present sense choices of all the disagreed words,
(s)he can press the validate all button in the top
pane. As a result, the present selection of senses
will be chosen as the final configuration for the en-
tire sentence at hand.
In the top pane, an icon beside each disagreed
</bodyText>
<table confidence="0.9998828">
Precision Recall
Nouns 75.80% (329/434) 63.75% (329/516)
Adjectives 74.19% (46/62) 22.33% (46/206)
Verbs 65.64% (107/163) 43.14% (107/248)
Total 73.14% (482/659) 49.69% (482/970)
</table>
<tableCaption confidence="0.999686">
Table 2: Results on 1,000 sentences from SemCor.
</tableCaption>
<bodyText confidence="0.911831">
word shows the validation status of the word: a
question mark indicates that the disagreement has
not yet been solved, while a checkmark indicates
that the validator solved the disagremeent.
</bodyText>
<sectionHeader confidence="0.996107" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999987166666667">
We briefly report here an experiment on the vali-
dation of manual sense annotations with the aid of
Valido. For more detailed experiments the reader
can refer to Navigli (2006).
1,000 sentences were uniformly selected from
the set of documents in the semantically-tagged
SemCor corpus (Miller et al., 1993). For each sen-
tence σ = w1w2 ... wk annotated in SemCor with
the senses sw1sw2 ... swk (swi E Senses(wi), i E
{1, 2, ... , k}), we randomly identified a word
wi E σ, and chose at random a different sense swi
for that word, that is swi E Senses(wi) \ {swi}.
In other words, we simulated in vitro a situation in
which an annotator provides an appropriate sense
and the other selects a different sense.
We applied Valido with policy (γ) to the anno-
tated sentences and evaluated the performance of
the approach in suggesting the appropriate choice
for the words with disagreement. The results are
reported in Table 2 for nouns, adjectives, and verbs
(we neglected adverbs as very few interconnec-
tions can be found for them).
The experiment shows that evidences of incon-
sistency due to inappropriate annotations are pro-
vided with good precision. The overall F1 mea-
sure is 59.18%. The chance baseline is 50%.
The low recall obtained for verbs, but especially
for adjectives, is due to a lack of connectivity in
the lexical knowledge base, when dealing with
connections across different parts of speech.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.998393428571429">
In this paper we presented Valido, a tool for the
validation of manual and automatic sense anno-
tations. Valido allows a validator to analyse the
coherency of different sense annotations provided
for the same word in terms of the respective se-
mantic interconnections with the other senses in
context. We reported an experiment showing that
</bodyText>
<page confidence="0.991201">
15
</page>
<figureCaption confidence="0.999922">
Figure 1: A screenshot of the tool.
</figureCaption>
<bodyText confidence="0.999981222222222">
the approach provides useful hints. Notice that
this experiment concerns the quality of the sugges-
tions, which are not necessarily taken into account
by the validator (implying a higher degree of ac-
curacy in the overall validation process).
We foresee an extension of the tool for sup-
porting the sense annotation phase. The tool can
indeed provide richer information than interfaces
like the Open Mind Word Expert (Chklovski and
Mihalcea, 2002), and the annotator can take ad-
vantage of the resulting graphs to improve aware-
ness in the decisions to be taken, so as to make
consistent choices with respect to the reference
lexicon.
Finally, we would like to propose the use of the
tool in the preparation of at least one of the test
sets for the next Senseval exercise, to be held sup-
posedly next year.
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9940115">
This work is partially funded by the Interop NoE
(508011), 6th European Union FP.
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999080485714286">
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In Proc.
of COLING 1996. Copenhagen, Denmark.
Tim Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with open mind word expert.
In Proc. of ACL 2002 Workshop on WSD: Recent
Successes and Future Directions. Philadelphia, PA.
Christiane Fellbaum, editor. 1998. WordNet: an Elec-
tronic Lexical Database. MIT Press.
Rada Mihalcea and Dan Moldovan. 2001. Automatic
generation of a coarse grained wordnet. In Proc.
ofNAACL Workshop on WordNet and Other Lexical
Resources. Pittsburgh, PA.
George Miller, Claudia Leacock, Tengi Randee, and
Ross Bunker. 1993. A semantic concordance. In
Proc. 3rd DARPA Workshop on Human Language
Technology. Plainsboro, New Jersey.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1).
Roberto Navigli and Paola Velardi. 2004. Learn-
ing domain ontologies from document warehouses
and dedicated websites. Computational Linguistics,
30(2).
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence (PAMI), 27(7).
Roberto Navigli. 2006. Experiments on the validation
of sense annotations assisted by lexical chains. In
Proc. of the European Chapter of the Annual Meet-
ing of the Association for Computational Linguistics
(EACL). Trento, Italy.
</reference>
<page confidence="0.998701">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959228">
<title confidence="0.999855">Valido: a Visual Tool for Validating Sense Annotations</title>
<author confidence="0.99997">Roberto Navigli</author>
<affiliation confidence="0.992977">Dipartimento di Informatica Universit`a di Roma “La Sapienza”</affiliation>
<address confidence="0.989736">Roma, Italy</address>
<email confidence="0.990799">navigli@di.uniroma1.it</email>
<abstract confidence="0.999241363636364">In this paper we present Valido, a tool that supports the difficult task of validating sense choices produced by a set of annotators. The validator can analyse the semantic graphs resulting from each sense choice and decide which sense is more coherent with respect to the structure of the adopted lexicon. We describe the interface and report an evaluation of the tool in the validation of manual sense annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density.</title>
<date>1996</date>
<booktitle>In Proc. of COLING</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2693" citStr="Agirre and Rigau, 1996" startWordPosition="458" endWordPosition="461">is asked to validate, that is to adjudicate a sense s E Senses(w) for a word w over the others. Notice that s is a word sense for w in the sense inventory, but is not necessarily in SA, although it is likely to be. Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise. 2 Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive </context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proc. of COLING 1996. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proc. of ACL 2002 Workshop on WSD: Recent Successes and Future Directions.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1293" citStr="Chklovski and Mihalcea, 2002" startWordPosition="202" endWordPosition="205"> of the appropriate senses to words in context. For each word, the senses are chosen with respect to a sense inventory encoded by a reference dictionary. The free availability and, as a result, the massive adoption of WordNet (Fellbaum, 1998) largely contributed to its status of de facto standard in the NLP community. Unfortunately, WordNet is a fine-grained resource, which encodes possibly subtle sense distictions. Several studies report an inter-annotator agreement around 70% when using WordNet as a reference sense inventory. For instance, the agreement in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002) was 67.3%. Such a low agreement is only in part due to the inexperience of sense annotators (e.g. volunteers on the web). Rather, to a large part it is due to the difficulty in making clear which are the real distinctions between close word senses in the WordNet inventory. Adjudicating sense choices, i.e. the task of validating word senses, is therefore critical in building a high-quality data set. The validation task can be defined as follows: let w be a word in a sentence u, previously annotated by a set of annotators A = {a1, a2, ..., anJ each providing a sense for w, and let SA = {s1, s2,</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Tim Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with open mind word expert. In Proc. of ACL 2002 Workshop on WSD: Recent Successes and Future Directions. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic generation of a coarse grained wordnet.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL Workshop on WordNet and Other Lexical Resources.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2723" citStr="Mihalcea and Moldovan, 2001" startWordPosition="462" endWordPosition="465">at is to adjudicate a sense s E Senses(w) for a word w over the others. Notice that s is a word sense for w in the sense inventory, but is not necessarily in SA, although it is likely to be. Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise. 2 Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built </context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 2001. Automatic generation of a coarse grained wordnet. In Proc. ofNAACL Workshop on WordNet and Other Lexical Resources. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Claudia Leacock</author>
<author>Tengi Randee</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. 3rd DARPA Workshop on Human Language Technology.</booktitle>
<publisher>Plainsboro,</publisher>
<location>New Jersey.</location>
<contexts>
<context position="2970" citStr="Miller et al., 1993" startWordPosition="497" endWordPosition="500">tic, depending upon the purpose of the exercise. 2 Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing seman1SSI is available online at http://lcl.di.uniroma1.it/ssi. 13 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13–16, Sydney, July 2006</context>
<context position="11018" citStr="Miller et al., 1993" startWordPosition="1869" endWordPosition="1872">163) 43.14% (107/248) Total 73.14% (482/659) 49.69% (482/970) Table 2: Results on 1,000 sentences from SemCor. word shows the validation status of the word: a question mark indicates that the disagreement has not yet been solved, while a checkmark indicates that the validator solved the disagremeent. 4 Evaluation We briefly report here an experiment on the validation of manual sense annotations with the aid of Valido. For more detailed experiments the reader can refer to Navigli (2006). 1,000 sentences were uniformly selected from the set of documents in the semantically-tagged SemCor corpus (Miller et al., 1993). For each sentence σ = w1w2 ... wk annotated in SemCor with the senses sw1sw2 ... swk (swi E Senses(wi), i E {1, 2, ... , k}), we randomly identified a word wi E σ, and chose at random a different sense swi for that word, that is swi E Senses(wi) \ {swi}. In other words, we simulated in vitro a situation in which an annotator provides an appropriate sense and the other selects a different sense. We applied Valido with policy (γ) to the annotated sentences and evaluated the performance of the approach in suggesting the appropriate choice for the words with disagreement. The results are reporte</context>
</contexts>
<marker>Miller, Leacock, Randee, Bunker, 1993</marker>
<rawString>George Miller, Claudia Leacock, Tengi Randee, and Ross Bunker. 1993. A semantic concordance. In Proc. 3rd DARPA Workshop on Human Language Technology. Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2642" citStr="Morris and Hirst, 1991" startWordPosition="451" endWordPosition="454">the reference inventory (e.g. WordNet). A validator is asked to validate, that is to adjudicate a sense s E Senses(w) for a word w over the others. Notice that s is a word sense for w in the sense inventory, but is not necessarily in SA, although it is likely to be. Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise. 2 Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in t</context>
<context position="4517" citStr="Morris and Hirst, 1991" startWordPosition="737" endWordPosition="740">sformed into a relatedness edge (Navigli and Velardi, 2005). Given a word context C = {w1, ..., wk}, SSI builds a graph G = (V, E) such that V = k U SensesWN(wi) and (s, s&apos;) E E if there is at i=1 least one semantic interconnection between s and s&apos; in the lexical knowledge base. A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created contextfree grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts. The grammar consists of a small number of rules, inspired by the notion of lexical chains (Morris and Hirst, 1991). An excerpt of the context-free grammar encoding semantic interconnection patterns for the WordNet lexicon is reported in Table 1. For the full set of interconnections the reader can refer to Navigli and Velardi (2004). SSI performs disambiguation in an iterative fashion, by maintaining a set C of senses as a semantic context. Initially, C = V (the entire set of senses of words in C). At each step, for each sense s in C, the algorithm calculates a score of the degree of connectivity between s and the other senses in C: E E 1 s&apos;EC\{s1 iEIC(s,s&apos;) length(i) E |IC(s,s&apos;)| s&apos;EC\{s1 where IC(s, s&apos;) </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning domain ontologies from document warehouses and dedicated websites.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="3119" citStr="Navigli and Velardi, 2004" startWordPosition="515" endWordPosition="519">plicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing seman1SSI is available online at http://lcl.di.uniroma1.it/ssi. 13 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 13–16, Sydney, July 2006. c�2006 Association for Computational Linguistics tic relatedness between sense pairs. Collocations are acquired from existing resources (like the O</context>
<context position="4736" citStr="Navigli and Velardi (2004)" startWordPosition="773" endWordPosition="776">interconnection between s and s&apos; in the lexical knowledge base. A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created contextfree grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts. The grammar consists of a small number of rules, inspired by the notion of lexical chains (Morris and Hirst, 1991). An excerpt of the context-free grammar encoding semantic interconnection patterns for the WordNet lexicon is reported in Table 1. For the full set of interconnections the reader can refer to Navigli and Velardi (2004). SSI performs disambiguation in an iterative fashion, by maintaining a set C of senses as a semantic context. Initially, C = V (the entire set of senses of words in C). At each step, for each sense s in C, the algorithm calculates a score of the degree of connectivity between s and the other senses in C: E E 1 s&apos;EC\{s1 iEIC(s,s&apos;) length(i) E |IC(s,s&apos;)| s&apos;EC\{s1 where IC(s, s&apos;) is the set of interconnections between senses s and s&apos;. The contribution of a single interconnection is given by the reciprocal of its length, calculated as the number of edges connecting its ends. The overall degree of</context>
</contexts>
<marker>Navigli, Velardi, 2004</marker>
<rawString>Roberto Navigli and Paola Velardi. 2004. Learning domain ontologies from document warehouses and dedicated websites. Computational Linguistics, 30(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="2770" citStr="Navigli and Velardi, 2005" startWordPosition="468" endWordPosition="471">ord w over the others. Notice that s is a word sense for w in the sense inventory, but is not necessarily in SA, although it is likely to be. Also note that the annotators in A can be either human or automatic, depending upon the purpose of the exercise. 2 Semantic Interconnections Semantic graphs are a notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships. Fields like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning (Navigli and Velardi, 2005), etc. have certainly benefited from the availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993). Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections1 (SSI) (Navigli and Velardi, 2004), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of semantic graphs. SSI exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with coll</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Experiments on the validation of sense annotations assisted by lexical chains.</title>
<date>2006</date>
<booktitle>In Proc. of the European Chapter of the Annual Meeting of the Association for Computational Linguistics (EACL).</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="10888" citStr="Navigli (2006)" startWordPosition="1852" endWordPosition="1853">agreed Precision Recall Nouns 75.80% (329/434) 63.75% (329/516) Adjectives 74.19% (46/62) 22.33% (46/206) Verbs 65.64% (107/163) 43.14% (107/248) Total 73.14% (482/659) 49.69% (482/970) Table 2: Results on 1,000 sentences from SemCor. word shows the validation status of the word: a question mark indicates that the disagreement has not yet been solved, while a checkmark indicates that the validator solved the disagremeent. 4 Evaluation We briefly report here an experiment on the validation of manual sense annotations with the aid of Valido. For more detailed experiments the reader can refer to Navigli (2006). 1,000 sentences were uniformly selected from the set of documents in the semantically-tagged SemCor corpus (Miller et al., 1993). For each sentence σ = w1w2 ... wk annotated in SemCor with the senses sw1sw2 ... swk (swi E Senses(wi), i E {1, 2, ... , k}), we randomly identified a word wi E σ, and chose at random a different sense swi for that word, that is swi E Senses(wi) \ {swi}. In other words, we simulated in vitro a situation in which an annotator provides an appropriate sense and the other selects a different sense. We applied Valido with policy (γ) to the annotated sentences and evalu</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Experiments on the validation of sense annotations assisted by lexical chains. In Proc. of the European Chapter of the Annual Meeting of the Association for Computational Linguistics (EACL). Trento, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>