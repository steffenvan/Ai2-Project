<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.984784">
Evolutionary Hierarchical Dirichlet Process for Timeline Summarization
</title>
<author confidence="0.998856">
Jiwei Li
</author>
<affiliation confidence="0.9962665">
School of Computer Science
Cornell University
</affiliation>
<address confidence="0.761991">
Ithaca, NY, 14853
</address>
<email confidence="0.995657">
jl3226@cornell.edu
</email>
<author confidence="0.995172">
Sujian Li
</author>
<affiliation confidence="0.9971615">
Laboratory of Computational Linguistics
Peking University
</affiliation>
<address confidence="0.824926">
Bejing, P.R.China, 150001
</address>
<email confidence="0.990665">
lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.997359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999703">
Timeline summarization aims at generat-
ing concise summaries and giving read-
ers a faster and better access to under-
stand the evolution of news. It is a new
challenge which combines salience rank-
ing problem with novelty detection. Pre-
vious researches in this field seldom ex-
plore the evolutionary pattern of topics
such as birth, splitting, merging, develop-
ing and death. In this paper, we develop
a novel model called Evolutionary Hier-
archical Dirichlet Process(EHDP) to cap-
ture the topic evolution pattern in time-
line summarization. In EHDP, time vary-
ing information is formulated as a series
of HDPs by considering time-dependent
information. Experiments on 6 different
datasets which contain 3156 documents
demonstrates the good performance of our
system with regard to ROUGE scores.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843389830508">
Faced with thousands of news articles, people usu-
ally try to ask the general aspects such as the
beginning, the evolutionary pattern and the end.
General search engines simply return the top rank-
ing articles according to query relevance and fail
to trace how a specific event goes. Timeline sum-
marization, which aims at generating a series of
concise summaries for news collection published
at different epochs can give readers a faster and
better access to understand the evolution of news.
The key of timeline summarization is how to
select sentences which can tell readers the evolu-
tionary pattern of topics in the event. It is very
common that the themes of a corpus evolve over
time, and topics of adjacent epochs usually exhibit
strong correlations. Thus, it is important to model
topics across different documents and over differ-
ent time periods to detect how the events evolve.
The task of timelime summarization is firstly
proposed by Allan et al.(2001) by extracting clus-
ters of noun phases and name entities. Chieu et
al.(2004) built a similar system in unit of sentences
with interest and burstiness. However, these meth-
ods seldom explored the evolutionary character-
istics of news. Recently, Yan et al.(2011) ex-
tended the graph based sentence ranking algorithm
used in traditional multi-document summarization
(MDS) to timeline generation by projecting sen-
tences from different time into one plane. They
further explored the timeline task from the opti-
mization of a function considering the combina-
tion of different respects such as relevance, cover-
age, coherence and diversity (Yan et al., 2011b).
However, their approaches just treat timeline gen-
eration as a sentence ranking or optimization prob-
lem and seldom explore the topic information lied
in the corpus.
Recently, topic models have been widely used
for capturing the dynamics of topics via time.
Many dynamic approaches based on LDA model
(Blei et al., 2003) or Hierarchical Dirichelt Pro-
cesses(HDP) (Teh et al., 2006) have been pro-
posed to discover the evolving patterns in the cor-
pus as well as the snapshot clusters at each time
epoch (Blei and Lafferty, 2006; Chakrabarti et al.,
2006; Wang and McCallum, 2007; Caron et al.,
2007; Ren et al., 2008; Ahmed and Xing, 2008;
Zhang et al., 2010).
In this paper, we propose EHDP: a evolution-
ary hierarchical Dirichlet process (HDP) model
for timeline summarization. In EHDP, each HDP
is built for multiple corpora at each time epoch,
and the time dependencies are incorporated into
epochs under the Markovian assumptions. Topic
popularity and topic-word distribution can be in-
ferred from a Chinese Restaurant Process (CRP).
Sentences are selected into timelines by consider-
ing different aspects such as topic relevance, cov-
erage and coherence. We built the evaluation sys-
</bodyText>
<page confidence="0.977664">
556
</page>
<bodyText confidence="0.7677395">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556–560,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tems which contain 6 real datasets and perfor-
mance of different models is evaluated accord-
ing to the ROUGE metrics. Experimental results
demonstrate the effectiveness of our model .
</bodyText>
<sectionHeader confidence="0.995907" genericHeader="method">
2 EHDP for Timeline Summarization
</sectionHeader>
<subsectionHeader confidence="0.994461">
2.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.622536444444444">
Given a general query Q = {wqi}i=Qn i=1 , we firstly
obtain a set of query related documents. We no-
tate different corpus as C = {Ct}t=T
t=1 according
to their published time where Ct = {Dti}i=Nt
i=1 de-
notes the document collection published at epoch
t. Document Dti is formulated as a collection of
sentences {st ij}j=Nti
</bodyText>
<equation confidence="0.974613">
j=1 . Each sentence is presented with a series of words st ij = {wt ijl}l=Nt
l=1 and as-
ij
</equation>
<bodyText confidence="0.817723">
sociated with a topic θtij. V denotes the vocabu-
lary size. The output of the algorithm is a series
of timelines summarization I = {It}t=T
</bodyText>
<equation confidence="0.7879855">
t=1 where
It ⊂ Ct
</equation>
<subsectionHeader confidence="0.970003">
2.2 EHDP
</subsectionHeader>
<bodyText confidence="0.999855692307692">
Our EHDP model is illustrated in Figure 2. Specif-
ically, each corpus Ct is modeled as a HDP. These
HDP shares an identical base measure G0, which
serves as an overall bookkeeping of overall mea-
sures. We use Gt0 to denote the base measure at
each epoch and draw the local measure Gti for each
document at time t from Gt0. In EHDP, each sen-
tence is assigned to an aspect θtij with the consid-
eration of words within current sentence.
To consider time dependency information in
EHDP, we link all time specific base measures Gt0
with a temporal Dirichlet mixture model as fol-
lows:
</bodyText>
<equation confidence="0.991630666666667">
Δ
Gt0 ∼ DP(γt, 1 G0+ 1 K
δ=0
</equation>
<bodyText confidence="0.980898384615385">
where F(v, δ) = exp(−δ/v) denotes the expo-
nential kernel function that controls the influence
of neighboring corpus. K denotes the normaliza-
tion factor where K = 1 + PΔδ=0 F(v, δ). Δ is
the time width and λ is the decay factor. In Chi-
nese Restaurant Process (CRP), each document is
referred to a restaurant and sentences are com-
pared to customers. Customers in the restaurant
sit around different tables and each table btin is as-
sociated with a dish (topic) Ψtin according to the
dish menu. Let mtk denote the number of ta-
bles enjoying dish k in all restaurants at epoch t,
mtk = PNt PNt ib
</bodyText>
<equation confidence="0.993651">
n=1 1(Ψt in =
i=1
for each epoch t E [1, T]
</equation>
<figure confidence="0.344951142857143">
1. draw global measure
Gt0 — DP(α KG0 + K �a 0 F(v, δ)Gt δ0)
2. for each document Dti at epoch t,
2.1 draw local measure Gti — DP(γ, Gt0)
2.2 for each sentence sti - in Dti
draw aspect θtij — G?
for w E stij draw w — f(w)|θtij
</figure>
<figureCaption confidence="0.9999">
Figure 1: Generation process for EHDP
</figureCaption>
<bodyText confidence="0.989856">
another parameter Mtk to incorporate time depen-
dency into EHDP.
</bodyText>
<equation confidence="0.9939745">
Mtk = XΔ F(v, δ) · mt−δ,k (2)
δ=0
</equation>
<bodyText confidence="0.949706083333333">
Let ntib denote the number of sentences sitting
around table b, in document i at epoch t. In CRP
for EHDP, when a new customer stij comes in,
he can sit on the existing table with probability
ntib/(nti−1+γ), sharing the dish (topic) Ψt ib served
at that table or picking a new table with probabil-
ity γ/(nti − 1 + γ). The customer has to select
a dish from the global dish menu if he chooses a
new table. A dish that has already been shared in
the global menu would be chosen with probability
Mtk/(Pk Mtk+α) and a new dish with probability
α/(Pk Mtk + α).
</bodyText>
<equation confidence="0.9804024">
θt ij|θt i1, ..., θt ij−1, α ∼
nt γ
ib δφne
nt i − 1 + γ δφjb + nt jb
i − 1 + γ
new
φti |φ, α ∼
tk α G0 (3)
P δφk + P
i Mti + αi Mti + α
</equation>
<bodyText confidence="0.99648075">
We can see that EHDP degenerates into a series of
independent HDPs when Δ = 0 and one global
HDP when Δ = T and v = ∞, as discussed in
Amred and Xings work (2008).
</bodyText>
<subsectionHeader confidence="0.99764">
2.3 Sentence Selection Strategy
</subsectionHeader>
<bodyText confidence="0.999851111111111">
The task of timeline summarization aims to pro-
duce a summary for each time and the generated
summary should meet criteria such as relevance ,
coverage and coherence (Li et al., 2009). To care
for these three criteria, we propose a topic scoring
algorithm based on Kullback-Leibler(KL) diver-
gence. We introduce the decreasing logistic func-
tion ζ(x) = 1/(1 + ex) to map the distance into
interval (0,1).
</bodyText>
<figure confidence="0.68361">
F(v, δ)·Gt−δ
0 ) (1)
k). We redefine
X
φtb=θij
w
X
k
M
557
</figure>
<figureCaption confidence="0.99145">
Figure 2: Graphical model of EHDP.
</figureCaption>
<bodyText confidence="0.9169">
Relevance: the summary should be related with
the proposed query Q.
</bodyText>
<equation confidence="0.968839">
FR(It) = ((KL(It||Q))
</equation>
<bodyText confidence="0.917081666666667">
Coverage: the summary should highly generalize
important topics mentioned in document collec-
tion at epoch t.
</bodyText>
<equation confidence="0.972772">
FCv(It) = ((KL(It||Ct))
</equation>
<bodyText confidence="0.998695666666667">
Coherence: News evolves over time and a good
component summary is coherent with neighboring
corpus so that a timeline tracks the gradual evolu-
tion trajectory for multiple correlative news.
Let Score(It) denote the score of the summary
and it is calculated in Equ.(4).
</bodyText>
<equation confidence="0.975536">
Score(It) = A1FR(It)+A2FCv(It)+A3FCh(It)
</equation>
<bodyText confidence="0.8948986">
(4)
Ei Ai = 1. Sentences with higher score are se-
lected into timeline. To avoid aspect redundancy,
MMR strategy (Goldstein et al., 1999) is adopted
in the process of sentence selection.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.991797">
3.1 Experiments set-up
</subsectionHeader>
<bodyText confidence="0.998520444444444">
We downloaded 3156 news articles from selected
sources such as BBC, New York Times and CNN
with various time spans and built the evaluation
systems which contains 6 real datasets. The news
belongs to different categories of Rule of Interpre-
tation (ROI) (Kumaran and Allan, 2004). Detailed
statistics are shown in Table 1. Dataset 2(Deep-
water Horizon oil spill), 3(Haiti Earthquake) and
5(Hurricane Sandy) are used as training data and
</bodyText>
<note confidence="0.9639394">
New Source Nation News Source Nation
BBC UK New York Times US
Guardian UK Washington Post US
CNN US Fox News US
ABC US MSNBC US
</note>
<tableCaption confidence="0.99962">
Table 1: New sources of datasets
</tableCaption>
<table confidence="0.999704571428571">
News Subjects (Query) #docs #epoch
1.Michael Jackson Death 744 162
2.Deepwater Horizon oil spill 642 127
3.Haiti Earthquake 247 83
4.American Presidential Election 1246 286
5.Hurricane Sandy 317 58
6.Jerry Sandusky Sexual Abuse 320 74
</table>
<tableCaption confidence="0.999714">
Table 2: Detailed information for datasets
</tableCaption>
<bodyText confidence="0.999367583333333">
the rest are used as test data. Summary at each
epoch is truncated to the same length of 50 words.
Summaries produced by baseline systems and
ours are automatically evaluated through ROUGE
evaluation metrics (Lin and Hovy, 2003). For
the space limit, we only report three ROUGE
ROUGE-2-F and ROUGE-W-F score. Reference
timeline in ROUGE evaluation is manually gener-
ated by using Amazon Mechanical Turk1. Work-
ers were asked to generate reference timeline for
news at each epoch in less than 50 words and we
collect 790 timelines in total.
</bodyText>
<subsectionHeader confidence="0.998791">
3.2 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.9999964375">
To tune the parameters A(i = 1, 2,3) and v in our
system, we adopt a gradient search strategy. We
firstly fix Ai to 1/3. Then we perform experiments
on with setting different values of v/#epoch in
the range from 0.02 to 0.2 at the interval of 0.02.
We find that the Rouge score reaches its peak at
round 0.1 and drops afterwards in the experiments.
Next, we set the value of v is set to 0.1 · #epoch
and gradually change the value of A1 from 0 to 1
with interval of 0.05, with simultaneously fixing
A2 and A3 to the same value of (1 − A1)/2. The
performance gets better as A1 increases from 0 to
0.25 and then declines. Then we set the value of
A1 to 0.25 and change the value of A2 from 0 to
0.75 with interval of 0.05. And the value of A2 is
set to 0.4, and A3 is set to 0.35 correspondingly.
</bodyText>
<subsectionHeader confidence="0.999399">
3.3 Comparison with other topic models
</subsectionHeader>
<bodyText confidence="0.7652778">
In this subsection, we compare our model with
4 topic model baselines on the test data. Stand-
HDP(1): A topic approach that models different
time epochs as a series of independent HDPs with-
out considering time dependency. Stand-HDP(2):
</bodyText>
<footnote confidence="0.93233">
1http://mturk.com
</footnote>
<equation confidence="0.9997738">
FCh(It) =
Eδ=Δ/2)
δ=−Δ/2 F(v, δ
�δ=Δ/2
δ=−Δ/2 F(v, δ) · ((KL(It||Ct−δ))
</equation>
<page confidence="0.993828">
558
</page>
<table confidence="0.999835285714286">
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Stand-HDP(1) 0.080 0.127 0.075 0.134 0.072 0.138
Stand-HDP(2) 0.077 0.124 0.072 0.127 0.071 0.131
Dyn-LDA 0.080 0.129 0.073 0.130 0.077 0.134
Stan-LDA 0.072 0.117 0.065 0.122 0.071 0.121
</table>
<tableCaption confidence="0.999608">
Table 3: Comparison with topic models
</tableCaption>
<table confidence="0.999901285714286">
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Centroid 0.057 0.101 0.054 0.098 0.060 0.132
Manifold 0.053 0.108 0.060 0.111 0.069 0.128
ETS 0.078 0.120 0.073 0.130 0.075 0.135
Chieu 0.064 0.107 0.064 0.122 0.071 0.131
</table>
<tableCaption confidence="0.999987">
Table 4: Comparison with other baselines
</tableCaption>
<bodyText confidence="0.999764733333333">
A global HDP which models the whole time span
as a restaurant. The third baseline, Dynamic-
LDA is based on Blei and Laffery(2007)‘s work
and Stan-LDA is based on standard LDA model.
In LDA based models, aspect number is prede-
fined as 80 2. Experimental results of different
models are shown in Table 2. As we can see,
EHDP achieves better results than the two stan-
dard HDP baselines where time information is not
adequately considered. We also find an interesting
result that Stan-HDP performs better than Stan-
LDA. This is partly because new aspects can be
automatically detected in HDP. As we know, how
to determine topic number in the LDA-based mod-
els is still an open problem.
</bodyText>
<subsectionHeader confidence="0.99943">
3.4 Comparison with other baselines
</subsectionHeader>
<bodyText confidence="0.999949115384616">
We implement several baselines used in tradi-
tional summarization or timeline summarization
for comparison. (1) Centroid applies the MEAD
algorithm (Radev et al., 2004) according to the
features including centroid value, position and
first-sentence overlap. (2) Manifold is a graph
based unsupervised method for summarization,
and the score of each sentence is got from the
propagation through the graph (Wan et al., 2007).
(3) ETS is the timeline summarization approach
developed by Yan et al., (2011a), which is a graph
based approach with optimized global and local
biased summarization. (4) Chieu is the time-
line system provided by (Chieu and Lee, 2004)
utilizing interest and bursty ranking but neglect-
ing trans-temporal news evolution. As we can
see from Table 3, Centroid and Manifold get
the worst results. This is probably because meth-
ods in multi-document summarization only care
about sentence selection and neglect the novelty
detection task. We can also see that EHDP under
our proposed framework outputs existing timeline
summarization approaches ETS and chieu. Our
approach outputs Yan et al.,(2011a)s model by
6.9% and 9.3% respectively with regard to the av-
erage score of ROUGE-2-F and ROUGE-W-F.
</bodyText>
<sectionHeader confidence="0.9964" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99649025">
In this paper we present an evolutionary HDP
model for timeline summarization. Our EHDP ex-
tends original HDP by incorporating time depen-
dencies and background information. We also de-
velop an effective sentence selection strategy for
candidate in the summaries. Experimental results
on real multi-time news demonstrate the effective-
ness of our topic model.
</bodyText>
<figureCaption confidence="0.8443836">
Oct. 3, 2012
S1: The first debate between President Obama and Mitt Rom-
ney, so long anticipated, quickly sunk into an unenlightening
recitation of tired talking points and mendacity. S2. Mr. Rom-
ney wants to restore the Bush-era tax cut that expires at the end
of this year and largely benefits the wealthy
Oct. 11, 2012
S1: The vice presidential debate took place on Thursday, Oc-
tober 11 at Kentucky’sCentre College, and was moderated by
Martha Raddatz. S2: The first and only debate between Vice
President Joe Biden and Congressman Paul Ryan focused on
domestic and foreign policy. The domestic policy segments in-
cluded questions on health care, abortion
Oct. 16, 2012
S1. President Obama fights back in his second debate with Mitt
Romney, banishing some of the doubts he raised in their first
showdown. S2: The second debate dealt primarily with domes-
tic affairs and include some segues into foreign policy. includ-
ing taxes, unemployment, job creation, the national debt, energy
and women’s rights, both legal and
</figureCaption>
<tableCaption confidence="0.825382">
Table 5: Selected timeline summarization gener-
ated by EHDP for American Presidential Election
</tableCaption>
<sectionHeader confidence="0.995119" genericHeader="conclusions">
5 Acknowledgement
</sectionHeader>
<bodyText confidence="0.5605364">
This research has been supported by NSFC grants
(No.61273278), National Key Technology RD
Program (No:2011BAH1B0403), National 863
Program (No.2012AA011101) and National So-
cial Science Foundation (No.12ZD227).
</bodyText>
<sectionHeader confidence="0.978465" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.970718333333333">
2In our experiments, the aspect number is set as 50, 80,
100 and 120 respectively and we select the best performed
result with the aspect number as 80
Amr Ahmed and Eric Xing. Dynamic non-parametric
mixture models and the recurrent chinese restaurant
process. 2008. In SDM.
</footnote>
<page confidence="0.992861">
559
</page>
<reference confidence="0.992765844155844">
James Allan, Rahul Gupta and Vikas Khandelwal.
Temporal summaries of new topics. 2001. In Pro-
ceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval
David Blei, Andrew Ng and Micheal Jordan. 2003.
Latent dirichlet allocation. In Journal of Machine
Learning Research.
David Blei and John Lafferty. Dynamic topic models.
2006. In Proceedings of the 23rd international con-
ference on Machine learning.
Francois Carol, Manuel Davy and Arnaud Doucet.
Generalized poly urn for time-varying dirichlet pro-
cess mixtures. 2007. In Proceedings of the Interna-
tional Conference on Uncertainty in Artificial Intel-
ligence.
Deepayan Chakrabarti, Ravi Kumar and Andrew
Tomkins. Evolutionary Clustering. InProceedings of
the 12th ACM SIGKDD international conference
Knowledge discoveryand data mining.
Hai-Leong Chieu and Yoong-Keok Lee. Query based
event extraction along a timeline. In Proceedings of
the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval
Giridhar Kumaran and James Allan. 2004. Text classifi-
cation and named entities for new event detection. In
Proceedings of the 27th annual international ACM
SIGIR04.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha
and Yong Yu. Enhancing diversity, coverage and bal-
ance for summarization through structure learning.
In Proceedings of the 18th international conference
on World wide web.
Chin-Yew Lin and Eduard Hovy. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the Human Language Technology
Conference of the NAACL. 2003.
Dragomar Radev, Hongyan. Jing, and Malgorzata Stys.
2004. Centroid-based summarization of multiple
documents. In Information Processing and Manage-
ment.
Lu Ren, David Dunson and Lawrence Carin. The dy-
namic hierarchical Dirichlet process. 2008. In Pro-
ceedings of the 25th international conference on
Machine Learning.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence.
Xuerui Wang and Andrew MaCallum. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
Yee Whye Teh, Michael Jordan, Matthew Beal and
David Blei. Hierarchical Dirichlet Processes. In
American Statistical Association.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li and Yan Zhang. 2011a. Evolutionary
Timeline Summarization: a Balanced Optimization
Framework via Iterative Substitution. In Proceed-
ings of the 34th international ACM SIGIR confer-
ence on Research and development in Information
Retrieval.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Jahna Otterbacher, Xiaoming Li and Yan Zhang.
Timeline Generation Evolutionary Trans-Temporal
Summarization. 2011b. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Jianwen Zhang, Yangqiu Song, Changshui Zhang and
Shixia Liu. 2010. Evolutionary Hierarchical Dirich-
let Processes for Multiple Correlated Time-varying
Corpora. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
</reference>
<page confidence="0.997018">
560
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.111783">
<title confidence="0.999326">Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</title>
<author confidence="0.97573">Jiwei</author>
<affiliation confidence="0.810717666666667">School of Computer Cornell Ithaca, NY,</affiliation>
<email confidence="0.996834">jl3226@cornell.edu</email>
<author confidence="0.645097">Sujian</author>
<affiliation confidence="0.689647">Laboratory of Computational Peking</affiliation>
<address confidence="0.692938">Bejing, P.R.China,</address>
<email confidence="0.932652">lisujian@pku.edu.cn</email>
<abstract confidence="0.999420142857143">Timeline summarization aims at generating concise summaries and giving readers a faster and better access to understand the evolution of news. It is a new challenge which combines salience ranking problem with novelty detection. Previous researches in this field seldom explore the evolutionary pattern of topics such as birth, splitting, merging, developing and death. In this paper, we develop a novel model called Evolutionary Hierarchical Dirichlet Process(EHDP) to capture the topic evolution pattern in timeline summarization. In EHDP, time varying information is formulated as a series of HDPs by considering time-dependent information. Experiments on 6 different datasets which contain 3156 documents demonstrates the good performance of our system with regard to ROUGE scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Rahul Gupta and Vikas Khandelwal. Temporal summaries of new topics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<marker>Allan, 2001</marker>
<rawString>James Allan, Rahul Gupta and Vikas Khandelwal. Temporal summaries of new topics. 2001. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Micheal Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="3016" citStr="Blei et al., 2003" startWordPosition="466" endWordPosition="469">marization (MDS) to timeline generation by projecting sentences from different time into one plane. They further explored the timeline task from the optimization of a function considering the combination of different respects such as relevance, coverage, coherence and diversity (Yan et al., 2011b). However, their approaches just treat timeline generation as a sentence ranking or optimization problem and seldom explore the topic information lied in the corpus. Recently, topic models have been widely used for capturing the dynamics of topics via time. Many dynamic approaches based on LDA model (Blei et al., 2003) or Hierarchical Dirichelt Processes(HDP) (Teh et al., 2006) have been proposed to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch (Blei and Lafferty, 2006; Chakrabarti et al., 2006; Wang and McCallum, 2007; Caron et al., 2007; Ren et al., 2008; Ahmed and Xing, 2008; Zhang et al., 2010). In this paper, we propose EHDP: a evolutionary hierarchical Dirichlet process (HDP) model for timeline summarization. In EHDP, each HDP is built for multiple corpora at each time epoch, and the time dependencies are incorporated into epochs under the Markovian a</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng and Micheal Jordan. 2003. Latent dirichlet allocation. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning.</booktitle>
<contexts>
<context position="3220" citStr="Blei and Lafferty, 2006" startWordPosition="502" endWordPosition="505">tion of different respects such as relevance, coverage, coherence and diversity (Yan et al., 2011b). However, their approaches just treat timeline generation as a sentence ranking or optimization problem and seldom explore the topic information lied in the corpus. Recently, topic models have been widely used for capturing the dynamics of topics via time. Many dynamic approaches based on LDA model (Blei et al., 2003) or Hierarchical Dirichelt Processes(HDP) (Teh et al., 2006) have been proposed to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch (Blei and Lafferty, 2006; Chakrabarti et al., 2006; Wang and McCallum, 2007; Caron et al., 2007; Ren et al., 2008; Ahmed and Xing, 2008; Zhang et al., 2010). In this paper, we propose EHDP: a evolutionary hierarchical Dirichlet process (HDP) model for timeline summarization. In EHDP, each HDP is built for multiple corpora at each time epoch, and the time dependencies are incorporated into epochs under the Markovian assumptions. Topic popularity and topic-word distribution can be inferred from a Chinese Restaurant Process (CRP). Sentences are selected into timelines by considering different aspects such as topic relev</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. Dynamic topic models. 2006. In Proceedings of the 23rd international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Carol</author>
<author>Manuel Davy</author>
<author>Arnaud Doucet</author>
</authors>
<title>Generalized poly urn for time-varying dirichlet process mixtures.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Uncertainty in Artificial Intelligence.</booktitle>
<marker>Carol, Davy, Doucet, 2007</marker>
<rawString>Francois Carol, Manuel Davy and Arnaud Doucet. Generalized poly urn for time-varying dirichlet process mixtures. 2007. In Proceedings of the International Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Deepayan Chakrabarti</author>
<author>Ravi Kumar</author>
<author>Andrew Tomkins</author>
</authors>
<booktitle>Evolutionary Clustering. InProceedings of the 12th ACM SIGKDD international conference Knowledge discoveryand data mining.</booktitle>
<marker>Chakrabarti, Kumar, Tomkins, </marker>
<rawString>Deepayan Chakrabarti, Ravi Kumar and Andrew Tomkins. Evolutionary Clustering. InProceedings of the 12th ACM SIGKDD international conference Knowledge discoveryand data mining.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hai-Leong Chieu</author>
<author>Yoong-Keok Lee</author>
</authors>
<title>Query based event extraction along a timeline.</title>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<marker>Chieu, Lee, </marker>
<rawString>Hai-Leong Chieu and Yoong-Keok Lee. Query based event extraction along a timeline. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Text classification and named entities for new event detection.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR04.</booktitle>
<contexts>
<context position="8905" citStr="Kumaran and Allan, 2004" startWordPosition="1536" endWordPosition="1539">s. Let Score(It) denote the score of the summary and it is calculated in Equ.(4). Score(It) = A1FR(It)+A2FCv(It)+A3FCh(It) (4) Ei Ai = 1. Sentences with higher score are selected into timeline. To avoid aspect redundancy, MMR strategy (Goldstein et al., 1999) is adopted in the process of sentence selection. 3 Experiments 3.1 Experiments set-up We downloaded 3156 news articles from selected sources such as BBC, New York Times and CNN with various time spans and built the evaluation systems which contains 6 real datasets. The news belongs to different categories of Rule of Interpretation (ROI) (Kumaran and Allan, 2004). Detailed statistics are shown in Table 1. Dataset 2(Deepwater Horizon oil spill), 3(Haiti Earthquake) and 5(Hurricane Sandy) are used as training data and New Source Nation News Source Nation BBC UK New York Times US Guardian UK Washington Post US CNN US Fox News US ABC US MSNBC US Table 1: New sources of datasets News Subjects (Query) #docs #epoch 1.Michael Jackson Death 744 162 2.Deepwater Horizon oil spill 642 127 3.Haiti Earthquake 247 83 4.American Presidential Election 1246 286 5.Hurricane Sandy 317 58 6.Jerry Sandusky Sexual Abuse 320 74 Table 2: Detailed information for datasets the </context>
</contexts>
<marker>Kumaran, Allan, 2004</marker>
<rawString>Giridhar Kumaran and James Allan. 2004. Text classification and named entities for new event detection. In Proceedings of the 27th annual international ACM SIGIR04.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Liangda Li</author>
<author>Ke Zhou</author>
</authors>
<title>Gui-Rong Xue, Hongyuan Zha and Yong Yu. Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<booktitle>In Proceedings of the 18th international conference on World wide web.</booktitle>
<marker>Li, Zhou, </marker>
<rawString>Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha and Yong Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th international conference on World wide web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="9729" citStr="Lin and Hovy, 2003" startWordPosition="1671" endWordPosition="1674">York Times US Guardian UK Washington Post US CNN US Fox News US ABC US MSNBC US Table 1: New sources of datasets News Subjects (Query) #docs #epoch 1.Michael Jackson Death 744 162 2.Deepwater Horizon oil spill 642 127 3.Haiti Earthquake 247 83 4.American Presidential Election 1246 286 5.Hurricane Sandy 317 58 6.Jerry Sandusky Sexual Abuse 320 74 Table 2: Detailed information for datasets the rest are used as test data. Summary at each epoch is truncated to the same length of 50 words. Summaries produced by baseline systems and ours are automatically evaluated through ROUGE evaluation metrics (Lin and Hovy, 2003). For the space limit, we only report three ROUGE ROUGE-2-F and ROUGE-W-F score. Reference timeline in ROUGE evaluation is manually generated by using Amazon Mechanical Turk1. Workers were asked to generate reference timeline for news at each epoch in less than 50 words and we collect 790 timelines in total. 3.2 Parameter Tuning To tune the parameters A(i = 1, 2,3) and v in our system, we adopt a gradient search strategy. We firstly fix Ai to 1/3. Then we perform experiments on with setting different values of v/#epoch in the range from 0.02 to 0.2 at the interval of 0.02. We find that the Rou</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the Human Language Technology Conference of the NAACL. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing</author>
<author>Malgorzata Stys</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>In Information Processing and Management.</booktitle>
<marker>Jing, Stys, 2004</marker>
<rawString>Dragomar Radev, Hongyan. Jing, and Malgorzata Stys. 2004. Centroid-based summarization of multiple documents. In Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Ren</author>
<author>David Dunson</author>
<author>Lawrence Carin</author>
</authors>
<title>The dynamic hierarchical Dirichlet process.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine Learning.</booktitle>
<contexts>
<context position="3309" citStr="Ren et al., 2008" startWordPosition="518" endWordPosition="521">1b). However, their approaches just treat timeline generation as a sentence ranking or optimization problem and seldom explore the topic information lied in the corpus. Recently, topic models have been widely used for capturing the dynamics of topics via time. Many dynamic approaches based on LDA model (Blei et al., 2003) or Hierarchical Dirichelt Processes(HDP) (Teh et al., 2006) have been proposed to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch (Blei and Lafferty, 2006; Chakrabarti et al., 2006; Wang and McCallum, 2007; Caron et al., 2007; Ren et al., 2008; Ahmed and Xing, 2008; Zhang et al., 2010). In this paper, we propose EHDP: a evolutionary hierarchical Dirichlet process (HDP) model for timeline summarization. In EHDP, each HDP is built for multiple corpora at each time epoch, and the time dependencies are incorporated into epochs under the Markovian assumptions. Topic popularity and topic-word distribution can be inferred from a Chinese Restaurant Process (CRP). Sentences are selected into timelines by considering different aspects such as topic relevance, coverage and coherence. We built the evaluation sys556 Proceedings of the 51st Annu</context>
</contexts>
<marker>Ren, Dunson, Carin, 2008</marker>
<rawString>Lu Ren, David Dunson and Lawrence Carin. The dynamic hierarchical Dirichlet process. 2008. In Proceedings of the 25th international conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Manifold-ranking based topic-focused multidocument summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="13007" citStr="Wan et al., 2007" startWordPosition="2242" endWordPosition="2245">is partly because new aspects can be automatically detected in HDP. As we know, how to determine topic number in the LDA-based models is still an open problem. 3.4 Comparison with other baselines We implement several baselines used in traditional summarization or timeline summarization for comparison. (1) Centroid applies the MEAD algorithm (Radev et al., 2004) according to the features including centroid value, position and first-sentence overlap. (2) Manifold is a graph based unsupervised method for summarization, and the score of each sentence is got from the propagation through the graph (Wan et al., 2007). (3) ETS is the timeline summarization approach developed by Yan et al., (2011a), which is a graph based approach with optimized global and local biased summarization. (4) Chieu is the timeline system provided by (Chieu and Lee, 2004) utilizing interest and bursty ranking but neglecting trans-temporal news evolution. As we can see from Table 3, Centroid and Manifold get the worst results. This is probably because methods in multi-document summarization only care about sentence selection and neglect the novelty detection task. We can also see that EHDP under our proposed framework outputs exis</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multidocument summarization. In Proceedings of International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xuerui Wang</author>
<author>Andrew MaCallum</author>
</authors>
<title>Topics over time: a non-Markov continuous-time model of topical trends.</title>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<marker>Wang, MaCallum, </marker>
<rawString>Xuerui Wang and Andrew MaCallum. Topics over time: a non-Markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<journal>In American Statistical Association.</journal>
<marker>Teh, Jordan, Beal, Blei, </marker>
<rawString>Yee Whye Teh, Michael Jordan, Matthew Beal and David Blei. Hierarchical Dirichlet Processes. In American Statistical Association.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
</authors>
<title>Congrui Huang, Xiaojun Wan, Xiaoming Li and Yan Zhang. 2011a. Evolutionary Timeline Summarization: a Balanced Optimization Framework via Iterative Substitution.</title>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval.</booktitle>
<marker>Yan, Kong, </marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li and Yan Zhang. 2011a. Evolutionary Timeline Summarization: a Balanced Optimization Framework via Iterative Substitution. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
</authors>
<title>Liang Kong, Congrui Huang, Xiaojun Wan, Jahna Otterbacher, Xiaoming Li and Yan Zhang. Timeline Generation Evolutionary Trans-Temporal Summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Yan, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Jahna Otterbacher, Xiaoming Li and Yan Zhang. Timeline Generation Evolutionary Trans-Temporal Summarization. 2011b. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianwen Zhang</author>
<author>Yangqiu Song</author>
<author>Changshui Zhang</author>
<author>Shixia Liu</author>
</authors>
<title>Evolutionary Hierarchical Dirichlet Processes for Multiple Correlated Time-varying Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="3352" citStr="Zhang et al., 2010" startWordPosition="526" endWordPosition="529">t timeline generation as a sentence ranking or optimization problem and seldom explore the topic information lied in the corpus. Recently, topic models have been widely used for capturing the dynamics of topics via time. Many dynamic approaches based on LDA model (Blei et al., 2003) or Hierarchical Dirichelt Processes(HDP) (Teh et al., 2006) have been proposed to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch (Blei and Lafferty, 2006; Chakrabarti et al., 2006; Wang and McCallum, 2007; Caron et al., 2007; Ren et al., 2008; Ahmed and Xing, 2008; Zhang et al., 2010). In this paper, we propose EHDP: a evolutionary hierarchical Dirichlet process (HDP) model for timeline summarization. In EHDP, each HDP is built for multiple corpora at each time epoch, and the time dependencies are incorporated into epochs under the Markovian assumptions. Topic popularity and topic-word distribution can be inferred from a Chinese Restaurant Process (CRP). Sentences are selected into timelines by considering different aspects such as topic relevance, coverage and coherence. We built the evaluation sys556 Proceedings of the 51st Annual Meeting of the Association for Computati</context>
</contexts>
<marker>Zhang, Song, Zhang, Liu, 2010</marker>
<rawString>Jianwen Zhang, Yangqiu Song, Changshui Zhang and Shixia Liu. 2010. Evolutionary Hierarchical Dirichlet Processes for Multiple Correlated Time-varying Corpora. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>