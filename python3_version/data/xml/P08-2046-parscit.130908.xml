<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028781">
<title confidence="0.98379">
Evolving new lexical association measures using genetic programming
</title>
<author confidence="0.996284">
Jan Snajder Bojana Dalbelo Baˇsi´c Saˇsa Petrovi´c Ivan Sikiri´c
</author>
<affiliation confidence="0.998201">
Faculty of Electrical Engineering and Computing, University of Zagreb
</affiliation>
<address confidence="0.571752">
Unska 3, Zagreb, Croatia
</address>
<email confidence="0.992398">
ban.snajder, bojana.dalbelo, sasa.petrovic, ivan.sikiric}@fer.hr
</email>
<sectionHeader confidence="0.993688" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996659272727273">
Automatic extraction of collocations from
large corpora has been the focus of many re-
search efforts. Most approaches concentrate
on improving and combining known lexical
association measures. In this paper, we de-
scribe a genetic programming approach for
evolving new association measures, which is
not limited to any specific language, corpus,
or type of collocation. Our preliminary experi-
mental results show that the evolved measures
outperform three known association measures.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983404761905">
A collocation is an expression consisting of two or
more words that correspond to some conventional
way of saying things (Manning and Sch¨utze, 1999).
Related to the term collocation is the term n-gram,
which is used to denote any sequence of n words.
There are many possible applications of colloca-
tions: automatic language generation, word sense
disambiguation, improving text categorization, in-
formation retrieval, etc. As different applications
require different types of collocations that are of-
ten not found in dictionaries, automatic extraction
of collocations from large textual corpora has been
the focus of much research in the last decade; see,
for example, (Pecina and Schlesinger, 2006; Evert
and Krenn, 2005).
Automatic extraction of collocations is usually
performed by employing lexical association mea-
sures (AMs) to indicate how strongly the words
comprising an n-gram are associated. However, the
use of lexical AMs for the purpose of collocation
extraction has reached a plateau; recent research
in this field has focused on combining the existing
AMs in the hope of improving the results (Pecina
and Schlesinger, 2006). In this paper, we propose
an approach for deriving new AMs for collocation
extraction based on genetic programming. A simi-
lar approach has been usefully applied in text min-
ing (Atkinson-Abutridy et al., 2004) as well as in
information retrieval (Gordon et al., 2006).
Genetic programming is an evolutionary compu-
tational technique designed to mimic the process of
natural evolution for the purpose of solving complex
optimization problems by stochastically searching
through the whole space of possible solutions (Koza,
1992). The search begins from an arbitrary seed
of possible solutions (the initial population), which
are then improved (evolved) through many iterations
by employing the operations of selection, crossover,
and mutation. The process is repeated until a termi-
nation criterion is met, which is generally defined by
the goodness of the best solution or the expiration of
a time limit.
</bodyText>
<sectionHeader confidence="0.943999" genericHeader="method">
2 Genetic programming of AMs
</sectionHeader>
<subsectionHeader confidence="0.978604">
2.1 AM representation
</subsectionHeader>
<bodyText confidence="0.999884272727273">
In genetic programming, possible solutions (in our
case lexical AMs) are mathematical expressions rep-
resented by a tree structure (Koza, 1992). The leaves
of the tree can be constants, or statistical or linguistic
information about an n-gram. A constant can be any
real number in an arbitrarily chosen interval; our ex-
periments have shown that variation of this interval
does not affect the performance. One special con-
stant that we use is the number of words in the cor-
pus. The statistical information about an n-gram can
be the frequency of any part of the n-gram. For ex-
</bodyText>
<page confidence="0.985293">
181
</page>
<reference confidence="0.221952">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181–184,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999946136363636">
ample, for a trigram abc the statistical information
can be the frequency f(abc) of the whole trigram,
frequencies f(ab) and f(bc) of the digrams, and
the frequencies of individual words f(a), f(b), and
f(c). The linguistic information about an n-gram is
the part-of-speech (POS) of any one of its words.
Inner nodes in the tree are operators. The binary
operators are addition, subtraction, multiplication,
and division. We also use one unary operator, the
natural logarithm, and one ternary operator, the IF-
THEN-ELSE operator. The IF-THEN-ELSE node
has three descendant nodes: the left descendant is
the condition in the form “i-th word of the n-gram
has a POS tag T,” and the other two descendants are
operators or constants. If the condition is true, then
the subexpression corresponding to the middle de-
scendant is evaluated, otherwise the subexpression
corresponding to the right descendant is evaluated.
The postfix expression of an AM can be obtained
by traversing its tree representation in postorder.
Figure 1 shows the representation of the Dice co-
efficient using our representation.
</bodyText>
<subsectionHeader confidence="0.997867">
2.2 Genetic operators
</subsectionHeader>
<bodyText confidence="0.999987186440678">
The crossover operator combines two parent solu-
tions into a new solution. We defined the crossover
operator as follows: from each of the two parents,
one node is chosen randomly, excluding any nodes
that represent the condition of the IF-THEN-ELSE
operator. A new solution is obtained by replacing
the subtree of the chosen node of the first parent with
the subtree of the chosen node of the second parent.
This method of defining the crossover operator is the
same as the one described in (Gordon et al., 2006).
The mutation operator introduces new “genetic
material” into a population by randomly changing
a solution. In our case, the mutation operator can
do one of two things: either remove a randomly se-
lected inner node (with probability of 25%), or insert
an inner node at a random position in the tree (with
probability of 75%). If a node is being removed
from the tree, one of its descendants (randomly cho-
sen) takes its place. An exception to this rule is the
IF-THEN-ELSE operator, which cannot be replaced
by its condition node. If a node is being inserted,
a randomly created operator node replaces an exist-
ing node that then becomes a descendant of the new
node. If the inserted node is not a unary operator,
the required number of random leaves is created.
The selection operator is used to copy the best so-
lutions into the next iteration. The goodness of the
solution is determined by the fitness function, which
assigns to each solution a number indicating how
good that particular solution actually is. We mea-
sure the goodness of an AM in terms of its F1 score,
obtained from the precision and recall computed on
a random sample consisting of 100 positive n-grams
(those considered collocations) and 100 negative n-
grams (non-collocations). These n-grams are ranked
according to the AM value assigned to them, after
which we compute the precision and recall by con-
sidering first n best-ranked n-grams as positives and
the rest as negatives, repeating this for each n be-
tween 1 and 200. The best F1 score is then taken as
the AM’s goodness.
Using the previous definition of the fitness func-
tion, preliminary experiments showed that solutions
soon become very complex in terms of number of
nodes in the tree (namely, on the order of tens
of thousands). This is a problem both in terms
of space and time efficiency; allowing unlimited
growth of the tree quickly consumes all computa-
tional resources. Also, it is questionable whether
the performance benefits from the increased size of
the solution. Thus, we modified the fitness func-
tion to also take into account the size of the tree
(that is, the less nodes a tree has, the better). Fa-
voring shorter solutions at the expense of some loss
in performance is known as parsimony, and it has
already been successfully used in genetic program-
ming (Koza, 1992). Therefore, the final formula for
the fitness function we used incorporates the parsi-
mony factor and is given by
</bodyText>
<equation confidence="0.926709666666667">
fitness (j) = F1(j) + qLmLmL(j)
(1)
ax
</equation>
<bodyText confidence="0.9996631">
where F1(j) is the F1 score (ranging from 0 to 1) of
the solution j, q is the parsimony factor, Lmax is the
maximal size (measured in number of nodes), and
L(j) is the size of solution j. By varying q we can
control how much loss of performance we will tol-
erate in order to get smaller, more elegant solutions.
Genetic programming algorithms usually iterate
until a termination criterion is met. In our case, the
algorithm terminates when a certain number, k, of
iterations has passed without an improvement in the
</bodyText>
<page confidence="0.955672">
182
</page>
<figure confidence="0.7702135">
Dice(a, b, c) = f(abc)
f(a)+f(b)+f(c)
</figure>
<figureCaption confidence="0.998663">
Figure 1: Dice coefficient for digrams represented by tree
</figureCaption>
<bodyText confidence="0.9994485">
results. To prevent the overfitting problem, we mea-
sure this improvement on another sample (valida-
tion sample) that also consists of 100 collocations
and 100 non-collocations.
</bodyText>
<sectionHeader confidence="0.993293" genericHeader="method">
3 Preliminary results
</sectionHeader>
<subsectionHeader confidence="0.977152">
3.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999986">
We use the previously described genetic program-
ming approach to evolve AMs for extracting collo-
cations consisting of three words from a corpus of
7008 Croatian legislative documents. Prior to this,
words from the corpus were lemmatized and POS
tagged. Conjunctions, propositions, pronouns, in-
terjections, and particles were treated as stop-words
and tagged with a POS tag X. N-grams starting or
ending with a stopword, or containing a verb, were
filtered out. For evaluation purposes we had a hu-
man expert annotate 200 collocations and 200 non-
collocations, divided into the evaluation and valida-
tion sample. We considered an n-gram to be a collo-
cation if it is a compound noun, terminological ex-
pression, or a proper name. Note that we could have
adopted any other definition of a collocation, since
this definition is implicit in the samples provided.
In our experiments, we varied a number of ge-
netic programming parameters. The size of the ini-
tial population varied between 50 and 50 thousand
randomly generated solutions. To examine the ef-
fects of including some known AMs on the perfor-
mance, the following AMs had a 50% chance of
being included in the initial population: pointwise
mutual information (Church and Hanks, 1990), the
Dice coefficient, and the heuristic measure defined
in (Petrovi´c et al., 2006):
</bodyText>
<equation confidence="0.97594825">
{ 2 log f(abc)
f(a)f(c) if POS(b) = X,
log f(abc)
f(a)f(b)f(c) otherwise.
</equation>
<bodyText confidence="0.995128769230769">
For the selection operator we used the well-known
three-tournament selection. The probability of mu-
tation was chosen from the interval [0.0001, 0.3],
and the parsimony factor q from the interval
[0, 0.05], thereby allowing a maximum of 5% loss
of F1 in favor of smaller solutions. The maximal
size of the tree in nodes was chosen from the inter-
val [20, 1000]. After the F1 score for the validation
sample began dropping, the algorithm would con-
tinue for another k iterations before stopping. The
parameter k was chosen from the interval [104, 107].
The experiments were run with 800 different random
combinations of the aforementioned parameters.
</bodyText>
<subsectionHeader confidence="0.870763">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999976461538462">
Around 20% of the evolved measures (that is, the so-
lutions that remained after the algorithm terminated)
achieved F1 scores of over 80% on both the evalu-
ation and validation samples. This proportion was
13% in the case when the initial population did not
include any known AMs, and 23% in the case when
it did, thus indicating that including known AMs in
the initial population is beneficial. The overall best
solution had 205 nodes and achieved an F1 score of
88.4%. In search of more elegant AMs, we singled
out solutions that had less than 30 nodes. Among
these, a solution that consisted of 13 nodes achieved
the highest F1. This measure is given by
</bodyText>
<equation confidence="0.98864125">
−0.423f(a)f(c)
f�(abc) if POS(b) = X,
1 − f(b) otherwise.
f(abc)
</equation>
<bodyText confidence="0.999911">
The association measure M13 is particularly inter-
esting because it takes into account whether the
middle word in a trigram is a stopword (denoted
by the POS tag X). This supports the claim laid
out in (Petrovi´c et al., 2006) that the trigrams con-
taining stopwords (e.g., cure for cancer) should be
treated differently, in that the frequency of the stop-
word should be ignored. It is important to note that
the aforementioned measure H was not included in
the initial population from which M13 evolved. It
is also worthwhile noting that in such populations,
out of 100 best evolved measures, all but four of
them featured a condition identical to that of M13
(POS(b) = X). In other words, the majority of
the measures evolved this condition completely in-
dependently, without H being included in the initial
population.
</bodyText>
<equation confidence="0.995716">
H(a, b, c) =
M13(a, b, c) = {
</equation>
<page confidence="0.998261">
183
</page>
<figure confidence="0.9873215">
F1 score
Number of n−grams (× 105)
</figure>
<figureCaption confidence="0.996022">
Figure 2: Comparison of association measures on a cor-
pus of 7008 Croatian documents
</figureCaption>
<bodyText confidence="0.98958475">
Figure 2 shows the comparison of AMs in terms
of their F1 score obtained on the corpus of 7008
documents. The x axis shows the number of n
best ranked n-grams that are considered positives
(we show only the range of n in which all the AMs
achieve their maximum F1; all measures tend to per-
form similarly with increasing n). The maximum
F1 score is achieved if we take 5 x 105 n-grams
ranked best by the M205 measure. From Fig. 2 we
can see that the evolved AMs M13 and M205 outper-
formed the other three considered AMs. For exam-
ple, collocations kosilica za travu (lawn mower) and
digitalna obrada podataka (digital data processing)
were ranked at the 22th and 34th percentile accord-
ing to Dice, whereas they were ranked at the 97th
and 87th percentile according to M13.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9994456">
In this paper we described a genetic programming
approach for evolving new lexical association mea-
sures in order to extract collocations.
The evolved association measure will perform at
least as good as any other AM included in the initial
population. However, the evolved association mea-
sure may be a complex expression that defies inter-
pretation, in which case it may be treated as a black-
box suitable for the specific task of collocation ex-
traction. Our approach only requires an evaluation
sample, thus it is not limited to any specific type of
collocation, language or corpus.
The preliminary experiments, conducted on a cor-
pus of Croatian documents, showed that the best
evolved measures outperformed other considered as-
sociation measures. Also, most of the best evolved
association measures took into account the linguis-
tic information about an n-gram (the POS of the in-
dividual words).
As part of future work, we intend to apply our ap-
proach to corpora in other languages and compare
the results with existing collocation extraction sys-
tems. We also intend to apply our approach to collo-
cations consisting of more than three words, and to
experiment with additional linguistic features.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999701">
This work has been supported by the Government
of Republic of Croatia, and Government of Flan-
ders under the grant No. 036-1300646-1986 and
KRO/009/06.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999015464285714">
John Atkinson-Abutridy, Chris Mellish, and Stuart
Aitken. 2004. Combining information extraction with
genetic algorithms for text mining. IEEE Intelligent
Systems, 19(3):22–30.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Stephan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statisti-
cal evaluation measures. Computer Speech and Lan-
guage, 19(4):450–466.
Michael Gordon, Weiguo Fan, and Praveen Pathak.
2006. Adaptive web search: Evolving a program
that finds information. IEEE Intelligent Systems,
21(5):72–77.
John R. Koza. 1992. Genetic programming: On the pro-
gramming of computers by means of natural selection.
MIT Press.
Christopher Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA, USA.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
Proc. of the COLING/ACL 2006, pages 651–658.
Saˇsa Petrovi´c, Jan ˇSnajder, Bojana Dalbelo Baˇsi´c, and
Mladen Kolar. 2006. Comparison of collocation ex-
traction measures for document indexing. J. of Com-
puting and Information Technology, 14(4):321–327.
</reference>
<figure confidence="0.997533352941177">
100
90
80
70
60
50
40
30
20
10
0
1 2 3 4 5 6 7 8 9 10
Dice
PMI
H
M13
M205
</figure>
<page confidence="0.966954">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980431">
<title confidence="0.999825">Evolving new lexical association measures using genetic programming</title>
<author confidence="0.999947">Jan Snajder Bojana Dalbelo Baˇsi´c Saˇsa Petrovi´c Ivan Sikiri´c</author>
<affiliation confidence="0.99943">Faculty of Electrical Engineering and Computing, University of Zagreb</affiliation>
<address confidence="0.997645">Unska 3, Zagreb, Croatia</address>
<email confidence="0.986216">bojana.dalbelo,sasa.petrovic,</email>
<abstract confidence="0.999748333333334">Automatic extraction of collocations from large corpora has been the focus of many research efforts. Most approaches concentrate on improving and combining known lexical association measures. In this paper, we describe a genetic programming approach for evolving new association measures, which is not limited to any specific language, corpus, or type of collocation. Our preliminary experimental results show that the evolved measures outperform three known association measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>181--184</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181–184,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Atkinson-Abutridy</author>
<author>Chris Mellish</author>
<author>Stuart Aitken</author>
</authors>
<title>Combining information extraction with genetic algorithms for text mining.</title>
<date>2004</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="2146" citStr="Atkinson-Abutridy et al., 2004" startWordPosition="311" endWordPosition="314">2005). Automatic extraction of collocations is usually performed by employing lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated. However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully applied in text mining (Atkinson-Abutridy et al., 2004) as well as in information retrieval (Gordon et al., 2006). Genetic programming is an evolutionary computational technique designed to mimic the process of natural evolution for the purpose of solving complex optimization problems by stochastically searching through the whole space of possible solutions (Koza, 1992). The search begins from an arbitrary seed of possible solutions (the initial population), which are then improved (evolved) through many iterations by employing the operations of selection, crossover, and mutation. The process is repeated until a termination criterion is met, which</context>
</contexts>
<marker>Atkinson-Abutridy, Mellish, Aitken, 2004</marker>
<rawString>John Atkinson-Abutridy, Chris Mellish, and Stuart Aitken. 2004. Combining information extraction with genetic algorithms for text mining. IEEE Intelligent Systems, 19(3):22–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Evert</author>
<author>Brigitte Krenn</author>
</authors>
<title>Using small random samples for the manual evaluation of statistical evaluation measures. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="1520" citStr="Evert and Krenn, 2005" startWordPosition="213" endWordPosition="216">way of saying things (Manning and Sch¨utze, 1999). Related to the term collocation is the term n-gram, which is used to denote any sequence of n words. There are many possible applications of collocations: automatic language generation, word sense disambiguation, improving text categorization, information retrieval, etc. As different applications require different types of collocations that are often not found in dictionaries, automatic extraction of collocations from large textual corpora has been the focus of much research in the last decade; see, for example, (Pecina and Schlesinger, 2006; Evert and Krenn, 2005). Automatic extraction of collocations is usually performed by employing lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated. However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully applied in text mining (Atkin</context>
</contexts>
<marker>Evert, Krenn, 2005</marker>
<rawString>Stephan Evert and Brigitte Krenn. 2005. Using small random samples for the manual evaluation of statistical evaluation measures. Computer Speech and Language, 19(4):450–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gordon</author>
<author>Weiguo Fan</author>
<author>Praveen Pathak</author>
</authors>
<title>Adaptive web search: Evolving a program that finds information.</title>
<date>2006</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>21</volume>
<issue>5</issue>
<contexts>
<context position="2204" citStr="Gordon et al., 2006" startWordPosition="321" endWordPosition="324">mploying lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated. However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully applied in text mining (Atkinson-Abutridy et al., 2004) as well as in information retrieval (Gordon et al., 2006). Genetic programming is an evolutionary computational technique designed to mimic the process of natural evolution for the purpose of solving complex optimization problems by stochastically searching through the whole space of possible solutions (Koza, 1992). The search begins from an arbitrary seed of possible solutions (the initial population), which are then improved (evolved) through many iterations by employing the operations of selection, crossover, and mutation. The process is repeated until a termination criterion is met, which is generally defined by the goodness of the best solution</context>
</contexts>
<marker>Gordon, Fan, Pathak, 2006</marker>
<rawString>Michael Gordon, Weiguo Fan, and Praveen Pathak. 2006. Adaptive web search: Evolving a program that finds information. IEEE Intelligent Systems, 21(5):72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Koza</author>
</authors>
<title>Genetic programming: On the programming of computers by means of natural selection.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2463" citStr="Koza, 1992" startWordPosition="359" endWordPosition="360">the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully applied in text mining (Atkinson-Abutridy et al., 2004) as well as in information retrieval (Gordon et al., 2006). Genetic programming is an evolutionary computational technique designed to mimic the process of natural evolution for the purpose of solving complex optimization problems by stochastically searching through the whole space of possible solutions (Koza, 1992). The search begins from an arbitrary seed of possible solutions (the initial population), which are then improved (evolved) through many iterations by employing the operations of selection, crossover, and mutation. The process is repeated until a termination criterion is met, which is generally defined by the goodness of the best solution or the expiration of a time limit. 2 Genetic programming of AMs 2.1 AM representation In genetic programming, possible solutions (in our case lexical AMs) are mathematical expressions represented by a tree structure (Koza, 1992). The leaves of the tree can b</context>
</contexts>
<marker>Koza, 1992</marker>
<rawString>John R. Koza. 1992. Genetic programming: On the programming of computers by means of natural selection. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
<author>Pavel Schlesinger</author>
</authors>
<title>Combining association measures for collocation extraction.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL</booktitle>
<pages>651--658</pages>
<contexts>
<context position="1496" citStr="Pecina and Schlesinger, 2006" startWordPosition="209" endWordPosition="212">rrespond to some conventional way of saying things (Manning and Sch¨utze, 1999). Related to the term collocation is the term n-gram, which is used to denote any sequence of n words. There are many possible applications of collocations: automatic language generation, word sense disambiguation, improving text categorization, information retrieval, etc. As different applications require different types of collocations that are often not found in dictionaries, automatic extraction of collocations from large textual corpora has been the focus of much research in the last decade; see, for example, (Pecina and Schlesinger, 2006; Evert and Krenn, 2005). Automatic extraction of collocations is usually performed by employing lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated. However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully appli</context>
</contexts>
<marker>Pecina, Schlesinger, 2006</marker>
<rawString>Pavel Pecina and Pavel Schlesinger. 2006. Combining association measures for collocation extraction. In Proc. of the COLING/ACL 2006, pages 651–658.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
<author>Mladen Kolar</author>
</authors>
<title>Comparison of collocation extraction measures for document indexing.</title>
<date>2006</date>
<journal>J. of Computing and Information Technology,</journal>
<volume>14</volume>
<issue>4</issue>
<marker>Petrovi´c, ˇSnajder, Baˇsi´c, Kolar, 2006</marker>
<rawString>Saˇsa Petrovi´c, Jan ˇSnajder, Bojana Dalbelo Baˇsi´c, and Mladen Kolar. 2006. Comparison of collocation extraction measures for document indexing. J. of Computing and Information Technology, 14(4):321–327.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>