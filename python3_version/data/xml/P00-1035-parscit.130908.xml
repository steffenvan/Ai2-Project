<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000650">
<title confidence="0.947208">
Language Independent, Minimally Supervised Induction of
Lexical Probabilities
</title>
<author confidence="0.92198">
Silviu Cucerzan and David Yarowsky
</author>
<affiliation confidence="0.929947333333333">
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.83334">
Baltimore, MD 21218
</address>
<email confidence="0.771853">
Isilviu,yarowskylOcs.jhu.edu
</email>
<sectionHeader confidence="0.989792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873333333334">
A central problem in part-of-speech
tagging, especially for new languages
for which limited annotated resources
are available, is estimating the distri-
bution of lexical probabilities for un-
known words. This paper introduces
a new paradigmatic similarity measure
and presents a minimally supervised
learning approach combining effective se-
lection and weighting methods based on
paradigmatic and contextual similarity
measures populated from large quanti-
ties of inexpensive raw text data. This
approach is highly language independent
and requires no modification to the al-
gorithm or implementation to shift be-
tween languages such as French and En-
glish.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862463414634">
Part-of-Speech tagging of English has reached
a level which seems to resist any improve-
ment. Methods like Transformation-based tag-
ging (Brill, 1995), MaxEnt (Ratnaparkhi, 1996),
Boosting (Abney et al., 1999), TnT/Markov mod-
els (Brants, 2000) achieve accuracies comparable
with human performance for this task.
However, if we break the results into two parts,
for known and unknown words, we can see that
the performance of English taggers is much lower
on the latter. The situation is even worse for
languages other than English, especially inflec-
tive languages, for two reasons: first, there is
usually less annotated data available and second,
the coverage of such data is much lower due to
the high number of different word-forms in these
languages (for comparison of properties and tag-
ging results for several such languages see (Haji,
2000)). Moreover, many of the words not found in
the (small) training data are in fact inflected forms
of quite common words. In the work described
herein we therefore concentrate on the problem
of unknown words in the context of probabilistic
tagging.
Although the annotated resources are limited
or even non-existent for most languages, the raw
text available online is effectively unlimited with
respect to the need of most NLP applications.
This paper presents a newly developed paradig-
matic similarity measure that tries to maximize
the benefits that can be obtained from limited
annotated resources using a large amount of raw
data by magnifying the impact and coverage of
the small tagged datasets.
To demonstrate the effectiveness and language
independence of the paradigmatic similarity mea-
sure in combination with contextual measures,
they are evaluated in the context of part-of-speech
tagger performance for 4 embedding algorithms
using French and English as representatives of
both inflective and analytical languages.
</bodyText>
<sectionHeader confidence="0.9900905" genericHeader="introduction">
2 Problem Description, Motivation
and Previous Work
</sectionHeader>
<bodyText confidence="0.9999196">
In this paper, we shall use the terms lexical
prior or tag prior for a given word to refer to
the probability P(t1w) of Part-of-Speech (POS)
tags for word w independent of context, as dis-
tinct from what we call the posterior distribution
P(ticontext;w), and also distinct from the con-
cept of channel model prior P (T), which refers to
the prior probability of a tag sequence T from a
generating source.
To facilitate clear exposition, we use here the
&amp;quot;direct&amp;quot; lexical probability of tag given word
P (t I w) , but corresponding arguments hold for the
classical HMM Bayesian method (Charniak et al.,
1993) used by the taggers we considered for eval-
uation purpose of the present work.
</bodyText>
<sectionHeader confidence="0.444698" genericHeader="method">
2.1 Training Data Characteristics with
Respect To Unknown Words
</sectionHeader>
<bodyText confidence="0.8573735">
Previously unseen (or &amp;quot;unknown&amp;quot;) words often
represent a significant portion of the vocabulary,
</bodyText>
<figure confidence="0.99689875">
90%
86.24%
80%
70%
types
OOV ratio
60%
50%
40%
35.74%
30%
32.99%
tokens
20%
8.89%
2k 4k 8k 15k 30k 60k
Training Set Size (number of tokens)
% of OOV words not found in the raw corpus
22%
20%
20.36%
18%
18.34%
types
16%
14%
tokens
12%
3 million 6 million 12 million
Untagged Corpus Size (number of tokens)
10%
5%
</figure>
<page confidence="0.918295">
12.95%
12.18%
</page>
<bodyText confidence="0.993203666666667">
that the observed lexical prior for a fixed suffix
(-ate) often differs substantially from the pan-
vocabulary universal tag probabilities (Table 2).
</bodyText>
<table confidence="0.893416294117647">
VB VBP NN NNP RB JJ
calculate 3 5 0 0 0 0
concentrate 25 2 4 0 0 0
delicate 0 0 0 0 0 7
extricate 2 0 0 0 0 0
fabricate 3 0 0 0 0 0
hate 5 7 1 0 0 0
inaccurate 0 0 0 0 0 6
inadequate 0 0 0 0 0 4
late 0 0 0 1 12 6
moderate 1 1 0 0 0 39
private 0 0 0 5 0 8
rate 1 2 575 0 0 0
surrogate 0 0 1 0 0 1
Suffix -ate prior (by token) .16 .04 .39 .13 .02 .26
Suffix -ate prior (by type) .40 .07 .16 .09 .00 .28
intricate 7 7 7 7 7 7
</table>
<tableCaption confidence="0.98766275">
Table 1: Some examples of words ending in -ate from
a 1-million-word tagged English corpus and the lexical
priors for suffix -ate as an estimation for unknown
word intricate (longest suffix match emphasized)
</tableCaption>
<table confidence="0.993478333333333">
VB VBP NN NNP RB JJ Others
Univ. prior (by token) .033 .015 .163 .115 .038 .075 .561
Univ. prior (by type) .030 .007 .174 .246 .000 .164 .389
</table>
<tableCaption confidence="0.9883995">
Table 2: Universal priors for 6 POS tags computed
over a 1-million-word annotated English corpus
</tableCaption>
<bodyText confidence="0.954963666666667">
On the other hand, Table 1 also illustrates two
problems with suffix-based estimation for part-of-
speech priors:
While a previously unseen word such as intri-
cate is primarily an adjective, the dominant part-
of-speech for the fixed-length suffix -ate is NN
(using token-weighted estimation), or VB (using
type-weighted estimation).
Modeling longer suffixes just makes things
worse in this case, as 14 out of 15 of the words end-
ing in -icate in the tagged corpus are exclusively
VB or VBP, and the two forms ending in -ricate
(extricate and fabricate) in the training text are
also exclusively tagged as VB. Suffixes clearly do
not capture all relevant information in predicting
tag probabilities for unknown words.
Linear Interpolation of Fixed-Length
Suffix Models
As a third baseline we considered an interpo-
lated suffix model to demonstrate the relative ef-
fectiveness of these approaches when restricted to
estimation by smoothed fixed length suffix mod-
els. We used the interpolation of 3 fixed-length
suffix priors:
</bodyText>
<equation confidence="0.99634">
P(tiw) = E3-1..3 A.7 • P(t 8143 (W))
</equation>
<bodyText confidence="0.9993195">
where suf3(w) denotes the suffix of length j of
word w.
</bodyText>
<subsectionHeader confidence="0.64061">
Variable-Length Suffix Models
</subsectionHeader>
<bodyText confidence="0.999846">
Given that the length of informative suffix con-
text varies considerably across suffixes, our fourth
and final baseline model uses a smoothed trie-
based architecture (similar to the one presented
in (Cucerzan and Yarowsky, 1999) for named en-
tity classification) to estimate
</bodyText>
<equation confidence="0.999549">
P(tiw) = E sufi (w))•
j&gt;1
(1)
</equation>
<bodyText confidence="0.9998863125">
In some cases, this variable-length suffix
method may have the opposite problem of fixed-
length method, over-training by giving too much
weight to the properties of the morphological form
most similar to the given word w encountered in
the training text (in the -ate example, the estima-
tion becomes worse as we considered longer and
longer suffixes). Still, our experiments show that
variable-length outperforms the fixed-length inter-
polation models.
However, many words with similar internal suf-
fixes are misleading indicators of the lexical pri-
ors for unseen words. Our goal, therefore, is to
borrow lexical probability estimates from a more
predictive set of previously seen exemplars.
The following sections propose such methods.
</bodyText>
<sectionHeader confidence="0.984817" genericHeader="method">
3 Similarity Measures
</sectionHeader>
<bodyText confidence="0.9998534">
Recall that the central task of lexical prior es-
timation is determining how much weight each
previously-seen exemplar&apos;s distribution should
contribute to an unknown word&apos;s distribution.
Rewriting formula (1) in the following equivalent
</bodyText>
<equation confidence="0.926363">
form: ---
P(tiw) = EP(tiv) • A&apos; (/cs(v, w)) (2)
</equation>
<bodyText confidence="0.9948498">
where /cs(., .) represents the longest common
suffix of two words, and A&apos; (sufk(w)) =
A(1, sufi (w)) + + A(k, sufk (w)), observe that
this is merely a special case of a more general rep-
resentation:
</bodyText>
<equation confidence="0.999173">
P(tiw) = itt Ep(tiv) • sim(w,v) (3)
</equation>
<bodyText confidence="0.9998002">
where sim(w,v) can be any weighting of potential
exemplars v for a target word w (a is a normal-
ization factor) .
But what should this similarity measure take
into account?
</bodyText>
<table confidence="0.916455411764706">
v known P(tiv)
sub (v) = sub (w)
0 Distributions at 0 position f(w, 0) -1 Erstributions over suffixes at 1st pos tion f(w, 1)
S(w, 0) E nt r ra s 1 other 8 S(w, 1) ais al ale aux ent e er era es ation el other 20
centre .713 .0002 .001 .0002 .284 centr .0001 .099 .167 .003 .0002 .521 .001 .0001 .208
structure .751 .001 .017 .229 .001 structur .023 .006 .004 .0008 .789 .014 .158 .002 .0008
montre .331 .122 .458 .033 .007 .049 montr .087 .238 .328 .024 .005 .318
-2 Distributions over suffixes at 2nd position f(w, 2)
S(w, 2) aine ime rais ral rale raux re rent rer rera res rel urion rons re res other 40
cent .012 .002 .0001 .072 .124 .003 .627 .0001 .001 .0001 .154 .003
structu .024 .006 .005 .793 .0008 .014 .156 .001
mont .079 .029 .109 .008 .002 - - .018 .058 .010 .654
POS estimate
as noun as verb
centre .99+ .01-
structure .99+ .01-
montre .01- .99+
</table>
<tableCaption confidence="0.998655">
Table 3 Suffix distribution for centre, structure, and montre as observed in a 12-million-word French corpus
</tableCaption>
<subsectionHeader confidence="0.999349">
3.1 Suffix-based Paradigmatic Distance
</subsectionHeader>
<bodyText confidence="0.997786646341464">
The primary intuition behind the following
paradigmatic distance measure is that words
which have similar probabilistic distributions of
added suffixes will also tend to have similar part-
of-speech tag distributions.
Consider the French words centre and structure,
which can be both singular nouns and 1P/3P-
singular-present verbs, with the noun usage signif-
icantly more common for both words. While their
internal suffixes differ at the 3rd position (-tre vs.
-ure), both words exhibit a very similar distribu-
tion of observed added suffixes (shown in Table 3).
Both are dominated by the noun-consistent signa-
tures E and +s, with a much smaller distribution
over the verb-consistent signatures +nt, + r and
+ra. In contrast, the word montre (almost exclu-
sively a 1P/3P-sing-present verb), while exhibit-
ing superficial internal suffix similarity to cen-
tre, exhibits a very different added suffix distri-
bution. The divergence is further illustrated by
considering added suffix distributions starting at
1-character and 2-character truncated forms of the
target words (e.g. structur and structu).
This distinction is important because centre
was never observed with a part-of-speech tag in
the selected 60k annotated text, and its tag dis-
tribution needs to be estimated as an unknown
word. Traditional internal suffix-based models
would base this estimate on the more orthograph-
ically similar montre, which is seen in the small
tagged corpus only as a verb, as well as other
orthographically similar words such as concentre
(seen as verb), contre (preposition), and rencon-
tre (encountered as both verb and noun), yielding
the misleading conclusion that centre is predom-
inantly a verb. In contrast, structure, which is
paradigmatically the most similar word present
in the small tagged corpus, occurs there exclu-
sively as a noun, and is thus a much better tag-
distribution exemplar for centre, which is also
overwhelmingly a noun.
Formally, let V be a vocabulary extracted from
an unannotated corpus C over a language L and
Suff the set of possible suffixes for that language,
extracted also from corpus C by considering all
the suffixes s for which there exists a certain num-
ber (dependent on the language and corpus con-
sidered) of distinct words w in V such that the
concatenations ws are also in the vocabulary. For
the studied languages we limit the length of suf-
fixes to 5 letters and we define the extended sets of
valid suffixes as Suff, = {xs I Ixi &lt; , xs &lt;5, 8 E
Suff, ]yi,..., yt distinct strings such that y3xs E V
for j E 1..T}, where T is a language and corpus
dependent threshold. Variations of this extensions
can be considered for languages with special inflec-
tional properties (such as umlant in German).
To build the suffix families S(w, i), we consider
all the vocabulary entries that can be obtained
from the word w by stripping the last i letters
and adding a valid suffix from Suffi:
S(w, i) = {8 E Suffil wiw2...wn_is E V}
The word break in front of the last i letters will
be called the ith position.
The distribution functions f (w,i) : S(w, i)
(0, 1] are obtained by counting the occurrences of
the vocabulary entries wi ...wn_is in C and nor-
malizing the counts.
The motivation for considering suffixation dis-
tributions from multiple word positions is that the
suffix families at the 0th position can often be
sparse and misleading, particularly for inflected
or rarely encountered words. For example, the
similar part-of-speech behavior for the English
achiever and retriever (Table 4) is not sufficiently
evident from the distributions at the 0th position
alone, due to the low frequency of the word form
achiever. Also, adjectives and nouns ending in -y
may have similar suffix families at the 0th position
{E} (e.g. creepy + {E} vs. philantropy + {E}), but
the suffix families at the 1st position capture dif-
ferent &amp;quot;nominal&amp;quot; and &amp;quot;adjectival&amp;quot; properties, mak-
</bodyText>
<figure confidence="0.974299555555556">
0
f(w,0)
S(w, 0)
.059
retriever
.941
.5
achiever
.5
</figure>
<table confidence="0.9542385">
4 f (W 11)
S (W , 1) E d r s rs able ment
retrieve .580 .351 .059 .004 .006
achieve .461 .391 .05 .003 .05 .0005 .135
-2 f(w,2)
S(w, 2) able al als e ed er ers es ing cable ement
retriev .002 .072 .003 .470 .284 .048 .003 .005 .112
achiev .006 .405 .342 .004 .004 .009 .115 .0005 .111
</table>
<tableCaption confidence="0.986583">
Table 4: Suffix distribution for retriever (32 occurrences) and achiever (2 occurrences.) in an 80-million-word
English corpus
</tableCaption>
<bodyText confidence="0.999536157894737">
ing the distinction between the two classes clean
and visible (e.g. creep + , jest, ily, ing, s, y} vs.
philantrop + {kat, ies, ist, ists, y}, as observed in
the considered untagged corpus). It is thus more
robust to also include suffixes distributions over
several truncated forms as well.
It was determined experimentally that the dis-
tributions at positions greater than 3 and the ones
obtained for words shorter than 4 letters are not
useful. This does not represent a major problem
because unknown words tend to have long forms
in most languages.
Various distance measures (cosine similarity,
Euclidean distance, L1 norm) and interpolation
methods were used in our experiments to deter-
mine the most suitable formula for the paradig-
matic distance.
The best scores were obtained for L1 norm us-
ing a weighted product combination dist(w, v) =
</bodyText>
<equation confidence="0.684179">
u/(10,1v1)(0i
ii=o + dist(w, v, i)) and a Jaccard-type
</equation>
<bodyText confidence="0.9768454">
(Salton and McGill, 1983) alteration to penalize
the cases in which major differences in the under-
lying suffix families (not only in the distributions)
are found:
dist(w, v , i)
</bodyText>
<equation confidence="0.995020333333333">
ses(w,ons(v,i)
8(w, v,i) ipw,i;8)-pv,i;8)1
SES(w,z) (S(v,z)
</equation>
<bodyText confidence="0.999983727272727">
Based on the paradigmatic distance computed
in this way, it is possible to filter out the words
with similar endings but occurring with different
suffix families and distributions. Furthermore,
this filter has the advantage of being trained on
completely untagged corpora, a potentially unlim-
ited resource.
Should a word not appear even in the large raw
text corpus, some smoothing technique based only
on suffix similarity would still be needed (such as
fixed or variable length suffix interpolation).
</bodyText>
<subsectionHeader confidence="0.999687">
3.2 Contextual Similarity
</subsectionHeader>
<bodyText confidence="0.972746692307692">
As a complement to the suffix-based paradigmatic
distance proposed in this paper, a word-context-
based similarity measure has been shown to be
useful for tagging unknown words. Brill (1995)
utilized word context neighborhoods to model and
predict tags for unknown words. Schfitze (1993)
explicitly formulated the concept of paradigmatic
inaccurate 0 0 0 0 0 6 0.040 0.016
inadequate 0 0 0 0 0 4 0.040 0.062
delicate 0 0 0 0 0 7 0.154 0.141
surrogate 0 0 1 0 0 1 2.258 0**
moderate 1 1 0 0 0 39 2.849 0.015
private 0 0 0 5 0 8 2.957 0.085
</bodyText>
<table confidence="0.958084083333333">
calculate 3 5 0 0 0 0 9.118 0.029
extricate 2 0 0 0 0 0 23.272 0.0006
concentrate 25 2 4 0 0 0 26.694 0.017
fabricate 3 0 0 0 0 0 34.097 0.0004
rate 1 2 575 0 0 0 107.809 0.075
late 0 0 0 1 12 6 114.420 0.294
hate 5 7 1 0 0 0 122.503 0.039
intricate 7 7 7 7 7 7 *computed from lines 1-3
lines 4-13
comparison
Paradigmatic 0 0 0 0 0 1 **values on
distribution* shown for
</table>
<tableCaption confidence="0.7610614">
Table 5: The words from Table 1 are ordered by
suffix-paradigmatic distance with respect to the tar-
get word intricate. Both the paradigmatic and contex-
tual measures were computed from an 80-million-word
unannotated corpus.
</tableCaption>
<bodyText confidence="0.999869">
similarity over nearby word contexts, using this in
an SVD framework for part-of-speech tagging.
We also utilized this relatively orthogonal infor-
mation source as a complement to the proposed
suffix-based paradigmatic distance. We chose un-
igram vectors to model left and right neighbor-
hoods, and used cosine similarity for its robust-
ness. Because cosine similarity over numerous
large-vocabulary contexts can be very expensive
to compute, we only incorporated this measure
when the suffix-based paradigmatic distance mea-
sure was within a certain threshold of viability.
</bodyText>
<subsectionHeader confidence="0.999807">
3.3 Using the Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999971777777778">
Table 5 illustrates the application of both the
suffixed-based paradigmatic distance and contex-
tual similarity measures to predicting the lexical
prior distribution for the previously unseen En-
glish word intricate. The potential exemplar can-
didates, such as shown in Table 1, are ordered by
the paradigmatic distance measure, filtered by the
more expensive and less effective context similar-
ity scores as noted above.
</bodyText>
<figure confidence="0.99095425">
Tag Prior Distribution
NNP
NN
VB
VBP
RB
JJ Paradigm.
Distance
sim(intricate, v)
Contextual
Similarity
E if(ll,i;8)-pv,i;8)1+
</figure>
<bodyText confidence="0.999921909090909">
We investigated several weighting functions for
computing the consensus distribution from this
space. While using just the single closest ex-
emplar&apos;s distribution performed surprisingly well,
the best performance was obtained by a uni-
form weighting of the distributions from exem-
plars within an experimentally determined dis-
tance threshold. Ongoing work is considering
word length and word frequency similarity as fur-
ther potential components of this weighting func-
tion.
</bodyText>
<sectionHeader confidence="0.979211" genericHeader="method">
4 Embedding Algorithm
</sectionHeader>
<bodyText confidence="0.9999736">
Since we obtain a tag probability distribution for
any unknown word, it is quite straightforward to
use this distribution in the context of any prob-
abilistic tagger, including the standard HMM n-
gram taggers. In this study, we use bigrams as the
base model, since we are dealing with a relatively
limited training data.
We contrasted four search algorithms: (a) a
classical beam-1 search (Beam 1); (b) a (taglleft-
history,right-history) combination of forward and
backward beam-1 searches (L-R beam 1), varia-
tion suggested by the high complementary rates
as defined in (Brill and Wu, 1998) - values in the
20-40% range; (c) a full Viterbi search; (d) an
adjusted variation of the latter that uses (taglleft-
tag,right-tag) trigrams for a correction pass (L-R
Viterbi).
It should be noted that our method of estimat-
ing lexical tag priors can be used in other tagging
paradigms, such as a maximum entropy tagger
(Ratnaparkhi, 1996), as well as non-probabilistic
taggers, such as the Brill&apos;s rule-based tagger (Brill,
1995), by initializing the tagger with a tag candi-
date set for every unknown word based on the
lexical prior estimates.
</bodyText>
<sectionHeader confidence="0.991278" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999932">
We have tested the new methods on two lan-
guages, French and English, using only small
amounts of annotated text for training (60k max.
for French, 200k max. for English) and relatively
large unannotated corpora (on the order of tens
of million words) for computing the paradigmatic
distance and contextual similarity. All parameters
of the embedding methods were estimated based
on a French development set and used unmodi-
fied for English, further emphasizing the relatively
language independent usage of the algorithm but
also partially explaining the lower boost on per-
formance on English.
Table 6 presents the results obtained by the
different methods for handling unknown words
into the L-R taggers. The Paradigmatic-1 row
represents the variation in which only the first
paradigmatically similar word found is used, while
Paradigmatic-n denotes the combination of up to
n most similar words as estimators. As men-
tioned previously, such words may not always
be found, therefore the suffix-based smoothing
scheme is used for back-off in these cases. The
results were obtained on a test set of 18k to-
kens from the French side of the Hansards using
two different training-set sizes, 15k tokens (av-
erage 00V ratio 17.3%) and 60k tokens (00V
ratio 8.9%), and an unannotated text of 12 mil-
lion words from the same corpus. The first
four rows present re-implementations of standard
methods; the boldface-typed methods use the
new paradigmatic distance proposed here (Section
3.1). VLS method uses a probabilistic trie-suffix
model.
Table 7 summarizes the consistent improve-
ment achieved by the addition of the suffix-
paradigmatic and contextual models to various
bigram taggers. The results obtained for Brill&apos;s
algorithm, trained using the same data (15k/60k
words annotated corpora, 12 million words unan-
notated corpus), are also presented, in conjunc-
tion with the improvement in accuracy gained by
the same algorithm when every unknown word in
the test sets is replaced with the paradigmatically
most similar known word from the training sets.
Table 8 presents the results obtained for English
on a contiguously selected test set from the WSJ
corpus, using contiguous training sets from differ-
ent regions of the same corpus. Numbers and cap-
italization variance were not treated as unknown
words in evaluation given their ease of POS predic-
tion. These results also show good improvement
relative to the baseline performance for the same
embedding algorithms.
Using our proposed method for predicting the
tag distributions for previously unseen words con-
sistently improves the results for a wide range of
training set sizes as well, as illustrated here in
Figures 3 and 4 using 2 different embedding algo-
rithms on French data. The one exception to this
trend is observed for only the smallest training set
size of 2k words for the L-R Viterbi tagger. In this
particular case, the space in which paradigmati-
cally similar words have to be searched is very lim-
ited and trie interpolation method used as back-off
in the case such words are not found gives exces-
sive weight to tiny available sets of tagged exem-
plars, a problem that could be addressed through
more conservative trie smoothing techniques.
</bodyText>
<figure confidence="0.99088554">
8k 15k 30k 60k
2k 4k
2k 4k 8k
15k 30k
60k
Accuracy
98%
96%
94%
92%
90%
78%
88%
86%
84%
82%
80%
86.62%
84.23%
80.76%
83.63%
Full Model (Paradigm + Context + VLS)
Interpolated Suffix
Universal Prior
82%
Full Model (Paradigm + Context +VLS)
Interpolated Suffix
Universal Prior
81.02%
98%
96.96%
80%
78.92%
78%
77.39%
97.31%
96.31%
95.83%
96%
94.42%
93.76%
94%
92%
Accuracy
90%
88%
86%
88.61%
88.17%
84%
</figure>
<table confidence="0.9730046">
Training Set Size (number of tokens)
Training Set Size (number of tokens)
Language: ENGLISH
Evaluation Type Full Performance Accuracy on 00V tokens
Bigram-based Algorithm: L-R beam 1 L-R Viterbi L-R beam 1 L-R Viterbi
Lexical Prior Model 50k 200k 50k 200k 50k 200k 50k 200k
Universal Prior 85.99 91.22 88.81 92.71 25.06 31.10 39.49 43.03
Interp. Suffix + Cap. 91.35 93.43 93.00 94.73 74.26 75.09 77.39 79.17
Trie VLS 91.26 93.55 92.97 94.98 74.03 75.69 77.17 79.53
Full Model 91.58 94.04 93.31 95.36 74.98 77.20 78.09 80.89
</table>
<tableCaption confidence="0.999823">
Table 8: Performance of 4 lexical prior estimation methods on reduced size sets from WSJ Corpus
</tableCaption>
<bodyText confidence="0.999950464285714">
rate reduction in full Viterbi tagger performance
for French over an interpolated-suffix model base-
line, and 12% error rate reduction for equivalent
full tagger performance on English. When com-
pared with a state-of-the-art model for hierarchi-
cally smoothed variable-length suffix tries, the ad-
dition of the paradigmatic and contextual distance
measures achieves a 7.8% error rate reduction for
French and 7.6% error reduction on English. Per-
formance shows a consistent improvement across
4 different embedding tagging algorithms.
Further studies are in progress to compare
the usefulness of these techniques on low-count
(rather than unseen) words, and also to extend
this work to Romanian, Czech and Slovenian, as
further examples of highly inflected languages.
Evidence from shifting applications from French
to English indicates that respectable performance
can be obtained without even the re-estimation
of parameters on new languages, although we do
expect that some parameter re-optimization could
prove useful. We believe that this approach should
show the greatest benefits for taggers designed
for highly inflective languages, such as (Ha*. and
Hladka,, 1998) and (Erjavec et al., 1999), given
that the associational power and potential for
the proposed paradigmatic similarity measure are
most compelling for such languages.
</bodyText>
<sectionHeader confidence="0.998106" genericHeader="conclusions">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999810333333333">
The authors would like to thank Jan Haji . for his
extremely valuable suggestions and feedback on
this work.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818690909091">
S. Abney, R.E. Schapire, and Y. Singer. 1999. Boost-
ing applied to tagging and PP attachment. Pro-
ceedings of EMNLP/VLC 1999, pages 38-45.
T. Brants. 2000. TnT - a statistical part-of-speech
tagger. Proceedings of ANLP 2000, pages 224-231.
E. Brill and J. Wu. 1998. Classifier combination
for improved lexical disambiguation. Proceeding of
COLING-ACL 1998, pages 191-195.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4), pages 543-565.
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for part-of-speech
tagging. Proceedings of the 11th National Confer-
ence on Artificial Intelligence 1993, pages 784-789.
K. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. Proceed-
ings of the 2nd Conference on Applied Natural Lan-
guage Processing 1988, pages 136-143
S. Cucerzan and D. Yarowsky. 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of
EMNLP/VLC 1999, pages 90-99.
C. G. de Marcken 1990 Parsing the LOB corpus.
Proceedings of ACL 1990, pages 243-251.
T. Erjavec, S. Dzeroski, and J. Zavrel. 1999. Mor-
phosyntactic tagging of slovene: Evaluating POS
taggers and tagsets. Technical report, Dept. of In-
telligent Systems, Jozef Stefan Institute, Ljubljana.
J. Haji 6 and B. Hladka. 1998. Tagging inflective lan-
guages: Prediction of morphological categories for
a rich, structured tagset. Proceeding of COLING-
ACL 1998, pages 483-490.
J. Haji. 2000. Morphological tagging: data vs. dic-
tionaries. Proceedings of NAACL 2000, pages 94-
101.
A. Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. Proceedings of EMNLP
1996, pages 133-142.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
C. Samuelsson. 1993. Morphological tagging based
entirely on Bayesian inference. 9th Nordic Confer-
ence on Computational Liguistics 1993.
H. Schfitze. 1993. Part-of-speech induction from
scratch. Proceedings of ACL 1993, pages 251-258.
S.M. Thede. 1998. Predicting part-of-speech in-
formation about unknown words using statistical
methods. Proceeding of COLING-ACL 1998, pages
1505-1507.
R. Weischedel, M. Meeter, R. Schwartz, L. Ramshaw,
and J. Palmucci. 1993. Coping with ambiguity
and unknown words through probabilistic models.
Computational Linguistics, 19(3), pages 359-382.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680142">
<title confidence="0.9980605">Language Independent, Minimally Supervised Induction of Lexical Probabilities</title>
<author confidence="0.999411">Silviu Cucerzan</author>
<author confidence="0.999411">David Yarowsky</author>
<affiliation confidence="0.998467333333333">Department of Computer Science Center for Language and Speech Processing Johns Hopkins University</affiliation>
<address confidence="0.999828">Baltimore, MD 21218</address>
<email confidence="0.99376">Isilviu,yarowskylOcs.jhu.edu</email>
<abstract confidence="0.983093842105263">A central problem in part-of-speech tagging, especially for new languages for which limited annotated resources are available, is estimating the distribution of lexical probabilities for unknown words. This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boosting applied to tagging and PP attachment.</title>
<date>1999</date>
<booktitle>Proceedings of EMNLP/VLC</booktitle>
<pages>38--45</pages>
<contexts>
<context position="1142" citStr="Abney et al., 1999" startWordPosition="154" endWordPosition="157">measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English. 1 Introduction Part-of-Speech tagging of English has reached a level which seems to resist any improvement. Methods like Transformation-based tagging (Brill, 1995), MaxEnt (Ratnaparkhi, 1996), Boosting (Abney et al., 1999), TnT/Markov models (Brants, 2000) achieve accuracies comparable with human performance for this task. However, if we break the results into two parts, for known and unknown words, we can see that the performance of English taggers is much lower on the latter. The situation is even worse for languages other than English, especially inflective languages, for two reasons: first, there is usually less annotated data available and second, the coverage of such data is much lower due to the high number of different word-forms in these languages (for comparison of properties and tagging results for s</context>
</contexts>
<marker>Abney, Schapire, Singer, 1999</marker>
<rawString>S. Abney, R.E. Schapire, and Y. Singer. 1999. Boosting applied to tagging and PP attachment. Proceedings of EMNLP/VLC 1999, pages 38-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>Proceedings of ANLP</booktitle>
<pages>224--231</pages>
<contexts>
<context position="1176" citStr="Brants, 2000" startWordPosition="161" endWordPosition="162">sed learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English. 1 Introduction Part-of-Speech tagging of English has reached a level which seems to resist any improvement. Methods like Transformation-based tagging (Brill, 1995), MaxEnt (Ratnaparkhi, 1996), Boosting (Abney et al., 1999), TnT/Markov models (Brants, 2000) achieve accuracies comparable with human performance for this task. However, if we break the results into two parts, for known and unknown words, we can see that the performance of English taggers is much lower on the latter. The situation is even worse for languages other than English, especially inflective languages, for two reasons: first, there is usually less annotated data available and second, the coverage of such data is much lower due to the high number of different word-forms in these languages (for comparison of properties and tagging results for several such languages see (Haji, 2</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT - a statistical part-of-speech tagger. Proceedings of ANLP 2000, pages 224-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation. Proceeding of COLING-ACL</title>
<date>1998</date>
<pages>191--195</pages>
<contexts>
<context position="18308" citStr="Brill and Wu, 1998" startWordPosition="3072" endWordPosition="3075">function. 4 Embedding Algorithm Since we obtain a tag probability distribution for any unknown word, it is quite straightforward to use this distribution in the context of any probabilistic tagger, including the standard HMM ngram taggers. In this study, we use bigrams as the base model, since we are dealing with a relatively limited training data. We contrasted four search algorithms: (a) a classical beam-1 search (Beam 1); (b) a (tagllefthistory,right-history) combination of forward and backward beam-1 searches (L-R beam 1), variation suggested by the high complementary rates as defined in (Brill and Wu, 1998) - values in the 20-40% range; (c) a full Viterbi search; (d) an adjusted variation of the latter that uses (tagllefttag,right-tag) trigrams for a correction pass (L-R Viterbi). It should be noted that our method of estimating lexical tag priors can be used in other tagging paradigms, such as a maximum entropy tagger (Ratnaparkhi, 1996), as well as non-probabilistic taggers, such as the Brill&apos;s rule-based tagger (Brill, 1995), by initializing the tagger with a tag candidate set for every unknown word based on the lexical prior estimates. 5 Evaluation We have tested the new methods on two langu</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. Proceeding of COLING-ACL 1998, pages 191-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="1083" citStr="Brill, 1995" startWordPosition="148" endWordPosition="149">This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English. 1 Introduction Part-of-Speech tagging of English has reached a level which seems to resist any improvement. Methods like Transformation-based tagging (Brill, 1995), MaxEnt (Ratnaparkhi, 1996), Boosting (Abney et al., 1999), TnT/Markov models (Brants, 2000) achieve accuracies comparable with human performance for this task. However, if we break the results into two parts, for known and unknown words, we can see that the performance of English taggers is much lower on the latter. The situation is even worse for languages other than English, especially inflective languages, for two reasons: first, there is usually less annotated data available and second, the coverage of such data is much lower due to the high number of different word-forms in these langua</context>
<context position="15140" citStr="Brill (1995)" startWordPosition="2542" endWordPosition="2543">lar endings but occurring with different suffix families and distributions. Furthermore, this filter has the advantage of being trained on completely untagged corpora, a potentially unlimited resource. Should a word not appear even in the large raw text corpus, some smoothing technique based only on suffix similarity would still be needed (such as fixed or variable length suffix interpolation). 3.2 Contextual Similarity As a complement to the suffix-based paradigmatic distance proposed in this paper, a word-contextbased similarity measure has been shown to be useful for tagging unknown words. Brill (1995) utilized word context neighborhoods to model and predict tags for unknown words. Schfitze (1993) explicitly formulated the concept of paradigmatic inaccurate 0 0 0 0 0 6 0.040 0.016 inadequate 0 0 0 0 0 4 0.040 0.062 delicate 0 0 0 0 0 7 0.154 0.141 surrogate 0 0 1 0 0 1 2.258 0** moderate 1 1 0 0 0 39 2.849 0.015 private 0 0 0 5 0 8 2.957 0.085 calculate 3 5 0 0 0 0 9.118 0.029 extricate 2 0 0 0 0 0 23.272 0.0006 concentrate 25 2 4 0 0 0 26.694 0.017 fabricate 3 0 0 0 0 0 34.097 0.0004 rate 1 2 575 0 0 0 107.809 0.075 late 0 0 0 1 12 6 114.420 0.294 hate 5 7 1 0 0 0 122.503 0.039 intricate 7</context>
<context position="18737" citStr="Brill, 1995" startWordPosition="3144" endWordPosition="3145">tagllefthistory,right-history) combination of forward and backward beam-1 searches (L-R beam 1), variation suggested by the high complementary rates as defined in (Brill and Wu, 1998) - values in the 20-40% range; (c) a full Viterbi search; (d) an adjusted variation of the latter that uses (tagllefttag,right-tag) trigrams for a correction pass (L-R Viterbi). It should be noted that our method of estimating lexical tag priors can be used in other tagging paradigms, such as a maximum entropy tagger (Ratnaparkhi, 1996), as well as non-probabilistic taggers, such as the Brill&apos;s rule-based tagger (Brill, 1995), by initializing the tagger with a tag candidate set for every unknown word based on the lexical prior estimates. 5 Evaluation We have tested the new methods on two languages, French and English, using only small amounts of annotated text for training (60k max. for French, 200k max. for English) and relatively large unannotated corpora (on the order of tens of million words) for computing the paradigmatic distance and contextual similarity. All parameters of the embedding methods were estimated based on a French development set and used unmodified for English, further emphasizing the relative</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4), pages 543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowitz</author>
</authors>
<title>Equations for part-of-speech tagging.</title>
<date>1993</date>
<booktitle>Proceedings of the 11th National Conference on Artificial Intelligence</booktitle>
<pages>784--789</pages>
<contexts>
<context position="3463" citStr="Charniak et al., 1993" startWordPosition="527" endWordPosition="530">and Previous Work In this paper, we shall use the terms lexical prior or tag prior for a given word to refer to the probability P(t1w) of Part-of-Speech (POS) tags for word w independent of context, as distinct from what we call the posterior distribution P(ticontext;w), and also distinct from the concept of channel model prior P (T), which refers to the prior probability of a tag sequence T from a generating source. To facilitate clear exposition, we use here the &amp;quot;direct&amp;quot; lexical probability of tag given word P (t I w) , but corresponding arguments hold for the classical HMM Bayesian method (Charniak et al., 1993) used by the taggers we considered for evaluation purpose of the present work. 2.1 Training Data Characteristics with Respect To Unknown Words Previously unseen (or &amp;quot;unknown&amp;quot;) words often represent a significant portion of the vocabulary, 90% 86.24% 80% 70% types OOV ratio 60% 50% 40% 35.74% 30% 32.99% tokens 20% 8.89% 2k 4k 8k 15k 30k 60k Training Set Size (number of tokens) % of OOV words not found in the raw corpus 22% 20% 20.36% 18% 18.34% types 16% 14% tokens 12% 3 million 6 million 12 million Untagged Corpus Size (number of tokens) 10% 5% 12.95% 12.18% that the observed lexical prior for</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for part-of-speech tagging. Proceedings of the 11th National Conference on Artificial Intelligence 1993, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>Proceedings of the 2nd Conference on Applied Natural Language Processing</booktitle>
<pages>136--143</pages>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proceedings of the 2nd Conference on Applied Natural Language Processing 1988, pages 136-143</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>Proceedings of EMNLP/VLC</booktitle>
<pages>90--99</pages>
<contexts>
<context position="6412" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="1070" endWordPosition="1073">xed-Length Suffix Models As a third baseline we considered an interpolated suffix model to demonstrate the relative effectiveness of these approaches when restricted to estimation by smoothed fixed length suffix models. We used the interpolation of 3 fixed-length suffix priors: P(tiw) = E3-1..3 A.7 • P(t 8143 (W)) where suf3(w) denotes the suffix of length j of word w. Variable-Length Suffix Models Given that the length of informative suffix context varies considerably across suffixes, our fourth and final baseline model uses a smoothed triebased architecture (similar to the one presented in (Cucerzan and Yarowsky, 1999) for named entity classification) to estimate P(tiw) = E sufi (w))• j&gt;1 (1) In some cases, this variable-length suffix method may have the opposite problem of fixedlength method, over-training by giving too much weight to the properties of the morphological form most similar to the given word w encountered in the training text (in the -ate example, the estimation becomes worse as we considered longer and longer suffixes). Still, our experiments show that variable-length outperforms the fixed-length interpolation models. However, many words with similar internal suffixes are misleading indicato</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. Proceedings of EMNLP/VLC 1999, pages 90-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G</author>
</authors>
<title>de Marcken</title>
<date>1990</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>243--251</pages>
<marker>G, 1990</marker>
<rawString>C. G. de Marcken 1990 Parsing the LOB corpus. Proceedings of ACL 1990, pages 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Erjavec</author>
<author>S Dzeroski</author>
<author>J Zavrel</author>
</authors>
<title>Morphosyntactic tagging of slovene: Evaluating POS taggers and tagsets.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Dept. of Intelligent Systems, Jozef Stefan Institute,</institution>
<location>Ljubljana.</location>
<marker>Erjavec, Dzeroski, Zavrel, 1999</marker>
<rawString>T. Erjavec, S. Dzeroski, and J. Zavrel. 1999. Morphosyntactic tagging of slovene: Evaluating POS taggers and tagsets. Technical report, Dept. of Intelligent Systems, Jozef Stefan Institute, Ljubljana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hladka</author>
</authors>
<title>Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. Proceeding of COLINGACL</title>
<date>1998</date>
<pages>483--490</pages>
<marker>Hladka, 1998</marker>
<rawString>J. Haji 6 and B. Hladka. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. Proceeding of COLINGACL 1998, pages 483-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Haji</author>
</authors>
<title>Morphological tagging: data vs. dictionaries.</title>
<date>2000</date>
<booktitle>Proceedings of NAACL</booktitle>
<pages>94--101</pages>
<contexts>
<context position="1780" citStr="Haji, 2000" startWordPosition="260" endWordPosition="261">, 2000) achieve accuracies comparable with human performance for this task. However, if we break the results into two parts, for known and unknown words, we can see that the performance of English taggers is much lower on the latter. The situation is even worse for languages other than English, especially inflective languages, for two reasons: first, there is usually less annotated data available and second, the coverage of such data is much lower due to the high number of different word-forms in these languages (for comparison of properties and tagging results for several such languages see (Haji, 2000)). Moreover, many of the words not found in the (small) training data are in fact inflected forms of quite common words. In the work described herein we therefore concentrate on the problem of unknown words in the context of probabilistic tagging. Although the annotated resources are limited or even non-existent for most languages, the raw text available online is effectively unlimited with respect to the need of most NLP applications. This paper presents a newly developed paradigmatic similarity measure that tries to maximize the benefits that can be obtained from limited annotated resources </context>
</contexts>
<marker>Haji, 2000</marker>
<rawString>J. Haji. 2000. Morphological tagging: data vs. dictionaries. Proceedings of NAACL 2000, pages 94-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1111" citStr="Ratnaparkhi, 1996" startWordPosition="151" endWordPosition="152">a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English. 1 Introduction Part-of-Speech tagging of English has reached a level which seems to resist any improvement. Methods like Transformation-based tagging (Brill, 1995), MaxEnt (Ratnaparkhi, 1996), Boosting (Abney et al., 1999), TnT/Markov models (Brants, 2000) achieve accuracies comparable with human performance for this task. However, if we break the results into two parts, for known and unknown words, we can see that the performance of English taggers is much lower on the latter. The situation is even worse for languages other than English, especially inflective languages, for two reasons: first, there is usually less annotated data available and second, the coverage of such data is much lower due to the high number of different word-forms in these languages (for comparison of prope</context>
<context position="18646" citStr="Ratnaparkhi, 1996" startWordPosition="3131" endWordPosition="3132">ining data. We contrasted four search algorithms: (a) a classical beam-1 search (Beam 1); (b) a (tagllefthistory,right-history) combination of forward and backward beam-1 searches (L-R beam 1), variation suggested by the high complementary rates as defined in (Brill and Wu, 1998) - values in the 20-40% range; (c) a full Viterbi search; (d) an adjusted variation of the latter that uses (tagllefttag,right-tag) trigrams for a correction pass (L-R Viterbi). It should be noted that our method of estimating lexical tag priors can be used in other tagging paradigms, such as a maximum entropy tagger (Ratnaparkhi, 1996), as well as non-probabilistic taggers, such as the Brill&apos;s rule-based tagger (Brill, 1995), by initializing the tagger with a tag candidate set for every unknown word based on the lexical prior estimates. 5 Evaluation We have tested the new methods on two languages, French and English, using only small amounts of annotated text for training (60k max. for French, 200k max. for English) and relatively large unannotated corpora (on the order of tens of million words) for computing the paradigmatic distance and contextual similarity. All parameters of the embedding methods were estimated based on</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. Proceedings of EMNLP 1996, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="14210" citStr="Salton and McGill, 1983" startWordPosition="2398" endWordPosition="2401">etermined experimentally that the distributions at positions greater than 3 and the ones obtained for words shorter than 4 letters are not useful. This does not represent a major problem because unknown words tend to have long forms in most languages. Various distance measures (cosine similarity, Euclidean distance, L1 norm) and interpolation methods were used in our experiments to determine the most suitable formula for the paradigmatic distance. The best scores were obtained for L1 norm using a weighted product combination dist(w, v) = u/(10,1v1)(0i ii=o + dist(w, v, i)) and a Jaccard-type (Salton and McGill, 1983) alteration to penalize the cases in which major differences in the underlying suffix families (not only in the distributions) are found: dist(w, v , i) ses(w,ons(v,i) 8(w, v,i) ipw,i;8)-pv,i;8)1 SES(w,z) (S(v,z) Based on the paradigmatic distance computed in this way, it is possible to filter out the words with similar endings but occurring with different suffix families and distributions. Furthermore, this filter has the advantage of being trained on completely untagged corpora, a potentially unlimited resource. Should a word not appear even in the large raw text corpus, some smoothing techn</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Morphological tagging based entirely on Bayesian inference.</title>
<date>1993</date>
<booktitle>9th Nordic Conference on Computational Liguistics</booktitle>
<marker>Samuelsson, 1993</marker>
<rawString>C. Samuelsson. 1993. Morphological tagging based entirely on Bayesian inference. 9th Nordic Conference on Computational Liguistics 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schfitze</author>
</authors>
<title>Part-of-speech induction from scratch.</title>
<date>1993</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>251--258</pages>
<contexts>
<context position="15237" citStr="Schfitze (1993)" startWordPosition="2556" endWordPosition="2557">filter has the advantage of being trained on completely untagged corpora, a potentially unlimited resource. Should a word not appear even in the large raw text corpus, some smoothing technique based only on suffix similarity would still be needed (such as fixed or variable length suffix interpolation). 3.2 Contextual Similarity As a complement to the suffix-based paradigmatic distance proposed in this paper, a word-contextbased similarity measure has been shown to be useful for tagging unknown words. Brill (1995) utilized word context neighborhoods to model and predict tags for unknown words. Schfitze (1993) explicitly formulated the concept of paradigmatic inaccurate 0 0 0 0 0 6 0.040 0.016 inadequate 0 0 0 0 0 4 0.040 0.062 delicate 0 0 0 0 0 7 0.154 0.141 surrogate 0 0 1 0 0 1 2.258 0** moderate 1 1 0 0 0 39 2.849 0.015 private 0 0 0 5 0 8 2.957 0.085 calculate 3 5 0 0 0 0 9.118 0.029 extricate 2 0 0 0 0 0 23.272 0.0006 concentrate 25 2 4 0 0 0 26.694 0.017 fabricate 3 0 0 0 0 0 34.097 0.0004 rate 1 2 575 0 0 0 107.809 0.075 late 0 0 0 1 12 6 114.420 0.294 hate 5 7 1 0 0 0 122.503 0.039 intricate 7 7 7 7 7 7 *computed from lines 1-3 lines 4-13 comparison Paradigmatic 0 0 0 0 0 1 **values on di</context>
</contexts>
<marker>Schfitze, 1993</marker>
<rawString>H. Schfitze. 1993. Part-of-speech induction from scratch. Proceedings of ACL 1993, pages 251-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Thede</author>
</authors>
<title>Predicting part-of-speech information about unknown words using statistical methods. Proceeding of COLING-ACL</title>
<date>1998</date>
<pages>1505--1507</pages>
<marker>Thede, 1998</marker>
<rawString>S.M. Thede. 1998. Predicting part-of-speech information about unknown words using statistical methods. Proceeding of COLING-ACL 1998, pages 1505-1507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meeter</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
<author>J Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>359--382</pages>
<marker>Weischedel, Meeter, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>R. Weischedel, M. Meeter, R. Schwartz, L. Ramshaw, and J. Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(3), pages 359-382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>