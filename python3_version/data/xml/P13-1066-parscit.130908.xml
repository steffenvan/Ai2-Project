<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.986976">
Discovering User Interactions in Ideological Discussions
</title>
<author confidence="0.998673">
Arjun Mukherjee Bing Liu
</author>
<affiliation confidence="0.99056">
Department of Computer Science
University of Illinois at Chicago
</affiliation>
<email confidence="0.969668">
arjun4787@gmail.com liub@cs.uic.edu
</email>
<sectionHeader confidence="0.993062" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999685740740741">
Online discussion forums are a popular
platform for people to voice their opinions on
any subject matter and to discuss or debate
any issue of interest. In forums where users
discuss social, political, or religious issues,
there are often heated debates among users or
participants. Existing research has studied
mining of user stances or camps on certain
issues, opposing perspectives, and contention
points. In this paper, we focus on identifying
the nature of interactions among user pairs.
The central questions are: How does each
pair of users interact with each other? Does
the pair of users mostly agree or disagree?
What is the lexicon that people often use to
express agreement and disagreement? We
present a topic model based approach to
answer these questions. Since agreement and
disagreement expressions are usually multi-
word phrases, we propose to employ a
ranking method to identify highly relevant
phrases prior to topic modeling. After
modeling, we use the modeling results to
classify the nature of interaction of each user
pair. Our evaluation results using real-life
discussion/debate posts demonstrate the
effectiveness of the proposed techniques.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960419354838">
Online discussion/debate forums allow people
with common interests to freely ask and answer
questions, to express their views and opinions on
any subject matter, and to discuss issues of
common interest. A large part of such
discussions is about social, political, and
religious issues. On such issues, there are often
heated discussions/debates, i.e., people agree or
disagree and argue with one another. Such
ideological discussions on a myriad of social and
political issues have practical implications in the
fields of communication and political science as
they give social scientists an opportunity to study
real-life discussions/debates of almost any issue
and analyze participant behaviors in a large scale.
In this paper, we present such an application,
which aims to perform fine-grained analysis of
user-interactions in online discussions.
There have been some related works that focus
on discovering the general topics and ideological
perspectives in online discussions (Ahmed and
Xing, 2010), placing users in support/oppose
camps (Agarwal et al., 2003), and classifying
user stances (Somasundaran and Wiebe, 2009).
However, these works are at a rather coarser
level and have not considered more fine-grained
characteristics of debates/discussions where users
interact with each other by quoting/replying each
other to express agreement or disagreement and
argue with one another. In this work, we want to
mine the following information:
</bodyText>
<listItem confidence="0.918458666666667">
1. The nature of interaction of each pair of users
or participants who have engaged in the
discussion of certain issues, i.e., whether the
two persons mostly agree or disagree with
each other in their interactions.
2. What language expressions are often used to
express agreement (e.g., “I agree” and “you’re
right”) and disagreement (e.g., “I disagree”
and “you speak nonsense”).
</listItem>
<bodyText confidence="0.999940947368421">
We note that although agreement and
disagreement expressions are distinct from
traditional sentiment expressions (words and
phrases) such as good, excellent, bad, and
horrible, agreement and disagreement clearly
express a kind of sentiment as well. They are
usually emitted during interactive exchanges of
arguments in ideological discussions. This idea
prompted us to introduce the concept of AD-
sentiment. We define the polarity of agreement
expressions as positive and the polarity of
disagreement expressions as negative. We refer
agreement and disagreement expressions as AD-
sentiment expressions, or AD-expressions for
short. AD-expressions are crucial for the analysis
of interactive discussions and debates just as
sentiment expressions are instrumental in
sentiment analysis (Liu, 2012). We thus regard
this work as an extension to traditional sentiment
</bodyText>
<page confidence="0.981128">
671
</page>
<note confidence="0.930329">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.990186727272727">
analysis (Pang and Lee, 2008; Liu, 2012).
In our earlier work (Mukherjee and Liu,
2012a), we proposed three topic models to mine
contention points, which also extract AD-
expressions. In this paper, we further improve the
work by coupling an information retrieval
method to rank good candidate phrases with topic
modeling in order to discover more accurate AD-
expressions. Furthermore, we apply the resulting
AD-expressions to the new task of classifying the
arguing or interaction nature of each pair of
users. Using discovered AD-expressions for
classification has an important advantage over
traditional classification because they are domain
independent. We employ a semi-supervised
generative model called JTE-P to jointly model
AD-expressions, pair interactions, and discussion
topics simultaneously in a single framework.
With such complex interactions mined, we can
produce many useful summaries of discussions.
For example, we can discover the most
contentious pairs for each topic and ideological
camps of participants, i.e., people who often
agree with each other are likely to belong to the
same camp. The proposed framework also
facilitates tracking users’ ideology shifts and the
resulting arguing nature.
The proposed methods have been evaluated
both qualitatively and quantitatively using a large
number of real-life discussion/debate posts from
four domains. Experimental results show that the
proposed model is highly effective in performing
its tasks and outperforms several baselines.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999624283783784">
There are several research areas that are related
to our work. We compare with them below.
Sentiment analysis: Sentiment analysis
determines positive and negative opinions
expressed on entities and aspects (Hu and Liu,
2004). Main tasks include aspect extraction (Hu
and Liu, 2004; Popescu and Etzioni, 2005),
polarity identification (Hassan and Radev, 2010;
Choi and Cardie, 2010) and subjectivity analysis
(Wiebe, 2000). As discussed earlier, agreement
and disagreement are a special form of
sentiments and are different from the sentiment
studied in the mainstream research. Traditional
sentiment is mainly expressed with sentiment
terms (e.g., great and bad), while agreement and
disagreement are inferred by AD-expressions
(e.g., I agree and I disagree), which we also call
AD-sentiment expressions. Thus, this work
expands the sentiment analysis research.
Topic models: Our work is also related to topic
modeling and joint modeling of topics and other
information as we jointly model several aspects
of discussions/debates.
Topic models like pLSA (Hofmann, 1999) and
LDA (Blei et al., 2003) have proved to be very
successful in mining topics from large text
collections. There have been various extensions
to multi-grain (Titov and McDonald, 2008),
labeled (Ramage et al., 2009), and sequential (Du
et al., 2010) topic models. Yet other approaches
extend topic models to produce author specific
topics (Rosen-Zvi et al., 2004), author persona
(Mimno and McCallum, 2007), social roles
(McCallum et al., 2007), etc. However, these
models do not model debates and hence are
unable to discover AD-expressions and
interaction natures of author pairs.
Also related are topic models in sentiment
analysis which are often referred to as Aspect
and Sentiment models (ASMs). ASMs come in
two main flavors: Type-1 ASMs discover aspect
(or topic) words sentiment-wise (i.e., discovering
positive and negative topic words and sentiments
for each topic without separating topic and
sentiment terms) (e.g., Lin and He, 2009; Brody
and Elhadad, 2010, Jo and Oh, 2011). Type-2
ASMs separately discover both aspects and
sentiments (e.g., Mei et al., 2007; Zhao et al.,
2010). Recently, domain knowledge induced
ASMs have also been proposed (Mukherjee and
Liu, 2012b; Chen et al., 2013). The generative
process of ASMs is, however, different from our
model. Specifically, Type-1 ASMs use
asymmetric hyper-parameters for aspects while
Type-2 assumes that sentiments and aspects are
emitted in the same sentence. However, AD-
expressions are emitted differently. They are
mostly interleaved with users’ topical viewpoints
and span different sentences. Further, we capture
the key characteristic of discussions by encoding
pair-wise user interactions. Existing models do
not model pair interactions.
In terms of discussions and comments, Yano
et al., (2009) proposed the CommentLDA model
which builds on the work of LinkLDA (Erosheva
et al., 2004). Mukherjee and Liu (2012d) mined
comment expressions. These works, however,
don’t model pair interactions in debates.
Support/oppose camp classification: Several
works have attempted to put debate authors into
support/oppose camps. Agrawal et al. (2003)
used a graph based method. Murakami and
Raymond (2010) used a rule-based method. In
(Galley et al., 2004; Hillard et al., 2003), speaker
</bodyText>
<page confidence="0.997909">
672
</page>
<bodyText confidence="0.999868285714286">
utterances were classified into agreement,
disagreement and backchannel classes.
Stances in online debates: Somasundaran and
Wiebe (2009), Thomas et al. (2006), Bansal et al.
(2008), Burfoot et al. (2011), and Anand et al.
(2011) proposed methods to recognize stances in
online debates. Some other research directions
include subgroup detection (Abu-Jbara et al.,
2012), tolerance analysis (Mukherjee et al.,
2013), mining opposing perspectives (Lin and
Hauptmann, 2006), linguistic accommodation
(Mukherjee and Liu, 2012c), and contention
point mining (Mukherjee and Liu, 2012a). For
this work, we adopt the JTE-P model in
(Mukherjee and Liu, 2012a), and make two
major advances. We propose a new method to
improve the AD-expression mining and a new
task of classifying pair interaction nature to
determine whether each pair of users who have
interacted based on replying relations mostly
agree or disagree with each other.
</bodyText>
<sectionHeader confidence="0.996417" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999385741935484">
We now introduce the JTE-P model with
additional details. JTE-P is a semi-supervised
generative model motivated by the joint
occurrence of expression types (agreement and
disagreement), topics in discussion posts, and
user pairwise interactions. Before proceeding, we
make the following observation about online
discussions.
In a typical debate/discussion post, the user
(author) mentions a few topics (using
semantically related topical terms) and expresses
some viewpoints with one or more AD-
expression types (using agreement and
disagreement expressions). AD-expressions are
directed towards other user(s), which we call
target(s). In this work, we focus on explicit
mentions (i.e., using @name or quoting other
authors’ posts). In our crawled dataset, 77% of
all posts exhibit explicit quoting/reply-to
relations excluding the first posts of threads
which start the discussions and usually have
nobody to quote/reply-to. Such author-target
exchanges usually go back and forth between
pairs of users populating a thread of discussion.
The discussion topics and AD-expressions
emitted are thus caused by the author-pairs’
topical interests and their nature of interaction
(agreeing vs. disagreeing).
In our discussion data obtained from
Volconvo.com, we found that a pair of users
typically exhibited a dominant arguing nature
</bodyText>
<figureCaption confidence="0.993923">
Figure 1: JTE-P Model in plate notation.
</figureCaption>
<table confidence="0.998526666666667">
Variable/Function Description
𝑑; 𝑎𝑑 A document (post) 𝑑 ; author 𝑎 of
document, 𝑑
𝑏𝑑 = [𝑏1 ... 𝑏𝑛] List of targets to whom 𝑎𝑑replies/quotes in d.
𝑝= (𝑎, 𝑎′) Pair of two authors interacting byreply/quote.
𝜃𝑝𝑇 ; Pair 𝑝 ’s distribution over topics ;
𝐸 , 𝐸 ,
𝜃𝑝𝐸(𝜃𝑝,𝐴𝑔 expression types (Agreement: 𝜃𝑝,𝐴𝑔
𝐸 ) 𝐸 )
𝜃𝑝,𝐷𝑖𝑠𝐴𝑔 Disagreement: 𝜃𝑝,𝐷𝑖𝑠𝐴𝑔
𝐸 Topic 𝑡 ’s ; Expression type 𝑒 ’s
𝜑𝑡𝑇; 𝜑𝑒∈{𝐴𝑔,𝐷𝑖𝑠𝐴𝑔} distribution over vocabulary terms
𝑇; 𝐸 Total number of topics; expression types
𝑉; 𝑃 Total number of vocabulary terms; pairs
𝑤𝑑,𝑗; 𝑁𝑑 𝑗𝑡ℎ term in 𝑑; Total # of terms in 𝑑
𝜓 𝑑,𝑗 Distribution over topics and AD-
expressions
𝑥𝑑,𝑗 Associated feature context of the
observed term 𝑤 𝑑,𝑗
𝜆 Learned Max-Ent parameters
𝑟𝑑 ,𝑗 ∈ {𝑡̂, 𝑒̂} Binary indicator/switch variable ( topic
(𝑡̂) or AD-expression (𝑒̂) ) for 𝑤𝑑,𝑗
𝑧𝑑,𝑗 Topic/Expression type of 𝑤𝑑,𝑗
𝛼𝑇; 𝛼𝐸; 𝛽𝑇; 𝛽𝐸 Dirichlet priors of 𝜃𝑝𝑇; 𝜃𝑝𝐸; 𝜑𝑡𝑇; 𝜑𝑒𝐸
𝑃𝐸 # of times topic 𝑡; expression type 𝑒
𝑃𝑇; 𝑛𝑝,𝑒 assigned to 𝑝
𝑛𝑝,𝑡
𝐶𝐸 # of times term 𝑣 appears in topic 𝑡;
𝐶𝑇; 𝑛𝑒,𝑣 expression type 𝑒
𝑛𝑡,𝑣
</table>
<tableCaption confidence="0.999768">
Table 1: List of Notations
</tableCaption>
<bodyText confidence="0.999823090909091">
(agreeing vs. disagreeing) towards each other
across various topics or threads. We believe this
is because our data consists of topics like
elections, theism, terrorism, vegetarianism, etc.
which are often heated and attract people with
pre-determined, strong, and polarized stances1.
This observation motivates the generative
process of our model. Referring to the notations
in Table 1, we explain the generative process of
JTE-P. Given a document (post) 𝑑, its author, 𝑎𝑑,
and the list of targets to whom 𝑎𝑑 replies/quotes
</bodyText>
<footnote confidence="0.8371692">
1 These hardened perspectives are supported by theoretical
studies in communications like the polarization effect
(Sunstein, 2002), and the hostile media effect, a scenario
where partisans rigidly hold on to their stances (Hansen and
Hyunjung, 2011).
</footnote>
<equation confidence="0.63254785">
βT
ad bd
P
c
φT
T
W W
Z
r
φE
E
ψ
X θE αE
Nd
D
βE
θT
P
λ
αT
</equation>
<page confidence="0.991565">
673
</page>
<bodyText confidence="0.999785557692308">
in 𝑑 , 𝑏𝑑 = [𝑏1 ... 𝑏𝑛] , the document 𝑑 exhibits
shared topics and arguing nature of various pairs,
𝑝 = (𝑎𝑑, 𝑐) , where 𝑐 ∈ 𝑏𝑑. More precisely, the
pair specific topic and AD-expression
distributions (𝜃𝑝𝑇 ; 𝜃𝑝𝐸) “shape” the topics and
AD-expressions emitted in 𝑑 as agreement and
disagreement on topical viewpoints are directed
towards certain target authors. Each topic (𝜑𝑡𝑇)
and AD-expression type (𝜑𝑒𝐸) is characterized by
a multinomial distribution over terms
(words/phrases). Assume we have 𝑡 = 1 ... 𝑇
topics and 𝑒 = 1 ... 𝐸 expression types in our
corpus. Note that in our case of discussion/debate
forums, we hypothesize 𝐸 = 2 as in debates, we
mostly find two expression types: agreement and
disagreement (more details in §6.1). Like most
generative models for text, a post (document) is
viewed as a bag of n-grams and each n-gram
(word/phrase) takes one value from a predefined
vocabulary. In this work, we use up to 4-grams,
i.e., n = 1, 2, 3, 4. Instead of using all n-grams, a
relevance based ranking method is proposed to
select a subset of highly relevant n-grams for
model building (details in §4). For notational
convenience, we use terms to denote both words
(unigrams) and phrases (n-grams).
JTE-P is a switching graphical model (Ahmed
and Xing, 2010; Zhao et al., 2010) performing a
switch between AD-expressions and topics. 𝜓𝑑,𝑗
denotes the distribution over topics and AD-
expressions with 𝑟𝑑,𝑗 ∈ {𝑡̂, 𝑒̂} denoting the binary
indicator/switch variable (topic or AD-
expression) for the 𝑗 th term of 𝑑 , 𝑤𝑑,𝑗 . To
perform the switch we use a maximum entropy
(Max-Ent) model. The idea is motivated by the
observation that topical and AD-expression terms
usually play different roles in a sentence. Topical
terms (e.g., “elections” and “income tax”) tend to
be noun and noun phrases while AD-expression
terms (“I refute”, “how can you say”, and
“probably agree”) usually contain pronouns,
verbs, wh-determiners, and modals. In order to
utilize the part-of-speech (POS) tag information,
we place the topic/AD-expression distribution
𝜓𝑑,𝑗 (the prior over the indicator variable 𝑟𝑑,𝑗) in
the term plate (see Figure 1) and set it from a
Max-Ent model conditioned on the observed
feature context 𝑥𝑑,𝑗 associated with 𝑤𝑑,𝑗 and the
learned Max-Ent parameters, 𝜆 (details in §6.1).
In this work, we use both lexical and POS
features of the previous, current, and next POS
tags/lexemes of the term 𝑤𝑑,𝑗 as the contextual
</bodyText>
<equation confidence="0.540623">
information, i.e., 𝑥𝑑,𝑗 = [𝑃𝑂𝑆𝑤𝑑,𝑗−1, 𝑃𝑂𝑆𝑤𝑑,𝑗,
𝑃𝑂𝑆𝑤𝑑,𝑗+1, 𝑤𝑑,𝑗−1,𝑤𝑑,𝑗, 𝑤𝑑,𝑗+1], which is used to
</equation>
<bodyText confidence="0.999740333333333">
produce the feature functions for Max-Ent. For
phrasal terms (n-grams), all POS tags and
lexemes of 𝑤𝑑,𝑗 are considered as contextual
information for computing feature functions in
Max-Ent. We now detail the generative process
of JTE-P (plate notation in Figure 1) as follows:
</bodyText>
<listItem confidence="0.9841488">
1. For each AD-expression type 𝑒, draw 𝜑𝑒𝐸~𝐷𝑖𝑟(𝛽𝐸)
2. For each topic 𝑡, draw 𝜑𝑡𝑇~𝐷𝑖𝑟(𝛽𝑇)
3. For each pair 𝑝, draw 𝜃𝑝𝐸 ~𝐷𝑖𝑟(𝛼𝐸); 𝜃𝑝𝑇 ~𝐷𝑖𝑟(𝛼𝐸)
4. For each forum discussion post 𝑑 ∈ {1 ... 𝐷}:
i. Given the author 𝑎𝑑 and the list of targets 𝑏𝑑, for
each term 𝑤𝑑,𝑗, 𝑗 ∈ {1 ... 𝑁𝑑}:
a. Draw a target 𝑐~𝑈𝑛𝑖(𝑏𝑑)
b. Form pair 𝑝 = (𝑎𝑑, 𝑐), 𝑐 ∈ 𝑏𝑑
c. Set 𝜓𝑑,𝑗 ← 𝑀𝑎𝑥𝐸𝑛𝑡(𝑥𝑑,𝑗;𝜆)
d. Draw 𝑟𝑑,𝑗~𝐵𝑒𝑟𝑛(𝜓𝑑,𝑗)
</listItem>
<equation confidence="0.990518833333333">
e. if (𝑟𝑑,𝑗 = 𝑒̂) // 𝑤𝑑,𝑗 is an AD-expression term
Draw 𝑧𝑑,𝑗~𝑀𝑢𝑙𝑡(𝜃𝑝𝐸 )
else // 𝑟𝑑,𝑗 = 𝑡̂, 𝑤𝑑,𝑗 is a topical term
Draw 𝑧𝑑,𝑗 ~𝑀𝑢𝑙𝑡(𝜃𝑝𝑇 )
f. Emit 𝑤𝑑,𝑗~𝑀𝑢𝑙𝑡(𝜑𝑧𝑑,𝑗
𝑟𝑑,𝑗)
</equation>
<bodyText confidence="0.992654777777778">
𝐷𝑖𝑟, 𝑀𝑢𝑙𝑡, 𝐵𝑒𝑟𝑛, and 𝑈𝑛𝑖 correspond to the
Dirichlet, Multinomial, Bernoulli, and Uniform
distributions respectively. To learn JTE-P, we
employ approximate posterior inference using
Monte Carlo Gibbs sampling. Denoting the
random variables {𝑤, 𝑧, 𝑝, 𝑟} associated with each
term by singular subscripts {𝑤𝑘, 𝑧𝑘, 𝑝𝑘, 𝑟𝑘}, 𝑘1...𝐾,
𝐾 = ∑𝑑 𝑁𝑑 , a single Gibbs sweep consists of
performing the following sampling.
</bodyText>
<equation confidence="0.962371037037037">
𝑝 (𝑧𝑘 = 𝑡, 𝑝𝑘 = 𝑝, 𝑟𝑘 = 𝑡̂ |... ) ∝
𝑒𝑥𝑝�∑ 𝜆𝑖𝑓𝑖�𝑥𝑑,𝑗,𝑡̂�
𝑛 �
𝑖=1
� ×
∑ 𝑒𝑥𝑝�∑ 𝜆𝑖𝑓𝑖�𝑥𝑑,𝑗,𝑦�
𝑛
𝑦∈{𝑒�,𝑡�} 𝑖=1
𝑛𝑡,𝑣𝐶𝑇¬𝑘+𝛽𝑇 (1)
𝑛𝑡,(·)𝑘+𝑉
𝛽𝑇
𝑝 (𝑧𝑘 = 𝑒, 𝑝𝑘 = 𝑝, 𝑟𝑘 = 𝑒̂ |... ) ∝
𝑒𝑥𝑝�∑ 𝜆𝑖𝑓𝑖�𝑥𝑑,𝑗,𝑒̂�
𝑛 �
𝑖=1
� ×
∑ 𝑒𝑥𝑝�∑ 𝜆𝑖𝑓𝑖�𝑥𝑑,𝑗,𝑦�
𝑛
𝑦∈{𝑒�,𝑡�} 𝑖=1
𝑛𝑒,𝑣
𝐶𝐸 ¬𝑘+𝛽𝐸 (2)
𝐶𝐸 𝛽𝐸
𝑛𝑒,(·)¬𝑘+𝑉
Count variables 𝑛𝑡,𝑣
𝐶𝑇 , 𝑛𝑒,𝑣
𝐶𝐸 , 𝑛𝑝,𝑡
𝑃𝑇 , and 𝑛𝑝,𝑒
</equation>
<bodyText confidence="0.996300538461538">
𝑃𝐸 are
detailed in Table 1. Omission of a latter index
denoted by (·) represents the marginalized sum
over the latter index. 𝑘 = (𝑑, 𝑗) denotes the 𝑗 th
term of document 𝑑 and the subscript ¬𝑘 denotes
the counts excluding the term at (𝑑, 𝑗). 𝜆1...𝑛 are
the parameters of the learned Max-Ent model
corresponding to the 𝑛 binary feature functions
𝑓1...𝑛 for Max-Ent. These learned Max-Ent 𝜆
parameters in conjunction with the observed
feature context, 𝑥𝑑,𝑗 feed the supervision signal
for topic/expression switch parameter, r which is
updated during inference in equations (1) and (2).
</bodyText>
<figure confidence="0.980708285714286">
1
|𝑏𝑑|
𝑃𝑇
𝑛𝑝,𝑡¬𝑘+𝛼𝑇
𝑃𝑇
𝑛𝑝,(·)
+𝑇𝛼𝑇
¬𝑘
1
|𝑏𝑑|
𝑃𝐸
𝑛𝑝,𝑒¬𝑘+𝛼𝐸
𝑃𝐸
𝑛𝑝,(·)¬𝑘+𝐸𝛼𝐸
</figure>
<page confidence="0.983147">
674
</page>
<sectionHeader confidence="0.945434" genericHeader="method">
4 Phrase Ranking based on Relevance
</sectionHeader>
<bodyText confidence="0.999953843137255">
We now detail our method of pre-processing n-
grams (phrases) based on relevance to select a
subset of highly relevant n-grams for model
building. This has two advantages: (i). A large
number of irrelevant n-grams slow inference. (ii).
Filtering irrelevant terms in the vocabulary
improves the quality of AD-expressions. Before
proceeding, we review some existing approaches.
Topics in most topic models like LDA are
usually unigram distributions. This offers a great
computational advantage compared to more
complex models which consider word ordering
(Wallach, 2006; Wang et al., 2007). This thread
of research models bigrams by encoding them
into the generative process. For each word, a
topic is sampled first, then its status as a unigram
or bigram is sampled, and finally the word is
sampled from a topic-specific unigram or bigram
distribution. This method, however, is expensive
computationally and has a limitation for arbitrary
length n-grams. In (Tomokiyo and Hurst, 2003),
a language model approach is used for bigram
phrase extraction.
Yet another thread of research post-processes
the discovered topical unigrams to form multi-
word phrases using likelihood scores (Blei and
Lafferty, 2009). This approach considers adjacent
word pairs and identifies n-grams which occur
much more often than one would expect by
chance alone by computing likelihood ratios.
While this is reasonable, a significant n-gram
with high likelihood score may not necessarily be
relevant to the problem domain. For instance, in
our case of discovering AD-expressions, the
likelihood score 2 of 𝑝1 = “the government of”
happens to be more than 𝑝2 = “I completely
disagree”. Clearly, the former is irrelevant for the
task of discovering AD-expressions. The reason
for this is that likelihood scores or other
statistical test scores rely on the relative counts in
the multi-way contingency table to compute
significance. Since the relative counts of different
fragments of the irrelevant phrase 𝑝1, e.g. “the
government”, and “government of”, happen to
appear more than the corresponding counts in the
contingency table of 𝑝2, the tests assign a higher
score. This is nothing wrong per se because the
statistical tests only judge significance of an n-
gram, but a significant n-gram may not
necessarily be relevant in a given problem
domain.
</bodyText>
<footnote confidence="0.7170065">
2 Computed using N-gram statistics package, NSP; http://n-
gram.sourceforge.net
</footnote>
<bodyText confidence="0.980431333333333">
Thus, the existing approaches have some
major shortcomings for our task. As our goal is
to enhance the expressiveness of our models by
considering relevant n-grams preserving the
advantages of exchangeable modeling, we
employ a pre-processing technique to rank n-
grams based on relevance and consider certain
number of top ranked n-grams based on coverage
(details follow) in our vocabulary. The idea
works as follows.
We first induce a unigram JTE-P whereby we
cluster the relevant AD-expression unigrams in
𝜑𝐴𝑔𝐸 and 𝜑𝐷𝑖𝑠𝐴𝑔
𝐸 . Our notion of relevance of AD-
expressions is already encoded into the model
using priors set from Max-Ent. Next, we rank the
candidate phrases (n-grams) using our
probabilistic ranking function. The ranking
function is grounded on the following
hypothesis: a relevant phrase is one whose
unigrams are closely related to (or appear with
high probabilities in) the given AD-expression
type, 𝑒 : Agreement ( 𝐴𝑔 ) or disagreement
(𝐷𝑖𝑠𝐴𝑔). Continuing from the previous example,
given the expression type 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔
𝐸 , 𝑝2 is relevant
while 𝑝1 is not as “government” and “disagree”
are highly unlikely and likely respectively to be
clustered in 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔 𝐸. Thus, we want to rank
phrases based on 𝑃(𝑅𝑒𝑙 = 1|𝑒,𝑝) where 𝑒 denotes
the expression type (Agreement/Disagreement),
𝑝 denotes a candidate phrase. Following the
probabilistic relevance model in (Lafferty and
Zhai, 2003), we use a similar technique to that in
(Zhao et al., 2011) for deriving our relevance
ranking function as follows:
</bodyText>
<equation confidence="0.83927775">
𝑃(𝑅𝑒𝑙=1|𝑒,𝑝)
𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) =
We further define 𝜀 = 𝑃(𝑅𝑒𝑙=0|𝑒)Without loss of
𝑃(𝑅𝑒𝑙=1|𝑒)
</equation>
<bodyText confidence="0.9936435">
generality, one can say that 𝑃(𝑅𝑒𝑙 = 0|𝑒) ≫
𝑃(𝑅𝑒𝑙 = 1|𝑒) , because there are many more
irrelevant phrases than relevant ones, i.e., 𝜀 ≫ 1.
Thus, taking log, from equation (3), we get,
</bodyText>
<equation confidence="0.993537666666667">
log 𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) = log � 1
1+𝜀×𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) � ≈
𝑃(𝑝|𝑅𝑒𝑙=1,𝑒)
log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒)
𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) × 1 𝜀� = log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒)
𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)� − log 𝜀 (4)
</equation>
<bodyText confidence="0.6726824">
Thus, our ranking function actually computes the
relevance score log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒)
𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)�. The last term,
log𝜀 being a constant is ignored because it
cancels out while comparing candidate n-grams.
</bodyText>
<equation confidence="0.970152571428571">
1
1+𝑃(𝑅𝑒𝑙=0|𝑒,𝑝)
𝑃(𝑅𝑒𝑙=1|𝑒,𝑝)
1
=
1+[𝑃(𝑝 |𝑅𝑒𝑙=0,𝑒)×𝑃(𝑅𝑒𝑙=0|𝑒)]
[𝑃(𝑝 |𝑅𝑒𝑙=1,𝑒)×𝑃(𝑅𝑒𝑙=1|𝑒)]
1+𝑃(𝑅𝑒𝑙=0,𝑝 |𝑒)
𝑃(𝑅𝑒𝑙=1,𝑝 |𝑒)
=
𝑃(𝑅𝑒𝑙 =0|𝑒,𝑝)+𝑃(𝑅𝑒𝑙=1|𝑒,𝑝)
1
=
(3)
</equation>
<page confidence="0.959573">
675
</page>
<bodyText confidence="0.976208">
We now estimate the relevance score of a phrase
</bodyText>
<equation confidence="0.637172">
𝑝 = (𝑤1, 𝑤2, . .., 𝑤𝑛). Using the conditional
</equation>
<bodyText confidence="0.994207333333333">
independence assumption of words given the
indicator variable 𝑅𝑒𝑙 and expression type 𝑒, we
have:
</bodyText>
<equation confidence="0.99927725">
log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒)
𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)� = ∑ log 𝑃(𝑤𝑖|𝑅𝑒𝑙=1,𝑒)
𝑛 (5)
𝑖=1 𝑃(𝑤𝑖|𝑅𝑒𝑙=0,𝑒)
</equation>
<bodyText confidence="0.997961666666667">
Given the expression model 𝜑𝑒𝐸 previously
learned by inducing the unigram JTE-P, it is
intuitive to set 𝑃(𝑤𝑖|𝑅𝑒𝑙 = 1,𝑒) to the point
</bodyText>
<equation confidence="0.508866">
𝐸𝑉 +𝛽𝐸
</equation>
<bodyText confidence="0.864898">
estimate of the posterior on 𝜑𝑒,𝑤𝑖
</bodyText>
<equation confidence="0.9732875">
𝐸 = 𝑛𝑒,𝑤𝑖 𝑛𝑒,(·)
𝐸𝑉 +𝑉𝛽𝐸 ,
</equation>
<bodyText confidence="0.956900157894737">
where 𝑛𝑒,𝑤𝑖
𝐸𝑉 is the number of times 𝑤𝑖 was
assigned to AD-expression type 𝑒 and 𝑛𝑒,(·)
𝐸𝑉
denotes the marginalized sum over the latter
index. On the other hand, 𝑃(𝑤𝑖|𝑅𝑒𝑙 = 0, 𝑒) can be
estimated using a Laplace smoothed ( 𝜇 = 1)
𝑛𝑤𝑖+𝜇
background model, i.e., (𝑤𝑖|𝑅𝑒𝑙 = 0, 𝑒) = 𝑛𝑉+𝑉𝜇 ,
where 𝑛𝑤𝑖 denotes the number of times 𝑤𝑖
appears in the whole corpus and 𝑛𝑉 denotes the
number of terms in the entire corpus.
Next, we throw light on the issue of choosing
the number of top k phrases from the ranked
candidate n-grams. Precisely, we want to analyze
the coverage of our proposed ranking based on
relevance models. By coverage, we mean that
having selected top k candidate n-grams based on
the proposed relevance ranking, we want to get
an estimate of how many relevant terms from a
sample of the collection were covered. To
compute coverage, we randomly sampled 500
documents from the corpus and listed the
candidate n-grams3 in the collection of sampled
500 documents. For this and subsequent human
judgment tasks, we use two judges (graduate
students well versed in English). We asked our
judges to mark all relevant AD-expressions.
Agreement study yielded xCohen = 0.77 showing
substantial agreement according to scale 4
provided in (Landis and Koch, 1977). This is
understandable as identifying AD-expressions is
a relatively easy task. Finally, a term was
considered to be relevant if both judges marked it
so. We then computed the coverage to see how
many of the relevant terms in the random sample
were also present in top k phrases from the
ranked candidate n-grams. We summarize the
</bodyText>
<footnote confidence="0.724020625">
3 These are terms appearing at least 20 times in the entire
collection. We do this for computational reasons as there
can be many n-grams and n-grams with very low frequency
are less likely to be relevant.
4 No agreement (x &lt; 0), slight agreement (0 &lt; x ≤ 0.2), fair
agreement (0.2 &lt; x ≤ 0.4), moderate agreement (0.4 &lt; x ≤
0.6), substantial agreement (0.6 &lt; x ≤ 0.8), and almost
perfect agreement 0.8 &lt; x ≤ 1.0.
</footnote>
<table confidence="0.88424525">
coverage results below in Table 2.
k 3000 4000 5000
JTE-P Agreement 81.34 84.24 87.01
Disagreement 84.96 87.86 89.64
</table>
<tableCaption confidence="0.999238">
Table 2: Coverage (in %) of AD-expressions.
</tableCaption>
<bodyText confidence="0.99999315">
We find that choosing top k = 5000 candidate n-
grams based on our proposed ranking, we obtain
a coverage of 87% for agreement and 89.64 for
disagreement expression types which are
reasonably good. Thus, we choose top 5000
candidate n-grams for each expression type and
add them to the vocabulary beyond all unigrams.
Like expression types 𝑒1...𝐸, we also ranked
candidate phrases for topics 𝑡1...𝑇 using
𝑃(𝑅𝑒𝑙 = 1|𝑡,𝑝). However, for topics, selecting k
based on coverage of each topic is more difficult
because we induce 50 topics and it is also much
more difficult to manually find relevant topical
phrases in the sampled data as a topical phrase
may belong to more than one topic. We selected
top 2000 ranked candidate phrases for each topic
using 𝑃(𝑅𝑒𝑙 = 1|𝑡, 𝑝) as we feel that is sufficient
for a topic. Note that phrases for topics are not as
crucial as for AD-expressions because topics can
more or less be defined by unigrams.
</bodyText>
<sectionHeader confidence="0.972813" genericHeader="method">
5 Classifying Pair Interaction Nature
</sectionHeader>
<bodyText confidence="0.999885230769231">
We now determine whether two users (also
called a user pair) mostly agree or disagree with
each other in their exchanges, i.e., their pair
interaction or arguing nature. This is a relatively
new task. We first summarize the closest related
works. In (Galley et al., 2004; Hillard et al.,
2003; Thomas et al., 2006, Bansal et al., 2008),
conversational speeches (i.e., U.S. Congress
meeting transcripts) are classified into for or
against an issue using various types of features:
durational (e.g., time taken by a speaker; speech
rate, etc.), structural (e.g., no. of speakers per
side, no. of votes cast by a speaker on a bill, etc.),
and lexical (e.g., first word, last word, n-grams,
etc.). Burfoot et al., (2011) builds on the work of
(Thomas et al., 2006) and proposes collective
classification using speaker contextual features
(e.g., speaker intentions based on vote labels).
However, above works do not discover pair
interactions (arguing nature) in debate authors.
Online discussion forums are textual rather than
conversational (e.g., U.S. Congress meeting
transcripts). Thus, the durational, structural, and
contextual features used in prior works are not
directly applicable.
Instead, the model posterior on 𝜃𝑝𝐸 for JTE-P
</bodyText>
<page confidence="0.998431">
676
</page>
<bodyText confidence="0.9937266">
can actually give an estimate of the overall
interaction nature of a pair, i.e., the probability
masses assigned to expression types, e =
Ag(Agreement) and e = DisAg (Disagreement).
As BE —Dir(aE), we have B�,�=��
</bodyText>
<equation confidence="0.975608">
� + B�,�=�����
� = 1.
</equation>
<bodyText confidence="0.99998225">
Hence, if the probability mass assigned to any
one of the expression types (agreement,
disagreement) &gt; 0.5 then according to the model
posterior, that expression type is dominant, i.e., if
</bodyText>
<equation confidence="0.406569">
B�,��
</equation>
<bodyText confidence="0.9990035">
� &gt; 0.5, the pair is agreeing else disagreeing.
However, this approach is not the best. As we
will see in the experiment section, supervised
classification using labeled training data with
discovered AD-expressions as features performs
better.
</bodyText>
<sectionHeader confidence="0.997569" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999904833333333">
We now evaluate the proposed techniques in the
context of the JTE-P model. We first evaluate the
discovered AD-expressions by comparing results
with and without using the phrase ranking
method in Section 4, and then evaluate the
classification of interaction nature of pairs.
</bodyText>
<subsectionHeader confidence="0.972573">
6.1 Dataset and Experiment Settings
</subsectionHeader>
<bodyText confidence="0.995439605263158">
We crawled debate/discussion forum posts from
Volconvo.com. The forum is divided into various
domains. Each domain consists of multiple
threads of discussions. For each post, we
extracted the post id, author, domain, ids of all
posts to which it replies/quotes, and the post
content. In all, we extracted 26137, 34986,
22354, and 16525 posts from Politics, Religion,
Society and Science domains respectively.
Experiment Data: As it is not interesting to
study pairs who only exchanged a few posts, we
restrict to pairs with at least 20 post exchanges.
This resulted in 1241 authors and 1461 pairs. The
reduced dataset consists of 1095586 tokens (after
n-gram preprocessing in §4), 40102 posts with an
average of 27 posts or interactions per pair. Data
from all 4 domains are combined for modeling.
Parameter Settings: For all our experiments, we
set the hyper-parameters to the heuristic values
aT = 50/T, aE = 50/E, !&apos;T = !&apos;E = 0.1 suggested
in (Griffiths and Steyvers, 2004). We set the
number of topics, T = 50 and the number of AD-
expression types, E = 2 (agreement and
disagreement) as in discussion/debate forums,
there are usually two expression types5. To learn
5 Values for E &gt; 2 were also tried. However, they did not
produce any new dominant expression type. There was also
a slight increase in the model perplexity showing that values
of E &gt; 2 do not fit the debate forum data well.
the Max-Ent parameters A, we randomly sampled
500 terms from the held-out data (10 threads in
our corpus which were excluded from the
evaluation of tasks in §6.2, §6.3) appearing at
least 10 times and labeled them as topical (361)
or AD-expressions (139) and used the
corresponding features of each term (in the
context of posts where it occurs, §3) to train the
Max-Ent model.
</bodyText>
<subsectionHeader confidence="0.999025">
6.2 AD-Expression Evaluation
</subsectionHeader>
<bodyText confidence="0.999892431818182">
We first list some discovered top AD-expressions
in Table 3 for qualitative inspection. From Table
3, we can see that JTE-P can cluster many correct
AD-expressions, e.g., “I accept”, “I agree”,
“you’re correct”, etc. in agreement and “I
disagree”, “don’t accept”, “I refute”, etc. in
disagreement. In addition, it also discovers and
clusters highly specific and more “distinctive”
expressions beyond those used in Max-Ent
training, e.g., “valid point”, “I do support”, and
“rightly said” in agreement; and phrases like “can
you prove”, “I don’t buy your”, and “you fail to”
in disagreement. Note that terms in black in
Table 3 were used in Max-Ent training. The
newly discovered terms are marked blue in
italics. Clustering errors are in red (bold).
For quantitative evaluation, topic models are
often compared using perplexity. However,
perplexity does not reflect our purpose since we
are not trying to evaluate how well the AD-
expressions in an unseen discussion data fit our
learned models. Instead our focus is to evaluate
how well our learned AD-expression types
perform in clustering semantic phrases of
agreement/disagreement. Since AD-expressions
(according to top terms in V&apos;) produced by JTE-
P are rankings, we choose precision @ n (p@n)
as our metric. p@n is commonly used to evaluate
a ranking when the total number of correct items
is unknown (e.g., Web search results, aspect
terms in topic models for sentiment analysis
(Zhao et al., 2010), etc.). This situation is similar
to our AD-expression rankings, V1. Further, as
V&apos;—Dir, the Dirichlet smoothing effect ensures
that every term in the vocabulary has some non-
zero mass to agreement or disagreement
expression type. Thus, it is the ranking of terms
in each AD-expression type that matters (i.e.,
whether the model is able to rank highly relevant
terms at the top).
The above method evaluates the original
ranking. Another way of evaluating the AD-
expression rankings is to evaluate only those
newly discovered terms, i.e., beyond those
</bodyText>
<page confidence="0.99566">
677
</page>
<table confidence="0.991467333333333">
𝐸 )
Disagreement expressions (𝜑𝑒=𝐷𝑖𝑠𝑎𝑔𝑟𝑒𝑒𝑚𝑒𝑛𝑡
I, disagree, I don’t, I disagree, argument, reject, claim, I reject, I refute, and, your, I refuse, won’t, the claim,
nonsense, I contest, dispute, I think, completely disagree, don’t accept, don’t agree, incorrect, doesn’t, hogwash, I
don’t buy your, I really doubt, your nonsense, true, can you prove, argument fails, you fail to, your assertions,
bullshit, sheer nonsense, doesn’t make sense, you have no clue, how can you say, do you even, contradict yourself, ...
𝐸 )
Agreement expressions (𝜑𝑒=𝐴𝑔𝑟𝑒𝑒𝑚𝑒𝑛𝑡
agree, I, correct, yes, true, accept, I agree, don’t, indeed correct, your, I accept, point, that, I concede, is valid, your
claim, not really, would agree, might, agree completely, yes indeed, absolutely, you’re correct, valid point,
argument, the argument, proves, do accept, support, agree with you, rightly said, personally, well put, I do
support, personally agree, doesn’t necessarily, exactly, very well put, kudos, point taken, ...
</table>
<tableCaption confidence="0.9732685">
Table 3: Top terms (comma delimited) of two expression types. Red (bold) terms denote possible errors.
Blue (italics) terms are newly discovered; rest (black) terms have been used in Max-Ent training.
</tableCaption>
<table confidence="0.999926333333333">
P@n JTE-P (all terms) JTE-P (excluding labeled ME terms)
L
Agreement Disagreement Agreement Disagreement
50 100 150 50 100 150 50 100 150 50 100 150
100 0.62 0.63 0.61 0.64 0.62 0.63 0.58 0.56 0.57 0.60 0.59 0.58
200 0.66 0.67 0.65 0.68 0.66 0.67 0.62 0.59 0.60 0.64 0.63 0.62
300 0.70 0.70 0.71 0.70 0.68 0.67 0.66 0.66 0.65 0.66 0.66 0.65
400 0.72 0.72 0.73 0.74 0.71 0.70 0.68 0.67 0.69 0.70 0.68 0.69
500 0.76 0.77 0.75 0.76 0.73 0.74 0.70 0.71 0.70 0.72 0.71 0.70
</table>
<tableCaption confidence="0.9327175">
Table 4: Results using terms based on phrase relevance ranking for P @ n= 50, 100, 150 across 100, 200,
..., 500 labeled examples (L) used for Max-Ent (ME) training.
</tableCaption>
<table confidence="0.9998876">
P@n JTE-P (all terms) JTE-P (excluding ME terms)
L
Agreement Disagreement Agreement Disagreement
50 100 150 50 100 150 50 100 150 50 100 150
500 0.66 0.69 0.69 0.72 0.70 0.70 0.66 0.65 0.64 0.68 0.66 0.65
</table>
<tableCaption confidence="0.9950515">
Table 5: Results using all tokens (without applying phrase relevance ranking) for P@50, 100, 150 and 500
labeled examples were used for Max-Ent (ME) training).
</tableCaption>
<table confidence="0.9999017">
Feature Setting Agreeing Disagreeing
P R F1 P R F1
JTE-P-posterior 0.59 0.61 0.60 0.81 0.70 0.75
W+POS 1-4 grams 0.63 0.66 0.64 0.83 0.82 0.82
W+POS 1-4grams + IG (top 1%) 0.64 0.67 0.65 0.84 0.82 0.83
W+POS 1-4 grams + IG (top 2%) 0.65 0.67 0.66 0.84 0.82 0.83
W+POS 1-4 grams + χ2 (top 1%) 0.65 0.68 0.66 0.84 0.83 0.83
W+POS 1-4 grams + χ2(top 2%) 0.64 0.68 0.69 0.84 0.82 0.83
AD-Expressions, Φ𝐸 (top 1000) 0.73 0.74 0.73 0.87 0.87 0.87
AD-Expressions, Φ𝐸 (top 2000) 0.77 0.81 0.78 0.90 0.88 0.89
</table>
<tableCaption confidence="0.9995745">
Table 6: Precision (P), recall (R), and F1 scores of pair interaction evaluation. Improvements in F1 using
AD-expression features (𝜑𝐸) are statistically significant (p&lt;0.01) using paired t-test across 5-fold CV.
</tableCaption>
<bodyText confidence="0.99965925">
labeled terms used in Max-Ent training. For this
evaluation, we remove those terms that have
been used in Max-Ent (ME) training. We report
both results in Table 4. We also studied inter-
rater agreement using two judges who
independently labeled the top n terms as correct
or incorrect. A term was marked correct if both
judges deemed it so which was then used to
compute p@n. Agreement using 𝜅𝐶𝑜ℎ𝑒𝑛 was
greater than 0.78 for all p@n computations
implying substantial and good agreements as
identifying whether a phrase implies agreement
or disagreement or none is an easy task. P@n
excluding ME labeled terms (Table 4, second
column) are slightly lower than those using all
terms but are still decent. This is because p@n
excluding ME labeled terms removes many
correct AD-expressions used in training.
Further to evaluate the sensitivity of
performance on the amount of labeled terms for
Max-Ent, we computed p@n across different
sizes of labeled terms. Table 4 shows p@n for
agreement and disagreement expressions across
different sizes of labeled terms (L). We find that
more labeled terms improves p@n which is
intuitive. We used 500 labeled terms in all our
subsequent experiments. The result in Table 4
uses relevance ranking (§4).
</bodyText>
<page confidence="0.997627">
678
</page>
<bodyText confidence="0.999934909090909">
We now compare with the performance of the
model without using phrase relevance ranking.
P@n results using all tokens (4356787) are
shown in Table 5 (with 500 labeled terms for
Max-Ent training). Clearly, P@n is lower than in
Table 4 (last row; with phrase relevance ranking)
because without phrase relevance ranking (Table
5) many irrelevant terms can rank high due to co-
occurrences which may not be semantically
related. This shows that relevance ranking of
phrases is beneficial.
</bodyText>
<subsectionHeader confidence="0.99528">
6.3 Pair Interaction Nature
</subsectionHeader>
<bodyText confidence="0.999966307692308">
We now evaluate the overall interaction nature of
each pair of users. The evaluation of this task
requires human judges to read all the posts where
the two users forming the pair have interacted.
Thus, it is hard to evaluate all 1461 pairs in our
dataset. Instead, we randomly sampled 500 pairs
(≈ 34% of the population) for evaluation. Two
human judges were asked to independently read
all the post interactions of 500 pairs and label
each pair as overall “disagreeing” or overall
“agreeing” or “none”. The Kcohen for this task
was 0.81. Pairs were finally labeled as agreeing
or disagreeing if both judges deemed them so.
This resulted in 320 disagreeing and 152
agreeing pairs. Out of the rest 28 pairs, 10 were
marked “none” by both judges while 18 pairs had
disagreement in labels. We only focus on the 472
agreeing and disagreeing pairs.
As we have labeled data for 472 pairs, we can
treat identifying pair arguing nature as a text
classification problem where all interactions
between a pair are merged in one document
representing the pair along with the label given
by judges: agreeing or disagreeing. To compare
classification performance, we use two feature
sets: (i) standard word + POS 1-4 grams and (ii)
AD-expressions from VE. We use TF-IDF as our
feature value assignment scheme. We also try
two well-known feature selection schemes Chi-
Squared Test (χ2) and Information Gain (IG). We
use the linear kernel6 SVM (SVMlight system in
(Joachims, 1999)) as our text classifier. For
feature selection using χ2 and IG, we use two
settings: top 1% and 2% of all features ranked
according to the selection metric. Also, for
estimated AD-expressions (according to
probabilities in VE ), we experiment with top
1000 and 2000 AD-expressions terms for both
agreement and disagreement. We summarize
</bodyText>
<footnote confidence="0.590899">
6 Other kernels polynomial, RBF, and sigmoid did not
perform as well.
</footnote>
<bodyText confidence="0.999969538461538">
comparison results using 5-fold Cross Validation
(CV) with two classes: agreeing and disagreeing
in Table 6. JTE-P-posterior represents the
method using simply the model posterior on B&apos;
to make the decision (see §5). From Table 6, we
can make the following observations.
Predicting agreeing arguing nature is harder
than that of disagreeing across all feature
settings. Feature selection improves performance.
χ2 and IG perform similarly. AD-expressions,
VEyields the best performance showing that the
discovered AD-expressions are of high quality
and reflect the user pair arguing nature well.
Selecting certain top terms in VE can also be
viewed as a form of feature selection. Although
prediction performance using model posterior
(JTE-P-posterior) is slightly lower than
supervised SVM (Table 6, second row), the F1
scores are decent. Using the discovered AD-
expressions (Table 6, last low) as features
renders a statistically significant (see Table 6
caption) improvement over other baseline feature
settings. This shows that discovered AD-
expressions are useful for downstream
applications, e.g., the task of identifying pair
interactions.
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999812">
This paper studied the problem of modeling user
pair interactions in online discussions with the
purpose of discovering the interaction or arguing
nature of each author pair and various AD-
expressions emitted in debates. A novel
technique was also proposed to rank n-gram
phrases where relevance based ranking was used
in conjunction with a semi-supervised generative
model. This method enables us to find better AD-
expressions. Experiments using real-life online
debate data showed the effectiveness of the
model. In our future work, we intend to extend
the model to account for stances, and issue
specific interactions which would pave the way
for user profiling and behavioral modeling.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993081875">
We would like thank Sharon Meraz (Department
of communication, University of Illinois at
Chicago) and Dennis Chong (Department of
Political Science, Northwestern University) for
several valuable discussions. This work was
supported in part by a grant from the National
Science Foundation (NSF) under grant no. IIS-
1111092.
</bodyText>
<page confidence="0.99889">
679
</page>
<sectionHeader confidence="0.990334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909504504504">
Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir
Radev. 2012. Subgroup detection in ideological
discussions. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL-2012).
Agrawal, R., Rajagopalan, S., Srikant, R., and Xu. Y.
2003. Mining newsgroups using networks arising
from social behavior. In Proceedings of the
International Conference on World Wide Web
(WWW-2003).
Ahmed, A and Xing, E. 2010. Staying informed:
supervised and semi-supervised multi-view topical
analysis of ideological perspective. In Proceedings
of the Empirical Methods in Natural Language
Processing (EMNLP-2010).
Anand, P., Walker, M., Abbott, R., Tree, J., Bowmani,
R., and Minor, M. 2011. Cats rule and dogs drool!:
Classifying stance in online debate. In Proceedings
of the 2nd Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis.
Bansal, M., Cardie, C., and Lee, L. 2008. The power
of negative thinking: Exploiting label disagreement
in the min-cut classification framework. In
Proceedings of the International Conference on
Computational Linguistics (Short Paper).
Blei, D., Ng, A., and Jordan, M. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research.
Blei, D. and Lafferty J. 2009. Visualizing topics with
multi-word expressions. Tech. Report.
arXiv:0907.1013v1.
Brody, S. and Elhadad, S. 2010. An Unsupervised
Aspect-Sentiment Model for Online Reviews. In
Proceedings of the Annual Conference of the North
American Chapter of the ACL (NAACL-2010).
Burfoot, C., Bird, S., and Baldwin, T. 2011. Collective
Classification of Congressional Floor-Debate
Transcripts. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL-2001).
Chang, J., Boyd-Graber, J., Wang, C. Gerrish, S.
Blei, D. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the Neural
Information Processing Systems (NIPS-2009).
Chen, Z., Mukherjee, A., Liu, B., Hsu, M.,
Castellanos, M., Ghosh, R. 2013. Leveraging Multi-
Domain Prior Knowledge in Topic Models. In
Proceedings of the International Joint Conference in
Artificial Intelligence (IJCAI-2013).
Choi, Y. and Cardie, C. 2010. Hierarchical sequential
learning for extracting opinions and their attributes
(Short Paper). In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL-2010).
Du, L., Buntine, W. L., and Jin, H. 2010. Sequential
Latent Dirichlet Allocation: Discover Underlying
Topic Structures within a Document. In
Proceedings of the IEEE International Conference
on Data Mining (ICDM-2010).
Erosheva, E., Fienberg, S. and Lafferty, J. 2004.
Mixed membership models of scientific
publications. In Proceedings of the National
Academy of Sciences (PNAS-2004).
Galley, M., McKeown, K., Hirschberg, J., and
Shriberg, E. 2004. Identifying agreement and
disagreement in conversational speech: Use of
Bayesian networks to model pragmatic
dependencies. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL-2004).
Griffiths, T. and Steyvers, M. 2004. Finding scientific
topics. In Proceedings of the National Academy of
Sciences (PNAS-2004).
Hansen, G. J., and Hyunjung, K. 2011. Is the media
biased against me? A meta-analysis of the hostile
media effect research. Communication Research
Reports, 28, 169-179.
Hillard, D., Ostendorf, M., and Shriberg, E. 2003.
Detection of agreement vs. disagreement in
meetings: Training with unlabeled data. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies (NAACL-HLT-2003).
Hassan, A. and Radev, D. 2010. Identifying text
polarity using random walks. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-2010).
Hofmann, T. 1999. Probabilistic latent semantic
analysis. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence (UAI-1999).
Hu, M. and Liu, B. 2004. Mining and summarizing
customer reviews. In Proceedings of the SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD-2004).
Jo, Y. and Oh, A. 2011. Aspect and sentiment
unification model for online review analysis. In
Proceedings of the International Conference on
Web Search and Data Mining (WSDM-2011).
Joachims, T. Making large-Scale SVM Learning
Practical. 1999. Advances in Kernel Methods -
Support Vector Learning, B. Schölkopf and C.
Burges and A. Smola (ed.), MIT-Press, 1999.
Lafferty, J. and Zhai, C. 2003. Probabilistic relevance
models based on document and query generation.
Language Modeling and Information Retrieval.
Landis, J. R. and Koch, G. G. 1977. The measurement
of observer agreement for categorical data.
Biometrics.
Lin, C. and He, Y. 2009. Joint sentiment/topic model
for sentiment analysis. In Proceedings of the
</reference>
<page confidence="0.971278">
680
</page>
<reference confidence="0.999859714285714">
International Conference on Knowledge
Management (CIKM-2009).
Lin, W. H., and Hauptmann, A. 2006. Are these
documents written from different perspectives?: a
test of different perspectives based on statistical
distribution divergence. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-2006).
Liu, B. 2012. Sentiment Analysis and Opinion Mining.
Morgan &amp; Claypool Publisher, USA.
McCallum, A., Wang, X., and Corrada-Emmanuel, A.
2007. Topic and Role Discovery in Social Networks
with Experiments on Enron and Academic Email.
Journal of Artificial Intelligence Research.
Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of the
International Conference on World Wide Web
(WWW-2007).
Mimno, D. and McCallum, A. 2007. Expertise
modeling for matching papers with reviewers. In
Proceedings of the SIGKDD International
Conference on Knowledge Discovery and Data
Mining (KDD-2007).
Mukherjee, A., Venkataraman, V., Liu, B., Meraz, S.
2013. Public Dialogue: Analysis of Tolerance in
Online Discussions. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL-2013).
Mukherjee, A. and Liu, B. 2012a. Mining Contentions
from Discussions and Debates. Proceedings of
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD-2012).
Mukherjee, A. and Liu, B. 2012b. Aspect Extraction
through Semi-Supervised Modeling. In Proceedings
of the Annual Meeting of the Association for
Computational Linguistics (ACL-2012).
Mukherjee, A. and Liu, B. 2012c. Analysis of
Linguistic Style Accommodation in Online Debates.
In Proceedings of the International Conference on
Computational Linguistics (COLING-2012).
Mukherjee, A. and Liu, B. 2012d. Modeling Review
Comments. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL-2012).
Murakami A. and Raymond, R. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the International Conference on
Computational Linguistics (Coling-2010).
Pang, B. and Lee, L. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Popescu, A. and Etzioni, O. 2005. Extracting product
features and opinions from reviews. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-2005).
Ramage, D., Hall, D., Nallapati, R, Manning, C. 2009.
Labeled LDA: A supervised topic model for credit
attribution in multi-labeled corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-2009).
Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smith,
P. 2004. The author-topic model for authors and
documents. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence (UAI-2004).
Sunstein, C. R. 2002. The law of group polarization.
Journal of political philosophy.
Somasundaran, S. and Wiebe, J. 2009. Recognizing
stances in online debates. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing (ACL-IJCNLP-2009).
Titov, I. and R. McDonald. 2008. Modeling online
reviews with multi-grain topic models. In
Proceedings of the International Conference on
World Wide Web (WWW-2008).
Thomas, M., Pang, B., and Lee, L. 2006. Get out the
vote: Determining support or opposition from
congressional floor-debate transcripts. In Proc. of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP-2006).
Tomokiyo, T., and Hurst, M. 2003. A language model
approach to keyphrase extraction. In Proceedings of
the ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment-Volume 18.
Wallach, H. 2006. Topic modeling: Beyond bag of
words. In Proceedings of the International
Conference on Machine Learning (ICML-2006).
Wang, X., McCallum, A., Wei, X. 2007. Topical N-
grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings
of the IEEE International Conference on Data
Mining (ICDM-2007).
Wiebe, J. 2000. Learning subjective adjectives from
corpora. In Proc. of National Conference on AI
(AAAI-2000).
Yano, T., Cohen, W. and Smith, N. 2009. Predicting
response to political blog posts with topic models.
In Proceedings of the N. American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT-2009).
Zhao, X., J. Jiang, J. He, Y. Song, P. Achananuparp,
E.P. LiM, and X. Li. 2011. Topical keyphrase
extraction from twitter. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-2011).
Zhao, X., Jiang, J., Yan, H., and Li, X. 2010. Jointly
modeling aspects and opinions with a MaxEnt-
LDA hybrid. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2010).
</reference>
<page confidence="0.9983">
681
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990032">
<title confidence="0.999825">Discovering User Interactions in Ideological Discussions</title>
<author confidence="0.995897">Arjun Mukherjee Bing</author>
<affiliation confidence="0.9999195">Department of Computer University of Illinois at Chicago</affiliation>
<email confidence="0.998414">arjun4787@gmail.comliub@cs.uic.edu</email>
<abstract confidence="0.99985425">Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abu-Jbara</author>
<author>P Dasigi</author>
<author>M Diab</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</booktitle>
<contexts>
<context position="9471" citStr="Abu-Jbara et al., 2012" startWordPosition="1397" endWordPosition="1400">ssification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model wit</context>
</contexts>
<marker>Abu-Jbara, Dasigi, Diab, Radev, 2012</marker>
<rawString>Abu-Jbara, A., Dasigi, P., Diab, M. and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on World Wide Web (WWW-2003).</booktitle>
<marker>Y, 2003</marker>
<rawString>Agrawal, R., Rajagopalan, S., Srikant, R., and Xu. Y. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of the International Conference on World Wide Web (WWW-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ahmed</author>
<author>E Xing</author>
</authors>
<title>Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective.</title>
<date>2010</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP-2010).</booktitle>
<contexts>
<context position="2380" citStr="Ahmed and Xing, 2010" startWordPosition="350" endWordPosition="353">rgue with one another. Such ideological discussions on a myriad of social and political issues have practical implications in the fields of communication and political science as they give social scientists an opportunity to study real-life discussions/debates of almost any issue and analyze participant behaviors in a large scale. In this paper, we present such an application, which aims to perform fine-grained analysis of user-interactions in online discussions. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al., 2003), and classifying user stances (Somasundaran and Wiebe, 2009). However, these works are at a rather coarser level and have not considered more fine-grained characteristics of debates/discussions where users interact with each other by quoting/replying each other to express agreement or disagreement and argue with one another. In this work, we want to mine the following information: 1. The nature of interaction of each pair of users or participants who have engaged in the discussion of certain issues, i.e., whether the two persons mo</context>
<context position="14572" citStr="Ahmed and Xing, 2010" startWordPosition="2237" endWordPosition="2240"> we mostly find two expression types: agreement and disagreement (more details in §6.1). Like most generative models for text, a post (document) is viewed as a bag of n-grams and each n-gram (word/phrase) takes one value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. Instead of using all n-grams, a relevance based ranking method is proposed to select a subset of highly relevant n-grams for model building (details in §4). For notational convenience, we use terms to denote both words (unigrams) and phrases (n-grams). JTE-P is a switching graphical model (Ahmed and Xing, 2010; Zhao et al., 2010) performing a switch between AD-expressions and topics. 𝜓𝑑,𝑗 denotes the distribution over topics and ADexpressions with 𝑟𝑑,𝑗 ∈ {𝑡̂, 𝑒̂} denoting the binary indicator/switch variable (topic or ADexpression) for the 𝑗 th term of 𝑑 , 𝑤𝑑,𝑗 . To perform the switch we use a maximum entropy (Max-Ent) model. The idea is motivated by the observation that topical and AD-expression terms usually play different roles in a sentence. Topical terms (e.g., “elections” and “income tax”) tend to be noun and noun phrases while AD-expression terms (“I refute”, “how can you say”, and “probably</context>
</contexts>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Ahmed, A and Xing, E. 2010. Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Anand</author>
<author>M Walker</author>
<author>R Abbott</author>
<author>J Tree</author>
<author>R Bowmani</author>
<author>M Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</booktitle>
<contexts>
<context position="9331" citStr="Anand et al. (2011)" startWordPosition="1378" endWordPosition="1381">erjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of </context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Anand, P., Walker, M., Abbott, R., Tree, J., Bowmani, R., and Minor, M. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>C Cardie</author>
<author>L Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (Short Paper).</booktitle>
<contexts>
<context position="9283" citStr="Bansal et al. (2008)" startWordPosition="1369" endWordPosition="1372">the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair inter</context>
<context position="26739" citStr="Bansal et al., 2008" startWordPosition="4278" endWordPosition="4281">lected top 2000 ranked candidate phrases for each topic using 𝑃(𝑅𝑒𝑙 = 1|𝑡, 𝑝) as we feel that is sufficient for a topic. Note that phrases for topics are not as crucial as for AD-expressions because topics can more or less be defined by unigrams. 5 Classifying Pair Interaction Nature We now determine whether two users (also called a user pair) mostly agree or disagree with each other in their exchanges, i.e., their pair interaction or arguing nature. This is a relatively new task. We first summarize the closest related works. In (Galley et al., 2004; Hillard et al., 2003; Thomas et al., 2006, Bansal et al., 2008), conversational speeches (i.e., U.S. Congress meeting transcripts) are classified into for or against an issue using various types of features: durational (e.g., time taken by a speaker; speech rate, etc.), structural (e.g., no. of speakers per side, no. of votes cast by a speaker on a bill, etc.), and lexical (e.g., first word, last word, n-grams, etc.). Burfoot et al., (2011) builds on the work of (Thomas et al., 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). However, above works do not discover pair interactio</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Bansal, M., Cardie, C., and Lee, L. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In Proceedings of the International Conference on Computational Linguistics (Short Paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="6873" citStr="Blei et al., 2003" startWordPosition="1008" endWordPosition="1011">special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment a</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., Ng, A., and Jordan, M. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>Visualizing topics with multi-word expressions.</title>
<date>2009</date>
<tech>Tech. Report. arXiv:0907.1013v1.</tech>
<contexts>
<context position="19295" citStr="Blei and Lafferty, 2009" startWordPosition="3032" endWordPosition="3035">l., 2007). This thread of research models bigrams by encoding them into the generative process. For each word, a topic is sampled first, then its status as a unigram or bigram is sampled, and finally the word is sampled from a topic-specific unigram or bigram distribution. This method, however, is expensive computationally and has a limitation for arbitrary length n-grams. In (Tomokiyo and Hurst, 2003), a language model approach is used for bigram phrase extraction. Yet another thread of research post-processes the discovered topical unigrams to form multiword phrases using likelihood scores (Blei and Lafferty, 2009). This approach considers adjacent word pairs and identifies n-grams which occur much more often than one would expect by chance alone by computing likelihood ratios. While this is reasonable, a significant n-gram with high likelihood score may not necessarily be relevant to the problem domain. For instance, in our case of discovering AD-expressions, the likelihood score 2 of 𝑝1 = “the government of” happens to be more than 𝑝2 = “I completely disagree”. Clearly, the former is irrelevant for the task of discovering AD-expressions. The reason for this is that likelihood scores or other statistic</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>Blei, D. and Lafferty J. 2009. Visualizing topics with multi-word expressions. Tech. Report. arXiv:0907.1013v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>S Elhadad</author>
</authors>
<title>An Unsupervised Aspect-Sentiment Model for Online Reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the ACL (NAACL-2010).</booktitle>
<contexts>
<context position="7816" citStr="Brody and Elhadad, 2010" startWordPosition="1153" endWordPosition="1156">vi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Furt</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Brody, S. and Elhadad, S. 2010. An Unsupervised Aspect-Sentiment Model for Online Reviews. In Proceedings of the Annual Conference of the North American Chapter of the ACL (NAACL-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burfoot</author>
<author>S Bird</author>
<author>T Baldwin</author>
</authors>
<title>Collective Classification of Congressional Floor-Debate Transcripts.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2001).</booktitle>
<contexts>
<context position="9306" citStr="Burfoot et al. (2011)" startWordPosition="1373" endWordPosition="1376">rosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determ</context>
<context position="27120" citStr="Burfoot et al., (2011)" startWordPosition="4339" endWordPosition="4342">h other in their exchanges, i.e., their pair interaction or arguing nature. This is a relatively new task. We first summarize the closest related works. In (Galley et al., 2004; Hillard et al., 2003; Thomas et al., 2006, Bansal et al., 2008), conversational speeches (i.e., U.S. Congress meeting transcripts) are classified into for or against an issue using various types of features: durational (e.g., time taken by a speaker; speech rate, etc.), structural (e.g., no. of speakers per side, no. of votes cast by a speaker on a bill, etc.), and lexical (e.g., first word, last word, n-grams, etc.). Burfoot et al., (2011) builds on the work of (Thomas et al., 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). However, above works do not discover pair interactions (arguing nature) in debate authors. Online discussion forums are textual rather than conversational (e.g., U.S. Congress meeting transcripts). Thus, the durational, structural, and contextual features used in prior works are not directly applicable. Instead, the model posterior on 𝜃𝑝𝐸 for JTE-P 676 can actually give an estimate of the overall interaction nature of a pair, i.e</context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>Burfoot, C., Bird, S., and Baldwin, T. 2011. Collective Classification of Congressional Floor-Debate Transcripts. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>C Gerrish Wang</author>
<author>S Blei</author>
<author>D</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Neural Information Processing Systems (NIPS-2009).</booktitle>
<marker>Chang, Boyd-Graber, Wang, Blei, D, 2009</marker>
<rawString>Chang, J., Boyd-Graber, J., Wang, C. Gerrish, S. Blei, D. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of the Neural Information Processing Systems (NIPS-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chen</author>
<author>A Mukherjee</author>
<author>B Liu</author>
<author>M Hsu</author>
<author>M Castellanos</author>
<author>R Ghosh</author>
</authors>
<title>Leveraging MultiDomain Prior Knowledge in Topic Models.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Joint Conference in Artificial Intelligence (IJCAI-2013).</booktitle>
<contexts>
<context position="8050" citStr="Chen et al., 2013" startWordPosition="1191" endWordPosition="1194">o related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model whi</context>
</contexts>
<marker>Chen, Mukherjee, Liu, Hsu, Castellanos, Ghosh, 2013</marker>
<rawString>Chen, Z., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M., Ghosh, R. 2013. Leveraging MultiDomain Prior Knowledge in Topic Models. In Proceedings of the International Joint Conference in Artificial Intelligence (IJCAI-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>C Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes (Short Paper).</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2010).</booktitle>
<contexts>
<context position="6158" citStr="Choi and Cardie, 2010" startWordPosition="902" endWordPosition="905"> and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we joint</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Choi, Y. and Cardie, C. 2010. Hierarchical sequential learning for extracting opinions and their attributes (Short Paper). In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Du</author>
<author>W L Buntine</author>
<author>H Jin</author>
</authors>
<title>Sequential Latent Dirichlet Allocation: Discover Underlying Topic Structures within a Document.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM-2010).</booktitle>
<contexts>
<context position="7095" citStr="Du et al., 2010" startWordPosition="1043" endWordPosition="1046">nferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and</context>
</contexts>
<marker>Du, Buntine, Jin, 2010</marker>
<rawString>Du, L., Buntine, W. L., and Jin, H. 2010. Sequential Latent Dirichlet Allocation: Discover Underlying Topic Structures within a Document. In Proceedings of the IEEE International Conference on Data Mining (ICDM-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Erosheva</author>
<author>S Fienberg</author>
<author>J Lafferty</author>
</authors>
<title>Mixed membership models of scientific publications.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences (PNAS-2004).</booktitle>
<contexts>
<context position="8706" citStr="Erosheva et al., 2004" startWordPosition="1286" endWordPosition="1289">s, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011)</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>Erosheva, E., Fienberg, S. and Lafferty, J. 2004. Mixed membership models of scientific publications. In Proceedings of the National Academy of Sciences (PNAS-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2004).</booktitle>
<contexts>
<context position="9065" citStr="Galley et al., 2004" startWordPosition="1338" endWordPosition="1341">teristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee</context>
<context position="26674" citStr="Galley et al., 2004" startWordPosition="4266" endWordPosition="4269">ata as a topical phrase may belong to more than one topic. We selected top 2000 ranked candidate phrases for each topic using 𝑃(𝑅𝑒𝑙 = 1|𝑡, 𝑝) as we feel that is sufficient for a topic. Note that phrases for topics are not as crucial as for AD-expressions because topics can more or less be defined by unigrams. 5 Classifying Pair Interaction Nature We now determine whether two users (also called a user pair) mostly agree or disagree with each other in their exchanges, i.e., their pair interaction or arguing nature. This is a relatively new task. We first summarize the closest related works. In (Galley et al., 2004; Hillard et al., 2003; Thomas et al., 2006, Bansal et al., 2008), conversational speeches (i.e., U.S. Congress meeting transcripts) are classified into for or against an issue using various types of features: durational (e.g., time taken by a speaker; speech rate, etc.), structural (e.g., no. of speakers per side, no. of votes cast by a speaker on a bill, etc.), and lexical (e.g., first word, last word, n-grams, etc.). Burfoot et al., (2011) builds on the work of (Thomas et al., 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on v</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Galley, M., McKeown, K., Hirschberg, J., and Shriberg, E. 2004. Identifying agreement and disagreement in conversational speech: Use of Bayesian networks to model pragmatic dependencies. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences (PNAS-2004).</booktitle>
<contexts>
<context position="29628" citStr="Griffiths and Steyvers, 2004" startWordPosition="4733" endWordPosition="4736">Politics, Religion, Society and Science domains respectively. Experiment Data: As it is not interesting to study pairs who only exchanged a few posts, we restrict to pairs with at least 20 post exchanges. This resulted in 1241 authors and 1461 pairs. The reduced dataset consists of 1095586 tokens (after n-gram preprocessing in §4), 40102 posts with an average of 27 posts or interactions per pair. Data from all 4 domains are combined for modeling. Parameter Settings: For all our experiments, we set the hyper-parameters to the heuristic values aT = 50/T, aE = 50/E, !&apos;T = !&apos;E = 0.1 suggested in (Griffiths and Steyvers, 2004). We set the number of topics, T = 50 and the number of ADexpression types, E = 2 (agreement and disagreement) as in discussion/debate forums, there are usually two expression types5. To learn 5 Values for E &gt; 2 were also tried. However, they did not produce any new dominant expression type. There was also a slight increase in the model perplexity showing that values of E &gt; 2 do not fit the debate forum data well. the Max-Ent parameters A, we randomly sampled 500 terms from the held-out data (10 threads in our corpus which were excluded from the evaluation of tasks in §6.2, §6.3) appearing at </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths, T. and Steyvers, M. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences (PNAS-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Hansen</author>
<author>K Hyunjung</author>
</authors>
<title>Is the media biased against me? A meta-analysis of the hostile media effect research.</title>
<date>2011</date>
<journal>Communication Research Reports,</journal>
<volume>28</volume>
<pages>169--179</pages>
<contexts>
<context position="13237" citStr="Hansen and Hyunjung, 2011" startWordPosition="1992" endWordPosition="1995">s, theism, terrorism, vegetarianism, etc. which are often heated and attract people with pre-determined, strong, and polarized stances1. This observation motivates the generative process of our model. Referring to the notations in Table 1, we explain the generative process of JTE-P. Given a document (post) 𝑑, its author, 𝑎𝑑, and the list of targets to whom 𝑎𝑑 replies/quotes 1 These hardened perspectives are supported by theoretical studies in communications like the polarization effect (Sunstein, 2002), and the hostile media effect, a scenario where partisans rigidly hold on to their stances (Hansen and Hyunjung, 2011). βT ad bd P c φT T W W Z r φE E ψ X θE αE Nd D βE θT P λ αT 673 in 𝑑 , 𝑏𝑑 = [𝑏1 ... 𝑏𝑛] , the document 𝑑 exhibits shared topics and arguing nature of various pairs, 𝑝 = (𝑎𝑑, 𝑐) , where 𝑐 ∈ 𝑏𝑑. More precisely, the pair specific topic and AD-expression distributions (𝜃𝑝𝑇 ; 𝜃𝑝𝐸) “shape” the topics and AD-expressions emitted in 𝑑 as agreement and disagreement on topical viewpoints are directed towards certain target authors. Each topic (𝜑𝑡𝑇) and AD-expression type (𝜑𝑒𝐸) is characterized by a multinomial distribution over terms (words/phrases). Assume we have 𝑡 = 1 ... 𝑇 topics and 𝑒 = 1 ... 𝐸 exp</context>
</contexts>
<marker>Hansen, Hyunjung, 2011</marker>
<rawString>Hansen, G. J., and Hyunjung, K. 2011. Is the media biased against me? A meta-analysis of the hostile media effect research. Communication Research Reports, 28, 169-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2003).</booktitle>
<contexts>
<context position="9088" citStr="Hillard et al., 2003" startWordPosition="1342" endWordPosition="1345">ns by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For t</context>
<context position="26696" citStr="Hillard et al., 2003" startWordPosition="4270" endWordPosition="4273">se may belong to more than one topic. We selected top 2000 ranked candidate phrases for each topic using 𝑃(𝑅𝑒𝑙 = 1|𝑡, 𝑝) as we feel that is sufficient for a topic. Note that phrases for topics are not as crucial as for AD-expressions because topics can more or less be defined by unigrams. 5 Classifying Pair Interaction Nature We now determine whether two users (also called a user pair) mostly agree or disagree with each other in their exchanges, i.e., their pair interaction or arguing nature. This is a relatively new task. We first summarize the closest related works. In (Galley et al., 2004; Hillard et al., 2003; Thomas et al., 2006, Bansal et al., 2008), conversational speeches (i.e., U.S. Congress meeting transcripts) are classified into for or against an issue using various types of features: durational (e.g., time taken by a speaker; speech rate, etc.), structural (e.g., no. of speakers per side, no. of votes cast by a speaker on a bill, etc.), and lexical (e.g., first word, last word, n-grams, etc.). Burfoot et al., (2011) builds on the work of (Thomas et al., 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). However, </context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>Hillard, D., Ostendorf, M., and Shriberg, E. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hassan</author>
<author>D Radev</author>
</authors>
<title>Identifying text polarity using random walks.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2010).</booktitle>
<contexts>
<context position="6134" citStr="Hassan and Radev, 2010" startWordPosition="898" endWordPosition="901">uated both qualitatively and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other</context>
</contexts>
<marker>Hassan, Radev, 2010</marker>
<rawString>Hassan, A. and Radev, D. 2010. Identifying text polarity using random walks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI-1999).</booktitle>
<contexts>
<context position="6845" citStr="Hofmann, 1999" startWordPosition="1004" endWordPosition="1005"> and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, T. 1999. Probabilistic latent semantic analysis. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI-1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004).</booktitle>
<contexts>
<context position="6001" citStr="Hu and Liu, 2004" startWordPosition="879" endWordPosition="882">framework also facilitates tracking users’ ideology shifts and the resulting arguing nature. The proposed methods have been evaluated both qualitatively and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work exp</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jo</author>
<author>A Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Web Search and Data Mining (WSDM-2011).</booktitle>
<contexts>
<context position="7834" citStr="Jo and Oh, 2011" startWordPosition="1157" endWordPosition="1160">persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture th</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Jo, Y. and Oh, A. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the International Conference on Web Search and Data Mining (WSDM-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning,</booktitle>
<editor>B. Schölkopf and C. Burges and A. Smola (ed.), MIT-Press,</editor>
<contexts>
<context position="38584" citStr="Joachims, 1999" startWordPosition="6228" endWordPosition="6229">e have labeled data for 472 pairs, we can treat identifying pair arguing nature as a text classification problem where all interactions between a pair are merged in one document representing the pair along with the label given by judges: agreeing or disagreeing. To compare classification performance, we use two feature sets: (i) standard word + POS 1-4 grams and (ii) AD-expressions from VE. We use TF-IDF as our feature value assignment scheme. We also try two well-known feature selection schemes ChiSquared Test (χ2) and Information Gain (IG). We use the linear kernel6 SVM (SVMlight system in (Joachims, 1999)) as our text classifier. For feature selection using χ2 and IG, we use two settings: top 1% and 2% of all features ranked according to the selection metric. Also, for estimated AD-expressions (according to probabilities in VE ), we experiment with top 1000 and 2000 AD-expressions terms for both agreement and disagreement. We summarize 6 Other kernels polynomial, RBF, and sigmoid did not perform as well. comparison results using 5-fold Cross Validation (CV) with two classes: agreeing and disagreeing in Table 6. JTE-P-posterior represents the method using simply the model posterior on B&apos; to mak</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. Making large-Scale SVM Learning Practical. 1999. Advances in Kernel Methods -Support Vector Learning, B. Schölkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Probabilistic relevance models based on document and query generation. Language Modeling and Information Retrieval.</title>
<date>2003</date>
<contexts>
<context position="21889" citStr="Lafferty and Zhai, 2003" startWordPosition="3441" endWordPosition="3444">g hypothesis: a relevant phrase is one whose unigrams are closely related to (or appear with high probabilities in) the given AD-expression type, 𝑒 : Agreement ( 𝐴𝑔 ) or disagreement (𝐷𝑖𝑠𝐴𝑔). Continuing from the previous example, given the expression type 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔 𝐸 , 𝑝2 is relevant while 𝑝1 is not as “government” and “disagree” are highly unlikely and likely respectively to be clustered in 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔 𝐸. Thus, we want to rank phrases based on 𝑃(𝑅𝑒𝑙 = 1|𝑒,𝑝) where 𝑒 denotes the expression type (Agreement/Disagreement), 𝑝 denotes a candidate phrase. Following the probabilistic relevance model in (Lafferty and Zhai, 2003), we use a similar technique to that in (Zhao et al., 2011) for deriving our relevance ranking function as follows: 𝑃(𝑅𝑒𝑙=1|𝑒,𝑝) 𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) = We further define 𝜀 = 𝑃(𝑅𝑒𝑙=0|𝑒)Without loss of 𝑃(𝑅𝑒𝑙=1|𝑒) generality, one can say that 𝑃(𝑅𝑒𝑙 = 0|𝑒) ≫ 𝑃(𝑅𝑒𝑙 = 1|𝑒) , because there are many more irrelevant phrases than relevant ones, i.e., 𝜀 ≫ 1. Thus, taking log, from equation (3), we get, log 𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) = log � 1 1+𝜀×𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) � ≈ 𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) 𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) × 1 𝜀� = log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) 𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)� − log 𝜀 (4) Thus, our ranking function actually computes the relevance sco</context>
</contexts>
<marker>Lafferty, Zhai, 2003</marker>
<rawString>Lafferty, J. and Zhai, C. 2003. Probabilistic relevance models based on document and query generation. Language Modeling and Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics.</journal>
<contexts>
<context position="24525" citStr="Landis and Koch, 1977" startWordPosition="3889" endWordPosition="3892">at having selected top k candidate n-grams based on the proposed relevance ranking, we want to get an estimate of how many relevant terms from a sample of the collection were covered. To compute coverage, we randomly sampled 500 documents from the corpus and listed the candidate n-grams3 in the collection of sampled 500 documents. For this and subsequent human judgment tasks, we use two judges (graduate students well versed in English). We asked our judges to mark all relevant AD-expressions. Agreement study yielded xCohen = 0.77 showing substantial agreement according to scale 4 provided in (Landis and Koch, 1977). This is understandable as identifying AD-expressions is a relatively easy task. Finally, a term was considered to be relevant if both judges marked it so. We then computed the coverage to see how many of the relevant terms in the random sample were also present in top k phrases from the ranked candidate n-grams. We summarize the 3 These are terms appearing at least 20 times in the entire collection. We do this for computational reasons as there can be many n-grams and n-grams with very low frequency are less likely to be relevant. 4 No agreement (x &lt; 0), slight agreement (0 &lt; x ≤ 0.2), fair </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R. and Koch, G. G. 1977. The measurement of observer agreement for categorical data. Biometrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>Y He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Knowledge Management (CIKM-2009).</booktitle>
<contexts>
<context position="7791" citStr="Lin and He, 2009" startWordPosition="1149" endWordPosition="1152">ic topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span </context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Lin, C. and He, Y. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of the International Conference on Knowledge Management (CIKM-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Lin</author>
<author>A Hauptmann</author>
</authors>
<title>Are these documents written from different perspectives?: a test of different perspectives based on statistical distribution divergence.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2006).</booktitle>
<contexts>
<context position="9572" citStr="Lin and Hauptmann, 2006" startWordPosition="1410" endWordPosition="1413">t al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence o</context>
</contexts>
<marker>Lin, Hauptmann, 2006</marker>
<rawString>Lin, W. H., and Hauptmann, A. 2006. Are these documents written from different perspectives?: a test of different perspectives based on statistical distribution divergence. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publisher, USA.</publisher>
<contexts>
<context position="4001" citStr="Liu, 2012" startWordPosition="589" endWordPosition="590">ent and disagreement clearly express a kind of sentiment as well. They are usually emitted during interactive exchanges of arguments in ideological discussions. This idea prompted us to introduce the concept of ADsentiment. We define the polarity of agreement expressions as positive and the polarity of disagreement expressions as negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover </context>
<context position="8029" citStr="Liu, 2012" startWordPosition="1189" endWordPosition="1190">r pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the</context>
<context position="9623" citStr="Liu, 2012" startWordPosition="1418" endWordPosition="1419">) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence of expression types (agreement and disagreement), to</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Liu, B. 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publisher, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>X Wang</author>
<author>A Corrada-Emmanuel</author>
</authors>
<title>Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="7290" citStr="McCallum et al., 2007" startWordPosition="1072" endWordPosition="1075">o related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and senti</context>
</contexts>
<marker>McCallum, Wang, Corrada-Emmanuel, 2007</marker>
<rawString>McCallum, A., Wang, X., and Corrada-Emmanuel, A. 2007. Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Ling</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on World Wide Web (WWW-2007).</booktitle>
<contexts>
<context position="7919" citStr="Mei et al., 2007" startWordPosition="1170" endWordPosition="1173">r, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the International Conference on World Wide Web (WWW-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Expertise modeling for matching papers with reviewers.</title>
<date>2007</date>
<booktitle>In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2007).</booktitle>
<contexts>
<context position="7252" citStr="Mimno and McCallum, 2007" startWordPosition="1066" endWordPosition="1069">s research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs sepa</context>
</contexts>
<marker>Mimno, McCallum, 2007</marker>
<rawString>Mimno, D. and McCallum, A. 2007. Expertise modeling for matching papers with reviewers. In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>V Venkataraman</author>
<author>B Liu</author>
<author>S Meraz</author>
</authors>
<title>Public Dialogue: Analysis of Tolerance in Online Discussions.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2013).</booktitle>
<contexts>
<context position="9516" citStr="Mukherjee et al., 2013" startWordPosition="1403" endWordPosition="1406">put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervi</context>
</contexts>
<marker>Mukherjee, Venkataraman, Liu, Meraz, 2013</marker>
<rawString>Mukherjee, A., Venkataraman, V., Liu, B., Meraz, S. 2013. Public Dialogue: Analysis of Tolerance in Online Discussions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Mining Contentions from Discussions and Debates.</title>
<date>2012</date>
<booktitle>Proceedings of SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-2012).</booktitle>
<contexts>
<context position="4346" citStr="Mukherjee and Liu, 2012" startWordPosition="638" endWordPosition="641">negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge</context>
<context position="8029" citStr="Mukherjee and Liu, 2012" startWordPosition="1187" endWordPosition="1190">tures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the</context>
<context position="9623" citStr="Mukherjee and Liu, 2012" startWordPosition="1416" endWordPosition="1419"> Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence of expression types (agreement and disagreement), to</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and Liu, B. 2012a. Mining Contentions from Discussions and Debates. Proceedings of SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Aspect Extraction through Semi-Supervised Modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</booktitle>
<contexts>
<context position="4346" citStr="Mukherjee and Liu, 2012" startWordPosition="638" endWordPosition="641">negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge</context>
<context position="8029" citStr="Mukherjee and Liu, 2012" startWordPosition="1187" endWordPosition="1190">tures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the</context>
<context position="9623" citStr="Mukherjee and Liu, 2012" startWordPosition="1416" endWordPosition="1419"> Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence of expression types (agreement and disagreement), to</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and Liu, B. 2012b. Aspect Extraction through Semi-Supervised Modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Analysis of Linguistic Style Accommodation in Online Debates.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING-2012).</booktitle>
<contexts>
<context position="4346" citStr="Mukherjee and Liu, 2012" startWordPosition="638" endWordPosition="641">negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge</context>
<context position="8029" citStr="Mukherjee and Liu, 2012" startWordPosition="1187" endWordPosition="1190">tures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the</context>
<context position="9623" citStr="Mukherjee and Liu, 2012" startWordPosition="1416" endWordPosition="1419"> Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence of expression types (agreement and disagreement), to</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and Liu, B. 2012c. Analysis of Linguistic Style Accommodation in Online Debates. In Proceedings of the International Conference on Computational Linguistics (COLING-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>B Liu</author>
</authors>
<title>Modeling Review Comments.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</booktitle>
<contexts>
<context position="4346" citStr="Mukherjee and Liu, 2012" startWordPosition="638" endWordPosition="641">negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge</context>
<context position="8029" citStr="Mukherjee and Liu, 2012" startWordPosition="1187" endWordPosition="1190">tures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the</context>
<context position="9623" citStr="Mukherjee and Liu, 2012" startWordPosition="1416" endWordPosition="1419"> Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence of expression types (agreement and disagreement), to</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Mukherjee, A. and Liu, B. 2012d. Modeling Review Comments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Murakami</author>
<author>R Raymond</author>
</authors>
<title>Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (Coling-2010).</booktitle>
<contexts>
<context position="9015" citStr="Murakami and Raymond (2010)" startWordPosition="1329" endWordPosition="1332">an different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and L</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>Murakami A. and Raymond, R. 2010. Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions. In Proceedings of the International Conference on Computational Linguistics (Coling-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</title>
<date>2008</date>
<contexts>
<context position="4288" citStr="Pang and Lee, 2008" startWordPosition="628" endWordPosition="631">tive and the polarity of disagreement expressions as negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because t</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, B. and Lee, L. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2005).</booktitle>
<contexts>
<context position="6085" citStr="Popescu and Etzioni, 2005" startWordPosition="892" endWordPosition="895">g arguing nature. The proposed methods have been evaluated both qualitatively and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topi</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Popescu, A. and Etzioni, O. 2005. Extracting product features and opinions from reviews. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</booktitle>
<contexts>
<context position="7061" citStr="Ramage et al., 2009" startWordPosition="1037" endWordPosition="1040">while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering pos</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Ramage, D., Hall, D., Nallapati, R, Manning, C. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smith</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI-2004).</booktitle>
<contexts>
<context position="7209" citStr="Rosen-Zvi et al., 2004" startWordPosition="1060" endWordPosition="1063">, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhada</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smith, 2004</marker>
<rawString>Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smith, P. 2004. The author-topic model for authors and documents. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Sunstein</author>
</authors>
<title>The law of group polarization.</title>
<date>2002</date>
<journal>Journal of</journal>
<note>political philosophy.</note>
<contexts>
<context position="13118" citStr="Sunstein, 2002" startWordPosition="1975" endWordPosition="1976">other across various topics or threads. We believe this is because our data consists of topics like elections, theism, terrorism, vegetarianism, etc. which are often heated and attract people with pre-determined, strong, and polarized stances1. This observation motivates the generative process of our model. Referring to the notations in Table 1, we explain the generative process of JTE-P. Given a document (post) 𝑑, its author, 𝑎𝑑, and the list of targets to whom 𝑎𝑑 replies/quotes 1 These hardened perspectives are supported by theoretical studies in communications like the polarization effect (Sunstein, 2002), and the hostile media effect, a scenario where partisans rigidly hold on to their stances (Hansen and Hyunjung, 2011). βT ad bd P c φT T W W Z r φE E ψ X θE αE Nd D βE θT P λ αT 673 in 𝑑 , 𝑏𝑑 = [𝑏1 ... 𝑏𝑛] , the document 𝑑 exhibits shared topics and arguing nature of various pairs, 𝑝 = (𝑎𝑑, 𝑐) , where 𝑐 ∈ 𝑏𝑑. More precisely, the pair specific topic and AD-expression distributions (𝜃𝑝𝑇 ; 𝜃𝑝𝐸) “shape” the topics and AD-expressions emitted in 𝑑 as agreement and disagreement on topical viewpoints are directed towards certain target authors. Each topic (𝜑𝑡𝑇) and AD-expression type (𝜑𝑒𝐸) is charac</context>
</contexts>
<marker>Sunstein, 2002</marker>
<rawString>Sunstein, C. R. 2002. The law of group polarization. Journal of political philosophy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP-2009).</booktitle>
<contexts>
<context position="2503" citStr="Somasundaran and Wiebe, 2009" startWordPosition="367" endWordPosition="370">cations in the fields of communication and political science as they give social scientists an opportunity to study real-life discussions/debates of almost any issue and analyze participant behaviors in a large scale. In this paper, we present such an application, which aims to perform fine-grained analysis of user-interactions in online discussions. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al., 2003), and classifying user stances (Somasundaran and Wiebe, 2009). However, these works are at a rather coarser level and have not considered more fine-grained characteristics of debates/discussions where users interact with each other by quoting/replying each other to express agreement or disagreement and argue with one another. In this work, we want to mine the following information: 1. The nature of interaction of each pair of users or participants who have engaged in the discussion of certain issues, i.e., whether the two persons mostly agree or disagree with each other in their interactions. 2. What language expressions are often used to express agreem</context>
<context position="9239" citStr="Somasundaran and Wiebe (2009)" startWordPosition="1361" endWordPosition="1364">(2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression min</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Somasundaran, S. and Wiebe, J. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on World Wide Web (WWW-2008).</booktitle>
<contexts>
<context position="7030" citStr="Titov and McDonald, 2008" startWordPosition="1032" endWordPosition="1035">timent terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/debates. Topic models like pLSA (Hofmann, 1999) and LDA (Blei et al., 2003) have proved to be very successful in mining topics from large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al., 2009), and sequential (Du et al., 2010) topic models. Yet other approaches extend topic models to produce author specific topics (Rosen-Zvi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentim</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Titov, I. and R. McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the International Conference on World Wide Web (WWW-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2006).</booktitle>
<contexts>
<context position="9261" citStr="Thomas et al. (2006)" startWordPosition="1365" endWordPosition="1368">model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of </context>
<context position="26717" citStr="Thomas et al., 2006" startWordPosition="4274" endWordPosition="4277">than one topic. We selected top 2000 ranked candidate phrases for each topic using 𝑃(𝑅𝑒𝑙 = 1|𝑡, 𝑝) as we feel that is sufficient for a topic. Note that phrases for topics are not as crucial as for AD-expressions because topics can more or less be defined by unigrams. 5 Classifying Pair Interaction Nature We now determine whether two users (also called a user pair) mostly agree or disagree with each other in their exchanges, i.e., their pair interaction or arguing nature. This is a relatively new task. We first summarize the closest related works. In (Galley et al., 2004; Hillard et al., 2003; Thomas et al., 2006, Bansal et al., 2008), conversational speeches (i.e., U.S. Congress meeting transcripts) are classified into for or against an issue using various types of features: durational (e.g., time taken by a speaker; speech rate, etc.), structural (e.g., no. of speakers per side, no. of votes cast by a speaker on a bill, etc.), and lexical (e.g., first word, last word, n-grams, etc.). Burfoot et al., (2011) builds on the work of (Thomas et al., 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). However, above works do not di</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Thomas, M., Pang, B., and Lee, L. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tomokiyo</author>
<author>M Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="19076" citStr="Tomokiyo and Hurst, 2003" startWordPosition="3000" endWordPosition="3003">sting approaches. Topics in most topic models like LDA are usually unigram distributions. This offers a great computational advantage compared to more complex models which consider word ordering (Wallach, 2006; Wang et al., 2007). This thread of research models bigrams by encoding them into the generative process. For each word, a topic is sampled first, then its status as a unigram or bigram is sampled, and finally the word is sampled from a topic-specific unigram or bigram distribution. This method, however, is expensive computationally and has a limitation for arbitrary length n-grams. In (Tomokiyo and Hurst, 2003), a language model approach is used for bigram phrase extraction. Yet another thread of research post-processes the discovered topical unigrams to form multiword phrases using likelihood scores (Blei and Lafferty, 2009). This approach considers adjacent word pairs and identifies n-grams which occur much more often than one would expect by chance alone by computing likelihood ratios. While this is reasonable, a significant n-gram with high likelihood score may not necessarily be relevant to the problem domain. For instance, in our case of discovering AD-expressions, the likelihood score 2 of 𝑝1</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>Tomokiyo, T., and Hurst, M. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
</authors>
<title>Topic modeling: Beyond bag of words.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML-2006).</booktitle>
<contexts>
<context position="18660" citStr="Wallach, 2006" startWordPosition="2935" endWordPosition="2936">)¬𝑘+𝐸𝛼𝐸 674 4 Phrase Ranking based on Relevance We now detail our method of pre-processing ngrams (phrases) based on relevance to select a subset of highly relevant n-grams for model building. This has two advantages: (i). A large number of irrelevant n-grams slow inference. (ii). Filtering irrelevant terms in the vocabulary improves the quality of AD-expressions. Before proceeding, we review some existing approaches. Topics in most topic models like LDA are usually unigram distributions. This offers a great computational advantage compared to more complex models which consider word ordering (Wallach, 2006; Wang et al., 2007). This thread of research models bigrams by encoding them into the generative process. For each word, a topic is sampled first, then its status as a unigram or bigram is sampled, and finally the word is sampled from a topic-specific unigram or bigram distribution. This method, however, is expensive computationally and has a limitation for arbitrary length n-grams. In (Tomokiyo and Hurst, 2003), a language model approach is used for bigram phrase extraction. Yet another thread of research post-processes the discovered topical unigrams to form multiword phrases using likeliho</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Wallach, H. 2006. Topic modeling: Beyond bag of words. In Proceedings of the International Conference on Machine Learning (ICML-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>A McCallum</author>
<author>X Wei</author>
</authors>
<title>Topical Ngrams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM-2007).</booktitle>
<contexts>
<context position="18680" citStr="Wang et al., 2007" startWordPosition="2937" endWordPosition="2940">hrase Ranking based on Relevance We now detail our method of pre-processing ngrams (phrases) based on relevance to select a subset of highly relevant n-grams for model building. This has two advantages: (i). A large number of irrelevant n-grams slow inference. (ii). Filtering irrelevant terms in the vocabulary improves the quality of AD-expressions. Before proceeding, we review some existing approaches. Topics in most topic models like LDA are usually unigram distributions. This offers a great computational advantage compared to more complex models which consider word ordering (Wallach, 2006; Wang et al., 2007). This thread of research models bigrams by encoding them into the generative process. For each word, a topic is sampled first, then its status as a unigram or bigram is sampled, and finally the word is sampled from a topic-specific unigram or bigram distribution. This method, however, is expensive computationally and has a limitation for arbitrary length n-grams. In (Tomokiyo and Hurst, 2003), a language model approach is used for bigram phrase extraction. Yet another thread of research post-processes the discovered topical unigrams to form multiword phrases using likelihood scores (Blei and </context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Wang, X., McCallum, A., Wei, X. 2007. Topical Ngrams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the IEEE International Conference on Data Mining (ICDM-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proc. of National Conference on AI (AAAI-2000).</booktitle>
<contexts>
<context position="6198" citStr="Wiebe, 2000" startWordPosition="909" endWordPosition="910">ife discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we jointly model several aspects of discussions/</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Wiebe, J. 2000. Learning subjective adjectives from corpora. In Proc. of National Conference on AI (AAAI-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yano</author>
<author>W Cohen</author>
<author>N Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the N. American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2009).</booktitle>
<contexts>
<context position="8616" citStr="Yano et al., (2009)" startWordPosition="1271" endWordPosition="1274">roposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasu</context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Yano, T., Cohen, W. and Smith, N. 2009. Predicting response to political blog posts with topic models. In Proceedings of the N. American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhao</author>
<author>J Jiang</author>
<author>J He</author>
<author>Y Song</author>
<author>P Achananuparp</author>
<author>E P LiM</author>
<author>X Li</author>
</authors>
<title>Topical keyphrase extraction from twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2011).</booktitle>
<contexts>
<context position="21948" citStr="Zhao et al., 2011" startWordPosition="3453" endWordPosition="3456"> related to (or appear with high probabilities in) the given AD-expression type, 𝑒 : Agreement ( 𝐴𝑔 ) or disagreement (𝐷𝑖𝑠𝐴𝑔). Continuing from the previous example, given the expression type 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔 𝐸 , 𝑝2 is relevant while 𝑝1 is not as “government” and “disagree” are highly unlikely and likely respectively to be clustered in 𝜑𝑒=𝐷𝑖𝑠𝐴𝑔 𝐸. Thus, we want to rank phrases based on 𝑃(𝑅𝑒𝑙 = 1|𝑒,𝑝) where 𝑒 denotes the expression type (Agreement/Disagreement), 𝑝 denotes a candidate phrase. Following the probabilistic relevance model in (Lafferty and Zhai, 2003), we use a similar technique to that in (Zhao et al., 2011) for deriving our relevance ranking function as follows: 𝑃(𝑅𝑒𝑙=1|𝑒,𝑝) 𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) = We further define 𝜀 = 𝑃(𝑅𝑒𝑙=0|𝑒)Without loss of 𝑃(𝑅𝑒𝑙=1|𝑒) generality, one can say that 𝑃(𝑅𝑒𝑙 = 0|𝑒) ≫ 𝑃(𝑅𝑒𝑙 = 1|𝑒) , because there are many more irrelevant phrases than relevant ones, i.e., 𝜀 ≫ 1. Thus, taking log, from equation (3), we get, log 𝑃(𝑅𝑒𝑙 = 1|𝑒, 𝑝) = log � 1 1+𝜀×𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) � ≈ 𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) 𝑃(𝑝|𝑅𝑒𝑙=0,𝑒) × 1 𝜀� = log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) 𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)� − log 𝜀 (4) Thus, our ranking function actually computes the relevance score log �𝑃(𝑝|𝑅𝑒𝑙=1,𝑒) 𝑃(𝑝|𝑅𝑒𝑙=0,𝑒)�. The last term, log𝜀 bei</context>
</contexts>
<marker>Zhao, Jiang, He, Song, Achananuparp, LiM, Li, 2011</marker>
<rawString>Zhao, X., J. Jiang, J. He, Y. Song, P. Achananuparp, E.P. LiM, and X. Li. 2011. Topical keyphrase extraction from twitter. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhao</author>
<author>J Jiang</author>
<author>H Yan</author>
<author>X Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a MaxEntLDA hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010).</booktitle>
<contexts>
<context position="7939" citStr="Zhao et al., 2010" startWordPosition="1174" endWordPosition="1177"> not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model</context>
<context position="14592" citStr="Zhao et al., 2010" startWordPosition="2241" endWordPosition="2244">pression types: agreement and disagreement (more details in §6.1). Like most generative models for text, a post (document) is viewed as a bag of n-grams and each n-gram (word/phrase) takes one value from a predefined vocabulary. In this work, we use up to 4-grams, i.e., n = 1, 2, 3, 4. Instead of using all n-grams, a relevance based ranking method is proposed to select a subset of highly relevant n-grams for model building (details in §4). For notational convenience, we use terms to denote both words (unigrams) and phrases (n-grams). JTE-P is a switching graphical model (Ahmed and Xing, 2010; Zhao et al., 2010) performing a switch between AD-expressions and topics. 𝜓𝑑,𝑗 denotes the distribution over topics and ADexpressions with 𝑟𝑑,𝑗 ∈ {𝑡̂, 𝑒̂} denoting the binary indicator/switch variable (topic or ADexpression) for the 𝑗 th term of 𝑑 , 𝑤𝑑,𝑗 . To perform the switch we use a maximum entropy (Max-Ent) model. The idea is motivated by the observation that topical and AD-expression terms usually play different roles in a sentence. Topical terms (e.g., “elections” and “income tax”) tend to be noun and noun phrases while AD-expression terms (“I refute”, “how can you say”, and “probably agree”) usually con</context>
<context position="31906" citStr="Zhao et al., 2010" startWordPosition="5112" endWordPosition="5115">r, perplexity does not reflect our purpose since we are not trying to evaluate how well the ADexpressions in an unseen discussion data fit our learned models. Instead our focus is to evaluate how well our learned AD-expression types perform in clustering semantic phrases of agreement/disagreement. Since AD-expressions (according to top terms in V&apos;) produced by JTEP are rankings, we choose precision @ n (p@n) as our metric. p@n is commonly used to evaluate a ranking when the total number of correct items is unknown (e.g., Web search results, aspect terms in topic models for sentiment analysis (Zhao et al., 2010), etc.). This situation is similar to our AD-expression rankings, V1. Further, as V&apos;—Dir, the Dirichlet smoothing effect ensures that every term in the vocabulary has some nonzero mass to agreement or disagreement expression type. Thus, it is the ranking of terms in each AD-expression type that matters (i.e., whether the model is able to rank highly relevant terms at the top). The above method evaluates the original ranking. Another way of evaluating the ADexpression rankings is to evaluate only those newly discovered terms, i.e., beyond those 677 𝐸 ) Disagreement expressions (𝜑𝑒=𝐷𝑖𝑠𝑎𝑔𝑟𝑒𝑒𝑚𝑒𝑛𝑡 </context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Zhao, X., Jiang, J., Yan, H., and Li, X. 2010. Jointly modeling aspects and opinions with a MaxEntLDA hybrid. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>