<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995465">
Descriptive and Empirical Approaches to Capturing Underlying
Dependencies among Parsing Errors
</title>
<author confidence="0.999812">
Tadayoshi Hara&apos; Yusuke Miyao&apos; Jun’ichi Tsujii&apos;,2,3
</author>
<affiliation confidence="0.959191333333333">
&apos;Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
</affiliation>
<address confidence="0.594708">
3NaCTeM (National Center for Text Mining)
</address>
<email confidence="0.995745">
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.982817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986705882353">
In this paper, we provide descriptive and
empirical approaches to effectively ex-
tracting underlying dependencies among
parsing errors. In the descriptive ap-
proach, we define some combinations of
error patterns and extract them from given
errors. In the empirical approach, on the
other hand, we re-parse a sentence with
a target error corrected and observe er-
rors corrected together. Experiments on
an HPSG parser show that each of these
approaches can clarify the dependencies
among individual errors from each point
of view. Moreover, the comparison be-
tween the results of the two approaches
shows that combining these approaches
can achieve a more detailed error analysis.
</bodyText>
<sectionHeader confidence="0.995168" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943153846154">
For any kind of technology, analyzing causes of
errors given by a system is a very helpful process
for improving its performance. In recent sophisti-
cated parsing technologies, the process has taken
on more and more important roles since critical
ideas for parsing performance have already been
introduced and the researches are now focusing on
exploring the rest of the pieces for making addi-
tional improvements.
In most cases for parsers’ error analysis, re-
searchers associate output errors with failures in
handling certain linguistic phenomena and attempt
to avoid them by adding or modifying correspond-
ing settings of their parsers. However, such an
analysis cannot been done so smoothly since pars-
ing errors sometimes depend on each other and the
underlying dependencies behind superficial phe-
nomena cannot be captured easily.
In this paper, we propose descriptive and em-
pirical approaches to effective extraction of de-
pendencies among parsing errors and engage in a
deeper error analysis with them. In our descriptive
approach, we define various combinations of error
patterns as organized error phenomena on the ba-
sis of linguistic knowledge, and then extract such
combinations from given errors. In our empirical
approach, on the other and, we re-parse a sentence
under the condition where a target error is cor-
rected, and errors which are additionally corrected
are regarded as dependent errors. By capturing de-
pendencies among parsing errors through system-
atic approaches, we can effectively collect errors
which are related to the same linguistic properties.
In the experiments, we applied both of our ap-
proaches to an HPSG parser Enju (Miyao and Tsu-
jii, 2005; Ninomiya et al., 2006), and then evalu-
ated the obtained error classes. After examining
the individual approaches, we explored the com-
bination of them.
</bodyText>
<sectionHeader confidence="0.649691" genericHeader="introduction">
2 Parser and its evaluation
</sectionHeader>
<bodyText confidence="0.9966745">
A parser is a system which interprets structures
of given sentences from some grammatical or in
some cases semantical viewpoints, and interpreted
structures are utilized as essential information for
various natural language tasks such as informa-
tion extraction, machine translation, and so on.
In most cases, an output structure of a parser is
based on a certain grammatical framework such as
CFG, CCG (Steedman, 2000), LFG (Kaplan and
Bresnan, 1995) or HPSG (Pollard and Sag, 1994).
Since such a framework can usually produce more
than one probable structure for a sentence, a parser
</bodyText>
<page confidence="0.344619">
1162
</page>
<note confidence="0.9995395">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162–1171,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999262">
Figure 1: Predicate argument relations
</figureCaption>
<figure confidence="0.9893259375">
Abbr. Full
lgs logical subject
coord coordination
conj conjunction
argN,... take argument(s)
adj adjunction
mod modify a word
relative relative
Table 1: Descriptions for predicate types
ARG1 ARG2
Correct answer: I watched the girl on TV
ARG1 ARG2
ARG1 ARG2
Parser output: I watched the girl on TV
ARG1 ARG2
Obtain inconsistent outputs as errors
ARG1
Error: I watched the girl on TV
Error
ARG1
Abbr. Full
aux auxiliary
verb verb
prep prepositional
det determiner
app apposition
(Neth, ...)
John
ARG1 ARG2
has : aux_arg12
ARG1
come : verb_arg1
</figure>
<bodyText confidence="0.999941916666666">
often utilizes some kind of disambiguation model
for choosing the best one.
While various parsers take different manners
in capturing linguistic phenomena based on their
frameworks, they are at least required to obtain
some kinds of relations between the words in sen-
tences. On the basis of the requirements, a parser
is usually evaluated on how correctly it gives in-
tended linguistic relations. “Predicate argument
relation” is one of the most common evaluation
measurements for a parser since it is a very fun-
damental linguistic behavior and is less dependent
on parser systems. This measure divides linguis-
tic structural phenomena in a sentence into min-
imal predicative events. In one predicate argu-
ment relation, a word which represents an event
(predicate) takes some words as participants (argu-
ments). Although no fixed formulation exists for
the relations, there are to a large extent common
conceptions for them based on linguistic knowl-
edge among researchers.
Figure 1 shows an example of predicate argu-
ment relations given by Enju. In the sentence
“John has come.”, “has” is a predicate of type
“aux arg12” and takes “John” and “come” as the
first and second arguments. “come” is also a pred-
icate of the type “verb arg1” and takes “John” as
the first and the only argument. In this formalism,
each predicate type is represented as a combina-
tion of “the grammatical nature of a word” and
“the arguments which it takes,” which are repre-
sented by the descriptions in Table 1. “aux arg12”
in Figure 1 indicates that it is an auxiliary word
and takes two arguments “ARG1” and “ARG2.”
In order to improve the performance of a parser,
analyzing parsing errors is very much worth the
</bodyText>
<figureCaption confidence="0.999871">
Figure 2: An example of parsing errors
Figure 3: Co-occurring parsing errors
</figureCaption>
<bodyText confidence="0.999890481481482">
effort. Since the errors are output according to
a given evaluation measurement such as “predi-
cate argument relation,” we researchers carefully
explore them and infer the linguistic phenom-
ena which cause the erroneous outputs. Figure 2
shows an example of parsing errors for sentence “I
watched the girl on TV.” Note that the errors are
based on predicate argument relations as shown
above and that the predicate types are abbreviated
in this figure. When we focus on the error output,
we can observe that “ARG1” of predicate “on”
was mistaken by the parser. In this case, “ARG1”
represents a modifiee of the preposition, and we
then conclude that the ill attachment of a prepo-
sitional phrase caused this error. By continuing
such error analysis, weak points of the parser are
revealed and can be useful clues for further im-
provements.
However, in most researches on parsing tech-
nologies, error analysis has been limited to narrow
and shallow explorations since there are various
dependencies behind erroneous outputs. In Fig-
ure 3, for example, two errors were given: wrong
outputs for “ARG1” of “which” and “ARG2” of
“read.” Both of these two errors originated from
the fact that the relative clause took a wrong an-
tecedent “the shelf.” In this sentence, the former
</bodyText>
<figure confidence="0.998970954545454">
ARG1
ARG2
Error: The book on the shelf which I read yesterday
ARG2
ARG1
1163
Can each error occur independently?
Error:
ARG1
ARG1
ARG1
ARG1
Analysis 2: (Impossible)
Analysis 1: (Possible)
They completed the sale of it to him for
They completed the sale of it to him for
ARG1 ARG1
ARG1
ARG1
ARG1
$1,000
They completed the sale of it to him for $1,000
ARG1
ARG1
ARG1
Confliction
$1,000
Erroneous phenomena
[Argument selection]
Prepositional attachment
Adjunction attachment
Conjunction attachment
Head selection for
noun phrase
Coordination
[Predicate type selection]
Preposition/Adjunction
Gerund acts as modifier/not
Coordination/conjunction
# of arguments
for preposition
Adjunction/adjunctive noun
[More structural errors]
To-infinitive for
modifier/argument of verb
Subject for passive sentence
or not
[Others]
Comma
Relative clause attachment
Matched patterns
ARG1 ofprep arg
ARG1 of adj arg
ARG1 of conj arg
ARG1 of det arg
ARG1/2 of coord arg
prep arg / adj arg
verb mod arg / verb arg
coord arg / conj arg
prep argX / prep argY
(X 54 1&apos;)
adj arg /noun arg
see Figure 7
see Figure 8
any error around “,”
see Figure 9
</figure>
<figureCaption confidence="0.999964">
Figure 4: Sketch of error propagation
</figureCaption>
<bodyText confidence="0.99966675">
“ARG1” directly corresponds to the antecedent
while the latter “ARG2” indirectly referred to the
same antecedent as the object of the verb “read.”
The two predicate argument relations thus took the
same word as their common arguments, and there-
fore the two errors co-occurred.
On the other hand, one-way inductive relations
also exist among errors. In Figure 4, “ARG1” of
“for” and “to” were mistaken by a parser. We can
know that each of the errors was caused by an ill
attachment of a prepositional phrase with the same
analysis as shown in Figure 2. What is important
in this example is the manner in their occurrences.
The former error can appear by itself (Analysis 1)
while the latter cannot because of the structural
conflict with the former error (Analysis 2). The
appearance of the latter error thus induces that of
the former error. In error analysis, we have to cor-
rectly capture such various relations, which leads
us to a costly and less rewarding analysis.
In order to make advancements on this prob-
lem, we propose two types of approaches to real-
izing a deeper error analysis on parsing. In the ex-
periments, we examine our approaches for actual
errors which are given by the HPSG parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al., 2006).
Enju was developed for capturing detailed syntac-
tic or semantic properties and relations for a sen-
tence with an HPSG framework (Pollard and Sag,
1994). In this research, we focus on error analysis
based on predicate argument relations, and in the
experiments with Enju, utilize the relations which
</bodyText>
<tableCaption confidence="0.973939">
Table 2: Patterns defined for descriptive approach
</tableCaption>
<bodyText confidence="0.452681">
are represented in parsed tree structures.
</bodyText>
<sectionHeader confidence="0.724678" genericHeader="method">
3 Two approaches for error analysis
</sectionHeader>
<bodyText confidence="0.999964666666667">
In this section, we propose two approaches for er-
ror analysis which enable us to capture underlying
dependencies among parsing errors. Our descrip-
tive approach matches the patterns of error com-
binations with given parsing errors and collects
matched erroneous participants. Our empirical ap-
proach, on the other hand, detects co-occurring
errors by re-parsing a sentence under a situation
where each of the errors is forcibly corrected.
</bodyText>
<subsectionHeader confidence="0.999367">
3.1 Descriptive approach
</subsectionHeader>
<bodyText confidence="0.999972882352941">
Our descriptive approach for capturing dependen-
cies among parsing errors is to extract certain rep-
resentative structures of errors and collect the er-
rors which involve them. Parsing errors have a ten-
dency to occur with certain patterns of structures
representing linguistic phenomena. We first define
such patterns through observations with a part of
error outputs, and then match them with the rest.
Table 2 summarizes the patterns for erroneous
phenomena which we defined for matching in
the experiments. In the table, the patterns for
14 phenomena are given and classified into four
types according to their matching manners. Each
of the patterns for “Argument selection” examine
whether a focused argument for a certain predi-
cate type is erroneous or not. Figure 5 shows the
pattern for “Prepositional attachment,” which col-
</bodyText>
<figure confidence="0.890271666666667">
1164
Example:
Correct output:
</figure>
<figureCaption confidence="0.975971">
Figure 5: Pattern for “Prepositional attachment”
</figureCaption>
<figure confidence="0.996979">
Example:
a package for them
a package for them
</figure>
<figureCaption confidence="0.810689">
Figure 6: Pattern for “Gerund acts as modifier or
not”
</figureCaption>
<bodyText confidence="0.999924434782609">
lects wrong ARG1 for predicate type “prep arg”.
From the sentence in the figure, we can obtain
two errors for “Prepositional attachment” around
prepositions “to” and “for.” On the other hand,
each “Predicate type selection” pattern collects er-
rors around a word whose predicate type is erro-
neous. Figure 6 shows the pattern for “Gerund
acts as modifier or not,” which collects errors
around gerunds whose predicate types are erro-
neous. From the example sentence in the figure,
we can obtain an erroneous predicate type for “ex-
pecting” and collect errors around it for “Gerund
acts as modifier or not.”
We can implement more structural errors than
simple argument or predicate type selections. Fig-
ures 7 and 8 show the patterns for “To-infinitive
for modifier/argument of verb” and “Subject for
passive sentence or not” respectively. The pat-
tern for the latter phenomenon collects errors on
recognitions of prepositional phrases which be-
have as subjects for passive expressions. The pat-
tern collects errors not only around prepositions
but also around the verbs which take the preposi-
</bodyText>
<figure confidence="0.413522">
to :
</figure>
<figureCaption confidence="0.7927485">
Figure 7: Pattern for “To-infinitive for modi-
fier/argument of verb”
</figureCaption>
<figure confidence="0.894083">
Example:
</figure>
<figureCaption confidence="0.9752545">
Figure 8: Pattern for “Subject for passive sentence
or not”
</figureCaption>
<bodyText confidence="0.9993354">
tional phrases as a subject.
Since these patterns are based on linguistic
knowledge given by a human, the process could
provide a relatively precise analysis with a lower
cost than a totally manual analysis.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Empirical approach
</subsectionHeader>
<bodyText confidence="0.999946357142857">
Our empirical approach, on the other hand, briefly
traces the parsing process which results in each of
the target errors. We collect co-occurring errors
as strongly relevant ones, and then extract depen-
dencies among the obtained groups. Parsing errors
could originate from wrong processing at certain
stages in the parsing, and errors with a common
origin would by necessity appear together. We re-
parse a target sentence under the condition where a
certain error is forcibly corrected and then collect
errors which are corrected together as the “rela-
tive” ones. An error group where all errors are
relative to each other can be regarded as a “co-
occurring error group.” Errors in the same co-
</bodyText>
<figure confidence="0.999278039603961">
prep_arg
ARG1 Error
Pattern:
ARG1
ARG1
Parser output:
They completed
the sale of
prep_arg12
$1,000
$1,000
it to :
of
him
They completed the sale
ARG1
ARG1
prep_arg12
it to :
Parser output: ...
him
for : prep_arg12
for : prep_arg12
Pattern: (Patterns of correct answer and parser output can be interchanged)
Correct answer: gerund: verb_mod_arg
Parser output: gerund: verb_arg
Correct output:
ARG1
ARG3
The customers walk the door
in
expecting: verb_mod_arg123 you to have
MOD ARG2
you to have
expecting: verb_arg123
The customers walk the door
in
ARG1
ARG2
Parser output:
Not exist
(MOD)
ARG3
aux_arg12
The figures ... were adjusted
Unknown subject
to : aux_mod_arg12
MOD ARG2
ARG1
ARG1
remove ...
Correct answer:
Parser output:
The figures ... were adjusted
remove ...
ARG3
verb1 ...
Correct output:
to : aux_mod_arg12
verb2
MOD
ARG2
ARG1
Parser output:
verb1 ...
to : aux_arg12
verb2
ARG3
Pattern: (Patterns of correct answer and parser output can be interchanged)
ARG1
Unknown subject
Example:
Pattern:
Correct output:
Parser output:
(Patterns of correct answer and parser output can be interchanged)
Unknown subject
ARG1
verb, ...
verb, ...
ARG1
prep_arg12
lgs_arg2
ARG1
ARG2
...
...
Correct answer: A 50-state study released in September by :
Parser output:
Unknown subject
A 50-state study released in September by : prep_arg12 Friends ...
ARG2
ARG1
ARG1
ARG1
lgs_arg12
ARG2
Friends ...
1165
Example:
Correct answer:
</figure>
<figureCaption confidence="0.9965925">
Figure 9: Pattern for “Relative clause attachment”
Figure 10: An image of our empirical approach
</figureCaption>
<bodyText confidence="0.999832944444444">
occurring error group are expected to participate
in the same phenomenon. Dependencies among
errors are then expected to be summarized with in-
ductions among co-occurring error groups.
Figure 10 shows an image of this approach. In
this example, “today” should modify noun phrase
“our workforce” while the parser decided that “to-
day” was also in the noun phrase. As a result, there
are five errors: three wrong outputs for “ARG2”
of “on” (Error 1) and “ARG1” of “our” (Error 2)
and “work” (Error 3), excess relation “ARG1” of
“force” (Error 4), and missing relation “ARG1” for
“today” (Error 5). By correcting each of the errors
1, 2, 3 and 4, all of these errors are corrected to-
gether, and therefore classified into the same co-
occurring error group. Although error 5 cannot
participate in the group, correcting error 5 can cor-
rect all of the errors in the group, and therefore an
</bodyText>
<table confidence="0.960171208333333">
Error types
� Analyzed
[Argument selection]
Prepositional attachment
Adjunction attachment
Conjunction attachment
Head selection for noun phrase
Coordination
[Predicate type selection]
Preposition/Adjunction
Gerund acts as modifier or not
Coordination/conjunction
# of arguments for preposition
Adjunction/adjunctive noun
[More structural errors]
To-infinitive for
modifier/argument of verb
Subject for passive sentence
or not
[Others]
Comma
Relative clause attachment
� Unanalyzed
Total
</table>
<tableCaption confidence="0.99991">
Table 3: Errors extracted with descriptive analysis
</tableCaption>
<bodyText confidence="0.999655">
inductive relation is given from error 5 to the co-
occurring error group. We can then finally obtain
the inductive relations as shown at the bottom of
Figure 10. This approach can trace the actual be-
havior of the parser precisely, and can therefore
capture underlying dependencies which cannot be
found only by observing error outputs.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999756">
We applied our approaches to parsing errors given
by the HPSG parser Enju, which was trained on
the Penn Treebank (Marcus et al., 1994) section
2-21. We first examined each approach, and then
explored the combination of the approaches.
</bodyText>
<subsectionHeader confidence="0.998101">
4.1 Evaluation of descriptive approach
</subsectionHeader>
<bodyText confidence="0.999928923076923">
We examined our descriptive approach. We first
parsed sentences in the Penn Treebank section 22
with Enju, and then observed the errors. Based on
the observation, we next described the patterns as
shown in Section 3. After that, we parsed section
0 and then applied the patterns to the errors.
Table 3 summarizes the extracted errors. As the
table shows, with the 14 error patterns, we suc-
cessfully matched 1,671 locations in error outputs
and covered 2,078 of 4,709 errors, which com-
prised of more than 40% of the total errors. This
was the first step of the application of our ap-
proach, and in the future work we would like to
</bodyText>
<figure confidence="0.913803691588785">
Parser output: ARG1/2
relative_arg1
ARG1
Error
Pattern:
ARG2
the shelf
which : relative_arg1
the shelf which: relative_arg1 I read yesterday
I read yesterday
Parser output:
The book on
ARG2
ARG1
The book on
ARG1
Re-parse a sentence under the condition where
each error is forcibly corrected
Error 4
Error 4
, Error 2 , Error 3
ARG1 ARG1
corrected together
corrected together
corrected together
corrected together
Correct Error 1 Error 2
Correct
Correct
Correct
Error 2
Error 3
Error 1
Error 1
Error 3
,
Error 3
,
Correct Error 5 Error 1 , Error 2 , Error 3 , Error 4 corrected together
,
,
Extract co-occurring error groups and inductive relations
, Error 2 , Error 4
Co-occurring error group Co-occurring error group
Error 4 Error 1 Error 5
Error 1 Error 2 Error 3 Error 4
Induce
Correct answer:
Parser output:
It has no bearing
It has no bearing on
on our work force today
ARG2 ARG1
Error 1
xo
our rk force
today
ARG2 ARG1 ARG1 ARG1
Error 2
Error 3
Error 4
Error 5
# of
Errors
2,078
579
261
43
30
202
108
84
54
51
13
120
8
444
102
2,631
4,709
Patterns
1,671
579
261
40
30
184
54
31
27
17
13
22
3
372
38
�
�
1166
Table 4: Summary of our empirical approach
1500
1000
500
0
1 2 3 4 5 6 7 8 9 10
Size of co—occurring error group
</figure>
<figureCaption confidence="0.652012">
Figure 11: Frequency of each size of co-occurring
error group
</figureCaption>
<figure confidence="0.99838496">
, researchers said
(c) (d)
Correct answer:
fiber , :
app_arg12
cro
cidolite
ARG1 ARG2
Parser output:
fiber , :
coord_arg12
cro
cidolite
ARG1 ARG2
Correct answer:
usually resilient ... the lungs ,
with
ARG1 ARG1
usually resilient ... the lungs , with
ARG1 ARG1
Sentence: The asbestos fiber , crocidolite , is unusually resilient once it enters the
(b) (a)
lungs , with even brief exposures to it causing symptoms that show up decades later
symptoms that show :
up decades later
ARG1 ARG2
verb_arg12
Parser output:
Correct answer:
symptoms that show :
up decades later
verb_arg1
ARG1 ARG1
Correct answer:
It causing symptoms that show up decades later
ARG1
Parser output:
It causing symptoms that show up decades
later
ARG1
Evaluated sentences (erroneous)
Errors (Correctable)
Co-occurring errors
Extracted inductive relations
F-score (LP/LR)
1,811 (1,009)
4,709 (3,085)
1,978
501
90.69 (90.78/93.59)
</figure>
<bodyText confidence="0.994015916666667">
add more patterns for capturing more phenomena.
When we focused on individual patterns, we
could observe that the simple error phenomena
such as the attachments were dominant. The first
reason for this would be that such phenomena
were among minimal linguistic events. This would
make the phenomena components of other more
complex ones. The second reason for the dom-
inance would be that the patterns for these error
phenomena were easy to implement only with ar-
gument inconsistencies, and only one or a few pat-
terns could cover every probable error. Among
these dominant error types, the number of prepo-
sitional attachments was outstanding. The er-
ror types which required matching with predicate
types were fewer than the attachment errors since
the limited patterns on the predicate types would
narrow the possible linguistic behavior of the can-
didate words. When we focus on more structural
errors, the table shows that the rates of the partici-
pant errors to matched locations were much larger
than those for simpler pattern errors. Once our pat-
terns matches, they could collect many errors at
the same time.
</bodyText>
<subsectionHeader confidence="0.998797">
4.2 Evaluation of empirical approach
</subsectionHeader>
<bodyText confidence="0.977317789473684">
Next, we applied our empirical approach in the
same settings as in the previous section. We first
parsed sentences in section 0 and then applied our
approach to the obtained errors. In the experi-
ments, some errors could not be forcibly corrected
by our approach. The parser “cut off” less proba-
ble parse substructures before giving the predicate
subject to such
as
ones,
and focused only on the remaining
errors. In our future work, we would like to con-
sider the
errors.
Table 4 shows the summary of the analysis with
our approach. Enju gave 4,709 errors for section
0. Among these errors, the correctable errors were
3,085, and from these errors, we successfully ob-
tained 1,978 co-occurring error groups and 501 in-
ductive relations. Figure 11 shows the frequency
for each size of co-occurring groups. About a half
of the groups contains only single errors, which
would indicate that the errors could have only one-
way inductive relations with other errors. The rest
of this section explores examples of the obtained
co-occurring error groups and inductive relations.
Figure 12 shows an example of the extracted co-
occurring error groups. For the sentence shown at
the top of the figure, Enju gave seven errors. By
introducing our empirical approach, these errors
were definitely classified into four co-occurring er-
ror groups (a) to (d), and there were no inductive
relations detected among them. Group (a) contains
two errors on the
local behavior as ap-
position or coordination. Group (b) contains the
errors on the words which gave almost the same
att
</bodyText>
<equation confidence="0.862019">
“cutoff”
“uncorrectable”
“correctable”
“uncorrectable”
comma’s
achment behaviors. Group (c) contains the er-
rors on whether the verb “show” took “decades”
is
Parser output: is
</equation>
<figureCaption confidence="0.858427">
Figure 12: Obtained co-occurring error groups
</figureCaption>
<bodyText confidence="0.642679">
argument relation for reducing the cost of parsing.
In this research, we ignored the errors which were
</bodyText>
<table confidence="0.984929090909091">
1167
Error types # of correctable errors # of independent errors Correction effect (errors)
[Argument selection]
Prepositional attachment 531 397 766
Adjunction attachment 196 111 352
Conjunction attachment 33 12 79
Head selection for noun phrase 22 0 84
Coordination 146 62 323
[Predicate type selection]
Preposition/Adjunction 72 30 114
Gerund acts as modifier or not 39 18 62
Coordination/conjunction 36 16 61
# of arguments for preposition 24 23 26
Adjunction/adjunctive noun 8 6 10
[More structural errors]
To-infinitive for 75 27 87
modifier/argument of verb
Subject for passive sentence or not 8 3 9
[Others]
Comma 372 147 723
Relative clause attachment 84 27 119
Total 1,646 979 −
</table>
<tableCaption confidence="0.999223">
Table 5: Induction relations between errors for each linguistic phenomenon and other errors
</tableCaption>
<bodyText confidence="0.979524878787879">
Figure 13: Inductive relation between obtained co-
occurring error groups
as its object or not. Group (d) contains an error on
the attachment of the adverb “later”. Regardless
of the overlap of the regions in the sentence for
(c) and (d), our approach successfully classified
the errors into the two independent groups. With
our approach, it would be empirically shown that
the errors in each group actually co-occurred and
the group was independent. This would enable us
to concentrate on each of the co-occurring error
groups without paying attention to the influences
from the errors in other groups.
Figure 13 shows another example of the anal-
ysis with our empirical approach. In this case, 8
errors for a sentence were classified into two co-
occurring error groups (a) and (b), and our ap-
proach showed that correction in group (a) re-
sulted in correcting group (b) together. The errors
in group (a) were on whether “help” behaved as an
auxiliary or pure verbal role. The errors in group
(b) were on whether “save” took only one object
“her teaching certificate,” or two objects “her” and
“teaching certificate.” Between group (a) and (b),
no “structural” conflict could be found when cor-
recting only each of the groups. We could then
guess that the inductive relation between these two
groups was implicitly given by the disambigua-
tion model of the parser. By dividing the errors
into minimum units and clarifying the effects of
correcting a target error, error analysis with our
empirical approach could suggest some policy for
parser improvements.
</bodyText>
<subsectionHeader confidence="0.999532">
4.3 Combination of two approaches
</subsectionHeader>
<bodyText confidence="0.992193846153846">
On the basis of the experiments shown in the pre-
vious sections, we would like to explore possibili-
ties for obtaining a more detailed analysis by com-
bining the two approaches.
4.3.1 Interactions between a target linguistic
phenomenon and other errors
Our descriptive approach could classify the pars-
ing errors according to the linguistic phenomena
they participated in. We then attempt to reveal how
such classified errors interacted with other errors
from the viewpoints of our empirical approach. In
order to enable the analysis by our empirical ap-
proach, we focused only on the correctable errors.
</bodyText>
<figure confidence="0.995196137931035">
Correcting (a) induced correcting (b)
Parser output:
... thought she could
... thought she could
ARG2
ARG2
ARG1 ARG2
ARG1 ARG2
help: verb_arg12
help : aux_arg12
ARG2
save her teaching certificate
save her teaching certificate
(a) Correct answer:
(b) Correct answer:
ARG1
ARG1 ARG2
save :verb_arg123
her teaching certificate
ARG2
Parser output:
... thought she could help
... thought she could help save : verb_arg12 her teaching certificate
ARG1 ARG2 ARG3
Sentence: She says she offered Mrs. Yeargin a quiet resignation
(a) (b)
and thought she could help save her teaching certificate
1168
Pattern matched: “Comma” , “Relative clause attachment”
</figure>
<figureCaption confidence="0.93357">
Figure 14: Combination of results given by de-
scriptive and empirical approaches (1)
</figureCaption>
<bodyText confidence="0.994937569444445">
Table 5 reports the degree to which the classi-
fied errors were related to other individual errors.
The leftmost numbers show the numbers of cor-
rectable errors, which were the focused errors in
the experiments. The central numbers show the
numbers of “independent” errors, that is, the errors
which could be corrected only by correcting them-
selves. The rightmost numbers show “correction
effects,” that is, the number of errors which would
consequently be corrected if all of the errors for
the focused phenomena were forcibly corrected.
“Independent” errors are obtained by collecting
error phenomena groups which consist of unions
of co-occurring error groups and each error in
which is not induced by other errors. Figure 14
shows an example of “independent” errors. For
the sentence at the top of the figure, the parser had
four errors on ARG1 of “overseas,” the comma,
“which” and “boosts.” Our empirical approach
then classified these errors into two co-occurring
error groups (a) and (b), and there was no induc-
tive relation between the groups. Our descrip-
tive approach, on the other hand, matched all of
the errors with the patterns for “Adjunction at-
tachment,” “Comma” and “Relative clause attach-
ment.” Since the error for the “Adjunction attach-
ment” equals to a co-occurring group (a) and is not
induced by other errors, the error is “independent.”
Table 5 shows that, for “Prepositional attach-
ment”, “Adjunction attachments,” “# of argu-
ments for preposition” and “Adjunction/adjunctive
noun,” more than half of the errors for the focused
phenomena are “independent.” Containing many
“independent” errors would mean that the parser
should handle these phenomena further more in-
tensively as an independent event.
Figure 15: Combination of results given by de-
scriptive and empirical approaches (2)
The “correction effect” for a focused linguistic
phenomenon can be obtained by counting errors in
the union of the correctable error set for the phe-
nomenon and the error sets which were induced by
the individual errors in the set. We would show an
example of correction effect in Figure 15. In the
figure, the parser had six errors for the sentence
at the top: three false outputs for ARG1 of “and,”
“this” and “U.S.,” two false outputs for ARG2 of
“of” and “and,” and missing output for ARG1 of
“sales.” Our empirical approach classified these
errors into two co-occurring error groups (a) and
(b), and extracted an inductive relation from (a) to
(b). Our descriptive approach, on the other hand,
matched two errors on “and” with pattern “Coor-
dination” and one error on “this” with “Head se-
lection for noun phrase.” When we focus on the
error for “Head selection of noun phrase” in co-
occurring group (a), the correction of the error in-
duced the rest of the errors in (a), and further in-
duced the error in (b) according to the inductive
relation from (a) to (b). Therefore, a “correction
effect” for the error results in six errors.
Table 5 shows that, for “Conjunction attach-
ment,” “Head selection for noun phrase” and “Co-
ordination,” each “correction effect” results in
more than twice the forcibly corrected errors. Im-
proving the parser so that it can resolve such high-
correction-effect erroneous phenomena may ad-
ditionally improve the parsing performances to a
great extent. On the other hand, “Head selection
for noun phrase” contains no “independent” error,
and therefore could not be handled independently
of other erroneous phenomena at all. Consider-
</bodyText>
<figure confidence="0.9909014375">
Error:
ARG1
Pattern matched:
“Adjunction attachment”
is currently waiving management fees
Error:
ARG1
ARG1
,
which
ARG1
ARG1
boosts its yield
ARG1
ARG1
Sentence: It invests heavily in dollar-denominated securities overseas and is
currently waiving management fees , which boosts its yield (a)
(b)
It invests heavily in dollar-denominated securities overseas : adj_arg1
ARG1
ARG2
ARG1
ARG1
ARG2
“Coordination” (fragment),
“Head selection of noun phrase”
Pattern matched:
Error:
ARG1
Pattern matched:
“Coordination” (fragment)
Correcting (a) induced correcting (b)
(b)
(a)
Sentence: Clark J. Vitulli was named senior vice president and general manager
of this U.S. sales and
(b) (a)
marketing arm of Japanese auto Maker Mazda Motor Corp
Error:
ARG2 ARG1 ARG1 ARG1 ARG2
manager of this :
det_arg1
U.S. sales
and : coord_arg12
marketing arm of
senior vice president and general manager of this U.S. sales and : coord_arg12
ARG1
1169
</figure>
<bodyText confidence="0.9996709">
ing the effects from outer events might make the
treatment of “Head selection for noun phrase” a
more complicated process than other phenomena,
regardless of its high “correction effect.”
Table 5 would thus suggest which phenomenon
we should resolve preferentially from the three
points of view: the number of errors, the number
of “independent” errors and its “correction effect.”
Considering these points, “Prepositional attach-
ment” seems most preferable for handling first.
</bodyText>
<subsubsectionHeader confidence="0.656627">
4.3.2 Possibilities for further analysis
</subsubsectionHeader>
<bodyText confidence="0.999993136363636">
Since the errors for the phenomenon were system-
atically collected with our descriptive approach,
we can work on further focused error analyses
which would answer such questions as “Which
preposition causes most errors in attachments?”,
“Which pair of a correct answer and an erroneous
output for predicate argument relations can occur
most frequently?”, and so on. Our descriptive ap-
proach would enable us to thoroughly obtain such
analyses with more closely-defined patterns. In
addition, our empirical approach would clarify the
influences of the obtained error properties on the
parser’s behaviors. The results of the focused anal-
yses might reasonably lead us to the features that
can be captured as parameters for model training,
or policies for re-ranking the parse candidates.
The combination of our approaches would give
us interesting clues for planning effective strate-
gies for improving the parser. Our challenges for
combining the two approaches are now in the pre-
liminary stage and there would be many possibili-
ties for further detailed analysis.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.9999884">
Although there have been many researches which
analyzed errors on their own systems in the part of
the experiments, there have been few researches
which focused mainly on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They observed
the accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded for one step in this point, and attempted
to reveal the way of the propagations. In exam-
ining the combination of the two types of pars-
ing, McDonald and Nivre (2007) utilized similar
approaches to our empirical analysis. They al-
lowed a parser to give only structures given by
the parsers. They implemented the ideas for eval-
uating the parser’s potentials whereas we imple-
mented the ideas for observing error propagations.
Dredze et al. (2007) showed the possibility
that many parsing errors in the domain adaptation
tasks came from inconsistencies between annota-
tion manners of training resources. Such findings
would further suggest that, comparing given errors
without considering the inconsistencies could lead
to the misunderstanding of what occurs in domain
transitions. The summarized error dependencies
given by our approaches would be useful clues for
extracting such domain-dependent error phenom-
ena.
Gim´enez and M`arquez (2008) proposed an au-
tomatic error analysis approach in machine trans-
lation (MT) technologies. They were developing
a metric set which could capture features in MT
outputs at different linguistic levels with different
levels of granularity. As we considered the parsing
systems, they explored the way to resolve costly
and non-rewarding error analysis in the MT field.
One of their objectives was to enable researchers
to easily access detailed linguistic reports on their
systems and to concentrate only on analyses for
the system improvements. From this point of view,
our research might provide an introduction into
such rewarding analysis in parsing.
</bodyText>
<sectionHeader confidence="0.99959" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999995285714286">
We proposed empirical and descriptive approaches
to extracting dependencies among parsing errors.
In the experiments, with each of our approaches,
we successfully obtained relevant errors. More-
over, the possibility was shown that the combina-
tion of our approaches would give a more detailed
error analysis which would bring us useful clues
for parser improvements.
In our future work, we will improve the per-
formance of our approaches by adding more pat-
terns for the descriptive approach and by handling
uncorrectable errors for the empirical approach.
With the obtained robust information, we will ex-
plore rewarding ways for parser improvements.
</bodyText>
<sectionHeader confidence="0.996559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994361">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
</bodyText>
<page confidence="0.590552">
1170
</page>
<sectionHeader confidence="0.99357" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765222222222">
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051–1055.
Jes´us Gim´enez and Lluis M`arquez. 2008. Towards
heterogeneous automatic MT error analysis. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC’08),
pages 1894–1901.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29–130.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122–131.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83–90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155–163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
</reference>
<page confidence="0.810007">
1171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.281913">
<title confidence="0.837132666666667">Descriptive and Empirical Approaches to Capturing Dependencies among Parsing Errors of Computer Science, University of</title>
<author confidence="0.379813">Tokyo Bunkyo-ku</author>
<affiliation confidence="0.4563055">of Computer Science, University of (National Center for Text</affiliation>
<abstract confidence="0.999737055555556">In this paper, we provide descriptive and empirical approaches to effectively extracting underlying dependencies among parsing errors. In the descriptive approach, we define some combinations of error patterns and extract them from given errors. In the empirical approach, on the other hand, we re-parse a sentence with a target error corrected and observe errors corrected together. Experiments on an HPSG parser show that each of these approaches can clarify the dependencies among individual errors from each point of view. Moreover, the comparison between the results of the two approaches shows that combining these approaches can achieve a more detailed error analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
<author>Partha Pratim Talukdar</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Fernando Pereira</author>
</authors>
<title>Frustratingly hard domain adaptation for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1051--1055</pages>
<marker>Dredze, Blitzer, Talukdar, Ganchev, Grac¸a, Pereira, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao V. Grac¸a, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proceedings of the CoNLL Shared Task Session of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1051–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Towards heterogeneous automatic MT error analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<pages>1894--1901</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2008. Towards heterogeneous automatic MT error analysis. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), pages 1894–1901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation. Formal Issues in LexicalFunctional Grammar,</title>
<date>1995</date>
<pages>29--130</pages>
<contexts>
<context position="3386" citStr="Kaplan and Bresnan, 1995" startWordPosition="512" endWordPosition="515">miya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG (Pollard and Sag, 1994). Since such a framework can usually produce more than one probable structure for a sentence, a parser 1162 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162–1171, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Predicate argument relations Abbr. Full lgs logical subject coord coordination conj conjunction argN,... take argument(s) adj adjunction mod modify a word relative relative Table 1: Descriptions for predicate types ARG1 ARG2 Correct answer: I watched the girl on TV ARG1 ARG2 ARG1 ARG2 Parser out</context>
</contexts>
<marker>Kaplan, Bresnan, 1995</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1995. Lexicalfunctional grammar: A formal system for grammatical representation. Formal Issues in LexicalFunctional Grammar, pages 29–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert Macintyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="17175" citStr="Marcus et al., 1994" startWordPosition="2751" endWordPosition="2754">assive sentence or not [Others] Comma Relative clause attachment � Unanalyzed Total Table 3: Errors extracted with descriptive analysis inductive relation is given from error 5 to the cooccurring error group. We can then finally obtain the inductive relations as shown at the bottom of Figure 10. This approach can trace the actual behavior of the parser precisely, and can therefore capture underlying dependencies which cannot be found only by observing error outputs. 4 Experiments We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al., 1994) section 2-21. We first examined each approach, and then explored the combination of the approaches. 4.1 Evaluation of descriptive approach We examined our descriptive approach. We first parsed sentences in the Penn Treebank section 22 with Enju, and then observed the errors. Based on the observation, we next described the patterns as shown in Section 3. After that, we parsed section 0 and then applied the patterns to the errors. Table 3 summarizes the extracted errors. As the table shows, with the 14 error patterns, we successfully matched 1,671 locations in error outputs and covered 2,078 of</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, Macintyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>122--131</pages>
<contexts>
<context position="33082" citStr="McDonald and Nivre (2007)" startWordPosition="5362" endWordPosition="5365">ptured as parameters for model training, or policies for re-ranking the parse candidates. The combination of our approaches would give us interesting clues for planning effective strategies for improving the parser. Our challenges for combining the two approaches are now in the preliminary stage and there would be many possibilities for further detailed analysis. 5 Related work Although there have been many researches which analyzed errors on their own systems in the part of the experiments, there have been few researches which focused mainly on error analysis itself. In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers. They observed the accuracy transitions from various points of view, and the obtained statistical data suggested that error propagation seemed to occur in the graph structures of parsing outputs. Our research proceeded for one step in this point, and attempted to reveal the way of the propagations. In examining the combination of the two types of parsing, McDonald and Nivre (2007) utilized similar approaches to our empirical analysis. They allowed a parser to give only structures given by the parsers. They implemented the</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>83--90</pages>
<contexts>
<context position="2755" citStr="Miyao and Tsujii, 2005" startWordPosition="412" endWordPosition="416">ous combinations of error patterns as organized error phenomena on the basis of linguistic knowledge, and then extract such combinations from given errors. In our empirical approach, on the other and, we re-parse a sentence under the condition where a target error is corrected, and errors which are additionally corrected are regarded as dependent errors. By capturing dependencies among parsing errors through systematic approaches, we can effectively collect errors which are related to the same linguistic properties. In the experiments, we applied both of our approaches to an HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000),</context>
<context position="9681" citStr="Miyao and Tsujii, 2005" startWordPosition="1560" endWordPosition="1563">ir occurrences. The former error can appear by itself (Analysis 1) while the latter cannot because of the structural conflict with the former error (Analysis 2). The appearance of the latter error thus induces that of the former error. In error analysis, we have to correctly capture such various relations, which leads us to a costly and less rewarding analysis. In order to make advancements on this problem, we propose two types of approaches to realizing a deeper error analysis on parsing. In the experiments, we examine our approaches for actual errors which are given by the HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006). Enju was developed for capturing detailed syntactic or semantic properties and relations for a sentence with an HPSG framework (Pollard and Sag, 1994). In this research, we focus on error analysis based on predicate argument relations, and in the experiments with Enju, utilize the relations which Table 2: Patterns defined for descriptive approach are represented in parsed tree structures. 3 Two approaches for error analysis In this section, we propose two approaches for error analysis which enable us to capture underlying dependencies among parsing errors. Our descrip</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>155--163</pages>
<contexts>
<context position="2779" citStr="Ninomiya et al., 2006" startWordPosition="417" endWordPosition="420">r patterns as organized error phenomena on the basis of linguistic knowledge, and then extract such combinations from given errors. In our empirical approach, on the other and, we re-parse a sentence under the condition where a target error is corrected, and errors which are additionally corrected are regarded as dependent errors. By capturing dependencies among parsing errors through systematic approaches, we can effectively collect errors which are related to the same linguistic properties. In the experiments, we applied both of our approaches to an HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan</context>
<context position="9705" citStr="Ninomiya et al., 2006" startWordPosition="1564" endWordPosition="1567">er error can appear by itself (Analysis 1) while the latter cannot because of the structural conflict with the former error (Analysis 2). The appearance of the latter error thus induces that of the former error. In error analysis, we have to correctly capture such various relations, which leads us to a costly and less rewarding analysis. In order to make advancements on this problem, we propose two types of approaches to realizing a deeper error analysis on parsing. In the experiments, we examine our approaches for actual errors which are given by the HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006). Enju was developed for capturing detailed syntactic or semantic properties and relations for a sentence with an HPSG framework (Pollard and Sag, 1994). In this research, we focus on error analysis based on predicate argument relations, and in the experiments with Enju, utilize the relations which Table 2: Patterns defined for descriptive approach are represented in parsed tree structures. 3 Two approaches for error analysis In this section, we propose two approaches for error analysis which enable us to capture underlying dependencies among parsing errors. Our descriptive approach matches th</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="3418" citStr="Pollard and Sag, 1994" startWordPosition="518" endWordPosition="521">ted the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG (Pollard and Sag, 1994). Since such a framework can usually produce more than one probable structure for a sentence, a parser 1162 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162–1171, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Predicate argument relations Abbr. Full lgs logical subject coord coordination conj conjunction argN,... take argument(s) adj adjunction mod modify a word relative relative Table 1: Descriptions for predicate types ARG1 ARG2 Correct answer: I watched the girl on TV ARG1 ARG2 ARG1 ARG2 Parser output: I watched the girl on TV AR</context>
<context position="9857" citStr="Pollard and Sag, 1994" startWordPosition="1589" endWordPosition="1592"> of the latter error thus induces that of the former error. In error analysis, we have to correctly capture such various relations, which leads us to a costly and less rewarding analysis. In order to make advancements on this problem, we propose two types of approaches to realizing a deeper error analysis on parsing. In the experiments, we examine our approaches for actual errors which are given by the HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006). Enju was developed for capturing detailed syntactic or semantic properties and relations for a sentence with an HPSG framework (Pollard and Sag, 1994). In this research, we focus on error analysis based on predicate argument relations, and in the experiments with Enju, utilize the relations which Table 2: Patterns defined for descriptive approach are represented in parsed tree structures. 3 Two approaches for error analysis In this section, we propose two approaches for error analysis which enable us to capture underlying dependencies among parsing errors. Our descriptive approach matches the patterns of error combinations with given parsing errors and collects matched erroneous participants. Our empirical approach, on the other hand, detec</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>THE MIT Press.</publisher>
<contexts>
<context position="3354" citStr="Steedman, 2000" startWordPosition="509" endWordPosition="510">and Tsujii, 2005; Ninomiya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG (Pollard and Sag, 1994). Since such a framework can usually produce more than one probable structure for a sentence, a parser 1162 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162–1171, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Predicate argument relations Abbr. Full lgs logical subject coord coordination conj conjunction argN,... take argument(s) adj adjunction mod modify a word relative relative Table 1: Descriptions for predicate types ARG1 ARG2 Correct answer: I watched the girl on T</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. THE MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>