<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000170">
<title confidence="0.876389">
Leveraging Hidden Dialogue State to Select Tutorial Moves
</title>
<author confidence="0.77203">
Kristy Robert Eun Young Michael Mladen A. James C.
Elizabeth Phillipsab Haa D. Vouka Lestera
Boyera Wallisab
</author>
<affiliation confidence="0.9511945">
aDepartment of Computer Science, North Carolina State University
bApplied Research Associates
</affiliation>
<address confidence="0.641639">
Raleigh, NC, USA
</address>
<email confidence="0.90226">
{keboyer, rphilli, eha, mdwallis, vouk, lester}@ncsu.edu
</email>
<sectionHeader confidence="0.995281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908055555555">
A central challenge for tutorial dialogue
systems is selecting an appropriate move
given the dialogue context. Corpus-based
approaches to creating tutorial dialogue
management models may facilitate more
flexible and rapid development of tutorial
dialogue systems and may increase the
effectiveness of these systems by allowing
data-driven adaptation to learning contexts
and to individual learners. This paper presents
a family of models, including first-order
Markov, hidden Markov, and hierarchical
hidden Markov models, for predicting tutor
dialogue acts within a corpus. This work takes
a step toward fully data-driven tutorial
dialogue management models, and the results
highlight important directions for future work
in unsupervised dialogue modeling.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999599846153846">
A central challenge for dialogue systems is
selecting appropriate system dialogue moves
(Bangalore, Di Fabbrizio, &amp; Stent, 2008; Frampton
&amp; Lemon, 2009; Young et al., 2009). For tutorial
dialogue systems, which aim to support learners
during conceptual or applied learning tasks,
selecting an appropriate dialogue move is
particularly important because the tutorial
approach could significantly influence cognitive
and affective outcomes for the learner (Chi,
Jordan, VanLehn, &amp; Litman, 2009). The strategies
implemented in tutorial dialogue systems have
historically been based on handcrafted rules
</bodyText>
<page confidence="0.954614">
66
</page>
<figureCaption confidence="0.686156">
derived from observing human tutors (e.g., Aleven,
McLaren, Roll, &amp; Koedinger, 2004; Evens &amp;
Michael, 2006; Graesser, Chipman, Haynes, &amp;
Olney, 2005; Jordan, Makatchev, Pappuswamy,
VanLehn, &amp; Albacete, 2006). While these systems
can achieve results on par with unskilled human
tutors, tutorial dialogue systems have not yet
matched the effectiveness of expert human tutors
(VanLehn et al., 2007).
</figureCaption>
<bodyText confidence="0.999652962962963">
A more flexible model of strategy selection may
enable tutorial dialogue systems to increase their
effectiveness by responding adaptively to a broader
range of contexts. A promising method for
deriving such a model is to learn it directly from
corpora of effective human tutoring. Data-driven
approaches have shown promise in task-oriented
domains outside of tutoring (Bangalore et al.,
2008; Hardy et al., 2006; Young et al., 2009), and
automatic dialogue policy creation for tutoring has
been explored recently (Chi, Jordan, VanLehn, &amp;
Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately,
devising data-driven approaches for developing
tutorial dialogue systems may constitute a key step
towards achieving the high learning gains that have
been observed with expert human tutors.
The work presented in this paper focuses on
learning a model of tutorial moves within a corpus
of human-human dialogue in the task-oriented
domain of introductory computer science. Unlike
the majority of task-oriented domains that have
been studied to date, our domain involves the
separate creation of a persistent artifact by the user
(the student). The modification of this artifact, in
our case a computer program, is the focus of the
dialogues. Our corpus consists of textual dialogue
utterances and a separate synchronous stream of
</bodyText>
<note confidence="0.9845225">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66–73,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99963175">
task actions. Our goal is to extract a data-driven
dialogue management model from the corpus, as
evidenced by predicting system (tutor) dialogue
acts.
In this paper, we present an annotation approach
that addresses dialogue utterances and task actions,
and we propose a unified sequential representation
for these separate synchronous streams of events.
We explore the predictive power of three
stochastic models — first-order Markov models,
hidden Markov models, and hierarchical hidden
Markov models — for predicting tutor dialogue
acts in the unified sequences. By leveraging these
models to capture effective tutorial dialogue
strategies, this work takes a step toward creating
data-driven tutorial dialogue management models.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999880673469388">
Much of the research on selecting system dialogue
acts relies on a Markov assumption (Levin,
Pieraccini, &amp; Eckert, 2000). This formulation is
often used in conjunction with reinforcement
learning (RL) to derive optimal dialogue policies
(Frampton &amp; Lemon, 2009). Sparse data and large
state spaces can pose serious obstacles to RL, and
recent work aims to address these issues (Ai,
Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp;
Georgila, 2008; Heeman, 2007; Young et al.,
2009). For tutorial dialogue, RL has been applied
to selecting a state space representation that best
facilitates learning an optimal dialogue policy
(Tetreault &amp; Litman, 2008). RL has also been used
to compare specific tutorial dialogue tactic choices
(Chi et al., 2008).
While RL learns a dialogue policy through
exploration, our work assumes that a flexible, good
(though possibly not optimal) dialogue policy is
realized in successful human-human dialogues. We
extract this dialogue policy by predicting tutor
(system) actions within a corpus. Using human
dialogues directly in this way has been the focus of
work in other task-oriented domains such as
finance (Hardy et al., 2006) and catalogue ordering
(Bangalore et al., 2008). Like the parse-based
models of Bangalore et al., our hierarchical hidden
Markov models (HHMM) explicitly capture the
hierarchical nesting of tasks and subtasks in our
domain. In other work, this level of structure has
been studied from a slightly different perspective
as conversational game (Poesio &amp; Mikheev, 1998).
For tutorial dialogue, there is compelling
evidence that human tutoring is a valuable model
for extracting dialogue system behaviors. The
CIRCSIM-TUTOR (Evens &amp; Michael, 2006),
ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp;
Tetreault, 2007; Forbes-Riley &amp; Litman, 2009),
and KSC-PAL (Kersey, Di Eugenio, Jordan, &amp;
Katz, 2009) projects have made extensive use of
data-driven techniques based on human corpora.
Perhaps most directly comparable to the current
work are the bigram models of Forbes-Riley et al.;
we explore first-order Markov models, which are
equivalent to bigram models, for predicting tutor
dialogue acts. In addition, we present HMMs and
HHMMs trained on our corpus. We found that
both of these models outperformed the bigram
model for predicting tutor moves.
</bodyText>
<sectionHeader confidence="0.97526" genericHeader="method">
3 Corpus and Annotation
</sectionHeader>
<bodyText confidence="0.999982631578947">
The corpus was collected during a human-human
tutoring study in which tutors and students worked
to solve an introductory computer programming
problem (Boyer et al., in press). The dialogues
were effective: on average, students exhibited a 7%
absolute gain from pretest to posttest (N=48, paired
t-test p&lt;0.0001).
The corpus contains 48 textual dialogues with a
separate, synchronous task event stream. Tutors
and students collaborated to solve an introductory
computer programming problem using an online
tutorial environment with shared workspace
viewing and textual dialogue. Each student
participated in exactly one tutoring session. The
corpus contains 1,468 student utterances, 3,338
tutor utterances, and 3,793 student task actions. In
order to build the dialogue model, we annotated
the corpus with dialogue act tags and task
annotation labels.
</bodyText>
<subsectionHeader confidence="0.999133">
3.1 Dialogue Act Annotation
</subsectionHeader>
<bodyText confidence="0.982214714285714">
We have developed a dialogue act tagset inspired
by schemes for conversational speech (Stolcke et
al., 2000), task-oriented dialogue (Core &amp; Allen,
1997), and tutoring (Litman &amp; Forbes-Riley,
2006). The dialogue act tags are displayed in Table
1. Overall reliability on 10% of the corpus for two
annotators was ĸ=0.80.
</bodyText>
<page confidence="0.999654">
67
</page>
<tableCaption confidence="0.998351">
Table 1. Dialogue act tags
</tableCaption>
<table confidence="0.542222444444444">
Stu. Tut.
Rel. Rel.
DA Description Freq. Freq. κ
Request for feedback on
task or conceptual .20 .11 .91
utterance.
Asides not relevant to the .08 .04 .79
tutoring task.
GROUNDING (G) Acknowledgement/thanks .26 .06 .92
</table>
<construct confidence="0.518076933333333">
Negative assessment with
explanation.
Lukewarm assessment of
task action or conceptual
utterance.
Negative assessment with
explanation.
Negative assessment of
task action or conceptual
utterance.
Positive assessment with
explanation.
Positive assessment of
task action or conceptual
utterance.
</construct>
<subsectionHeader confidence="0.999386">
3.2 Task Annotation
</subsectionHeader>
<bodyText confidence="0.999844342857143">
The dialogues focused on the task of solving an
introductory computer programming problem. The
task actions were recorded as a separate but
synchronous event stream. This stream included
97,509 keystroke-level user task events. These
events were manually aggregated and annotated for
subtask structure and then for correctness. The task
annotation scheme was hierarchical, reflecting the
nested nature of the subtasks. An excerpt from the
task annotation scheme is depicted in Figure 1; the
full scheme contains 66 leaves. The task annotation
scheme was designed to reflect the different depth
of possible subtasks nested within the overall task.
Each labeled task action was also judged for
correctness according to the requirements of the
task, with categories CORRECT, BUGGY,
INCOMPLETE, and DISPREFERRED (technically
correct but not accomplishing the pedagogical
goals of the task).
Each group of task keystrokes that occurred
between dialogue utterances was tagged, possibly
with many subtask labels, by a human judge. A
second judge tagged 20% of the corpus in a
reliability study for which one-to-one subtask
identification was not enforced (giving judges
maximum flexibility to apply the tags). To ensure a
conservative reliability statistic, all unmatched
subtask tags were treated as disagreements. The
resulting unweighted kappa statistic was ĸsimple=
0.58, but the weighted Kappa ĸweighted=0.86 is more
meaningful because it takes into account the
ordinal nature of the labels that result from
sequential subtasks. On task actions for which the
two judges agreed on subtask tag, the agreement
statistic for correctness was ĸsimple=0.80.
</bodyText>
<figureCaption confidence="0.997009">
Figure 1. Portion of task annotation scheme
</figureCaption>
<subsectionHeader confidence="0.997903">
3.3 Adjacency Pair Joining
</subsectionHeader>
<bodyText confidence="0.999938461538462">
Some dialogue acts establish an expectation for
another dialogue act to occur next (Schegloff &amp;
Sacks, 1973). Our previous work has found that
identifying the statistically significant adjacency
pairs in a corpus and joining them as atomic
observations prior to model building produces
more interpretable descriptive models. The models
reported here were trained on hybrid sequences of
dialogue acts and adjacency pairs. A full
description of the adjacency pair identification
methodology and joining algorithm is reported in
(Boyer et al., 2009). A partial list of the most
highly statistically significant adjacency pairs,
</bodyText>
<figure confidence="0.998843823529412">
QUESTION (Q)
question.
STATEMENT (S)
assertion.
Task or conceptual
Task or conceptual
ASSESSING
QUESTION (AQ)
EXTRA‐DOMAIN
(EX)
LUKEWARM
CONTENT
FEEDBACK (LCF)
LUKEWARM
FEEDBACK(LF)
NEGATIVE
CONTENT
FEEDBACK
(NCF)
NEGATIVE
FEEDBACK(NF)
POSITIVE
CONTENT
FEEDBACK (PCF)
POSITIVE
FEEDBACK (PF)
.01 .03 .53
.02 .03 .49
.01 .10 .61
.05 .02 .76
.02 .03 .43
.09 .16 .81
.09 .03 .85
.16 .41 .82
</figure>
<page confidence="0.990832">
68
</page>
<tableCaption confidence="0.636109">
which for this work include task actions, is
displayed in Table 2.
Table 2. Subset of significant adjacency pairs
</tableCaption>
<table confidence="0.996437222222222">
CORRECTTASKACTION-CORRECTTASKACTION;
EXTRADOMAINS-EXTRADOMAINT; GROUNDINGS-GROUNDINGT;
ASSESSINGQUESTIONT-POSITIVEFEEDBACKS;
ASSESSINGQUESTIONS-POSITIVEFEEDBACKT; QUESTIONT-STATEMENTS;
ASSESSINGQUESTIONT-STATEMENTS; EXTRADOMAINT-EXTRADOMAINS;
QUESTIONS-STATEMENTT; NEGATIVEFEEDBACKS-GROUNDINGT;
INCOMPLETETASKACTION-INCOMPLETETASKACTION;
POSITIVEFEEDBACKS-GROUNDINGT;
BUGGYTASKACTION-BUGGYTASKACTION
</table>
<sectionHeader confidence="0.990886" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.999882333333333">
We learned three types of models using cross-
validation with systematic sampling of training and
testing sets.
</bodyText>
<subsectionHeader confidence="0.913952">
4.1 First-Order Markov Model
</subsectionHeader>
<bodyText confidence="0.999899727272727">
The simplest model we discuss is the first-order
Markov model (MM), or bigram model (Figure 2).
A MM that generates observation (state) sequence
o1o2...or is defined in the following way. The
observation symbols are drawn from the alphabet
Y_={61, 62, ..., 6m}, and the initial probability
distribution is H=[7ci] where 7ci is the probability of
a sequence beginning with observation symbol 6i.
The transition probability distribution is A=[aij],
where aij is the probability of observation j
occurring immediately after observation i.
</bodyText>
<figureCaption confidence="0.990658">
Figure 2. Time-slice topology of MM
</figureCaption>
<bodyText confidence="0.99985375">
We trained MMs on our corpus of dialogue acts
and task events using ten-fold cross-validation to
produce a model that could be queried for the next
predicted tutorial dialogue act given the history.
</bodyText>
<subsectionHeader confidence="0.947543">
4.2 Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.999539761904762">
A hidden Markov model (HMM) augments the
MM framework, resulting in a doubly stochastic
structure (Rabiner, 1989). For a first-order HMM,
the observation symbol alphabet is defined as
above, along with a set of hidden states
S={s1,s2,...,sN}. The transition and initial
probability distributions are defined analogously to
MMs, except that they operate on hidden states
rather than on observation symbols (Figure 3).
That is, H=[7ci] where 7ci is the probability of a
sequence beginning in hidden state si. The
transition matrix is A=[aij], where aij is the
probability of the model transitioning from hidden
state i to hidden state j. This framework constitutes
the first stochastic layer of the model, which can be
thought of as modeling hidden, or unobservable,
structure. The second stochastic layer of the model
governs the production of observation symbols: the
emission probability distribution is B=[bik] where
bik is the probability of state i emitting observation
symbol k.
</bodyText>
<figureCaption confidence="0.967302">
Figure 3. Time-slice topology of HMM
</figureCaption>
<bodyText confidence="0.999370153846154">
The notion that dialogue has an overarching
unobservable structure that influences the
observations is widely accepted. In tutoring, this
overarching structure may correspond to tutorial
strategies. We have explored HMMs’ descriptive
power for extracting these strategies (Boyer et al.,
2009), and this paper explores the hypothesis that
HMMs provide better predictive power than MMs
on our dialogue sequences. We trained HMMs on
the corpus using the standard Baum-Welch
expectation maximization algorithm and applied
state labels that reflect post-hoc interpretation
(Figure 4).
</bodyText>
<figureCaption confidence="0.988814">
Figure 4. Portion of learned HMM
</figureCaption>
<page confidence="0.997128">
69
</page>
<subsectionHeader confidence="0.995638">
4.3 Hierarchical Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.99992408">
Hierarchical hidden Markov models (HHMMs)
allow for explicit representation of multilevel
stochastic structure. A complete formal definition
of HHMMs can be found in (Fine, Singer, &amp;
Tishby, 1998), but here we present an informal
description. HHMMs include two types of hidden
states: internal nodes, which do not produce
observation symbols, and production nodes, which
do produce observations. An internal node includes
a set of substates that correspond to its potential
children, S={s1, s2, É, sN}, each of which is itself
the root of an HHMM. The initial probability
distribution Π=[π;] for each internal node governs
the probability that the model will make a vertical
transition to substate si from this internal node; that
is, that this internal node will produce substate s; as
its leftmost child. Horizontal transitions are
governed by a transition probability distribution
similar to that described above for flat HMMs.
Production nodes are defined by their observation
symbol alphabet and an emission probability
distribution over the symbols; HHMMs do not
require a global observation symbol alphabet. The
generative topology of our HHMMs is illustrated
in Figure 5.
</bodyText>
<figureCaption confidence="0.85807">
Figure 5. Generative topology of HHMM
</figureCaption>
<bodyText confidence="0.750884727272727">
HHMMs of arbitrary topology can be trained using
a generalized version of the Baum-Welch
algorithm (Fine et al., 1998). Our HHMMs
featured a pre-specified model topology based on
known task/subtask structure. A Bayesian view of
a portion of the best-fit HHMM is depicted in
Figure 6. This model was trained using five-fold
cross-validation to address the absence of symbols
from the training set that were present in the
testing set, a sparsity problem that arose from
splitting the data hierarchically.
</bodyText>
<figureCaption confidence="0.976605">
Figure 6. Portion of learned HHMM
</figureCaption>
<page confidence="0.992954">
70
</page>
<sectionHeader confidence="0.802188" genericHeader="evaluation">
5 Results not outperform baseline (p=0.40) for the
UNDERSTAND THE PROBLEM subtask, and
</sectionHeader>
<bodyText confidence="0.815517954545455">
qualitative inspection of the corpus reveals that the
dialogue during this phase of tutoring exhibits
limited regularities between students.
We trained and tested MMs, HMMs, and HHMMs
on the corpus and compared prediction accuracy
for tutorial dialogue acts by providing the model
with partial sequences from the test set and
querying for the next tutorial move. The baseline
prediction accuracy for this task is 41.1%,
corresponding to the most frequent tutorial
dialogue act (STATEMENT). As depicted in
Figure 7, a first-order MM performed worse than
baseline (p&lt;0.001)1 at 27% average prediction
accuracy ( σMM=6%). HMMs performed better
than baseline (p&lt;0.0001), with an average accuracy
of 48% (σö HMM=3%). HHMMs averaged 57%
accuracy, significantly higher than baseline
€
(p=0.002) but weakly significantly higher than
HMMs (p=0.04), and with high variation
€
( σ ö HHMM=23%).
</bodyText>
<figureCaption confidence="0.9404475">
Figure 8. Average prediction accuracies of
HHMMs by subtask
</figureCaption>
<sectionHeader confidence="0.32597" genericHeader="references">
6 Discussion
</sectionHeader>
<bodyText confidence="0.904387509090909">
The results support our hypothesis that HMMs,
because of their capacity for explicitly representing
dialogue structure at an abstract level, perform
better than MMs for predicting tutor moves. The
results also suggest that explicitly modeling
hierarchical task structure can further improve
prediction accuracy of the model. The below-
baseline performance of the bigram model
illustrates that in our complex task-oriented
domain, an immediately preceding event is not
highly predictive of the next move. While this
finding may not hold for conversational dialogue
or some task-oriented dialogue with a more
balanced distribution of utterances between
speakers, the unbalanced nature of our tutoring
sessions may not be as easily captured.
In our corpus, tutor utterances outnumber
student utterances by more than two to one. This
large difference is due to the fact that tutors
frequently guided students and provided multi-turn
explanations, the impetus for which are not
captured in the corpus, but rather, involve external
pedagogical goals. The MM, or bigram model, has
no mechanism for capturing this layer of stochastic
behavior. On the other hand, the HMM can
account for unobserved influential variables, and
the HHMM can do so to an even greater extent by
explicitly modeling task/subtask structure.
Considering the performance of the HHMM on
individual subtasks reveals interesting properties of
our dialogues. First, the HHMM is unable to
outperform baseline on the UNDERSTAND THE
PROBLEM subtask. To address this issue, our
ongoing work investigates taking into account
Figure 7. Average prediction accuracies of three
model types on tutor dialogue acts
To further explore the performance of the
HHMMs, Figure 8 displays their prediction
accuracy on each of six labeled subtasks. These
subtasks correspond to the top level of the
hierarchical task/subtask annotation scheme. The
UNDERSTAND THE PROBLEM subtask corresponds
to the initial phase of most tutoring sessions, in
which the student and tutor agree to some extent
on a problem-solving plan. Subtasks 1, 2, and 3
account for the implementation and debugging of
three distinct modules within the learning task, and
Subtask 4 involves testing and assessing the
student’s finalized program. The EXTRA-DOMAIN
subtask involves side conversations whose topics
are outside of the domain.
The HHMM performed as well as or better
(p&lt;0.01) than baseline on the first three in-domain
subtasks. The performance on SUBTASK 4 was not
distinguishable from baseline (p=0.06); relatively
few students reached this subtask. The model did
1 All p-values in this section were produced by two-sample
one-tailed t-tests with unequal sample variances.
71
student characteristics such as incoming the potential of a learned dialogue management
knowledge level and self-confidence. On all four model.
in-domain subtasks, the HHMM achieved a 30% to 7 Conclusion and Future Work
50% increase over baseline. For extra-domain Learning models that predict system moves within
dialogues, which involve side conversations that a corpus is a first step toward building fully data-
are not task-related, the HHMM achieved 86% driven dialogue management models. We have
prediction accuracy on tutor moves, which presented Markov models, hidden Markov models,
constitutes a 115% improvement over baseline. and hierarchical hidden Markov models trained on
This high accuracy may be due in part to the fact sequences of manually annotated dialogue acts and
that out-of-domain asides were almost exclusively task events. Of the three models, the hierarchical
initiated by the student, and tutors rarely engaged models appear to perform best in our domain,
in such exchanges beyond providing a single which involves an intrinsically hierarchical
response. This regularity likely facilitated task/subtask structure.
prediction of the tutor’s dialogue moves during The models’ performance points to promising
out-of-domain talk. future work that includes utilizing additional
We are aware of only one recent project that lexical and syntactic features along with fixed user
reports extensively on predicting system actions (student) characteristics within a hierarchical
from a corpus of human-human dialogue. hidden Markov modeling framework. More
Bangalore et al.’s (2008) flat task/dialogue model broadly, the results point to the importance of
in a catalogue-ordering domain achieved a considering task structure when modeling a
prediction accuracy of 55% for system dialogue complex domain such as those that often
acts, a 175% improvement over baseline. When accompany task-oriented tutoring. Finally, a key
explicitly modeling the hierarchical task/subtask direction for data-driven dialogue management
dialogue structure, they report a prediction models involves learning unsupervised dialogue
accuracy of 35.6% for system moves, act and task classification models.
approximately 75% above baseline (Bangalore &amp; Acknowledgements. This work is supported in
Stent, 2009). These findings were obtained by part by the North Carolina State University
utilizing a variety of lexical and syntactic features Department of Computer Science and the National
along with manually annotated dialogue acts and Science Foundation through a Graduate Research
task/subtask labels. In comparison, our HHMM Fellowship and Grants CNS-0540523, REC-
achieved an average 42% improvement over 0632450 and IIS-0812291. Any opinions, findings,
baseline using only annotated dialogue acts and conclusions, or recommendations expressed in this
task/subtask labels. In ongoing work we are report are those of the participants, and do not
exploring the utility of additional features for this necessarily represent the official views, opinions,
prediction task. or policy of the National Science Foundation.
Our best model performed better than baseline References
by a significant margin. The absolute prediction Ai, H., Tetreault, J. R., &amp; Litman, D. J. (2007).
accuracy achieved by the HHMM was 57% across Comparing user simulation models for dialog strategy
the corpus, which at first blush may appear too low learning. Proceedings of NAACL HLT, Companion
to be of practical use. However, the choice of Volume, Rochester, New York. 1-4.
tutorial move involves some measure of Aleven, V., McLaren, B., Roll, I., &amp; Koedinger, K.
subjectivity, and in many contexts there may be no (2004). Toward tutoring help seeking: Applying
uniquely appropriate dialogue act. Work in other cognitive modeling to meta-cognitive skills.
domains has dealt with this uncertainty by Proceedings of ITS, 227-239.
maintaining multiple hypotheses (Wright Hastie, Bangalore, S., Di Fabbrizio, G., &amp; Stent, A. (2008).
Poesio, &amp; Isard, 2002) and by mapping to clustered Learning the structure of task-driven human-human
sets of moves rather than maintaining policies for dialogs. IEEE Transactions on Audio, Speech, and
each possible system selection (Young et al., Language Processing, 16(7), 1249-1259.
2009). Such approaches may prove useful in our
domain as well, and may help to more fully realize
72
</bodyText>
<reference confidence="0.995831302752293">
Bangalore, S., &amp; Stent, A. J. (2009). Incremental
parsing models for dialog task structure. Proceedings
of the EACL, 94-102.
Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D.,
Vouk, M. A., &amp; Lester, J. C. (2009). Modeling
dialogue structure with adjacency pair analysis and
hidden Markov models. Proceedings of NAACL HLT
(Short Papers), 19-26.
Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis,
M. D., Vouk, M. A., &amp; Lester, J. C. (In press).
Characterizing the effectiveness of tutorial dialogue
with hidden Markov models. Proceedings of ITS,
Pittsburgh, Pennsylvania.
Chi, M., Jordan, P., VanLehn, K., &amp; Hall, M. (2008).
Reinforcement learning-based feature selection for
developing pedagogically effective tutorial dialogue
tactics. Proceedings of EDM, Montreal, Canada. 258-
265.
Chi, M., Jordan, P., VanLehn, K., &amp; Litman, D. (2009).
To elicit or to tell: Does it matter? Proceedings of
AIED, 197-204.
Core, M., &amp; Allen, J. (1997). Coding dialogs with the
DAMSL annotation scheme. AAAI Fall Symposium on
Communicative Action in Humans and Machines, 28–
35.
Evens, M., &amp; Michael, J. (2006). One-on-one tutoring
by humans and computers. Mahwah, New Jersey:
Lawrence Erlbaum Associates.
Fine, S., Singer, Y., &amp; Tishby, N. (1998). The
hierarchical hidden Markov model: Analysis and
applications. Machine Learning, 32(1), 41-62.
Forbes-Riley, K., Rotaru, M., Litman, D. J., &amp;
Tetreault, J. (2007). Exploring affect-context
dependencies for adaptive system development.
Proceedings of NAACL HLT (Short Papers), 41-44.
Forbes-Riley, K., &amp; Litman, D. (2009). Adapting to
student uncertainty improves tutoring dialogues.
Proceedings of AIED, 33-40.
Frampton, M., &amp; Lemon, O. (2009). Recent research
advances in reinforcement learning in spoken dialogue
systems. The Knowledge Engineering Review, 24(4),
375-408.
Graesser, A. C., Chipman, P., Haynes, B. C., &amp; Olney,
A. (2005). AutoTutor: An intelligent tutoring system
with mixed-initiative dialogue. IEEE Transactions on
Education, 48(4), 612-618.
Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A.,
Strzalkowski, T., Ursu, C., Webb, N., &amp; Wu, M.
(2006). The Amities system: Data-driven techniques
for automated dialogue. Speech Communication, 48(3-
4), 354-373.
Heeman, P. A. (2007). Combining reinforcement
learning with information-state update rules.
Proceedings of NAACL HLT, 268-275.
Henderson, J., Lemon, O., &amp; Georgila, K. (2008).
Hybrid reinforcement/supervised learning of dialogue
policies from fixed data sets. Computational
Linguistics, 34(4), 487-511.
Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn,
K., &amp; Albacete, P. (2006). A natural language tutorial
dialogue system for physics. Proceedings of FLAIRS,
521-526.
Kersey, C., Di Eugenio, B., Jordan, P., &amp; Katz, S.
(2009). KSC-PaL: A peer learning agent that
encourages students to take the initiative. Proceedings
of the NAACL HLT Workshop on Innovative use of
NLP for Building Educational Applications, Boulder,
Colorado. 55-63.
Levin, E., Pieraccini, R., &amp; Eckert, W. (2000). A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1), 11-23.
Litman, D., &amp; Forbes-Riley, K. (2006). Correlations
between dialogue acts and learning in spoken tutoring
dialogues. Natural Language Engineering, 12(2), 161-
176.
Poesio, M., &amp; Mikheev, A. (1998). The predictive
power of game structure in dialogue act recognition:
Experimental results using maximum entropy
estimation. Proceedings of ICSLP, 90-97.
Rabiner, L. R. (1989). A tutorial on hidden Markov
models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2), 257-286.
Schegloff, E., &amp; Sacks, H. (1973). Opening up closings.
Semiotica, 7(4), 289-327.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates,
R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-
Dykema, C., &amp; Meteer, M. (2000). Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3), 339-373.
Tetreault, J. R., &amp; Litman, D. J. (2008). A
reinforcement learning approach to evaluating state
representations in spoken dialogue systems. Speech
Communication, 50(8-9), 683-696.
VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan,
P., Olney, A., &amp; Rose, C. P. (2007). When are tutorial
dialogues more effective than reading? Cognitive
Science, 31(1), 3-62.
Wright Hastie, H., Poesio, M., &amp; Isard, S. (2002).
Automatically predicting dialogue structure using
prosodic features. Speech Communication, 36(1-2),
63-79.
Young, S., Gasic, M., Keizer, S., Mairesse, F.,
Schatzmann, J., Thomson, B., &amp; Yu, K. (2009). The
hidden information state model: A practical
framework for POMDP-based spoken dialogue
management. Computer Speech and Language, 24(2),
150-174.
</reference>
<page confidence="0.99929">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577693">
<title confidence="0.999723">Leveraging Hidden Dialogue State to Select Tutorial Moves</title>
<author confidence="0.941652">Kristy Elizabeth Eun Young Michael Mladen James</author>
<affiliation confidence="0.806289">D. of Computer Science, North Carolina State Research</affiliation>
<address confidence="0.994004">Raleigh, NC, USA</address>
<email confidence="0.99982">keboyer@ncsu.edu</email>
<email confidence="0.99982">rphilli@ncsu.edu</email>
<email confidence="0.99982">eha@ncsu.edu</email>
<email confidence="0.99982">mdwallis@ncsu.edu</email>
<email confidence="0.99982">vouk@ncsu.edu</email>
<email confidence="0.99982">lester@ncsu.edu</email>
<abstract confidence="0.999424421052631">A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A J Stent</author>
</authors>
<title>Incremental parsing models for dialog task structure.</title>
<date>2009</date>
<booktitle>Proceedings of the EACL,</booktitle>
<pages>94--102</pages>
<marker>Bangalore, Stent, 2009</marker>
<rawString>Bangalore, S., &amp; Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the EACL, 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>E Y Ha</author>
<author>M D Wallis</author>
<author>M A Vouk</author>
<author>J C Lester</author>
</authors>
<title>Modeling dialogue structure with adjacency pair analysis and hidden Markov models.</title>
<date>2009</date>
<booktitle>Proceedings of NAACL HLT (Short Papers),</booktitle>
<contexts>
<context position="10740" citStr="Boyer et al., 2009" startWordPosition="1578" endWordPosition="1581">. Figure 1. Portion of task annotation scheme 3.3 Adjacency Pair Joining Some dialogue acts establish an expectation for another dialogue act to occur next (Schegloff &amp; Sacks, 1973). Our previous work has found that identifying the statistically significant adjacency pairs in a corpus and joining them as atomic observations prior to model building produces more interpretable descriptive models. The models reported here were trained on hybrid sequences of dialogue acts and adjacency pairs. A full description of the adjacency pair identification methodology and joining algorithm is reported in (Boyer et al., 2009). A partial list of the most highly statistically significant adjacency pairs, QUESTION (Q) question. STATEMENT (S) assertion. Task or conceptual Task or conceptual ASSESSING QUESTION (AQ) EXTRA‐DOMAIN (EX) LUKEWARM CONTENT FEEDBACK (LCF) LUKEWARM FEEDBACK(LF) NEGATIVE CONTENT FEEDBACK (NCF) NEGATIVE FEEDBACK(NF) POSITIVE CONTENT FEEDBACK (PCF) POSITIVE FEEDBACK (PF) .01 .03 .53 .02 .03 .49 .01 .10 .61 .05 .02 .76 .02 .03 .43 .09 .16 .81 .09 .03 .85 .16 .41 .82 68 which for this work include task actions, is displayed in Table 2. Table 2. Subset of significant adjacency pairs CORRECTTASKACTION</context>
<context position="13979" citStr="Boyer et al., 2009" startWordPosition="2027" endWordPosition="2030">he model, which can be thought of as modeling hidden, or unobservable, structure. The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[bik] where bik is the probability of state i emitting observation symbol k. Figure 3. Time-slice topology of HMM The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted. In tutoring, this overarching structure may correspond to tutorial strategies. We have explored HMMs’ descriptive power for extracting these strategies (Boyer et al., 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4). Figure 4. Portion of learned HMM 69 4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, &amp; Tishby, 1998), but here we present an informal descripti</context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Vouk, Lester, 2009</marker>
<rawString>Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., &amp; Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of NAACL HLT (Short Papers), 19-26.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>A Ingram</author>
<author>E Y Ha</author>
<author>M D Wallis</author>
<author>M A Vouk</author>
<author>J C Lester</author>
</authors>
<title>(In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models.</title>
<booktitle>Proceedings of ITS,</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<marker>Boyer, Phillips, Ingram, Ha, Wallis, Vouk, Lester, </marker>
<rawString>Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M. D., Vouk, M. A., &amp; Lester, J. C. (In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models. Proceedings of ITS, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chi</author>
<author>P Jordan</author>
<author>K VanLehn</author>
<author>M Hall</author>
</authors>
<title>Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics.</title>
<date>2008</date>
<booktitle>Proceedings of EDM,</booktitle>
<pages>258--265</pages>
<location>Montreal,</location>
<contexts>
<context position="2665" citStr="Chi, Jordan, VanLehn, &amp; Hall, 2008" startWordPosition="372" endWordPosition="377">have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 2006; Young et al., 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, &amp; Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors. The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification</context>
<context position="5136" citStr="Chi et al., 2008" startWordPosition="744" endWordPosition="747">formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nestin</context>
</contexts>
<marker>Chi, Jordan, VanLehn, Hall, 2008</marker>
<rawString>Chi, M., Jordan, P., VanLehn, K., &amp; Hall, M. (2008). Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics. Proceedings of EDM, Montreal, Canada. 258-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chi</author>
<author>P Jordan</author>
<author>K VanLehn</author>
<author>D Litman</author>
</authors>
<title>To elicit or to tell: Does it matter?</title>
<date>2009</date>
<booktitle>Proceedings of AIED,</booktitle>
<pages>197--204</pages>
<contexts>
<context position="1608" citStr="Chi, Jordan, VanLehn, &amp; Litman, 2009" startWordPosition="215" endWordPosition="220">ial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, &amp; Stent, 2008; Frampton &amp; Lemon, 2009; Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increas</context>
</contexts>
<marker>Chi, Jordan, VanLehn, Litman, 2009</marker>
<rawString>Chi, M., Jordan, P., VanLehn, K., &amp; Litman, D. (2009). To elicit or to tell: Does it matter? Proceedings of AIED, 197-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>J Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<volume>28</volume>
<pages>35</pages>
<contexts>
<context position="7746" citStr="Core &amp; Allen, 1997" startWordPosition="1133" endWordPosition="1136">tudents collaborated to solve an introductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al., 2000), task-oriented dialogue (Core &amp; Allen, 1997), and tutoring (Litman &amp; Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ĸ=0.80. 67 Table 1. Dialogue act tags Stu. Tut. Rel. Rel. DA Description Freq. Freq. κ Request for feedback on task or conceptual .20 .11 .91 utterance. Asides not relevant to the .08 .04 .79 tutoring task. GROUNDING (G) Acknowledgement/thanks .26 .06 .92 Negative assessment with explanation. Lukewarm assessment of task action or conceptual utterance. Negative assessment with explanation. Negative assessment of task action or conceptual u</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Core, M., &amp; Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evens</author>
<author>J Michael</author>
</authors>
<title>One-on-one tutoring by humans and computers.</title>
<date>2006</date>
<location>Mahwah, New Jersey: Lawrence Erlbaum Associates.</location>
<contexts>
<context position="1826" citStr="Evens &amp; Michael, 2006" startWordPosition="248" endWordPosition="251">(Bangalore, Di Fabbrizio, &amp; Stent, 2008; Frampton &amp; Lemon, 2009; Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have </context>
<context position="6093" citStr="Evens &amp; Michael, 2006" startWordPosition="888" endWordPosition="891">cus of work in other task-oriented domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio &amp; Mikheev, 1998). For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens &amp; Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2007; Forbes-Riley &amp; Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, &amp; Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al.; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts. In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Cor</context>
</contexts>
<marker>Evens, Michael, 2006</marker>
<rawString>Evens, M., &amp; Michael, J. (2006). One-on-one tutoring by humans and computers. Mahwah, New Jersey: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fine</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>The hierarchical hidden Markov model: Analysis and applications.</title>
<date>1998</date>
<booktitle>Machine Learning,</booktitle>
<volume>32</volume>
<issue>1</issue>
<pages>41--62</pages>
<contexts>
<context position="14535" citStr="Fine, Singer, &amp; Tishby, 1998" startWordPosition="2107" endWordPosition="2111">’ descriptive power for extracting these strategies (Boyer et al., 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4). Figure 4. Portion of learned HMM 69 4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, &amp; Tishby, 1998), but here we present an informal description. HHMMs include two types of hidden states: internal nodes, which do not produce observation symbols, and production nodes, which do produce observations. An internal node includes a set of substates that correspond to its potential children, S={s1, s2, É, sN}, each of which is itself the root of an HHMM. The initial probability distribution Π=[π;] for each internal node governs the probability that the model will make a vertical transition to substate si from this internal node; that is, that this internal node will produce substate s; as its left</context>
<context position="15676" citStr="Fine et al., 1998" startWordPosition="2285" endWordPosition="2288">l node; that is, that this internal node will produce substate s; as its leftmost child. Horizontal transitions are governed by a transition probability distribution similar to that described above for flat HMMs. Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global observation symbol alphabet. The generative topology of our HHMMs is illustrated in Figure 5. Figure 5. Generative topology of HHMM HHMMs of arbitrary topology can be trained using a generalized version of the Baum-Welch algorithm (Fine et al., 1998). Our HHMMs featured a pre-specified model topology based on known task/subtask structure. A Bayesian view of a portion of the best-fit HHMM is depicted in Figure 6. This model was trained using five-fold cross-validation to address the absence of symbols from the training set that were present in the testing set, a sparsity problem that arose from splitting the data hierarchically. Figure 6. Portion of learned HHMM 70 5 Results not outperform baseline (p=0.40) for the UNDERSTAND THE PROBLEM subtask, and qualitative inspection of the corpus reveals that the dialogue during this phase of tutori</context>
</contexts>
<marker>Fine, Singer, Tishby, 1998</marker>
<rawString>Fine, S., Singer, Y., &amp; Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1), 41-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>D J Litman</author>
<author>J Tetreault</author>
</authors>
<title>Exploring affect-context dependencies for adaptive system development.</title>
<date>2007</date>
<booktitle>Proceedings of NAACL HLT (Short Papers),</booktitle>
<pages>41--44</pages>
<contexts>
<context position="6151" citStr="Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2007" startWordPosition="893" endWordPosition="898">d domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio &amp; Mikheev, 1998). For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens &amp; Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2007; Forbes-Riley &amp; Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, &amp; Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al.; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts. In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, Tetreault, 2007</marker>
<rawString>Forbes-Riley, K., Rotaru, M., Litman, D. J., &amp; Tetreault, J. (2007). Exploring affect-context dependencies for adaptive system development. Proceedings of NAACL HLT (Short Papers), 41-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Adapting to student uncertainty improves tutoring dialogues.</title>
<date>2009</date>
<booktitle>Proceedings of AIED,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6181" citStr="Forbes-Riley &amp; Litman, 2009" startWordPosition="899" endWordPosition="902">d catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio &amp; Mikheev, 1998). For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens &amp; Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2007; Forbes-Riley &amp; Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, &amp; Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al.; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts. In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human-human tutoring study in which</context>
</contexts>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>Forbes-Riley, K., &amp; Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frampton</author>
<author>O Lemon</author>
</authors>
<title>Recent research advances in reinforcement learning in spoken dialogue systems.</title>
<date>2009</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>375--408</pages>
<contexts>
<context position="1268" citStr="Frampton &amp; Lemon, 2009" startWordPosition="170" endWordPosition="173">hese systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, &amp; Stent, 2008; Frampton &amp; Lemon, 2009; Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005</context>
<context position="4653" citStr="Frampton &amp; Lemon, 2009" startWordPosition="667" endWordPosition="670">r of three stochastic models — first-order Markov models, hidden Markov models, and hierarchical hidden Markov models — for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not </context>
</contexts>
<marker>Frampton, Lemon, 2009</marker>
<rawString>Frampton, M., &amp; Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>P Chipman</author>
<author>B C Haynes</author>
<author>A Olney</author>
</authors>
<title>AutoTutor: An intelligent tutoring system with mixed-initiative dialogue.</title>
<date>2005</date>
<journal>IEEE Transactions on Education,</journal>
<volume>48</volume>
<issue>4</issue>
<pages>612--618</pages>
<contexts>
<context position="1868" citStr="Graesser, Chipman, Haynes, &amp; Olney, 2005" startWordPosition="252" endWordPosition="257">o, &amp; Stent, 2008; Frampton &amp; Lemon, 2009; Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains out</context>
</contexts>
<marker>Graesser, Chipman, Haynes, Olney, 2005</marker>
<rawString>Graesser, A. C., Chipman, P., Haynes, B. C., &amp; Olney, A. (2005). AutoTutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>A Biermann</author>
<author>R B Inouye</author>
<author>A McKenzie</author>
<author>T Strzalkowski</author>
<author>C Ursu</author>
<author>N Webb</author>
<author>M Wu</author>
</authors>
<title>The Amities system: Data-driven techniques for automated dialogue.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>354--373</pages>
<contexts>
<context position="2528" citStr="Hardy et al., 2006" startWordPosition="353" endWordPosition="356">&amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 2006; Young et al., 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, &amp; Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors. The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that </context>
<context position="5550" citStr="Hardy et al., 2006" startWordPosition="808" endWordPosition="811">tate space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio &amp; Mikheev, 1998). For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens &amp; Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 200</context>
</contexts>
<marker>Hardy, Biermann, Inouye, McKenzie, Strzalkowski, Ursu, Webb, Wu, 2006</marker>
<rawString>Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., &amp; Wu, M. (2006). The Amities system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
</authors>
<title>Combining reinforcement learning with information-state update rules.</title>
<date>2007</date>
<booktitle>Proceedings of NAACL HLT,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="4849" citStr="Heeman, 2007" startWordPosition="701" endWordPosition="702"> to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in th</context>
</contexts>
<marker>Heeman, 2007</marker>
<rawString>Heeman, P. A. (2007). Combining reinforcement learning with information-state update rules. Proceedings of NAACL HLT, 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>O Lemon</author>
<author>K Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<pages>487--511</pages>
<contexts>
<context position="4835" citStr="Henderson, Lemon, &amp; Georgila, 2008" startWordPosition="696" endWordPosition="700">equences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues </context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>Henderson, J., Lemon, O., &amp; Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>K VanLehn</author>
<author>P Albacete</author>
</authors>
<title>A natural language tutorial dialogue system for physics.</title>
<date>2006</date>
<booktitle>Proceedings of FLAIRS,</booktitle>
<pages>521--526</pages>
<contexts>
<context position="1926" citStr="Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006" startWordPosition="258" endWordPosition="264">Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 20</context>
</contexts>
<marker>Jordan, Makatchev, Pappuswamy, VanLehn, Albacete, 2006</marker>
<rawString>Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn, K., &amp; Albacete, P. (2006). A natural language tutorial dialogue system for physics. Proceedings of FLAIRS, 521-526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kersey</author>
<author>B Di Eugenio</author>
<author>P Jordan</author>
<author>S Katz</author>
</authors>
<title>KSC-PaL: A peer learning agent that encourages students to take the initiative.</title>
<date>2009</date>
<booktitle>Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications,</booktitle>
<pages>55--63</pages>
<location>Boulder, Colorado.</location>
<marker>Kersey, Di Eugenio, Jordan, Katz, 2009</marker>
<rawString>Kersey, C., Di Eugenio, B., Jordan, P., &amp; Katz, S. (2009). KSC-PaL: A peer learning agent that encourages students to take the initiative. Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Boulder, Colorado. 55-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>11--23</pages>
<contexts>
<context position="4511" citStr="Levin, Pieraccini, &amp; Eckert, 2000" startWordPosition="646" endWordPosition="650">es and task actions, and we propose a unified sequential representation for these separate synchronous streams of events. We explore the predictive power of three stochastic models — first-order Markov models, hidden Markov models, and hierarchical hidden Markov models — for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic c</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Levin, E., Pieraccini, R., &amp; Eckert, W. (2000). A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Correlations between dialogue acts and learning in spoken tutoring dialogues.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>161--176</pages>
<contexts>
<context position="7790" citStr="Litman &amp; Forbes-Riley, 2006" startWordPosition="1139" endWordPosition="1142">troductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al., 2000), task-oriented dialogue (Core &amp; Allen, 1997), and tutoring (Litman &amp; Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ĸ=0.80. 67 Table 1. Dialogue act tags Stu. Tut. Rel. Rel. DA Description Freq. Freq. κ Request for feedback on task or conceptual .20 .11 .91 utterance. Asides not relevant to the .08 .04 .79 tutoring task. GROUNDING (G) Acknowledgement/thanks .26 .06 .92 Negative assessment with explanation. Lukewarm assessment of task action or conceptual utterance. Negative assessment with explanation. Negative assessment of task action or conceptual utterance. Positive assessment with explanati</context>
</contexts>
<marker>Litman, Forbes-Riley, 2006</marker>
<rawString>Litman, D., &amp; Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>A Mikheev</author>
</authors>
<title>The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation.</title>
<date>1998</date>
<booktitle>Proceedings of ICSLP,</booktitle>
<pages>90--97</pages>
<contexts>
<context position="5916" citStr="Poesio &amp; Mikheev, 1998" startWordPosition="863" endWordPosition="866">successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore et al., our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio &amp; Mikheev, 1998). For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens &amp; Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2007; Forbes-Riley &amp; Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, &amp; Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al.; we explore first-order Markov models, which are equivalent to bigram models, for predicting tuto</context>
</contexts>
<marker>Poesio, Mikheev, 1998</marker>
<rawString>Poesio, M., &amp; Mikheev, A. (1998). The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation. Proceedings of ICSLP, 90-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<journal>Semiotica,</journal>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<contexts>
<context position="12779" citStr="Rabiner, 1989" startWordPosition="1848" endWordPosition="1849">ribution is H=[7ci] where 7ci is the probability of a sequence beginning with observation symbol 6i. The transition probability distribution is A=[aij], where aij is the probability of observation j occurring immediately after observation i. Figure 2. Time-slice topology of MM We trained MMs on our corpus of dialogue acts and task events using ten-fold cross-validation to produce a model that could be queried for the next predicted tutorial dialogue act given the history. 4.2 Hidden Markov Model A hidden Markov model (HMM) augments the MM framework, resulting in a doubly stochastic structure (Rabiner, 1989). For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s1,s2,...,sN}. The transition and initial probability distributions are defined analogously to MMs, except that they operate on hidden states rather than on observation symbols (Figure 3). That is, H=[7ci] where 7ci is the probability of a sequence beginning in hidden state si. The transition matrix is A=[aij], where aij is the probability of the model transitioning from hidden state i to hidden state j. This framework constitutes the first stochastic layer of the model, which can</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286. Schegloff, E., &amp; Sacks, H. (1973). Opening up closings. Semiotica, 7(4), 289-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van EssDykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van EssDykema, C., &amp; Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Tetreault</author>
<author>D J Litman</author>
</authors>
<title>A reinforcement learning approach to evaluating state representations in spoken dialogue systems.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>8</issue>
<pages>683--696</pages>
<contexts>
<context position="2692" citStr="Tetreault &amp; Litman, 2008" startWordPosition="378" endWordPosition="381">ss of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 2006; Young et al., 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, &amp; Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors. The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification of this artifact, in our c</context>
<context position="5041" citStr="Tetreault &amp; Litman, 2008" startWordPosition="728" endWordPosition="731">selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al., 2006) and catalogue ordering (Bangalore et al., 2008). Like the parse-based models of Bangalore </context>
</contexts>
<marker>Tetreault, Litman, 2008</marker>
<rawString>Tetreault, J. R., &amp; Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9), 683-696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>A C Graesser</author>
<author>G T Jackson</author>
<author>P Jordan</author>
<author>A Olney</author>
<author>C P Rose</author>
</authors>
<title>When are tutorial dialogues more effective than reading?</title>
<date>2007</date>
<journal>Cognitive Science,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>3--62</pages>
<contexts>
<context position="2115" citStr="VanLehn et al., 2007" startWordPosition="290" endWordPosition="293">uld significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, &amp; Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 2006; Young et al., 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, &amp; Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately, devising </context>
</contexts>
<marker>VanLehn, Graesser, Jackson, Jordan, Olney, Rose, 2007</marker>
<rawString>VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., &amp; Rose, C. P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wright Hastie</author>
<author>H Poesio</author>
<author>M</author>
<author>S Isard</author>
</authors>
<title>Automatically predicting dialogue structure using prosodic features.</title>
<date>2002</date>
<journal>Speech Communication,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>63--79</pages>
<marker>Hastie, Poesio, M, Isard, 2002</marker>
<rawString>Wright Hastie, H., Poesio, M., &amp; Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>150--174</pages>
<contexts>
<context position="1289" citStr="Young et al., 2009" startWordPosition="174" endWordPosition="177"> data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, &amp; Stent, 2008; Frampton &amp; Lemon, 2009; Young et al., 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, &amp; Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 66 derived from observing human tutors (e.g., Aleven, McLaren, Roll, &amp; Koedinger, 2004; Evens &amp; Michael, 2006; Graesser, Chipman, Haynes, &amp; Olney, 2005; Jordan, Makatchev, </context>
<context position="2549" citStr="Young et al., 2009" startWordPosition="357" endWordPosition="360">hile these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al., 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al., 2008; Hardy et al., 2006; Young et al., 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, &amp; Hall, 2008; Tetreault &amp; Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors. The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to </context>
<context position="4870" citStr="Young et al., 2009" startWordPosition="703" endWordPosition="706">fective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, &amp; Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton &amp; Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, &amp; Litman, 2007; Henderson, Lemon, &amp; Georgila, 2008; Heeman, 2007; Young et al., 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault &amp; Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al., 2008). While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the f</context>
</contexts>
<marker>Young, Gasic, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2009</marker>
<rawString>Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., &amp; Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>