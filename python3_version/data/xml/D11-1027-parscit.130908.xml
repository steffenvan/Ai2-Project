<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000593">
<title confidence="0.992465">
Minimally Supervised Event Causality Identification
</title>
<author confidence="0.99934">
Quang Xuan Do Yee Seng Chan Dan Roth
</author>
<affiliation confidence="0.859273">
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998681">
{quangdo2,chanys,danr}@illinois.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999630133333334">
This paper develops a minimally supervised
approach, based on focused distributional sim-
ilarity methods and discourse connectives,
for identifying of causality relations between
events in context. While it has been shown
that distributional similarity can help identify-
ing causality, we observe that discourse con-
nectives and the particular discourse relation
they evoke in context provide additional in-
formation towards determining causality be-
tween events. We show that combining dis-
course relation predictions and distributional
similarity methods in a global inference pro-
cedure provides additional improvements to-
wards determining event causality.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999813">
An important part of text understanding arises from
understanding the semantics of events described in
the narrative, such as identifying the events that are
mentioned and how they are related semantically.
For instance, when given a sentence “The police
arrested him because he killed someone.”, humans
understand that there are two events, triggered by
the words “arrested” and “killed”, and that there is
a causality relationship between these two events.
Besides being an important component of discourse
understanding, automatically identifying causal re-
lations between events is important for various nat-
ural language processing (NLP) applications such
as question answering, etc. In this work, we auto-
matically detect and extract causal relations between
events in text.
Despite its importance, prior work on event
causality extraction in context in the NLP litera-
ture is relatively sparse. In (Girju, 2003), the au-
thor used noun-verb-noun lexico-syntactic patterns
to learn that “mosquitoes cause malaria”, where the
cause and effect mentions are nominals and not nec-
essarily event evoking words. In (Sun et al., 2007),
the authors focused on detecting causality between
search query pairs in temporal query logs. (Beamer
and Girju, 2009) tried to detect causal relations be-
tween verbs in a corpus of screen plays, but limited
themselves to consecutive, or adjacent verb pairs.
In (Riaz and Girju, 2010), the authors first cluster
sentences into topic-specific scenarios, and then fo-
cus on building a dataset of causal text spans, where
each span is headed by a verb. Thus, their focus was
not on identifying causal relations between events in
a given text document.
In this paper, given a text document, we first iden-
tify events and their associated arguments. We then
identify causality or relatedness relations between
event pairs. To do this, we develop a minimally su-
pervised approach using focused distributional sim-
ilarity methods, such as co-occurrence counts of
events collected automatically from an unannotated
corpus, to measure and predict existence of causal-
ity relations between event pairs. Then, we build on
the observation that discourse connectives and the
particular discourse relation they evoke in context
provide additional information towards determining
causality between events. For instance, in the ex-
ample sentence provided at the beginning of this
section, the words “arrested” and “killed” probably
have a relatively high apriori likelihood of being ca-
</bodyText>
<page confidence="0.972482">
294
</page>
<note confidence="0.957851">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294–303,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999550166666667">
sually related. However, knowing that the connec-
tive “because” evokes a contingency discourse re-
lation between the text spans “The police arrested
him” and “he killed someone” provides further ev-
idence towards predicting causality. The contribu-
tions of this paper are summarized below:
</bodyText>
<listItem confidence="0.9763206">
• Our focus is on identifying causality between
event pairs in context. Since events are of-
ten triggered by either verbs (e.g. “attack”) or
nouns (e.g. “explosion”), we allow for detec-
tion of causality between verb-verb, verb-noun,
and noun-noun triggered event pairs. To the
best of our knowledge, this formulation of the
task is novel.
• We developed a minimally supervised ap-
proach for the task using focused distributional
similarity methods that are automatically col-
lected from an unannotated corpus. We show
that our approach achieves better performance
than two approaches: one based on a frequently
used metric that measures association, and an-
other based on the effect-control-dependency
(ECD) metric described in a prior work (Riaz
and Girju, 2010).
• We leverage on the interactions between event
causality prediction and discourse relations
</listItem>
<bodyText confidence="0.8503216">
prediction. We combine these knowledge
sources through a global inference procedure,
which we formalize via an Integer Linear Pro-
gramming (ILP) framework as a constraint op-
timization problem (Roth and Yih, 2004). This
allows us to easily define appropriate con-
straints to ensure that the causality and dis-
course predictions are coherent with each other,
thereby improving the performance of causality
identification.
</bodyText>
<sectionHeader confidence="0.988802" genericHeader="introduction">
2 Event Causality
</sectionHeader>
<bodyText confidence="0.998235153846154">
In this work, we define an event as an action or oc-
currence that happens with associated participants
or arguments. Formally, we define an event e
as: p(a1, a2, ... , an), where the predicate p is the
word that triggers the presence of e in text, and
a1, a2, ... , an are the arguments associated with
e. Examples of predicates could be verbs such as
“attacked”, “employs”, nouns such as “explosion”,
“protest”, etc., and examples of the arguments of
“attacked” could be its subject and object nouns.
To measure the causality association between a
pair of events ei and ej (in general, ei and ej
could be extracted from the same or different doc-
uments), we should use information gathered about
their predicates and arguments. A simple approach
would be to directly calculate the pointwise mu-
tual information (PMI)1 between pi(ai1, ai2, ... , ain)
and pj(aj1, aj2, ... , ajm). However, this leads to very
sparse counts as the predicate pi with its list of ar-
guments ai1,... , ain would rarely co-occur (within
some reasonable context distance) with predicate pj
and its entire list of arguments aj1, ... , ajm. Hence,
in this work, we measure causality association us-
ing three separate components and focused distribu-
tional similarity methods collected about event pairs
as described in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.93736">
2.1 Cause-Effect Association
</subsectionHeader>
<bodyText confidence="0.989925666666667">
We measure the causality or cause-effect association
(CEA) between two events ei and ej using the fol-
lowing equation:
</bodyText>
<equation confidence="0.999577">
CEA(ei, ej) =
spp(ei, ej) + spa(ei, ej) + saa(ei, ej) (1)
</equation>
<bodyText confidence="0.999978333333333">
where spp measures the association between event
predicates, spa measures the association between the
predicate of an event and the arguments of the other
event, and saa measures the association between
event arguments. In our work, we regard each event
e as being triggered and rooted at a predicate p.
</bodyText>
<subsubsectionHeader confidence="0.695079">
2.1.1 Predicate-Predicate Association
</subsubsectionHeader>
<bodyText confidence="0.998314">
We define spp as follows:
</bodyText>
<equation confidence="0.9838365">
spp(ei, ej) = PMI(pi, pj) x max(ui, uj)
xIDF(pi, pj) x Dist(pi, pj) (2)
</equation>
<bodyText confidence="0.999623333333333">
which takes into account the PMI between pred-
icates pi and pj of events ei and ej respectively,
as well as various other pieces of information. In
Suppes’ Probabilistic theory of Casuality (Suppes,
1970), he highlighted that event e is a possible cause
of event e&apos;, if e&apos; happens more frequently with e than
</bodyText>
<footnote confidence="0.9984685">
1PMI is frequently used to measure association between
variables.
</footnote>
<page confidence="0.998131">
295
</page>
<bodyText confidence="0.973388">
by itself, i.e. P(e0|e) &gt; P(e0). This can be easily
rewritten as P(e
</bodyText>
<equation confidence="0.991753166666667">
of PMI: P(e)P(e )
,e,), &gt; 1, similar to the definition
P(e, e0)
PMI(e, e0) = log P(e)P(e0)
which is only positive when P (e,e,)
P (e)P (e,) &gt; 1.
</equation>
<bodyText confidence="0.999730961538461">
Next, we build on the intuition that event predi-
cates appearing in a large number of documents are
probably not important or discriminative. Thus, we
penalize these predicates when calculating spp by
adopting the inverse document frequency (idf):
IDF (pi, pj) = idf(pi) x idf(pj) x idf(pi, pj),
where idf(p) = log D
1+N , D is the total number of
documents in the collection and N is the number of
documents that p occurs in.
We also award event pairs that are closer together,
while penalizing event pairs that are further apart in
texts, by incorporating the distance measure of Lea-
cock and Chodorow (1998), which was originally
used to measure similarity between concepts:
where sent(p) gives the sentence number (index) in
which p occurs and ws indicates the window-size
(of sentences) used. If pi and pj are drawn from the
same sentence, the numerator of the above fraction
will return 1. In our work, we set ws to 3 and thus,
if pi occurs in sentence k, the furthest sentence that
pj will be drawn from, is sentence k + 2.
The final component of Equation 2, max(ui, uj),
takes into account whether predicates (events) pi and
pj appear most frequently with each other. ui and uj
are defined as follows:
</bodyText>
<equation confidence="0.996304">
i P(pi,pj)
u =maxk[P (pi, pk)] − P (pi, pj) + E
P(pi,pj)
uj = maxk[P(pk,pj)] − P(pi,pj) + E,
</equation>
<bodyText confidence="0.99992475">
where we set e = 0.01 to avoid zeros in the denom-
inators. ui will be maximized if there is no other
predicate pk having a higher co-occurrence proba-
bility with pi, i.e. pk = pj. uj is treated similarly.
</bodyText>
<subsectionHeader confidence="0.7857105">
2.1.2 Predicate-Argument and
Argument-Argument Association
</subsectionHeader>
<bodyText confidence="0.998947">
We define spa as follows:
</bodyText>
<equation confidence="0.970692">
ei
spa (, ej) = 1 1:PMI (pi, a)
|Aej |a∈Aej
+ 1 PMI(pj, a), (3)
|Aei |a∈Aei
</equation>
<bodyText confidence="0.999121666666667">
where Aei and Aej are the sets of arguments of ei
and ej respectively.
Finally, we define saa as follows:
</bodyText>
<equation confidence="0.999620666666667">
saa(ei, ej) =
 |Aei   ||Ae; I a∈Aei a,∈A j , ) (4)
1 1: PMI(a a
</equation>
<bodyText confidence="0.9992744">
Together, spa and saa provide additional contexts
and robustness (in addition to spp) for measuring the
cause-effect association between events ei and ej.
Our formulation of CEA is inspired by the ECD
metric defined in (Riaz and Girju, 2010):
</bodyText>
<equation confidence="0.996594333333333">
dis(a, b)
ECD(a, b) = max(v, w) x −log (5)
2 x maxDistance
</equation>
<bodyText confidence="0.540838">
where
</bodyText>
<equation confidence="0.95523425">
P(a, b)
maxt[P(a, bt)] − P(a, b) + c
P(a, b)
maxt[P(at, b)] − P(a, b) + c,
</equation>
<bodyText confidence="0.999972230769231">
where ECD(a,b) measures the causality between two
events a and b (headed by verbs), and the sec-
ond component in the ECD equation is similar to
Dist(pi, pj). In our experiments, we will evaluate
the performance of ECD against our proposed ap-
proach.
So far, our definitions in this section are generic
and allow for any list of event argument types. In
this work, we focus on two argument types: agent
(subject) and patient (object), which are typical core
arguments of any event. We describe how we extract
event predicates and their associated arguments in
the section below.
</bodyText>
<sectionHeader confidence="0.994168" genericHeader="method">
3 Verbal and Nominal Predicates
</sectionHeader>
<bodyText confidence="0.9999685">
We consider that events are not only triggered by
verbs but also by nouns. For a verb (verbal predi-
cate), we extract its subject and object from its as-
sociated dependency parse. On the other hand, since
</bodyText>
<equation confidence="0.99869825">
Dist(pi, pj) = −log |sent(pi) − sent(pj) |+ 1
2x ws ,
P(a, b) x
P(b) − P(a, b) + e
P(a, b) x
P(a) − P(a, b) + c
v =
w=
</equation>
<page confidence="0.988486">
296
</page>
<bodyText confidence="0.9988255">
events are also frequently triggered by nominal pred-
icates, it is important to identify an appropriate list
of event triggering nouns. In our work, we gathered
such a list using the following approach:
</bodyText>
<listItem confidence="0.965921382352941">
• We first gather a list of deverbal nouns from the
set of most frequently occurring (in the Giga-
word corpus) 3,000 verbal predicate types. For
each verb type v, we go through all its Word-
Net2 senses and gather all its derivationally re-
lated nouns N„ 3.
• From N„, we heuristically remove nouns that
are less than three characters in length. We also
remove nouns whose first three characters are
different from the first three characters of v. For
each of the remaining nouns in N„, we mea-
sured its Levenstein (edit) distance from v and
keep the noun(s) with the minimum distance.
When multiple nouns have the same minimum
distance from v, we keep all of them.
• To further prune the list of nouns, we next re-
moved all nouns ending in “er”, “or”, or “ee”,
as these nouns typically refer to a person, e.g.
“writer”, “doctor”, “employee”. We also re-
move nouns that are not hyponyms (children)
of the first WordNet sense of the noun “event”4.
• Since we are concerned with nouns denoting
events, FrameNet (Ruppenhofer et al., 2010)
(FN) is a good resource for mining such nouns.
FN consists of frames denoting situations and
events. As part of the FN resource, each FN
frame consists of a list of lexical units (mainly
verbs and nouns) representing the semantics of
the frame. Various frame-to-frame relations are
also defined (in particular the inheritance re-
lation). Hence, we gathered all the children
frames of the FN frame “Event”. From these
children frames, we then gathered all their noun
lexical units (words) and add them to our list of
</listItem>
<footnote confidence="0.990685142857143">
2http://wordnet.princeton.edu/
3The WordNet resource provides derivational information
on words that are in different syntactic (i.e. part-of-speech) cat-
egories, but having the same root (lemma) form and that are
semantically related.
4The first WordNet sense of the noun “event” has the mean-
ing: “something that happens at a given place and time”
</footnote>
<bodyText confidence="0.999695647058824">
nouns. Finally, we also add a few nouns denot-
ing natural disaster from Wikipedia5.
Using the above approach, we gathered a list of
about 2,000 noun types. This current approach is
heuristics based which we intend to improve in the
future, and any such improvements should subse-
quently improve the performance of our causality
identification approach.
Event triggering deverbal nouns could have as-
sociated arguments (for instance, acting as subject,
object of the deverbal noun). To extract these ar-
guments, we followed the approach of (Gurevich
et al., 2008). Briefly, the approach uses linguistic
patterns to extract subjects and objects for deverbal
nouns, using information from dependency parses.
For more details, we refer the reader to (Gurevich et
al., 2008).
</bodyText>
<sectionHeader confidence="0.995013" genericHeader="method">
4 Discourse and Causality
</sectionHeader>
<bodyText confidence="0.9838928">
Discourse connectives are important for relating dif-
ferent text spans, helping us to understand a piece of
text in relation to its context:
[The police arrested him] because [he killed someone].
In the example sentence above, the discourse con-
nective (“because”) and the discourse relation it
evokes (in this case, the Cause relation) allows read-
ers to relate its two associated text spans, “The po-
lice arrested him” and “he killed someone”. Also,
notice that the verbs “arrested” and “killed”, which
cross the two text spans, are causally related. To
aid in extracting causal relations, we leverage on the
identification of discourse relations to provide addi-
tional contextual information.
To identify discourse relations, we use the Penn
Discourse Treebank (PDTB) (Prasad et al., 2007),
which contains annotations of discourse relations
in context. The annotations are done over the
Wall Street Journal corpus and the PDTB adopts a
predicate-argument view of discourse relations. A
discourse connective (e.g. because) takes two text
spans as its arguments. In the rest of this section,
we briefly describe the discourse relations in PDTB
and highlight how we might leverage them to aid in
determining event causality.
</bodyText>
<footnote confidence="0.996825">
5http://en.wikipedia.org/wiki/Natural disaster
</footnote>
<page confidence="0.994039">
297
</page>
<tableCaption confidence="0.998929">
Table 1: Coarse-grained and fine-grained discourse relations.
</tableCaption>
<table confidence="0.9808122">
Coarse-grained relations
Fine-grained relations
Concession, Contrast, Pragmatic-concession, Pragmatic-contrast
Cause, Condition, Pragmatic-cause, Pragmatic-condition
Alternative, Conjunction, Exception, Instantiation, List, Restatement
Asynchronous, Synchronous
Comparison
Contingency
Expansion
Temporal
</table>
<subsectionHeader confidence="0.973799">
4.1 Discourse Relations
</subsectionHeader>
<bodyText confidence="0.994969514285714">
PDTB contains annotations for four coarse-grained
discourse relation types, as shown in the left column
of Table 1. Each of these are further refined into
several fine-grained discourse relations, as shown in
the right column of the table.6 Next, we briefly de-
scribe these relations, highlighting those that could
potentially help to determine event causality.
Comparison A Comparison discourse relation
between two text spans highlights prominent differ-
ences between the situations described in the text
spans. An example sentence is:
Contrast: [According to the survey, x% of Chinese Inter-
net users prefer Google] whereas [y% prefer Baidu].
According to the PDTB annotation manual
(Prasad et al., 2007), the truth of both spans is in-
dependent of the established discourse relation. This
means that the text spans are not causally related and
thus, the existence of a Comparison relation should
imply that there is no causality relation across the
two text spans.
Contingency A Contingency relation between
two text spans indicates that the situation described
in one text span causally influences the situation in
the other. An example sentence is:
Cause: [The first priority is search and rescue] because
[many people are trapped under the rubble].
Existence of a Contingency relation potentially
implies that there exists at least one causal event
pair crossing the two text spans. The PDTB an-
notation manual states that while the Cause and
Condition discourse relations indicate casual influ-
ence in their text spans, there is no causal in-
fluence in the text spans of the Pragmatic-cause
and Pragmatic-condition relations. For instance,
Pragmatic-condition indicates that one span pro-
</bodyText>
<footnote confidence="0.6717455">
6PDTB further refines these fine-grained relations into a fi-
nal third level of relations, but we do not use them in this work.
</footnote>
<bodyText confidence="0.983267">
vides the context in which the description of the sit-
uation in the other span is relevant; for example:
Pragmatic-condition: If [you are thirsty], [there’s beer in
the fridge].
Hence, there is a need to also identify fine-grained
discourse relations.
Expansion Connectives evoking Expansion dis-
course relations expand the discourse, such as by
providing additional information, illustrating alter-
native situations, etc. An example sentence is:
Conjunction: [Over the past decade, x women were
killed] and [y went missing].
Most of the Expansion fine-grained relations (ex-
cept for Conjunction, which could connect arbitrary
pieces of text spans) should not contain causality re-
lations across its text spans.
Temporal These indicate that the situations de-
scribed in the text spans are related temporally. An
example sentence is:
Synchrony: [He was sitting at his home] when [the whole
world started to shake].
Temporal precedence of the (cause) event over the
(effect) event is a necessary, but not sufficient req-
uisite for causality. Hence by itself, Temporal re-
lations are probably not discriminative enough for
determining event causality.
</bodyText>
<subsectionHeader confidence="0.967183">
4.2 Discourse Relation Extraction System
</subsectionHeader>
<bodyText confidence="0.999935727272727">
Our work follows the approach and features de-
scribed in the state-of-the-art Ruby-based discourse
system of (Lin et al., 2010), to build an in-
house Java-based discourse relation extraction sys-
tem. Our system identifies explicit connectives in
text, predict their discourse relations, as well as their
associated text spans. Similar to (Lin et al., 2010),
we achieved a competitive performance of slightly
over 80% F1-score in identifying fine-grained rela-
tions for explicit connectives. Our system is devel-
oped using the Learning Based Java modeling lan-
</bodyText>
<page confidence="0.988432">
298
</page>
<bodyText confidence="0.9997836">
guage (LBJ) (Rizzolo and Roth, 2010) and will be
made available soon. Due to space constraints, we
refer interested readers to (Lin et al., 2010) for de-
tails on the features, etc.
In the example sentences given thus far in this sec-
tion, all the connectives were explicit, as they appear
in the texts. PDTB also provides annotations for im-
plicit connectives, which we do not use in this work.
Identifying implicit connectives is a harder task and
incorporating these is a possible future work.
</bodyText>
<sectionHeader confidence="0.978364" genericHeader="method">
5 Joint Inference for Causality Extraction
</sectionHeader>
<bodyText confidence="0.99950925">
To exploit the interactions between event pair
causality extraction and discourse relation identifi-
cation, we define appropriate constraints between
them, which can be enforced through the Con-
strained Conditional Models framework (aka ILP for
NLP) (Roth and Yih, 2007; Chang et al., 2008). In
doing this, the predictions of CEA (Section 2.1) and
the discourse system are forced to cohere with each
other. More importantly, this should improve the
performance of using only CEA to extract causal
event pairs. To the best of our knowledge, this ap-
proach for causality extraction is novel.
</bodyText>
<subsectionHeader confidence="0.953448">
5.1 CEA &amp; Discourse: Implementation Details
</subsectionHeader>
<bodyText confidence="0.999684609756098">
Let E denote the set of event mentions in a docu-
ment. Let EP = {(ei, ej) ∈ E × E  |ei ∈ E, ej ∈
E,i &lt; j, |sent(ei) − sent(ej) |≤ 2} denote the
set of event mention pairs in the document, where
sent(e) gives the sentence number in which event e
occurs. Note that in this work, we only extract event
pairs that are at most two sentences apart. Next, we
define LER = {“causal”, “¬ causal”} to be the set of
event relation labels that an event pair ep ∈ EP can
be associated with.
Note that the CEA metric as defined in Section 2.1
simply gives a score without it being bounded to be
between 0 and 1.0. However, to use the CEA score
as part of the inference process, we require that it be
bounded and thus can be used as a binary prediction,
that is, predicting an event pair as causal or ¬causal.
To enable this, we use a few development documents
to automatically find a threshold CEA score that sep-
arates scores indicating causal vs ¬causal. Based
on this threshold, the original CEA scores are then
rescaled to fall within 0 to 1.0. More details on this
are in Section 6.2.
Let C denote the set of connective mentions in a
document. We slightly modify our discourse sys-
tem as follows. We define LDR to be the set of
discourse relations. We initially add all the fine-
grained discourse relations listed in Table 1 to LDR.
In the PDTB corpus, some connective examples are
labeled with just a coarse-grained relation, with-
out further specifying a fine-grained relation. To
accommodate these examples, we add the coarse-
grained relations Comparison, Expansion, and Tem-
poral to LDR. We omit the coarse-grained Con-
tingency relation from LDR, as we want to sepa-
rate Cause and Condition from Pragmatic-cause and
Pragmatic-condition. This discards very few exam-
ples as only a very small number of connective ex-
amples are simply labeled with a Contingency label
without further specifying a fine-grained label. We
then retrained our discourse system to predict labels
in LDR.
</bodyText>
<subsectionHeader confidence="0.99138">
5.2 Constraints
</subsectionHeader>
<bodyText confidence="0.999986076923077">
We now describe the constraints used to support
joint inference, based on the predictions of the CEA
metric and the discourse classifier. Let sc(dr) be
the probability that connective c is predicated to be
of discourse relation dr, based on the output of our
discourse classifier. Let sep(er) be the CEA pre-
diction score (rescaled to range in [0,1]) that event
pair ep takes on the causal or ¬causal label er. Let
x(c,dr) be a binary indicator variable which takes on
the value 1 iff c is labeled with the discourse relation
dr. Similarly, let y(ep,er) be a binary variable which
takes on the value 1 iff ep is labeled as er. We then
define our objective function as follows:
</bodyText>
<equation confidence="0.99664475">
h X
max |LDR|
+|LER |X X isep(er) · y(ep,er) (6)
epE£P erEGER
</equation>
<bodyText confidence="0.799511">
subject to the following constraints:
</bodyText>
<equation confidence="0.999890444444444">
X x(c,dr) = 1 ∀c ∈ C (7)
drEGDR
X y(ep,er) = 1 ∀ep ∈ EP (8)
erEGER
x(c,dr) ∈ {0, 1} ∀c ∈ C, dr ∈ LDR (9)
y(ep,er) ∈ {0, 1} ∀ep ∈ EP, er ∈ LER(10)
X
cEC drEGDR
sc(dr) · x(c,dr)
</equation>
<page confidence="0.994599">
299
</page>
<bodyText confidence="0.99889825">
Equation (7) requires that each connective c can
only be assigned one discourse relation. Equation
(8) requires that each event pair ep can only be
causal or ¬causal. Equations (9) and (10) indicate
that x(c,dr) and y(ep,er) are binary variables.
To capture the relationship between event pair
causality and discourse relations, we use the follow-
ing constraints:
</bodyText>
<equation confidence="0.9738855">
�x(c,“Cause”) ≤ y(ep,“causal”) (11)
epE£Pc
�x(c,“Condition”) ≤ y(ep,“causal”), (12)
epE£Pc
</equation>
<bodyText confidence="0.999674">
where both equations are defined ∀c ∈ C. EPc is
defined to be the set of event pairs that cross the two
text spans associated with c. For instance, if the first
text span of c contains two event mentions ei, ej,
and there is one event mention ek in the second text
span of c, then EPc = {(ei, ek), (ej, ek)}. Finally,
the logical form of Equation (11) can be written as:
x(c,“Cause”) ⇒ y(epi,“causal”) ∨ ... ∨ y(epj,“causal”),
where epi, ... , epj are elements in EPc. This states
that if we assign the Cause discourse label to c,
then at least one of epi, ... , epj must be assigned as
causal. The interpretation of Equation (12) is simi-
lar.
We use two more constraints to capture the inter-
actions between event causality and discourse rela-
tions. First, we defined Cep as the set of connectives
c enclosing each event of ep in each of its text spans,
i.e.: one of the text spans of c contain one of the
event in ep, while the other text span of c contain the
other event in ep. Next, based on the discourse rela-
tions in Section 4.1, we propose that when an event
pair ep is judged to be causal, then the connective
c that encloses it should be evoking one of the dis-
course relations in LDRa = {“Cause”, “Condition”,
“Temporal”, “Asynchronous”, “Synchrony”, “Con-
junction”}. We capture this using the following con-
straint:
</bodyText>
<equation confidence="0.934504">
�y(ep,“causal”) ≤ x(c,dra) ∀c ∈ Cep (13)
draEGDRa
</equation>
<bodyText confidence="0.931178538461538">
The logical form of Equation (13) can be written as:
y(ep,“causal”) ⇒ x(c,“Cause”) ∨ x(c,“Condition”) . . . ∨
x(c,“Conjunction”). This states that if we assign ep as
causal, then we must assign to c one of the labels in
LDRa.
Finally, we propose that for any connectives evok-
ing discourse relations LDRb = {“Comparison”,
“Concession”, “Contrast”, “Pragmatic-concession”,
“Pragmatic-contrast”, “Expansion”, “Alternative”,
“Exception”, “Instantiation”, “List”, “Restate-
ment”}, any event pair(s) that it encloses should be
¬causal. We capture this using the following con-
straint:
</bodyText>
<equation confidence="0.9689625">
x(c,drb) ≤ y(ep,“�causal”)
∀ drb ∈ LDRb, ep ∈ EPc, (14)
</equation>
<bodyText confidence="0.9969865">
where the logical form of Equation (14) can be writ-
ten as: x(c,drb) ⇒ y(ep,“�causal”).
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999351">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999918379310345">
To collect the distributional statistics for measuring
CEA as defined in Equation (1), we applied part-
of-speech tagging, lemmatization, and dependency
parsing (Marneffe et al., 2006) on about 760K docu-
ments in the English Gigaword corpus (LDC catalog
number LDC2003T05).
We are not aware of any benchmark corpus for
evaluating event causality extraction in contexts.
Hence, we created an evaluation corpus using the
following process: Using news articles collected
from CNN7 during the first three months of 2010, we
randomly selected 20 articles (documents) as evalu-
ation data, and 5 documents as development data.
Two annotators annotated the documents for
causal event pairs, using two simple notions for
causality: the Cause event should temporally pre-
cede the Effect event, and the Effect event occurs be-
cause the Cause event occurs. However, sometimes
it is debatable whether two events are involved in a
causal relation, or whether they are simply involved
in an uninteresting temporal relation. Hence, we al-
lowed annotations of C to indicate causality, and R
to indicate relatedness (for situations when the exis-
tence of causality is debatable). The annotators will
simply identify and annotate the C or R relations be-
tween predicates of event pairs. Event arguments are
not explicitly annotated, although the annotators are
free to look at the entire document text while mak-
ing their annotation decisions. Finally, they are free
</bodyText>
<footnote confidence="0.993376">
7http://www.cnn.com
</footnote>
<page confidence="0.97656">
300
</page>
<table confidence="0.999893">
System Rec% Pre% F1%
PMIpp 26.6 20.8 23.3
ECDpp &amp;PMIpa,aa 40.9 23.5 29.9
CEA 62.2 28.0 38.6
CEA+Discourse 65.1 30.7 41.7
</table>
<tableCaption confidence="0.976726">
Table 2: Performance of baseline systems and our ap-
proaches on extracting Causal event relations.
</tableCaption>
<table confidence="0.9999518">
System Rec% Pre% F1%
PMIpp 27.8 24.9 26.2
ECDpp &amp;PMIpa,aa 42.4 28.5 34.1
CEA 63.1 33.7 43.9
CEA+Discourse 65.3 36.5 46.9
</table>
<tableCaption confidence="0.992011">
Table 3: Performance of the systems on extracting Causal
and Related event relations.
</tableCaption>
<bodyText confidence="0.99903352631579">
to annotate relations between predicates that have
any number of sentences in between and are not re-
stricted to a fixed sentence window-size.
After adjudication, we obtained a total of 492
C+R relation annotations, and 414 C relation anno-
tations on the evaluation documents. On the devel-
opment documents, we obtained 92 C +R and 71 C
relation annotations. The annotators overlapped on
10 evaluation documents. On these documents, the
first (second) annotator annotated 215 (199) C + R
relations, agreeing on 166 of these relations. To-
gether, they annotated 248 distinct relations. Us-
ing this number, their agreement ratio would be 0.67
(166/248). The corresponding agreement ratio for
C relations is 0.58. These numbers highlight that
causality identification is a difficult task, as there
could be as many as N2 event pairs in a document
(N is the number of events in the document). We
plan to make this annotated dataset available soon.8
</bodyText>
<subsectionHeader confidence="0.9981">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999836375">
As mentioned in Section 5.1, to enable translat-
ing (the unbounded) CEA scores into binary causal,
¬causal predictions, we need to rescale or calibrate
these scores to range in [0,1]. To do this, we first
rank all the CEA scores of all event pairs in the de-
velopment documents. Most of these event pairs will
be ¬causal. Based on the relation annotations in
these development documents, we scanned through
</bodyText>
<footnote confidence="0.968289">
8http://cogcomp.cs.illinois.edu/page/publication view/663
</footnote>
<note confidence="0.628956">
Precision(%) on top K event causality predictions
</note>
<figure confidence="0.910599">
5 10 15 20 25 30 35 40
K (number of causality predictions)
</figure>
<figureCaption confidence="0.999981">
Figure 1: Precision of the top K causality C predictions.
</figureCaption>
<bodyText confidence="0.999988939393939">
this ranked list of scores to locate the CEA score
t that gives the highest F1-score (on the develop-
ment documents) when used as a threshold between
causal vs ¬causal decisions. We then ranked all
the CEA scores of all event pairs gathered from the
760K Gigaword documents, discretized all scores
higher than t into B bins, and all scores lower than
t into B bins. Together, these 2B bins represent the
range [0,1]. We used B = 500. Thus, consecu-
tive bins represent a difference of 0.001 in calibrated
scores.
To measure the causality between a pair of
events ei and ej, a simple baseline is to calculate
PMI(pi, pj). Using a similar thresholding and cali-
bration process to translate PMI(pi, pj) scores into
binary causality decisions, we obtained a F1 score of
23.1 when measured over the causality C relations,
as shown in the row PMIpp of Table 2.
As mentioned in Section 2.1.2, Riaz and Girju
(2010) proposed the ECD metric to measure
causality between two events. Thus, as a point of
comparison, we replaced spp of Equation (1) with
ECD(a, b) of Equation (5), substituting a = pi and
b = pj. After thresholding and calibrating the scores
of this approach, we obtained a F1-score of 29.7, as
shown in the row ECDpp&amp;PMIpa,aa of Table 2.
Next, we evaluated our proposed CEA approach
and obtained a F1-score of 38.6, as shown in the row
CEA of Table 2. Thus, our proposed approach ob-
tained significantly better performance than the PMI
baseline and the ECD approach. Next, we per-
formed joint inference with the discourse relation
predictions as described in Section 5 and obtained
</bodyText>
<figure confidence="0.997778647058824">
Precision(%)
55
50
45
40
35
30
25
20
60
15
10
5
0
CEA
ECDpp &amp; PMIpa,aa
PMIpp
</figure>
<page confidence="0.995232">
301
</page>
<bodyText confidence="0.999978914285714">
an improved F1-score of 41.7. We note that we ob-
tained improvements in both recall and precision.
This means that with the aid of discourse relations,
we are able to recover more causal relations, as well
as reduce false-positive predictions.
Constraint Equations (11) and (12) help to re-
cover causal relations. For improvements in pre-
cision, as stated in the last paragraph of Section
5.2, identifying other discourse relations such as
“Comparison”, “Contrast”, etc., provides counter-
evidence to causality. Together with constraint
Equation (14), this helps to eliminate false-positive
event pairs as classified by CEA and contributes
towards CEA+Discourse having a higher precision
than CEA.
The corresponding results for extracting both
causality and relatedness C + R relations are given
in Table 3. For these experiments, the aim was for a
more relaxed evaluation and we simply collapsed C
and R into a single label.
Finally, we also measured the precision of the
top K causality C predictions, showing the preci-
sion trends in Figure 1. As shown, CEA in general
achieves higher precision when compared to PMIpp
and ECDpp&amp;PMIpa,aa. The trends for C + R pre-
dictions are similar.
Thus far, we had included both verbal and nom-
inal predicates in our evaluation. When we repeat
the experiments for ECDpp&amp;PMIpa,aa and CEA
on just verbal predicates, we obtained the respective
F1-scores of 31.8 and 38.3 on causality relations.
The corresponding F1-scores for casuality and relat-
edness relations are 35.7 and 43.3. These absolute
F1-scores are similar to those in Tables 2 and 3, dif-
fering by 1-2%.
</bodyText>
<sectionHeader confidence="0.994181" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.99998408">
We randomly selected 50 false-positive predictions
and 50 false-negative causality relations to analyze
the mistakes made by CEA.
Among the false-positives (precision errors), the
most frequent error type (56% of the errors) is that
CEA simply assigns a high score to event pairs that
are not causal; more knowledge sources are required
to support better predictions in these cases. The next
largest group of error (22%) involves events contain-
ing pronouns (e.g. “he”, “it”) as arguments. Ap-
plying coreference to replace these pronouns with
their canonical entity strings or labeling them with
semantic class information might be useful.
Among the false-negatives (recall errors), 23%
of the errors are due to CEA simply assigning a
low score to causal event pairs and more contex-
tual knowledge seems necessary for better predic-
tions. 19% of the recall errors arises from causal
event pairs involving nominal predicates that are not
in our list of event evoking noun types (described in
Section 3). A related 17% of recall errors involves
nominal predicates without any argument. For these,
less information is available for CEA to make pre-
dictions. The remaining group (15% of errors) in-
volves events containing pronouns as arguments.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999948466666667">
Although prior work in event causality extraction
in context is relatively sparse, there are many prior
works concerning other semantic aspects of event
extraction. Ji and Grishman (2008) extracts event
mentions (belonging to a predefined list of target
event types) and their associated arguments. In other
prior work (Chen et al., 2009; Bejan and Harabagiu,
2010), the authors focused on identifying another
type of event pair semantic relation: event corefer-
ence. Chambers and Jurafsky (2008; 2009) chain
events sharing a common (protagonist) participant.
They defined events as verbs and given an existing
chain of events, they predict the next likely event in-
volving the protagonist. This is different from our
task of detecting causality between arbitrary event
pairs that might or might not share common argu-
ments. Also, we defined events more broadly, as
those that are triggered by either verbs or nouns. Fi-
nally, although our proposed CEA metric has resem-
blance the ECD metric in (Riaz and Girju, 2010), our
task is different from theirs and our work differs in
many aspects. They focused on building a dataset of
causal text spans, whereas we focused on identifying
causal relations between events in a given text doc-
ument. They considered text spans headed by verbs
while we considered events triggered by both verbs
and nouns. Moreover, we combined event causality
prediction and discourse relation prediction through
a global inference procedure to further improve the
performance of event causality prediction.
</bodyText>
<page confidence="0.998026">
302
</page>
<sectionHeader confidence="0.99859" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99995475">
In this paper, using general tools such as the depen-
dency and discourse parsers which are not trained
specifically towards our target task, and a minimal
set of development documents for threshold tuning,
we developed a minimally supervised approach to
identify causality relations between events in con-
text. We also showed how to incorporate discourse
relation predictions to aid event causality predictions
through a global inference procedure. There are sev-
eral interesting directions for future work, including
the incorporation of other knowledge sources such
as coreference and semantic class predictions, which
were shown to be potentially important in our er-
ror analysis. We could also use discourse relations
to aid in extracting other semantic relations between
events.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999940307692308">
The authors thank the anonymous reviewers for their
insightful comments and suggestions. University of
Illinois at Urbana-Champaign gratefully acknowl-
edges the support of Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first
author thanks the Vietnam Education Foundation
(VEF) for its sponsorship. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796417910448">
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In CI-
CLING.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In ACL-HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In ACL.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In RANLP
workshop on Events in Emerging Text Types.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In ACL workshop on
Multilingual Summarization and Question Answering.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in knowl-
edge representation. Journal of Logic and Computa-
tion, 18, June.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In ACL.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. MIT Press.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. Tech-
nical report. http://www.comp.nus.edu.sg/ linzi-
hen/publications/tech2010.pdf.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2007. The penn discourse tree-
bank 2.0 annotation manual. Technical report.
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf.
Mehwish Riaz and Roxana Girju. 2010. Another look at
causality: Discovering scenario-specific contingency
relationships with no supervision. In ICSC.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC.
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In CoNLL.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2010. FrameNet II: Extended Theory and Practice.
http://framenet.icsi.berkeley.edu.
Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan,
Benyu Zhang, and Zheng Chen. 2007. Causal rela-
tion of queries from temporal logs. In WWW.
Patrick Suppes. 1970. A Probabilistic Theory of Causal-
ity. Amsterdam: North-Holland Publishing Company.
</reference>
<page confidence="0.999467">
303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933495">
<title confidence="0.999573">Minimally Supervised Event Causality Identification</title>
<author confidence="0.999942">Quang Xuan Do Yee Seng Chan Dan Roth</author>
<affiliation confidence="0.998361">Department of Computer University of Illinois at</affiliation>
<address confidence="0.965222">Urbana, IL 61801,</address>
<abstract confidence="0.9980669375">This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brandon Beamer</author>
<author>Roxana Girju</author>
</authors>
<title>Using a bigram event model to predict causal potential.</title>
<date>2009</date>
<booktitle>In CICLING.</booktitle>
<contexts>
<context position="2144" citStr="Beamer and Girju, 2009" startWordPosition="304" endWordPosition="307"> processing (NLP) applications such as question answering, etc. In this work, we automatically detect and extract causal relations between events in text. Despite its importance, prior work on event causality extraction in context in the NLP literature is relatively sparse. In (Girju, 2003), the author used noun-verb-noun lexico-syntactic patterns to learn that “mosquitoes cause malaria”, where the cause and effect mentions are nominals and not necessarily event evoking words. In (Sun et al., 2007), the authors focused on detecting causality between search query pairs in temporal query logs. (Beamer and Girju, 2009) tried to detect causal relations between verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs. In (Riaz and Girju, 2010), the authors first cluster sentences into topic-specific scenarios, and then focus on building a dataset of causal text spans, where each span is headed by a verb. Thus, their focus was not on identifying causal relations between events in a given text document. In this paper, given a text document, we first identify events and their associated arguments. We then identify causality or relatedness relations between event pairs. To </context>
</contexts>
<marker>Beamer, Girju, 2009</marker>
<rawString>Brandon Beamer and Roxana Girju. 2009. Using a bigram event model to predict causal potential. In CICLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="34010" citStr="Bejan and Harabagiu, 2010" startWordPosition="5574" endWordPosition="5577">tion 3). A related 17% of recall errors involves nominal predicates without any argument. For these, less information is available for CEA to make predictions. The remaining group (15% of errors) involves events containing pronouns as arguments. 8 Related Work Although prior work in event causality extraction in context is relatively sparse, there are many prior works concerning other semantic aspects of event extraction. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. In other prior work (Chen et al., 2009; Bejan and Harabagiu, 2010), the authors focused on identifying another type of event pair semantic relation: event coreference. Chambers and Jurafsky (2008; 2009) chain events sharing a common (protagonist) participant. They defined events as verbs and given an existing chain of events, they predict the next likely event involving the protagonist. This is different from our task of detecting causality between arbitrary event pairs that might or might not share common arguments. Also, we defined events more broadly, as those that are triggered by either verbs or nouns. Finally, although our proposed CEA metric has resem</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="34139" citStr="Chambers and Jurafsky (2008" startWordPosition="5593" endWordPosition="5596">le for CEA to make predictions. The remaining group (15% of errors) involves events containing pronouns as arguments. 8 Related Work Although prior work in event causality extraction in context is relatively sparse, there are many prior works concerning other semantic aspects of event extraction. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. In other prior work (Chen et al., 2009; Bejan and Harabagiu, 2010), the authors focused on identifying another type of event pair semantic relation: event coreference. Chambers and Jurafsky (2008; 2009) chain events sharing a common (protagonist) participant. They defined events as verbs and given an existing chain of events, they predict the next likely event involving the protagonist. This is different from our task of detecting causality between arbitrary event pairs that might or might not share common arguments. Also, we defined events more broadly, as those that are triggered by either verbs or nouns. Finally, although our proposed CEA metric has resemblance the ECD metric in (Riaz and Girju, 2010), our task is different from theirs and our work differs in many aspects. They foc</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Nicholas Rizzolo</author>
<author>Dan Roth</author>
</authors>
<title>Learning and inference with constraints.</title>
<date>2008</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="19787" citStr="Chang et al., 2008" startWordPosition="3175" endWordPosition="3178">given thus far in this section, all the connectives were explicit, as they appear in the texts. PDTB also provides annotations for implicit connectives, which we do not use in this work. Identifying implicit connectives is a harder task and incorporating these is a possible future work. 5 Joint Inference for Causality Extraction To exploit the interactions between event pair causality extraction and discourse relation identification, we define appropriate constraints between them, which can be enforced through the Constrained Conditional Models framework (aka ILP for NLP) (Roth and Yih, 2007; Chang et al., 2008). In doing this, the predictions of CEA (Section 2.1) and the discourse system are forced to cohere with each other. More importantly, this should improve the performance of using only CEA to extract causal event pairs. To the best of our knowledge, this approach for causality extraction is novel. 5.1 CEA &amp; Discourse: Implementation Details Let E denote the set of event mentions in a document. Let EP = {(ei, ej) ∈ E × E |ei ∈ E, ej ∈ E,i &lt; j, |sent(ei) − sent(ej) |≤ 2} denote the set of event mention pairs in the document, where sent(e) gives the sentence number in which event e occurs. Note t</context>
</contexts>
<marker>Chang, Ratinov, Rizzolo, Roth, 2008</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and Dan Roth. 2008. Learning and inference with constraints. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
<author>Robert Haralick</author>
</authors>
<title>A pairwise event coreference model, feature impact and evaluation for event coreference resolution.</title>
<date>2009</date>
<booktitle>In RANLP workshop on Events in Emerging Text Types.</booktitle>
<contexts>
<context position="33982" citStr="Chen et al., 2009" startWordPosition="5570" endWordPosition="5573">s (described in Section 3). A related 17% of recall errors involves nominal predicates without any argument. For these, less information is available for CEA to make predictions. The remaining group (15% of errors) involves events containing pronouns as arguments. 8 Related Work Although prior work in event causality extraction in context is relatively sparse, there are many prior works concerning other semantic aspects of event extraction. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. In other prior work (Chen et al., 2009; Bejan and Harabagiu, 2010), the authors focused on identifying another type of event pair semantic relation: event coreference. Chambers and Jurafsky (2008; 2009) chain events sharing a common (protagonist) participant. They defined events as verbs and given an existing chain of events, they predict the next likely event involving the protagonist. This is different from our task of detecting causality between arbitrary event pairs that might or might not share common arguments. Also, we defined events more broadly, as those that are triggered by either verbs or nouns. Finally, although our p</context>
</contexts>
<marker>Chen, Ji, Haralick, 2009</marker>
<rawString>Zheng Chen, Heng Ji, and Robert Haralick. 2009. A pairwise event coreference model, feature impact and evaluation for event coreference resolution. In RANLP workshop on Events in Emerging Text Types.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In ACL workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="1812" citStr="Girju, 2003" startWordPosition="255" endWordPosition="256">s understand that there are two events, triggered by the words “arrested” and “killed”, and that there is a causality relationship between these two events. Besides being an important component of discourse understanding, automatically identifying causal relations between events is important for various natural language processing (NLP) applications such as question answering, etc. In this work, we automatically detect and extract causal relations between events in text. Despite its importance, prior work on event causality extraction in context in the NLP literature is relatively sparse. In (Girju, 2003), the author used noun-verb-noun lexico-syntactic patterns to learn that “mosquitoes cause malaria”, where the cause and effect mentions are nominals and not necessarily event evoking words. In (Sun et al., 2007), the authors focused on detecting causality between search query pairs in temporal query logs. (Beamer and Girju, 2009) tried to detect causal relations between verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs. In (Riaz and Girju, 2010), the authors first cluster sentences into topic-specific scenarios, and then focus on building a datas</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for question answering. In ACL workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Gurevich</author>
<author>Richard Crouch</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
</authors>
<title>Deverbal nouns in knowledge representation.</title>
<date>2008</date>
<journal>Journal of Logic and Computation,</journal>
<volume>18</volume>
<marker>Gurevich, Crouch, King, de Paiva, 2008</marker>
<rawString>Olga Gurevich, Richard Crouch, Tracy Holloway King, and Valeria de Paiva. 2008. Deverbal nouns in knowledge representation. Journal of Logic and Computation, 18, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through unsupervised cross-document inference.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="33832" citStr="Ji and Grishman (2008)" startWordPosition="5546" endWordPosition="5549">etter predictions. 19% of the recall errors arises from causal event pairs involving nominal predicates that are not in our list of event evoking noun types (described in Section 3). A related 17% of recall errors involves nominal predicates without any argument. For these, less information is available for CEA to make predictions. The remaining group (15% of errors) involves events containing pronouns as arguments. 8 Related Work Although prior work in event causality extraction in context is relatively sparse, there are many prior works concerning other semantic aspects of event extraction. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. In other prior work (Chen et al., 2009; Bejan and Harabagiu, 2010), the authors focused on identifying another type of event pair semantic relation: event coreference. Chambers and Jurafsky (2008; 2009) chain events sharing a common (protagonist) participant. They defined events as verbs and given an existing chain of events, they predict the next likely event involving the protagonist. This is different from our task of detecting causality between arbitrary event pairs that might or</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through unsupervised cross-document inference. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining Local Context and WordNet Similarity for Word Sense Identification.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8320" citStr="Leacock and Chodorow (1998)" startWordPosition="1306" endWordPosition="1310">) P (e)P (e,) &gt; 1. Next, we build on the intuition that event predicates appearing in a large number of documents are probably not important or discriminative. Thus, we penalize these predicates when calculating spp by adopting the inverse document frequency (idf): IDF (pi, pj) = idf(pi) x idf(pj) x idf(pi, pj), where idf(p) = log D 1+N , D is the total number of documents in the collection and N is the number of documents that p occurs in. We also award event pairs that are closer together, while penalizing event pairs that are further apart in texts, by incorporating the distance measure of Leacock and Chodorow (1998), which was originally used to measure similarity between concepts: where sent(p) gives the sentence number (index) in which p occurs and ws indicates the window-size (of sentences) used. If pi and pj are drawn from the same sentence, the numerator of the above fraction will return 1. In our work, we set ws to 3 and thus, if pi occurs in sentence k, the furthest sentence that pj will be drawn from, is sentence k + 2. The final component of Equation 2, max(ui, uj), takes into account whether predicates (events) pi and pj appear most frequently with each other. ui and uj are defined as follows: </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow, 1998. Combining Local Context and WordNet Similarity for Word Sense Identification. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2010</date>
<tech>Technical report. http://www.comp.nus.edu.sg/ linzihen/publications/tech2010.pdf.</tech>
<contexts>
<context position="18532" citStr="Lin et al., 2010" startWordPosition="2975" endWordPosition="2978">its text spans. Temporal These indicate that the situations described in the text spans are related temporally. An example sentence is: Synchrony: [He was sitting at his home] when [the whole world started to shake]. Temporal precedence of the (cause) event over the (effect) event is a necessary, but not sufficient requisite for causality. Hence by itself, Temporal relations are probably not discriminative enough for determining event causality. 4.2 Discourse Relation Extraction System Our work follows the approach and features described in the state-of-the-art Ruby-based discourse system of (Lin et al., 2010), to build an inhouse Java-based discourse relation extraction system. Our system identifies explicit connectives in text, predict their discourse relations, as well as their associated text spans. Similar to (Lin et al., 2010), we achieved a competitive performance of slightly over 80% F1-score in identifying fine-grained relations for explicit connectives. Our system is developed using the Learning Based Java modeling lan298 guage (LBJ) (Rizzolo and Roth, 2010) and will be made available soon. Due to space constraints, we refer interested readers to (Lin et al., 2010) for details on the feat</context>
</contexts>
<marker>Lin, Ng, Kan, 2010</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A pdtb-styled end-to-end discourse parser. Technical report. http://www.comp.nus.edu.sg/ linzihen/publications/tech2010.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Eleni Miltsakaki</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Livio Robaldo</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0 annotation manual.</title>
<date>2007</date>
<tech>Technical report. http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtbannotation-manual.pdf.</tech>
<contexts>
<context position="14535" citStr="Prasad et al., 2007" startWordPosition="2387" endWordPosition="2390">ecause [he killed someone]. In the example sentence above, the discourse connective (“because”) and the discourse relation it evokes (in this case, the Cause relation) allows readers to relate its two associated text spans, “The police arrested him” and “he killed someone”. Also, notice that the verbs “arrested” and “killed”, which cross the two text spans, are causally related. To aid in extracting causal relations, we leverage on the identification of discourse relations to provide additional contextual information. To identify discourse relations, we use the Penn Discourse Treebank (PDTB) (Prasad et al., 2007), which contains annotations of discourse relations in context. The annotations are done over the Wall Street Journal corpus and the PDTB adopts a predicate-argument view of discourse relations. A discourse connective (e.g. because) takes two text spans as its arguments. In the rest of this section, we briefly describe the discourse relations in PDTB and highlight how we might leverage them to aid in determining event causality. 5http://en.wikipedia.org/wiki/Natural disaster 297 Table 1: Coarse-grained and fine-grained discourse relations. Coarse-grained relations Fine-grained relations Conces</context>
<context position="16113" citStr="Prasad et al., 2007" startWordPosition="2598" endWordPosition="2601"> in the left column of Table 1. Each of these are further refined into several fine-grained discourse relations, as shown in the right column of the table.6 Next, we briefly describe these relations, highlighting those that could potentially help to determine event causality. Comparison A Comparison discourse relation between two text spans highlights prominent differences between the situations described in the text spans. An example sentence is: Contrast: [According to the survey, x% of Chinese Internet users prefer Google] whereas [y% prefer Baidu]. According to the PDTB annotation manual (Prasad et al., 2007), the truth of both spans is independent of the established discourse relation. This means that the text spans are not causally related and thus, the existence of a Comparison relation should imply that there is no causality relation across the two text spans. Contingency A Contingency relation between two text spans indicates that the situation described in one text span causally influences the situation in the other. An example sentence is: Cause: [The first priority is search and rescue] because [many people are trapped under the rubble]. Existence of a Contingency relation potentially impl</context>
</contexts>
<marker>Prasad, Miltsakaki, Dinesh, Lee, Joshi, Robaldo, Webber, 2007</marker>
<rawString>Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie Webber. 2007. The penn discourse treebank 2.0 annotation manual. Technical report. http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtbannotation-manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Another look at causality: Discovering scenario-specific contingency relationships with no supervision.</title>
<date>2010</date>
<booktitle>In ICSC.</booktitle>
<contexts>
<context position="2309" citStr="Riaz and Girju, 2010" startWordPosition="332" endWordPosition="335">importance, prior work on event causality extraction in context in the NLP literature is relatively sparse. In (Girju, 2003), the author used noun-verb-noun lexico-syntactic patterns to learn that “mosquitoes cause malaria”, where the cause and effect mentions are nominals and not necessarily event evoking words. In (Sun et al., 2007), the authors focused on detecting causality between search query pairs in temporal query logs. (Beamer and Girju, 2009) tried to detect causal relations between verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs. In (Riaz and Girju, 2010), the authors first cluster sentences into topic-specific scenarios, and then focus on building a dataset of causal text spans, where each span is headed by a verb. Thus, their focus was not on identifying causal relations between events in a given text document. In this paper, given a text document, we first identify events and their associated arguments. We then identify causality or relatedness relations between event pairs. To do this, we develop a minimally supervised approach using focused distributional similarity methods, such as co-occurrence counts of events collected automatically f</context>
<context position="4632" citStr="Riaz and Girju, 2010" startWordPosition="685" endWordPosition="688">r nouns (e.g. “explosion”), we allow for detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. To the best of our knowledge, this formulation of the task is novel. • We developed a minimally supervised approach for the task using focused distributional similarity methods that are automatically collected from an unannotated corpus. We show that our approach achieves better performance than two approaches: one based on a frequently used metric that measures association, and another based on the effect-control-dependency (ECD) metric described in a prior work (Riaz and Girju, 2010). • We leverage on the interactions between event causality prediction and discourse relations prediction. We combine these knowledge sources through a global inference procedure, which we formalize via an Integer Linear Programming (ILP) framework as a constraint optimization problem (Roth and Yih, 2004). This allows us to easily define appropriate constraints to ensure that the causality and discourse predictions are coherent with each other, thereby improving the performance of causality identification. 2 Event Causality In this work, we define an event as an action or occurrence that happe</context>
<context position="9785" citStr="Riaz and Girju, 2010" startWordPosition="1577" endWordPosition="1580">ity with pi, i.e. pk = pj. uj is treated similarly. 2.1.2 Predicate-Argument and Argument-Argument Association We define spa as follows: ei spa (, ej) = 1 1:PMI (pi, a) |Aej |a∈Aej + 1 PMI(pj, a), (3) |Aei |a∈Aei where Aei and Aej are the sets of arguments of ei and ej respectively. Finally, we define saa as follows: saa(ei, ej) = |Aei ||Ae; I a∈Aei a,∈A j , ) (4) 1 1: PMI(a a Together, spa and saa provide additional contexts and robustness (in addition to spp) for measuring the cause-effect association between events ei and ej. Our formulation of CEA is inspired by the ECD metric defined in (Riaz and Girju, 2010): dis(a, b) ECD(a, b) = max(v, w) x −log (5) 2 x maxDistance where P(a, b) maxt[P(a, bt)] − P(a, b) + c P(a, b) maxt[P(at, b)] − P(a, b) + c, where ECD(a,b) measures the causality between two events a and b (headed by verbs), and the second component in the ECD equation is similar to Dist(pi, pj). In our experiments, we will evaluate the performance of ECD against our proposed approach. So far, our definitions in this section are generic and allow for any list of event argument types. In this work, we focus on two argument types: agent (subject) and patient (object), which are typical core arg</context>
<context position="30027" citStr="Riaz and Girju (2010)" startWordPosition="4922" endWordPosition="4925">zed all scores higher than t into B bins, and all scores lower than t into B bins. Together, these 2B bins represent the range [0,1]. We used B = 500. Thus, consecutive bins represent a difference of 0.001 in calibrated scores. To measure the causality between a pair of events ei and ej, a simple baseline is to calculate PMI(pi, pj). Using a similar thresholding and calibration process to translate PMI(pi, pj) scores into binary causality decisions, we obtained a F1 score of 23.1 when measured over the causality C relations, as shown in the row PMIpp of Table 2. As mentioned in Section 2.1.2, Riaz and Girju (2010) proposed the ECD metric to measure causality between two events. Thus, as a point of comparison, we replaced spp of Equation (1) with ECD(a, b) of Equation (5), substituting a = pi and b = pj. After thresholding and calibrating the scores of this approach, we obtained a F1-score of 29.7, as shown in the row ECDpp&amp;PMIpa,aa of Table 2. Next, we evaluated our proposed CEA approach and obtained a F1-score of 38.6, as shown in the row CEA of Table 2. Thus, our proposed approach obtained significantly better performance than the PMI baseline and the ECD approach. Next, we performed joint inference </context>
<context position="34657" citStr="Riaz and Girju, 2010" startWordPosition="5679" endWordPosition="5682">dentifying another type of event pair semantic relation: event coreference. Chambers and Jurafsky (2008; 2009) chain events sharing a common (protagonist) participant. They defined events as verbs and given an existing chain of events, they predict the next likely event involving the protagonist. This is different from our task of detecting causality between arbitrary event pairs that might or might not share common arguments. Also, we defined events more broadly, as those that are triggered by either verbs or nouns. Finally, although our proposed CEA metric has resemblance the ECD metric in (Riaz and Girju, 2010), our task is different from theirs and our work differs in many aspects. They focused on building a dataset of causal text spans, whereas we focused on identifying causal relations between events in a given text document. They considered text spans headed by verbs while we considered events triggered by both verbs and nouns. Moreover, we combined event causality prediction and discourse relation prediction through a global inference procedure to further improve the performance of event causality prediction. 302 9 Conclusion In this paper, using general tools such as the dependency and discour</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>Mehwish Riaz and Roxana Girju. 2010. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Learning based java for rapid development of nlp systems.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="18999" citStr="Rizzolo and Roth, 2010" startWordPosition="3047" endWordPosition="3050">se Relation Extraction System Our work follows the approach and features described in the state-of-the-art Ruby-based discourse system of (Lin et al., 2010), to build an inhouse Java-based discourse relation extraction system. Our system identifies explicit connectives in text, predict their discourse relations, as well as their associated text spans. Similar to (Lin et al., 2010), we achieved a competitive performance of slightly over 80% F1-score in identifying fine-grained relations for explicit connectives. Our system is developed using the Learning Based Java modeling lan298 guage (LBJ) (Rizzolo and Roth, 2010) and will be made available soon. Due to space constraints, we refer interested readers to (Lin et al., 2010) for details on the features, etc. In the example sentences given thus far in this section, all the connectives were explicit, as they appear in the texts. PDTB also provides annotations for implicit connectives, which we do not use in this work. Identifying implicit connectives is a harder task and incorporating these is a possible future work. 5 Joint Inference for Causality Extraction To exploit the interactions between event pair causality extraction and discourse relation identific</context>
</contexts>
<marker>Rizzolo, Roth, 2010</marker>
<rawString>N. Rizzolo and D. Roth. 2010. Learning based java for rapid development of nlp systems. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen Tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="4938" citStr="Roth and Yih, 2004" startWordPosition="730" endWordPosition="733">that are automatically collected from an unannotated corpus. We show that our approach achieves better performance than two approaches: one based on a frequently used metric that measures association, and another based on the effect-control-dependency (ECD) metric described in a prior work (Riaz and Girju, 2010). • We leverage on the interactions between event causality prediction and discourse relations prediction. We combine these knowledge sources through a global inference procedure, which we formalize via an Integer Linear Programming (ILP) framework as a constraint optimization problem (Roth and Yih, 2004). This allows us to easily define appropriate constraints to ensure that the causality and discourse predictions are coherent with each other, thereby improving the performance of causality identification. 2 Event Causality In this work, we define an event as an action or occurrence that happens with associated participants or arguments. Formally, we define an event e as: p(a1, a2, ... , an), where the predicate p is the word that triggers the presence of e in text, and a1, a2, ... , an are the arguments associated with e. Examples of predicates could be verbs such as “attacked”, “employs”, no</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen Tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen Tau Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="19766" citStr="Roth and Yih, 2007" startWordPosition="3171" endWordPosition="3174">e example sentences given thus far in this section, all the connectives were explicit, as they appear in the texts. PDTB also provides annotations for implicit connectives, which we do not use in this work. Identifying implicit connectives is a harder task and incorporating these is a possible future work. 5 Joint Inference for Causality Extraction To exploit the interactions between event pair causality extraction and discourse relation identification, we define appropriate constraints between them, which can be enforced through the Constrained Conditional Models framework (aka ILP for NLP) (Roth and Yih, 2007; Chang et al., 2008). In doing this, the predictions of CEA (Section 2.1) and the discourse system are forced to cohere with each other. More importantly, this should improve the performance of using only CEA to extract causal event pairs. To the best of our knowledge, this approach for causality extraction is novel. 5.1 CEA &amp; Discourse: Implementation Details Let E denote the set of event mentions in a document. Let EP = {(ei, ej) ∈ E × E |ei ∈ E, ej ∈ E,i &lt; j, |sent(ei) − sent(ej) |≤ 2} denote the set of event mention pairs in the document, where sent(e) gives the sentence number in which e</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>Dan Roth and Wen Tau Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice.</title>
<date>2010</date>
<note>http://framenet.icsi.berkeley.edu.</note>
<contexts>
<context position="12091" citStr="Ruppenhofer et al., 2010" startWordPosition="2001" endWordPosition="2004">nt from the first three characters of v. For each of the remaining nouns in N„, we measured its Levenstein (edit) distance from v and keep the noun(s) with the minimum distance. When multiple nouns have the same minimum distance from v, we keep all of them. • To further prune the list of nouns, we next removed all nouns ending in “er”, “or”, or “ee”, as these nouns typically refer to a person, e.g. “writer”, “doctor”, “employee”. We also remove nouns that are not hyponyms (children) of the first WordNet sense of the noun “event”4. • Since we are concerned with nouns denoting events, FrameNet (Ruppenhofer et al., 2010) (FN) is a good resource for mining such nouns. FN consists of frames denoting situations and events. As part of the FN resource, each FN frame consists of a list of lexical units (mainly verbs and nouns) representing the semantics of the frame. Various frame-to-frame relations are also defined (in particular the inheritance relation). Hence, we gathered all the children frames of the FN frame “Event”. From these children frames, we then gathered all their noun lexical units (words) and add them to our list of 2http://wordnet.princeton.edu/ 3The WordNet resource provides derivational informati</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2010</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2010. FrameNet II: Extended Theory and Practice. http://framenet.icsi.berkeley.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yizhou Sun</author>
</authors>
<title>Ning Liu, Kunqing Xie, Shuicheng</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<marker>Sun, 2007</marker>
<rawString>Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan, Benyu Zhang, and Zheng Chen. 2007. Causal relation of queries from temporal logs. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Suppes</author>
</authors>
<title>A Probabilistic Theory of Causality.</title>
<date>1970</date>
<publisher>North-Holland Publishing Company.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="7323" citStr="Suppes, 1970" startWordPosition="1130" endWordPosition="1131">tion between event predicates, spa measures the association between the predicate of an event and the arguments of the other event, and saa measures the association between event arguments. In our work, we regard each event e as being triggered and rooted at a predicate p. 2.1.1 Predicate-Predicate Association We define spp as follows: spp(ei, ej) = PMI(pi, pj) x max(ui, uj) xIDF(pi, pj) x Dist(pi, pj) (2) which takes into account the PMI between predicates pi and pj of events ei and ej respectively, as well as various other pieces of information. In Suppes’ Probabilistic theory of Casuality (Suppes, 1970), he highlighted that event e is a possible cause of event e&apos;, if e&apos; happens more frequently with e than 1PMI is frequently used to measure association between variables. 295 by itself, i.e. P(e0|e) &gt; P(e0). This can be easily rewritten as P(e of PMI: P(e)P(e ) ,e,), &gt; 1, similar to the definition P(e, e0) PMI(e, e0) = log P(e)P(e0) which is only positive when P (e,e,) P (e)P (e,) &gt; 1. Next, we build on the intuition that event predicates appearing in a large number of documents are probably not important or discriminative. Thus, we penalize these predicates when calculating spp by adopting th</context>
</contexts>
<marker>Suppes, 1970</marker>
<rawString>Patrick Suppes. 1970. A Probabilistic Theory of Causality. Amsterdam: North-Holland Publishing Company.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>