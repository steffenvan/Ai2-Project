<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016554">
<title confidence="0.9971005">
Crowdsourcing the evaluation of a domain-adapted named entity
recognition system
</title>
<author confidence="0.900887">
Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek
</author>
<affiliation confidence="0.997505">
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.946616">
College Park, MD 20742
</address>
<email confidence="0.8472095">
asayeed@cs.umd.edu,
tmeyer1@umd.edu,
{hcnguyen88,olivia.buzek}
@gmail.com
</email>
<sectionHeader confidence="0.995423" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998109083333333">
Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.99978">
1.1 Named entities and errors
</subsectionHeader>
<bodyText confidence="0.998589">
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.
Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social
</bodyText>
<author confidence="0.472163">
Amy Weinberg
</author>
<affiliation confidence="0.8348">
Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.76668">
College Park, MD 20742
</address>
<email confidence="0.917105">
weinberg@umiacs.umd.edu
</email>
<bodyText confidence="0.999963441176471">
network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.
Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.
Machine learning is currently used extensively in
building NER systems. One such system is BBN’s
Identifinder (Bikel et al., 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.
The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system’s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-
</bodyText>
<page confidence="0.986445">
345
</page>
<subsubsectionHeader confidence="0.756573">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345–348,
</subsubsectionHeader>
<bodyText confidence="0.8253275">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.
</bodyText>
<subsectionHeader confidence="0.993804">
1.2 Crowdsourcing
</subsectionHeader>
<bodyText confidence="0.999980263157895">
Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al., 2008; Callison-Burch, 2009).
The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.
Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.
</bodyText>
<sectionHeader confidence="0.991036" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.994802">
2.1 Process overview
</subsectionHeader>
<bodyText confidence="0.999734">
The overall process for running this experiment was
as follows (figure 1).
</bodyText>
<figureCaption confidence="0.999569">
Figure 1: Diagram of data pipeline.
</figureCaption>
<bodyText confidence="0.99994752">
First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-
neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.
Next, we constructed an Amazon Mechanical
Turk-based interface for naive web users or “Turk-
ers” to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.
We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.
</bodyText>
<subsectionHeader confidence="0.993537">
2.2 Performance evaluation
</subsectionHeader>
<bodyText confidence="0.999939727272727">
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.
As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.
</bodyText>
<sectionHeader confidence="0.996239" genericHeader="method">
3 Experiments and results
</sectionHeader>
<bodyText confidence="0.999956333333333">
Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.
</bodyText>
<subsectionHeader confidence="0.994425">
3.1 Baseline performance assessment
</subsectionHeader>
<bodyText confidence="0.9998325">
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.
After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:
</bodyText>
<page confidence="0.990373">
346
</page>
<table confidence="0.999913857142857">
Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85
</table>
<tableCaption confidence="0.999883">
Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.
</tableCaption>
<listItem confidence="0.998154333333333">
• IdentiFinder tags words that are simply not
named entities.
• IdentiFinder assigns the wrong category (per-
son or organization) to an entity.
• IdentiFinder includes extraneous words in an
otherwise correct entity.
</listItem>
<bodyText confidence="0.955064315789474">
The second and third types of error are particu-
larly challenging. An example of the second type is
the following:
Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .
Google is marked twice incorrectly as being a person
rather than an organization.
Finally, here is an example of the third error type:
A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.
IdentiFinder incorrectly marks “danced” as part of a
person tag.
We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al., 1999).
</bodyText>
<subsectionHeader confidence="0.999458">
3.2 Domain-specific error reduction
</subsectionHeader>
<bodyText confidence="0.999978277777778">
We wrote a series of rule-based filters to remove
instances of the error types—of which there were
many subtypes—described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
“danced” was labelled as a verb, and entities with
tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder’s majority labelling of Google throughout the
corpus—as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.
This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.
</bodyText>
<subsectionHeader confidence="0.998515">
3.3 Mechanical Turk tasks
</subsectionHeader>
<bodyText confidence="0.999956652173913">
The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.
We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: “Person,” “Organization,”
“Neither,” and “Don’t Know.” Then for each entity,
the user selects exactly one of the four options.
Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.
We calculated the agreement between our annota-
tions and those developed from the Turker majority
</bodyText>
<page confidence="0.996347">
347
</page>
<bodyText confidence="0.999802928571428">
vote scheme. This yields a Cohen’s r. of 0.68. We
considered this to be substantial agreement.
After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).
We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.883072">
4.1 Benefits
</subsectionHeader>
<bodyText confidence="0.999980884615384">
It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.
However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.
One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.
This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.
</bodyText>
<subsectionHeader confidence="0.843677">
4.2 Costs
</subsectionHeader>
<bodyText confidence="0.999943222222222">
It took not more than an estimated two person weeks
to complete this work. This includes doing the
expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.
For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).
</bodyText>
<sectionHeader confidence="0.998718" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977294117647">
This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.
Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al., 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99965775">
This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681714285714">
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what‘s
in a name. Mach. Learn., 34(1-3).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In EMNLP 2009, Singapore, August.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.
</reference>
<page confidence="0.998054">
348
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964312">
<title confidence="0.9907795">Crowdsourcing the evaluation of a domain-adapted named recognition system</title>
<author confidence="0.993299">Asad B Sayeed</author>
<author confidence="0.993299">Timothy J Hieu C Nguyen</author>
<author confidence="0.993299">Olivia</author>
<affiliation confidence="0.9999185">Department of Computer University of</affiliation>
<address confidence="0.993551">College Park, MD</address>
<email confidence="0.999442">@gmail.com</email>
<abstract confidence="0.999844846153846">Named entity recognition systems sometimes have difficulty when applied to data from domains that do not closely match the training data. We first use a simple rule-based technique for domain adaptation. Data for robust validation of the technique is then generated, and we use crowdsourcing techniques to show that this strategy produces reliable results even on data not seen by the rule designers. We show that it is possible to extract large improvements on the target data rapidly at low cost using these techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what‘s in a name.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="2737" citStr="Bikel et al., 1999" startWordPosition="417" endWordPosition="420">raphs of social networks. Not all errors carry the same price. In some applications, omitting a named entity has the consequence of reducing the availability of training data, but including an incorrectly identified piece of text as as a named entity has the consequence of producing misleading results. Our application would be opinion mining; an omitted entity may prevent the system from attributing an opinion to a source, but an incorrect entity reveals non-existent opinion sources. Machine learning is currently used extensively in building NER systems. One such system is BBN’s Identifinder (Bikel et al., 1999). The IdentiFinder algorithm, based on Hidden Markov Models, has been shown to achieve F-measure scores above 90% when the training and testing data happen to be derived from Wall Street Journal text produced in the 1990s. We use IdentiFinder 3.3 as a starting point for performance improvement in this paper. The use of machine learning in existing systems requires us to produce new and costly training data if we want to adapt these systems directly to other domains. Our post hoc error reduction strategy is therefore profoundly different: it relieves us of the burden of generating complete trai</context>
<context position="8668" citStr="Bikel et al., 1999" startWordPosition="1377" endWordPosition="1380">owing: Yahoo is a reasonably strong competitor to Google. It gets about half as much online revenue and search traffic as Google, . . . Google is marked twice incorrectly as being a person rather than an organization. Finally, here is an example of the third error type: A San Diego bartender reported that Bill Gates danced the night away in his bar on Nov. 11. IdentiFinder incorrectly marks “danced” as part of a person tag. We were able to find the precision of IdentiFinder against our annotations: 0.74. This is poorer than the reported performance of IdentiFinder on Wall Street Journal text (Bikel et al., 1999). 3.2 Domain-specific error reduction We wrote a series of rule-based filters to remove instances of the error types—of which there were many subtypes—described in the previous section. For instance, the third example above was eliminated via the use of a part-of-speech tagger; “danced” was labelled as a verb, and entities with tagged verbs were removed. In the second case, the mislabelling of Google as a person rather than an organization is identified by looking at IdentiFinder’s majority labelling of Google throughout the corpus—as an organization. Simple rules about capitalization allow in</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what‘s in a name. Mach. Learn., 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In EMNLP 2009, Singapore,</booktitle>
<contexts>
<context position="4322" citStr="Callison-Burch, 2009" startWordPosition="669" endWordPosition="670">tics cision, while minimizing damage to recall, unlike an evaluation based on retraining with new, fullyannotated text. 1.2 Crowdsourcing Crowdsourcing is the use of the mass collaboration of Internet passers-by for large enterprises on the World Wide Web such as Wikipedia and survey companies. However, a generalized way to monetize the many small tasks that make up a larger task is relatively new. Crowdsourcing platforms like Amazon Mechanical Turk have allowed some NLP researchers to acquire data for small amounts of money from large, unspecified groups of Internet users (Snow et al., 2008; Callison-Burch, 2009). The use of crowdsourcing for an NLP annotation task required careful definition of the specifics of the task. The individuals who perform these tasks have no specific training, and they are trying to get through as many tasks as they can, so each task must be specified very simply and clearly. Part of our work was to define a named entity error detection task simply enough that the results would be consistent across anonymous annotators. 2 Methodology 2.1 Process overview The overall process for running this experiment was as follows (figure 1). Figure 1: Diagram of data pipeline. First, we </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In EMNLP 2009, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>Xiaoxun Zhang</author>
<author>Xian Wu</author>
<author>Zhong Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<booktitle>In NAACL 2009,</booktitle>
<location>Morristown, NJ, USA.</location>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, Xian Wu, and Zhong Su. 2009. Domain adaptation with latent semantic association for named entity recognition. In NAACL 2009, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP 2008,</booktitle>
<location>Morristown, NJ, USA.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP 2008, Morristown, NJ, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>