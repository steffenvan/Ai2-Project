<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056362">
<title confidence="0.990651">
Improved Extraction Assessment through Better Language Models
</title>
<author confidence="0.984578">
Arun Ahuja, Doug Downey
</author>
<affiliation confidence="0.968838">
EECS Dept., Northwestern University
</affiliation>
<address confidence="0.539166">
Evanston, IL 60208
</address>
<email confidence="0.998006">
{arun.ahuja, ddowney}@eecs.northwestern.edu
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927625">
A variety of information extraction techniques
rely on the fact that instances of the same
relation are “distributionally similar,” in that
they tend to appear in similar textual con-
texts. We demonstrate that extraction accu-
racy depends heavily on the accuracy of the
language model utilized to estimate distribu-
tional similarity. An unsupervised model se-
lection technique based on this observation is
shown to reduce extraction and type-checking
error by 26% over previous results, in experi-
ments with Hidden Markov Models. The re-
sults suggest that optimizing statistical lan-
guage models over unlabeled data is a promis-
ing direction for improving weakly supervised
and unsupervised information extraction.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999461837209302">
Many weakly supervised and unsupervised informa-
tion extraction techniques assess the correctness of
extractions using the distributional hypothesis—the
notion that words with similar meanings tend to oc-
cur in similar contexts (Harris, 1985). A candidate
extraction of a relation is deemed more likely to be
correct when it appears in contexts similar to those
of “seed” instances of the relation, where the seeds
may be specified by hand (Pas¸ca et al., 2006), taken
from an existing, incomplete knowledge base (Snow
et al., 2006; Pantel et al., 2009), or obtained in an un-
supervised manner using a generic extractor (Banko
et al., 2007). We refer to this technique as Assess-
ment by Distributional Similarity (ADS).
Typically, distributional similarity is computed by
comparing co-occurrence counts of extractions and
seeds with various contexts found in the corpus. Sta-
tistical Language Models (SLMs) include methods
for more accurately estimating co-occurrence proba-
bilities via back-off, smoothing, and clustering tech-
niques (e.g. (Chen and Goodman, 1996; Rabiner,
1989; Bell et al., 1990)). Because SLMs can be
trained from only unlabeled text, they can be applied
for ADS even when the relations of interest are not
specified in advance (Downey et al., 2007). Unla-
beled text is abundant in large corpora like the Web,
making nearly-ceaseless automated optimization of
SLMs possible. But how fruitful is such an effort
likely to be—to what extent does optimizing a lan-
guage model over a fixed corpus lead to improve-
ments in assessment accuracy?
In this paper, we show that an ADS technique
based on SLMs is improved substantially when
the language model it employs becomes more ac-
curate. In a large-scale set of experiments, we
quantify how language model perplexity correlates
with ADS performance over multiple data sets and
SLM techniques. The experiments show that accu-
racy over unlabeled data can be used for selecting
among SLMs—for an ADS approach utilizing Hid-
den Markov Models, this results in an average error
reduction of 26% over previous results in extraction
and type-checking tasks.
</bodyText>
<sectionHeader confidence="0.961141" genericHeader="method">
2 Extraction Assessment with Language
Models
</sectionHeader>
<bodyText confidence="0.99977925">
We begin by formally defining the extraction and
typechecking tasks we consider, then discuss statis-
tical language models and their utilization for ex-
traction assessment.
</bodyText>
<page confidence="0.972017">
225
</page>
<subsubsectionHeader confidence="0.579824">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 225–228,
</subsubsectionHeader>
<subsectionHeader confidence="0.277325">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.993175">
The extraction task we consider is formalized as
follows: given a corpus, a target relation R, a list
of seed instances SR, and a list of candidate extrac-
tions UR, the task is to order elements of UR such
that correct instances for R are ranked above extrac-
tion errors. Let URi denote the set of the ith argu-
ments of the extractions in UR, and let SRi be de-
fined similarly for the seed set SR. For relations of
arity greater than one, we consider the typechecking
task, an important sub-task of extraction (Downey et
al., 2007). The typechecking task is to rank extrac-
tions with arguments that are of the proper type for a
relation above type errors. As an example, the ex-
traction Founded(Bill Gates, Oracle) is
type correct, but is not correct for the extraction task.
</bodyText>
<subsectionHeader confidence="0.994092">
2.1 Statistical Language Models
</subsectionHeader>
<bodyText confidence="0.99990325">
A Statistical Language Model (SLM) is a probabil-
ity distribution P(w) over word sequences w =
(w1, ..., wr). The most common SLM techniques
are n-gram models, which are Markov models in
which the probability of a given word is dependent
on only the previous n−1 words. The accuracy of an
n-gram model of a corpus depends on two key fac-
tors: the choice of n, and the smoothing technique
employed to assign probabilities to word sequences
seen infrequently in training. We experiment with
choices of n from 2 to 4, and two popular smoothing
approaches, Modified Kneser-Ney (Chen and Good-
man, 1996) and Witten-Bell (Bell et al., 1990).
Unsupervised Hidden Markov Models (HMMs)
are an alternative SLM approach previously shown
to offer accuracy and scalability advantages over n-
gram models in ADS (Downey et al., 2007). An
HMM models a sentence w as a sequence of obser-
vations wi each generated by a hidden state variable
ti. Here, hidden states take values from {1, ... , T},
and each hidden state variable is itself generated by
some number k of previous hidden states. Formally,
the joint distribution of a word sequence w given a
corresponding state sequence t is:
</bodyText>
<equation confidence="0.9954265">
P(w|t) = � P(wi|ti)P(ti|ti−1, ... ,ti−k) (1)
i
</equation>
<bodyText confidence="0.9995462">
The distributions on the right side of Equation 1 are
learned from the corpus in an unsupervised manner
using Expectation-Maximization, such that words
distributed similarly in the corpus tend to be gen-
erated by similar hidden states (Rabiner, 1989).
</bodyText>
<subsectionHeader confidence="0.992751">
2.2 Performing ADS with SLMs
</subsectionHeader>
<bodyText confidence="0.999982904761905">
The Assessment by Distributional Similarity (ADS)
technique is to rank extractions in UR in decreas-
ing order of distributional similarity to the seeds,
as estimated from the corpus. In our experiments,
we utilize an ADS approach previously proposed for
HMMs (Downey et al., 2007) and adapt it to also ap-
ply to n-gram models, as detailed below.
Define a context of an extraction argument ei to
be a string containing the m words preceding and m
words following an occurrence of ei in the corpus.
Let Ci = {c1, c2,..., c|Ci|} be the union of all con-
texts of extraction arguments ei and seed arguments
si for a given relation R. We create a probabilis-
tic context vector for each extraction ei where the
j-th dimension of the vector is the probability of the
context surrounding given the extraction, P(cj|ei),
computed from the language model. 1
We rank the extractions in UR according to how
similar their arguments’ contextual distributions,
P(c|ei), are to those of the seed arguments. Specifi-
cally, extractions are ranked according to:
</bodyText>
<equation confidence="0.99435975">
f(e) =
eiEe
� KL(E&amp;ESM P(c |w&apos;) P(c |ei)) (2)
SRi |
</equation>
<bodyText confidence="0.999945714285714">
where KL represents KL Divergence, and the outer
sum is taken over arguments ei of the extraction e.
For HMMs, we alternatively rank extractions us-
ing the HMM state distributions P(t|ei) in place of
the probabilistic context vectors P(c|ei). Our exper-
iments show that state distributions are much more
accurate for ADS than are HMM context vectors.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999959">
In this section, we present experiments showing that
SLM accuracy correlates strongly with ADS perfor-
mance. We also show that SLM performance can be
used for model selection, leading to an ADS tech-
nique that outperforms previous results.
</bodyText>
<subsectionHeader confidence="0.98174">
3.1 Experimental Methodology
</subsectionHeader>
<bodyText confidence="0.999662333333333">
We experiment with a wide range of n-gram and
HMM models. The n-gram models are trained us-
ing the SRILM toolkit (Stolcke, 2002). Evaluating a
</bodyText>
<footnote confidence="0.99631225">
1For example, for context cj = “I visited in July” and ex-
traction ei = “Boston,” P(cj|ei) is P(“I visited Boston in July”)
/ P(“Boston”), where each string probability is computed using
the language model.
</footnote>
<page confidence="0.993972">
226
</page>
<table confidence="0.999875636363636">
LM Unary Binary Wikipedia
HMM 1-5 -.911 -.361 -.994
HMM 2-5 -.856 .120 -.930
HMM 3-5 -.823 -.683 .922
HMM 1-10 -.916 -.967 -.905
HMM 2-10 -.877 -.797 -.963
HMM 3-10 -.957 -.669 -.924
HMM 1-25 -.933 -.850 -.959
HMM 1-50 -.942 -.942 -.947
HMM 1-100 -.896 -.877 -.942
N-Gram -.512 -.999 .024
</table>
<tableCaption confidence="0.8898928">
Table 1: Pearson Correlation value for extraction perfor-
mance (in AUC) and SLM performance (in perplexity).
Extraction accuracy increases as perplexity decreases,
with an average correlation coefficient of -0.742. “HMM
k-T” denotes an HMM model of order k, with T states.
</tableCaption>
<figureCaption confidence="0.990848666666667">
Figure 1: HMM 1-100 Performance. Information Extrac-
tion performance (in AUC) increases as SLM accuracy
improves (perplexity decreases).
</figureCaption>
<bodyText confidence="0.9998186875">
variety of HMM configurations over a large corpus
requires a scalable training architecture. We con-
structed a parallel HMM codebase using the Mes-
sage Passing Interface (MPI), and trained the models
on a supercomputing cluster. All language models
were trained on a corpus of 2.8M sentences of Web
text (about 60 million tokens). SLM performance is
measured using the standard perplexity metric, and
assessment accuracy is measured using area under
the precision-recall curve (AUC), a standard metric
for ranked lists of extractions. We evaluated perfor-
mance on three distinct data sets. The first two data
sets evaluate ADS for unsupervised information ex-
traction, and were taken from (Downey et al., 2007).
The first, Unary, was an extraction task for unary
relations (Company, Country, Language, Film) and
the second, Binary, was a type-checking task for
binary relations (Conquered, Founded, Headquar-
tered, Merged). The 10 most frequent extractions
served as bootstrapped seeds. The two test sets con-
tained 361 and 265 extractions, respectively. The
third data set, Wikipedia, evaluates ADS on weakly-
supervised extraction, using seeds and extractions
taken from Wikipedia ’List of’ pages (Pantel et al.,
2009). Seed sets of various sizes (5, 10, 15 and
20) were randomly selected from each list, and we
present results averaged over 10 random samplings.
Other members of the seed list were added to a test
set as correct extractions, and elements from other
lists were added as errors. The data set included
2264 extractions across 36 unary relations, includ-
ing Composers and US Internet Companies.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Optimizing Language Models for IE
</subsectionHeader>
<bodyText confidence="0.999971428571429">
The first question we investigate is whether opti-
mizing individual language models leads to bet-
ter performance in ADS. We measured the correla-
tion between SLM perplexity and ADS performance
as training proceeds in HMMs, and as n and the
smoothing technique vary in the n-gram models. Ta-
ble 1 shows that as the SLM becomes more accurate
(i.e. as perplexity decreases), ADS performance in-
creases. The correlation is strong (averaging -0.742)
and is consistent across model configurations and
data sets. The low positive correlation for the n-
gram models on Wikipedia is likely due to a ”floor
effect”; the models have low performance overall
on the difficult Wikipedia data set. The lowest-
perplexity n-gram model (Mod Kneser-Ney smooth-
ing with n=3, KN3) does exhibit the best IE per-
formance, at 0.039 (the average performance of the
HMM models is more than twice this, at 0.084). Fig-
ure 1 shows the relationship between SLM and ADS
performance in detail for the best-performing HMM
configuration.
</bodyText>
<subsectionHeader confidence="0.999764">
3.3 Model Selection
</subsectionHeader>
<bodyText confidence="0.999872666666667">
Different language models can be configured in dif-
ferent ways: for example, HMMs require choices for
the hyperparameters k and T. Here, we show that
</bodyText>
<page confidence="0.995486">
227
</page>
<figureCaption confidence="0.998721666666667">
Figure 2: Model Selection for HMMs. SLM perfor-
mance is a good predictor of extraction performance
across model configurations.
</figureCaption>
<bodyText confidence="0.999984923076923">
SLM perplexity can be used to select a high-quality
model configuration for ADS using only unlabeled
data. We evaluate on the Unary and Binary data sets,
since they have been employed in previous work
on our corpora. Figure 2 shows that for HMMs,
ADS performance increases as perplexity decreases
across various model configurations (a similar rela-
tionship holds for n-gram models). A model selec-
tion technique that picks the HMM model with low-
est perplexity (HMM 1-100) results in better ADS
performance than previous results. As shown in Ta-
ble 2, HMM 1-100 reduces error over the HMM-T
model in (Downey et al., 2007) by 26%, on average.
The experiments also reveal an important difference
between the HMM and n-gram approaches. While
KN3 is more accurate in SLM than our HMM mod-
els, it performs worse in ADS on average. For exam-
ple, HMM 1-25 underperforms KN3 in perpexity, at
537.2 versus 227.1, but wins in ADS, 0.880 to 0.853.
We hypothesize that this is because the latent state
distributions in the HMMs provide a more informa-
tive distributional similarity measure. Indeed, when
we compute distributional similarity for HMMs us-
ing probabilistic context vectors as opposed to state
distributions, ADS performance for HMM 1-25 de-
creases to 5.8% below that of KN3.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.998996666666667">
We presented experiments showing that estimating
distributional similarity with more accurate statisti-
cal language models results in more accurate extrac-
</bodyText>
<table confidence="0.999846909090909">
Relation HMM-T Best HMM
Company .966 .985
Country .886 .942
Languages .936 .914
Film .803 .801
Unary Avg .898 .911
Conquered .917 .923
Founded .827 .799
Merged .920 .925
Headquartered .734 .964
Binary Average .849 .903
</table>
<tableCaption confidence="0.967992">
Table 2: Extraction Performance Results in AUC for In-
dividual Relations. The lowest-perplexity HMM, 1-100,
outperforms the HMM-T model from previous work.
</tableCaption>
<bodyText confidence="0.9998768">
tion assessment. We note that significantly larger,
more powerful language models are possible beyond
those evaluated here, which (based on the trajectory
observed in Figure 2) may offer significant improve-
ments in assessment accuracy.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903851851851">
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
T. C. Bell, J. G. Cleary, and I. H. Witten. 1990. Text
Compression. Prentice Hall, January.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proc. of ACL.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proc. of EMNLP.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257–286.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2.
</reference>
<page confidence="0.997667">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769497">
<title confidence="0.997178">Improved Extraction Assessment through Better Language Models</title>
<author confidence="0.982647">Arun Ahuja</author>
<author confidence="0.982647">Doug</author>
<affiliation confidence="0.952089">EECS Dept., Northwestern</affiliation>
<address confidence="0.793927">Evanston, IL</address>
<abstract confidence="0.999531588235294">A variety of information extraction techniques rely on the fact that instances of the same relation are “distributionally similar,” in that they tend to appear in similar textual contexts. We demonstrate that extraction accuracy depends heavily on the accuracy of the language model utilized to estimate distributional similarity. An unsupervised model selection technique based on this observation is shown to reduce extraction and type-checking error by 26% over previous results, in experiments with Hidden Markov Models. The results suggest that optimizing statistical language models over unlabeled data is a promising direction for improving weakly supervised and unsupervised information extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web. In Procs. of IJCAI.</title>
<date>2007</date>
<contexts>
<context position="1553" citStr="Banko et al., 2007" startWordPosition="228" endWordPosition="231">ised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis—the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985). A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of “seed” instances of the relation, where the seeds may be specified by hand (Pas¸ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in adv</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Procs. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall,</publisher>
<contexts>
<context position="2008" citStr="Bell et al., 1990" startWordPosition="292" endWordPosition="295"> existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007). Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be—to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate. </context>
<context position="4869" citStr="Bell et al., 1990" startWordPosition="770" endWordPosition="773"> Language Model (SLM) is a probability distribution P(w) over word sequences w = (w1, ..., wr). The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n−1 words. The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990). Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS (Downey et al., 2007). An HMM models a sentence w as a sequence of observations wi each generated by a hidden state variable ti. Here, hidden states take values from {1, ... , T}, and each hidden state variable is itself generated by some number k of previous hidden states. Formally, the joint distribution of a word sequence w given a corresponding state sequence t is: P(w|t) = � P(wi|ti)P(ti|ti−1, ... ,ti−k) (1) i The distributions o</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>T. C. Bell, J. G. Cleary, and I. H. Witten. 1990. Text Compression. Prentice Hall, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1973" citStr="Chen and Goodman, 1996" startWordPosition="286" endWordPosition="289">nd (Pas¸ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007). Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be—to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model</context>
<context position="4833" citStr="Chen and Goodman, 1996" startWordPosition="763" endWordPosition="767">Statistical Language Models A Statistical Language Model (SLM) is a probability distribution P(w) over word sequences w = (w1, ..., wr). The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n−1 words. The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990). Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS (Downey et al., 2007). An HMM models a sentence w as a sequence of observations wi each generated by a hidden state variable ti. Here, hidden states take values from {1, ... , T}, and each hidden state variable is itself generated by some number k of previous hidden states. Formally, the joint distribution of a word sequence w given a corresponding state sequence t is: P(w|t) = � P(wi|ti)P(ti|ti−1, </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>S Schoenmackers</author>
<author>O Etzioni</author>
</authors>
<title>Sparse information extraction: Unsupervised language models to the rescue.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2179" citStr="Downey et al., 2007" startWordPosition="322" endWordPosition="325">efer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007). Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be—to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate. In a large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques. The experiments </context>
<context position="3963" citStr="Downey et al., 2007" startWordPosition="615" endWordPosition="618">25–228, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances SR, and a list of candidate extractions UR, the task is to order elements of UR such that correct instances for R are ranked above extraction errors. Let URi denote the set of the ith arguments of the extractions in UR, and let SRi be defined similarly for the seed set SR. For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction (Downey et al., 2007). The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors. As an example, the extraction Founded(Bill Gates, Oracle) is type correct, but is not correct for the extraction task. 2.1 Statistical Language Models A Statistical Language Model (SLM) is a probability distribution P(w) over word sequences w = (w1, ..., wr). The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n−1 words. The accuracy of an n-gram model of a corpus depends on t</context>
<context position="6009" citStr="Downey et al., 2007" startWordPosition="957" endWordPosition="960">equence t is: P(w|t) = � P(wi|ti)P(ti|ti−1, ... ,ti−k) (1) i The distributions on the right side of Equation 1 are learned from the corpus in an unsupervised manner using Expectation-Maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989). 2.2 Performing ADS with SLMs The Assessment by Distributional Similarity (ADS) technique is to rank extractions in UR in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs (Downey et al., 2007) and adapt it to also apply to n-gram models, as detailed below. Define a context of an extraction argument ei to be a string containing the m words preceding and m words following an occurrence of ei in the corpus. Let Ci = {c1, c2,..., c|Ci|} be the union of all contexts of extraction arguments ei and seed arguments si for a given relation R. We create a probabilistic context vector for each extraction ei where the j-th dimension of the vector is the probability of the context surrounding given the extraction, P(cj|ei), computed from the language model. 1 We rank the extractions in UR accord</context>
<context position="9204" citStr="Downey et al., 2007" startWordPosition="1484" endWordPosition="1487">cture. We constructed a parallel HMM codebase using the Message Passing Interface (MPI), and trained the models on a supercomputing cluster. All language models were trained on a corpus of 2.8M sentences of Web text (about 60 million tokens). SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions. We evaluated performance on three distinct data sets. The first two data sets evaluate ADS for unsupervised information extraction, and were taken from (Downey et al., 2007). The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged). The 10 most frequent extractions served as bootstrapped seeds. The two test sets contained 361 and 265 extractions, respectively. The third data set, Wikipedia, evaluates ADS on weaklysupervised extraction, using seeds and extractions taken from Wikipedia ’List of’ pages (Pantel et al., 2009). Seed sets of various sizes (5, 10, 15 and 20) were randomly selected from each list, and we </context>
<context position="12050" citStr="Downey et al., 2007" startWordPosition="1947" endWordPosition="1950">. SLM perplexity can be used to select a high-quality model configuration for ADS using only unlabeled data. We evaluate on the Unary and Binary data sets, since they have been employed in previous work on our corpora. Figure 2 shows that for HMMs, ADS performance increases as perplexity decreases across various model configurations (a similar relationship holds for n-gram models). A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results. As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in (Downey et al., 2007) by 26%, on average. The experiments also reveal an important difference between the HMM and n-gram approaches. While KN3 is more accurate in SLM than our HMM models, it performs worse in ADS on average. For example, HMM 1-25 underperforms KN3 in perpexity, at 537.2 versus 227.1, but wins in ADS, 0.880 to 0.853. We hypothesize that this is because the latent state distributions in the HMMs provide a more informative distributional similarity measure. Indeed, when we compute distributional similarity for HMMs using probabilistic context vectors as opposed to state distributions, ADS performance</context>
</contexts>
<marker>Downey, Schoenmackers, Etzioni, 2007</marker>
<rawString>D. Downey, S. Schoenmackers, and O. Etzioni. 2007. Sparse information extraction: Unsupervised language models to the rescue. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics.</booktitle>
<editor>In J. J. Katz, editor,</editor>
<contexts>
<context position="1156" citStr="Harris, 1985" startWordPosition="161" endWordPosition="162">del selection technique based on this observation is shown to reduce extraction and type-checking error by 26% over previous results, in experiments with Hidden Markov Models. The results suggest that optimizing statistical language models over unlabeled data is a promising direction for improving weakly supervised and unsupervised information extraction. 1 Introduction Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis—the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985). A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of “seed” instances of the relation, where the seeds may be specified by hand (Pas¸ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various con</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Z. Harris. 1985. Distributional structure. In J. J. Katz, editor, The Philosophy of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: Fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In Procs. of ACL/COLING</booktitle>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Names and similarities on the web: Fact extraction in the fast lane. In Procs. of ACL/COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>E Crestan</author>
<author>A Borkovsky</author>
<author>A M Popescu</author>
<author>V Vyas</author>
</authors>
<title>Web-scale distributional similarity and entity set expansion.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1467" citStr="Pantel et al., 2009" startWordPosition="213" endWordPosition="216">y supervised and unsupervised information extraction. 1 Introduction Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis—the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985). A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of “seed” instances of the relation, where the seeds may be specified by hand (Pas¸ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, th</context>
<context position="9710" citStr="Pantel et al., 2009" startWordPosition="1559" endWordPosition="1562">irst two data sets evaluate ADS for unsupervised information extraction, and were taken from (Downey et al., 2007). The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged). The 10 most frequent extractions served as bootstrapped seeds. The two test sets contained 361 and 265 extractions, respectively. The third data set, Wikipedia, evaluates ADS on weaklysupervised extraction, using seeds and extractions taken from Wikipedia ’List of’ pages (Pantel et al., 2009). Seed sets of various sizes (5, 10, 15 and 20) were randomly selected from each list, and we present results averaged over 10 random samplings. Other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors. The data set included 2264 extractions across 36 unary relations, including Composers and US Internet Companies. 3.2 Optimizing Language Models for IE The first question we investigate is whether optimizing individual language models leads to better performance in ADS. We measured the correlation between SLM perplexity an</context>
</contexts>
<marker>Pantel, Crestan, Borkovsky, Popescu, Vyas, 2009</marker>
<rawString>P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and V. Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="1988" citStr="Rabiner, 1989" startWordPosition="290" endWordPosition="291">, taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007). Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible. But how fruitful is such an effort likely to be—to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy? In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs bec</context>
<context position="5699" citStr="Rabiner, 1989" startWordPosition="910" endWordPosition="911"> sequence of observations wi each generated by a hidden state variable ti. Here, hidden states take values from {1, ... , T}, and each hidden state variable is itself generated by some number k of previous hidden states. Formally, the joint distribution of a word sequence w given a corresponding state sequence t is: P(w|t) = � P(wi|ti)P(ti|ti−1, ... ,ti−k) (1) i The distributions on the right side of Equation 1 are learned from the corpus in an unsupervised manner using Expectation-Maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989). 2.2 Performing ADS with SLMs The Assessment by Distributional Similarity (ADS) technique is to rank extractions in UR in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs (Downey et al., 2007) and adapt it to also apply to n-gram models, as detailed below. Define a context of an extraction argument ei to be a string containing the m words preceding and m words following an occurrence of ei in the corpus. Let Ci = {c1, c2,..., c|Ci|} be the union of all contexts of extraction ar</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In COLING/ACL</booktitle>
<contexts>
<context position="1445" citStr="Snow et al., 2006" startWordPosition="209" endWordPosition="212">for improving weakly supervised and unsupervised information extraction. 1 Introduction Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis—the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985). A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of “seed” instances of the relation, where the seeds may be specified by hand (Pas¸ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007). We refer to this technique as Assessment by Distributional Similarity (ADS). Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus. Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)). Because SLMs can be trained from o</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>2</volume>
<contexts>
<context position="7576" citStr="Stolcke, 2002" startWordPosition="1225" endWordPosition="1226">s using the HMM state distributions P(t|ei) in place of the probabilistic context vectors P(c|ei). Our experiments show that state distributions are much more accurate for ADS than are HMM context vectors. 3 Experiments In this section, we present experiments showing that SLM accuracy correlates strongly with ADS performance. We also show that SLM performance can be used for model selection, leading to an ADS technique that outperforms previous results. 3.1 Experimental Methodology We experiment with a wide range of n-gram and HMM models. The n-gram models are trained using the SRILM toolkit (Stolcke, 2002). Evaluating a 1For example, for context cj = “I visited in July” and extraction ei = “Boston,” P(cj|ei) is P(“I visited Boston in July”) / P(“Boston”), where each string probability is computed using the language model. 226 LM Unary Binary Wikipedia HMM 1-5 -.911 -.361 -.994 HMM 2-5 -.856 .120 -.930 HMM 3-5 -.823 -.683 .922 HMM 1-10 -.916 -.967 -.905 HMM 2-10 -.877 -.797 -.963 HMM 3-10 -.957 -.669 -.924 HMM 1-25 -.933 -.850 -.959 HMM 1-50 -.942 -.942 -.947 HMM 1-100 -.896 -.877 -.942 N-Gram -.512 -.999 .024 Table 1: Pearson Correlation value for extraction performance (in AUC) and SLM perform</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP, volume 2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>