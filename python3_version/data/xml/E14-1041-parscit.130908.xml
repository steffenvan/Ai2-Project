<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.989952">
Acquisition of Noncontiguous Class Attributes from Web Search Queries
</title>
<author confidence="0.931768">
Marius Pas¸ca
</author>
<affiliation confidence="0.855768">
Google Inc.
</affiliation>
<address confidence="0.679187">
1600 Amphitheatre Parkway
Mountain View, California 94043
</address>
<email confidence="0.999109">
mars@google.com
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998581875">
Previous methods for extracting attributes
(e.g., capital, population) of classes (Em-
pires) from Web documents or search
queries assume that relevant attributes oc-
cur verbatim in the source text. The ex-
tracted attributes are short phrases that
correspond to quantifiable properties of
various instances (ottoman empire, ro-
man empire, mughal empire) of the class.
This paper explores the extraction of non-
contiguous class attributes (manner (it)
claimed legitimacy of rule), from fact-
seeking and explanation-seeking queries.
The attributes cover properties that are
not always likely to be extracted as short
phrases from inherently-noisy queries.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848682539683">
Motivation: Resources such as Wikipedia (Remy,
2002) and Freebase (Bollacker et al., 2008) aim
at organizing knowledge around classes (Food in-
gredients, Astronomical objects, Religions) and
their instances (wheat flower, uranus, hinduism).
Due to inherent limitations associated with main-
taining and expanding human-curated resources,
their content may be incomplete. For example,
attributes representing the energy (or energy per
100g) or solubility in water are available in both
Wikipedia and Freebase for many instances of
Food ingredients (e.g., for olive oil, honey, fennel).
But the attributes are missing for some instances
(e.g., cornmeal). Moreover, structured informa-
tion about how long (it) lasts unopened or manner
(it) helps in weight loss is generally missing for
Food ingredients, from both resources. Such in-
formation is also often absent from among the at-
tributes acquired from either documents or queries
by previous extraction methods (Pas¸ca et al., 2007;
Van Durme et al., 2008). Previously extracted at-
tributes tend to be short, often nominal, phrases
like nutritional value and taste. Even when ex-
tracted attributes are not nominal (Pas¸ca, 2012),
they remain relatively short phrases such as good
for skin. As such, previous attributes have limited
ability to capture the finer-grained properties be-
ing asked about in queries such as “how long does
olive oil last unopened” and “how does honey help
in weight loss”. The presence of such queries
suggests that such information is relevant to Web
users. Identifying noncontiguous properties, or
attributes of interest to Web users, helps filling
some of the gaps in existing knowledge resources,
which otherwise could not be filled by attributes
extracted with previous methods.
Contributions: The contributions of this paper
are twofold. First, it introduces a method for the
acquisition of noncontiguous class attributes, from
fact or explanation-seeking Web search queries
like “how long does olive oil last unopened” or
“how does honey help in weight loss”. The re-
sulting attributes are more diverse than, and there-
fore subsume, the scope of attributes extracted
by previous methods. Indeed, previous meth-
ods are unlikely to extract attributes as specific
as length/duration (it) lasts unopened and man-
ner (it) helps in weight loss, for the instances olive
oil and honey of the class Food ingredients. Con-
versely, previously extracted attributes like nutri-
tional value and solubility in water are roughly
equivalent to the finer-grained nutritional value
(it) has and reason (it) dissolves in water, ex-
tracted from the queries “what nutritional value
does honey have” and “why does glucose dissolve
in water” respectively. Second, the noncontiguous
attributes can be simultaneously interpreted as bi-
nary relations pertaining to instances and classes.
The relations (helps in weight loss) connect an in-
stance (honey) or, more generally, a class (Food
ingredients), on one hand; and a loosely-typed un-
known argument (manner) whose value is of in-
terest to Web users, on the other hand. Because
</bodyText>
<page confidence="0.980411">
386
</page>
<note confidence="0.9930095">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 386–394,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9988674">
Web users already inquire about the value of one
of their arguments, the extracted relations are more
likely to be relevant for the respective instances
and classes, than relations extracted from arbitrary
document sentences (Fader et al., 2011).
</bodyText>
<sectionHeader confidence="0.968847" genericHeader="method">
2 Noncontiguous Attributes
</sectionHeader>
<bodyText confidence="0.999879666666667">
Intuitions: Users tend to formulate their Web
search queries based on knowledge that they al-
ready possess at the time of the search (Pas¸ca,
2007). Therefore, search queries play two roles
simultaneously: in addition to requesting new in-
formation, they indirectly convey knowledge in
the process. In particular, attributes correspond
to quantifiable properties of instances and their
classes. The extraction of attributes from queries
starts from the intuition that, if an attribute A is rel-
evant for a class C, then users are likely to ask for
the value of the attribute A, for various instances
I of the class C. If nutritional value and diameter
are relevant attributes of the classes Food ingre-
dients and Astronomical objects respectively, it is
likely that users submit queries to inquire about
the values of the attributes for instances of the
two classes. Such queries could take the form
“what is the (nutritional value)A of (olive oil)I”
and “what is the (diameter)A of (jupiter)I”; or
the more compact “(nutritional value)A of (olive
oil)I” and “(diameter)A of (jupiter)I”. In this
case, the attributes are relatively short phrases
(nutritional value, diameter), and are expected to
appear as contiguous phrases within queries. Pre-
vious methods on attribute extraction from queries
specifically target this type of attributes. In fact,
some methods apply dedicated extraction patterns
(e.g., A of I) over either queries (Pas¸ca et al.,
2007) or documents (Tokunaga et al., 2005). Other
methods expand manually-provided seed sets of
attributes, with other phrases that co-occur with
instances within queries, in similar contexts as the
seed attributes do (Pas¸ca, 2007).
While simpler properties are often mentioned in
queries as short, contiguous phrases, finer-grained
properties often are not. Queries seeking the rea-
son for solidification for some Food ingredients
could, but rarely do, contain the attribute ver-
batim (“what is the reason for the solidification
of honey”). Instead, queries are more likely to
inquire about the expected value, while specify-
ing the instance and the properties encoded by
the attribute (“(why)A does (honey)I (solidify)A”).
Readable descriptions (names) of the attributes
can be recovered from the queries, by assembling
the type of the expected value and the proper-
ties together (reason (it) solidifies). Thus, fact
and explanation-seeking queries are an intriguing
source of noncontiguous attributes that are not re-
stricted to short phrases, and are not required to
occur as contiguous phrases in queries.
Acquisition from Queries: The extraction
method proposed in this paper takes as input a set
of target classes, each of which is available as a
set of instances that belong to the class; and a set
of anonymized queries independent from one an-
other. As illustrated in Figure 1, the method se-
lects queries that contain an instance of a class
together with what is deemed to be likely a non-
contiguous attribute, and outputs ranked lists of
attributes for each class. The extraction consists
in several stages:
</bodyText>
<listItem confidence="0.9850888">
• selection of a subset of queries that contain
an instance in a form that suggests the queries ask
for the value of a noncontiguous attribute of the
instance;
• extraction of noncontiguous attributes, from
query fragments that describe the property of in-
terest and the type of its expected value;
• aggregation and ranking of attributes of in-
dividual instances of a class, into attributes of a
class.
</listItem>
<bodyText confidence="0.980620545454545">
Extraction Patterns: In order to determine
whether a query contains an attribute of a class,
the query is matched against the extraction pat-
terns from Table 1. The use of patterns in attribute
extraction has been previously suggested in (Pas¸ca
et al., 2007; Tokunaga et al., 2005), where the pat-
tern what is the A of I extracts noun-phrase A
attributes of instances I from queries and docu-
ments. In our case, the patterns are constructed
such that they match fact-seeking and explanation-
seeking questions that likely inquire about the
value of a relevant property of an instance I of the
class C. For example, the first pattern from Ta-
ble 1 matches queries such as “when did everquest
become free to play” and “when was radon dis-
covered as an element”, which inquire about the
date or time when certain events affected certain
properties of the instances everquest and radon re-
spectively. Instances I of the class C may be avail-
able as non-disambiguated items, that is, as strings
(java) whose meaning is otherwise unknown; or
as disambiguated items, that is, as strings associ-
</bodyText>
<page confidence="0.999354">
387
</page>
<tableCaption confidence="0.963959">
Target classes
Chemical elements: {radon, chlorine, argon, nitrogen, oxygen, carbon,
hydrogen, iron, zinc, ...}
Programming languages: {c#, javascript, haskell, json, perl, java, python, prolog,
cobol, lisp, actionscript, ...}
Video games: {minecraft, black ops II, league of legends, halo reach, everquest,
fable 2, world of warcraft, band hero, ...}
</tableCaption>
<subsectionHeader confidence="0.773883">
Query logs
</subsectionHeader>
<bodyText confidence="0.999870692307692">
when was radon discovered as an element how does oxygen return to the atmosphere
who discovered the element iron what family does zinc belong to in the periodic table
why does chlorine react with water what elements does argon combine with
how does oxygen interact with other elements how does nitrogen enter the soil
how many electrons does chlorine gain who is using lisp
how does javascript run who created haskell how does java execute
who invented the programming language cobol how long does python take to learn
how does java compile when was c# first released where does python install to
how does c# differ from c++ how does javascript store dates
when did minecraft come out for xbox 360 when did everquest become free to play
when was fable 2 released how much does world of warcraft cost to play online
who does the voice in black ops 2 when did league of legends become free to play
who can you unlock in band hero how many copies did halo reach sell the first day
</bodyText>
<equation confidence="0.27988">
Extracted class attributes
Chemical elements: {
</equation>
<bodyText confidence="0.824127">
date/time (it) was discovered as an element, manner (it) returns to the atmosphere,
who discovered the element, manner (it) enters the soil, reason (it) reacts with water,
elements (it) combines with, manner (it) reacts with other elements,
family (it) belongs to in the periodic table, number of electrons (it) gains, ...}
</bodyText>
<table confidence="0.4175837">
Programming languages: {
manner (it) runs, who created (it), who invented the programming language,
who is using (it), location (it) installs to, date/time (it) was first released,
manner (it) differs from c++, manner (it) compiles, manner (it) stores dates,
manner (it) executes, length/duration (it) takes to learn, file extension (it) uses, ...}
Video games: {
date/time (it) came out for xbox 360, date/time it came out for ps2,
date/time (it) was released, number of copies (it) sold first day,
date/time (it) came out for pc, who does the voice in (it), who can you unlock in (it),
price/quantity/degree (it) costs to play online, date/time (it) became free to play, ...}
</table>
<figureCaption confidence="0.999554">
Figure 1: Overview of extraction of noncontiguous attributes from Web search queries
</figureCaption>
<bodyText confidence="0.999991571428571">
ated with pointers to knowledge base entries with a
disambiguated meaning (Java (programming lan-
guage)). In the first case, the matching of a query
fragment, on one hand, to the portion of an ex-
traction pattern corresponding to an instance I, on
the other hand, consists in simple string match-
ing. In the second case, the matching requires
that the disambiguation of the query fragment, in
the context of the query, matches the desired dis-
ambiguated meaning of I from the pattern. The
subset of queries matching any of the extraction
patterns, for any instances I of a class C, are the
queries that contribute to extracting noncontigu-
ous attributes of the class C.
</bodyText>
<subsectionHeader confidence="0.960502">
Collecting Attributes of Individual Instances:
</subsectionHeader>
<bodyText confidence="0.9996692">
A small set of rules optionally converts wh-
prefixes into coarse-grained types of the expected
values (e.g., how long into length/duration; or
when into date/time). In the case of what-prefixed
queries, the adjacent noun phrase, if any, is con-
sidered to be the expected type (“what nutritional
value ..” into nutritional value). Similar rules
have been employed for shallow analysis of open-
domain questions (Dumais et al., 2002). The pred-
icate verbs in the remainder of the query are up-
dated, to match the tense specified by the auxil-
iary verb (e.g., “when did ..”), if any, following
the wh-prefix. Thus, the verb come is converted
to the past tense came, in the case of the query
“when did minecraft come out for xbox 360”. An
</bodyText>
<page confidence="0.981222">
388
</page>
<table confidence="0.765270095238095">
Extraction Pattern
→ Examples of Matched Queries
when [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ when did everquest become free to play
why [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ why does chlorine interact with water
where [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ where does radon occur naturally
how [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ how does nitrogen enter the soil
who [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ who did claude monet study under
how A [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ how fast does oxygen dissolve in water
who A I
→ who invented the programming language cobol
(Note: A does not start with [is|are|was|were])
what A [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ what elements does argon combine with
which A [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A
→ which ports does minecraft use
</table>
<tableCaption confidence="0.997297">
Table 1: The extraction patterns match queries that
</tableCaption>
<bodyText confidence="0.994347742424243">
are likely to inquire about the value of a noncon-
tiguous attribute of an instance (I=a required in-
stance; A=a required non-empty sequence of arbi-
trary tokens)
attribute is constructed from the concatenation of
the wh-prefix or expected type (date/time); the
slot pronoun it, in lieu of the instance (date/time
(it)); and the query remainder after tense conver-
sion (date/time (it) came out for xbox 360). If the
linking verb following the wh-prefix is a form of
be (e.g., was), then the linking verb is also re-
tained after the slot pronoun, to form a more co-
herent attribute (date/time (it) was first released).
Since constructed attributes are noun phrases, they
are more consistent with, and can be more eas-
ily inserted among, existing attributes in struc-
tured data repositories (infobox entries of articles
in Wikipedia, or property names or topics in Free-
base).
Aggregation into Class Attributes: Attributes of
a class C are aggregated from attributes of indi-
vidual instances I of the class. An attribute A
is deemed more relevant for C if the attribute is
extracted for more of the instances I of the class
C, and for fewer instances I that do not belong to
the class C. Concretely, the score of an attribute
for a class is the lower bound of the Wilson score
interval (Brown et al., 2001) where the number
of positive observations is the number of queries
for which the attribute A is extracted for some in-
stance I in the class C, |{Query(I, A)}I∈C|; and
the number of negative observations is the num-
ber of queries for which the attribute A is ex-
tracted for some instances I outside of the class
C, |{Query(I, A)}I /∈C|. The scores are internally
computed at 95% confidence. Attributes of each
class are ranked in decreasing order of their scores.
Reduction of Near-Duplicate Attributes: Due to
lexical variations across queries from which at-
tributes are extracted, some of the attributes are
equivalent or nearly equivalent to one another. For
example, gained independence, won its indepen-
dence and gained its freedom of the class Coun-
tries are roughly equivalent, although they employ
distinct tokens. The diversity and potential useful-
ness of a ranked list of attributes can be increased,
if groups of near-duplicate attributes are identified
in the list, and merged together.
A lower-ranked attribute is marked as a near-
duplicate of a higher-ranked (i.e., earlier) attribute
from the list, if all tokens from the lower-ranked
attribute match either tokens from the higher-
ranked attribute (gained independence vs. won
its independence), or tokens from synonyms of
phrases from the earlier attribute (gained indepen-
dence vs. won its independence; or takes to show
symptoms vs. takes to come out). Stop words,
which include linking verbs, pronouns, determin-
ers, conjunctions, wh-prefixes and prepositions,
are not required to match. Synonyms may be ei-
ther derived from existing lexical resources (e.g.,
WordNet (Fellbaum, 1998)), or mined from large
document collections (Madnani and Dorr, 2010).
Lower-ranked near-duplicate attributes are merged
with the higher-ranked ones from the ranked list,
thus improving the diversity of the list.
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<bodyText confidence="0.998688933333333">
Textual Data Sources: The experiments rely
on a random sample of around 1 billion fully-
anonymized queries in English, submitted to a
general-purpose Web search engine. Each query
is available independently from other queries, and
is accompanied by its frequency of occurrence in
the query logs.
Target Classes: Table 2 shows the set of 40 tar-
get classes for evaluating the attributes extracted
from queries. In an effort to reuse experimental
setup proposed in previous work, each of the 40
manually-compiled classes introduced in (Pas¸ca,
2007) is mapped into the Wikipedia category that
best matches it. For example, the evaluation
classes Aircraft Model, Movie, Religion and Ter-
</bodyText>
<page confidence="0.994879">
389
</page>
<subsectionHeader confidence="0.898717">
Class (Examples of Instances)
</subsectionHeader>
<bodyText confidence="0.982034338983051">
Actors (keanu reeves, milla jovovich, ben affleck), Air-
craft (boeing 737, bombardier crj200, embraer 170), An-
imated characters (bugs bunny, pink panther (character),
yosemite sam), Association football clubs (a.s. roma, flu-
minense football club, real madrid), Astronomical objects
(alpha centauri, jupiter, delta corvi), Automobiles (nis-
san gt-r, tesla model s, toyota prius), Awards (grammy
award, justin winsor prize (library), palme d’or), Battles
and operations of world war ii (battle of midway, opera-
tion postmaster, battle of milne bay), Chemical elements
(plutonium, radon, hydrogen), Cities (rio de janeiro, os-
aka, chiang mai), Companies (best buy, aveeno, pep-
sico), Countries (costa rica, rwanda, south korea), Cur-
rencies by country (japanese yen, swiss franc, korean
won), Digital cameras (canon eos 400d, nikon d3000,
pentax k10d), Diseases and disorders (anorexia nervosa,
hyperlysinemia, repetitive strain injury), Drugs (flutica-
sone propionate, phentermine, tramadol), Empires (ot-
toman empire, roman empire, mughal empire), Films (the
fifth element, mockingbird don’t sing, ten thousand years
older), Flowers (trachelospermum jasminoides, lavandula
stoechas, evergreen rose), Food ingredients (carrot, olive
oil, fennel), Holidays (good friday, easter, halloween),
Hurricanes in North America (hurricane katrina, hurri-
cane wilma, hurricane dennis), Internet search engines
(google, baidu, lycos), Mobile phones (nokia n900, htc
desire, samsung s5560), Mountains (mount rainier, cerro
san luis obispo, steel peak), National Basketball Associa-
tion teams (los angeles lakers, cleveland cavaliers, indiana
pacers), National parks (yosemite national park, orang na-
tional park, tortuguero national park), Newspapers (the
economist, corriere del trentino, seattle medium), Organi-
zations designated as terrorist (taliban, shining path, eta),
Painters (claude monet, domingo antonio velasco, tarci-
sio merati), Programming languages (javascript, prolog,
obliq), Religious faiths traditions and movements (con-
fucianism, fudoki, omnism), Rivers (danube, pingo river,
viehmoorgraben), Skyscrapers (taipei 101, 15 penn plaza,
eqt plaza), Sports events (tour de france, 1984 scottish cup
final, rotlewi versus rubinstein), Stadiums (fenway park,
chengdu longquanyi, stade geoffroy-guichard), Treaties
(treaty of versailles, franco-indian alliance, treaty of cor-
doba), Universities and colleges (cornell university, nu-
gaal university, gale college), Video games (minecraft,
league of legends, everquest), Wine (madeira wine, yel-
low tail (wine), port wine)
Table 2: Set of 40 Wikipedia categories used as
target classes in the evaluation of attributes
roristGroup from (Pas¸ca, 2007) are mapped into
the Wikipedia categories Aircraft, Films, Religious
faiths traditions and movements and Organiza-
tions designated as terrorist respectively. The
name of the Wikipedia category only serves as a
convenience label for its target class, and is not
otherwise exploited in any way during the evalua-
tion. Instead, a target class consists in a set of titles
of Wikipedia articles, of which sample titles (e.g.,
the Wikipedia article titled nissan gt-r) are shown
in lowercase for each class (e.g., Automobiles) in
</bodyText>
<tableCaption confidence="0.988627">
Table 2. The set of instances of a class is selected
from all articles listed under the respective cate-
</tableCaption>
<table confidence="0.999660769230769">
Label Examples of Attributes
vital Astronomical objects: manner (it) generates its
energy
Food ingredients: temperature (it) solidifies
Religion: date/time (it) became a religion
okay Astronomical objects: manner (it) became a
constellation
Food ingredients: reason (it) sparks in the mi-
crowave
Religion: manner (it) feels about abortion
wrong Astronomical objects: reason (it) has arms
Food ingredients: manner (it) cleans pennies
Religion: who owns (it)
</table>
<tableCaption confidence="0.997893">
Table 3: Correctness labels manually assigned to
attributes extracted for various classes
</tableCaption>
<bodyText confidence="0.97303964516129">
gory in Wikipedia, or listed under sub-categories
of the respective category.
The target classes contain between 41 (for Na-
tional Basketball Association teams) and 66,934
(for Films) instances, with an average of 10,730
instances per class.
Synonym Repository: A synonym repository ex-
tracted separately from Web documents contains
mappings from each of around 60,000 phrases in
English, to lists of their synonym phrases. For ex-
ample, the top synonyms available for the phrases
turn off and contagious are [switch off, extinguish,
turn out, ..] and [infectious, catching, communica-
ble, ..] respectively.
Parameter Settings: Queries that match any of
the extraction patterns from Table 1 are syntac-
tically parsed (Petrov et al., 2010). As a pre-
requisite, the portion I of the patterns from the
table must match a disambiguated instance from
a query.
A variation of the tagger introduced
in (Cucerzan, 2007) maps query fragments
to their disambiguated, corresponding Wikipedia
instances (i.e., to Wikipedia articles). The tagger
is simplified to select the longest instance men-
tions, and does not use gazetteers or queries for
training. Depending on the sources of textual
data available for training, any taggers (Cucerzan,
2007; Ratinov et al., 2011; Pantel et al., 2012) that
disambiguate text fragments relative to Wikipedia
entries can be employed.
</bodyText>
<sectionHeader confidence="0.995605" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.9904732">
Attribute Accuracy: The top 50 attributes, from
the ranked lists extracted for each target class, are
manually assigned correctness labels. As shown in
Table 3, an attribute is marked as vital, if it must
be present among representative attributes of the
</bodyText>
<page confidence="0.994294">
390
</page>
<table confidence="0.985373363636364">
Class Precision of Extracted Attributes
%vital %okay %wrong Score
Awards 29 14 7 0.72
Chemical elements 46 2 2 0.94
Companies 42 1 7 0.85
Food ingredients 31 9 10 0.71
Programming languages 31 7 12 0.69
Stadiums 42 5 3 0.89
Video games 33 14 3 0.80
...
Avg-All-Classes 33 10 7 0.76
</table>
<tableCaption confidence="0.995477">
Table 4: Accuracy of top 50 class attributes ex-
</tableCaption>
<bodyText confidence="0.976431794871795">
tracted from fact-seeking and explanation-seeking
queries, over the evaluation set of 40 target classes
class; okay, if it provides useful but non-essential
information; and wrong, if it is incorrect (Pas¸ca,
2007). For example, the attributes manner (it) gen-
erates its energy, manner (it) became a constella-
tion and reason (it) has arms are annotated as vital,
okay and wrong respectively for the class Astro-
nomical objects. To compute the precision score
over a set of attributes, the correctness labels are
converted to numeric values: vital to 1.0, okay to
0.5, and wrong to 0.0. Precision is the sum of the
correctness values of the attributes, divided by the
number of attributes.
Table 4 summarizes the precision scores over
the evaluation set of target classes. The scores
vary from one class to another, for example 0.71
for Food ingredients but 0.94 for Chemical el-
ements. The average score is 0.76, indicating
that attributes extracted from fact and explanation-
seeking queries have encouraging levels of accu-
racy. The results already take into account the
detection of near-duplicate attributes. More pre-
cisely, the highest-ranked attribute in each group
of near-duplicate attributes, examples of which are
shown in Table 5, is retained and evaluated; the
lower-ranked attributes from each group are not
considered in the evaluation. Attributes like num-
ber of passengers (it) can hold, number of pas-
sengers it fits and number of passengers it seats
are nearly equivalent, but are still not marked as
near-duplicates for the class Aircraft, when they
should. Conversely, the attribute location (it)
lives is marked as a near-duplicate of location (it)
lives in new york, when it should not. Never-
theless, a significant number of near-duplicates,
which would otherwise crowd the ranked lists of
attributes with redundant information, are identi-
fied and discarded.
</bodyText>
<figureCaption confidence="0.60600852173913">
Target Class: Group of Near-Duplicate Attributes
Actors: movies (it) plays in, played in, acts in, acted in,
played, played on
Automobiles: date (it) was first manufactured, first pro-
duced, first made
Battles and operations of World War II: reason (it) hap-
pened, took place, occurred
Chemical elements: manner (it) returns to the atmo-
sphere, gets back into the atmosphere, got into the atmo-
sphere, gets into the atmosphere, enters the environment,
enters the atmosphere
Companies: location (it) makes its products, manufac-
tures its products, produces its products, gets its products,
makes its products, manufactures their products
Companies: date/time (it) began outsourcing, started out-
sourcing, outsourced
Countries: date (it) got its independence, gained indepen-
dence, gained its independence, got independence, got
their independence, won its independence, achieved inde-
pendence, received its independence, gained its freedom
Diseases and disorders: length/duration (it) takes to show
symptoms, takes to show up, takes to show, takes to ap-
pear, takes to manifest, takes to come out
</figureCaption>
<bodyText confidence="0.99315396875">
Table 5: Groups of near-duplicate attributes iden-
tified for various classes. Attributes within a group
are ranked according to their individual scores.
Removing all but the first attribute of each group,
from the ranked list of attributes of the respective
class, improves the diversity of the list
Discussion: The set of patterns shown in Table 1
is extensible. Moreover, the patterns are subject
to errors. They may cause false matches, resulting
in erroneous extractions. The extent to which this
occurs is indirectly measured in the overall preci-
sion results. The modification of some of the pat-
terns, or the addition of new ones, would likely af-
fect the expected coverage and precision of the ex-
tracted attributes. If a pattern is particularly noisy,
it is likely to cause systematic errors, and therefore
produce attributes of lower quality.
Since attributes in Wikipedia and Freebase are
initially entered manually by human editors, their
correctness is virtually guaranteed. As for at-
tributes extracted automatically, previous compar-
isons indicate that attributes tend to have higher
quality when extracted from queries instead of
documents (Pas¸ca, 2007). Indeed, a set of
extraction patterns applied to text produces at-
tributes whose average precision at rank 50 is 0.44
when extracted from documents, vs. 0.63 from
queries (Pas¸ca et al., 2007). More importantly,
previously available or extracted attributes are vir-
tually always simple, short noun phrases like nu-
tritional value, taste or solubility in water. Even if
not confined to noun phrases, they are still short,
</bodyText>
<page confidence="0.996332">
391
</page>
<table confidence="0.7855785">
Run: [Ranked Attributes for a Sample of Classes]
Class: Automobiles:
</table>
<bodyText confidence="0.8616725">
D: [(it) goes on sale, (it) will go on sale, (it) is an en-
gineering playground, (it) will be available in japan, (it)
shows up in japan, (it) is a technical tour de force, (it) un-
veiled at tas 2008, (it) runs a 7:38, (it) is a unique car, (it)
uses a premium midship package, (it) features an all-new
3.8-litre, (it) is one of the fastest cars, (it) made a quick
drive-by,..]
Q: [price/quantity/degree (it) weights, year (it) was
banned from bathurst, manner (it) launch control
works, engine (it) has, kind of engine (it) has,
price/quantity/degree (it) costs in japan, number of horse-
power (it) has, price/quantity/degree horsepower (it) has,
number of seats (it) has, speed (it) goes, who designed
(it), ..]
</bodyText>
<subsectionHeader confidence="0.567035">
Class: Mobile phones:
</subsectionHeader>
<bodyText confidence="0.6633966">
D: [(it) was announced on september 17 2008, (it) ceased
with version, (it) was scheduled to be released in late
2010, (it) also supports qt (toolkit), (it) supports hardware
capable, (it) can synchronize with microsoft outlook, (it)
also supports python (programming language),..]
Q: [date/time (it) came out in australia, who carries (it),
reason (it) keeps rebooting, colours (it) comes in, video
format (it) supports, date/time (it) was released, date/time
(it) came out in the uk, length/duration (it)’s battery lasts,
who sells (it), how much (it) costs,..]
</bodyText>
<subsectionHeader confidence="0.476899">
Class: Mountains:
</subsectionHeader>
<bodyText confidence="0.998201564516129">
D: [(it) is an active volcano, (it) is in the distance, (it)
is the highest peak in cascade range, (it) is 14,410 feet,
(it) was established in 1899, (it) comes into view, (it) was
established as a national park, ..]
Q: [date/time (it) last erupted, manner (it) erupted in 1882,
manner (it) formed, date/time (it) first became active,
manner (it) got its name, number of eruptions (it) had,
type of magma (it) has, reason (it) became a national park,
kind of animals (it) has, ..]
Table 6: Top relations extracted for a sample of
target classes via open-domain relations from doc-
uments (D) or via attributes from queries (Q)
like vegan, healthy or gluten free (Van Durme et
al., 2008; Pas¸ca, 2012). In comparison, attributes
extracted in this paper accommodate properties
that are sometimes awkward or even impossible
to express through short phrases.
Noncontiguous Attributes as Relations: Non-
contiguous attributes extracted from fact-seeking
queries are embodiments of relations linking the
instances mentioned in the queries, on one hand,
and the values being requested by the queries, on
the other hand. Therefore, the method proposed in
this paper can also be regarded as a method for the
acquisition of relevant relations of various classes.
The extracted relations specify the left argument
(i.e., the instance) and the linking relation name
(i.e., the attribute). They only specify the type
of the, but not the actual, right argument (i.e., the
value being requested).
An additional experiment compares the accu-
racy of relations extracted as noncontiguous at-
tributes from queries, vs. relations extracted by a
previous open-domain method (Fader et al., 2011)
from 500 million Web documents. The previous
method, including its extraction patterns and its
ranking scheme, is designed with instances rather
than classes in mind. For fairness to the method
in (Fader et al., 2011), the evaluation procedure
is slightly adjusted. The set of instances associ-
ated with each target class, over which the two
methods are evaluated, is reduced to a single repre-
sentative instance selected a-priori. The instances
are shown as the first instances in parentheses for
each class in the earlier Table 2. Thus, the class
attributes are extracted using only the instances
keanu reeves, boeing 737 and bugs bunny in the
case of the classes Actors, Aircraft and Animated
characters respectively.
Table 6 suggests that noncontiguous attributes
extracted from queries tend to capture higher-
quality relations than arbitrary relations extracted
from documents. Because fact-seeking queries in-
quire about the value of some relations (attributes)
of an instance, the relations themselves tends to
be more relevant than relations extracted from ar-
bitrary document sentences. Nevertheless, rela-
tions derived from queries likely serve as a useful
complement, rather than replacement, of relations
from documents. The former only discover what
relations may be relevant; the latter also identify
their occurrences within text.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999225277777778">
Sources of text from which relations (Zhu et
al., 2009; Carlson et al., 2010; Lao et al.,
2011) and, more specifically, attributes can be
extracted include Web documents and data in
human-compiled encyclopedia. In Web docu-
ments, attributes are available within unstruc-
tured (Tokunaga et al., 2005; Pas¸ca et al., 2007),
structured (Raju et al., 2008) and semi-structured
text (Yoshinaga and Torisawa, 2007), layout for-
matting tags (Wong et al., 2008), itemized lists or
tables (Cafarella et al., 2008). In human-compiled
encyclopedia (Wu and Weld, 2010), data relevant
to attribute extraction includes infoboxes and cat-
egory labels (Nastase and Strube, 2008; Hoffart
et al., 2013) associated with Wikipedia articles.
In order to acquire class attributes, a common
strategy is to first acquire attributes of instances,
then aggregate or propagate (Talukdar and Pereira,
</bodyText>
<page confidence="0.994123">
392
</page>
<bodyText confidence="0.9999295">
2010) attributes, from instances to the classes to
which the instances belong. The role of Web
search queries, as an alternative textual data source
to Web documents in open-domain information
extraction, has been investigated in the tasks of at-
tribute extraction (Pas¸ca, 2007; Pas¸ca, 2012), as
well as in collecting sets of related instances (Jain
and Pennacchiotti, 2010).
To increase diversity within a ranked list of at-
tributes, the extraction method in this paper em-
ploys a synonym vocabulary to approximately
identify groups of near-duplicate attributes. As
reported for previous methods, the resulting lists
may still contain lexically different but semanti-
cally equivalent attributes. Scenarios where de-
tecting all equivalent attributes is important may
benefit from other techniques for paraphrase ac-
quisition (Madnani and Dorr, 2010).
Sophisticated techniques are sometimes em-
ployed to identify the type of the expected an-
swers of open-domain questions (Pinchak et al.,
2009). In comparison, the loose typing of the
values of our noncontiguous attributes is mostly
coarse-grained. It relies on wh-prefixes (when,
how long, where, how) and possibly subsequent
words (what nutritional value) from the queries,
to determine whether the values are expected to
be a date/time, length/duration, location, manner,
nutritional value etc.
Relations extracted from document sentences
(e.g., “Claude Monet was born in Paris”) are tu-
ples of an instance (claude monet), a text fragment
acting as the lexicalized relation (was born in), and
another instance (paris) (cf. (Fader et al., 2011;
Mausam et al., 2012)). For convenience, the re-
lation and second instance may be concatenated,
as in was born in paris for claude monet. But
document sentences mentioning an instance do not
necessarily refer to properties of the instance that
people other than the author of the document are
likely to inquire about. Consequently, even top-
ranked extracted relations occasionally include
less informative ones, such as comes into view for
mount rainier, is on the table for madeira wine,
or allows for features for javascript (Fader et al.,
2011). Comparatively, relations extracted via non-
contiguous attributes from queries tend to refer to
properties that have values that Web users inquire
about in their search queries. Therefore, the rela-
tions extracted from queries are more likely to re-
fer to salient properties, such as date/time (it) had
its last eruption for mount rainier; length/duration
(it) lasts for madeira wine; and manner (it) stores
date information for javascript.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975105263158">
By requesting values for attributes of individual
instances, fact-seeking and explanation-seeking
queries implicitly assert the relevance of the prop-
erties encoded by the attributes, for the respec-
tive instances and their classes. The extracted at-
tributes are not required to take the form of con-
tiguous short phrases in the source queries, thus
allowing for the acquisition of a broader range of
attributes than those extracted by previous meth-
ods. Furthermore, since Web users are interested
in their values, the relations to which the ex-
tracted attributes refer tend to be more relevant
than relations extracted from arbitrary documents
using previous methods. Current work explores
the role of distributional similarities in expanding
extracted attributes for narrow classes; and the ex-
traction of noncontiguous attributes and relations
from natural-language queries without a wh-prefix
(e.g., cars driven by james bond).
</bodyText>
<sectionHeader confidence="0.997477" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999100666666667">
The author would like to thank Efrat Farkash and
Michael Kleyman for assistance with the synonym
repository.
</bodyText>
<sectionHeader confidence="0.998598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998666913043478">
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.
2008. Freebase: A collaboratively created graph database
for structuring human knowledge. In Proceedings of the
2008 International Conference on Management of Data
(SIGMOD-08), pages 1247–1250, Vancouver, Canada.
L. Brown, T. Cai, and A. DasGupta. 2001. Interval es-
timation for a binomial proportion. Statistical Science,
16(2):101–117.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on the
Web. In Proceedings of the 34th Conference on Very
Large Data Bases (VLDB-08), pages 538–549, Auckland,
New Zealand.
A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and
T. Mitchell. 2010. Coupled semi-supervised learning for
information extraction. In Proceedings of the 3rd ACM
Conference on Web Search and Data Mining (WSDM-10),
pages 101–110, New York.
S. Cucerzan. 2007. Large-scale named entity disambigua-
tion based on Wikipedia data. In Proceedings of the 2007
Conference on Empirical Methods in Natural Language
Processing (EMNLP-07), pages 708–716, Prague, Czech
Republic.
</reference>
<page confidence="0.995997">
393
</page>
<reference confidence="0.999870763157895">
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: Is more always better? In Pro-
ceedings of the 24th ACM Conference on Research and
Development in Information Retrieval (SIGIR-02), pages
207–214, Tampere, Finland.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proceedings
of the 2011 Conference on Empirical Methods in Iatu-
ral Language Processing (EMILP-11), pages 1535–1545,
Edinburgh, Scotland.
C. Fellbaum, editor. 1998. WordIet: An Electronic Lexical
Database and Some of its Applications. MIT Press.
J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. 2013.
YAGO2: a spatially and temporally enhanced knowledge
base from Wikipedia. Artificial Intelligence Journal. Spe-
cial Issue on Artificial Intelligence, Wikipedia and Semi-
Structured Resources, 194:28–61.
A. Jain and M. Pennacchiotti. 2010. Open entity extrac-
tion from Web search query logs. In Proceedings of the
23rd International Conference on Computational Linguis-
tics (COLIIG-10), pages 510–518, Beijing, China.
N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk in-
ference and learning in a large scale knowledge base. In
Proceedings of the 2011 Conference on Empirical Meth-
ods in Iatural Language Processing (EMILP-11), pages
529–539, Edinburgh, Scotland.
N. Madnani and B. Dorr. 2010. Generating phrasal and
sentential paraphrases: a survey of data-driven methods.
Computational Linguistics, 36(3):341–387.
Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni.
2012. Open language learning for information extraction.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Iatural Language Processing and Computa-
tional Iatural Language Learning (EMILP-CoILL-12),
pages 523–534, Jeju Island, Korea.
V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd Iational Conference on Artificial Intelligence
(AAAI-08), pages 1219–1224, Chicago, Illinois.
M. Pas¸ca, B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of the 16th International Conference
on Information and Knowledge Management (CIKM-07),
pages 485–494, Lisbon, Portugal.
M. Pas¸ca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101–110, Banff, Canada.
M. Pas¸ca. 2012. Attribute extraction from conjectural
queries. In Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLIIG-12), Mum-
bai, India.
P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity types
from query logs via user intent modeling. In Proceedings
of the 50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-12), pages 563–571, Jeju Island,
Korea.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing. In
Proceedings of the 2010 Conference on Empirical Meth-
ods in Iatural Language Processing (EMILP-10), pages
705–713, Cambridge, Massachusetts.
C. Pinchak, D. Lin, and D. Rafiei. 2009. Flexible answer
typing with discriminative preference ranking. In Pro-
ceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics (EACL-
09), pages 666–674, Athens, Greece.
S. Raju, P. Pingali, and V. Varma. 2008. An unsupervised ap-
proach to product attribute extraction. In Proceedings of
the 31st International Conference on Research and Devel-
opment in Information Retrieval (SIGIR-08), pages 35–42,
Singapore.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation to
Wikipedia. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL-11),
pages 1375–1384, Portland, Oregon.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
P. Talukdar and F. Pereira. 2010. Experiments in graph-based
semi-supervised learning methods for class-instance ac-
quisition. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-10),
pages 1473–1481, Uppsala, Sweden.
K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Automatic
discovery of attribute words from Web documents. In
Proceedings of the 2nd International Joint Conference on
Iatural Language Processing (IJCILP-05), pages 106–
118, Jeju Island, Korea.
B. Van Durme, T. Qian, and L. Schubert. 2008. Class-
driven attribute extraction. In Proceedings of the 22nd
International Conference on Computational Linguistics
(COLIIG-08), pages 921–928, Manchester, United King-
dom.
T. Wong, W. Lam, and T. Wong. 2008. An unsuper-
vised framework for extracting and normalizing product
attributes from multiple Web sites. In Proceedings of the
31st International Conference on Research and Develop-
ment in Information Retrieval (SIGIR-08), pages 35–42,
Singapore.
F. Wu and D. Weld. 2010. Open information extraction using
Wikipedia. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-10),
pages 118–127, Uppsala, Sweden.
N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured texts. In
Proceedings of the 6th International Semantic Web Con-
ference (ISWC-07), Workshop on Text to Knowledge: The
Lexicon/Ontology Interface (OntoLex-2007), pages 55–
66, Busan, South Korea.
J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. Stat-
Snowball: a statistical approach to extracting entity rela-
tionships. In Proceedings of the 18th World Wide Web
Conference (WWW-09), pages 101–110, Madrid, Spain.
</reference>
<page confidence="0.998996">
394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372847">
<title confidence="0.7393765">Acquisition of Noncontiguous Class Attributes from Web Search Queries Google</title>
<address confidence="0.8663295">1600 Amphitheatre Mountain View, California</address>
<email confidence="0.99979">mars@google.com</email>
<abstract confidence="0.999296647058823">Previous methods for extracting attributes of classes from Web documents or search queries assume that relevant attributes occur verbatim in the source text. The extracted attributes are short phrases that correspond to quantifiable properties of instances roof the class. This paper explores the extraction of nonclass attributes (it) legitimacy of from factseeking and explanation-seeking queries. The attributes cover properties that are not always likely to be extracted as short phrases from inherently-noisy queries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Management of Data (SIGMOD-08),</booktitle>
<pages>1247--1250</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="931" citStr="Bollacker et al., 2008" startWordPosition="127" endWordPosition="130">assume that relevant attributes occur verbatim in the source text. The extracted attributes are short phrases that correspond to quantifiable properties of various instances (ottoman empire, roman empire, mughal empire) of the class. This paper explores the extraction of noncontiguous class attributes (manner (it) claimed legitimacy of rule), from factseeking and explanation-seeking queries. The attributes cover properties that are not always likely to be extracted as short phrases from inherently-noisy queries. 1 Introduction Motivation: Resources such as Wikipedia (Remy, 2002) and Freebase (Bollacker et al., 2008) aim at organizing knowledge around classes (Food ingredients, Astronomical objects, Religions) and their instances (wheat flower, uranus, hinduism). Due to inherent limitations associated with maintaining and expanding human-curated resources, their content may be incomplete. For example, attributes representing the energy (or energy per 100g) or solubility in water are available in both Wikipedia and Freebase for many instances of Food ingredients (e.g., for olive oil, honey, fennel). But the attributes are missing for some instances (e.g., cornmeal). Moreover, structured information about h</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 International Conference on Management of Data (SIGMOD-08), pages 1247–1250, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Brown</author>
<author>T Cai</author>
<author>A DasGupta</author>
</authors>
<title>Interval estimation for a binomial proportion.</title>
<date>2001</date>
<journal>Statistical Science,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="15085" citStr="Brown et al., 2001" startWordPosition="2399" endWordPosition="2402">nsistent with, and can be more easily inserted among, existing attributes in structured data repositories (infobox entries of articles in Wikipedia, or property names or topics in Freebase). Aggregation into Class Attributes: Attributes of a class C are aggregated from attributes of individual instances I of the class. An attribute A is deemed more relevant for C if the attribute is extracted for more of the instances I of the class C, and for fewer instances I that do not belong to the class C. Concretely, the score of an attribute for a class is the lower bound of the Wilson score interval (Brown et al., 2001) where the number of positive observations is the number of queries for which the attribute A is extracted for some instance I in the class C, |{Query(I, A)}I∈C|; and the number of negative observations is the number of queries for which the attribute A is extracted for some instances I outside of the class C, |{Query(I, A)}I /∈C|. The scores are internally computed at 95% confidence. Attributes of each class are ranked in decreasing order of their scores. Reduction of Near-Duplicate Attributes: Due to lexical variations across queries from which attributes are extracted, some of the attribute</context>
</contexts>
<marker>Brown, Cai, DasGupta, 2001</marker>
<rawString>L. Brown, T. Cai, and A. DasGupta. 2001. Interval estimation for a binomial proportion. Statistical Science, 16(2):101–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>A Halevy</author>
<author>D Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>WebTables: Exploring the power of tables on the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08),</booktitle>
<pages>538--549</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="32943" citStr="Cafarella et al., 2008" startWordPosition="5137" endWordPosition="5140">rmer only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-domain information extraction, has been investigated i</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. WebTables: Exploring the power of tables on the Web. In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08), pages 538–549, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>R Wang</author>
<author>E Hruschka</author>
<author>T Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM Conference on Web Search and Data Mining (WSDM-10),</booktitle>
<pages>101--110</pages>
<location>New York.</location>
<contexts>
<context position="32518" citStr="Carlson et al., 2010" startWordPosition="5072" endWordPosition="5075">quality relations than arbitrary relations extracted from documents. Because fact-seeking queries inquire about the value of some relations (attributes) of an instance, the relations themselves tends to be more relevant than relations extracted from arbitrary document sentences. Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and T. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the 3rd ACM Conference on Web Search and Data Mining (WSDM-10), pages 101–110, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07),</booktitle>
<pages>708--716</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="22433" citStr="Cucerzan, 2007" startWordPosition="3484" endWordPosition="3485">d separately from Web documents contains mappings from each of around 60,000 phrases in English, to lists of their synonym phrases. For example, the top synonyms available for the phrases turn off and contagious are [switch off, extinguish, turn out, ..] and [infectious, catching, communicable, ..] respectively. Parameter Settings: Queries that match any of the extraction patterns from Table 1 are syntactically parsed (Petrov et al., 2010). As a prerequisite, the portion I of the patterns from the table must match a disambiguated instance from a query. A variation of the tagger introduced in (Cucerzan, 2007) maps query fragments to their disambiguated, corresponding Wikipedia instances (i.e., to Wikipedia articles). The tagger is simplified to select the longest instance mentions, and does not use gazetteers or queries for training. Depending on the sources of textual data available for training, any taggers (Cucerzan, 2007; Ratinov et al., 2011; Pantel et al., 2012) that disambiguate text fragments relative to Wikipedia entries can be employed. 4 Evaluation Results Attribute Accuracy: The top 50 attributes, from the ranked lists extracted for each target class, are manually assigned correctness </context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07), pages 708–716, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>M Banko</author>
<author>E Brill</author>
<author>J Lin</author>
<author>A Ng</author>
</authors>
<title>Web question answering: Is more always better?</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th ACM Conference on Research and Development in Information Retrieval (SIGIR-02),</booktitle>
<pages>207--214</pages>
<location>Tampere, Finland.</location>
<contexts>
<context position="12556" citStr="Dumais et al., 2002" startWordPosition="1985" endWordPosition="1988">the extraction patterns, for any instances I of a class C, are the queries that contribute to extracting noncontiguous attributes of the class C. Collecting Attributes of Individual Instances: A small set of rules optionally converts whprefixes into coarse-grained types of the expected values (e.g., how long into length/duration; or when into date/time). In the case of what-prefixed queries, the adjacent noun phrase, if any, is considered to be the expected type (“what nutritional value ..” into nutritional value). Similar rules have been employed for shallow analysis of opendomain questions (Dumais et al., 2002). The predicate verbs in the remainder of the query are updated, to match the tense specified by the auxiliary verb (e.g., “when did ..”), if any, following the wh-prefix. Thus, the verb come is converted to the past tense came, in the case of the query “when did minecraft come out for xbox 360”. An 388 Extraction Pattern → Examples of Matched Queries when [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A → when did everquest become free to play why [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A → why does chlorine interact with water where [does|did|do|was|were] [a|an|the|&lt;nothing&gt;] I A → wher</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering: Is more always better? In Proceedings of the 24th ACM Conference on Research and Development in Information Retrieval (SIGIR-02), pages 207–214, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP-11),</booktitle>
<pages>1535--1545</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="4369" citStr="Fader et al., 2011" startWordPosition="652" endWordPosition="655">ly, a class (Food ingredients), on one hand; and a loosely-typed unknown argument (manner) whose value is of interest to Web users, on the other hand. Because 386 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 386–394, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Web users already inquire about the value of one of their arguments, the extracted relations are more likely to be relevant for the respective instances and classes, than relations extracted from arbitrary document sentences (Fader et al., 2011). 2 Noncontiguous Attributes Intuitions: Users tend to formulate their Web search queries based on knowledge that they already possess at the time of the search (Pas¸ca, 2007). Therefore, search queries play two roles simultaneously: in addition to requesting new information, they indirectly convey knowledge in the process. In particular, attributes correspond to quantifiable properties of instances and their classes. The extraction of attributes from queries starts from the intuition that, if an attribute A is relevant for a class C, then users are likely to ask for the value of the attribute</context>
<context position="31086" citStr="Fader et al., 2011" startWordPosition="4852" endWordPosition="4855">he values being requested by the queries, on the other hand. Therefore, the method proposed in this paper can also be regarded as a method for the acquisition of relevant relations of various classes. The extracted relations specify the left argument (i.e., the instance) and the linking relation name (i.e., the attribute). They only specify the type of the, but not the actual, right argument (i.e., the value being requested). An additional experiment compares the accuracy of relations extracted as noncontiguous attributes from queries, vs. relations extracted by a previous open-domain method (Fader et al., 2011) from 500 million Web documents. The previous method, including its extraction patterns and its ranking scheme, is designed with instances rather than classes in mind. For fairness to the method in (Fader et al., 2011), the evaluation procedure is slightly adjusted. The set of instances associated with each target class, over which the two methods are evaluated, is reduced to a single representative instance selected a-priori. The instances are shown as the first instances in parentheses for each class in the earlier Table 2. Thus, the class attributes are extracted using only the instances ke</context>
<context position="34903" citStr="Fader et al., 2011" startWordPosition="5430" endWordPosition="5433">hak et al., 2009). In comparison, the loose typing of the values of our noncontiguous attributes is mostly coarse-grained. It relies on wh-prefixes (when, how long, where, how) and possibly subsequent words (what nutritional value) from the queries, to determine whether the values are expected to be a date/time, length/duration, location, manner, nutritional value etc. Relations extracted from document sentences (e.g., “Claude Monet was born in Paris”) are tuples of an instance (claude monet), a text fragment acting as the lexicalized relation (was born in), and another instance (paris) (cf. (Fader et al., 2011; Mausam et al., 2012)). For convenience, the relation and second instance may be concatenated, as in was born in paris for claude monet. But document sentences mentioning an instance do not necessarily refer to properties of the instance that people other than the author of the document are likely to inquire about. Consequently, even topranked extracted relations occasionally include less informative ones, such as comes into view for mount rainier, is on the table for madeira wine, or allows for features for javascript (Fader et al., 2011). Comparatively, relations extracted via noncontiguous</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP-11), pages 1535–1545, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<title>WordIet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordIet: An Electronic Lexical Database and Some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>F Suchanek</author>
<author>K Berberich</author>
<author>G Weikum</author>
</authors>
<title>YAGO2: a spatially and temporally enhanced knowledge base from Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence Journal. Special Issue on Artificial Intelligence, Wikipedia and SemiStructured Resources,</journal>
<pages>194--28</pages>
<contexts>
<context position="33122" citStr="Hoffart et al., 2013" startWordPosition="5163" endWordPosition="5166">lson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-domain information extraction, has been investigated in the tasks of attribute extraction (Pas¸ca, 2007; Pas¸ca, 2012), as well as in collecting sets of related instances (Jain and Pennacchiotti, 2010). To increase diversity within a</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. 2013. YAGO2: a spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence Journal. Special Issue on Artificial Intelligence, Wikipedia and SemiStructured Resources, 194:28–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>M Pennacchiotti</author>
</authors>
<title>Open entity extraction from Web search query logs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLIIG-10),</booktitle>
<pages>510--518</pages>
<location>Beijing, China.</location>
<contexts>
<context position="33690" citStr="Jain and Pennacchiotti, 2010" startWordPosition="5249" endWordPosition="5252">egory labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-domain information extraction, has been investigated in the tasks of attribute extraction (Pas¸ca, 2007; Pas¸ca, 2012), as well as in collecting sets of related instances (Jain and Pennacchiotti, 2010). To increase diversity within a ranked list of attributes, the extraction method in this paper employs a synonym vocabulary to approximately identify groups of near-duplicate attributes. As reported for previous methods, the resulting lists may still contain lexically different but semantically equivalent attributes. Scenarios where detecting all equivalent attributes is important may benefit from other techniques for paraphrase acquisition (Madnani and Dorr, 2010). Sophisticated techniques are sometimes employed to identify the type of the expected answers of open-domain questions (Pinchak e</context>
</contexts>
<marker>Jain, Pennacchiotti, 2010</marker>
<rawString>A. Jain and M. Pennacchiotti. 2010. Open entity extraction from Web search query logs. In Proceedings of the 23rd International Conference on Computational Linguistics (COLIIG-10), pages 510–518, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lao</author>
<author>T Mitchell</author>
<author>W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP-11),</booktitle>
<pages>529--539</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="32537" citStr="Lao et al., 2011" startWordPosition="5076" endWordPosition="5079"> arbitrary relations extracted from documents. Because fact-seeking queries inquire about the value of some relations (attributes) of an instance, the relations themselves tends to be more relevant than relations extracted from arbitrary document sentences. Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated wit</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP-11), pages 529–539, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: a survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="16794" citStr="Madnani and Dorr, 2010" startWordPosition="2669" endWordPosition="2672">ibute from the list, if all tokens from the lower-ranked attribute match either tokens from the higherranked attribute (gained independence vs. won its independence), or tokens from synonyms of phrases from the earlier attribute (gained independence vs. won its independence; or takes to show symptoms vs. takes to come out). Stop words, which include linking verbs, pronouns, determiners, conjunctions, wh-prefixes and prepositions, are not required to match. Synonyms may be either derived from existing lexical resources (e.g., WordNet (Fellbaum, 1998)), or mined from large document collections (Madnani and Dorr, 2010). Lower-ranked near-duplicate attributes are merged with the higher-ranked ones from the ranked list, thus improving the diversity of the list. 3 Experimental Setting Textual Data Sources: The experiments rely on a random sample of around 1 billion fullyanonymized queries in English, submitted to a general-purpose Web search engine. Each query is available independently from other queries, and is accompanied by its frequency of occurrence in the query logs. Target Classes: Table 2 shows the set of 40 target classes for evaluating the attributes extracted from queries. In an effort to reuse exp</context>
<context position="34160" citStr="Madnani and Dorr, 2010" startWordPosition="5317" endWordPosition="5320">ated in the tasks of attribute extraction (Pas¸ca, 2007; Pas¸ca, 2012), as well as in collecting sets of related instances (Jain and Pennacchiotti, 2010). To increase diversity within a ranked list of attributes, the extraction method in this paper employs a synonym vocabulary to approximately identify groups of near-duplicate attributes. As reported for previous methods, the resulting lists may still contain lexically different but semantically equivalent attributes. Scenarios where detecting all equivalent attributes is important may benefit from other techniques for paraphrase acquisition (Madnani and Dorr, 2010). Sophisticated techniques are sometimes employed to identify the type of the expected answers of open-domain questions (Pinchak et al., 2009). In comparison, the loose typing of the values of our noncontiguous attributes is mostly coarse-grained. It relies on wh-prefixes (when, how long, where, how) and possibly subsequent words (what nutritional value) from the queries, to determine whether the values are expected to be a date/time, length/duration, location, manner, nutritional value etc. Relations extracted from document sentences (e.g., “Claude Monet was born in Paris”) are tuples of an i</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>N. Madnani and B. Dorr. 2010. Generating phrasal and sentential paraphrases: a survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmitz Mausam</author>
<author>S Soderland</author>
<author>R Bart</author>
<author>O Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning (EMILP-CoILL-12),</booktitle>
<pages>523--534</pages>
<location>Jeju Island,</location>
<contexts>
<context position="34925" citStr="Mausam et al., 2012" startWordPosition="5434" endWordPosition="5437">n comparison, the loose typing of the values of our noncontiguous attributes is mostly coarse-grained. It relies on wh-prefixes (when, how long, where, how) and possibly subsequent words (what nutritional value) from the queries, to determine whether the values are expected to be a date/time, length/duration, location, manner, nutritional value etc. Relations extracted from document sentences (e.g., “Claude Monet was born in Paris”) are tuples of an instance (claude monet), a text fragment acting as the lexicalized relation (was born in), and another instance (paris) (cf. (Fader et al., 2011; Mausam et al., 2012)). For convenience, the relation and second instance may be concatenated, as in was born in paris for claude monet. But document sentences mentioning an instance do not necessarily refer to properties of the instance that people other than the author of the document are likely to inquire about. Consequently, even topranked extracted relations occasionally include less informative ones, such as comes into view for mount rainier, is on the table for madeira wine, or allows for features for javascript (Fader et al., 2011). Comparatively, relations extracted via noncontiguous attributes from queri</context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning (EMILP-CoILL-12), pages 523–534, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>M Strube</author>
</authors>
<title>Decoding Wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd Iational Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1219--1224</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="33099" citStr="Nastase and Strube, 2008" startWordPosition="5159" endWordPosition="5162">ons (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-domain information extraction, has been investigated in the tasks of attribute extraction (Pas¸ca, 2007; Pas¸ca, 2012), as well as in collecting sets of related instances (Jain and Pennacchiotti, 2010). To incr</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>V. Nastase and M. Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In Proceedings of the 23rd Iational Conference on Artificial Intelligence (AAAI-08), pages 1219–1224, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>B Van Durme</author>
<author>N Garera</author>
</authors>
<title>The role of documents vs. queries in extracting class attributes from text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on Information and Knowledge Management (CIKM-07),</booktitle>
<pages>485--494</pages>
<location>Lisbon, Portugal.</location>
<marker>Pas¸ca, Van Durme, Garera, 2007</marker>
<rawString>M. Pas¸ca, B. Van Durme, and N. Garera. 2007. The role of documents vs. queries in extracting class attributes from text. In Proceedings of the 16th International Conference on Information and Knowledge Management (CIKM-07), pages 485–494, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th World Wide Web Conference (WWW-07),</booktitle>
<pages>101--110</pages>
<location>Banff, Canada.</location>
<marker>Pas¸ca, 2007</marker>
<rawString>M. Pas¸ca. 2007. Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds. In Proceedings of the 16th World Wide Web Conference (WWW-07), pages 101–110, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Attribute extraction from conjectural queries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLIIG-12),</booktitle>
<location>Mumbai, India.</location>
<marker>Pas¸ca, 2012</marker>
<rawString>M. Pas¸ca. 2012. Attribute extraction from conjectural queries. In Proceedings of the 24th International Conference on Computational Linguistics (COLIIG-12), Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>T Lin</author>
<author>M Gamon</author>
</authors>
<title>Mining entity types from query logs via user intent modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-12),</booktitle>
<pages>563--571</pages>
<location>Jeju Island,</location>
<contexts>
<context position="22799" citStr="Pantel et al., 2012" startWordPosition="3537" endWordPosition="3540">he extraction patterns from Table 1 are syntactically parsed (Petrov et al., 2010). As a prerequisite, the portion I of the patterns from the table must match a disambiguated instance from a query. A variation of the tagger introduced in (Cucerzan, 2007) maps query fragments to their disambiguated, corresponding Wikipedia instances (i.e., to Wikipedia articles). The tagger is simplified to select the longest instance mentions, and does not use gazetteers or queries for training. Depending on the sources of textual data available for training, any taggers (Cucerzan, 2007; Ratinov et al., 2011; Pantel et al., 2012) that disambiguate text fragments relative to Wikipedia entries can be employed. 4 Evaluation Results Attribute Accuracy: The top 50 attributes, from the ranked lists extracted for each target class, are manually assigned correctness labels. As shown in Table 3, an attribute is marked as vital, if it must be present among representative attributes of the 390 Class Precision of Extracted Attributes %vital %okay %wrong Score Awards 29 14 7 0.72 Chemical elements 46 2 2 0.94 Companies 42 1 7 0.85 Food ingredients 31 9 10 0.71 Programming languages 31 7 12 0.69 Stadiums 42 5 3 0.89 Video games 33 </context>
</contexts>
<marker>Pantel, Lin, Gamon, 2012</marker>
<rawString>P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity types from query logs via user intent modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-12), pages 563–571, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>P Chang</author>
<author>M Ringgaard</author>
<author>H Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Iatural Language Processing (EMILP-10),</booktitle>
<pages>705--713</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="22261" citStr="Petrov et al., 2010" startWordPosition="3452" endWordPosition="3455">41 (for National Basketball Association teams) and 66,934 (for Films) instances, with an average of 10,730 instances per class. Synonym Repository: A synonym repository extracted separately from Web documents contains mappings from each of around 60,000 phrases in English, to lists of their synonym phrases. For example, the top synonyms available for the phrases turn off and contagious are [switch off, extinguish, turn out, ..] and [infectious, catching, communicable, ..] respectively. Parameter Settings: Queries that match any of the extraction patterns from Table 1 are syntactically parsed (Petrov et al., 2010). As a prerequisite, the portion I of the patterns from the table must match a disambiguated instance from a query. A variation of the tagger introduced in (Cucerzan, 2007) maps query fragments to their disambiguated, corresponding Wikipedia instances (i.e., to Wikipedia articles). The tagger is simplified to select the longest instance mentions, and does not use gazetteers or queries for training. Depending on the sources of textual data available for training, any taggers (Cucerzan, 2007; Ratinov et al., 2011; Pantel et al., 2012) that disambiguate text fragments relative to Wikipedia entrie</context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Iatural Language Processing (EMILP-10), pages 705–713, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pinchak</author>
<author>D Lin</author>
<author>D Rafiei</author>
</authors>
<title>Flexible answer typing with discriminative preference ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL09),</booktitle>
<pages>666--674</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="34302" citStr="Pinchak et al., 2009" startWordPosition="5339" endWordPosition="5342">ti, 2010). To increase diversity within a ranked list of attributes, the extraction method in this paper employs a synonym vocabulary to approximately identify groups of near-duplicate attributes. As reported for previous methods, the resulting lists may still contain lexically different but semantically equivalent attributes. Scenarios where detecting all equivalent attributes is important may benefit from other techniques for paraphrase acquisition (Madnani and Dorr, 2010). Sophisticated techniques are sometimes employed to identify the type of the expected answers of open-domain questions (Pinchak et al., 2009). In comparison, the loose typing of the values of our noncontiguous attributes is mostly coarse-grained. It relies on wh-prefixes (when, how long, where, how) and possibly subsequent words (what nutritional value) from the queries, to determine whether the values are expected to be a date/time, length/duration, location, manner, nutritional value etc. Relations extracted from document sentences (e.g., “Claude Monet was born in Paris”) are tuples of an instance (claude monet), a text fragment acting as the lexicalized relation (was born in), and another instance (paris) (cf. (Fader et al., 201</context>
</contexts>
<marker>Pinchak, Lin, Rafiei, 2009</marker>
<rawString>C. Pinchak, D. Lin, and D. Rafiei. 2009. Flexible answer typing with discriminative preference ranking. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL09), pages 666–674, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raju</author>
<author>P Pingali</author>
<author>V Varma</author>
</authors>
<title>An unsupervised approach to product attribute extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st International Conference on Research and Development in Information Retrieval (SIGIR-08),</booktitle>
<pages>35--42</pages>
<contexts>
<context position="32792" citStr="Raju et al., 2008" startWordPosition="5114" endWordPosition="5117">es. Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong</context>
</contexts>
<marker>Raju, Pingali, Varma, 2008</marker>
<rawString>S. Raju, P. Pingali, and V. Varma. 2008. An unsupervised approach to product attribute extraction. In Proceedings of the 31st International Conference on Research and Development in Information Retrieval (SIGIR-08), pages 35–42, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11),</booktitle>
<pages>1375--1384</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="22777" citStr="Ratinov et al., 2011" startWordPosition="3533" endWordPosition="3536">es that match any of the extraction patterns from Table 1 are syntactically parsed (Petrov et al., 2010). As a prerequisite, the portion I of the patterns from the table must match a disambiguated instance from a query. A variation of the tagger introduced in (Cucerzan, 2007) maps query fragments to their disambiguated, corresponding Wikipedia instances (i.e., to Wikipedia articles). The tagger is simplified to select the longest instance mentions, and does not use gazetteers or queries for training. Depending on the sources of textual data available for training, any taggers (Cucerzan, 2007; Ratinov et al., 2011; Pantel et al., 2012) that disambiguate text fragments relative to Wikipedia entries can be employed. 4 Evaluation Results Attribute Accuracy: The top 50 attributes, from the ranked lists extracted for each target class, are manually assigned correctness labels. As shown in Table 3, an attribute is marked as vital, if it must be present among representative attributes of the 390 Class Precision of Extracted Attributes %vital %okay %wrong Score Awards 29 14 7 0.72 Chemical elements 46 2 2 0.94 Companies 42 1 7 0.85 Food ingredients 31 9 10 0.71 Programming languages 31 7 12 0.69 Stadiums 42 5 </context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11), pages 1375–1384, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Remy</author>
</authors>
<title>Wikipedia: The free encyclopedia.</title>
<date>2002</date>
<journal>Online Information Review,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="893" citStr="Remy, 2002" startWordPosition="123" endWordPosition="124">cuments or search queries assume that relevant attributes occur verbatim in the source text. The extracted attributes are short phrases that correspond to quantifiable properties of various instances (ottoman empire, roman empire, mughal empire) of the class. This paper explores the extraction of noncontiguous class attributes (manner (it) claimed legitimacy of rule), from factseeking and explanation-seeking queries. The attributes cover properties that are not always likely to be extracted as short phrases from inherently-noisy queries. 1 Introduction Motivation: Resources such as Wikipedia (Remy, 2002) and Freebase (Bollacker et al., 2008) aim at organizing knowledge around classes (Food ingredients, Astronomical objects, Religions) and their instances (wheat flower, uranus, hinduism). Due to inherent limitations associated with maintaining and expanding human-curated resources, their content may be incomplete. For example, attributes representing the energy (or energy per 100g) or solubility in water are available in both Wikipedia and Freebase for many instances of Food ingredients (e.g., for olive oil, honey, fennel). But the attributes are missing for some instances (e.g., cornmeal). Mo</context>
</contexts>
<marker>Remy, 2002</marker>
<rawString>M. Remy. 2002. Wikipedia: The free encyclopedia. Online Information Review, 26(6):434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
<author>F Pereira</author>
</authors>
<title>Experiments in graph-based semi-supervised learning methods for class-instance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1473--1481</pages>
<location>Uppsala,</location>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>P. Talukdar and F. Pereira. 2010. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1473–1481, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tokunaga</author>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Automatic discovery of attribute words from Web documents.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Iatural Language Processing (IJCILP-05),</booktitle>
<pages>106--118</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5886" citStr="Tokunaga et al., 2005" startWordPosition="890" endWordPosition="893">h queries could take the form “what is the (nutritional value)A of (olive oil)I” and “what is the (diameter)A of (jupiter)I”; or the more compact “(nutritional value)A of (olive oil)I” and “(diameter)A of (jupiter)I”. In this case, the attributes are relatively short phrases (nutritional value, diameter), and are expected to appear as contiguous phrases within queries. Previous methods on attribute extraction from queries specifically target this type of attributes. In fact, some methods apply dedicated extraction patterns (e.g., A of I) over either queries (Pas¸ca et al., 2007) or documents (Tokunaga et al., 2005). Other methods expand manually-provided seed sets of attributes, with other phrases that co-occur with instances within queries, in similar contexts as the seed attributes do (Pas¸ca, 2007). While simpler properties are often mentioned in queries as short, contiguous phrases, finer-grained properties often are not. Queries seeking the reason for solidification for some Food ingredients could, but rarely do, contain the attribute verbatim (“what is the reason for the solidification of honey”). Instead, queries are more likely to inquire about the expected value, while specifying the instance a</context>
<context position="8140" citStr="Tokunaga et al., 2005" startWordPosition="1253" endWordPosition="1256">n a form that suggests the queries ask for the value of a noncontiguous attribute of the instance; • extraction of noncontiguous attributes, from query fragments that describe the property of interest and the type of its expected value; • aggregation and ranking of attributes of individual instances of a class, into attributes of a class. Extraction Patterns: In order to determine whether a query contains an attribute of a class, the query is matched against the extraction patterns from Table 1. The use of patterns in attribute extraction has been previously suggested in (Pas¸ca et al., 2007; Tokunaga et al., 2005), where the pattern what is the A of I extracts noun-phrase A attributes of instances I from queries and documents. In our case, the patterns are constructed such that they match fact-seeking and explanationseeking questions that likely inquire about the value of a relevant property of an instance I of the class C. For example, the first pattern from Table 1 matches queries such as “when did everquest become free to play” and “when was radon discovered as an element”, which inquire about the date or time when certain events affected certain properties of the instances everquest and radon respe</context>
<context position="32738" citStr="Tokunaga et al., 2005" startWordPosition="5105" endWordPosition="5108"> than relations extracted from arbitrary document sentences. Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from </context>
</contexts>
<marker>Tokunaga, Kazama, Torisawa, 2005</marker>
<rawString>K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Automatic discovery of attribute words from Web documents. In Proceedings of the 2nd International Joint Conference on Iatural Language Processing (IJCILP-05), pages 106– 118, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>T Qian</author>
<author>L Schubert</author>
</authors>
<title>Classdriven attribute extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLIIG-08),</booktitle>
<pages>921--928</pages>
<location>Manchester, United Kingdom.</location>
<marker>Van Durme, Qian, Schubert, 2008</marker>
<rawString>B. Van Durme, T. Qian, and L. Schubert. 2008. Classdriven attribute extraction. In Proceedings of the 22nd International Conference on Computational Linguistics (COLIIG-08), pages 921–928, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wong</author>
<author>W Lam</author>
<author>T Wong</author>
</authors>
<title>An unsupervised framework for extracting and normalizing product attributes from multiple Web sites.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st International Conference on Research and Development in Information Retrieval (SIGIR-08),</booktitle>
<pages>35--42</pages>
<contexts>
<context position="32892" citStr="Wong et al., 2008" startWordPosition="5129" endWordPosition="5132">placement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-dom</context>
</contexts>
<marker>Wong, Lam, Wong, 2008</marker>
<rawString>T. Wong, W. Lam, and T. Wong. 2008. An unsupervised framework for extracting and normalizing product attributes from multiple Web sites. In Proceedings of the 31st International Conference on Research and Development in Information Retrieval (SIGIR-08), pages 35–42, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>118--127</pages>
<location>Uppsala,</location>
<contexts>
<context position="32995" citStr="Wu and Weld, 2010" startWordPosition="5144" endWordPosition="5147">tter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative textual data source to Web documents in open-domain information extraction, has been investigated in the tasks of attribute extraction (Pas¸ca, 2007; P</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>F. Wu and D. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 118–127, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Yoshinaga</author>
<author>K Torisawa</author>
</authors>
<title>Open-domain attribute-value acquisition from semi-structured texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference (ISWC-07), Workshop on Text to Knowledge: The Lexicon/Ontology Interface (OntoLex-2007),</booktitle>
<pages>55--66</pages>
<location>Busan, South</location>
<contexts>
<context position="32848" citStr="Yoshinaga and Torisawa, 2007" startWordPosition="5121" endWordPosition="5124">ies likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2008; Hoffart et al., 2013) associated with Wikipedia articles. In order to acquire class attributes, a common strategy is to first acquire attributes of instances, then aggregate or propagate (Talukdar and Pereira, 392 2010) attributes, from instances to the classes to which the instances belong. The role of Web search queries, as an alternative text</context>
</contexts>
<marker>Yoshinaga, Torisawa, 2007</marker>
<rawString>N. Yoshinaga and K. Torisawa. 2007. Open-domain attribute-value acquisition from semi-structured texts. In Proceedings of the 6th International Semantic Web Conference (ISWC-07), Workshop on Text to Knowledge: The Lexicon/Ontology Interface (OntoLex-2007), pages 55– 66, Busan, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>Z Nie</author>
<author>X Liu</author>
<author>B Zhang</author>
<author>J Wen</author>
</authors>
<title>StatSnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th World Wide Web Conference (WWW-09),</booktitle>
<pages>101--110</pages>
<location>Madrid,</location>
<contexts>
<context position="32496" citStr="Zhu et al., 2009" startWordPosition="5068" endWordPosition="5071"> to capture higherquality relations than arbitrary relations extracted from documents. Because fact-seeking queries inquire about the value of some relations (attributes) of an instance, the relations themselves tends to be more relevant than relations extracted from arbitrary document sentences. Nevertheless, relations derived from queries likely serve as a useful complement, rather than replacement, of relations from documents. The former only discover what relations may be relevant; the latter also identify their occurrences within text. 5 Related Work Sources of text from which relations (Zhu et al., 2009; Carlson et al., 2010; Lao et al., 2011) and, more specifically, attributes can be extracted include Web documents and data in human-compiled encyclopedia. In Web documents, attributes are available within unstructured (Tokunaga et al., 2005; Pas¸ca et al., 2007), structured (Raju et al., 2008) and semi-structured text (Yoshinaga and Torisawa, 2007), layout formatting tags (Wong et al., 2008), itemized lists or tables (Cafarella et al., 2008). In human-compiled encyclopedia (Wu and Weld, 2010), data relevant to attribute extraction includes infoboxes and category labels (Nastase and Strube, 2</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of the 18th World Wide Web Conference (WWW-09), pages 101–110, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>