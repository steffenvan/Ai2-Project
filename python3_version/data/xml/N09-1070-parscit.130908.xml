<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001588">
<title confidence="0.99917">
Unsupervised Approaches for Automatic Keyword Extraction Using
Meeting Transcripts
</title>
<author confidence="0.999297">
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu
</author>
<affiliation confidence="0.999589">
Computer Science Department
The University of Texas at Dallas
</affiliation>
<address confidence="0.730904">
Richardson, TX 75080, USA
</address>
<email confidence="0.999702">
{ffliu,deana,feiliu,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999630083333333">
This paper explores several unsupervised ap-
proaches to automatic keyword extraction
using meeting transcripts. In the TFIDF
(term frequency, inverse document frequency)
weighting framework, we incorporated part-
of-speech (POS) information, word clustering,
and sentence salience score. We also evalu-
ated a graph-based approach that measures the
importance of a word based on its connection
with other sentences or words. The system
performance is evaluated in different ways, in-
cluding comparison to human annotated key-
words using F-measure and a weighted score
relative to the oracle system performance, as
well as a novel alternative human evaluation.
Our results have shown that the simple un-
supervised TFIDF approach performs reason-
ably well, and the additional information from
POS and sentence score helps keyword ex-
traction. However, the graph method is less
effective for this domain. Experiments were
also performed using speech recognition out-
put and we observed degradation and different
patterns compared to human transcripts.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886186046512">
Keywords in a document provide important infor-
mation about the content of the document. They
can help users search through information more effi-
ciently or decide whether to read a document. They
can also be used for a variety of language process-
ing tasks such as text categorization and informa-
tion retrieval. However, most documents do not
provide keywords. This is especially true for spo-
ken documents. Current speech recognition system
performance has improved significantly, but there
is no rich structural information such as topics and
keywords in the transcriptions. Therefore, there is
a need to automatically generate keywords for the
large amount of written or spoken documents avail-
able now.
There have been many efforts toward keyword ex-
traction for text domain. In contrast, there is less
work on speech transcripts. In this paper we fo-
cus on one speech genre — the multiparty meeting
domain. Meeting speech is significantly different
from written text and most other speech data. For
example, there are typically multiple participants
in a meeting, the discussion is not well organized,
and the speech is spontaneous and contains disflu-
encies and ill-formed sentences. It is thus ques-
tionable whether we can adopt approaches that have
been shown before to perform well in written text
for automatic keyword extraction in meeting tran-
scripts. In this paper, we evaluate several differ-
ent keyword extraction algorithms using the tran-
scripts of the ICSI meeting corpus. Starting from
the simple TFIDF baseline, we introduce knowl-
edge sources based on POS filtering, word cluster-
ing, and sentence salience score. In addition, we
also investigate a graph-based algorithm in order to
leverage more global information and reinforcement
from summary sentences. We used different per-
formance measurements: comparing to human an-
notated keywords using individual F-measures and
a weighted score relative to the oracle system per-
formance, and conducting novel human evaluation.
Experiments were conducted using both the human
transcripts and the speech recognition (ASR) out-
</bodyText>
<page confidence="0.967169">
620
</page>
<note confidence="0.890489">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620–628,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.998740166666667">
put. Overall the TFIDF based framework seems to
work well for this domain, and the additional knowl-
edge sources help improve system performance. The
graph-based approach yielded worse results, espe-
cially for the ASR condition, suggesting further in-
vestigation for this task.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999965">
TFIDF weighting has been widely used for keyword
or key phrase extraction. The idea is to identify
words that appear frequently in a document, but do
not occur frequently in the entire document collec-
tion. Much work has shown that TFIDF is very ef-
fective in extracting keywords for scientific journals,
e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al.,
2005). However, we may not have a big background
collection that matches the test domain for a reli-
able IDF estimate. (Matsuo and Ishizuka, 2004) pro-
posed a co-occurrence distribution based method us-
ing a clustering strategy for extracting keywords for
a single document without relying on a large corpus,
and reported promising results.
Web information has also been used as an ad-
ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first and
then determined whether to add another keyword hy-
pothesis based on its PMI (point-wise mutual infor-
mation) score to the current selected keywords. The
preselected keywords can be generated using basic
extraction algorithms such as TFIDF. It is impor-
tant to ensure the quality of the first selection for the
subsequent addition of keywords. Other researchers
also used PMI scores between each pair of candidate
keywords to select the top k% of words that have
the highest average PMI scores as the final keywords
(Inkpen and Desilets, 2004).
Keyword extraction has also been treated as a
classification task and solved using supervised ma-
chine learning approaches (Frank et al., 1999; Tur-
ney, 2000; Kerner et al., 2005; Turney, 2002; Tur-
ney, 2003). In these approaches, the learning al-
gorithm needs to learn to classify candidate words
in the documents into positive or negative examples
using a set of features. Useful features for this ap-
proach include TFIDF and its variations, position of
a phrase, POS information, and relative length of a
phrase (Turney, 2000). Some of these features may
not work well for meeting transcripts. For exam-
ple, the position of a phrase (measured by the num-
ber of words before its first appearance divided by
the document length) is very useful for news article
text, since keywords often appear early in the doc-
ument (e.g., in the first paragraph). However, for
the less well structured meeting domain (lack of ti-
tle and paragraph), these kinds of features may not
be indicative. A supervised approach to keyword ex-
traction was used in (Liu et al., 2008). Even though
the data set in that study is not very big, it seems that
a supervised learning approach can achieve reason-
able performance for this task.
Another line of research for keyword extrac-
tion has adopted graph-based methods similar to
Google’s PageRank algorithm (Brin and Page,
1998). In particular, (Wan et al., 2007) attempted
to use a reinforcement approach to do keyword ex-
traction and summarization simultaneously, on the
assumption that important sentences usually contain
keywords and keywords are usually seen in impor-
tant sentences. We also find that this assumption also
holds using statistics obtained from the meeting cor-
pus used in this study. Graph-based methods have
not been used in a genre like the meeting domain;
therefore, it remains to be seen whether these ap-
proaches can be applied to meetings.
Not many studies have been performed on speech
transcripts for keyword extraction. The most rel-
evant work to our study is (Plas et al., 2004),
where the task is keyword extraction in the mul-
tiparty meeting corpus. They showed that lever-
aging semantic resources can yield significant per-
formance improvement compared to the approach
based on the relative frequency ratio (similar to
IDF). There is also some work using keywords for
other speech processing tasks, e.g., (Munteanu et
al., 2007; Bulyko et al., 2007; Wu et al., 2007; De-
silets et al., 2002; Rogina, 2002). (Wu et al., 2007)
showed that keyword extraction combined with se-
mantic verification can be used to improve speech
retrieval performance on broadcast news data. In
(Rogina, 2002), keywords were extracted from lec-
ture slides, and then used as queries to retrieve rel-
evant web documents, resulting in an improved lan-
guage model and better speech recognition perfor-
mance of lectures. There are many differences be-
tween written text and speech — meetings in par-
ticular. Thus our goal in this paper is to investi-
</bodyText>
<page confidence="0.997756">
621
</page>
<bodyText confidence="0.99994">
gate whether we can successfully apply some exist-
ing techniques, as well as propose new approaches
to extract keywords for the meeting domain. The
aim of this study is to set up some starting points for
research in this area.
</bodyText>
<sectionHeader confidence="0.996192" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999067137931034">
We used the meetings from the ICSI meeting data
(Janin et al., 2003), which are recordings of naturally
occurring meetings. All the meetings have been
transcribed and annotated with dialog acts (DA)
(Shriberg et al., 2004), topics, and extractive sum-
maries (Murray et al., 2005). The ASR output for
this corpus is obtained from a state-of-the-art SRI
conversational telephone speech system (Zhu et al.,
2005), with a word error rate of about 38.2% on
the entire corpus. We align the human transcripts
and ASR output, then map the human annotated DA
boundaries and topic boundaries to the ASR words,
such that we have human annotation of these infor-
mation for the ASR output.
We recruited three Computer Science undergradu-
ate students to annotate keywords for each topic seg-
ment, using 27 selected ICSI meetings.1 Up to five
indicative key words or phrases were annotated for
each topic. In total, we have 208 topics annotated
with keywords. The average length of the topics
(measured using the number of dialog acts) among
all the meetings is 172.5, with a high standard devi-
ation of 236.8. We used six meetings as our devel-
opment set (the same six meetings as the test set in
(Murray et al., 2005)) to optimize our keyword ex-
traction methods, and the remaining 21 meetings for
final testing in Section 5.
One example of the annotated keywords for a
topic segment is:
</bodyText>
<listItem confidence="0.998550833333333">
• Annotator I: analysis, constraints, template
matcher;
• Annotator II: syntactic analysis, parser, pattern
matcher, finite-state transducers;
• Annotator III: lexicon, set processing, chunk
parser.
</listItem>
<bodyText confidence="0.996516">
Note that these meetings are research discussions,
and that the annotators may not be very familiar with
</bodyText>
<footnote confidence="0.783876333333333">
1We selected these 27 meetings because they have been used
in previous work for topic segmentation and summarization
(Galley et al., 2003; Murray et al., 2005).
</footnote>
<bodyText confidence="0.999978105263158">
the topics discussed and often had trouble deciding
the important sentences or keywords. In addition,
limiting the number of keywords that an annotator
can select for a topic also created some difficulty.
Sometimes there are more possible keywords and
the annotators felt it is hard to decide which five are
the most topic indicative. Among the three annota-
tors, we notice that in general the quality of anno-
tator I is the poorest. This is based on the authors’
judgment, and is also confirmed later by an indepen-
dent human evaluation (in Section 6).
For a better understanding of the gold standard
used in this study and the task itself, we thoroughly
analyzed the human annotation consistency. We re-
moved the topics labeled with “chitchat” by at least
one annotator, and also the digit recording part in
the ICSI data, and used the remaining 140 topic seg-
ments. We calculated the percentage of keywords
agreed upon by different annotators for each topic,
as well as the average for all the meetings. All of the
consistency analysis is performed based on words.
Figure 1 illustrates the annotation consistency over
different meetings and topics. The average consis-
tency rate across topics is 22.76% and 5.97% among
any two and all three annotators respectively. This
suggests that people do not have a high agreement
on keywords for a given document. We also notice
that the two person agreement is up to 40% for sev-
eral meetings and 80% for several individual top-
ics, and the agreement among all three annotators
reaches 20% and 40% for some meetings or topics.
This implies that the consistency depends on topics
(e.g., the difficulty or ambiguity of a topic itself, the
annotators’ knowledge of that topic). Further studies
are needed for the possible factors affecting human
agreement. We are currently creating more annota-
tions for this data set for better agreement measure
and also high quality annotation.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999678">
Our task is to extract keywords for each of the topic
segments in each meeting transcript. Therefore, by
“document”, we mean a topic segment in the re-
mainder of this paper. Note that our task is different
from keyword spotting, where a keyword is provided
and the task is to spot it in the audio (along with its
transcript).
The core part of keyword extraction is for the sys-
</bodyText>
<page confidence="0.99572">
622
</page>
<figureCaption confidence="0.99924875">
Figure 1: Human annotation consistency across differ-
ent topics (upper graph) and meetings (lower graph). Y-
axis is the percent of the keywords agreed upon by two or
three annotators.
</figureCaption>
<bodyText confidence="0.993305869565217">
tem to assign an importance score to a word, and
then pick the top ranked words as keywords. We
compare different methods for weight calculation in
this study, broadly divided into the following two
categories: the TFIDF framework and the graph-
based model. Both are unsupervised learning meth-
ods.2 In all of the following approaches, when se-
lecting the final keywords, we filter out any words
appearing on the stopword list. These stopwords are
generated based on the IDF values of the words us-
ing all the meeting data by treating each topic seg-
ment as a document. The top 250 words from this
list (with the lowest IDF values) were used as stop-
words. We generated two different stopword lists for
human transcripts and ASR output respectively. In
addition, in this paper we focus on performing key-
word extraction at the single word level, therefore
no key phrases are generated.
2Note that by unsupervised methods, we mean that no data
annotated with keywords is needed. These methods do require
the use of some data to generate information such as IDF, or
possibly a development set to optimize some parameters or
heuristic rules.
</bodyText>
<subsectionHeader confidence="0.845814">
4.1 TFIDF Framework
</subsectionHeader>
<bodyText confidence="0.854054">
(A) Basic TFIDF weighting
The term frequency (TF) for a word wi in a doc-
ument is the number of times the word occurs in the
document. The IDF value is:
</bodyText>
<equation confidence="0.928346">
IDFi = log(N/Ni)
</equation>
<bodyText confidence="0.9995744">
where Ni denotes the number of the documents con-
taining word wi, and N is the total number of the
documents in the collection. We also performed L2
normalization for the IDF values when combining
them with other scores.
</bodyText>
<subsectionHeader confidence="0.669672">
(B) Part of Speech (POS) filtering
</subsectionHeader>
<bodyText confidence="0.9969193">
In addition to using a stopword list to remove
words from consideration, we also leverage POS in-
formation to filter unlikely keywords. Our hypothe-
sis is that verb, noun and adjective words are more
likely to be keywords, so we restrict our selection to
words with these POS tags only. We used the TnT
POS tagger (Brants, 2000) trained from the Switch-
board data to tag the meeting transcripts.
(C) Integrating word clustering
One weakness of the baseline TFIDF is that it
counts the frequency for a particular word, without
considering any words that are similar to it in terms
of semantic meaning. In addition, when the docu-
ment is short, the TF may not be a reliable indicator
of the importance of the word. Our idea is therefore
to account for the frequency of other similar words
when calculating the TF of a word in the document.
For this, we group all the words into clusters in an
unsupervised fashion. If the total term frequency
of all the words in one cluster is high, it is likely
that this cluster contributes more to the current topic
from a thematic point of view. Thus we want to as-
sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for
automatic word clustering over the entire docu-
ment collection. It minimizes the perplexity of the
induced class-based n-gram language model com-
pared to the original word-based model. Using the
clusters, we then adjust the TF weighting by inte-
grating with the cluster term frequency (CTF):
</bodyText>
<equation confidence="0.476533">
TF CTF(wi) = TF(wi)∗α(Pwl∈Cz,wl�wz freq(wl))
</equation>
<bodyText confidence="0.999474666666667">
where the last summation component means the to-
tal term frequency of all the other words in this docu-
ment that belong to the same cluster Ci as the current
</bodyText>
<figure confidence="0.991521277777778">
0.8
0.6
0.4
0.2
0.5
0.4
0.3
0.2
0.1
0
1
0
0 30 60 90 120
1 3 5 7 9 11 13 15 17 19 21 23 25 27
3 agree
2 agree
3 agree
2 agree
</figure>
<page confidence="0.972191">
623
</page>
<bodyText confidence="0.791982">
word wi. We set parameter α to be slightly larger
than 1. We did not include stopwords when adding
the term frequencies for the words in a cluster.
</bodyText>
<subsectionHeader confidence="0.839326">
(D) Combining with sentence salience score
</subsectionHeader>
<bodyText confidence="0.9999225">
Intuitively, the words in an important sentence
should be assigned a high weight for keyword ex-
traction. In order to leverage the sentence infor-
mation, we adjust a word’s weight by the salience
scores of the sentences containing that word. The
sentence score is calculated based on its cosine sim-
ilarity to the entire meeting. This score is often used
in extractive summarization to select summary sen-
tences (Radev et al., 2001). The cosine similarity
between two vectors, D1 and D2, is defined as:
</bodyText>
<equation confidence="0.82328">
sim(D1, D2) = Ei t1it2i
</equation>
<bodyText confidence="0.9998065">
where ti is the term weight for a word wi, for which
we use the TFIDF value.
</bodyText>
<sectionHeader confidence="0.792504" genericHeader="method">
4.2 Graph-based Methods
</sectionHeader>
<bodyText confidence="0.9999298">
For the graph-based approach, we adopt the itera-
tive reinforcement approach from (Wan et al., 2007)
in the hope of leveraging sentence information for
keyword extraction. This algorithm is based on the
assumption that important sentences/words are con-
nected to other important sentences/words.
Four graphs are created: one graph in which sen-
tences are connected to other sentences (S-S graph),
one in which words are connected to other words
(W-W graph), and two graphs connecting words to
sentences with uni-directional edges (W-S and S-W
graphs). Stopwords are removed before the creation
of the graphs so they will be ineligible to be key-
words.
The final weight for a word node depends on its
connection to other words (W-W graph) and other
sentences (W-S graph); similarly, the weight for
a sentence node is dependent on its connection to
other sentences (S-S graph) and other words (S-W
graph). That is,
</bodyText>
<equation confidence="0.99634">
u = αUTu + Q WˆTv
v = αV T v + QWTu
</equation>
<bodyText confidence="0.988314142857143">
where u and v are the weight vectors for sentence
and word nodes respectively, U, V, W, Wˆ represent
the S-S, W-W, S-W, and W-S connections. α and Q
specify the contributions from the homogeneous and
the heterogeneous nodes. The initial weight is a uni-
form one for the word and sentence vector. Then
the iterative reinforcement algorithm is used until
the node weight values converge (the difference be-
tween scores at two iterations is below 0.0001 for all
nodes) or 5,000 iterations are reached.
We have explored various ways to assign weights
to the edges in the graphs. Based on the results on
the development set, we use the following setup in
this paper:
</bodyText>
<listItem confidence="0.899004263157895">
• W-W Graph: We used a diagonal matrix for
the graph connection, i.e., there is no connec-
tion among words. The self-loop values are
the TFIDF values of the words. This is also
equivalent to using an identity matrix for the
word-word connection and TFIDF as the initial
weight for each vertex in the graph. We investi-
gated other strategies to assign a weight for the
edge between two word nodes; however, so far
the best result we obtained is using this diago-
nal matrix.
• S-W and W-S Graphs: The weight for an
edge between a sentence and a word is the TF
of the word in the sentence multiplied by the
word’s IDF value. These weights are initially
added only to the S-W graph, as in (Wan et al.,
2007); then that graph is normalized and trans-
posed to create the W-S graph.
• S-S Graph: The sentence node uses a vector
</listItem>
<bodyText confidence="0.878468875">
space model and is composed of the weights of
those words connected to this sentence in the
S-W graph. We then use cosine similarity be-
tween two sentence vectors.
Similar to the above TFIDF framework, we also
use POS filtering for the graph-based approach. Af-
ter the weights for all the words are determined, we
select the top ranked words with the POS restriction.
</bodyText>
<sectionHeader confidence="0.996844" genericHeader="method">
5 Experimental Results: Automatic
Evaluation
</sectionHeader>
<bodyText confidence="0.99953725">
Using the approaches described above, we com-
puted weights for the words and then picked the top
five words as the keywords for a topic. We chose five
keywords since this is the number of keywords that
</bodyText>
<equation confidence="0.9983785">
✓E ✓E
i t2 1i × i t22i
</equation>
<page confidence="0.992175">
624
</page>
<bodyText confidence="0.999970392156863">
human annotators used as a guideline, and it also
yielded good performance in the development set.
To evaluate system performance, in this section we
use human annotated keywords as references, and
compare the system output to them. The first metric
we use is F-measure, which has been widely used
for this task and other detection tasks. We compare
the system output with respect to each human anno-
tation, and calculate the maximum and the average
F-scores. Note that our keyword evaluation is word-
based. When human annotators choose key phrases
(containing more than one word), we split them into
words and measure the matching words. Therefore,
when the system only generates five keywords, the
upper bound of the recall rate may not be 100%. In
(Liu et al., 2008), a lenient metric is used which ac-
counts for some inflection of words. Since that is
highly correlated with the results using exact word
match, we report results based on strict matching in
the following experiments.
The second metric we use is similar to Pyramid
(Nenkova and Passonneau, 2004), which has been
used for summarization evaluation. Instead of com-
paring the system output with each individual hu-
man annotation, the method creates a “pyramid”
using all the human annotated keywords, and then
compares system output to this pyramid. The pyra-
mid consists of all the annotated keywords at dif-
ferent levels. Each keyword has a score based on
how many annotators have selected this one. The
higher the score, the higher up the keyword will be in
the pyramid. Then we calculate an oracle score that
a system can obtain when generating k keywords.
This is done by selecting keywords in the decreas-
ing order in terms of the pyramid levels until we
obtain k keywords. Finally for the system hypoth-
esized k keywords, we compute its score by adding
the scores of the keywords that match those in the
pyramid. The system’s performance is measured us-
ing the relative performance of the system’s pyramid
scores divided by the oracle score.
Table 1 shows the results using human transcripts
for different methods on the 21 test meetings (139
topic segments in total). For comparison, we also
show results using the supervised approach as in
(Liu et al., 2008), which is the average of the 21-
fold cross validation. We only show the maximum
F-measure with respect to individual annotations,
since the average scores show similar trend. In ad-
dition, the weighted relative scores already accounts
for the different annotation and human agreement.
</bodyText>
<table confidence="0.999845125">
Methods F-measure weighted relative score
TFIDF 0.267 0.368
+ POS 0.275 0.370
+ Clustering 0.277 0.367
+ Sent weight 0.290 0.404
Graph 0.258 0.364
Graph+POS 0.277 0.380
Supervised 0.312 0.401
</table>
<tableCaption confidence="0.9957555">
Table 1: Keyword extraction results using human tran-
scripts compared to human annotations.
</tableCaption>
<bodyText confidence="0.999868">
We notice that for the TFIDF framework, adding
POS information slightly helps the basic TFIDF
method. In all the meetings, our statistics show that
adding POS filtering removed 2.3% of human anno-
tated keywords from the word candidates; therefore,
this does not have a significant negative impact on
the upper bound recall rate, but helps eliminate un-
likely keyword candidates. Using word clustering
does not yield a performance gain, most likely be-
cause of the clustering technique we used — it does
clustering simply based on word co-occurrence and
does not capture semantic similarity properly.
Combining the term weight with the sentence
salience score improves performance, supporting the
hypothesis that summary sentences and keywords
can reinforce each other. In fact we performed an
analysis of keywords and summaries using the fol-
lowing two statistics:
</bodyText>
<equation confidence="0.99775">
Psummary(wi)
(1) k =
Ptopic(wi)
</equation>
<bodyText confidence="0.966139909090909">
where Psummary(wi) and Ptopic(wi) represent the
the normalized frequency of a keyword wi in the
summary and the entire topic respectively; and
PSsummary
PStopic
where PSsummary represents the percentage of the
sentences containing at least one keyword among all
the sentences in the summary, and similarly PStopic
is measured using the entire topic segment. We
found that the average k and s are around 3.42 and
6.33 respectively. This means that keywords are
</bodyText>
<listItem confidence="0.430964">
(2) s =
</listItem>
<page confidence="0.995686">
625
</page>
<bodyText confidence="0.99994552238806">
more likely to occur in the summary compared to the
rest of the topic, and the chance for a summary sen-
tence to contain at least one keyword is much higher
than for the other sentences in the topic.
For the graph-based methods, we notice that
adding POS filtering also improves performance,
similar to the TFIDF framework. However, the
graph method does not perform as well as the TFIDF
approach. Comparing with using TFIDF alone, the
graph method (without using POS) yielded worse re-
sults. In addition to using the TFIDF for the word
nodes, information from the sentences is used in the
graph method since a word is linked to sentences
containing this word. The global information in the
S-S graph (connecting a sentence to other sentences
in the document) is propagated to the word nodes.
Unlike the study in (Wan et al., 2007), this infor-
mation does not yield any gain. We did find that the
graph approach performed better in the development
set, but it seems that it does not generalize to this test
set.
Compared to the supervised results, the TFIDF
approach is worse in terms of the individual maxi-
mum F-measure, but achieves similar performance
when using the weighted relative score. However,
the unsupervised TFIDF approach is much simpler
and does not require any annotated data for train-
ing. Therefore it may be easily applied to a new
domain. Again note that these results used word-
based selection. (Liu et al., 2008) investigated
adding bigram key phrases, which we expect to
be independent of these unigram-based approaches
and adding bigram phrases will yield further per-
formance gain for the unsupervised approach. Fi-
nally, we analyzed if the system’s keyword ex-
traction performance is correlated with human an-
notation disagreement using the unsupervised ap-
proach (TFIDF+POS+Sent weight). The correla-
tion (Spearman’s ρ value) between the system’s
F-measure and the three-annotator consistency on
the 27 meetings is 0.5049 (p=0.0072). This indi-
cates that for the meetings with a high disagreement
among human annotators, it is also challenging for
the automatic systems.
Table 2 shows the results using ASR output for
various approaches. The performance measure is
the same as used in Table 1. We find that in gen-
eral, there is a performance degradation compared
to using human transcripts, which is as expected.
We found that only 59.74% of the human annotated
keywords appear in ASR output, that is, the upper
bound of recall is very low. The TFIDF approach
still outperforms the graph method. Unlike on hu-
man transcripts, the addition of information sources
in the TFIDF approach did not yield significant per-
formance gain. A big difference from the human
transcript condition is the use of sentence weight-
ing — adding it degrades performance in ASR, in
contrast to the improvement in human transcripts.
This is possibly because the weighting of the sen-
tences is poor when there are many recognition er-
rors from content words. In addition, compared to
the supervised results, the TFIDF method has sim-
ilar maximum F-measure, but is slightly worse us-
ing the weighted score. Further research is needed
for the ASR condition to investigate better modeling
approaches.
</bodyText>
<table confidence="0.999842375">
Methods F-measure weighted relative score
TFIDF 0.191 0.257
+ POS 0.196 0.259
+ Clustering 0.196 0.259
+ Sent weigh 0.178 0.241
Graph 0.173 0.223
Graph+POS 0.183 0.233
Supervised 0.197 0.269
</table>
<tableCaption confidence="0.998155">
Table 2: Keyword extraction results using ASR output.
</tableCaption>
<sectionHeader confidence="0.9975875" genericHeader="method">
6 Experimental Results: Human
Evaluation
</sectionHeader>
<bodyText confidence="0.999798466666667">
Given the disagreement among human annotators,
one question we need to answer is whether F-
measure or even the weighted relative scores com-
pared with human annotations are appropriate met-
rics to evaluate system-generated keywords. For
example, precision measures among the system-
generated keywords how many are correct. How-
ever, this does not measure if the unmatched system-
generated keywords are bad or acceptable. We
therefore performed a small scale human evaluation.
We selected four topic segments from four differ-
ent meetings, and gave output from different sys-
tems to five human subjects. The subjects ranged
in age from 22 to 63, and all but one had only basic
knowledge of computers. We first asked the eval-
</bodyText>
<page confidence="0.99709">
626
</page>
<bodyText confidence="0.999806">
uators to read the entire topic transcript, and then
presented them with the system-generated keywords
(randomly ordered by different systems). For com-
parison, the keywords annotated by our three hu-
man annotators were also included without reveal-
ing which sets of keywords were generated by a
human and which by a computer. Because there
was such disagreement between annotators regard-
ing what made good keywords, we instead asked our
evaluators to mark any words that were definitely
not keywords. Systems that produced more of these
rejected words (such as “basically” or “mmm-hm”)
are assumed to be worse than those containing fewer
rejected words. We then measured the percentage of
rejected keywords for each system/annotator. The
results are shown in Table 3. Not surprisingly, the
human annotations rank at the top. Overall, we find
human evaluation results to be consistent with the
automatic evaluation metrics in terms of the ranking
of different systems.
</bodyText>
<table confidence="0.994607">
Systems Rejection rate
Annotator 2 8%
Annotator 3 19%
Annotator 1 25%
TFIDF + POS 28%
TFIDF 30%
</table>
<tableCaption confidence="0.967234">
Table 3: Human evaluation results: percentage of the re-
jected keywords by human evaluators for different sys-
tems/annotators.
</tableCaption>
<bodyText confidence="0.999963692307693">
Note this rejection rate is highly related to the re-
call/precision measure in the sense that it measures
how many keywords are acceptable (or rejected)
among the system generated ones. However, instead
of comparing to a fixed set of human annotated key-
words (e.g., five) and using that as a gold standard
to compute recall/precision, in this evaluation, the
human evaluator may have a larger set of accept-
able keywords in their mind. We also measured the
human evaluator agreement regarding the accepted
or bad keywords. We found that the agreement on
a bad keyword among five, four, and three human
evaluator is 10.1%, 14.8%, and 10.1% respectively.
This suggests that humans are more likely to agree
on a bad keyword selection compared to agreement
on the selected keywords, as discussed in Section 3
(even though the data sets in these two analysis are
not the same). Another observation from the human
evaluation is that sometimes a person rejects a key-
word from one system output, but accepts that on
the list from another system. We are not sure yet
whether this is the inconsistency from human evalu-
ators or whether the judgment is based on a word’s
occurrence with other provided keywords and thus
some kind of semantic coherence. Further investi-
gation on human evaluation is still needed.
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999959918918919">
In this paper, we evaluated unsupervised keyword
extraction performance for the meeting domain, a
genre that is significantly different from most pre-
vious work. We compared several different ap-
proaches using the transcripts of the ICSI meeting
corpus. Our results on the human transcripts show
that the simple TFIDF based method is very compet-
itive. Adding additional knowledge such as POS and
sentence salience score helps improve performance.
The graph-based approach performs less well in this
task, possibly because of the lack of structure in
this domain. We use different performance measure-
ments, including F-measure with respect to individ-
ual human annotations and a weighted metric rela-
tive to the oracle system performance. We also per-
formed a new human evaluation for this task and our
results show consistency with the automatic mea-
surement. In addition, experiments on the ASR out-
put show performance degradation, but more impor-
tantly, different patterns in terms of the contributions
of information sources compared to using human
transcripts. Overall the unsupervised approaches are
simple but effective; however, system performance
compared to the human performance is still low,
suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-
ferent weighting algorithms for the graph-based ap-
proach. We also need a better way to decide the
number of keywords to generate instead of using a
fixed number. Furthermore, since there are multiple
speakers in the meeting domain, we plan to incor-
porate speaker information in various approaches.
More importantly, we will perform a more rigorous
human evaluation, and also use extrinsic evaluation
to see whether automatically generated keywords fa-
cilitate tasks such as information retrieval or meeting
browsing.
</bodyText>
<page confidence="0.997658">
627
</page>
<sectionHeader confidence="0.999545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99966675">
This work is supported by NSF award IIS-0714132.
Any opinions expressed in this work are those of the
authors and do not necessarily reflect the views of
NSF.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846113636364">
T. Brants. 2000. TnT – a statistical part-of-speech tagger.
In Proceedings of the 6th Applied NLP Conference.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5:1–25.
A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting
keyphrases from spoken audio documents. In Infor-
mation Retrieval Techniques for Speech Applications,
pages 339–342.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 688–673.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216–223.
D. Inkpen and A. Desilets. 2004. Extracting
semantically-coherent keyphrases from speech. Cana-
dian Acoustics Association, 32:130–131.
A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI meeting corpus. In
Proceedings of ICASSP.
Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic
extraction and learning of keyphrases from scientific
articles. In Computational Linguistics and Intelligent
Text Processing, pages 657–669.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proceedings of IEEE
SLT.
Y. Matsuo and M. Ishizuka. 2004. Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artifi-
cialIntelligence, 13(1):157–169.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modeling for automatic lecture tran-
scription. In Proceedings of Interspeech.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005.
Evaluating automatic summaries of meeting record-
ings. In Proceedings of ACL 2005 MTSE Workshop,
pages 33–40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings of HLT/NAACL.
L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004.
Automatic keyword extraction from spoken text. a
comparison of two lexical resources: the EDR and
WordNet. In Proceedings of the LREC.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proceedings of The First Docu-
ment Understanding Conference.
I. Rogina. 2002. Lecture and presentation tracking in an
intelligent meeting room. In Proceedings of ICMI.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proceedings of SIGDial Workshop, pages
97–100.
A. Stolcke. 2002. SRILM – An extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901–904.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303–336.
P.D. Turney. 2002. Mining the web for lexical knowl-
edge to improve keyphrase extraction: Learning from
labeled and unlabeled data. In National Research
Council, Institute for Information Technology, Techni-
cal Report ERB-1096.
P.D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434–439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Pro-
ceedings of ACL, pages 552–559.
C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007.
Speech retrieval using spoken keyword extraction and
semantic verification. In Proceedings of IEEE Region
10 Conference, pages 1–4.
Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005.
Using MLP features in SRI’s conversational speech
recognition system. In Proceedings of Interspeech.
</reference>
<page confidence="0.997359">
628
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783939">
<title confidence="0.999014">Unsupervised Approaches for Automatic Keyword Extraction Meeting Transcripts</title>
<author confidence="0.992679">Feifan Liu</author>
<author confidence="0.992679">Deana Pennell</author>
<author confidence="0.992679">Fei Liu</author>
<author confidence="0.992679">Yang</author>
<affiliation confidence="0.9977555">Computer Science The University of Texas at</affiliation>
<address confidence="0.805388">Richardson, TX 75080,</address>
<abstract confidence="0.99945032">This paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts. In the TFIDF (term frequency, inverse document frequency) weighting framework, we incorporated partof-speech (POS) information, word clustering, and sentence salience score. We also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words. The system performance is evaluated in different ways, including comparison to human annotated keywords using F-measure and a weighted score relative to the oracle system performance, as well as a novel alternative human evaluation. Our results have shown that the simple unsupervised TFIDF approach performs reasonably well, and the additional information from POS and sentence score helps keyword extraction. However, the graph method is less effective for this domain. Experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference.</booktitle>
<contexts>
<context position="14678" citStr="Brants, 2000" startWordPosition="2401" endWordPosition="2402"> IDF value is: IDFi = log(N/Ni) where Ni denotes the number of the documents containing word wi, and N is the total number of the documents in the collection. We also performed L2 normalization for the IDF values when combining them with other scores. (B) Part of Speech (POS) filtering In addition to using a stopword list to remove words from consideration, we also leverage POS information to filter unlikely keywords. Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts. (C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning. In addition, when the document is short, the TF may not be a reliable indicator of the importance of the word. Our idea is therefore to account for the frequency of other similar words when calculating the TF of a word in the document. For this, we group all the words into clusters in an unsupervised fashion. If the total term</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the 6th Applied NLP Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<volume>30</volume>
<contexts>
<context position="6610" citStr="Brin and Page, 1998" startWordPosition="1033" endWordPosition="1036">) is very useful for news article text, since keywords often appear early in the document (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative. A supervised approach to keyword extraction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task. Another line of research for keyword extraction has adopted graph-based methods similar to Google’s PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for ke</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bulyko</author>
<author>M Ostendorf</author>
<author>M Siu</author>
<author>T Ng</author>
<author>A Stolcke</author>
<author>O Cetin</author>
</authors>
<title>Web resources for language modeling in conversational speech recognition.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<pages>5--1</pages>
<contexts>
<context position="7656" citStr="Bulyko et al., 2007" startWordPosition="1203" endWordPosition="1206">e the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully</context>
</contexts>
<marker>Bulyko, Ostendorf, Siu, Ng, Stolcke, Cetin, 2007</marker>
<rawString>I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and O. Cetin. 2007. Web resources for language modeling in conversational speech recognition. ACM Transactions on Speech and Language Processing, 5:1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Desilets</author>
<author>B D Bruijn</author>
<author>J Martin</author>
</authors>
<title>Extracting keyphrases from spoken audio documents. In Information Retrieval Techniques for Speech Applications,</title>
<date>2002</date>
<pages>339--342</pages>
<contexts>
<context position="7696" citStr="Desilets et al., 2002" startWordPosition="1211" endWordPosition="1215">mains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully apply some existing techniques, as well</context>
</contexts>
<marker>Desilets, Bruijn, Martin, 2002</marker>
<rawString>A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting keyphrases from spoken audio documents. In Information Retrieval Techniques for Speech Applications, pages 339–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>688--673</pages>
<contexts>
<context position="4215" citStr="Frank et al., 1999" startWordPosition="636" endWordPosition="639">put. Overall the TFIDF based framework seems to work well for this domain, and the additional knowledge sources help improve system performance. The graph-based approach yielded worse results, especially for the ASR condition, suggesting further investigation for this task. 2 Related Work TFIDF weighting has been widely used for keyword or key phrase extraction. The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection. Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reliable IDF estimate. (Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results. Web information has also been used as an additional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of IJCAI, pages 688–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10266" citStr="Galley et al., 2003" startWordPosition="1640" endWordPosition="1643">., 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5. One example of the annotated keywords for a topic segment is: • Annotator I: analysis, constraints, template matcher; • Annotator II: syntactic analysis, parser, pattern matcher, finite-state transducers; • Annotator III: lexicon, set processing, chunk parser. Note that these meetings are research discussions, and that the annotators may not be very familiar with 1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005). the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annotators, we notice that in general the quality of annotator I is the poorest. This is based on the authors’ judgment, and is also confirmed later by an independent human evaluation (in Section 6). For a better understandin</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="4228" citStr="Hulth, 2003" startWordPosition="640" endWordPosition="641">DF based framework seems to work well for this domain, and the additional knowledge sources help improve system performance. The graph-based approach yielded worse results, especially for the ASR condition, suggesting further investigation for this task. 2 Related Work TFIDF weighting has been widely used for keyword or key phrase extraction. The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection. Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reliable IDF estimate. (Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results. Web information has also been used as an additional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual inform</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>A. Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of EMNLP, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Inkpen</author>
<author>A Desilets</author>
</authors>
<title>Extracting semantically-coherent keyphrases from speech. Canadian Acoustics Association,</title>
<date>2004</date>
<pages>32--130</pages>
<contexts>
<context position="5267" citStr="Inkpen and Desilets, 2004" startWordPosition="807" endWordPosition="810">dge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is important to ensure the quality of the first selection for the subsequent addition of keywords. Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004). Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003). In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features. Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000). Some of these features may not work well for meeting transcripts. For ex</context>
</contexts>
<marker>Inkpen, Desilets, 2004</marker>
<rawString>D. Inkpen and A. Desilets. 2004. Extracting semantically-coherent keyphrases from speech. Canadian Acoustics Association, 32:130–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan Gelbart</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<marker>Gelbart, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y H Kerner</author>
<author>Z Gross</author>
<author>A Masa</author>
</authors>
<title>Automatic extraction and learning of keyphrases from scientific articles.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>657--669</pages>
<contexts>
<context position="4250" citStr="Kerner et al., 2005" startWordPosition="642" endWordPosition="645">ework seems to work well for this domain, and the additional knowledge sources help improve system performance. The graph-based approach yielded worse results, especially for the ASR condition, suggesting further investigation for this task. 2 Related Work TFIDF weighting has been widely used for keyword or key phrase extraction. The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection. Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reliable IDF estimate. (Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results. Web information has also been used as an additional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the cu</context>
</contexts>
<marker>Kerner, Gross, Masa, 2005</marker>
<rawString>Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic extraction and learning of keyphrases from scientific articles. In Computational Linguistics and Intelligent Text Processing, pages 657–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>F Liu</author>
<author>Y Liu</author>
</authors>
<title>Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion.</title>
<date>2008</date>
<booktitle>In Proceedings of IEEE SLT.</booktitle>
<contexts>
<context position="6317" citStr="Liu et al., 2008" startWordPosition="985" endWordPosition="988">ations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000). Some of these features may not work well for meeting transcripts. For example, the position of a phrase (measured by the number of words before its first appearance divided by the document length) is very useful for news article text, since keywords often appear early in the document (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative. A supervised approach to keyword extraction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task. Another line of research for keyword extraction has adopted graph-based methods similar to Google’s PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds usi</context>
<context position="20807" citStr="Liu et al., 2008" startWordPosition="3493" endWordPosition="3496">use human annotated keywords as references, and compare the system output to them. The first metric we use is F-measure, which has been widely used for this task and other detection tasks. We compare the system output with respect to each human annotation, and calculate the maximum and the average F-scores. Note that our keyword evaluation is wordbased. When human annotators choose key phrases (containing more than one word), we split them into words and measure the matching words. Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%. In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words. Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments. The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. Instead of comparing the system output with each individual human annotation, the method creates a “pyramid” using all the human annotated keywords, and then compares system output to this pyramid. The pyramid consists of all the annotated keywords at d</context>
<context position="22264" citStr="Liu et al., 2008" startWordPosition="3741" endWordPosition="3744">g k keywords. This is done by selecting keywords in the decreasing order in terms of the pyramid levels until we obtain k keywords. Finally for the system hypothesized k keywords, we compute its score by adding the scores of the keywords that match those in the pyramid. The system’s performance is measured using the relative performance of the system’s pyramid scores divided by the oracle score. Table 1 shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total). For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21- fold cross validation. We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend. In addition, the weighted relative scores already accounts for the different annotation and human agreement. Methods F-measure weighted relative score TFIDF 0.267 0.368 + POS 0.275 0.370 + Clustering 0.277 0.367 + Sent weight 0.290 0.404 Graph 0.258 0.364 Graph+POS 0.277 0.380 Supervised 0.312 0.401 Table 1: Keyword extraction results using human transcripts compared to human annotations. We notice that for the TFIDF fr</context>
<context position="25627" citStr="Liu et al., 2008" startWordPosition="4291" endWordPosition="4294">Wan et al., 2007), this information does not yield any gain. We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set. Compared to the supervised results, the TFIDF approach is worse in terms of the individual maximum F-measure, but achieves similar performance when using the weighted relative score. However, the unsupervised TFIDF approach is much simpler and does not require any annotated data for training. Therefore it may be easily applied to a new domain. Again note that these results used wordbased selection. (Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further performance gain for the unsupervised approach. Finally, we analyzed if the system’s keyword extraction performance is correlated with human annotation disagreement using the unsupervised approach (TFIDF+POS+Sent weight). The correlation (Spearman’s ρ value) between the system’s F-measure and the three-annotator consistency on the 27 meetings is 0.5049 (p=0.0072). This indicates that for the meetings with a high disagreement among human annot</context>
</contexts>
<marker>Liu, Liu, Liu, 2008</marker>
<rawString>F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion. In Proceedings of IEEE SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Keyword extraction from a single document using word co-occurrence statistical information.</title>
<date>2004</date>
<journal>International Journal on ArtificialIntelligence,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="4390" citStr="Matsuo and Ishizuka, 2004" startWordPosition="666" endWordPosition="669">ch yielded worse results, especially for the ASR condition, suggesting further investigation for this task. 2 Related Work TFIDF weighting has been widely used for keyword or key phrase extraction. The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection. Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reliable IDF estimate. (Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results. Web information has also been used as an additional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is important to ensur</context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Y. Matsuo and M. Ishizuka. 2004. Keyword extraction from a single document using word co-occurrence statistical information. International Journal on ArtificialIntelligence, 13(1):157–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Munteanu</author>
<author>G Penn</author>
<author>R Baecker</author>
</authors>
<title>Webbased language modeling for automatic lecture transcription.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="7635" citStr="Munteanu et al., 2007" startWordPosition="1199" endWordPosition="1202">een used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whethe</context>
</contexts>
<marker>Munteanu, Penn, Baecker, 2007</marker>
<rawString>C. Munteanu, G. Penn, and R. Baecker. 2007. Webbased language modeling for automatic lecture transcription. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Evaluating automatic summaries of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005 MTSE Workshop,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="8735" citStr="Murray et al., 2005" startWordPosition="1387" endWordPosition="1390">ences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully apply some existing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area. 3 Data We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005). The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus. We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output. We recruited three Computer Science undergraduate students to annotate keywords for each topic segment, using 27 selected ICSI meetings.1 Up to five indicative key words or phrases were annotated for eac</context>
<context position="10288" citStr="Murray et al., 2005" startWordPosition="1644" endWordPosition="1647"> our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5. One example of the annotated keywords for a topic segment is: • Annotator I: analysis, constraints, template matcher; • Annotator II: syntactic analysis, parser, pattern matcher, finite-state transducers; • Annotator III: lexicon, set processing, chunk parser. Note that these meetings are research discussions, and that the annotators may not be very familiar with 1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005). the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annotators, we notice that in general the quality of annotator I is the poorest. This is based on the authors’ judgment, and is also confirmed later by an independent human evaluation (in Section 6). For a better understanding of the gold standard</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2005</marker>
<rawString>G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evaluating automatic summaries of meeting recordings. In Proceedings of ACL 2005 MTSE Workshop, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="21102" citStr="Nenkova and Passonneau, 2004" startWordPosition="3542" endWordPosition="3545">and the average F-scores. Note that our keyword evaluation is wordbased. When human annotators choose key phrases (containing more than one word), we split them into words and measure the matching words. Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%. In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words. Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments. The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. Instead of comparing the system output with each individual human annotation, the method creates a “pyramid” using all the human annotated keywords, and then compares system output to this pyramid. The pyramid consists of all the annotated keywords at different levels. Each keyword has a score based on how many annotators have selected this one. The higher the score, the higher up the keyword will be in the pyramid. Then we calculate an oracle score that a system can obtain when generating k keywords. This is done by selecting keywords in the</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Plas</author>
<author>V Pallotta</author>
<author>M Rajman</author>
<author>H Ghorbel</author>
</authors>
<title>Automatic keyword extraction from spoken text. a comparison of two lexical resources: the EDR and WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC.</booktitle>
<contexts>
<context position="7286" citStr="Plas et al., 2004" startWordPosition="1144" endWordPosition="1147">nforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, </context>
</contexts>
<marker>Plas, Pallotta, Rajman, Ghorbel, 2004</marker>
<rawString>L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004. Automatic keyword extraction from spoken text. a comparison of two lexical resources: the EDR and WordNet. In Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>S Blair-Goldensohn</author>
<author>Z Zhang</author>
</authors>
<title>Experiments in single and multi-document summarization using MEAD.</title>
<date>2001</date>
<booktitle>In Proceedings of The First Document Understanding Conference.</booktitle>
<contexts>
<context position="16783" citStr="Radev et al., 2001" startWordPosition="2782" endWordPosition="2785">word wi. We set parameter α to be slightly larger than 1. We did not include stopwords when adding the term frequencies for the words in a cluster. (D) Combining with sentence salience score Intuitively, the words in an important sentence should be assigned a high weight for keyword extraction. In order to leverage the sentence information, we adjust a word’s weight by the salience scores of the sentences containing that word. The sentence score is calculated based on its cosine similarity to the entire meeting. This score is often used in extractive summarization to select summary sentences (Radev et al., 2001). The cosine similarity between two vectors, D1 and D2, is defined as: sim(D1, D2) = Ei t1it2i where ti is the term weight for a word wi, for which we use the TFIDF value. 4.2 Graph-based Methods For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction. This algorithm is based on the assumption that important sentences/words are connected to other important sentences/words. Four graphs are created: one graph in which sentences are connected to other sentences (S-S graph), one in whi</context>
</contexts>
<marker>Radev, Blair-Goldensohn, Zhang, 2001</marker>
<rawString>D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Experiments in single and multi-document summarization using MEAD. In Proceedings of The First Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Rogina</author>
</authors>
<title>Lecture and presentation tracking in an intelligent meeting room.</title>
<date>2002</date>
<booktitle>In Proceedings of ICMI.</booktitle>
<contexts>
<context position="7711" citStr="Rogina, 2002" startWordPosition="1216" endWordPosition="1217">r these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully apply some existing techniques, as well as propose new</context>
</contexts>
<marker>Rogina, 2002</marker>
<rawString>I. Rogina. 2002. Lecture and presentation tracking in an intelligent meeting room. In Proceedings of ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGDial Workshop,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="8679" citStr="Shriberg et al., 2004" startWordPosition="1378" endWordPosition="1381">recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully apply some existing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area. 3 Data We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005). The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus. We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output. We recruited three Computer Science undergraduate students to annotate keywords for each topic segment, using 27 selected ICSI meetings.1 Up to fiv</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proceedings of SIGDial Workshop, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="15539" citStr="Stolcke, 2002" startWordPosition="2557" endWordPosition="2558">rms of semantic meaning. In addition, when the document is short, the TF may not be a reliable indicator of the importance of the word. Our idea is therefore to account for the frequency of other similar words when calculating the TF of a word in the document. For this, we group all the words into clusters in an unsupervised fashion. If the total term frequency of all the words in one cluster is high, it is likely that this cluster contributes more to the current topic from a thematic point of view. Thus we want to assign higher weights to the words in this cluster. We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection. It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model. Using the clusters, we then adjust the TF weighting by integrating with the cluster term frequency (CTF): TF CTF(wi) = TF(wi)∗α(Pwl∈Cz,wl�wz freq(wl)) where the last summation component means the total term frequency of all the other words in this document that belong to the same cluster Ci as the current 0.8 0.6 0.4 0.2 0.5 0.4 0.3 0.2 0.1 0 1 0 0 30 60 90 120 1 3 5 7 9 11 13 15 17 19 21 23 25 27 3 agree 2 a</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of ICSLP, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction. Information Retrieval,</title>
<date>2000</date>
<pages>2--303</pages>
<contexts>
<context position="5424" citStr="Turney, 2000" startWordPosition="833" endWordPosition="835">se mutual information) score to the current selected keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is important to ensure the quality of the first selection for the subsequent addition of keywords. Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004). Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003). In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features. Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000). Some of these features may not work well for meeting transcripts. For example, the position of a phrase (measured by the number of words before its first appearance divided by the document length) is very useful for news article </context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>P.D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2:303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the web for lexical knowledge to improve keyphrase extraction: Learning from labeled and unlabeled data.</title>
<date>2002</date>
<journal>In National Research</journal>
<tech>Technical Report ERB-1096.</tech>
<institution>Council, Institute for Information Technology,</institution>
<contexts>
<context position="4690" citStr="Turney, 2002" startWordPosition="714" endWordPosition="715">ent collection. Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reliable IDF estimate. (Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results. Web information has also been used as an additional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is important to ensure the quality of the first selection for the subsequent addition of keywords. Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004). Keyword extraction ha</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P.D. Turney. 2002. Mining the web for lexical knowledge to improve keyphrase extraction: Learning from labeled and unlabeled data. In National Research Council, Institute for Information Technology, Technical Report ERB-1096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>434--439</pages>
<contexts>
<context position="5474" citStr="Turney, 2003" startWordPosition="842" endWordPosition="844">ted keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is important to ensure the quality of the first selection for the subsequent addition of keywords. Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004). Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003). In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features. Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000). Some of these features may not work well for meeting transcripts. For example, the position of a phrase (measured by the number of words before its first appearance divided by the document length) is very useful for news article text, since keywords often appear early in the doc</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>P.D. Turney. 2003. Coherent keyphrase extraction via web mining. In Proceedings of IJCAI, pages 434–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="6645" citStr="Wan et al., 2007" startWordPosition="1039" endWordPosition="1042">, since keywords often appear early in the document (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative. A supervised approach to keyword extraction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task. Another line of research for keyword extraction has adopted graph-based methods similar to Google’s PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant</context>
<context position="17078" citStr="Wan et al., 2007" startWordPosition="2835" endWordPosition="2838">n order to leverage the sentence information, we adjust a word’s weight by the salience scores of the sentences containing that word. The sentence score is calculated based on its cosine similarity to the entire meeting. This score is often used in extractive summarization to select summary sentences (Radev et al., 2001). The cosine similarity between two vectors, D1 and D2, is defined as: sim(D1, D2) = Ei t1it2i where ti is the term weight for a word wi, for which we use the TFIDF value. 4.2 Graph-based Methods For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction. This algorithm is based on the assumption that important sentences/words are connected to other important sentences/words. Four graphs are created: one graph in which sentences are connected to other sentences (S-S graph), one in which words are connected to other words (W-W graph), and two graphs connecting words to sentences with uni-directional edges (W-S and S-W graphs). Stopwords are removed before the creation of the graphs so they will be ineligible to be keywords. The final weight for a word node depends on its con</context>
<context position="19284" citStr="Wan et al., 2007" startWordPosition="3227" endWordPosition="3230">connection among words. The self-loop values are the TFIDF values of the words. This is also equivalent to using an identity matrix for the word-word connection and TFIDF as the initial weight for each vertex in the graph. We investigated other strategies to assign a weight for the edge between two word nodes; however, so far the best result we obtained is using this diagonal matrix. • S-W and W-S Graphs: The weight for an edge between a sentence and a word is the TF of the word in the sentence multiplied by the word’s IDF value. These weights are initially added only to the S-W graph, as in (Wan et al., 2007); then that graph is normalized and transposed to create the W-S graph. • S-S Graph: The sentence node uses a vector space model and is composed of the weights of those words connected to this sentence in the S-W graph. We then use cosine similarity between two sentence vectors. Similar to the above TFIDF framework, we also use POS filtering for the graph-based approach. After the weights for all the words are determined, we select the top ranked words with the POS restriction. 5 Experimental Results: Automatic Evaluation Using the approaches described above, we computed weights for the words </context>
<context position="25027" citStr="Wan et al., 2007" startWordPosition="4189" endWordPosition="4192">-based methods, we notice that adding POS filtering also improves performance, similar to the TFIDF framework. However, the graph method does not perform as well as the TFIDF approach. Comparing with using TFIDF alone, the graph method (without using POS) yielded worse results. In addition to using the TFIDF for the word nodes, information from the sentences is used in the graph method since a word is linked to sentences containing this word. The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes. Unlike the study in (Wan et al., 2007), this information does not yield any gain. We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set. Compared to the supervised results, the TFIDF approach is worse in terms of the individual maximum F-measure, but achieves similar performance when using the weighted relative score. However, the unsupervised TFIDF approach is much simpler and does not require any annotated data for training. Therefore it may be easily applied to a new domain. Again note that these results used wordbased selection. (Liu et al., 2008)</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang, and J. Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of ACL, pages 552–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Wu</author>
<author>C L Huang</author>
<author>C S Hsu</author>
<author>K M Lee</author>
</authors>
<title>Speech retrieval using spoken keyword extraction and semantic verification.</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE Region 10 Conference,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="7673" citStr="Wu et al., 2007" startWordPosition="1207" endWordPosition="1210"> therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech — meetings in particular. Thus our goal in this paper is to investi621 gate whether we can successfully apply some exist</context>
</contexts>
<marker>Wu, Huang, Hsu, Lee, 2007</marker>
<rawString>C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007. Speech retrieval using spoken keyword extraction and semantic verification. In Proceedings of IEEE Region 10 Conference, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhu</author>
<author>A Stolcke</author>
<author>B Chen</author>
<author>N Morgan</author>
</authors>
<title>Using MLP features in SRI’s conversational speech recognition system.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="8865" citStr="Zhu et al., 2005" startWordPosition="1407" endWordPosition="1410">essfully apply some existing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area. 3 Data We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005). The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus. We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output. We recruited three Computer Science undergraduate students to annotate keywords for each topic segment, using 27 selected ICSI meetings.1 Up to five indicative key words or phrases were annotated for each topic. In total, we have 208 topics annotated with keywords. The average length of the topics (measured using the number of dial</context>
</contexts>
<marker>Zhu, Stolcke, Chen, Morgan, 2005</marker>
<rawString>Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005. Using MLP features in SRI’s conversational speech recognition system. In Proceedings of Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>