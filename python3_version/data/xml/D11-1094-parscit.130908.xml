<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.848093">
Latent Vector Weighting for Word Meaning in Context
</title>
<author confidence="0.6560265">
Tim Van de Cruys Thierry Poibeau Anna Korhonen
RCEAL LaTTiCe, UMR8094 Computer Laboratory &amp; RCEAL
</author>
<affiliation confidence="0.891386">
University of Cambridge CNRS &amp; ENS University of Cambridge
</affiliation>
<email confidence="0.971807">
tv234@cam.ac.uk thierry.poibeau@ens.fr alk23@cam.ac.uk
</email>
<sectionHeader confidence="0.998366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999509125">
This paper presents a novel method for the com-
putation of word meaning in context. We make
use of a factorization model in which words, to-
gether with their window-based context words
and their dependency relations, are linked to
latent dimensions. The factorization model al-
lows us to determine which dimensions are im-
portant for a particular context, and adapt the
dependency-based feature vector of the word
accordingly. The evaluation on a lexical substi-
tution task – carried out for both English and
French – indicates that our approach is able to
reach better results than state-of-the-art meth-
ods in lexical substitution, while at the same
time providing more accurate meaning repre-
sentations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996916">
According to the distributional hypothesis of meaning
(Harris, 1954), words that occur in similar contexts
tend to be semantically similar. In the spirit of this by
now well-known adage, numerous algorithms have
sprouted up that try to capture the semantics of words
by looking at their distribution in texts, and compar-
ing those distributions in a vector space model.
Up till now, the majority of computational ap-
proaches to semantic similarity represent the mean-
ing of a word as the aggregate of the word’s contexts,
and hence do not differentiate between the different
senses of a word. The meaning of a word, however, is
largely dependent on the particular context in which
it appears. Take for example the word work in sen-
tences (1) and (2).
</bodyText>
<listItem confidence="0.772146333333333">
(1) The painter’s recent work is a classic example
of art brut.
(2) Equal pay for equal work!
</listItem>
<bodyText confidence="0.9999364">
The meaning of work is quite different in both sen-
tences. In sentence (1), work refers to the product of a
creative act, viz. a painting. In sentence (2), it refers
to labour carried out as a source of income. The
NLP community’s standard answer to the ambiguity
problem has always been some flavour of word sense
disambiguation (WSD), which in its standard form
boils down to choosing the best-possible fit from a
pre-defined sense inventory. In recent years, it has
become clear that this is in fact a very hard task to
solve for computers and humans alike (Ide and Wilks,
2006; Erk et al., 2009; Erk, 2010).
With these findings in mind, researchers have
started looking at different methods to tackle lan-
guage’s ambiguity, ranging from coarser-grained
sense inventories (Hovy et al., 2006) and graded
sense assignment (Erk and McCarthy, 2009), over
word sense induction (Sch¨utze, 1998; Pantel and Lin,
2002; Agirre et al., 2006), to the computation of indi-
vidual word meaning in context (Erk and Pad´o, 2008;
Thater et al., 2010; Dinu and Lapata, 2010). This
research inscribes itself in the same line of thought,
in which the meaning disambiguation of a word is
not just the assignment of a pre-defined sense; in-
stead, the original meaning representation of a word
is adapted ‘on the fly’, according to – and specifi-
cally tailored for – the particular context in which
it appears. To be able to do so, we build a factor-
ization model in which words, together with their
window-based context words and their dependency
</bodyText>
<page confidence="0.968006">
1012
</page>
<note confidence="0.9577905">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999286">
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which dimen-
sions are important for a particular context, and adapt
the dependency-based feature vector of the word ac-
cordingly. The evaluation on a lexical substitution
task – carried out for both English and French – indi-
cates that our method is able to reach better results
than state-of-the-art methods in lexical substitution,
while at the same time providing more accurate mean-
ing representations.
The remainder of this paper is organized as follows.
In section 2, we present some earlier work that is
related to the research presented here. Section 3
describes the methodology of our method, focusing
on the factorization model, and the computation of
meaning in context. Section 4 presents a thorough
evaluation on a lexical substitution task, both for
English and French. The last section then draws
conclusions, and presents a number of topics that
deserve further exploration.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999839287671233">
One of the best known computational models of se-
mantic similarity is latent semantic analysis — LSA
(Landauer and Dumais, 1997; Landauer et al., 1998).
In LSA, a term-document matrix is created, that con-
tains the frequency of each word in a particular doc-
ument. This matrix is then decomposed into three
other matrices with a mathematical factorization tech-
nique called singular value decomposition (SVD).
The most important dimensions that come out of the
SVD are said to represent latent semantic dimensions,
according to which nouns and documents can be rep-
resented more efficiently. Our model also applies
a factorization technique (albeit a different one) in
order to find a reduced semantic space.
The nature of a word’s context is a determining
factor in the kind of the semantic similarity that is in-
duced. A broad context window (e.g. a paragraph or
document) yields broad, topical similarity, whereas
a small context window yields tight, synonym-like
similarity. This has lead a number of researchers
(e.g. Lin (1998)) to use the dependency relations that
a particular word takes part in as context features.
An overview of dependency-based semantic space
models is given in Pad´o and Lapata (2007).
A number of researchers have exploited the no-
tion of context to differentiate between the different
senses of a word in an unsupervised way (a task la-
beled word sense induction or WSI). Sch¨utze (1998)
proposed a context-clustering approach, in which
context vectors are created for the different instances
of a particular word, and those contexts are grouped
into a number of clusters, representing the different
senses of the word. The context vectors are rep-
resented as second-order co-occurrences (i.e. the
contexts of the target word are similar if the words
they in turn co-occur with are similar). Van de Cruys
(2008) proposed a model for sense induction based
on latent semantic dimensions. Using a factorization
technique based on non-negative matrix factorization,
the model induces a latent semantic space according
to which both dependency features and broad con-
textual features are classified. Using the latent space,
the model is able to discriminate between different
word senses. Our approach makes use of a simi-
lar factorization model, but we extend the approach
with a probabilistic framework that is able to adapt
the original vector according to the context of the
instance.
Recently, a number of models emerged that aim
to model the individual meaning of words in context.
Erk and Pad´o (2008, 2009) make use of selectional
preferences to express the meaning of a word in con-
text; the meaning of a word in the presence of an
argument is computed by multiplying the word’s vec-
tor with a vector that captures the inverse selectional
preferences of the argument. Thater et al. (2009) and
Thater et al. (2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model; their model allows
first-order co-occurrences to act as a filter upon the
second-order vector space, which allows for the com-
putation of meaning in context.
Erk and Pad´o (2010) propose an exemplar-based
approach, in which the meaning of a word in context
is represented by the activated exemplars that are
most similar to it. And Mitchell and Lapata (2008)
propose a model for vector composition, focusing on
the different functions that might be used to combine
the constituent vectors. Their results indicate that
a model based on pointwise multiplication achieves
better results than models based on vector addition.
Finally, Dinu and Lapata (2010) propose a proba-
bilistic framework that models the meaning of words
</bodyText>
<page confidence="0.98342">
1013
</page>
<bodyText confidence="0.9998351875">
as a probability distribution over latent dimensions
(‘senses’). Contextualized meaning is then mod-
eled as a change in the original sense distribution.
The model presented in this paper bears some resem-
blances to their approach; however, while their ap-
proach computes the contextualized meaning directly
within the latent space, our model exploits the latent
space to determine the features that are important
for a particular context, and adapt the original (out-
of-context) dependency-based feature vector of the
target word accordingly. This allows for a more pre-
cise and more distinct computation of word meaning
in context. Secondly, Dinu and Lapata use window-
based context features to build their latent model,
while our approach combines both window-based
and dependency-based features.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999472">
3.1 Non-negative Matrix Factorization
</subsectionHeader>
<bodyText confidence="0.999982956521739">
Our model uses non-negative matrix factorization
(Lee and Seung, 2000) in order to find latent dimen-
sions. There are a number of reasons to prefer NMF
over the better known singular value decomposition
used in LSA. First of all, NMF allows us to mini-
mize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of
the factorization ensures that only additive and no
subtractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the resulting
model to be interpreted probabilistically, which is not
straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
</bodyText>
<equation confidence="0.891343">
Ai×j ≈ Wi×kHk×j (1)
</equation>
<bodyText confidence="0.999442588235294">
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler di-
vergence as an objective function, we want to find the
matrices W and H for which the Kullback-Leibler
divergence between A and WH (the multiplication
of W and H) is the smallest. This factorization is
carried out through the iterative application of update
rules. Matrices W and H are randomly initialized,
and the rules in 2 and 3 are iteratively applied – alter-
nating between them. In each iteration, each vector is
adequately normalized, so that all dimension values
sum to 1.
</bodyText>
<equation confidence="0.976291">
A.
H H Ei Wia (WH)iµ
Haµ � aµ E (2)
k Wka
EAiµ
µ Haµ (WH)iµ
Ev Hav
</equation>
<subsectionHeader confidence="0.999976">
3.2 Combining syntax and context words
</subsectionHeader>
<bodyText confidence="0.99997696">
Using an extension of non-negative matrix factor-
ization (Van de Cruys, 2008), it is possible to
jointly induce latent factors for three different modes:
words, their window-based context words, and their
dependency-based context features. As input to
the algorithm, three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified
by dependency-based features, the second matrix
contains co-occurrence frequencies of words cross-
classified by words that appear in the word’s context
window, and the third matrix contains co-occurrence
frequencies of dependency-based features cross-
classified by co-occurring context words. NMF is
then applied to the three matrices, and the separate
factorizations are interleaved (i.e. the results of the
former factorization are used to initialize the factor-
ization of the next matrix). A graphical represen-
tation of the interleaved factorization algorithm is
given in figure 1.
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based features) are all represented
according to a limited number of latent factors.
</bodyText>
<figure confidence="0.86576">
Wia ← Wia
(3)
</figure>
<page confidence="0.791555">
1014
</page>
<figureCaption confidence="0.999675">
Figure 1: A graphical representation of the interleaved
</figureCaption>
<bodyText confidence="0.934334333333333">
NMF
The factorization that comes out of the NMF model
can be interpreted probabilistically (Gaussier and
Goutte, 2005; Ding et al., 2008). More specifically,
we can transform the factorization into a standard
latent variable model of the form
</bodyText>
<equation confidence="0.998554">
K
p(wi, dj) = p(z)p(wi|z)p(dj|z) (4)
z=1
</equation>
<bodyText confidence="0.975809">
by introducing two K x K diagonal scaling matrices
X and Y, such that Xkk = Ei Wik and Ykk =
Ej Hkj. The factorization WH can then be rewritten
as
</bodyText>
<equation confidence="0.9991795">
WH = (WX−1X)(YY−1H)
= (WX−1)(XY)(Y−1H) (5)
</equation>
<bodyText confidence="0.999787">
such that WX−1 represents p(wi|z), (Y−1H)T rep-
resents p(dj|z), and XY represents p(z). Using
Bayes’ theorem, it is now straightforward to deter-
mine p(z|wi) and p(z|dj).
</bodyText>
<equation confidence="0.98395875">
p(z|wi) = p(wi|z)p(z) (6)
p(wi)
p(z|dj) = p(dj|z)p(z) (7)
p(dj)
</equation>
<subsectionHeader confidence="0.9942405">
3.3 Meaning in Context
3.3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999563846153846">
Using the results of the factorization model de-
scribed above, we can now adapt a word’s feature vec-
tor according to the context in which it appears. Intu-
itively, the contextual features of the word (i.e. the
window-based context words or dependency-based
context features) pinpoint the important semantic di-
mensions of the particular instance, creating a proba-
bility distribution over latent factors. For a number of
context words of a particular instance, we determine
the probability distribution over latent factors given
the context, p(z|C), as the average of the context
words’ probability distributions over latent factors
(equation 8).
</bodyText>
<equation confidence="0.986363">
p(z |C) = EwiEC(8)
|C|
</equation>
<bodyText confidence="0.999975785714286">
The probability distribution over latent factors
given a number of dependency-based context features
can be computed in a similar fashion, replacing wi
with dj. Additionally, this step allows us to combine
both windows-based context words and dependency-
based context features in order to determine the latent
probability distribution (e.g. by taking a linear com-
bination).
The resulting probability distribution over latent
factors can be interpreted as a semantic fingerprint of
the passage in which the target word appears. Using
this fingerprint, we can now determine a new prob-
ability distribution over dependency features given
the context.
</bodyText>
<equation confidence="0.998493">
p(d|C) = p(z|C)p(d|z) (9)
</equation>
<bodyText confidence="0.999899">
The last step is to weight the original probability
vector of the word according to the probability vector
of the dependency features given the word’s context,
by taking the pointwise multiplication of probability
vectors p(d|wi) and p(d|C).
</bodyText>
<equation confidence="0.974137">
p(d|wi, C) = p(d|wi) - p(d|C) (10)
</equation>
<bodyText confidence="0.9999328">
Note that this final step is a crucial one in our ap-
proach. We do not just build a model based on latent
factors, but we use the latent factors to determine
which of the features in the original word vector are
the salient ones given a particular context. This al-
lows us to compute an accurate adaptation of the
original word vector in context.
Also note the resemblance to Mitchell and Lap-
ata’s best scoring vector composition model which,
likewise, uses pointwise multiplication. However,
</bodyText>
<figure confidence="0.881578282051282">
j
k
j
s
C
context words x
dependency relations
k
i
x
=
V
k
s
=
x
U
A
words x
dependency relations
H
s
B
words x
context words
j
s
G
j
F
k
i
=
x
W
k
k
i
i
</figure>
<page confidence="0.918283">
1015
</page>
<bodyText confidence="0.999967111111111">
the model presented here has two advantages. First
of all, it allows to take multiple context features into
account, each of which contributes to the probability
distribution over latent factors. Secondly, the target
word and its features do not need to live in the same
vector space (i.e. they do not need to be defined ac-
cording to the same features), as the connections and
the appropriate weightings are determined through
the latent model.
</bodyText>
<subsectionHeader confidence="0.779484">
3.3.2 Example
</subsectionHeader>
<bodyText confidence="0.99726775">
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun record in the context of example
sentences (3) and (4).
</bodyText>
<listItem confidence="0.999685">
(3) Jack is listening to a record.
(4) Jill updated the record.
</listItem>
<bodyText confidence="0.997616037037037">
First, we extract the context features for both in-
stances, in this case C1 = {listen−1
prep(to)} for sen-
tence (3), and C2 = {update−1
obj} for sentence (4).1
Next, we compute p(z|C1) and p(z|C2) – the proba-
bility distributions over latent factors given the con-
text – by averaging over the latent probability dis-
tributions of the individual context features.2 Using
these probability distributions over latent factors, we
can now determine the probability of each depen-
dency feature given the different contexts – p(d|C1)
and p(d|C2).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears. For the first sen-
tence, features associated with the music sense of
record (or more specifically, the dependency features
associated with latent factors that are related to the
feature {listen−1
prep(to)}) will be emphasized, while
</bodyText>
<footnote confidence="0.9481082">
1In this example we use dependency features, but the compu-
tations are similar for window-based context words.
2In this case, the sets of context features contain only one
item, so the average probability distribution of the sets is just the
latent probability distribution of their respective item.
</footnote>
<bodyText confidence="0.999796909090909">
features associated with unrelated latent factors are
leveled out. For the second sentence, features that
are associated with the administrative sense of record
(dependency features associated with latent factors
that are related to the feature {update−1
obj}) are em-
phasized, while unrelated features are played down.
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of record given the different contexts, which
yields the results presented below.
</bodyText>
<listItem confidence="0.994817">
1. recordN, C1: album, song, recording, track, cd
2. recordN, C2: file, datum, document, database,
list
</listItem>
<sectionHeader confidence="0.998595" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999425">
In this section, we present a thorough evaluation of
the method described above, and compare it with
related methods for meaning computation in context.
In order to test the applicability of the method to
multiple languages, we present evaluation results for
both English and French.
</bodyText>
<subsectionHeader confidence="0.954384">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999975157894737">
For English, we make use of the SEMEVAL 2007 En-
glish Lexical Substitution task (McCarthy and Nav-
igli, 2007; McCarthy and Navigli, 2009). The task’s
goal is to find suitable substitutes for a target word in
a particular context. The complete data set contains
200 target words (about 50 for each part of speech,
viz. nouns, verbs, adjectives, and adverbs). Each
target word occurs in 10 different sentences, which
yields a total of 2000 sentences. Five annotators pro-
vided suitable substitutes for each target word in the
different contexts.
For French, we developed a small-scale lexical sub-
stitution task ourselves, closely following the guide-
lines of the original English task. We manually se-
lected 10 ambiguous French nouns, and for each noun
we selected 10 different sentences from the FRWaC
corpus (Baroni et al., 2009). Four different native
French speakers were then asked to provide suitable
substitutes for the nouns in context.3
</bodyText>
<footnote confidence="0.9996705">
3The task is provided as supplementary material to this paper;
it is also available from the first author’s website.
</footnote>
<page confidence="0.982511">
1016
</page>
<subsectionHeader confidence="0.937392">
4.2 Implementational details
</subsectionHeader>
<bodyText confidence="0.999977973684211">
The model for English has been trained on part of the
UKWaC corpus (Baroni et al., 2009), covering about
500M words. The corpus has been part of speech
tagged and lemmatized with Stanford Part-Of-Speech
Tagger (Toutanova and Manning, 2000; Toutanova
et al., 2003), and parsed with MaltParser (Nivre et
al., 2006) trained on sections 2-21 of the Wall Street
Journal section of the Penn Treebank extended with
about 4000 questions from the QuestionBank4, so
that dependency triples could be extracted. The sen-
tences of the English lexical substitution task have
been tagged, lemmatized and parsed in the same way.
The model for French has been trained on the French
version of Wikipedia (f 100M words), parsed with
the FRMG parser (Villemonte de La Clergerie, 2010)
for French.
For English, we built different models for each
part of speech (nouns, verbs, adjectives and adverbs),
which yields four models in total. For each model, the
matrices needed for our interleaved NMF factoriza-
tion are extracted from the corpus. The noun model,
for example, was built using 5K nouns, 80K depen-
dency relations, and 2K context words5 (excluding
stop words) with highest frequency in the training
set, which yields matrices of 5K nouns x 80K de-
pendency relations, 5K nouns x 2K context words,
and 80K dependency relations x 2K context words.
The models for the three other parts of speech were
constructed in a similar vein. For French, we only
constructed a model for nouns, as our lexical substi-
tution task for French is limited to this part of speech.
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in
the model), and applying 100 iterations.6 The inter-
leaved NMF algorithm was implemented in Matlab;
the preprocessing scripts and scripts for vector com-
putation in context were written in Python. Cosine
was used as a similarity measure.
</bodyText>
<footnote confidence="0.8206225">
4http://maltparser.org/mco/english_
parser/engmalt.html
5We used a fairly large, paragraph-like window of four sen-
tences.
6We experimented with different values (in the range 300–
1500) for K, but the models did not seem to improve much
beyond K = 600; hence, we stuck with 600 factors, due to
speed and memory advantages of a lower number of factors.
</footnote>
<subsectionHeader confidence="0.987067">
4.3 Measures
</subsectionHeader>
<bodyText confidence="0.999851377777778">
Up till now, most researchers have interpreted the
lexical substitution task as a ranking problem, in
which the possible substitutes are given beforehand
and the goal is to rank the substitutes according to
their suitability in a particular context, so that sound
substitutes are given a higher rank than their non-
suitable counterparts. This means that all possible
substitutes for a given target word (extracted from the
gold standard) are lumped together, and the system
then has to produce a ranking for the complete set of
substitutes.
We also adopt this approach in our evaluation
framework, but we complement it with the original
evaluation measures of the lexical substitution task,
in which the system is not given a list of possible sub-
stitutes beforehand, but has to come up with the suit-
able candidates itself. This is a much harder task, but
we believe that such an approach is more compelling
in assessing the system’s ability to induce a proper
meaning representation for word usage in context.
We coin the former approach paraphrase ranking,
and the latter one paraphrase induction. In the next
paragraphs, we will describe the actual evaluation
measures that have been used for both approaches.
Paraphrase ranking Following Dinu and Lapata
(2010), we compare the ranking produced by our
model with the gold standard ranking using Kendall’s
τb (which is adjusted for ties). For reasons of com-
parison, we also compute general average precision
(GAP, Kishida (2005)), which was used by Erk and
Pad´o (2010) and Thater et al. (2010) to evaluate their
rankings. Differences between models are tested for
significance using stratified shuffling (Yeh, 2000),
using a standard number of 10000 iterations.
We compare the results for paraphrase ranking to
two different baselines. The first baseline is a ran-
dom one, in which the gold standard is compared
to an arbitrary ranking. The second baseline is a
dependency-based vector space model that does not
take the context of the particular instance into ac-
count (and thus returns the same ranking for each
instance of the target word). This is a fairly competi-
tive baseline, as noted by other researchers (Erk and
Pad´o, 2008; Thater et al., 2009; Dinu and Lapata,
2010).
</bodyText>
<page confidence="0.991374">
1017
</page>
<bodyText confidence="0.999937615384615">
Paraphrase induction To evaluate the system’s
ability to come up with suitable substitutes from
scratch, we use the measures designed to evaluate
systems that took part in the original English lexical
substitution task (McCarthy and Navigli, 2007). Two
different measures were used, which were coined
best and out-of-ten (oot). The strict best measure
allows the system to give as many candidate substi-
tutes as it considers appropriate, but the credit for
each correct substitute is divided by the total number
of guesses. Recall is then calculated as the average
annotator response frequency of substitutes found by
the system over all items T.
</bodyText>
<equation confidence="0.9994845">
Rbest = EsEMnG f (s) (11)
|M |· EsEG f(s)
</equation>
<bodyText confidence="0.999695">
where M is the system’s candidate list7, G is the gold-
standard data, and f(s) is the annotator response
frequency of the candidate.
The out-of-ten measure is more liberal; it allows
the system to give up to ten substitutes, and the credit
for each correct substitute is not divided by the total
number of guesses. The more liberal measure was
introduced to account for the fact that the lexical
substitution task’s gold standard is susceptible to a
considerate amount of variation, and there is only a
limited number of annotators.
</bodyText>
<equation confidence="0.9979305">
P10 = EsEMnG f (s) (12)
EsEG f(s)
</equation>
<bodyText confidence="0.999635">
where M is the system’s list of 10 candidates, and
G and f(s) are the same as above. Because we only
use the best guess with Rbest, the two measures are
exactly the same except for the number of candidates
</bodyText>
<sectionHeader confidence="0.429541" genericHeader="evaluation">
M.
</sectionHeader>
<subsectionHeader confidence="0.677752">
4.4 Results
4.4.1 English
</subsectionHeader>
<bodyText confidence="0.999932428571428">
Table 1 presents the paraphrase ranking results of
our approach, comparing them to the two baselines
and to a number of previous approaches to meaning
computation in context.
The first two models represent our baselines. The
first baseline is the random baseline, where the can-
didate substitutes are ranked randomly (Tb close to
</bodyText>
<footnote confidence="0.9927835">
7In our evaluations, we calculate best using the system’s best
guess only, so the candidate list contains only one item.
</footnote>
<table confidence="0.9983714">
model Tb GAP
random -0.61 29.98
vectordep 16.57 45.08
EP09 – 32.2 •
EP10 – 39.9 •
TFP 45.94•
DL 16.56 41.68
NMFcontext 20.64** 47.60**
NMFdep 22.49** 48.97**
NMFc+d 22.59** 49.02**
</table>
<tableCaption confidence="0.950366">
Table 1: Kendall’s Tb and GAP paraphrase ranking scores
for the English lexical substitution task. Scores marked
with ‘•’ are copied from the authors’ respective papers.
Scores marked with ‘**’ are statistically significant with
p &lt; 0.01 compared to the second baseline.
</tableCaption>
<bodyText confidence="0.999407814814815">
zero indicates that there is no correlation). The sec-
ond baseline is a standard dependency-based vector
space model, which yields the same ranking for all
instances of a target word. Note that the second base-
line is a rather competitive one.
The next four models represent previous ap-
proaches to meaning computation in context. EP09
is Erk and Pado’s (2009) selectional preference ap-
proach; EP10 is Erk and Pado’s (2010) exemplar-
based approach; TFP stands for Thater et al.’s (2010)
approach; and DL is Dinu and Lapata’s (2010) latent
modeling approach. The results are reproduced from
their respective papers, except for Dinu and Lapata’s
approach, which we reimplemented ourselves.8 Note
that the reproduced results (EP09, EP10 and TFP) are
not entirely comparable, because the authors only use
a subset of the lexical substitution task.
The last three models are instantiations of our ap-
proach: NMFcontext is a model that uses window-
based context features, NMFdep is a model that uses
dependency-based context features, and NMFc+d is
a model that uses a linear combination of window-
based and dependency-based context features, giving
equal weight to both.
The three instantiations of our approach reach bet-
ter results than all previous approaches. Moreover,
our approach is the only one able to significantly
</bodyText>
<footnote confidence="0.9984005">
8The original paper reports a slightly lower τb of 16.01 for
their best scoring model.
</footnote>
<page confidence="0.996624">
1018
</page>
<bodyText confidence="0.999317272727273">
beat our second (competitive) baseline of a stan-
dard dependency-based vector model. Comparing
our three instantiations, the model that combines
window-based context and dependency-based con-
text scores best, closely followed by the dependency-
based model. The model that only uses window-
based context gets the lowest score of the three, but
is still fairly competitive compared to the previous
approaches. The differences between the models are
statistically significant (p &lt; 0.01), except for the
difference between NMFdep and NMFc+d.
</bodyText>
<table confidence="0.8722742">
model n v a r
vectordep 15.85 11.68 16.71 25.29
NMFcontext 20.58 16.24 21.00 27.22
NMFdep 21.96 17.33 24.57 28.16
NMFc+d 22.68 17.47 23.84 28.66
</table>
<tableCaption confidence="0.966011666666667">
Table 2: Kendall’s Tb paraphrase ranking scores for the
English lexical substitution task across different parts of
speech
</tableCaption>
<bodyText confidence="0.997045444444444">
Table 2 shows the performance of the three model
instantiations on paraphrase ranking across different
parts of speech. The results largely confirm tenden-
cies reported by other researchers (cfr. Dinu and
Lapata (2010)), viz. that verbs are the most difficult,
followed by nouns and adjectives. These parts of
speech also benefit the most from the use of a contex-
tualized model. Adverbs are easier, but there is less
to be gained from using contextualized models.
</bodyText>
<table confidence="0.99934675">
model Rbest P10
vectordep 8.78 30.21
DL 1.06 7.59
KU 20.65 46.15
IRST2 20.33 68.90
NMFcontext 8.81 30.49
NMFdep 7.73 26.92
NMFc+d 8.96 29.26
</table>
<tableCaption confidence="0.8868395">
Table 3: Rbest and Plo paraphrase induction scores for
the English lexical substitution task
</tableCaption>
<bodyText confidence="0.955589914285714">
Table 3 shows the performance of the different
models on the paraphrase induction task. Note
once again that our baseline vectordep – a simple
dependency-based vector space model – is a highly
competitive one. NMFcontext and NMFc+d are able to
reach marginally better results, but the differences are
not statistically significant. However, all of our mod-
els are able to reach much better results than Dinu
and Lapata’s approach. The results indicate that our
approach, after vector adaptation in context, is still
able to provide accurate similarity calculations across
the complete word space. While other algorithms are
able to rank candidate substitutes at the expense of
accurate similarity calculations, our approach is able
to do both. This is one of the important advantages
of our approach.
For reasons of comparison, we also included the
scores of the best performing models that partici-
pated in the SEMEVAL 2007 lexical substitution task
(KU (Yuret, 2007) and IRST2 (Giuliano et al., 2007),
which got the best scores for Rbest and P10, respec-
tively). These models reach better scores compared
to our models. Note, however, that all participants
of the SEMEVAL 2007 lexical substitution task relied
on a predefined sense inventory (i.e. WordNet, or
a machine readable thesaurus). Our system, on the
other hand, induces paraphrases in a fully unsuper-
vised way. To our knowledge, this is the first time a
fully unsupervised system is tested on the paraphrase
induction task.
model n v a r
vectordep 31.66 23.53 29.91 38.43
NMFcontext 33.73** 25.21* 28.58 36.45
NMFdep 31.40 25.97** 20.56 31.48
NMFc+d 33.37* 25.99** 24.20 35.81
</bodyText>
<tableCaption confidence="0.882803">
Table 4: Plo paraphrase induction scores for the English
lexical substitution task across different parts of speech.
Scores marked with ‘**’ and ‘*’ are statistically significant
</tableCaption>
<bodyText confidence="0.916454222222222">
with respectively p &lt; 0.01 and p &lt; 0.05 compared to the
baseline.
Table 4 presents the results for paraphrase induc-
tion (oot) across the different parts of speech. The
results indicate that paraphrase induction works best
for nouns and verbs, with statistically significant im-
provements over the baseline. The differences among
the models themselves are not significant. Adjectives
and adverbs yield lower scores, indicating that their
</bodyText>
<page confidence="0.993724">
1019
</page>
<bodyText confidence="0.9998432">
contextualization yields less precise vectors for mean-
ing computation. Note, however, that the NMFcontext
model is still quite apt for meaning computation,
yielding results that are only slightly lower than the
dependency-based vector space model.
</bodyText>
<subsectionHeader confidence="0.789755">
4.4.2 French
</subsectionHeader>
<bodyText confidence="0.9998115">
This section presents the results on the French lex-
ical substitution task. Table 5 presents the results for
paraphrase ranking, while table 6 shows the models’
performance on the paraphrase induction task.
</bodyText>
<figure confidence="0.7260175">
model Kendall’s Tb GAP
vectordep 7.79 36.46
DL 17.99 41.73
NMFcontext 18.63 44.96
NMFdep 17.15 44.66
NMFc+d 18.40 43.14
</figure>
<tableCaption confidence="0.906419">
Table 5: Kendall’s Tb and GAP paraphrase ranking scores
for the French lexical substitution task
</tableCaption>
<bodyText confidence="0.999943769230769">
The results for paraphrase ranking in French (ta-
ble 5) show similar tendencies as the results for En-
glish: all of our models are able to improve signifi-
cantly over the dependency-based vector space base-
line. Note, however, thar our models generally score
a bit lower compared to the English results. This drop
in performance is not present for Dinu and Lapata’s
model. The difference might be due to the differ-
ence in corpora size: for the method to operate at full
power, we need to make a good estimate of the co-
occurrences of three modes (words, window-based
context words and dependency-based features), and
thus our methods requires a significant amount of
data. Nevertheless, our approach still yields the best
results, with NMFcontext as the best scoring model.
Finally, the results for paraphrase induction in
French (table 6) interestingly show a significant and
large improvement over the baseline. The improve-
ments indicate once again that the models are able
to carry out precise similarity computations over the
whole word space, while at the same time providing
an adequately adapted contextualized meaning vector.
Dinu and Lapata’s model, which performs similarity
calculations in the latent space, is not able to provide
accurate word vectors, and thus perform worse at the
paraphrase induction task.
</bodyText>
<table confidence="0.999006166666667">
model Rbest P10
vectordep 6.38 24.43
DL 0.50 5.34
NMFcontext 10.71 31.42
NMFdep 9.65 28.52
NMFc+d 10.64 35.32
</table>
<tableCaption confidence="0.779736">
Table 6: Rbest and P10 paraphrase induction scores for
the French lexical substitution task
</tableCaption>
<sectionHeader confidence="0.998452" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990142857143">
In this paper, we presented a novel method for the
modeling of word meaning in context. We make use
of a factorization model based on non-negative ma-
trix factorization, in which words, together with their
window-based context words and their dependency
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which particular
dimensions are important for a target word in a partic-
ular context. A key feature of the algorithm is that we
adapt the original dependency-based feature vector
of the target word through the latent semantic space.
By doing so, our model is able to make accurate simi-
larity calculations for word meaning in context across
the whole word space. Our evaluation shows that the
approach presented here is able to improve upon the
state-of-the art performance on paraphrase ranking.
Moreover, our approach scores well for both para-
phrase ranking and paraphrase induction, whereas
previous approaches only seem capable of improving
performance on the former task at the expense of the
latter.
During our research, a number of topics surfaced
that we consider worth exploring in the future. First
of all, we would like to further investigate the opti-
mal configuration for combining window-based and
dependency-based contexts. At the moment, the per-
formance of the combined model does not yield a
uniform picture. The results might improve further
if window-based context and dependency-based con-
text are combined in an optimal way. Secondly, we
would like to subject our approach to further evalu-
ation, in particular on a number of different evalua-
tion tasks, such as semantic compositionality. And
thirdly, we would like to transfer the general idea
of the approach presented in this paper to a tensor-
</bodyText>
<page confidence="0.976774">
1020
</page>
<bodyText confidence="0.999636833333333">
based framework (which is able to capture the multi-
way co-occurrences of words, together with their
window-based and dependency-based context fea-
tures, in a natural way) and investigate whether such
a framework proves beneficial for the modeling of
word meaning in context.
</bodyText>
<sectionHeader confidence="0.996406" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99848925">
The work reported in this paper was funded by
the Isaac Newton Trust (Cambridge, UK), the
EU FP7 project ‘PANACEA’, the EPSRC grant
EP/G051070/1 and the Royal Society (UK).
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736333333333">
Eneko Agirre, David Mart´ınez, Oier L´opez de Lacalle, and
Aitor Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In Proceedings of the Empirical
Methods in Natural Language Processing (EMNLP)
Conference, pages 585–593, Sydney, Australia.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209–226.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics &amp; Data Analysis, 52(8):3913–3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172, Cambridge,
MA, October.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 440–449, Suntec, Singapore.
Katrin Erk and Sebastian Pad´o. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897–906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pad´o. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57–65, Athens, Greece.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based
models for word meaning in context. In Proceedings of
the ACL 2010 Conference Short Papers, pages 92–97,
Uppsala, Sweden.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009.
Investigations on word senses and word usages. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 10–18.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?). In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 17–26.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601–602, Salvador, Brazil.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations, pages 145–148.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57–60, New York, New York, USA.
Nancy Ide and Yorick Wilks. 2006. Making Sense About
Sense. In Word Sense Disambiguation: Algorithms
And Applications, chapter 3. Springer, Dordrecht.
Kazuaki Kishida. 2005. Property of average precision and
its generalization: An examination of evaluation indi-
cator for information retrieval experiments. Technical
report, National Institute of Informatics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato’s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211–240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295–284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556–
562.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics (COLING-ACL98), Volume 2, pages 768–
774, Montreal, Quebec, Canada.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
</reference>
<page confidence="0.807035">
1021
</page>
<reference confidence="0.999140410714286">
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48–53.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139–159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236–244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216–
2219.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613–619, Edmonton, Alberta, Canada.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44–47, Suntec, Singapore.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948–957, Uppsala, Sweden.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63–70.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of HLT-NAACL 2003, pages 252–259.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929–936, Manchester.
Eric Villemonte de La Clergerie. 2010. Building factor-
ized TAGs with meta-grammars. In Proceedings of
the 10th International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
111–118, New Haven, Connecticut, USA.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947–953, Saarbr¨ucken, Germany.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 207–213.
</reference>
<page confidence="0.994089">
1022
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511339">
<title confidence="0.999224">Latent Vector Weighting for Word Meaning in Context</title>
<author confidence="0.999984">Tim Van_de_Cruys Thierry Poibeau Anna Korhonen</author>
<affiliation confidence="0.9463225">RCEAL LaTTiCe, UMR8094 Computer Laboratory &amp; RCEAL University of Cambridge CNRS &amp; ENS University of</affiliation>
<email confidence="0.613303">tv234@cam.ac.ukthierry.poibeau@ens.fralk23@cam.ac.uk</email>
<abstract confidence="0.995432411764706">This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task – carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Mart´ınez</author>
<author>Oier L´opez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art wsd.</title>
<date>2006</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP) Conference,</booktitle>
<pages>585--593</pages>
<location>Sydney, Australia.</location>
<marker>Agirre, Mart´ınez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Mart´ınez, Oier L´opez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for state-of-the-art wsd. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP) Conference, pages 585–593, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="19388" citStr="Baroni et al., 2009" startWordPosition="3118" endWordPosition="3121">n a particular context. The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs). Each target word occurs in 10 different sentences, which yields a total of 2000 sentences. Five annotators provided suitable substitutes for each target word in the different contexts. For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task. We manually selected 10 ambiguous French nouns, and for each noun we selected 10 different sentences from the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3The task is provided as supplementary material to this paper; it is also available from the first author’s website. 1016 4.2 Implementational details The model for English has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
</authors>
<title>On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing.</title>
<date>2008</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>52</volume>
<issue>8</issue>
<contexts>
<context position="12557" citStr="Ding et al., 2008" startWordPosition="1999" endWordPosition="2002">erleaved (i.e. the results of the former factorization are used to initialize the factorization of the next matrix). A graphical representation of the interleaved factorization algorithm is given in figure 1. When the factorization is finished, the three different modes (words, window-based context words and dependency-based features) are all represented according to a limited number of latent factors. Wia ← Wia (3) 1014 Figure 1: A graphical representation of the interleaved NMF The factorization that comes out of the NMF model can be interpreted probabilistically (Gaussier and Goutte, 2005; Ding et al., 2008). More specifically, we can transform the factorization into a standard latent variable model of the form K p(wi, dj) = p(z)p(wi|z)p(dj|z) (4) z=1 by introducing two K x K diagonal scaling matrices X and Y, such that Xkk = Ei Wik and Ykk = Ej Hkj. The factorization WH can then be rewritten as WH = (WX−1X)(YY−1H) = (WX−1)(XY)(Y−1H) (5) such that WX−1 represents p(wi|z), (Y−1H)T represents p(dj|z), and XY represents p(z). Using Bayes’ theorem, it is now straightforward to determine p(z|wi) and p(z|dj). p(z|wi) = p(wi|z)p(z) (6) p(wi) p(z|dj) = p(dj|z)p(z) (7) p(dj) 3.3 Meaning in Context 3.3.1 O</context>
</contexts>
<marker>Ding, Li, Peng, 2008</marker>
<rawString>Chris Ding, Tao Li, and Wei Peng. 2008. On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. Computational Statistics &amp; Data Analysis, 52(8):3913–3927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="2885" citStr="Dinu and Lapata, 2010" startWordPosition="470" endWordPosition="473">nventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in which words, together with their window-based context words and their dependency 1012 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022, Edinburgh, Scotland, UK,</context>
<context position="8149" citStr="Dinu and Lapata (2010)" startWordPosition="1315" endWordPosition="1318"> co-occurrences to act as a filter upon the second-order vector space, which allows for the computation of meaning in context. Erk and Pad´o (2010) propose an exemplar-based approach, in which the meaning of a word in context is represented by the activated exemplars that are most similar to it. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors. Their results indicate that a model based on pointwise multiplication achieves better results than models based on vector addition. Finally, Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words 1013 as a probability distribution over latent dimensions (‘senses’). Contextualized meaning is then modeled as a change in the original sense distribution. The model presented in this paper bears some resemblances to their approach; however, while their approach computes the contextualized meaning directly within the latent space, our model exploits the latent space to determine the features that are important for a particular context, and adapt the original (outof-context) dependency-based feature vector of the target word a</context>
<context position="23155" citStr="Dinu and Lapata (2010)" startWordPosition="3733" endWordPosition="3736">uation measures of the lexical substitution task, in which the system is not given a list of possible substitutes beforehand, but has to come up with the suitable candidates itself. This is a much harder task, but we believe that such an approach is more compelling in assessing the system’s ability to induce a proper meaning representation for word usage in context. We coin the former approach paraphrase ranking, and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared </context>
<context position="28941" citStr="Dinu and Lapata (2010)" startWordPosition="4675" endWordPosition="4678">aches. The differences between the models are statistically significant (p &lt; 0.01), except for the difference between NMFdep and NMFc+d. model n v a r vectordep 15.85 11.68 16.71 25.29 NMFcontext 20.58 16.24 21.00 27.22 NMFdep 21.96 17.33 24.57 28.16 NMFc+d 22.68 17.47 23.84 28.66 Table 2: Kendall’s Tb paraphrase ranking scores for the English lexical substitution task across different parts of speech Table 2 shows the performance of the three model instantiations on paraphrase ranking across different parts of speech. The results largely confirm tendencies reported by other researchers (cfr. Dinu and Lapata (2010)), viz. that verbs are the most difficult, followed by nouns and adjectives. These parts of speech also benefit the most from the use of a contextualized model. Adverbs are easier, but there is less to be gained from using contextualized models. model Rbest P10 vectordep 8.78 30.21 DL 1.06 7.59 KU 20.65 46.15 IRST2 20.33 68.90 NMFcontext 8.81 30.49 NMFdep 7.73 26.92 NMFc+d 8.96 29.26 Table 3: Rbest and Plo paraphrase induction scores for the English lexical substitution task Table 3 shows the performance of the different models on the paraphrase induction task. Note once again that our baselin</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded word sense assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>440--449</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2674" citStr="Erk and McCarthy, 2009" startWordPosition="434" endWordPosition="437">unity’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in whic</context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded word sense assignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 440–449, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Waikiki, Hawaii, USA.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897–906, Waikiki, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>57--65</pages>
<location>Athens, Greece.</location>
<marker>Erk, Pad´o, 2009</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 57–65, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<location>Uppsala,</location>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="2426" citStr="Erk et al., 2009" startWordPosition="398" endWordPosition="401">ay for equal work! The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a p</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>What is word meaning, really? (and how can distributional models help us describe it?).</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>17--26</pages>
<contexts>
<context position="2438" citStr="Erk, 2010" startWordPosition="402" endWordPosition="403"> The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined s</context>
</contexts>
<marker>Erk, 2010</marker>
<rawString>Katrin Erk. 2010. What is word meaning, really? (and how can distributional models help us describe it?). In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 17–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
</authors>
<title>Relation between PLSA and NMF and implications.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>601--602</pages>
<location>Salvador, Brazil.</location>
<contexts>
<context position="12537" citStr="Gaussier and Goutte, 2005" startWordPosition="1995" endWordPosition="1998">rate factorizations are interleaved (i.e. the results of the former factorization are used to initialize the factorization of the next matrix). A graphical representation of the interleaved factorization algorithm is given in figure 1. When the factorization is finished, the three different modes (words, window-based context words and dependency-based features) are all represented according to a limited number of latent factors. Wia ← Wia (3) 1014 Figure 1: A graphical representation of the interleaved NMF The factorization that comes out of the NMF model can be interpreted probabilistically (Gaussier and Goutte, 2005; Ding et al., 2008). More specifically, we can transform the factorization into a standard latent variable model of the form K p(wi, dj) = p(z)p(wi|z)p(dj|z) (4) z=1 by introducing two K x K diagonal scaling matrices X and Y, such that Xkk = Ei Wik and Ykk = Ej Hkj. The factorization WH can then be rewritten as WH = (WX−1X)(YY−1H) = (WX−1)(XY)(Y−1H) (5) such that WX−1 represents p(wi|z), (Y−1H)T represents p(dj|z), and XY represents p(z). Using Bayes’ theorem, it is now straightforward to determine p(z|wi) and p(z|dj). p(z|wi) = p(wi|z)p(z) (6) p(wi) p(z|dj) = p(dj|z)p(z) (7) p(dj) 3.3 Meanin</context>
</contexts>
<marker>Gaussier, Goutte, 2005</marker>
<rawString>Eric Gaussier and Cyril Goutte. 2005. Relation between PLSA and NMF and implications. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 601–602, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>145--148</pages>
<contexts>
<context position="30421" citStr="Giuliano et al., 2007" startWordPosition="4914" endWordPosition="4917">uch better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST2 (Giuliano et al., 2007), which got the best scores for Rbest and P10, respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMFcontext 33.73** 25.21* 28.58 36.45 NMFdep 31.40 25.97** 20.56 31.4</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2007</marker>
<rawString>Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava. 2007. Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 145–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1056" citStr="Harris, 1954" startWordPosition="158" endWordPosition="159">ontext words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task – carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations. 1 Introduction According to the distributional hypothesis of meaning (Harris, 1954), words that occur in similar contexts tend to be semantically similar. In the spirit of this by now well-known adage, numerous algorithms have sprouted up that try to capture the semantics of words by looking at their distribution in texts, and comparing those distributions in a vector space model. Up till now, the majority of computational approaches to semantic similarity represent the meaning of a word as the aggregate of the word’s contexts, and hence do not differentiate between the different senses of a word. The meaning of a word, however, is largely dependent on the particular context</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>57--60</pages>
<location>New York, New York, USA.</location>
<contexts>
<context position="2621" citStr="Hovy et al., 2006" startWordPosition="426" endWordPosition="429"> carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be </context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, pages 57–60, New York, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Making Sense About Sense. In Word Sense Disambiguation: Algorithms And Applications, chapter 3.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2408" citStr="Ide and Wilks, 2006" startWordPosition="394" endWordPosition="397">art brut. (2) Equal pay for equal work! The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the</context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks. 2006. Making Sense About Sense. In Word Sense Disambiguation: Algorithms And Applications, chapter 3. Springer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>National Institute of Informatics.</institution>
<contexts>
<context position="23371" citStr="Kishida (2005)" startWordPosition="3770" endWordPosition="3771"> that such an approach is more compelling in assessing the system’s ability to induce a proper meaning representation for word usage in context. We coin the former approach paraphrase ranking, and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments. Technical report, National Institute of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychology Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="4672" citStr="Landauer and Dumais, 1997" startWordPosition="756" endWordPosition="759">The remainder of this paper is organized as follows. In section 2, we present some earlier work that is related to the research presented here. Section 3 describes the methodology of our method, focusing on the factorization model, and the computation of meaning in context. Section 4 presents a thorough evaluation on a lexical substitution task, both for English and French. The last section then draws conclusions, and presents a number of topics that deserve further exploration. 2 Related work One of the best known computational models of semantic similarity is latent semantic analysis — LSA (Landauer and Dumais, 1997; Landauer et al., 1998). In LSA, a term-document matrix is created, that contains the frequency of each word in a particular document. This matrix is then decomposed into three other matrices with a mathematical factorization technique called singular value decomposition (SVD). The most important dimensions that come out of the SVD are said to represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. The nature of a word’s</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychology Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--295</pages>
<contexts>
<context position="4696" citStr="Landauer et al., 1998" startWordPosition="760" endWordPosition="763"> is organized as follows. In section 2, we present some earlier work that is related to the research presented here. Section 3 describes the methodology of our method, focusing on the factorization model, and the computation of meaning in context. Section 4 presents a thorough evaluation on a lexical substitution task, both for English and French. The last section then draws conclusions, and presents a number of topics that deserve further exploration. 2 Related work One of the best known computational models of semantic similarity is latent semantic analysis — LSA (Landauer and Dumais, 1997; Landauer et al., 1998). In LSA, a term-document matrix is created, that contains the frequency of each word in a particular document. This matrix is then decomposed into three other matrices with a mathematical factorization technique called singular value decomposition (SVD). The most important dimensions that come out of the SVD are said to represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. The nature of a word’s context is a determinin</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter Foltz, and Darrell Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25:295–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="9137" citStr="Lee and Seung, 2000" startWordPosition="1462" endWordPosition="1465">directly within the latent space, our model exploits the latent space to determine the features that are important for a particular context, and adapt the original (outof-context) dependency-based feature vector of the target word accordingly. This allows for a more precise and more distinct computation of word meaning in context. Secondly, Dinu and Lapata use windowbased context features to build their latent model, while our approach combines both window-based and dependency-based features. 3 Methodology 3.1 Non-negative Matrix Factorization Our model uses non-negative matrix factorization (Lee and Seung, 2000) in order to find latent dimensions. There are a number of reasons to prefer NMF over the better known singular value decomposition used in LSA. First of all, NMF allows us to minimize the Kullback-Leibler divergence as an objective function, whereas SVD minimizes the Euclidean distance. The Kullback-Leibler divergence is better suited for language phenomena. Minimizing the Euclidean distance requires normally distributed data, and language phenomena are typically not normally distributed. Secondly, the non-negative nature of the factorization ensures that only additive and no subtractive rela</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556– 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL98),</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="5573" citStr="Lin (1998)" startWordPosition="903" endWordPosition="904">portant dimensions that come out of the SVD are said to represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. The nature of a word’s context is a determining factor in the kind of the semantic similarity that is induced. A broad context window (e.g. a paragraph or document) yields broad, topical similarity, whereas a small context window yields tight, synonym-like similarity. This has lead a number of researchers (e.g. Lin (1998)) to use the dependency relations that a particular word takes part in as context features. An overview of dependency-based semantic space models is given in Pad´o and Lapata (2007). A number of researchers have exploited the notion of context to differentiate between the different senses of a word in an unsupervised way (a task labeled word sense induction or WSI). Sch¨utze (1998) proposed a context-clustering approach, in which context vectors are created for the different instances of a particular word, and those contexts are grouped into a number of clusters, representing the different sen</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL98), Volume 2, pages 768– 774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<contexts>
<context position="18670" citStr="McCarthy and Navigli, 2007" startWordPosition="3001" endWordPosition="3005">the two adapted vectors of record given the different contexts, which yields the results presented below. 1. recordN, C1: album, song, recording, track, cd 2. recordN, C2: file, datum, document, database, list 4 Evaluation In this section, we present a thorough evaluation of the method described above, and compare it with related methods for meaning computation in context. In order to test the applicability of the method to multiple languages, we present evaluation results for both English and French. 4.1 Datasets For English, we make use of the SEMEVAL 2007 English Lexical Substitution task (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). The task’s goal is to find suitable substitutes for a target word in a particular context. The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs). Each target word occurs in 10 different sentences, which yields a total of 2000 sentences. Five annotators provided suitable substitutes for each target word in the different contexts. For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task. We manually selected 10 am</context>
<context position="24374" citStr="McCarthy and Navigli, 2007" startWordPosition="3931" endWordPosition="3934">ompared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the target word). This is a fairly competitive baseline, as noted by other researchers (Erk and Pad´o, 2008; Thater et al., 2009; Dinu and Lapata, 2010). 1017 Paraphrase induction To evaluate the system’s ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). Two different measures were used, which were coined best and out-of-ten (oot). The strict best measure allows the system to give as many candidate substitutes as it considers appropriate, but the credit for each correct substitute is divided by the total number of guesses. Recall is then calculated as the average annotator response frequency of substitutes found by the system over all items T. Rbest = EsEMnG f (s) (11) |M |· EsEG f(s) where M is the system’s candidate list7, G is the goldstandard data, and f(s) is the annotator response frequency of the candidate. The out-of-ten measure is m</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--2</pages>
<contexts>
<context position="18699" citStr="McCarthy and Navigli, 2009" startWordPosition="3006" endWordPosition="3009">ecord given the different contexts, which yields the results presented below. 1. recordN, C1: album, song, recording, track, cd 2. recordN, C2: file, datum, document, database, list 4 Evaluation In this section, we present a thorough evaluation of the method described above, and compare it with related methods for meaning computation in context. In order to test the applicability of the method to multiple languages, we present evaluation results for both English and French. 4.1 Datasets For English, we make use of the SEMEVAL 2007 English Lexical Substitution task (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). The task’s goal is to find suitable substitutes for a target word in a particular context. The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs). Each target word occurs in 10 different sentences, which yields a total of 2000 sentences. Five annotators provided suitable substitutes for each target word in the different contexts. For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task. We manually selected 10 ambiguous French nouns, and for</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language resources and evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition. proceedings of ACL08: HLT,</title>
<date>2008</date>
<pages>236--244</pages>
<contexts>
<context position="7854" citStr="Mitchell and Lapata (2008)" startWordPosition="1271" endWordPosition="1274">ltiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009) and Thater et al. (2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model; their model allows first-order co-occurrences to act as a filter upon the second-order vector space, which allows for the computation of meaning in context. Erk and Pad´o (2010) propose an exemplar-based approach, in which the meaning of a word in context is represented by the activated exemplars that are most similar to it. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors. Their results indicate that a model based on pointwise multiplication achieves better results than models based on vector addition. Finally, Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words 1013 as a probability distribution over latent dimensions (‘senses’). Contextualized meaning is then modeled as a change in the original sense distribution. The model presented in this paper bears some resemblances to their approach; how</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. proceedings of ACL08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="19966" citStr="Nivre et al., 2006" startWordPosition="3209" endWordPosition="3212">om the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3The task is provided as supplementary material to this paper; it is also available from the first author’s website. 1016 4.2 Implementational details The model for English has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4, so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (f 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjectives and adverbs), which yields four models in total</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216– 2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="2739" citStr="Pantel and Lin, 2002" startWordPosition="444" endWordPosition="447">e flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in which words, together with their window-based context words and their</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 613–619, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Georgiana Dinu</author>
<author>Manfred Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="7357" citStr="Thater et al. (2009)" startWordPosition="1192" endWordPosition="1195">t word senses. Our approach makes use of a similar factorization model, but we extend the approach with a probabilistic framework that is able to adapt the original vector according to the context of the instance. Recently, a number of models emerged that aim to model the individual meaning of words in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009) and Thater et al. (2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model; their model allows first-order co-occurrences to act as a filter upon the second-order vector space, which allows for the computation of meaning in context. Erk and Pad´o (2010) propose an exemplar-based approach, in which the meaning of a word in context is represented by the activated exemplars that are most similar to it. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to comb</context>
<context position="24096" citStr="Thater et al., 2009" startWordPosition="3889" endWordPosition="3892"> between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the target word). This is a fairly competitive baseline, as noted by other researchers (Erk and Pad´o, 2008; Thater et al., 2009; Dinu and Lapata, 2010). 1017 Paraphrase induction To evaluate the system’s ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). Two different measures were used, which were coined best and out-of-ten (oot). The strict best measure allows the system to give as many candidate substitutes as it considers appropriate, but the credit for each correct substitute is divided by the total number of guesses. Recall is then calculated as the average annot</context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44–47, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="19892" citStr="Toutanova and Manning, 2000" startWordPosition="3197" endWordPosition="3200">10 ambiguous French nouns, and for each noun we selected 10 different sentences from the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3The task is provided as supplementary material to this paper; it is also available from the first author’s website. 1016 4.2 Implementational details The model for English has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4, so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (f 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="19917" citStr="Toutanova et al., 2003" startWordPosition="3201" endWordPosition="3204">d for each noun we selected 10 different sentences from the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3The task is provided as supplementary material to this paper; it is also available from the first author’s website. 1016 4.2 Implementational details The model for English has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4, so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (f 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjective</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>Using three way data for word sense discrimination.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>929--936</pages>
<location>Manchester.</location>
<marker>Van de Cruys, 2008</marker>
<rawString>Tim Van de Cruys. 2008. Using three way data for word sense discrimination. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 929–936, Manchester.</rawString>
</citation>
<citation valid="true">
<title>Eric Villemonte de La Clergerie.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+10),</booktitle>
<pages>111--118</pages>
<location>New Haven, Connecticut, USA.</location>
<contexts>
<context position="7382" citStr="(2010)" startWordPosition="1200" endWordPosition="1200">f a similar factorization model, but we extend the approach with a probabilistic framework that is able to adapt the original vector according to the context of the instance. Recently, a number of models emerged that aim to model the individual meaning of words in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009) and Thater et al. (2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model; their model allows first-order co-occurrences to act as a filter upon the second-order vector space, which allows for the computation of meaning in context. Erk and Pad´o (2010) propose an exemplar-based approach, in which the meaning of a word in context is represented by the activated exemplars that are most similar to it. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vecto</context>
<context position="23155" citStr="(2010)" startWordPosition="3736" endWordPosition="3736">of the lexical substitution task, in which the system is not given a list of possible substitutes beforehand, but has to come up with the suitable candidates itself. This is a much harder task, but we believe that such an approach is more compelling in assessing the system’s ability to induce a proper meaning representation for word usage in context. We coin the former approach paraphrase ranking, and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared </context>
<context position="26938" citStr="(2010)" startWordPosition="4368" endWordPosition="4368">itution task. Scores marked with ‘•’ are copied from the authors’ respective papers. Scores marked with ‘**’ are statistically significant with p &lt; 0.01 compared to the second baseline. zero indicates that there is no correlation). The second baseline is a standard dependency-based vector space model, which yields the same ranking for all instances of a target word. Note that the second baseline is a rather competitive one. The next four models represent previous approaches to meaning computation in context. EP09 is Erk and Pado’s (2009) selectional preference approach; EP10 is Erk and Pado’s (2010) exemplarbased approach; TFP stands for Thater et al.’s (2010) approach; and DL is Dinu and Lapata’s (2010) latent modeling approach. The results are reproduced from their respective papers, except for Dinu and Lapata’s approach, which we reimplemented ourselves.8 Note that the reproduced results (EP09, EP10 and TFP) are not entirely comparable, because the authors only use a subset of the lexical substitution task. The last three models are instantiations of our approach: NMFcontext is a model that uses windowbased context features, NMFdep is a model that uses dependency-based context feature</context>
<context position="28941" citStr="(2010)" startWordPosition="4678" endWordPosition="4678">rences between the models are statistically significant (p &lt; 0.01), except for the difference between NMFdep and NMFc+d. model n v a r vectordep 15.85 11.68 16.71 25.29 NMFcontext 20.58 16.24 21.00 27.22 NMFdep 21.96 17.33 24.57 28.16 NMFc+d 22.68 17.47 23.84 28.66 Table 2: Kendall’s Tb paraphrase ranking scores for the English lexical substitution task across different parts of speech Table 2 shows the performance of the three model instantiations on paraphrase ranking across different parts of speech. The results largely confirm tendencies reported by other researchers (cfr. Dinu and Lapata (2010)), viz. that verbs are the most difficult, followed by nouns and adjectives. These parts of speech also benefit the most from the use of a contextualized model. Adverbs are easier, but there is less to be gained from using contextualized models. model Rbest P10 vectordep 8.78 30.21 DL 1.06 7.59 KU 20.65 46.15 IRST2 20.33 68.90 NMFcontext 8.81 30.49 NMFdep 7.73 26.92 NMFc+d 8.96 29.26 Table 3: Rbest and Plo paraphrase induction scores for the English lexical substitution task Table 3 shows the performance of the different models on the paraphrase induction task. Note once again that our baselin</context>
</contexts>
<marker>2010</marker>
<rawString>Eric Villemonte de La Clergerie. 2010. Building factorized TAGs with meta-grammars. In Proceedings of the 10th International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+10), pages 111–118, New Haven, Connecticut, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="23559" citStr="Yeh, 2000" startWordPosition="3799" endWordPosition="3800"> and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the target word). This is a fairly competitive baseline, as noted by other researchers (Erk and Pad´o, 2008; Thater et al., 2009; Dinu and Lapata, 2010). 1017 Paraphrase induction To evaluate</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics, pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Ku: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>207--213</pages>
<contexts>
<context position="30387" citStr="Yuret, 2007" startWordPosition="4910" endWordPosition="4911">dels are able to reach much better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST2 (Giuliano et al., 2007), which got the best scores for Rbest and P10, respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMFcontext 33.73** 25.21* 28.58 36.</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. Ku: Word sense disambiguation by substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 207–213.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>