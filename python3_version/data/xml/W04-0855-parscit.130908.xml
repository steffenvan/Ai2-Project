<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007712">
<note confidence="0.767088">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
UBB system at Senseval3
</note>
<author confidence="0.9874">
Gabriela Serban
</author>
<affiliation confidence="0.847400666666667">
Department of Computer Science
University ”Babes-Bolyai”
Romania
</affiliation>
<email confidence="0.989832">
gabis@cs.ubbcluj.ro
</email>
<author confidence="0.986064">
Doina Tatar
</author>
<affiliation confidence="0.847164">
Department of Computer Science
University ”Babes-Bolyai”
Romania
</affiliation>
<email confidence="0.988067">
dtatar@cs.ubbcluj.ro
</email>
<sectionHeader confidence="0.99643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799">
It is known that whenever a system’s actions
depend on the meaning of the text being pro-
cessed, disambiguation is beneficial or even nec-
essary. The contest Senseval is an international
frame where the research in this important field
is validated in an hierarchical manner. In this
paper we present our system participating for
the first time at Senseval 3 contest on WSD,
contest developed in March-April 2004. We
present also our intentions on improving our
system, intentions occurred from the study of
results.
</bodyText>
<sectionHeader confidence="0.998312" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997455263157895">
Word Sense Disambiguation (WSD) is the pro-
cess of identifying the correct meanings of words
in particular contexts (Manning and Schutze,
1999). It is only an intermediate task in NLP,
like POS tagging or parsing. Examples of final
tasks are Machine Translation, Information Ex-
traction or Dialogue systems. WSD has been a
research area in NLP for almost the beginning
of this field due to the phenomenon of polysemy
that means multiple related meanings with a
single word (Widdows, 2003). The most im-
portant robust methods in WSD are: machine
learning methods and dictionary based meth-
ods. While for English exist some machine read-
able dictionaries, the most known being Word-
Net (Christiane Fellbaum, 1998), for Romanian
until now does not exist any. Therefore for our
application we used the machine learning ap-
proach.
</bodyText>
<sectionHeader confidence="0.989075" genericHeader="method">
2 Machine learning approach in
WSD
</sectionHeader>
<bodyText confidence="0.9992146875">
Our system falls in the supervised learning ap-
proach category. It was trained to learn a clas-
sifier that can be used to assign a yet unseen ex-
ample to one or two of a fixed number of senses.
We had a trained corpus (a number of annotated
contexts), from where the system learned the
classifier, and a test corpus which the system
will annotate.
In our system we used the Vector Space
Model: a context c was represented as a vec-
tor c~ of some features which we will present bel-
low. By a context we mean the same definition
as in Senseval denotation: the content between
¡context¿ and¡/context¿.
The notations used to explain our method are
(Manning and Schutze, 1999):
</bodyText>
<listItem confidence="0.99966475">
• w - the word to be disambiguate;
• s1, · · · , sNs the senses for w;
• c1, · · · , cNc the contexts for w;
• v1, · · · , vNf the features selected.
</listItem>
<bodyText confidence="0.998735">
As we treated each word w to be disam-
biguated separately, let us explain the method
for a single word. The features selected was
the set of ALL words used in the trained corpus
(nouns, verbs, prepositions, etc) , so we used the
cooccurrence paradigm (Dagan, Lee and Pereira
, 1994).
The vector of a context c of the target word
w is defined as:
</bodyText>
<listItem confidence="0.989036">
• c~ = (w1, · · · , w,W,) where wi is the number
of occurences of the word vi in the context
c and vi is a word from the entire trained
corpus of  |W  |words.
</listItem>
<bodyText confidence="0.999118666666667">
The similarity between two contexts ca, cb is
the normalised cosine between the vectors ~ca
and ~cb (Jurafsky and Martin, 2000):
</bodyText>
<equation confidence="0.861767166666667">
ETm
1 wa,j X wb,j
cos(~ca, ~cb) =
and sim(~ca, ~cb) = cos(~ca, ~cb).
m 2 m 2
�j=1 wa,j X �j=1 wb,j
</equation>
<bodyText confidence="0.9998005">
The number wi is the weight of the feature
vi. This can be the frequency of the feature vi
(term frequency or tf), or ”inverse document
frequency ”, denoted by idf. In our system we
considered all the words from the entire corpus,
so both these aspects are satisfied.
</bodyText>
<sectionHeader confidence="0.953499" genericHeader="method">
3 k-NN or memory based learning
</sectionHeader>
<bodyText confidence="0.996770714285714">
At training time, our k-NN model memorizes all
the contexts in the training set by their associ-
ated features. Later, when proceeds a new con-
text cnew, the classifier first selects k contexts
in the training set that are closest to cnew, then
pick the best sense (senses) for cnew (Jackson
and Moulinier, 2002).
</bodyText>
<listItem confidence="0.799531333333333">
• TRAINING: Calculate c~ for each context c.
• TEST: Calculate
Step1.
A = {~c  |sim( ~cnew,~c) is maxim,  |A |= k}
that means A is the set of the k nearest
neighbors contexts of ~cnew.
</listItem>
<equation confidence="0.898742666666667">
Step2.
�Score(cnew, sj) = (sim(~cnew, ~ci) x aij)
~c6∈A
</equation>
<bodyText confidence="0.992333">
where aij is 1 if ~ci has the sense sj and aij
is 0 otherwise.
</bodyText>
<equation confidence="0.89004">
Step3. Finally,
s&apos; = argmaxjScore(cnew, sj).
</equation>
<bodyText confidence="0.9952414">
We used the value of k set to 3 after some
experimental verifications.
A major problem with supervised approaches
is the need for a large sense tagged training set.
The bootstrapping methods use a small number
of contexts labeled with senses having a high
degree of confidence.
These labeled contexts are used as seeds to
train an initial classifier. This is then used to
extract a larger training set from the remain-
ing untagged contexts. Repeating this process,
the number of training contexts grows and the
number of untagged contexts reduces. We will
stop when the remaining unannotated corpus is
empty or any new context can’t be annotated.
In (Tatar and Serban, 2001), (Serban and Tatar,
2003) we presented an algorithm which falls in
this category. The algorithm is based on the two
principles of Yarowsky (Resnik and Yarowsky,
1999):
</bodyText>
<listItem confidence="0.9682405">
• One sense per discourse: the sense of a tar-
get word is highly consistent within a given
discourse (document);
• One sense per collocation: the contextual
features ( nearby words) provide strong
clues to the sense of a target word.
</listItem>
<bodyText confidence="0.9826315">
Also, for each iteration, the algorithm uses
a NBC classifier. We intend to present a sec-
ond system based on this algorithm at the next
Senseval contest.
</bodyText>
<sectionHeader confidence="0.992964" genericHeader="conclusions">
4 Implementation details
</sectionHeader>
<bodyText confidence="0.998176">
Our disambiguation system is written in JDK
1.4.
In order to improve the performance of the
disambiguation algorithm, we made the follow-
ing refinements in the above k-NN algorithm.
First one is to substitute the lack of an efficient
tool for stemming words in Romanian.
</bodyText>
<listItem confidence="0.962340103448276">
1. We defined a relation between words as δ :
W x W, where W is the set of words. If
w1 E W and w2 E W are two words, we
say that (w1, w2) E δ if w1 and w2 have
the same grammatical root. Therefore, if
w is a word and C is a context, we say that
w occurs in C iff exists a word w2 E C
so that (w, w2) E δ. In other words, we
replaced the stemming step with collecting
all the words with the same root in a single
class. This collection is made considering
the rules for romanian morphology;
2. The step 3 of the algorithm for choosing
the appropriate sense (senses) of a poly-
semic word w in a given context C (in
fact the sense that maximizes the set S =
{Score(C, sj)  |j = 1, · · · Ns} of scores for
C) is divided in three sub-steps:
• If there is a single sense s that maxi-
mizes S, then s is reported as the ap-
propriate sense for C;
• If there are two senses s1 and s2 that
maximize S, then s1 and s2 are re-
ported as the appropriate senses for C;
• Consider that Max1 and Max2 are
the first two maximum values from S
where (Max1 &gt; Max2). If Max1 is
obtained for a sense s1 and if Max2 is
obtained for a sense s2 and if
</listItem>
<equation confidence="0.834332">
Max1 − Max2 &lt; P
where P = Max1−Min and Min is the
(Ns−
1)
</equation>
<bodyText confidence="0.999134333333333">
minimum score from S, then s1 and s2
are reported as the appropriate senses
for C.
Experimentally, we proved that the above im-
provements grow the precision of the disam-
biguation process.
</bodyText>
<figure confidence="0.854380642857143">
5 Conclusions after the evaluation
Coarse-grained score for our system UBB using
key ”EVAL/RomanianLS.test.key” was:
precision: 0.722 (2555.00 correct of 3541.00
attempted)
recall: 0.722 (2555.00 correct of 3541.00 in
total)
attempted: 100.00
Fine-grained score was:
precision: 0.671 (2376.50 correct of 3541.00
attempted)
recall: 0.671 (2376.50 correct of 3541.00 in
total)
attempted: 100.00
</figure>
<bodyText confidence="0.99993015">
Considering as baseline procedure the major-
ity sense (all contexts are solved with the most
frequent sense in the training corpus), for the
word nucleu (noun) is obtained a precision of
0,78 while our procedure obtained 0,81. Also,
for the word desena (verb) the baseline proce-
dure of the majority sense obtains precision 0,81
while our procedure obtained 0,85.
At this stage our system has not as a goal to
label with U (unknown) a context, every time
choosing one or two from the best scored senses.
Annotating with the label U is one of our com-
ing improving. This can be done simply by
adding as a new sense for each word the sense
U. A simple experiment reported a number of
right annotated contexts.
Another direction to improve our system is
to exploit better the senses as they are done in
training corpus: our system simply consider the
first sense.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881060606061">
I. Dagan, L. Lee and F. C. N. Pereira. 1994.
Similarity-based Estimation of Word Cooc-
curences Probabilities. Meeting of the Asso-
ciation for Computational Linguistics, 272–
278.
Christiane Fellbaum. 1998. WordNet: An elec-
tronic lexical database. The MIT Press.
P. Jackson and I. Moulinier. 2002. Natural
Language Processing for Online Applications.
John Benjamin Publ. Company.
D. Jurafsky and J. Martin. 2000. Speech and
language processing. Prentice-Hall, NJ.
C. Manning and H. Schutze. 1999. Foundation
of statistical natural language processing. The
MIT Press.
Ruslan Mitkov,editor. 2002 The Oxford Hand-
book of Computational Linguistics. Oxford
University Press.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing Systems and Distinguishing sense: new
evaluation methods for WSD. Natural Lan-
guage Engineering, 5(2):113-134.
G. Serban and D. Tatar. 2003. Word Sense Dis-
ambiguation for Untagged Corpus: Applica-
tion to Romanian Language. CICLing-2003,
LNCS 2588, 270–275.
D. Tatar and G. Serban. 2001. A new algorithm
for WSD. Studia Univ. ”Babes-Bolyai”, In-
formatica, 2 99–108.
D. Widdows. 2003. A mathematical model for
context and word meaning. Fourth Interna-
tional Conference on Modeling and using con-
text, Stanford, California, June 23-25.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.220933">
<note confidence="0.94848775">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics UBB system at Senseval3</note>
<author confidence="0.488747">Gabriela</author>
<affiliation confidence="0.988821">Department of Computer University</affiliation>
<email confidence="0.92826">gabis@cs.ubbcluj.ro</email>
<author confidence="0.691095">Doina</author>
<affiliation confidence="0.9800105">Department of Computer University</affiliation>
<email confidence="0.953406">dtatar@cs.ubbcluj.ro</email>
<abstract confidence="0.998857692307692">It is known that whenever a system’s actions depend on the meaning of the text being processed, disambiguation is beneficial or even necessary. The contest Senseval is an international frame where the research in this important field is validated in an hierarchical manner. In this paper we present our system participating for the first time at Senseval 3 contest on WSD, contest developed in March-April 2004. We present also our intentions on improving our system, intentions occurred from the study of results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F C N Pereira</author>
</authors>
<title>Similarity-based Estimation of Word Cooccurences Probabilities. Meeting of the Association for Computational Linguistics,</title>
<date>1994</date>
<pages>272--278</pages>
<marker>Dagan, Lee, Pereira, 1994</marker>
<rawString>I. Dagan, L. Lee and F. C. N. Pereira. 1994. Similarity-based Estimation of Word Cooccurences Probabilities. Meeting of the Association for Computational Linguistics, 272– 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1642" citStr="Fellbaum, 1998" startWordPosition="247" endWordPosition="248">s in particular contexts (Manning and Schutze, 1999). It is only an intermediate task in NLP, like POS tagging or parsing. Examples of final tasks are Machine Translation, Information Extraction or Dialogue systems. WSD has been a research area in NLP for almost the beginning of this field due to the phenomenon of polysemy that means multiple related meanings with a single word (Widdows, 2003). The most important robust methods in WSD are: machine learning methods and dictionary based methods. While for English exist some machine readable dictionaries, the most known being WordNet (Christiane Fellbaum, 1998), for Romanian until now does not exist any. Therefore for our application we used the machine learning approach. 2 Machine learning approach in WSD Our system falls in the supervised learning approach category. It was trained to learn a classifier that can be used to assign a yet unseen example to one or two of a fixed number of senses. We had a trained corpus (a number of annotated contexts), from where the system learned the classifier, and a test corpus which the system will annotate. In our system we used the Vector Space Model: a context c was represented as a vector c~ of some features </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jackson</author>
<author>I Moulinier</author>
</authors>
<title>Natural Language Processing for Online Applications.</title>
<date>2002</date>
<publisher>John Benjamin Publ. Company.</publisher>
<contexts>
<context position="3948" citStr="Jackson and Moulinier, 2002" startWordPosition="687" endWordPosition="690">1 wa,j X �j=1 wb,j The number wi is the weight of the feature vi. This can be the frequency of the feature vi (term frequency or tf), or ”inverse document frequency ”, denoted by idf. In our system we considered all the words from the entire corpus, so both these aspects are satisfied. 3 k-NN or memory based learning At training time, our k-NN model memorizes all the contexts in the training set by their associated features. Later, when proceeds a new context cnew, the classifier first selects k contexts in the training set that are closest to cnew, then pick the best sense (senses) for cnew (Jackson and Moulinier, 2002). • TRAINING: Calculate c~ for each context c. • TEST: Calculate Step1. A = {~c |sim( ~cnew,~c) is maxim, |A |= k} that means A is the set of the k nearest neighbors contexts of ~cnew. Step2. �Score(cnew, sj) = (sim(~cnew, ~ci) x aij) ~c6∈A where aij is 1 if ~ci has the sense sj and aij is 0 otherwise. Step3. Finally, s&apos; = argmaxjScore(cnew, sj). We used the value of k set to 3 after some experimental verifications. A major problem with supervised approaches is the need for a large sense tagged training set. The bootstrapping methods use a small number of contexts labeled with senses having a </context>
</contexts>
<marker>Jackson, Moulinier, 2002</marker>
<rawString>P. Jackson and I. Moulinier. 2002. Natural Language Processing for Online Applications. John Benjamin Publ. Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J Martin</author>
</authors>
<title>Speech and language processing.</title>
<date>2000</date>
<publisher>Prentice-Hall, NJ.</publisher>
<contexts>
<context position="3238" citStr="Jurafsky and Martin, 2000" startWordPosition="555" endWordPosition="558">we treated each word w to be disambiguated separately, let us explain the method for a single word. The features selected was the set of ALL words used in the trained corpus (nouns, verbs, prepositions, etc) , so we used the cooccurrence paradigm (Dagan, Lee and Pereira , 1994). The vector of a context c of the target word w is defined as: • c~ = (w1, · · · , w,W,) where wi is the number of occurences of the word vi in the context c and vi is a word from the entire trained corpus of |W |words. The similarity between two contexts ca, cb is the normalised cosine between the vectors ~ca and ~cb (Jurafsky and Martin, 2000): ETm 1 wa,j X wb,j cos(~ca, ~cb) = and sim(~ca, ~cb) = cos(~ca, ~cb). m 2 m 2 �j=1 wa,j X �j=1 wb,j The number wi is the weight of the feature vi. This can be the frequency of the feature vi (term frequency or tf), or ”inverse document frequency ”, denoted by idf. In our system we considered all the words from the entire corpus, so both these aspects are satisfied. 3 k-NN or memory based learning At training time, our k-NN model memorizes all the contexts in the training set by their associated features. Later, when proceeds a new context cnew, the classifier first selects k contexts in the t</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J. Martin. 2000. Speech and language processing. Prentice-Hall, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundation of statistical natural language processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1079" citStr="Manning and Schutze, 1999" startWordPosition="152" endWordPosition="155">epend on the meaning of the text being processed, disambiguation is beneficial or even necessary. The contest Senseval is an international frame where the research in this important field is validated in an hierarchical manner. In this paper we present our system participating for the first time at Senseval 3 contest on WSD, contest developed in March-April 2004. We present also our intentions on improving our system, intentions occurred from the study of results. 1 Introduction Word Sense Disambiguation (WSD) is the process of identifying the correct meanings of words in particular contexts (Manning and Schutze, 1999). It is only an intermediate task in NLP, like POS tagging or parsing. Examples of final tasks are Machine Translation, Information Extraction or Dialogue systems. WSD has been a research area in NLP for almost the beginning of this field due to the phenomenon of polysemy that means multiple related meanings with a single word (Widdows, 2003). The most important robust methods in WSD are: machine learning methods and dictionary based methods. While for English exist some machine readable dictionaries, the most known being WordNet (Christiane Fellbaum, 1998), for Romanian until now does not exi</context>
<context position="2457" citStr="Manning and Schutze, 1999" startWordPosition="392" endWordPosition="395">ing approach category. It was trained to learn a classifier that can be used to assign a yet unseen example to one or two of a fixed number of senses. We had a trained corpus (a number of annotated contexts), from where the system learned the classifier, and a test corpus which the system will annotate. In our system we used the Vector Space Model: a context c was represented as a vector c~ of some features which we will present bellow. By a context we mean the same definition as in Senseval denotation: the content between ¡context¿ and¡/context¿. The notations used to explain our method are (Manning and Schutze, 1999): • w - the word to be disambiguate; • s1, · · · , sNs the senses for w; • c1, · · · , cNc the contexts for w; • v1, · · · , vNf the features selected. As we treated each word w to be disambiguated separately, let us explain the method for a single word. The features selected was the set of ALL words used in the trained corpus (nouns, verbs, prepositions, etc) , so we used the cooccurrence paradigm (Dagan, Lee and Pereira , 1994). The vector of a context c of the target word w is defined as: • c~ = (w1, · · · , w,W,) where wi is the number of occurences of the word vi in the context c and vi i</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundation of statistical natural language processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>editor</author>
</authors>
<date>2002</date>
<booktitle>The Oxford Handbook of Computational Linguistics.</booktitle>
<publisher>Oxford University Press.</publisher>
<marker>Mitkov, editor, 2002</marker>
<rawString>Ruslan Mitkov,editor. 2002 The Oxford Handbook of Computational Linguistics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Yarowsky</author>
</authors>
<title>Distinguishing Systems and Distinguishing sense: new evaluation methods for WSD.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<pages>5--2</pages>
<contexts>
<context position="5137" citStr="Resnik and Yarowsky, 1999" startWordPosition="893" endWordPosition="896">ontexts labeled with senses having a high degree of confidence. These labeled contexts are used as seeds to train an initial classifier. This is then used to extract a larger training set from the remaining untagged contexts. Repeating this process, the number of training contexts grows and the number of untagged contexts reduces. We will stop when the remaining unannotated corpus is empty or any new context can’t be annotated. In (Tatar and Serban, 2001), (Serban and Tatar, 2003) we presented an algorithm which falls in this category. The algorithm is based on the two principles of Yarowsky (Resnik and Yarowsky, 1999): • One sense per discourse: the sense of a target word is highly consistent within a given discourse (document); • One sense per collocation: the contextual features ( nearby words) provide strong clues to the sense of a target word. Also, for each iteration, the algorithm uses a NBC classifier. We intend to present a second system based on this algorithm at the next Senseval contest. 4 Implementation details Our disambiguation system is written in JDK 1.4. In order to improve the performance of the disambiguation algorithm, we made the following refinements in the above k-NN algorithm. First</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>P. Resnik and D. Yarowsky. 1999. Distinguishing Systems and Distinguishing sense: new evaluation methods for WSD. Natural Language Engineering, 5(2):113-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Serban</author>
<author>D Tatar</author>
</authors>
<title>Word Sense Disambiguation for Untagged Corpus: Application to Romanian Language.</title>
<date>2003</date>
<tech>CICLing-2003, LNCS 2588,</tech>
<pages>270--275</pages>
<contexts>
<context position="4996" citStr="Serban and Tatar, 2003" startWordPosition="870" endWordPosition="873">or problem with supervised approaches is the need for a large sense tagged training set. The bootstrapping methods use a small number of contexts labeled with senses having a high degree of confidence. These labeled contexts are used as seeds to train an initial classifier. This is then used to extract a larger training set from the remaining untagged contexts. Repeating this process, the number of training contexts grows and the number of untagged contexts reduces. We will stop when the remaining unannotated corpus is empty or any new context can’t be annotated. In (Tatar and Serban, 2001), (Serban and Tatar, 2003) we presented an algorithm which falls in this category. The algorithm is based on the two principles of Yarowsky (Resnik and Yarowsky, 1999): • One sense per discourse: the sense of a target word is highly consistent within a given discourse (document); • One sense per collocation: the contextual features ( nearby words) provide strong clues to the sense of a target word. Also, for each iteration, the algorithm uses a NBC classifier. We intend to present a second system based on this algorithm at the next Senseval contest. 4 Implementation details Our disambiguation system is written in JDK 1</context>
</contexts>
<marker>Serban, Tatar, 2003</marker>
<rawString>G. Serban and D. Tatar. 2003. Word Sense Disambiguation for Untagged Corpus: Application to Romanian Language. CICLing-2003, LNCS 2588, 270–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tatar</author>
<author>G Serban</author>
</authors>
<title>A new algorithm for WSD.</title>
<date>2001</date>
<journal>Studia Univ. ”Babes-Bolyai”, Informatica,</journal>
<volume>2</volume>
<pages>99--108</pages>
<contexts>
<context position="4970" citStr="Tatar and Serban, 2001" startWordPosition="866" endWordPosition="869">ental verifications. A major problem with supervised approaches is the need for a large sense tagged training set. The bootstrapping methods use a small number of contexts labeled with senses having a high degree of confidence. These labeled contexts are used as seeds to train an initial classifier. This is then used to extract a larger training set from the remaining untagged contexts. Repeating this process, the number of training contexts grows and the number of untagged contexts reduces. We will stop when the remaining unannotated corpus is empty or any new context can’t be annotated. In (Tatar and Serban, 2001), (Serban and Tatar, 2003) we presented an algorithm which falls in this category. The algorithm is based on the two principles of Yarowsky (Resnik and Yarowsky, 1999): • One sense per discourse: the sense of a target word is highly consistent within a given discourse (document); • One sense per collocation: the contextual features ( nearby words) provide strong clues to the sense of a target word. Also, for each iteration, the algorithm uses a NBC classifier. We intend to present a second system based on this algorithm at the next Senseval contest. 4 Implementation details Our disambiguation </context>
</contexts>
<marker>Tatar, Serban, 2001</marker>
<rawString>D. Tatar and G. Serban. 2001. A new algorithm for WSD. Studia Univ. ”Babes-Bolyai”, Informatica, 2 99–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>A mathematical model for context and word meaning.</title>
<date>2003</date>
<booktitle>Fourth International Conference on Modeling and using context,</booktitle>
<location>Stanford, California,</location>
<contexts>
<context position="1423" citStr="Widdows, 2003" startWordPosition="212" endWordPosition="213">ril 2004. We present also our intentions on improving our system, intentions occurred from the study of results. 1 Introduction Word Sense Disambiguation (WSD) is the process of identifying the correct meanings of words in particular contexts (Manning and Schutze, 1999). It is only an intermediate task in NLP, like POS tagging or parsing. Examples of final tasks are Machine Translation, Information Extraction or Dialogue systems. WSD has been a research area in NLP for almost the beginning of this field due to the phenomenon of polysemy that means multiple related meanings with a single word (Widdows, 2003). The most important robust methods in WSD are: machine learning methods and dictionary based methods. While for English exist some machine readable dictionaries, the most known being WordNet (Christiane Fellbaum, 1998), for Romanian until now does not exist any. Therefore for our application we used the machine learning approach. 2 Machine learning approach in WSD Our system falls in the supervised learning approach category. It was trained to learn a classifier that can be used to assign a yet unseen example to one or two of a fixed number of senses. We had a trained corpus (a number of anno</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>D. Widdows. 2003. A mathematical model for context and word meaning. Fourth International Conference on Modeling and using context, Stanford, California, June 23-25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>