<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.985028">
PARADIGM: Paraphrase Diagnostics through Grammar Matching
</title>
<author confidence="0.997236">
Jonathan Weese and Juri Ganitkevitch Chris Callison-Burch
</author>
<affiliation confidence="0.997599">
Johns Hopkins University University of Pennsylvania
</affiliation>
<sectionHeader confidence="0.989333" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863620689655">
Paraphrase evaluation is typically done ei-
ther manually or through indirect, task-
based evaluation. We introduce an in-
trinsic evaluation PARADIGM which mea-
sures the goodness of paraphrase col-
lections that are represented using syn-
chronous grammars. We formulate two
measures that evaluate these paraphrase
grammars using gold standard sentential
paraphrases drawn from a monolingual
parallel corpus. The first measure calcu-
lates how often a paraphrase grammar is
able to synchronously parse the sentence
pairs in the corpus. The second mea-
sure enumerates paraphrase rules from the
monolingual parallel corpus and calculates
the overlap between this reference para-
phrase collection and the paraphrase re-
source being evaluated. We demonstrate
the use of these evaluation metrics on para-
phrase collections derived from three dif-
ferent data types: multiple translations
of classic French novels, comparable sen-
tence pairs drawn from different newspa-
pers, and bilingual parallel corpora. We
show that PARADIGM correlates with hu-
man judgments more strongly than BLEU
on a task-based evaluation of paraphrase
quality.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994188">
Paraphrases are useful in a wide range of natu-
ral language processing applications. A variety
of data-driven approaches have been proposed to
generate paraphrase resources (see Madnani and
Dorr (2010) for a survey of these methods). Few
objective metrics have been established to evalu-
ate these resources. Instead, paraphrases are typi-
cally evaluated using subjective manual evaluation
or through task-based evaluations.
Different researchers have used different crite-
ria for manual evaluations. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were “ap-
proximately conceptually equivalent.” Ibrahim
et al. (2003) asked judges whether their para-
phrases were “roughly interchangeable given the
genre.” Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
sentences and asked judges whether the substitu-
tions “preserved meaning and remained grammat-
ical.” The results of these subjective evaluations
are not easily reusable.
Other researchers have evaluated their para-
phrases through task-based evaluations. Lin and
Pantel (2001) measured their potential impact on
question-answering. Cohn and Lapata (2007)
evaluate their applicability in the text-to-text gen-
eration task of sentence compression. Zhao et al.
(2009) use them to perform sentence compression
and simplification and to compute sentence simi-
larity. Several researchers have demonstrated that
paraphrases can improve machine translation eval-
uation (c.f. Kauchak and Barzilay (2006), Zhou
et al. (2006), Madnani (2010) and Snover et al.
(2010)).
We introduce an automatic evaluation met-
ric called PARADIGM, PARAphrase DIagnostics
through Grammar Matching. This metric eval-
uates paraphrase collections that are represented
using synchronous grammars. Synchronous tree-
adjoining grammars (STAGs), synchronous tree
substitution grammars (STSGs), and synchronous
context free grammars (SCFGs) are popular for-
malisms for representing paraphrase rules (Dras,
1997; Cohn and Lapata, 2007; Madnani, 2010;
Ganitkevitch et al., 2011). We present two mea-
sures that evaluate these paraphrase grammars us-
ing gold standard sentential paraphrases drawn
from a monolingual parallel corpus, which have
been previously proposed as a good resource
</bodyText>
<page confidence="0.969131">
192
</page>
<note confidence="0.994372">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9533871">
for paraphrase evaluation (Callison-Burch et al.,
2008; Cohn et al., 2008).
The first of our two proposed metrics calculates
how often a paraphrase grammar is able to syn-
chronously parse the sentence pairs in a test set.
The second measure enumerates paraphrase rules
from a monolingual parallel corpus and calculates
the overlap between this reference paraphrase col-
lection, and the paraphrase resource being evalu-
ated.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="introduction">
2 Related work and background
</sectionHeader>
<bodyText confidence="0.999933">
The most closely related work is ParaMetric
(Callison-Burch et al., 2008), which is a set of
objective measures for evaluating the quality of
phrase-based paraphrases. ParaMetric extracts a
set of gold-standard phrasal paraphrases from sen-
tential paraphrases that have been manually word-
aligned. The sentential paraphrases used in Para-
Metric were drawn from a data set originally cre-
ated to evaluate machine translation output using
the BLEU metric. Cohn et al. (2008) argue that
these sorts of monolingual parallel corpora are ap-
propriate for evaluating paraphrase systems, be-
cause they are naturally occurring sources of para-
phrases.
Callison-Burch et al. (2008) calculated three
types of metrics in ParaMetric. The manual word
alignments were used to calculate how well an
automatic paraphrasing technique is able to align
the paraphrases in a sentence pair. This measure
is limited to a class of paraphrasing techniques
that perform alignment (like MacCartney et al.
(2008)). Most methods produce a list of para-
phrases for a given input phrase. So Callison-
Burch et al. (2008) calculate two more gener-
ally applicable measures by comparing the para-
phrases in an automatically extracted resource to
gold standard paraphrases extracted via the align-
ments. These allow a lower-bound on precision
and relative recall to be calculated.
Liu et al. (2010) introduce the PEM metric as an
alternative to BLEU, since BLEU prefers iden-
tical paraphrases. PEM uses a second language
as a pivot to judge semantic equivalence. This re-
quires use of some bilingual data. Chen and Dolan
(2011) suggest using BLEU together with their
metric PINC, which uses n-grams to measure lex-
ical difference between paraphrases.
PARADIGM extends the ideas in ParaMetric
from lexical and phrasal paraphrasing techniques
to paraphrasing techniques that also generate syn-
tactic templates, such as Zhao et al. (2008), Cohn
and Lapata (2009), Madnani (2010) and Ganitke-
vitch et al. (2011). Instead of extracting gold stan-
dard paraphrases using techniques from phrase-
based machine translation, we use grammar ex-
traction techniques (Weese et al., 2011) to ex-
tract gold standard paraphrase grammar rules from
ParaMetric’s word-aligned sentential paraphrases.
Using these rules, we calculate the overlap be-
tween a gold standard paraphrase grammar and an
automatically generated paraphrase grammar.
Moreover, like ParaMetric, PARADIGM is able
to do further analysis on a restricted class of para-
phrasing models. In this case, PARADIGM evalu-
ates how well certain models are able to produce
synchronous parses of sentence pairs drawn from
monolingual parallel corpora. PARADIGM’s dif-
ferent metrics are explained in Section 4, but first
we give background on synchronous parsing and
synchronous grammars.
</bodyText>
<subsectionHeader confidence="0.9971725">
2.1 Synchronous parsing with SCFGs
Synchronous context-free grammars
</subsectionHeader>
<bodyText confidence="0.999896666666667">
An SCFG (Lewis and Stearns, 1968; Aho and
Ullman, 1972) is similar to a context-free gram-
mar, except that it generates pairs of strings
in correspondence. Each production rule in an
SCFG rewrites a non-terminal symbol as a pair of
phrases, which may have contain a mix of words
and non-terminals symbols. The grammar is syn-
chronous because both phrases in the pair must
have an identical set of non-terminals (though they
can come in different orders), and corresponding
non-terminals must be rewritten using the same
rule.
Much recent work in MT (and, by extension,
paraphrasing approaches that use MT machinery)
has been focused on choosing an appropriate set of
non-terminal symbols. The Hiero model (Chiang,
2007) used a single non-terminal symbol X. Other
approaches have read symbols from constituent
parses of the training data (Galley et al., 2004;
Galley et al., 2006; Zollmann and Venugopal,
2006). Labels based combinatory categorial gram-
mar (Steedman and Baldridge, 2011) have also
been used (Almaghout et al., 2010; Weese et al.,
2012).
</bodyText>
<subsectionHeader confidence="0.939943">
Synchronous parsing
</subsectionHeader>
<bodyText confidence="0.9665375">
Wu (1997) introduced a parsing algorithm using
a variant of CKY. Dyer recently showed (2010)
</bodyText>
<page confidence="0.999357">
193
</page>
<figureCaption confidence="0.979362">
Figure 1: PARADIGM extracts lexical, phrasal and
syntactic paraphrases from parsed, word-aligned
sentence pairs.
</figureCaption>
<bodyText confidence="0.9964489">
that the average parse time can be significantly im-
proved by using a two-pass algorithm.
The question of whether a source-reference pair
is reachable under a model must be addressed in
end-to-end discriminative training in MT (Liang
et al., 2006a; Gimpel and Smith, 2012). Auli et
al. (2009) showed that only approximately 30% of
training pairs are reachable under a phrase-based
model. This result is confirmed by our results in
paraphrasing.
</bodyText>
<sectionHeader confidence="0.947263" genericHeader="method">
3 Paraphrase grammar extraction
</sectionHeader>
<bodyText confidence="0.999398142857143">
Like ParaMetric, PARADIGM extracts gold stan-
dard paraphrases from word-aligned sentential
paraphrases. PARADIGM goes further by parsing
one of the two input sentences, and uses the parse
tree to extract syntactic paraphrase rules, follow-
ing recent advances in syntactic approaches to ma-
chine translation (like Galley et al. (2004), Zoll-
mann and Venugopal (2006), and others). Figure 1
shows an example of a parsed sentence pair. From
that pair it is possible to extract a wide variety
of non-identical paraphrases, which include lexi-
cal paraphrases (single word synonyms), phrasal
paraphrases, and syntactic paraphrases that in-
clude a mix of words and syntactic non-terminal
</bodyText>
<table confidence="0.803973666666667">
CC → and while
VBP → want propose
VBP → expect want
DT → some some people
S → him to step down him to resign
VP → step down resign
VP → to step down to resign
VP → want to impeach him propose to impeach him
VP → want VP propose VP
VP → want to impeach PRP propose to impeach PRP
VP → VBP him to step down VBP him to resign
S → PRP to step down PRP to resign
</table>
<figureCaption confidence="0.876475333333333">
Figure 2: Four examples each of lexical, phrasal,
and syntactic paraphrases that can be extracted
from the sentence pair in Figure 1.
</figureCaption>
<bodyText confidence="0.946212">
symbols. Figure 2 shows a set of four examples
for each type that can be extracted from Figure 1.
These rules are formulated as SCFG rules,
with a syntactic left-hand nonterminal symbol
and two English right-hand sides representing the
paraphrase. The examples above include non-
terminal symbols that represent whole syntac-
tic constituents. It is also possible to create
more complex non-terminal symbols that describe
CCG-like non-constituent phrases. For example,
we could extract a rule like
S/VP → &lt;NNS want him to, NNS expect him to&gt;
Using constituents only, we are able to ex-
tract 45 paraphrase rules from Figure 1. Adding
CCG-style slashed constituents yields 66 addi-
tional rules.
</bodyText>
<sectionHeader confidence="0.8072025" genericHeader="method">
4 PARADIGM: Evaluating paraphrase
grammars
</sectionHeader>
<bodyText confidence="0.99290925">
By considering a paraphrase model as a syn-
chronous context-free grammar, we propose to
measure the model’s goodness using the following
criteria:
</bodyText>
<listItem confidence="0.989313">
1. What percentage of sentential paraphrases
are reachable under the model? That is, given
a collection of sentence pairs (ai, bi) and an
SCFG G, where each pair of a and b are sen-
tential paraphrases, how many of the pairs are
in the language of G? We evaluate this by
producing a synchronous parse for the pairs,
as shown in Figure 3.
2. Given a collection of gold-standard para-
phrase rules, how many of those paraphrases
exist as rules in G? To calculate this, we
look at the overlap of grammars (described in
</listItem>
<figure confidence="0.991742563636364">
S
S
VP
S
VP
NP VP
TP VB
V U
N
E c
ai 3
some
VB
to
impeach
him
S
VP
NP
S
VP
and
NNS
VBP
others
expect
NP
PRP
him
to
VB
PRT
.
VP
step
down
.
PR
PRP
CC
people
propose
to
impeach
him
,
while
others
want
him
to
resign
.
194
12 of the cartoons that were offensive to the islamic prophet sparked riots
</figure>
<figureCaption confidence="0.588236833333333">
Figure 3: We measure the goodness of paraphrase
grammars by determine how often they can be
used to synchronously parse gold-standard sen-
tential paraphrases. Note we do not require the
synchronous derivation to match a gold-standard
parse tree.
</figureCaption>
<bodyText confidence="0.999804222222222">
Section 4.2 below), examining different cate-
gories of rules and thresholding based on how
frequently the rule was used in the gold stan-
dard data.
These criteria correspond to properties that we
think are desirable in paraphrase models. They
also have the advantage that they do not depend
on human judgments and so can be calculated au-
tomatically.
</bodyText>
<subsectionHeader confidence="0.998963">
4.1 Synchronous parse coverage
</subsectionHeader>
<bodyText confidence="0.9999755">
Paraphrase grammars should be able to explain
sentential paraphrases. For example, Figure
3 shows a sentence pair that is synchronously
parseable by one paraphrase grammar. In general,
we say that the more such sentence pairs that a
paraphrase grammar can synchronously parse, the
better it is.
The synchronous derivation allows us to draw
inferences about parts of the sentence pair that are
in correspondence; for instance, in Figure 3, vi-
olent unrest corresponds to riots and mohammad
corresponds to the islamic prophet.
</bodyText>
<subsectionHeader confidence="0.985659">
4.2 Grammar overlap defined
</subsectionHeader>
<bodyText confidence="0.963632708333333">
We measure grammar overlap by comparing the
sets of production rules for two different gram-
mars. If the grammars contain rules that are equiv-
alent, the equivalent rules are in the grammars’
overlap.
We consider two types of overlapping, which
we will call strict and non-strict overlap. For strict
overlap, we say that two rules are equivalent if
they are identical, that is, if they have the same
left-hand side non-terminal symbol, their source
sides are identical strings, and their target sides are
identical strings. (This includes identical indexing
on non-terminal symbols on the right hand sides
of the rule.)
To calculate non-strict overlap, we ignore the
identities of non-terminal symbols in the left-hand
and right-hand sides of the rules. That is, two rules
are considered equivalent if they are identical after
all the non-terminal symbols have been replaced
by one equivalent symbol.
For example, in non-strict overlap, the syntactic
rule
NP —* (N1 ’s N2; the N2 of N1)
would match the Hiero rule
</bodyText>
<equation confidence="0.94755">
X —* (X1 ’s X2; the X2 of X1)
</equation>
<bodyText confidence="0.99984">
If we are considering two Hiero grammars,
strict and non-strict intersection are the same op-
eration since they only have on non-terminal X.
</bodyText>
<subsectionHeader confidence="0.997943">
4.3 Precision lower bound and relative recall
</subsectionHeader>
<bodyText confidence="0.999993083333333">
Callison-Burch et al. (2008) use the notion of over-
lap between two paraphrase sets to define two met-
rics, precision lower bound and relative recall.
These are calculated the same way as standard
precision and recall. Relative recall is qualified
as “relative” because it is calculated on a poten-
tially incomplete set of gold standard paraphrases.
There may exist valid paraphrases that do not oc-
cur in that set. Similarly, only a lower bound on
precision can be calculated because the candidate
set may contain valid paraphrases that do not oc-
cur in the gold standard set.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.8969">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999517333333333">
We extracted paraphrase grammars from a vari-
ety of different data sources, including four collec-
tions of sentential paraphrases. These included:
</bodyText>
<listItem confidence="0.977680166666667">
• Multiple translation corpora that were
compiled by the Linguistics Data Consortium
(LDC) for the purposes of evaluating ma-
chine translation quality with the BLEU met-
ric. We collected eight LDC corpora that all
have multiple English translations.1
</listItem>
<footnote confidence="0.505178333333333">
1LDC Catalog numbers LDC2002T01, LDC2005T05,
LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14,
LDC2010T17, and LDC2010T23.
</footnote>
<table confidence="0.942176">
violent unrest was caused by twelve cartoons insulting mohammad
NP
S
VBD
VP
CD NNS JJ
NP
NP
VP
NP
CD NNS JJ
NP
NP
VP
NP
S
VBD NP
VP
195
Grammar Rules
LDC Hiero 52,784,462
Lit. Hiero 3,288,546
MSR Hiero 2,456,513
ParaMetric Hiero 584,944
LDC Syntax 23,978,477
Lit. Syntax 715,154
MSR Syntax 406,115
ParaMetric Syntax 317,772
PPDB-v0.2-small 1,292,224
PPDB-v0.2-large 9,456,356
PPDB-v0.2-xl 46,592,161
Corpus sentence total
pairs words
LDC Multiple Translations 83,284 2,254,707
Classic French Literature 75,106 682,978
MSR Paraphrase Corpus 5,801 219,492
ParaMetric 970 21,944
</table>
<tableCaption confidence="0.918271714285714">
Table 1: Amount of English–English parallel data.
LDC data has 4 parallel translations per sentence.
Literature data is from Barzilay and McKeown
(2001). MSR data is from Quirk et al. (2004)
and Dolan et al. (2004). ParaMertic data is from
Callison-Burch et al. (2008).
Table 2: Size of various paraphrase grammars.
</tableCaption>
<listItem confidence="0.6963476">
• Classic French Literature that were trans-
lated by different translators, and which were
compiled by Barzilay and McKeown (2001).
• The MSR Paraphrase corpus which con-
sists of sentence pairs drawn from compara-
ble news articles drawn from different web
sites in the same date rate. The sentence pairs
were aligned heuristically aligned and then
manually judged to be paraphrases.
• The ParaMetric data which consists of 900
manually word-aligned sentence pairs col-
lected by Cohn et al. (2008). 300 sentence
pairs were drawn from each of the 3 above
sources. We use this to extract the gold stan-
dard paraphrase grammar.
</listItem>
<bodyText confidence="0.999136285714285">
The size of the data from each source is summa-
rized in Table 1.
For each dataset, after tokenizing and normaliz-
ing, we parsed one sentence in each English pair
using the Berkeley constituency parser (Liang et
al., 2006b). We then obtained word-level align-
ments, either using GIZA++ (Och and Ney, 2000)
or, in the case of ParaMetric, using human annota-
tions.
We used the Thrax grammar extractor (Weese
et al., 2011) to extract Hiero-style and syntactic
SCFGs from the paraphrase data. In the syntac-
tic setting we allowed labeling of rules with ei-
ther constituent labels or CCG-style slashed cat-
egories. The size of the extracted grammars is
shown in Table 2.
We also used version 0.2 of the SCFG-based
paraphrase collection known as the ParaPhrase
DataBase or PPDB (Ganitkevitch et al., 2013).
The PPDB paraphrases were extracted using the
pivoting technique (Bannard and Callison-Burch,
</bodyText>
<table confidence="0.999819181818182">
Grammar freq. &gt; 1 freq. &gt; 2
ParaMetric Syntax 317,772 21,709
LDC Hiero 5,840 (1.8%) 416 (1.9%)
Lit. Hiero 6,152 (1.9%) 359 (1.7%)
MSR Hiero 10,012 (3.2%) 315 (1.5%)
LDC Syntax 48,833 (15.3%) 7,748 (35.6%)
Lit. Syntax 14,431 (4.5%) 1,960 (9.0%)
MSR Syntax 21,197 (6.7%) 2,053 (9.5%)
PPDB-v0.2-small 15,831 (5.0%) 5,673 (26.1%)
PPDB-v0.2-large 31,277 (9.8%) 8,245 (37.9%)
PPDB-v0.2-xl 47,720 (15.0%) 10,049 (46.2%)
</table>
<tableCaption confidence="0.995961">
Table 3: Size of strict overlap (number of rules and
</tableCaption>
<bodyText confidence="0.992238538461538">
% of the gold standard) of each grammar with a
syntactic grammar derived from ParaMetric. freq.
≥ 2 means we first removed all rules that ap-
peared only once from the ParaMetric grammar.
The number in parentheses shows the percentage
of ParaMetric rules that are present in the overlap.
2005) on bilingual parallel corpora containing
over 42 million sentence pairs.
The PPDB release includes a tool for pruning
the grammar to a smaller size by retaining only
high-precision paraphrases. We include PPDB
grammars for several different pruning settings in
our analysis.
</bodyText>
<subsectionHeader confidence="0.992852">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999603545454545">
We calculated our two metrics for each of the
grammars listed in Table 2.
To perform synchronous parsing, we used the
Joshua decoder (Post et al., 2013), which includes
an implementation of Dyer’s two-pass parsing al-
gorithm (2010). After splitting the LDC data into
10 equal pieces, we trained paraphrase models on
nine-tenths of the data and parsed the other tenth.
Grammars trained from other sources (the MSR
corpus, French literature domain, and PPDB) were
also evaluated on the held-out tenth of LDC data.
</bodyText>
<page confidence="0.997111">
196
</page>
<table confidence="0.963136083333333">
Grammar freq. &gt; 1 freq. &gt; 2
ParaMetric Syntax 200,385 20,699
LDC Hiero 41,346 (20.6%) 5,323 (25.8%)
Lit. Hiero 36,873 (18.4%) 4,606 (22.3%)
MSR Hiero 58,970 (29.4%) 6,741 (32.6%)
LDC Syntax 37,231 (11.7%) 5,055 (24.5%)
Lit. Syntax 19,530 (9.7%) 3,121 (15.1%)
MSR Syntax 28,016 (14.0%) 3,564 (17.2%)
PPDB-v0.2-small 13,003 (6.5%) 3,661 (17.7%)
PPDB-v0.2-large 22,431 (11.2%) 4,837 (23.4%)
PPDB-v0.2-xl 31,294 (15.6%) 5,590 (27.0%)
Precision Lower Bound
</table>
<figure confidence="0.966492411764706">
Relative Recall
10 100 1k 10k 100k
Recall
Prec.
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.16
0.12
0.08
0.04
0
</figure>
<figureCaption confidence="0.8709998">
Table 4: Size of non-strict overlap of each gram-
mar with the syntactic grammar derived from
ParaMetric. The number in parentheses shows the
percentage of ParaMetric rules that are present in
the overlap.
</figureCaption>
<figure confidence="0.829109">
Number of rules
</figure>
<figureCaption confidence="0.962343666666667">
Figure 4: Precision lower bound and relative recall
when overlapping different sizes of PPDB with the
syntactic ParaMetric grammar.
</figureCaption>
<table confidence="0.8054138">
Grammar syntactic phrasal lexical
ParaMetric 238,646 73,320 5,806
LDCSyn 36,375 (15%) 8,806 (12%) 3,652 (62%)
MSRSyn 7,734 (3%) 11,254 (15%) 2,209 (38%)
PPDB-xl 40,822 (17%) 3,765 (5%) 3,142 (54%)
</table>
<tableCaption confidence="0.907395">
Table 5: Number of paraphrases of each type
</tableCaption>
<bodyText confidence="0.99273775">
in each grammar’s strict overlap with the syntac-
tic ParaMetric grammar. Numbers in parentheses
show the percentage of ParaMetric rules of each
type.
Note that the LDC data contains 4 independent
translations of each foreign sentence, giving 6 pos-
sible (unordered) paraphrase pairs. We evaluated
coverage in two ways (corresponding to the two
columns in Table 6): first, considering all possible
sentence pairs from the test data, how many were
able to be parsed?
Secondly, if we consider all the English sen-
tences that correspond to one foreign sentence,
how many foreign sentences had at least one pair
of English translations that could be parsed syn-
chronously?
For grammar overlap, we perform both strict
and non-strict calculations (see Section 4.2)
against a syntactic grammar derived from hand-
aligned ParaMetric data.
</bodyText>
<subsectionHeader confidence="0.993788">
5.3 Grammar overlap results
</subsectionHeader>
<bodyText confidence="0.9997126">
In Table 5 we see a breakdown of the types of para-
phrases in the overlap for three of the models. Al-
though the PPDB-xl overlap is much larger than
the other two, about 80% of its rules are syntac-
tic transformations. The LDC and MSR models
have a much larger proportion of phrasal and lexi-
cal rules.
Next we will look at the grammar overlap num-
bers presented in Table 3 and Table 4.
Note the non-intuitive result that for some
grammars (notably PPDB), the non-strict overlap
is smaller than the strict overlap. This is because
rules with different non-terminals only count once
in the non-strict overlap; for example, in PPDB-
small,
</bodyText>
<equation confidence="0.9884985">
NN ( answer ; reply)
VB ( answer ; reply)
</equation>
<bodyText confidence="0.999955166666667">
count as separate entries when calculating strictly,
but when ignoring non-terminals, they count as
only one type of rule.
The fact that the non-strict overlaps are smaller
means that there must be many rules in PPDB that
are identical except for non-terminal labels.
</bodyText>
<subsectionHeader confidence="0.972217">
5.4 Precision and recall results
</subsectionHeader>
<bodyText confidence="0.99996275">
Figure 4 shows relative recall and precision lower
bound calculated for various sizes of PPDB rela-
tive to the ParaMetric grammar. The x-axis rep-
resents the size of the grammar as we vary from
keeping only the most probable rules to including
less probable ones. Restricting to high probability
rules makes the grammar much smaller, resulting
in higher precision.
</bodyText>
<subsectionHeader confidence="0.994934">
5.5 Synchronous parsing results
</subsectionHeader>
<bodyText confidence="0.999927142857143">
Table 6 shows the percentage of sentence pairs that
were reachable in a held-out portion of the LDC
multiple-translation data.
We find that a grammar trained on LDC data
vastly outperforms data from any other domain.
This is not surprising — we shouldn’t expect a
model trained on French literature to be able to
</bodyText>
<page confidence="0.994959">
197
</page>
<table confidence="0.999512727272727">
Grammar % (all) % (any)
LDC Hiero 9.5 33.0
Lit. Hiero 1.8 9.6
MSR Hiero 1.7 9.2
LDC Syntax 9.1 30.2
Lit. Syntax 2.0 10.7
MSR Syntax 1.9 10.4
PM Syntax 1.7 9.8
PPDB-v0.2-small 1.8 3.3
PPDB-v0.2-large 2.5 4.5
PPDB-v0.2-xl 3.5 6.2
</table>
<tableCaption confidence="0.964753">
Table 6: Parse coverage on held-out LDC data.
</tableCaption>
<bodyText confidence="0.974942368421053">
The all column considers every possible sentential
paraphrase in the test set. The any column consid-
ers a sentence parsed if any of its paraphrases was
able to parsed.
handle some of the vocabulary found in news sto-
ries that were originally in Arabic or Chinese.
The PPDB data outperforms both French litera-
ture and MSR models if we look all possible sen-
tence pairs from test data (the column labeled “all”
in the table). However, when we consider whether
any pair from a set of 4 translations can be trans-
lated, the PPDB models do not do as well. This
implies that PPDB tends to be able to reach many
pairs from the same set of translations, but there
are many translations that it cannot handle at all.
By contrast, the literature- and MSR-trained mod-
els can reach at least one pair from 10% of the
test examples, even though the absolute number
of pairs they can reach is lower.
</bodyText>
<subsectionHeader confidence="0.990472">
5.6 Effects of grammar size and choice of
syntactic labels
</subsectionHeader>
<bodyText confidence="0.999948190476191">
Table 2 shows that the PPDB-derived grammars
are much larger than the syntactic models derived
from other domains. It may seem surprising that
they should perform worse, but adding more rules
to the grammar just by varying non-terminal labels
isn’t likely to help overall parse coverage. This
suggests a new pruning method: keep only the top
k label variations for each rule type.
If we compare the syntactic models to the Hi-
ero models trained from the same data, we see
that their overall reachability performance is not
very different. This implies that paraphrases can
be annotated with linguistic information without
necessarily hurting their ability to explain partic-
ular sentence pairs. Contrast this result, with, for
example, those of Koehn et al. (2003), showing
that restricting translation models to only syntac-
tic phrases hurts overall translation performance.
The comparable performance between Hiero and
syntactic models seems to hold regardless of do-
main.
</bodyText>
<sectionHeader confidence="0.623926" genericHeader="method">
6 Correlation with human judgments
</sectionHeader>
<bodyText confidence="0.999944909090909">
To validate PARADIGM, we calculated its correla-
tion with human judgments of paraphrase quality
on the sentence compression text-to-text genera-
tion task, which has been used to evaluate para-
phrase grammars in previous research (Cohn and
Lapata, 2007; Zhao et al., 2009; Ganitkevitch et
al., 2011; Napoles et al., 2011). We created sen-
tence compression systems for five of the para-
phrase grammars described in Section 5.1. We fol-
lowed the methodology outlined by Ganitkevitch
et al. (2011) and did the following:
</bodyText>
<listItem confidence="0.951673857142857">
• Each paraphrase grammar was augmented
with an appropriate set of rule-level features
that capture information pertinent to the task.
In this case, the paraphrase rules were given
two additional features that shows how the
number of words and characters changed af-
ter applying the rule.
• Similarly to how the weights of the mod-
els are set using minimum error rate training
in statistical machine translation, the weights
for each of the paraphrase grammars using
the PRO tuning method (Hopkins and May,
2011).
• Instead of optimizing to the BLEU metric, as
is done in machine translation, we optimized
to PR´ECIS, a metric developed for sentence
compression that adapts BLEU so that it in-
cludes a “verbosity penalty” (Ganitkevitch et
al., 2011) to encourage the compression sys-
tems to produce shorter output.
• We created a development set with sentence
</listItem>
<bodyText confidence="0.8904362">
compressions by selecting 1000 pairs of sen-
tences from the multiple translation corpus
where two English translations of the same
foreign sentences differed in each other by a
length ratio of 0.67–0.75.
</bodyText>
<listItem confidence="0.7356875">
• We decoded a test set of 1000 sentences us-
ing each of the grammars and its optimized
</listItem>
<page confidence="0.997575">
198
</page>
<bodyText confidence="0.999495020408163">
weights with the Joshua decoder (Ganitke-
vitch et al., 2012). The selected in the same
fashion as the dev sentences, so each one had
a human-created reference compression.
We conducted a human evaluation to judge the
meaning and grammaticality of the sentence com-
pressions derived from each paraphrase grammar.
We presented workers on Mechanical Turk with
the input sentence to the compression sentence
(the long sentence), along with 5 shortened out-
puts from our compression systems. To ensure
that workers were producing reliable judgments
we also presented them with a positive control (a
reference compression written by a person) and a
negative controls (a compressed output that was
generated by randomly deleted words). We ex-
cluded judgments from workers who did not per-
form well on the positive and negative controls.
Meaning and grammaticality were scored on
5-point scales where 5 is best. These human
scores were averaged over 2000 judgments (1000
sentences x 2 annotators) for each system. The
systems’ outputs were then scored with BLEU,
PR´ECIS, and their paraphrase grammars were
scored PARADIGM’s relative recall and precision
lower-bound estimates. For each grammar, we
also calculated the average length of parseable
sentences.
We calculated the correlation between the hu-
man judgements and the automatic scores, using
Spearman’s rank correlation coefficient p. This
is methodology is the same that is used to quan-
tify the goodness of automatic evaluation metrics
in the machine translation literature (Przybocki et
al., 2008; Callison-Burch et al., 2010). The pos-
sible values of p range between 1 (where all sys-
tems are ranked in the same order) and −1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for p is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute p.
Table 7 shows that our PARADIGM scores cor-
relate more highly with human judgments than ei-
ther BLEU or PR´ECIS for the 5 systems in our eval-
uation. This suggests that it may be a better predic-
tor of the goodness of paraphrase grammars than
MT metrics, when the paraphrase grammars are
used for text-to-text generation tasks.
</bodyText>
<table confidence="0.996732857142857">
MEANING GRAMMAR
BLEU -0.7 -0.1
PR´ECIS -0.6 +0.2
PINC +0.1 +0.4
PARADIGMprecision +0.6 +0.1
PARADIGMrecall +0.1 +0.4
PARADIGMavg−len -0.3 +0.4
</table>
<tableCaption confidence="0.75497">
Table 7: The correlation (Spearman’s p) of dif-
ferent automatic evaluation metrics with human
judgments of paraphrase quality for the text-to-
text generation task of sentence compression.
</tableCaption>
<sectionHeader confidence="0.99689" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.707846571428571">
We have introduced two new metrics for evaluat-
ing paraphrase grammars, and looked at several
models from a variety of domains. Using these
metrics we can perform a variety of analyses about
SCFG-based paraphrase models:
• Automatically-extracted grammars can parse
a small fraction of held-out data (≤30%).
This is comparable to results in MT (Auli et
al., 2009).
• In-domain training data is necessary in or-
der to parse held-out data. A model trained
on newswire data parsed 30% of held-out
newswire sentence pairs, versus to &lt;10% for
literature or parliamentary data.
</bodyText>
<listItem confidence="0.96017525">
• SCFGs with syntactic labels perform just as
well as simpler models with a single non-
terminal label.
• Automatically-extracted syntactic grammars
</listItem>
<bodyText confidence="0.977518533333333">
tend to have a reasonable overlap with gram-
mars derived from human-aligned data, in-
cluding more 45% of the gold-standard gram-
mar’s paraphrase rules that occurred at least
twice.
• We showed that PARADIGM more strongly
correlates with human judgments of the
meaning and grammaticality of paraphrases
produced by sentence compression systems
than standard automatic evaluation measures
like BLEU.
PARADIGM will help researchers developing
paraphrase resources to perform similar diagnos-
tics on their models, and quickly evaluate their
systems.
</bodyText>
<page confidence="0.998504">
199
</page>
<sectionHeader confidence="0.998505" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994355">
This material is based on research sponsored by
the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709505617977">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling. Pren-
tice Hall.
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG augmented hierarchical phrase-based machine
translation. In Proc. of IWSLT.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proc. of ACL.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. ParaMetric: An automatic evaluation
metric for paraphrasing. In Proc. of COLING.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation (WMT10).
David L. Chen and William Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proc. of ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of EMNLP-
CoLing.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research (JAIR), 34:637–674.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrases cor-
pora: Exploiting massively parallel news sources. In
Proc. of COLING.
Mark Dras. 1997. Representing paraphrases using
synchronous tree adjoining grammars. In Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 516–518,
Madrid, Spain, July. Association for Computational
Linguistics.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263–266. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages
273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ofACL, pages 961–968.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283–291, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. NAACL.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proc. of the Second International
Workshop on Paraphrasing.
</reference>
<page confidence="0.939685">
200
</page>
<reference confidence="0.999755719626168">
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philip M. Lewis and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the ACM,
15(3):465–488.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proc. of
ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 104–111, New
York City, USA, June. Association for Computa-
tional Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343–360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: a paraphrase evaluation metric exploit-
ing parallel texts. In Proc. of EMNLP.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 802–811, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–388.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91–97, Portland, Oregon,
June. Association for Computational Linguistics.
Franz Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440–447, Hong Kong,
China, October.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Proc. of
WMT.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 “Met-
rics for MAchine TRanslation” challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monlingual machine translation for paraphrase gen-
eration. In Proc. of EMNLP.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117–127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Robert Borsley and
Kersti B¨orjars, editors, Non-Transformational Syn-
tax. Wiley-Blackwell.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of ACL.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
478–484, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proc. of WMT.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings ofACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using para-
phrases to evaluate summaries automatically. In
Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York City,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.9984">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473804">
<title confidence="0.999269">Paraphrase Diagnostics through Grammar Matching</title>
<author confidence="0.89354">Weese Ganitkevitch Chris Callison-Burch</author>
<affiliation confidence="0.508904">Johns Hopkins University University of Pennsylvania</affiliation>
<abstract confidence="0.997972">Paraphrase evaluation is typically done either manually or through indirect, taskbased evaluation. We introduce an inevaluation measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We that with hujudgments more strongly than on a task-based evaluation of paraphrase quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="7128" citStr="Aho and Ullman, 1972" startWordPosition="1043" endWordPosition="1046">between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain models are able to produce synchronous parses of sentence pairs drawn from monolingual parallel corpora. PARADIGM’s different metrics are explained in Section 4, but first we give background on synchronous parsing and synchronous grammars. 2.1 Synchronous parsing with SCFGs Synchronous context-free grammars An SCFG (Lewis and Stearns, 1968; Aho and Ullman, 1972) is similar to a context-free grammar, except that it generates pairs of strings in correspondence. Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropria</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hala Almaghout</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>CCG augmented hierarchical phrase-based machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of IWSLT.</booktitle>
<contexts>
<context position="8100" citStr="Almaghout et al., 2010" startWordPosition="1198" endWordPosition="1201">als (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs ar</context>
</contexts>
<marker>Almaghout, Jiang, Way, 2010</marker>
<rawString>Hala Almaghout, Jie Jiang, and Andy Way. 2010. CCG augmented hierarchical phrase-based machine translation. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="8644" citStr="Auli et al. (2009)" startWordPosition="1282" endWordPosition="1285">(Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a phrase-based model. This result is confirmed by our results in paraphrasing. 3 Paraphrase grammar extraction Like ParaMetric, PARADIGM extracts gold standard paraphrases from word-aligned sentential paraphrases. PARADIGM goes further by parsing one of the two input sentences, and uses the parse tree to extract syntactic paraphrase rules, following recent advances in syntactic approaches to machine translation (like Galley et al. (2004), Zollmann and Venugopal (2006), and others). Figure 1 shows an example of a parsed s</context>
<context position="29676" citStr="Auli et al., 2009" startWordPosition="4746" endWordPosition="4749">0.6 +0.1 PARADIGMrecall +0.1 +0.4 PARADIGMavg−len -0.3 +0.4 Table 7: The correlation (Spearman’s p) of different automatic evaluation metrics with human judgments of paraphrase quality for the text-totext generation task of sentence compression. 7 Summary We have introduced two new metrics for evaluating paraphrase grammars, and looked at several models from a variety of domains. Using these metrics we can perform a variety of analyses about SCFG-based paraphrase models: • Automatically-extracted grammars can parse a small fraction of held-out data (≤30%). This is comparable to results in MT (Auli et al., 2009). • In-domain training data is necessary in order to parse held-out data. A model trained on newswire data parsed 30% of held-out newswire sentence pairs, versus to &lt;10% for literature or parliamentary data. • SCFGs with syntactic labels perform just as well as simpler models with a single nonterminal label. • Automatically-extracted syntactic grammars tend to have a reasonable overlap with grammars derived from human-aligned data, including more 45% of the gold-standard grammar’s paraphrase rules that occurred at least twice. • We showed that PARADIGM more strongly correlates with human judgm</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search spaces. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2092" citStr="Bannard and Callison-Burch (2005)" startWordPosition="293" endWordPosition="296">rase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. Different researchers have used different criteria for manual evaluations. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence simila</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1838" citStr="Barzilay and McKeown (2001)" startWordPosition="260" endWordPosition="263">s more strongly than BLEU on a task-based evaluation of paraphrase quality. 1 Introduction Paraphrases are useful in a wide range of natural language processing applications. A variety of data-driven approaches have been proposed to generate paraphrase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. Different researchers have used different criteria for manual evaluations. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potentia</context>
<context position="15922" citStr="Barzilay and McKeown (2001)" startWordPosition="2493" endWordPosition="2496">NNS JJ NP NP VP NP S VBD NP VP 195 Grammar Rules LDC Hiero 52,784,462 Lit. Hiero 3,288,546 MSR Hiero 2,456,513 ParaMetric Hiero 584,944 LDC Syntax 23,978,477 Lit. Syntax 715,154 MSR Syntax 406,115 ParaMetric Syntax 317,772 PPDB-v0.2-small 1,292,224 PPDB-v0.2-large 9,456,356 PPDB-v0.2-xl 46,592,161 Corpus sentence total pairs words LDC Multiple Translations 83,284 2,254,707 Classic French Literature 75,106 682,978 MSR Paraphrase Corpus 5,801 219,492 ParaMetric 970 21,944 Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually wor</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>ParaMetric: An automatic evaluation metric for paraphrasing.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="3844" citStr="Callison-Burch et al., 2008" startWordPosition="536" endWordPosition="539"> grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus, which have been previously proposed as a good resource 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics for paraphrase evaluation (Callison-Burch et al., 2008; Cohn et al., 2008). The first of our two proposed metrics calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in a test set. The second measure enumerates paraphrase rules from a monolingual parallel corpus and calculates the overlap between this reference paraphrase collection, and the paraphrase resource being evaluated. 2 Related work and background The most closely related work is ParaMetric (Callison-Burch et al., 2008), which is a set of objective measures for evaluating the quality of phrase-based paraphrases. ParaMetric extracts a set of gold-s</context>
<context position="14111" citStr="Callison-Burch et al. (2008)" startWordPosition="2215" endWordPosition="2218">te non-strict overlap, we ignore the identities of non-terminal symbols in the left-hand and right-hand sides of the rules. That is, two rules are considered equivalent if they are identical after all the non-terminal symbols have been replaced by one equivalent symbol. For example, in non-strict overlap, the syntactic rule NP —* (N1 ’s N2; the N2 of N1) would match the Hiero rule X —* (X1 ’s X2; the X2 of X1) If we are considering two Hiero grammars, strict and non-strict intersection are the same operation since they only have on non-terminal X. 4.3 Precision lower bound and relative recall Callison-Burch et al. (2008) use the notion of overlap between two paraphrase sets to define two metrics, precision lower bound and relative recall. These are calculated the same way as standard precision and recall. Relative recall is qualified as “relative” because it is calculated on a potentially incomplete set of gold standard paraphrases. There may exist valid paraphrases that do not occur in that set. Similarly, only a lower bound on precision can be calculated because the candidate set may contain valid paraphrases that do not occur in the gold standard set. 5 Experiments 5.1 Data We extracted paraphrase grammars</context>
<context position="16038" citStr="Callison-Burch et al. (2008)" startWordPosition="2514" endWordPosition="2517">Metric Hiero 584,944 LDC Syntax 23,978,477 Lit. Syntax 715,154 MSR Syntax 406,115 ParaMetric Syntax 317,772 PPDB-v0.2-small 1,292,224 PPDB-v0.2-large 9,456,356 PPDB-v0.2-xl 46,592,161 Corpus sentence total pairs words LDC Multiple Translations 83,284 2,254,707 Classic French Literature 75,106 682,978 MSR Paraphrase Corpus 5,801 219,492 ParaMetric 970 21,944 Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sou</context>
</contexts>
<marker>Callison-Burch, Cohn, Lapata, 2008</marker>
<rawString>Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 2008. ParaMetric: An automatic evaluation metric for paraphrasing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT10).</booktitle>
<contexts>
<context position="28300" citStr="Callison-Burch et al., 2010" startWordPosition="4518" endWordPosition="4521">dgments (1000 sentences x 2 annotators) for each system. The systems’ outputs were then scored with BLEU, PR´ECIS, and their paraphrase grammars were scored PARADIGM’s relative recall and precision lower-bound estimates. For each grammar, we also calculated the average length of parseable sentences. We calculated the correlation between the human judgements and the automatic scores, using Spearman’s rank correlation coefficient p. This is methodology is the same that is used to quantify the goodness of automatic evaluation metrics in the machine translation literature (Przybocki et al., 2008; Callison-Burch et al., 2010). The possible values of p range between 1 (where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for p is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute p. Table 7 shows that our PARADIGM scores correlate more highly with human judgments than either BLEU or PR´ECIS for the 5 systems in our evaluation. This suggests that it may be a better predictor of the goodness of paraphrase grammars than MT metrics</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5818" citStr="Chen and Dolan (2011)" startWordPosition="849" endWordPosition="852">Cartney et al. (2008)). Most methods produce a list of paraphrases for a given input phrase. So CallisonBurch et al. (2008) calculate two more generally applicable measures by comparing the paraphrases in an automatically extracted resource to gold standard paraphrases extracted via the alignments. These allow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaM</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7790" citStr="Chiang, 2007" startWordPosition="1152" endWordPosition="1153">t it generates pairs of strings in correspondence. Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be si</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoLing.</booktitle>
<contexts>
<context position="2492" citStr="Cohn and Lapata (2007)" startWordPosition="350" endWordPosition="353">asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous gramm</context>
<context position="25315" citStr="Cohn and Lapata, 2007" startWordPosition="4039" endWordPosition="4042">hurting their ability to explain particular sentence pairs. Contrast this result, with, for example, those of Koehn et al. (2003), showing that restricting translation models to only syntactic phrases hurts overall translation performance. The comparable performance between Hiero and syntactic models seems to hold regardless of domain. 6 Correlation with human judgments To validate PARADIGM, we calculated its correlation with human judgments of paraphrase quality on the sentence compression text-to-text generation task, which has been used to evaluate paraphrase grammars in previous research (Cohn and Lapata, 2007; Zhao et al., 2009; Ganitkevitch et al., 2011; Napoles et al., 2011). We created sentence compression systems for five of the paraphrase grammars described in Section 5.1. We followed the methodology outlined by Ganitkevitch et al. (2011) and did the following: • Each paraphrase grammar was augmented with an appropriate set of rule-level features that capture information pertinent to the task. In this case, the paraphrase rules were given two additional features that shows how the number of words and characters changed after applying the rule. • Similarly to how the weights of the models are </context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of EMNLPCoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>34--637</pages>
<contexts>
<context position="6148" citStr="Cohn and Lapata (2009)" startWordPosition="898" endWordPosition="901">precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates h</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research (JAIR), 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing corpora for the development and evaluation of paraphrase systems.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="3864" citStr="Cohn et al., 2008" startWordPosition="540" endWordPosition="543"> formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus, which have been previously proposed as a good resource 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics for paraphrase evaluation (Callison-Burch et al., 2008; Cohn et al., 2008). The first of our two proposed metrics calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in a test set. The second measure enumerates paraphrase rules from a monolingual parallel corpus and calculates the overlap between this reference paraphrase collection, and the paraphrase resource being evaluated. 2 Related work and background The most closely related work is ParaMetric (Callison-Burch et al., 2008), which is a set of objective measures for evaluating the quality of phrase-based paraphrases. ParaMetric extracts a set of gold-standard phrasal para</context>
<context position="16578" citStr="Cohn et al. (2008)" startWordPosition="2601" endWordPosition="2604"> and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraph</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing corpora for the development and evaluation of paraphrase systems. Computational Linguistics, 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrases corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="15984" citStr="Dolan et al. (2004)" startWordPosition="2506" endWordPosition="2509">Lit. Hiero 3,288,546 MSR Hiero 2,456,513 ParaMetric Hiero 584,944 LDC Syntax 23,978,477 Lit. Syntax 715,154 MSR Syntax 406,115 ParaMetric Syntax 317,772 PPDB-v0.2-small 1,292,224 PPDB-v0.2-large 9,456,356 PPDB-v0.2-xl 46,592,161 Corpus sentence total pairs words LDC Multiple Translations 83,284 2,254,707 Classic French Literature 75,106 682,978 MSR Paraphrase Corpus 5,801 219,492 ParaMetric 970 21,944 Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrases corpora: Exploiting massively parallel news sources. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Representing paraphrases using synchronous tree adjoining grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>516--518</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="3303" citStr="Dras, 1997" startWordPosition="462" endWordPosition="463">ity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus, which have been previously proposed as a good resource 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics for paraphrase evaluation (Callison-Burch et al., 2008; Cohn et al., 2008). The first of our two proposed metrics</context>
</contexts>
<marker>Dras, 1997</marker>
<rawString>Mark Dras. 1997. Representing paraphrases using synchronous tree adjoining grammars. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 516–518, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Two monolingual parses are better than one (synchronous parse).</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>263--266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Dyer, 2010</marker>
<rawString>Chris Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proceedings of HLT/NAACL, pages 263–266. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="7928" citStr="Galley et al., 2004" startWordPosition="1172" endWordPosition="1175">es, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be add</context>
<context position="9159" citStr="Galley et al. (2004)" startWordPosition="1358" endWordPosition="1361">nd-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a phrase-based model. This result is confirmed by our results in paraphrasing. 3 Paraphrase grammar extraction Like ParaMetric, PARADIGM extracts gold standard paraphrases from word-aligned sentential paraphrases. PARADIGM goes further by parsing one of the two input sentences, and uses the parse tree to extract syntactic paraphrase rules, following recent advances in syntactic approaches to machine translation (like Galley et al. (2004), Zollmann and Venugopal (2006), and others). Figure 1 shows an example of a parsed sentence pair. From that pair it is possible to extract a wide variety of non-identical paraphrases, which include lexical paraphrases (single word synonyms), phrasal paraphrases, and syntactic paraphrases that include a mix of words and syntactic non-terminal CC → and while VBP → want propose VBP → expect want DT → some some people S → him to step down him to resign VP → step down resign VP → to step down to resign VP → want to impeach him propose to impeach him VP → want VP propose VP VP → want to impeach PRP</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL 2004: Main Proceedings, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve Deneefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="7949" citStr="Galley et al., 2006" startWordPosition="1176" endWordPosition="1179">ntain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, Deneefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. ofACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Ganitkevitch, Callison-Burch, Napoles, Van Durme, 2011</marker>
<rawString>Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme. 2011. Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Yuan Cao</author>
<author>Jonathan Weese</author>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, pro, and paraphrases.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>283--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="26787" citStr="Ganitkevitch et al., 2012" startWordPosition="4284" endWordPosition="4288">lation, we optimized to PR´ECIS, a metric developed for sentence compression that adapts BLEU so that it includes a “verbosity penalty” (Ganitkevitch et al., 2011) to encourage the compression systems to produce shorter output. • We created a development set with sentence compressions by selecting 1000 pairs of sentences from the multiple translation corpus where two English translations of the same foreign sentences differed in each other by a length ratio of 0.67–0.75. • We decoded a test set of 1000 sentences using each of the grammars and its optimized 198 weights with the Joshua decoder (Ganitkevitch et al., 2012). The selected in the same fashion as the dev sentences, so each one had a human-created reference compression. We conducted a human evaluation to judge the meaning and grammaticality of the sentence compressions derived from each paraphrase grammar. We presented workers on Mechanical Turk with the input sentence to the compression sentence (the long sentence), along with 5 shortened outputs from our compression systems. To ensure that workers were producing reliable judgments we also presented them with a positive control (a reference compression written by a person) and a negative controls (</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch. 2012. Joshua 4.0: Packing, pro, and paraphrases. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 283–291, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proc. NAACL.</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="8624" citStr="Gimpel and Smith, 2012" startWordPosition="1278" endWordPosition="1281">atory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a phrase-based model. This result is confirmed by our results in paraphrasing. 3 Paraphrase grammar extraction Like ParaMetric, PARADIGM extracts gold standard paraphrases from word-aligned sentential paraphrases. PARADIGM goes further by parsing one of the two input sentences, and uses the parse tree to extract syntactic paraphrase rules, following recent advances in syntactic approaches to machine translation (like Galley et al. (2004), Zollmann and Venugopal (2006), and others). Figure 1 shows an e</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="26088" citStr="Hopkins and May, 2011" startWordPosition="4167" endWordPosition="4170">ribed in Section 5.1. We followed the methodology outlined by Ganitkevitch et al. (2011) and did the following: • Each paraphrase grammar was augmented with an appropriate set of rule-level features that capture information pertinent to the task. In this case, the paraphrase rules were given two additional features that shows how the number of words and characters changed after applying the rule. • Similarly to how the weights of the models are set using minimum error rate training in statistical machine translation, the weights for each of the paraphrase grammars using the PRO tuning method (Hopkins and May, 2011). • Instead of optimizing to the BLEU metric, as is done in machine translation, we optimized to PR´ECIS, a metric developed for sentence compression that adapts BLEU so that it includes a “verbosity penalty” (Ganitkevitch et al., 2011) to encourage the compression systems to produce shorter output. • We created a development set with sentence compressions by selecting 1000 pairs of sentences from the multiple translation corpus where two English translations of the same foreign sentences differed in each other by a length ratio of 0.67–0.75. • We decoded a test set of 1000 sentences using eac</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Ibrahim</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Extracting structural paraphrases from aligned monolingual corpora.</title>
<date>2003</date>
<booktitle>In Proc. of the Second International Workshop on Paraphrasing.</booktitle>
<contexts>
<context position="1971" citStr="Ibrahim et al. (2003)" startWordPosition="277" endWordPosition="280">l language processing applications. A variety of data-driven approaches have been proposed to generate paraphrase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. Different researchers have used different criteria for manual evaluations. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence c</context>
</contexts>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extracting structural paraphrases from aligned monolingual corpora. In Proc. of the Second International Workshop on Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2829" citStr="Kauchak and Barzilay (2006)" startWordPosition="397" endWordPosition="400">ved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase gra</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="24823" citStr="Koehn et al. (2003)" startWordPosition="3967" endWordPosition="3970">they should perform worse, but adding more rules to the grammar just by varying non-terminal labels isn’t likely to help overall parse coverage. This suggests a new pruning method: keep only the top k label variations for each rule type. If we compare the syntactic models to the Hiero models trained from the same data, we see that their overall reachability performance is not very different. This implies that paraphrases can be annotated with linguistic information without necessarily hurting their ability to explain particular sentence pairs. Contrast this result, with, for example, those of Koehn et al. (2003), showing that restricting translation models to only syntactic phrases hurts overall translation performance. The comparable performance between Hiero and syntactic models seems to hold regardless of domain. 6 Correlation with human judgments To validate PARADIGM, we calculated its correlation with human judgments of paraphrase quality on the sentence compression text-to-text generation task, which has been used to evaluate paraphrase grammars in previous research (Cohn and Lapata, 2007; Zhao et al., 2009; Ganitkevitch et al., 2011; Napoles et al., 2011). We created sentence compression syste</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="7105" citStr="Lewis and Stearns, 1968" startWordPosition="1039" endWordPosition="1042">we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain models are able to produce synchronous parses of sentence pairs drawn from monolingual parallel corpora. PARADIGM’s different metrics are explained in Section 4, but first we give background on synchronous parsing and synchronous grammars. 2.1 Synchronous parsing with SCFGs Synchronous context-free grammars An SCFG (Lewis and Stearns, 1968; Aho and Ullman, 1972) is similar to a context-free grammar, except that it generates pairs of strings in correspondence. Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused o</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction. Journal of the ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006a. An end-to-end discriminative approach to machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="8598" citStr="Liang et al., 2006" startWordPosition="1274" endWordPosition="1277">. Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a phrase-based model. This result is confirmed by our results in paraphrasing. 3 Paraphrase grammar extraction Like ParaMetric, PARADIGM extracts gold standard paraphrases from word-aligned sentential paraphrases. PARADIGM goes further by parsing one of the two input sentences, and uses the parse tree to extract syntactic paraphrase rules, following recent advances in syntactic approaches to machine translation (like Galley et al. (2004), Zollmann and Venugopal (2006), and ot</context>
<context position="16923" citStr="Liang et al., 2006" startWordPosition="2663" endWordPosition="2666">articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006b. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules from text.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2414" citStr="Lin and Pantel (2001)" startWordPosition="340" endWordPosition="343">ons. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules from text. Natural Language Engineering, 7(3):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>PEM: a paraphrase evaluation metric exploiting parallel texts.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5591" citStr="Liu et al. (2010)" startWordPosition="810" endWordPosition="813">gnments were used to calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. This measure is limited to a class of paraphrasing techniques that perform alignment (like MacCartney et al. (2008)). Most methods produce a list of paraphrases for a given input phrase. So CallisonBurch et al. (2008) calculate two more generally applicable measures by comparing the paraphrases in an automatically extracted resource to gold standard paraphrases extracted via the alignments. These allow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. PEM: a paraphrase evaluation metric exploiting parallel texts. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>802--811</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="5218" citStr="MacCartney et al. (2008)" startWordPosition="749" endWordPosition="752">from a data set originally created to evaluate machine translation output using the BLEU metric. Cohn et al. (2008) argue that these sorts of monolingual parallel corpora are appropriate for evaluating paraphrase systems, because they are naturally occurring sources of paraphrases. Callison-Burch et al. (2008) calculated three types of metrics in ParaMetric. The manual word alignments were used to calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. This measure is limited to a class of paraphrasing techniques that perform alignment (like MacCartney et al. (2008)). Most methods produce a list of paraphrases for a given input phrase. So CallisonBurch et al. (2008) calculate two more generally applicable measures by comparing the paraphrases in an automatically extracted resource to gold standard paraphrases extracted via the alignments. These allow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011)</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="1502" citStr="Madnani and Dorr (2010)" startWordPosition="213" endWordPosition="216">ource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality. 1 Introduction Paraphrases are useful in a wide range of natural language processing applications. A variety of data-driven approaches have been proposed to generate paraphrase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. Different researchers have used different criteria for manual evaluations. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
</authors>
<title>The Circle of Meaning: From Translation to Paraphrasing and Back.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Maryland College Park.</institution>
<contexts>
<context position="2865" citStr="Madnani (2010)" startWordPosition="405" endWordPosition="406">s of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold standard sentential</context>
<context position="6164" citStr="Madnani (2010)" startWordPosition="902" endWordPosition="903">ecall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain </context>
</contexts>
<marker>Madnani, 2010</marker>
<rawString>Nitin Madnani. 2010. The Circle of Meaning: From Translation to Paraphrasing and Back. Ph.D. thesis, Department of Computer Science, University of Maryland College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Evaluating sentence compression: Pitfalls and suggested remedies.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-ToText Generation,</booktitle>
<pages>91--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>Napoles, Van Durme, Callison-Burch, 2011</marker>
<rawString>Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating sentence compression: Pitfalls and suggested remedies. In Proceedings of the Workshop on Monolingual Text-ToText Generation, pages 91–97, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="17006" citStr="Och and Ney, 2000" startWordPosition="2676" endWordPosition="2679">ere aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were extracted using the pivoting technique (Bannard and Callison-Burch, Grammar freq. </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Juri Ganitkevitch</author>
<author>Luke Orland</author>
<author>Jonathan Weese</author>
<author>Yuan Cao</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 5.0: Sparser, better, faster, server.</title>
<date>2013</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="18800" citStr="Post et al., 2013" startWordPosition="2963" endWordPosition="2966">ll rules that appeared only once from the ParaMetric grammar. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap. 2005) on bilingual parallel corpora containing over 42 million sentence pairs. The PPDB release includes a tool for pruning the grammar to a smaller size by retaining only high-precision paraphrases. We include PPDB grammars for several different pruning settings in our analysis. 5.2 Experimental setup We calculated our two metrics for each of the grammars listed in Table 2. To perform synchronous parsing, we used the Joshua decoder (Post et al., 2013), which includes an implementation of Dyer’s two-pass parsing algorithm (2010). After splitting the LDC data into 10 equal pieces, we trained paraphrase models on nine-tenths of the data and parsed the other tenth. Grammars trained from other sources (the MSR corpus, French literature domain, and PPDB) were also evaluated on the held-out tenth of LDC data. 196 Grammar freq. &gt; 1 freq. &gt; 2 ParaMetric Syntax 200,385 20,699 LDC Hiero 41,346 (20.6%) 5,323 (25.8%) Lit. Hiero 36,873 (18.4%) 4,606 (22.3%) MSR Hiero 58,970 (29.4%) 6,741 (32.6%) LDC Syntax 37,231 (11.7%) 5,055 (24.5%) Lit. Syntax 19,530</context>
</contexts>
<marker>Post, Ganitkevitch, Orland, Weese, Cao, Callison-Burch, 2013</marker>
<rawString>Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan Weese, Yuan Cao, and Chris Callison-Burch. 2013. Joshua 5.0: Sparser, better, faster, server. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>Sebastian Bronsart</author>
</authors>
<date>2008</date>
<booktitle>Official results of the NIST 2008 “Metrics for MAchine TRanslation” challenge (MetricsMATR08). In AMTA-2008 workshop on Metrics for Machine Translation.</booktitle>
<contexts>
<context position="28270" citStr="Przybocki et al., 2008" startWordPosition="4514" endWordPosition="4517">re averaged over 2000 judgments (1000 sentences x 2 annotators) for each system. The systems’ outputs were then scored with BLEU, PR´ECIS, and their paraphrase grammars were scored PARADIGM’s relative recall and precision lower-bound estimates. For each grammar, we also calculated the average length of parseable sentences. We calculated the correlation between the human judgements and the automatic scores, using Spearman’s rank correlation coefficient p. This is methodology is the same that is used to quantify the goodness of automatic evaluation metrics in the machine translation literature (Przybocki et al., 2008; Callison-Burch et al., 2010). The possible values of p range between 1 (where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for p is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute p. Table 7 shows that our PARADIGM scores correlate more highly with human judgments than either BLEU or PR´ECIS for the 5 systems in our evaluation. This suggests that it may be a better predictor of the goodness of parap</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>Mark Przybocki, Kay Peterson, and Sebastian Bronsart. 2008. Official results of the NIST 2008 “Metrics for MAchine TRanslation” challenge (MetricsMATR08). In AMTA-2008 workshop on Metrics for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monlingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="15960" citStr="Quirk et al. (2004)" startWordPosition="2501" endWordPosition="2504">es LDC Hiero 52,784,462 Lit. Hiero 3,288,546 MSR Hiero 2,456,513 ParaMetric Hiero 584,944 LDC Syntax 23,978,477 Lit. Syntax 715,154 MSR Syntax 406,115 ParaMetric Syntax 317,772 PPDB-v0.2-small 1,292,224 PPDB-v0.2-large 9,456,356 PPDB-v0.2-xl 46,592,161 Corpus sentence total pairs words LDC Multiple Translations 83,284 2,254,707 Classic French Literature 75,106 682,978 MSR Paraphrase Corpus 5,801 219,492 ParaMetric 970 21,944 Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles drawn from different web sites in the same date rate. The sentence pairs were aligned heuristically aligned and then manually judged to be paraphrases. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by </context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monlingual machine translation for paraphrase generation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="2890" citStr="Snover et al. (2010)" startWordPosition="408" endWordPosition="411">ve evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2010</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2010. Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation, 23(2-3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar.</title>
<date>2011</date>
<editor>In Robert Borsley and Kersti B¨orjars, editors, Non-Transformational Syntax.</editor>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="8056" citStr="Steedman and Baldridge, 2011" startWordPosition="1190" endWordPosition="1193">n the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that</context>
</contexts>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Mark Steedman and Jason Baldridge. 2011. Combinatory categorial grammar. In Robert Borsley and Kersti B¨orjars, editors, Non-Transformational Syntax. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Juri Ganitkevitch</author>
<author>Chris CallisonBurch</author>
<author>Matt Post</author>
<author>Adam Lopez</author>
</authors>
<title>Joshua 3.0: Syntax-based machine translation with the thrax grammar extractor.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>478--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="6357" citStr="Weese et al., 2011" startWordPosition="930" endWordPosition="933">tic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain models are able to produce synchronous parses of sentence pairs drawn from monolingual parallel corpora. PARADIGM’s different metrics are explained in Section 4, but first we give background on</context>
<context position="17119" citStr="Weese et al., 2011" startWordPosition="2696" endWordPosition="2699">ts of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were extracted using the pivoting technique (Bannard and Callison-Burch, Grammar freq. &gt; 1 freq. &gt; 2 ParaMetric Syntax 317,772 21,709 LDC Hiero 5,840 (1.8%) 416 (1.9%) Lit. Hiero 6,152 (1.9%) 359 (1.7</context>
</contexts>
<marker>Weese, Ganitkevitch, CallisonBurch, Post, Lopez, 2011</marker>
<rawString>Jonathan Weese, Juri Ganitkevitch, Chris CallisonBurch, Matt Post, and Adam Lopez. 2011. Joshua 3.0: Syntax-based machine translation with the thrax grammar extractor. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 478–484, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Using categorial grammar to label translation rules.</title>
<date>2012</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="8121" citStr="Weese et al., 2012" startWordPosition="1202" endWordPosition="1205">e in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a p</context>
</contexts>
<marker>Weese, Callison-Burch, Lopez, 2012</marker>
<rawString>Jonathan Weese, Chris Callison-Burch, and Adam Lopez. 2012. Using categorial grammar to label translation rules. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="8152" citStr="Wu (1997)" startWordPosition="1208" endWordPosition="1209"> non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (Liang et al., 2006a; Gimpel and Smith, 2012). Auli et al. (2009) showed that only approximately 30% of training pairs are reachable under a phrase-based model. This result </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot approach for extracting paraphrase patterns from bilingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="6124" citStr="Zhao et al. (2008)" startWordPosition="894" endWordPosition="897">ow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this ca</context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot approach for extracting paraphrase patterns from bilingual corpora. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Application-driven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2601" citStr="Zhao et al. (2009)" startWordPosition="366" endWordPosition="369">s whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchron</context>
<context position="25334" citStr="Zhao et al., 2009" startWordPosition="4043" endWordPosition="4046">o explain particular sentence pairs. Contrast this result, with, for example, those of Koehn et al. (2003), showing that restricting translation models to only syntactic phrases hurts overall translation performance. The comparable performance between Hiero and syntactic models seems to hold regardless of domain. 6 Correlation with human judgments To validate PARADIGM, we calculated its correlation with human judgments of paraphrase quality on the sentence compression text-to-text generation task, which has been used to evaluate paraphrase grammars in previous research (Cohn and Lapata, 2007; Zhao et al., 2009; Ganitkevitch et al., 2011; Napoles et al., 2011). We created sentence compression systems for five of the paraphrase grammars described in Section 5.1. We followed the methodology outlined by Ganitkevitch et al. (2011) and did the following: • Each paraphrase grammar was augmented with an appropriate set of rule-level features that capture information pertinent to the task. In this case, the paraphrase rules were given two additional features that shows how the number of words and characters changed after applying the rule. • Similarly to how the weights of the models are set using minimum e</context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical paraphrase generation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2849" citStr="Zhou et al. (2006)" startWordPosition="401" endWordPosition="404">matical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold sta</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="7980" citStr="Zollmann and Venugopal, 2006" startWordPosition="1180" endWordPosition="1183">and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 Figure 1: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs. that the average parse time can be significantly improved by using a two-pass algorithm. The question of whether a source-reference pair is reachable under a model must be addressed in end-to-end discriminative training in MT (</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>