<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000104">
<title confidence="0.9983035">
Computing Consensus Translation from Multiple Machine Translation
Systems Using Enhanced Hypotheses Alignment
</title>
<author confidence="0.958935">
Evgeny Matusov, Nicola Ueffing, Hermann Ney
</author>
<affiliation confidence="0.8542785">
Lehrstuhl f¨ur Informatik VI - Computer Science Department
RWTH Aachen University, Aachen, Germany.
</affiliation>
<email confidence="0.99602">
{matusov,ueffing,ney}@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985357142857">
This paper describes a novel method for
computing a consensus translation from
the outputs of multiple machine trans-
lation (MT) systems. The outputs are
combined and a possibly new transla-
tion hypothesis can be generated. Simi-
larly to the well-established ROVER ap-
proach of (Fiscus, 1997) for combining
speech recognition hypotheses, the con-
sensus translation is computed by voting
on a confusion network. To create the con-
fusion network, we produce pairwise word
alignments of the original machine trans-
lation hypotheses with an enhanced sta-
tistical alignment algorithm that explicitly
models word reordering. The context of a
whole document of translations rather than
a single sentence is taken into account to
produce the alignment.
The proposed alignment and voting ap-
proach was evaluated on several machine
translation tasks, including a large vocab-
ulary task. The method was also tested in
the framework of multi-source and speech
translation. On all tasks and conditions,
we achieved significant improvements in
translation quality, increasing e. g. the
BLEU score by as much as 15% relative.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952954545455">
In this work we describe a novel technique for
computing a consensus translation from the out-
puts of multiple machine translation systems.
Combining outputs from different systems
was shown to be quite successful in automatic
speech recognition (ASR). Voting schemes like
the ROVER approach of (Fiscus, 1997) use edit
distance alignment and time information to cre-
ate confusion networks from the output of several
ASR systems.
Some research on multi-engine machine trans-
lation has also been performed in recent years.
The most straightforward approaches simply se-
lect, for each sentence, one of the provided hy-
potheses. The selection is made based on the
scores of translation, language, and other mod-
els (Nomoto, 2004; Paul et al., 2005). Other
approaches combine lattices or N-best lists from
several different MT systems (Frederking and
Nirenburg, 1994). To be successful, such ap-
proaches require compatible lattices and compa-
rable scores of the (word) hypotheses in the lat-
tices. However, the scores of most statistical ma-
chine translation (SMT) systems are not normal-
ized and therefore not directly comparable. For
some other MT systems (e.g. knowledge-based
systems), the lattices and/or scores of hypotheses
may not be even available.
(Bangalore et al., 2001) used the edit distance
alignment extended to multiple sequences to con-
struct a confusion network from several transla-
tion hypotheses. This algorithm produces mono-
tone alignments only (i. e. allows insertion, dele-
tion, and substitution of words); it is not able to
align translation hypotheses with significantly dif-
ferent word order. (Jayaraman and Lavie, 2005)
try to overcome this problem. They introduce a
method that allows non-monotone alignments of
words in different translation hypotheses for the
same sentence. However, this approach uses many
heuristics and is based on the alignment that is per-
formed to calculate a specific MT error measure;
the performance improvements are reported only
in terms of this measure.
</bodyText>
<page confidence="0.998253">
33
</page>
<bodyText confidence="0.999955916666667">
Here, we propose an alignment procedure that
explicitly models reordering of words in the hy-
potheses. In contrast to existing approaches, the
context of the whole document rather than a sin-
gle sentence is considered in this iterative, unsu-
pervised procedure, yielding a more reliable align-
ment.
Based on the alignment, we construct a con-
fusion network from the (possibly reordered)
translation hypotheses, similarly to the approach
of (Bangalore et al., 2001). Using global system
probabilities and other statistical models, the vot-
ing procedure selects the best consensus hypoth-
esis from the confusion network. This consen-
sus translation may be different from the original
translations.
This paper is organized as follows. In Section 2,
we will describe the computation of consensus
translations with our approach. In particular, we
will present details of the enhanced alignment and
reordering procedure. A large set of experimental
results on several machine translation tasks is pre-
sented in Section 3, which is followed by a sum-
mary.
</bodyText>
<sectionHeader confidence="0.499737" genericHeader="method">
2 Description of the Algorithm
</sectionHeader>
<bodyText confidence="0.998299315789474">
The proposed approach takes advantage of mul-
tiple translations for a whole test corpus to com-
pute a consensus translation for each sentence in
this corpus. Given a single source sentence in the
test corpus, we combine M translation hypothe-
ses Ei, ... , EM from M MT engines. We first
choose one of the hypotheses Em as the primary
one. We consider this primary hypothesis to have
the “correct” word order. We then align and re-
order the other, secondary hypotheses En(n =
1, ..., M; n =� m) to match this word order. Since
each hypothesis may have an acceptable word or-
der, we let every hypothesis play the role of the
primary translation once, and thus align all pairs
of hypotheses (En, Em); n =� m.
In the following subsections, we will explain
the word alignment procedure, the reordering ap-
proach, and the construction of confusion net-
works.
</bodyText>
<subsectionHeader confidence="0.991945">
2.1 Statistical Alignment
</subsectionHeader>
<bodyText confidence="0.999800625">
The word alignment is performed in analogy to the
training procedure in SMT. The difference is that
the two sentences that have to be aligned are in the
same language. We consider the conditional prob-
ability Pr(En|Em) of the event that, given Em,
another hypothesis En is generated from the Em.
Then, the alignment between the two hypotheses
is introduced as a hidden variable:
</bodyText>
<equation confidence="0.935675">
Pr(En|Em) = � Pr(En, A|Em)
A
</equation>
<bodyText confidence="0.754139666666667">
This probability is then decomposed into the align-
ment probability Pr(A|Em) and the lexicon prob-
ability Pr(En|A, Em):
</bodyText>
<equation confidence="0.638253">
Pr(En, A|Em) = Pr(A|Em) · Pr(En|A, Em)
</equation>
<bodyText confidence="0.999956710526316">
As in statistical machine translation, we make
modelling assumptions. We use the IBM Model 1
(Brown et al., 1993) (uniform distribution) and the
Hidden Markov Model (HMM, first-order depen-
dency, (Vogel et al., 1996)) to estimate the align-
ment model. The lexicon probability of a sentence
pair is modelled as a product of single-word based
probabilities of the aligned words.
The training corpus for alignment is created
from a test corpus of N sentences (usually a few
hundred) translated by all of the involved MT en-
gines. However, the effective size of the training
corpus is larger than N, since all pairs of different
hypotheses have to be aligned. Thus, the effective
size of the training corpus is M · (M −1) · N. The
single-word based lexicon probabilities p(en|em)
are initialized with normalized lexicon counts col-
lected over the sentence pairs (En, Em) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring equal words, i. e.
if en is the same word as em. In addition, we add
a fraction of a count for words with identical pre-
fixes. The initialization could be furthermore im-
proved by using word classes, part-of-speech tags,
or a list of synonyms.
The model parameters are trained iteratively in
an unsupervised manner with the EM algorithm
using the GIZA++ toolkit (Och and Ney, 2003).
The training is performed in the directions En →
Em and Em → En. The updated lexicon tables
from the two directions are interpolated after each
iteration.
The final alignments are determined using cost
matrices defined by the state occupation probabil-
ities of the trained HMM (Matusov et al., 2004).
The alignments are used for reordering each sec-
ondary translation En and for computing the con-
fusion network.
</bodyText>
<page confidence="0.997577">
34
</page>
<figureCaption confidence="0.977809">
Figure 1: Example of creating a confusion network from monotone one-to-one word alignments (denoted
</figureCaption>
<bodyText confidence="0.5960155">
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an e-arc in the corresponding part of the confusion network.
</bodyText>
<tableCaption confidence="0.62322325">
1. would you like coffee or tea
original 2. would you have tea or coffee
hypotheses 3. would you like your coffee or
4. I have some coffee tea would you like
</tableCaption>
<bodyText confidence="0.965264142857143">
alignment would|would you|you have|like coffee|coffee or|or tea|tea
and would|would you|you like|like your|$ coffee|coffee or|or $|tea
reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would you like $ $ coffee or tea
confusion $ would you have $ $ coffee or tea
network $ would you like your $ coffee or $
I would you like have some coffee $ tea
</bodyText>
<subsectionHeader confidence="0.998299">
2.2 Word Reordering
</subsectionHeader>
<bodyText confidence="0.999990363636364">
The alignment between En and the primary hy-
pothesis E,,t used for reordering is computed as a
function of words in the secondary translation En
with minimal costs, with an additional constraint
that identical words in En can not be all aligned to
the same word in E,,t. This constraint is necessary
to avoid that reordered hypotheses with e. g. multi-
ple consecutive articles “the” would be produced if
fewer articles were used in the primary hypothesis.
The new word order for En is obtained through
sorting the words in En by the indices of the words
in E,,t to which they are aligned. Two words in
En which are aligned to the same word in E,,t are
kept in the original order. After reordering each
secondary hypothesis En, we determine M − 1
monotone one-to-one alignments between E,,t and
En, n = 1, ... , M; n =6 m. In case of many-to-
one connections of words in En to a single word in
E,,t, we only keep the connection with the lowest
alignment costs. The one-to-one alignments are
convenient for constructing a confusion network
in the next step of the algorithm.
</bodyText>
<subsectionHeader confidence="0.999135">
2.3 Building Confusion Networks
</subsectionHeader>
<bodyText confidence="0.999977225">
Given the M−1 monotone one-to-one alignments,
the transformation to a confusion network as de-
scribed by (Bangalore et al., 2001) is straightfor-
ward. It is explained by the example in Figure 1.
Here, the original 4 hypotheses are shown, fol-
lowed by the alignment of the reordered secondary
hypotheses 2-4 with the primary hypothesis 1. The
alignment is shown with the  |symbol, and the
words of the primary hypothesis are to the right
of this symbol. The symbol $ denotes a null align-
ment or an e-arc in the corresponding part of the
confusion network, which is shown at the bottom
of the figure.
Note that the word “have” in translation 2 is
aligned to the word “like” in translation 1. This
alignment is acceptable considering the two trans-
lations alone. However, given the presence of the
word “have” in translation 4, this is not the best
alignment. Yet the problems of this type can in
part be solved by the proposed approach, since ev-
ery translation once plays the role of the primary
translation. For each sentence, we obtain a total of
M confusion networks and unite them in a single
lattice. The consensus translation can be chosen
among different alignment and reordering paths in
this lattice.
The “voting” on the union of confusion net-
works is straightforward and analogous to the
ROVER system. We sum up the probabilities of
the arcs which are labeled with the same word
and have the same start and the same end state.
These probabilities are the global probabilities as-
signed to the different MT systems. They are man-
ually adjusted based on the performance of the in-
volved MT systems on a held-out development set.
In general, a better consensus translation can be
produced if the words hypothesized by a better-
performing system get a higher probability. Ad-
ditional scores like word confidence measures can
be used to score the arcs in the lattice.
</bodyText>
<subsectionHeader confidence="0.980697">
2.4 Extracting Consensus Translation
</subsectionHeader>
<bodyText confidence="0.9998995">
In the final step, the consensus translation is ex-
tracted as the best path from the union of confu-
</bodyText>
<page confidence="0.999117">
35
</page>
<tableCaption confidence="0.999812">
Table 1: Corpus statistics of the test corpora.
</tableCaption>
<table confidence="0.9998938">
BTEC IWSLT04 BTEC CSTAR03 EPPS TC-STAR
Chinese Japanese English Italian English Spanish English
Sentences 500 506 1073
Running Words 3 681 4131 3 092 3176 2 942 2 889 18 896 18 289
Distinct Words 893 979 1 125 1 134 1028 942 3 302 3 742
</table>
<bodyText confidence="0.994972">
sion networks. Note that the extracted consensus
translation can be different from the original M
translations. Alternatively, the N-best hypothe-
ses can be extracted for rescoring by additional
models. We performed experiments with both ap-
proaches.
Since M confusion networks are used, the lat-
tice may contain two best paths with the same
probability, the same words, but different word
order. We extended the algorithm to favor more
well-formed word sequences. We assign a higher
probability to each arc of the primary (unre-
ordered) translation in each of the M confusion
networks. Experimentally, this extension im-
proved translation fluency on some tasks.
</bodyText>
<sectionHeader confidence="0.996805" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.998667">
3.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999265416666667">
The alignment and voting algorithm was evaluated
on both small and large vocabulary tasks. Initial
experiments were performed on the IWSLT 2004
Chinese-English and Japanese-English tasks (Ak-
iba et al., 2004). The data for these tasks come
from the Basic Travel Expression corpus (BTEC),
consisting of tourism-related sentences. We com-
bined the outputs of several MT systems that had
officially been submitted to the IWSLT 2004 eval-
uation. Each system had used 20K sentence pairs
(180K running words) from the BTEC corpus for
training.
Experiments with translations of automatically
recognized speech were performed on the BTEC
Italian-English task (Federico, 2003). Here, the
involved MT systems had used about 60K sen-
tence pairs (420K running words) for training.
Finally, we also computed consensus translation
from some of the submissions to the TC-STAR
2005 evaluation campaign (TC-STAR, 2005). The
TC-STAR participants had submitted translations
of manually transcribed speeches from the Euro-
pean Parliament Plenary Sessions (EPPS). In our
experiments, we used the translations from Span-
</bodyText>
<tableCaption confidence="0.986779">
Table 2: Improved translation results for the con-
sensus translation computed from 5 translation
outputs on the Chinese-English IWSLT04 task.
</tableCaption>
<table confidence="0.999540142857143">
BTEC WER PER BLEU
Chinese-English [%] [%] [%]
worst single system ’04 58.3 46.6 34.6
best single system∗ ’04 54.6 42.6 40.3
consensus of 5 systems 47.8 38.0 46.2
from 2004
system (*) in 2005 50.3 40.5 45.1
</table>
<bodyText confidence="0.999056736842105">
ish to English. The MT engines for this task had
been trained on 1.2M sentence pairs (32M running
words).
Table 1 gives an overview of the test corpora,
on which the enhanced hypotheses alignment was
computed, and for which the consensus transla-
tions were determined. The official IWSLT04
test corpus was used for the IWSLT 04 tasks; the
CSTAR03 test corpus was used for the speech
translation task. The March 2005 test corpus of
the TC-STAR evaluation (verbatim condition) was
used for the EPPS task. In Table 1, the number of
running words in English is the average number of
running words in the hypotheses, from which the
consensus translation was computed; the vocabu-
lary of English is the merged vocabulary of these
hypotheses. For the BTEC IWSLT04 corpus, the
statistics for English is given for the experiments
described in Sections 3.3 and 3.5, respectively.
</bodyText>
<subsectionHeader confidence="0.997744">
3.2 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.999916777777778">
Well-established objective evaluation measures
like the word error rate (WER), position-
independent word error rate (PER), and the BLEU
score (Papineni et al., 2002) were used to assess
the translation quality. All measures were com-
puted with respect to multiple reference transla-
tions. The evaluation (as well as the alignment
training) was case-insensitive, without consider-
ing the punctuation marks.
</bodyText>
<page confidence="0.995372">
36
</page>
<subsectionHeader confidence="0.981954">
3.3 Chinese-English Translation
</subsectionHeader>
<bodyText confidence="0.999975659090909">
Different applications of the proposed combina-
tion method have been evaluated. First, we fo-
cused on combining different MT systems which
have the same source and target language. The
initial experiments were performed on the BTEC
Chinese-English task. We combined translations
produced by 5 different MT systems. Table 2
shows the performance of the best and the worst of
these systems in terms of the BLEU score. The re-
sults for the consensus translation show a dramatic
improvement in translation quality. The word er-
ror rate is reduced e. g. from 54.6 to 47.8%. The
research group which had submitted the best trans-
lation in 2004 translated the same test set a year
later with an improved system. We compared
the consensus translation with this new translation
(last line of Table 2). It can be observed that the
consensus translation based on the MT systems
developed in 2004 is still superior to this 2005 sin-
gle system translation in terms of all error mea-
sures.
We also checked how many sentences in the
consensus translation of the test corpus are differ-
ent from the 5 original translations. 185 out of 500
sentences (37%) had new translations. Computing
the error measures on these sentences only, we ob-
served significant improvements in WER and PER
and a small improvement in BLEU with respect
to the original translations. Thus, the quality of
previously unseen consensus translations as gen-
erated from the original translations is acceptable.
In this experiment, the global system proba-
bilities for scoring the confusion networks were
tuned manually on a development set. The distri-
bution was 0.35, 0.25, 0.2, 0.1, 0.1, with 0.35 for
the words of the best single system and 0.1 for the
words of the worst single system. We observed
that the consensus translation did not change sig-
nificantly with small perturbations of these val-
ues. However, the relation between the proba-
bilities is very important for good performance.
No improvement can be achieved with a uniform
probability distribution – it is necessary to penal-
ize translations of low quality.
</bodyText>
<subsectionHeader confidence="0.92221">
3.4 Spanish-English Translation
</subsectionHeader>
<bodyText confidence="0.99442125">
The improvements in translation quality are
also significant on the TC-STAR EPPS Spanish-
English task. Here, we combined four different
systems which performed best in the TC-STAR
</bodyText>
<tableCaption confidence="0.969833666666667">
Table 3: Improved translation results for the con-
sensus translation computed from 4 translation
outputs on the Spanish-English TC-STAR task.
</tableCaption>
<table confidence="0.999599166666667">
EPPS WER PER BLEU
Spanish-English [%] [%] [%]
worst single system 49.1 38.2 39.6
best single system 41.0 30.2 47.7
consensus of 4 systems 39.1 29.1 49.3
+ rescoring 38.8 29.0 50.7
</table>
<bodyText confidence="0.997559466666667">
2005 evaluation, see Table 3. Compared to the
best performing single system, the consensus hy-
pothesis reduces the WER from 41.0 to 39.1%.
This result is further improved by rescoring the
N-best lists derived from the confusion networks
(N=1000). For rescoring, a word penalty fea-
ture, the IBM Model 1, and a 4-gram target lan-
guage model were included. The linear interpola-
tion weights of these models and the score from
the confusion network were optimized on a sep-
arate development set with respect to word error
rate.
Table 4 gives examples of improved translation
quality by using the consensus translation as de-
rived from the rescored N-best lists.
</bodyText>
<subsectionHeader confidence="0.939465">
3.5 Multi-source Translation
</subsectionHeader>
<bodyText confidence="0.999953086956522">
In the IWSLT 2004 evaluation, the English ref-
erence translations for the Chinese-English and
Japanese-English test corpora were the same, ex-
cept for a permutation of the sentences. Thus, we
could combine MT systems which have different
source and the same target language, performing
multi-source machine translation (described e. g.
by (Och and Ney, 2001)). We combined two
Japanese-English and two Chinese-English sys-
tems. The best performing system was a Japanese-
English system with a BLEU score of 44.7%, see
Table 5. By computing the consensus translation,
we improved this score to 49.6%, and also signifi-
cantly reduced the error rates.
To investigate the potential of the proposed ap-
proach, we generated the N-best lists (N = 1000)
of consensus translations. Then, for each sentence,
we selected the hypothesis in the N-best list with
the lowest word error rate with respect to the mul-
tiple reference translations for the sentence. We
then evaluated the quality of these “oracle” trans-
lations with all error measures. In a contrastive
experiment, for each sentence we simply selected
</bodyText>
<page confidence="0.999708">
37
</page>
<tableCaption confidence="0.811717">
Table 4: Examples of improved translation quality with the consensus translations on the Spanish-English
TC-STAR EPPS task (case-insensitive output).
</tableCaption>
<table confidence="0.975906666666667">
best system I also authorised to committees to certain reports
consensus I also authorised to certain committees to draw up reports
reference I have also authorised certain committees to prepare reports
best system human rights which therefore has fought the european union
consensus human rights which the european union has fought
reference human rights for which the european union has fought so hard
best system we of the following the agenda
consensus moving on to the next point on the agenda
reference we go on to the next point of the agenda
</table>
<tableCaption confidence="0.96008">
Table 5: Multi-source translation: improvements
in translation quality when computing consen-
sus translation using the output of two Chinese-
English and two Japanese-English systems on the
IWSLT04 task.
</tableCaption>
<table confidence="0.9998388">
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
worst single system 58.0 41.8 39.5
best single system 51.3 38.6 44.7
consensus of 4 systems 44.9 33.9 49.6
</table>
<tableCaption confidence="0.9929355">
Table 6: Consensus-based combination vs. se-
lection: potential for improvement (multi-source
translation, selection/combination of 4 translation
outputs).
</tableCaption>
<table confidence="0.999445333333333">
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
best single system 51.3 38.6 44.7
oracle selection 33.3 29.3 59.2
oracle consensus
(1000-best list) 27.0 22.8 64.2
</table>
<bodyText confidence="0.999282">
the translation with the lowest WER from the orig-
inal 4 MT system outputs. Table 6 shows that the
potential for improvement is significantly larger
for the consensus-based combination of transla-
tion outputs than for simple selection of the best
translation1. In our future work, we plan to im-
prove the scoring of hypotheses in the confusion
networks to explore this large potential.
</bodyText>
<subsectionHeader confidence="0.973755">
3.6 Speech Translation
</subsectionHeader>
<bodyText confidence="0.995309">
Some state-of-the-art speech translation systems
can translate either the first best recognition hy-
</bodyText>
<subsectionHeader confidence="0.428598">
1Similar “oracle” results were observed on other tasks.
</subsectionHeader>
<bodyText confidence="0.999944193548387">
potheses or the word lattices of an ASR system. It
has been previously shown that word lattice input
generally improves translation quality. In practice,
however, the translation system may choose, for
some sentences, the paths in the lattice with many
recognition errors and thus produce inferior trans-
lations. These translations can be improved if we
compute a consensus translation from the output
of at least two different speech translation systems.
From each system, we take the translation of the
single best ASR output, and the translation of the
ASR word lattice.
Two different statistical MT systems capable of
translating ASR word lattices have been compared
by (Matusov and Ney, 2005). Both systems pro-
duced translations of better quality on the BTEC
Italian-English speech translation task when using
lattices instead of single best ASR output. We
obtained the output of each of the two systems
under each of these translation scenarios on the
CSTAR03 test corpus. The first-best recognition
word error rate on this corpus is 22.3%. The objec-
tive error measures for the 4 translation hypothe-
ses are given in Table 7. We then computed a con-
sensus translation of the 4 outputs with the pro-
posed method. The better performing word lattice
translations were given higher system probabili-
ties. With the consensus hypothesis, the word er-
ror rate went down from 29.5 to 28.5%. Thus, the
negative effect of recognition errors on the trans-
lation quality was further reduced.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="method">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99310475">
In this work, we proposed a novel, theoretically
well-founded procedure for computing a possi-
bly new consensus translation from the outputs of
multiple MT systems. In summary, the main con-
</bodyText>
<page confidence="0.99898">
38
</page>
<tableCaption confidence="0.930556">
Table 7: Improvements in translation quality on
</tableCaption>
<bodyText confidence="0.7674335">
the BTEC Italian-English task through comput-
ing consensus translations from the output of two
speech translation systems with different types of
source language input.
</bodyText>
<table confidence="0.998134125">
system input WER PER BLEU
[%] [%] [%]
2 correct text 23.3 19.3 65.6
1 a) single best 32.8 28.6 53.9
b) lattice 30.7 26.7 55.9
2 c) single best 31.6 27.5 54.7
d) lattice 29.5 26.1 58.2
consensus a-d 28.5 25.0 58.9
</table>
<bodyText confidence="0.810989416666667">
tributions of this work compared to previous ap-
proaches are as follows:
• The words of the original translation hy-
potheses are aligned in order to create a con-
fusion network. The alignment procedure ex-
plicitly models word reordering.
• A test corpus of translations generated by
each of the systems is used for the unsuper-
vised statistical alignment training. Thus, the
decision on how to align two translations of
a sentence takes the whole document context
into account.
</bodyText>
<listItem confidence="0.934793777777778">
• Large and significant gains in translation
quality were obtained on various translation
tasks and conditions.
• A significant improvement of translation
quality was achieved in a multi-source trans-
lation scenario. Here, we combined the
output of MT systems which have different
source and the same target language.
• The proposed method can be effectively ap-
</listItem>
<bodyText confidence="0.9814381">
plied in speech translation in order to cope
with the negative impact of speech recogni-
tion errors on translation accuracy.
An important feature of a real-life application of
the proposed alignment technique is that the lex-
icon and alignment probabilities can be updated
with each translated sentence and/or text. Thus,
the correspondence between words in different hy-
potheses and, consequently, the consensus transla-
tion can be improved overtime.
</bodyText>
<sectionHeader confidence="0.998082" genericHeader="conclusions">
5 Acknowledgement
</sectionHeader>
<bodyText confidence="0.928605142857143">
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. This work was also in part funded by the
European Union under the integrated project TC-
STAR – Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
</bodyText>
<sectionHeader confidence="0.993718" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936214285715">
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa,
M. Paul, and J. Tsujii. 2004. Overview of the
IWSLT04 Evaluation Campaign. Int. Workshop
on Spoken Language Translation, pp. 1–12, Kyoto,
Japan.
S. Bangalore, G. Bordel, G. Riccardi. 2001. Comput-
ing Consensus Translation from Multiple Machine
Translation Systems. IEEE Workshop on Automatic
Speech Recognition and Understanding, Madonna
di Campiglio, Italy.
P. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statisti-
cal Machine Translation. Computational Linguis-
tics, vol. 19(2):263–311.
J. G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). IEEE Workshop on
Automatic Speech Recognition and Understanding.
S. Jayaraman and A. Lavie. 2005. Multi-Engline Ma-
chine Translation Guided by Explicit Word Match-
ing. 10th Conference of the European Association
for Machine Translation, pp. 143-152, Budapest,
Hungary.
M. Federico 2003. Evaluation Frameworks for Speech
Translation Technologies. Proc. of Eurospeech,
pp. 377-380, Geneva, Switzerland.
R. Frederking and S. Nirenburg. 1994. Three Heads
are Better Than One. Fourth Conference on Applied
Natural Language Processing, Stuttgart, Germany.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. 20th Int. Conf. on Computational Linguistics,
pp. 219–225, Geneva, Switzerland.
E. Matusov and H. Ney. 2005. Phrase-based Trans-
lation of Speech Recognizer Word Lattices Using
Loglinear Model Combination. IEEE Workshop on
Automatic Speech Recognition and Understanding,
pp. 110-115, San Juan, Puerto-Rico.
T. Nomoto. 2004. Multi-Engine Machine Transla-
tion with Voted Language Model. 42nd Confer-
ence of the Association for Computational Linguis-
tics (ACL), pp. 494-501, Barcelona, Spain.
</reference>
<page confidence="0.99049">
39
</page>
<reference confidence="0.974308954545454">
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. MT Summit VIII, pp. 253-258, Santi-
ago de Compostela, Spain.
F. J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19–51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Annual Meeting of the ACL, pp.
311–318, Philadelphia, PA, USA.
M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma,
and E. Sumita. 2005. Nobody is Perfect: ATR’s
Hybrid Approach to Spoken Language Translation.
International Workshop on Spoken Language Trans-
lation, pp. 55-62, Pittsburgh, PA, USA.
TC-STAR Spoken Language Translation Progress Re-
port. 2005. http://www.tc-star.org/documents/
deliverable/Deliv D5 Total 21May05.pdf
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
Word Alignment in Statistical Translation. 16th Int.
Conf. on Computational Linguistics, pp. 836–841,
Copenhagen, Denmark.
</reference>
<page confidence="0.998639">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.547479">
<title confidence="0.996775">Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment</title>
<author confidence="0.996755">Evgeny Matusov</author>
<author confidence="0.996755">Nicola Ueffing</author>
<author confidence="0.996755">Hermann Ney</author>
<affiliation confidence="0.94784">Lehrstuhl f¨ur Informatik VI - Computer Science Department</affiliation>
<address confidence="0.61091">RWTH Aachen University, Aachen, Germany.</address>
<abstract confidence="0.996617655172414">This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Akiba</author>
<author>M Federico</author>
<author>N Kando</author>
<author>H Nakaiwa</author>
<author>M Paul</author>
<author>J Tsujii</author>
</authors>
<date>2004</date>
<booktitle>Overview of the IWSLT04 Evaluation Campaign. Int. Workshop on Spoken Language Translation,</booktitle>
<pages>1--12</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="12812" citStr="Akiba et al., 2004" startWordPosition="2103" endWordPosition="2107">attice may contain two best paths with the same probability, the same words, but different word order. We extended the algorithm to favor more well-formed word sequences. We assign a higher probability to each arc of the primary (unreordered) translation in each of the M confusion networks. Experimentally, this extension improved translation fluency on some tasks. 3 Experimental Results 3.1 Corpus Statistics The alignment and voting algorithm was evaluated on both small and large vocabulary tasks. Initial experiments were performed on the IWSLT 2004 Chinese-English and Japanese-English tasks (Akiba et al., 2004). The data for these tasks come from the Basic Travel Expression corpus (BTEC), consisting of tourism-related sentences. We combined the outputs of several MT systems that had officially been submitted to the IWSLT 2004 evaluation. Each system had used 20K sentence pairs (180K running words) from the BTEC corpus for training. Experiments with translations of automatically recognized speech were performed on the BTEC Italian-English task (Federico, 2003). Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training. Finally, we also computed consensus transl</context>
</contexts>
<marker>Akiba, Federico, Kando, Nakaiwa, Paul, Tsujii, 2004</marker>
<rawString>Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul, and J. Tsujii. 2004. Overview of the IWSLT04 Evaluation Campaign. Int. Workshop on Spoken Language Translation, pp. 1–12, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Bordel</author>
<author>G Riccardi</author>
</authors>
<title>Computing Consensus Translation from Multiple Machine Translation Systems.</title>
<date>2001</date>
<booktitle>IEEE Workshop on Automatic Speech Recognition and Understanding, Madonna di Campiglio,</booktitle>
<location>Italy.</location>
<contexts>
<context position="2693" citStr="Bangalore et al., 2001" startWordPosition="399" endWordPosition="402">ased on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005). Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). To be successful, such approaches require compatible lattices and comparable scores of the (word) hypotheses in the lattices. However, the scores of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable. For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available. (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses. This algorithm produces monotone alignments only (i. e. allows insertion, deletion, and substitution of words); it is not able to align translation hypotheses with significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem. They introduce a method that allows non-monotone alignments of words in different translation hypotheses for the same sentence. However, this approach uses many heuristics and is based on the alignment that </context>
<context position="9746" citStr="Bangalore et al., 2001" startWordPosition="1584" endWordPosition="1587">ch are aligned to the same word in E,,t are kept in the original order. After reordering each secondary hypothesis En, we determine M − 1 monotone one-to-one alignments between E,,t and En, n = 1, ... , M; n =6 m. In case of many-toone connections of words in En to a single word in E,,t, we only keep the connection with the lowest alignment costs. The one-to-one alignments are convenient for constructing a confusion network in the next step of the algorithm. 2.3 Building Confusion Networks Given the M−1 monotone one-to-one alignments, the transformation to a confusion network as described by (Bangalore et al., 2001) is straightforward. It is explained by the example in Figure 1. Here, the original 4 hypotheses are shown, followed by the alignment of the reordered secondary hypotheses 2-4 with the primary hypothesis 1. The alignment is shown with the |symbol, and the words of the primary hypothesis are to the right of this symbol. The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network, which is shown at the bottom of the figure. Note that the word “have” in translation 2 is aligned to the word “like” in translation 1. This alignment is acceptable considering t</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>S. Bangalore, G. Bordel, G. Riccardi. 2001. Computing Consensus Translation from Multiple Machine Translation Systems. IEEE Workshop on Automatic Speech Recognition and Understanding, Madonna di Campiglio, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation. Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6043" citStr="Brown et al., 1993" startWordPosition="947" endWordPosition="950"> procedure in SMT. The difference is that the two sentences that have to be aligned are in the same language. We consider the conditional probability Pr(En|Em) of the event that, given Em, another hypothesis En is generated from the Em. Then, the alignment between the two hypotheses is introduced as a hidden variable: Pr(En|Em) = � Pr(En, A|Em) A This probability is then decomposed into the alignment probability Pr(A|Em) and the lexicon probability Pr(En|A, Em): Pr(En, A|Em) = Pr(A|Em) · Pr(En|A, Em) As in statistical machine translation, we make modelling assumptions. We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model. The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words. The training corpus for alignment is created from a test corpus of N sentences (usually a few hundred) translated by all of the involved MT engines. However, the effective size of the training corpus is larger than N, since all pairs of different hypotheses have to be aligned. Thus, the effective size of the training corpus is M · (M −1</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation. Computational Linguistics, vol. 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
</authors>
<title>A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER).</title>
<date>1997</date>
<booktitle>IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="1735" citStr="Fiscus, 1997" startWordPosition="251" endWordPosition="252">e translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative. 1 Introduction In this work we describe a novel technique for computing a consensus translation from the outputs of multiple machine translation systems. Combining outputs from different systems was shown to be quite successful in automatic speech recognition (ASR). Voting schemes like the ROVER approach of (Fiscus, 1997) use edit distance alignment and time information to create confusion networks from the output of several ASR systems. Some research on multi-engine machine translation has also been performed in recent years. The most straightforward approaches simply select, for each sentence, one of the provided hypotheses. The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005). Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). To be successful, such approaches require compatib</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J. G. Fiscus. 1997. A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER). IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<date>2005</date>
<booktitle>Multi-Engline Machine Translation Guided by Explicit Word Matching. 10th Conference of the European Association for Machine Translation,</booktitle>
<pages>143--152</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="3055" citStr="Jayaraman and Lavie, 2005" startWordPosition="454" endWordPosition="457">s of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable. For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available. (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses. This algorithm produces monotone alignments only (i. e. allows insertion, deletion, and substitution of words); it is not able to align translation hypotheses with significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem. They introduce a method that allows non-monotone alignments of words in different translation hypotheses for the same sentence. However, this approach uses many heuristics and is based on the alignment that is performed to calculate a specific MT error measure; the performance improvements are reported only in terms of this measure. 33 Here, we propose an alignment procedure that explicitly models reordering of words in the hypotheses. In contrast to existing approaches, the context of the whole document rather than a single sentence is considered in this iterati</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-Engline Machine Translation Guided by Explicit Word Matching. 10th Conference of the European Association for Machine Translation, pp. 143-152, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
</authors>
<title>Evaluation Frameworks for Speech Translation Technologies.</title>
<date>2003</date>
<booktitle>Proc. of Eurospeech,</booktitle>
<pages>377--380</pages>
<location>Geneva,</location>
<contexts>
<context position="13269" citStr="Federico, 2003" startWordPosition="2175" endWordPosition="2176">on both small and large vocabulary tasks. Initial experiments were performed on the IWSLT 2004 Chinese-English and Japanese-English tasks (Akiba et al., 2004). The data for these tasks come from the Basic Travel Expression corpus (BTEC), consisting of tourism-related sentences. We combined the outputs of several MT systems that had officially been submitted to the IWSLT 2004 evaluation. Each system had used 20K sentence pairs (180K running words) from the BTEC corpus for training. Experiments with translations of automatically recognized speech were performed on the BTEC Italian-English task (Federico, 2003). Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training. Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005). The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS). In our experiments, we used the translations from SpanTable 2: Improved translation results for the consensus translation computed from 5 translation outputs on the Chinese-English IWSLT04 task. BTEC WER PER BLEU Chinese-Engli</context>
</contexts>
<marker>Federico, 2003</marker>
<rawString>M. Federico 2003. Evaluation Frameworks for Speech Translation Technologies. Proc. of Eurospeech, pp. 377-380, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>S Nirenburg</author>
</authors>
<title>Three Heads are Better Than One.</title>
<date>1994</date>
<booktitle>Fourth Conference on Applied Natural Language Processing,</booktitle>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="2283" citStr="Frederking and Nirenburg, 1994" startWordPosition="335" endWordPosition="338"> speech recognition (ASR). Voting schemes like the ROVER approach of (Fiscus, 1997) use edit distance alignment and time information to create confusion networks from the output of several ASR systems. Some research on multi-engine machine translation has also been performed in recent years. The most straightforward approaches simply select, for each sentence, one of the provided hypotheses. The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005). Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). To be successful, such approaches require compatible lattices and comparable scores of the (word) hypotheses in the lattices. However, the scores of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable. For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available. (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses. This algorithm produces monotone alignments only (i. e.</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>R. Frederking and S. Nirenburg. 1994. Three Heads are Better Than One. Fourth Conference on Applied Natural Language Processing, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric Word Alignments for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>20th Int. Conf. on Computational Linguistics,</booktitle>
<pages>219--225</pages>
<location>Geneva,</location>
<contexts>
<context position="7564" citStr="Matusov et al., 2004" startWordPosition="1205" endWordPosition="1208">on, we add a fraction of a count for words with identical prefixes. The initialization could be furthermore improved by using word classes, part-of-speech tags, or a list of synonyms. The model parameters are trained iteratively in an unsupervised manner with the EM algorithm using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions En → Em and Em → En. The updated lexicon tables from the two directions are interpolated after each iteration. The final alignments are determined using cost matrices defined by the state occupation probabilities of the trained HMM (Matusov et al., 2004). The alignments are used for reordering each secondary translation En and for computing the confusion network. 34 Figure 1: Example of creating a confusion network from monotone one-to-one word alignments (denoted with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. 1. would you like coffee or tea original 2. would you have tea or coffee hypotheses 3. would you like your coffee or 4. I have some coffee tea would you like alignment would|would you|you have|like coffee|coffe</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word Alignments for Statistical Machine Translation. 20th Int. Conf. on Computational Linguistics, pp. 219–225, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>Phrase-based Translation of Speech Recognizer Word Lattices Using Loglinear Model Combination.</title>
<date>2005</date>
<booktitle>IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>110--115</pages>
<location>San Juan, Puerto-Rico.</location>
<contexts>
<context position="22409" citStr="Matusov and Ney, 2005" startWordPosition="3627" endWordPosition="3630">wn that word lattice input generally improves translation quality. In practice, however, the translation system may choose, for some sentences, the paths in the lattice with many recognition errors and thus produce inferior translations. These translations can be improved if we compute a consensus translation from the output of at least two different speech translation systems. From each system, we take the translation of the single best ASR output, and the translation of the ASR word lattice. Two different statistical MT systems capable of translating ASR word lattices have been compared by (Matusov and Ney, 2005). Both systems produced translations of better quality on the BTEC Italian-English speech translation task when using lattices instead of single best ASR output. We obtained the output of each of the two systems under each of these translation scenarios on the CSTAR03 test corpus. The first-best recognition word error rate on this corpus is 22.3%. The objective error measures for the 4 translation hypotheses are given in Table 7. We then computed a consensus translation of the 4 outputs with the proposed method. The better performing word lattice translations were given higher system probabili</context>
</contexts>
<marker>Matusov, Ney, 2005</marker>
<rawString>E. Matusov and H. Ney. 2005. Phrase-based Translation of Speech Recognizer Word Lattices Using Loglinear Model Combination. IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 110-115, San Juan, Puerto-Rico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>Multi-Engine Machine Translation with Voted Language Model.</title>
<date>2004</date>
<booktitle>42nd Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>494--501</pages>
<location>Barcelona,</location>
<contexts>
<context position="2145" citStr="Nomoto, 2004" startWordPosition="317" endWordPosition="318">ple machine translation systems. Combining outputs from different systems was shown to be quite successful in automatic speech recognition (ASR). Voting schemes like the ROVER approach of (Fiscus, 1997) use edit distance alignment and time information to create confusion networks from the output of several ASR systems. Some research on multi-engine machine translation has also been performed in recent years. The most straightforward approaches simply select, for each sentence, one of the provided hypotheses. The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005). Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). To be successful, such approaches require compatible lattices and comparable scores of the (word) hypotheses in the lattices. However, the scores of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable. For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available. (Bangalore et al., 2001) used the edit distance alignment extended to multip</context>
</contexts>
<marker>Nomoto, 2004</marker>
<rawString>T. Nomoto. 2004. Multi-Engine Machine Translation with Voted Language Model. 42nd Conference of the Association for Computational Linguistics (ACL), pp. 494-501, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Statistical Multi-Source Translation.</title>
<date>2001</date>
<booktitle>MT Summit VIII,</booktitle>
<pages>253--258</pages>
<location>Santiago de Compostela,</location>
<contexts>
<context position="19007" citStr="Och and Ney, 2001" startWordPosition="3097" endWordPosition="3100">the confusion network were optimized on a separate development set with respect to word error rate. Table 4 gives examples of improved translation quality by using the consensus translation as derived from the rescored N-best lists. 3.5 Multi-source Translation In the IWSLT 2004 evaluation, the English reference translations for the Chinese-English and Japanese-English test corpora were the same, except for a permutation of the sentences. Thus, we could combine MT systems which have different source and the same target language, performing multi-source machine translation (described e. g. by (Och and Ney, 2001)). We combined two Japanese-English and two Chinese-English systems. The best performing system was a JapaneseEnglish system with a BLEU score of 44.7%, see Table 5. By computing the consensus translation, we improved this score to 49.6%, and also significantly reduced the error rates. To investigate the potential of the proposed approach, we generated the N-best lists (N = 1000) of consensus translations. Then, for each sentence, we selected the hypothesis in the N-best list with the lowest word error rate with respect to the multiple reference translations for the sentence. We then evaluated</context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>F. J. Och and H. Ney. 2001. Statistical Multi-Source Translation. MT Summit VIII, pp. 253-258, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7264" citStr="Och and Ney, 2003" startWordPosition="1155" endWordPosition="1158"> N. The single-word based lexicon probabilities p(en|em) are initialized with normalized lexicon counts collected over the sentence pairs (En, Em) on this corpus. Since all of the hypotheses are in the same language, we count co-occurring equal words, i. e. if en is the same word as em. In addition, we add a fraction of a count for words with identical prefixes. The initialization could be furthermore improved by using word classes, part-of-speech tags, or a list of synonyms. The model parameters are trained iteratively in an unsupervised manner with the EM algorithm using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions En → Em and Em → En. The updated lexicon tables from the two directions are interpolated after each iteration. The final alignments are determined using cost matrices defined by the state occupation probabilities of the trained HMM (Matusov et al., 2004). The alignments are used for reordering each secondary translation En and for computing the confusion network. 34 Figure 1: Example of creating a confusion network from monotone one-to-one word alignments (denoted with symbol |). The words of the primary hypothesis are printed in bold. The symbol $</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="15100" citStr="Papineni et al., 2002" startWordPosition="2466" endWordPosition="2469">TC-STAR evaluation (verbatim condition) was used for the EPPS task. In Table 1, the number of running words in English is the average number of running words in the hypotheses, from which the consensus translation was computed; the vocabulary of English is the merged vocabulary of these hypotheses. For the BTEC IWSLT04 corpus, the statistics for English is given for the experiments described in Sections 3.3 and 3.5, respectively. 3.2 Evaluation Criteria Well-established objective evaluation measures like the word error rate (WER), positionindependent word error rate (PER), and the BLEU score (Papineni et al., 2002) were used to assess the translation quality. All measures were computed with respect to multiple reference translations. The evaluation (as well as the alignment training) was case-insensitive, without considering the punctuation marks. 36 3.3 Chinese-English Translation Different applications of the proposed combination method have been evaluated. First, we focused on combining different MT systems which have the same source and target language. The initial experiments were performed on the BTEC Chinese-English task. We combined translations produced by 5 different MT systems. Table 2 shows </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Annual Meeting of the ACL, pp. 311–318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>T Doi</author>
<author>Y Hwang</author>
<author>K Imamura</author>
<author>H Okuma</author>
<author>E Sumita</author>
</authors>
<title>Nobody is Perfect: ATR’s Hybrid Approach to Spoken Language Translation.</title>
<date>2005</date>
<booktitle>International Workshop on Spoken Language Translation,</booktitle>
<pages>55--62</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="2165" citStr="Paul et al., 2005" startWordPosition="319" endWordPosition="322">anslation systems. Combining outputs from different systems was shown to be quite successful in automatic speech recognition (ASR). Voting schemes like the ROVER approach of (Fiscus, 1997) use edit distance alignment and time information to create confusion networks from the output of several ASR systems. Some research on multi-engine machine translation has also been performed in recent years. The most straightforward approaches simply select, for each sentence, one of the provided hypotheses. The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005). Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). To be successful, such approaches require compatible lattices and comparable scores of the (word) hypotheses in the lattices. However, the scores of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable. For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available. (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to cons</context>
</contexts>
<marker>Paul, Doi, Hwang, Imamura, Okuma, Sumita, 2005</marker>
<rawString>M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma, and E. Sumita. 2005. Nobody is Perfect: ATR’s Hybrid Approach to Spoken Language Translation. International Workshop on Spoken Language Translation, pp. 55-62, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TC-STAR Spoken</author>
</authors>
<title>Language Translation Progress Report.</title>
<date>2005</date>
<note>http://www.tc-star.org/documents/ deliverable/Deliv D5 Total 21May05.pdf</note>
<marker>Spoken, 2005</marker>
<rawString>TC-STAR Spoken Language Translation Progress Report. 2005. http://www.tc-star.org/documents/ deliverable/Deliv D5 Total 21May05.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based Word Alignment</title>
<date>1996</date>
<booktitle>in Statistical Translation. 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen,</location>
<contexts>
<context position="6145" citStr="Vogel et al., 1996" startWordPosition="962" endWordPosition="965">nguage. We consider the conditional probability Pr(En|Em) of the event that, given Em, another hypothesis En is generated from the Em. Then, the alignment between the two hypotheses is introduced as a hidden variable: Pr(En|Em) = � Pr(En, A|Em) A This probability is then decomposed into the alignment probability Pr(A|Em) and the lexicon probability Pr(En|A, Em): Pr(En, A|Em) = Pr(A|Em) · Pr(En|A, Em) As in statistical machine translation, we make modelling assumptions. We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model. The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words. The training corpus for alignment is created from a test corpus of N sentences (usually a few hundred) translated by all of the involved MT engines. However, the effective size of the training corpus is larger than N, since all pairs of different hypotheses have to be aligned. Thus, the effective size of the training corpus is M · (M −1) · N. The single-word based lexicon probabilities p(en|em) are initialized with normalized lexicon co</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based Word Alignment in Statistical Translation. 16th Int. Conf. on Computational Linguistics, pp. 836–841, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>