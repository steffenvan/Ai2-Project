<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103703">
<title confidence="0.9723145">
Incremental Recurrent Neural Network Dependency Parser
with Search-based Discriminative Training
</title>
<author confidence="0.98818">
Majid Yazdani James Henderson
</author>
<affiliation confidence="0.961512">
Computer Science Department Xerox Research Center Europe
University of Geneva james.henderson@xrce.xerox.com
</affiliation>
<email confidence="0.996673">
majid.yazdani@unige.ch
</email>
<sectionHeader confidence="0.997364" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999435">
We propose a discriminatively trained re-
current neural network (RNN) that pre-
dicts the actions for a fast and accurate
shift-reduce dependency parser. The RNN
uses its output-dependent model struc-
ture to compute hidden vectors that en-
code the preceding partial parse, and uses
them to estimate probabilities of parser ac-
tions. Unlike a similar previous generative
model (Henderson and Titov, 2010), the
RNN is trained discriminatively to opti-
mize a fast beam search. This beam search
prunes after each shift action, so we add
a correctness probability to each shift ac-
tion and train this score to discriminate be-
tween correct and incorrect sequences of
parser actions. We also speed up pars-
ing time by caching computations for fre-
quent feature combinations, including dur-
ing training, giving us both faster training
and a form of backoff smoothing. The re-
sulting parser is over 35 times faster than
its generative counterpart with nearly the
same accuracy, producing state-of-art de-
pendency parsing results while requiring
minimal feature engineering.
</bodyText>
<sectionHeader confidence="0.99591" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.998747923076923">
There has been significant interest recently in ma-
chine learning and natural language processing
community in models that learn hidden multi-
layer representations to solve various tasks. Neu-
ral networks have been popular in this area as a
powerful and yet efficient models. For example,
feed forward neural networks were used in lan-
guage modeling (Bengio et al., 2003; Collobert
and Weston, 2008), and recurrent neural networks
(RNNs) have yielded state-of-art results in lan-
guage modeling (Mikolov et al., 2010), language
generation (Sutskever et al., 2011) and language
understanding (Yao et al., 2013).
</bodyText>
<subsectionHeader confidence="0.941666">
1.1 Neural Network Parsing
</subsectionHeader>
<bodyText confidence="0.999944702702703">
Neural networks have also been popular in pars-
ing. These models can be divided into those
whose design are motivated mostly by inducing
useful vector representations (e.g. (Socher et al.,
2011; Socher et al., 2013; Collobert, 2011)), and
those whose design are motivated mostly by ef-
ficient inference and decoding (e.g. (Henderson,
2003; Henderson and Titov, 2010; Henderson et
al., 2013; Chen and Manning, 2014)).
The first group of neural network parsers are all
deep models, such as RNNs, which gives them the
power to induce vector representations for com-
plex linguistic structures without extensive feature
engineering. However, decoding in these models
can only be done accurately if they are used to re-
rank the best parse trees of another parser (Socher
et al., 2013).
The second group of parsers use a shift-reduce
parsing architecture so that they can use search
based decoding algorithms with effective pruning
strategies. The more accurate parsers also use a
RNN architecture (see Section 6), and use gener-
ative models to allow beam search. These mod-
els are accurate but are relatively slow, and accu-
racy degrades when you choose decoding settings
to optimize speed. Because they are generative,
they need to predict the words as the parse pro-
ceeds through the sentence, which requires nor-
malization over all the vocabulary of words. Also,
the beam search must maintain many candidates
in the beam in order to check how well each one
predicts future words. Recently (Chen and Man-
ning, 2014) propose a discriminative neural net-
work shift-reduce parser, which is very fast but
less accurate (see Section 6). However, this parser
uses a feed-forward neural network with a large set
of hand-coded features, making it of limited inter-
</bodyText>
<page confidence="0.963361">
142
</page>
<note confidence="0.980508">
Proceedings of the 19th Conference on Computational Language Learning, pages 142–152,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9619895">
est for inducing vector representations of complex
linguistic structures.
</bodyText>
<sectionHeader confidence="0.849492" genericHeader="method">
1.2 Incremental Recurrent Neural Network
Architecture
</sectionHeader>
<bodyText confidence="0.999984266666667">
In both approaches to neural network parsing,
RNN models have the advantage that they need
minimal feature engineering and therefore they
can be used with little effort for a variety of lan-
guages and applications. As with other deep neu-
ral network architectures, RNNs induce complex
features automatically by passing induced (hid-
den) features as input to other induced features
in a recursive structure. This is a particular ad-
vantage for domain adaptation, multi-task learn-
ing, transfer learning, and semi-supervised learn-
ing, where hand-crafted feature engineering is of-
ten particularly difficult. The information that is
transferred from one task to another is embedded
in the induced feature vectors in a shared latent
space, which is input to another hidden layer for
the target model (Henderson et al., 2013; Raina
et al., 2007; Collobert et al., 2011; Glorot et al.,
2011). This transferred information has proven
to be particularly useful when it comes from very
large datasets, such as web-scale text corpora, but
learning and inference on such datasets is only
practical with efficient algorithms.
In this work, we propose a fast discriminative
RNN model of shift-reduce dependency parsing.
We choose a left-to-right shift-reduce dependency
parsing architecture to benefit from efficient de-
coding. It also easily supports incremental in-
terpretation in dialogue systems, or incremental
language modeling for speech recognition. We
choose a RNN architecture to benefit from the au-
tomatic induction of informative vector represen-
tations of complex linguistic structures and the re-
sulting reduction in the required feature engineer-
ing. This hidden vector representation is trained to
encode the partial parse tree that has been built by
the preceding parse, and is used to predict the next
parser action conditioned on this history.
As our RNN architecture, we use the neural
network approximation of ISBNs (Henderson and
Titov, 2010), which we refer to as an Incremental
Neural Network (INN). INNs are a kind of RNN
where the model structure is built incrementally
as a function of the values of previous output vari-
ables. In our case, the hidden vector used to make
the current parser decision is connected to the hid-
den vector from previous decisions based on the
partial parse structure that has been built by the
previous decisions. So any information about the
unbounded parse history can potentially be passed
to the current decision through a chain of hidden
vectors that reflects locality in the parse tree, and
not just locality in the derivation sequence (Hen-
derson, 2003; Titov and Henderson, 2007b). As in
all deep neural network architectures, this chain-
ing of nonlinear vector computations gives the
model a very powerful mechanism to induce com-
plex features from combinations of features in the
history, which is difficult to replicate with hand-
coded features.
</bodyText>
<subsectionHeader confidence="0.994158">
1.3 Search-based Discriminative Training
</subsectionHeader>
<bodyText confidence="0.999919314285714">
We propose a discriminative model because it al-
lows us to use lookahead instead of word predic-
tion. As mentioned above, generative word pre-
diction is costly, both to compute and because it
requires larger beams to be effective. With looka-
head, it is possible to condition on words that
are farther ahead in the string, and thereby avoid
hypothesizing parses that are incompatible with
those future words. This allows the parser to prune
much more aggressively without losing accuracy.
Discriminative learning further improves this ag-
gressive pruning, because it can optimize for the
discrete choice of whether to prune or not (Huang
et al., 2012; Zhang and Clark, 2011).
Our proposed model primarily differs from pre-
vious discriminative models of shift-reduce de-
pendency parsing in the nature of the discrimina-
tive choices that are made and the way these deci-
sions are modeled and learned. Rather than learn-
ing to make pruning decisions at each parse ac-
tion, we learn to choose between sequences of ac-
tions that occur in between two shift actions. This
way of grouping action sequences into chunks as-
sociated with each word has been used previously
for efficient pruning strategies in generative pars-
ing (Henderson, 2003), and for synchronizing syn-
tactic parsing and semantic role labeling in a joint
model (Henderson et al., 2013). We show empiri-
cally that making discriminative parsing decisions
at the scale of these chunks also provides a good
balance between grouping decisions so that more
context can be used to make accurate parsing de-
cisions and dividing decisions so that the space of
alternatives for each decision can be considered
quickly (see Figure 4 below).
</bodyText>
<page confidence="0.998517">
143
</page>
<bodyText confidence="0.999908631578947">
In line with this pruning strategy, we define a
score called the correctness probability for every
shift action. This score is trained discriminatively
to indicate whether the entire parse prefix is cor-
rect or not. This gives us a score function that is
trained to optimize the pruning decisions during
search (c.f. (Daum´e III and Marcu, 2005)). By
combining the scores for all the shift actions in
each candidate parse, we can also discriminate be-
tween multiple parses in a beam of parses, thereby
giving us the option of using beam search to im-
prove accuracy in cases where bounded lookahead
does not provide enough information to make a de-
terministic decision. The correctness probability
is estimated by only looking at the hidden vector at
its shift action, which encourages the hidden units
to encode any information about the parse history
that is relevant to deciding whether this is a good
or bad parse, including long distance features.
</bodyText>
<subsectionHeader confidence="0.997919">
1.4 Feature Decomposition and Caching
</subsectionHeader>
<bodyText confidence="0.999973088888889">
Another popular method of previous neural net-
work models that we use and extend in this paper
is the decomposition of input feature parameters
using vector-matrix multiplication (Bengio et al.,
2003; Collobert et al., 2011; Collobert and We-
ston, 2008). As the previous work shows, this de-
composition overcomes the features sparsity com-
mon in NLP tasks and also enables us to use un-
labeled data effectively. For example, the param-
eter vector for the feature word-on-top-of-stack is
decomposed into the multiplication of a parame-
ter vector representing the word and a parameter
matrix representing top-of-stack. But sparsity is
not always a problem, since the frequency of such
features follows a power law distribution, so there
are some very frequent feature combinations. Pre-
vious work has noticed that the vector-matrix mul-
tiplication of these frequent feature combinations
takes most of the computation time during test-
ing, so they cache these computations (Bengio et
al., 2003; Devlin et al., 2014; Chen and Manning,
2014).
We note that these computations also take most
of the computation time during training, and that
the abundance of data for these feature combina-
tions removes the statistical motivation for decom-
posing them. We propose to treat the cached vec-
tors for high frequency feature combinations as
parameters in their own right, using them both dur-
ing training and during testing.
In summary, this paper makes several contri-
butions to neural network parsing by consider-
ing different scales in the parse sequence and in
the parametrization. We propose a discrimina-
tive recurrent neural network model of depen-
dency parsing that is trained to optimize an effi-
cient form of beam search that prunes based on
the sub-sequences of parser actions between two
shifts, rather than pruning after each parser ac-
tion. We cache high frequency parameter compu-
tations during both testing and training, and train
the cached vectors as separate parameters. As
shown in section 6, these improvements signifi-
cantly reduce both training and testing times while
preserving accuracy.
</bodyText>
<sectionHeader confidence="0.966774" genericHeader="method">
2 History Based Neural Network Parsing
</sectionHeader>
<bodyText confidence="0.999978666666667">
In this section we briefly specify the action se-
quences that we model and the neural network ar-
chitecture that we use to model them.
</bodyText>
<subsectionHeader confidence="0.969464">
2.1 The Parsing Model
</subsectionHeader>
<bodyText confidence="0.999841724137931">
In shift-reduce dependency parsing, at each step of
the parse, the configuration of the parser consists
of a stack S of words, the queue Q of words and
the partial labeled dependency trees constructed
by the previous history of parser actions. The
parser starts with an empty stack S and all the in-
put words in the queue Q, and terminates when it
reaches a configuration with an empty queue Q.
We use an arc-eager algorithm, which has 4 ac-
tions that all manipulate the word s on top of the
stack S and the word q on the front of the queue
Q: The decision Left-Arcr adds a dependency arc
from q to s labeled r. Word s is then popped from
the stack. The decision Right-Arcr adds an arc
from s to q labeled r. The decision Reduce pops
s from the stack. The decision Shift shifts q from
the queue to the stack. For more details we refer
the reader to (Nivre et al., 2004). In this paper we
chose the exact definition of the parse actions that
are used in (Titov and Henderson, 2007b).
At each step of the parse, the parser needs to
choose between the set of possible next actions.
To train a classifier to choose the best actions,
previous work has proposed memory-based clas-
sifiers (Nivre et al., 2004), SVMs (Nivre et al.,
2006), structured perceptron (Huang et al., 2012;
Zhang and Clark, 2011), two-layer neural net-
works (Chen and Manning, 2014), and Incremen-
tal Sigmoid Belief Networks (ISBN) (Titov and
</bodyText>
<page confidence="0.991649">
144
</page>
<bodyText confidence="0.9544332">
Henderson, 2007b), amongst other approaches.
We take a history based approach to model these
sequences of parser actions, which decomposes
the conditional probability of the parse using the
chain rule:
</bodyText>
<equation confidence="0.998882666666667">
P(T|S) = P(D1· · ·Dm|S)
Y=
t P(Dt|D1···Dt−1, S)
</equation>
<bodyText confidence="0.999913444444445">
where T is the parse tree, D1· · ·Dm is its equiva-
lent sequence of shift-reduce parser actions and S
is the input sentence. The probability of Left-Arcr
and Right-Arcr include both the probability of the
attachment decision and the chosen label r. But
unlike in (Titov and Henderson, 2007b), the prob-
ability of Shift does not include a probability pre-
dicting the next word, since all the words S are
included in the conditioning.
</bodyText>
<subsectionHeader confidence="0.999192">
2.2 Estimating Action Probabilities
</subsectionHeader>
<bodyText confidence="0.999943714285714">
To estimate each P(Dt|D1· · ·Dt−1, S), we
need to handle the unbounded nature of both
D1· · ·Dt−1 and S. We can divide S into the words
that have already been shifted, which are handled
as part of our encoding of D1· · ·Dt−1, and the
words on the queue. To condition on the words
in the queue, we use a bounded lookahead:
</bodyText>
<equation confidence="0.9898055">
P (T |S) ≈ Y P(Dt|D1···Dt−1, wt a1···wtak)
t
</equation>
<bodyText confidence="0.999891307692308">
where wt a1· · ·wtak is the first k words on the front
of the queue at time t. At every Shift action the
lookahead changes, moving one word onto the
stack and adding a new word from the input.
To estimate the probability of a decision at time
t conditioned on the history of actions D1· · ·Dt−1,
we overcome the problem of conditioning on an
unbounded amount of information by using a neu-
ral network to induce hidden representations of the
parse history sequence. The relevant information
about the whole parse history at time t is encoded
in its hidden representation, denoted by the vector
ht of size d.
</bodyText>
<equation confidence="0.9723795">
Y P(Dt|D1 ··· Dt−1,wta1 ··· wtak ) = Y P(Dt|ht)
t t
</equation>
<bodyText confidence="0.9977118">
The hidden representation at time t is induced
from hidden representations of the relevant previ-
ous states, plus pre-defined features F computed
from the previous decision and the current queue
and stack:
</bodyText>
<equation confidence="0.99106175">
X
htcWc H +
H
fEF
</equation>
<bodyText confidence="0.998986964285714">
In which C is the set of link types for the previous
relevant hidden representations, htc is the hidden
representation of time tc&lt;t that is relevant to ht by
the relation c, WcHH is the hidden to hidden tran-
sition weights for the link type c, and WIH is the
weights from features F to hidden representations.
Q is the sigmoid function and W (i, :) shows row
i of matrix W. F and C are the only hand-coded
parts of the model.
The decomposition of features has attracted a
lot of attention in NLP tasks, because it overcomes
feature sparsity. There is transfer learning from the
same word (or POS tag, Dependency label, etc.) in
different positions or to similar words. Also unsu-
pervised training of word embeddings can be used
effectively within decomposed features. The use
of unsupervised word embeddings in various nat-
ural language processing tasks has received much
attention (Bengio et al., 2003; Collobert and We-
ston, 2008; Collobert, 2011). Word embeddings
are real-valued feature vectors that are induced
from large corpora of unlabeled text data. Using
word embeddings with a large dictionary improves
domain adaptation, and in the case of a small train-
ing set can improve the performance of the model.
Given these advantages, we use feature decom-
positions to define the input-to-hidden weights
WIH.
</bodyText>
<equation confidence="0.812938">
WIH(f, :) = Wemb.(val(f), :)WfHH
</equation>
<bodyText confidence="0.9967905">
Every row in Wemb. is an embedding for a feature
value, which may be a word, lemma, pos tag, or
dependency relation. val(f) is the index of the
value for feature type f, for example the particular
word that is at the front of the queue. Matrix WfHH
is the transition matrix from the feature value em-
beddings to the hidden vector, for the given feature
type f. For simplicity, we assume here that the
size of the embeddings and the size of the hidden
representations of the INN are the same.
In this way, the parameters of the embedding
matrix Wemb. is shared among various feature in-
put link types f, which can improve the model in
the case of sparse features. It also allows the use of
word embeddings that are available on the web to
improve coverage of sparse features, but we leave
</bodyText>
<equation confidence="0.937185666666667">
Xht = Q(
cEC
WIH(f,:))
</equation>
<page confidence="0.989207">
145
</page>
<bodyText confidence="0.998653833333333">
this investigation to future work since it is orthog-
onal to the contributions of this paper.
Finally, the probability of each decision is nor-
malized across other alternative decisions, and
only conditioned on the hidden representation
(softmax layer):
</bodyText>
<equation confidence="0.993412">
P(Dt=d|ht) = —d&apos; ehtWHO(:,d&apos;)
</equation>
<bodyText confidence="0.9997115">
where WHO is the weight matrix from hidden rep-
resentations to the outputs.
</bodyText>
<sectionHeader confidence="0.96131" genericHeader="method">
3 Discrimination of Partial Parses
</sectionHeader>
<bodyText confidence="0.999916315789474">
Unlike in a generative model, the above formulas
for computing the probability of a tree make inde-
pendence assumptions in that words to the right of
wtay are assumed to be independent of Dt. And
even for words in the lookahead it can be diffi-
cult to learn dependencies with the unstructured
lookahead string. The generative model first con-
structs a structure and then uses word prediction
to test how well that matches the next word. If
a discriminative model uses normalized estimates
for decisions, then once a wrong decision is made
there is no way for the estimates to express that
this decision has lead to a structure that is incom-
patible with the current or future lookahead string
(see (Lafferty et al., 2001) for more discussion).
More generally, any discriminative model that is
trained to predict individual actions has this prob-
lem. In this section we discuss how to overcome
this issue.
</bodyText>
<subsectionHeader confidence="0.996169">
3.1 Discriminating Correct Parse Chunks
</subsectionHeader>
<bodyText confidence="0.999983753623189">
Due to this problem, discriminative parsers typi-
cally make irrevocable choices for each individual
action in the parse. We propose a method for train-
ing a discriminative parser which addresses this
problem in two ways. First, we train the model
to discriminate between larger sub-sequences of
actions, namely the actions between two Shift ac-
tions, which we call chunks. This allows the parser
to delay choosing between actions that occur early
in a chunk until all the structure associated with
that chunk has been built. Second, the model’s
score can be used to discriminate between two
parses long after they have diverged, making it ap-
propriate for a beam search.
We employ a search strategy where we prune
at each shift action, but in between shift actions
we consider all possible sequences of actions,
similarly to the generative parser in (Henderson,
2003). The INN model is discriminatively trained
to choose between these chunks of sequences of
actions.
The most straightforward way to model these
chunk decisions would be to use unnormalized
scores for the decisions in a chunk and sum these
scores to make the decision, as would be done for a
structured perceptron or conditional random field.
Preliminary experiments applying this approach to
our INN parser did not work as well as having lo-
cal normalization of action decisions. We hypoth-
esize that the main reason for this result is that
updating on entire chunks does not provide a suf-
ficiently focused training signal. With a locally
normalized action score, such as softmax, increas-
ing the score of the correct chunk has the effect
of decreasing the score of all the incorrect actions
at each individual action decision. This update
is an approximation to a discriminative update on
all incorrect parses that continue from an incor-
rect decision (Henderson, 2004). Another pos-
sible reason is that local normalization prevents
one action’s score from dominating the score of
the whole parse, as can happen with high fre-
quency decisions. In general, this problem can
not be solved just by using norm regularization on
weights.
The places in a parse where the generative up-
date and the discriminative update differ substan-
tially are at word predictions, where the genera-
tive model considers that all words are possible but
a discriminative model already knows what word
comes next and so does not need to predict any-
thing. We discriminatively train the INN to choose
between chunks of actions by adding a new score
at these places in the parse. After each shift ac-
tion, we introduce a correctness probability that
is trained to discriminate between cases where the
chunk of actions since the previous shift is cor-
rect and those where this chunk is incorrect. Thus,
the search strategy chooses between all possible
sequences of actions between two shifts using a
combination of the normalized scores for each ac-
tion and the correctness probability.
In addition to discriminative training at the
chunk level, the correctness probability allows us
to search using a beam of parses. If a correct de-
cision can not be disambiguated, because of the
independence assumptions with words beyond the
lookahead or because of the difficulty of inferring
from an unstructured lookahead, the correctness
</bodyText>
<equation confidence="0.751586">
ehtWHO(:,d)
</equation>
<page confidence="0.988845">
146
</page>
<figureCaption confidence="0.999764">
Figure 1: INN computations for one decision
</figureCaption>
<bodyText confidence="0.999747333333334">
probability score will drop whenever the mistake
becomes evident. This means that we can not only
compare two partial parses that differ in the most
recent chunk, but we can also compare two partial
parses that differ in earlier chunks. This allows
us to use beam search decoding. Instead of deter-
ministically choosing a single partial parse at each
shift action, we can maintain a small beam of al-
ternatives and choose between them based on how
compatible they are with future lookahead strings
by comparing their respective correctness proba-
bilities for those future shifts.
We combine this correctness probability with
the action probabilities by simply multiplying:
and Henderson, 2007b). These sequences of ac-
tions provide us with positive examples. For dis-
criminative training, we also need incorrect parses
to act as the negative examples. In particular, we
want negative examples that will allow us to opti-
mize the pruning decisions made by the parser.
To optimize the pruning decisions made by the
parsing model, we use the parsing model itself
to generate the negative examples (Collins and
Roark, 2004). Using the current parameters of the
model, we apply our search-based decoding strat-
egy to find our current approximation to the high-
est scoring complete parse, which is the output of
our current parsing model. If this parse differs
from the correct one, then we train the parameters
of all the correctness probabilities in each parse
so as to increase the score of the correct parse and
decrease the score of the incorrect output parse.
By repeatedly decreasing the score of the incorrect
best parse as the model parameters are learned,
training will efficiently decreases the score of all
incorrect parses.
As discussed above, we train the scores of in-
dividual parser actions to optimize the locally-
normalized conditional probability of the correct
action. Putting this together with the above train-
ing of the correctness probabilities P(Correct|ht),
we get the following objective function:
</bodyText>
<equation confidence="0.938192857142857">
fl P(Dt|ht)P(Correct|ht) argmaxo
P (T |S) ≈
t
� � logP(dt|ht) + logP(Correct|ht)
T ETpos tET
�− � logP(Correct|ht)
TETφney tET
</equation>
<bodyText confidence="0.9999354">
where we train P(Correct|ht) to be the correct-
ness probability for the cases where Dt is a shift
action, and define P(Correct|ht) = 1 otherwise.
For the shift actions, P(Correct|ht) is defined us-
ing the sigmoid function:
</bodyText>
<equation confidence="0.997607">
P(Correct|ht) = Q(htWCor)
</equation>
<bodyText confidence="0.999902">
In this way, the hidden representations are trained
not only to choose the right action, but also to en-
code correctness of the partial parses. Figure 1
shows the model schematically.
</bodyText>
<subsectionHeader confidence="0.999631">
3.2 Training the Parameters
</subsectionHeader>
<bodyText confidence="0.999974714285714">
We want to train the correctness probabilities to
discriminate correct parses from incorrect parses.
The correct parses can be extracted from the train-
ing treebank by converting each dependency tree
into its equivalent sequence of arc-eager shift-
reduce parser actions (Nivre et al., 2004; Titov
where 0 is the set of all parameters of the model
(namely WHH, Wemb., WHO, and WCor), Tpos
is the set of correct parses, and Toneg is the set
of incorrect parses which the model 0 scores
higher than their corresponding correct parses.
The derivative of this objective function is the er-
ror signal that the neural network learns to mini-
mize. This error signal is illustrated schematically
in Figure 2.
We optimize the above objective using stochas-
tic gradient descent. For each parameter update,
a positive tree is chosen randomly and a negative
tree is built using the above strategy. The resulting
error signals are backpropagated through the INN
to compute the derivatives for gradient descent.
</bodyText>
<page confidence="0.9971">
147
</page>
<figureCaption confidence="0.999357">
Figure 2: Positive and negative derivation branches and their training signals
</figureCaption>
<figure confidence="0.992074">
(a) Log-Log frequency by (b) Cumulative fraction of
rank of features features ranked by frequency
</figure>
<figureCaption confidence="0.999292">
Figure 3: Feature frequency distributions
</figureCaption>
<sectionHeader confidence="0.984331" genericHeader="method">
4 Caching Frequent Features
</sectionHeader>
<bodyText confidence="0.999963315789474">
Decomposing the parametrization of input fea-
tures using vector-matrix multiplication, as was
described in section 2, overcomes the features
sparsity common in NLP tasks and also makes it
possible to use unlabeled data effectively. But it
adds a huge amount of computation to both the
training and decoding, since for every input fea-
ture a vector-matrix multiplication is needed. This
problem is even more severe in our algorithm be-
cause at every training iteration we search for the
best incorrect parse.
The frequency of features follows a power law
distribution; there are a few very frequent features,
and a long tail of infrequent features. Previous
work has noticed that the vector-matrix multiplica-
tion of the frequent features takes most of the com-
putation time during testing, so they cache these
computations (Bengio et al., 2003; Devlin et al.,
2014; Chen and Manning, 2014). For example
in the Figure 3(b), only 2100 features are respon-
sible of 90% of the computations among ∼400k
features, so cashing these computations can have
a huge impact on speed. We note that these com-
putations also take most of the computation time
during training. First, this computation is the dom-
inant part of the forward and backward error com-
putations. Second, at every iteration we need to
decode to find the highest scoring incorrect parse.
We propose to treat the cached vectors for high
frequency features as parameters in their own
right, using them both during training and during
testing. This speeds up training because it is no
longer necessary to do the high-frequency vector-
matrix multiplications, neither to do the forward
error computations nor to do the backpropagation
through the vector-matrix multiplications. Also,
the cached vectors used in decoding do not need to
be recomputed every time the parameters change,
since the vectors are updated directly by the pa-
rameter updates.
Another possible motivation for treating the
cached vectors as parameters is that it results in
a kind of backoff smoothing; high frequency fea-
tures are given specific parameters, and for low
frequency features we back off to the decomposed
model. Results from smoothing methods for sym-
bolic statistical models indicate that it is better to
smooth low frequency features with other low fre-
quency features, and treat high frequency features
individually. In this paper we do not systemati-
cally investigate this potential advantage, leaving
this for future work.
In our experiments we cache features that make
up to 90% of the feature frequencies. This gives
us about a 20 times speed up during training and
about a 7 times speed up during testing, while per-
formance is preserved.
</bodyText>
<sectionHeader confidence="0.891457" genericHeader="method">
5 Parsing Complexity
</sectionHeader>
<bodyText confidence="0.9879195">
If the length of the latent vectors is d, for a sen-
tence of length L, and beam B, the decoding com-
</bodyText>
<page confidence="0.996296">
148
</page>
<figureCaption confidence="0.9983725">
Figure 4: Histogram of number of candidate ac-
tions between shifts
</figureCaption>
<bodyText confidence="0.998640517241379">
plexity is O(L x M x B x (JFJ + JCJ) x d2).1 If
we choose the best partial parse tree at every shift,
then B = 1. M is the total number of actions in
the candidate chunks that are generated between
two shifts, so L x M is the total number of can-
didate actions in a parse. For shift-reduce depen-
dency parsing, the total number of chosen actions
is necessarily linear in L, but because we are ap-
plying best-first search in between shifts, M is not
necessarily independent of L. To investigate the
impact of M on the speed of the parser, we em-
pirically measure the number of candidate actions
generated by the parser between 33368 different
shifts. The resulting distribution is plotted in Fig-
ure 4. We observe that it forms a power law distri-
bution. Most of the time the number of actions is
very small (2 or 3), with the maximum number be-
ing 40, and the average being 2.25. We conclude
from this that the value of M is not a major factor
in the parser’s speed.
Remember that JFJ is the number of input fea-
ture types. Caching 90% of the input feature com-
putations allows us to reasonably neglect this term.
Because JCJ is the number of hidden-to-hidden
connection types, we cannot apply caching to re-
duce this term. However, JCJ is much smaller
than JFJ (here JCJ=3). This is why caching in-
put feature computations has such a large impact
on parser speed.
</bodyText>
<footnote confidence="0.97261975">
1For this analysis, we assume that the output computation
is negligable compared to the hidden representation compu-
tation, because the output computation grows with d while
the hidden computation grows with d2.
</footnote>
<sectionHeader confidence="0.985933" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.99998792">
We used syntactic dependencies from the English
section of the CoNLL 2009 shared task dataset
(Hajiˇc et al., 2009). Standard splits of training,
development and test sets were used. We compare
our model to the generative INN model (Titov and
Henderson, 2007b), MALT parser, MST parser,
and the feed-forward neural network parser of
(Chen and Manning, 2014) (“C&amp;M”). All these
models and our own models are trained only on the
CoNLL 2009 syntactic data; they use no external
word embeddings or other unsupervised training
on additional data. This is one reason for choos-
ing these models for comparison. In addition, the
generative model (“Generative INN, large beam”
in Table 1) was compared extensively to state-of-
art parsers on various languages and tasks in pre-
vious work (Titov and Henderson, 2007b; Titov
and Henderson, 2007a; Henderson et al., 2008).
Therefore, here our objective is not repeating an
extensive comparison to the available parsers.
Table 1 shows the labeled and unlabeled ac-
curacy of attachments for these models. The
MALT and MST parser scores come from (Sur-
deanu and Manning, 2010), which compared dif-
ferent parsing models using CoNLL 2008 shared
task dataset, which is the same as CoNLL 2009
for English syntactic parsing. The results for the
generative INN with a large beam were taken from
(Henderson and Titov, 2010), which uses an archi-
tecture with 80 hidden units. We replicate this set-
ting for the other generative INN results and our
discriminative INN results. The parser of (Chen
and Manning, 2014) was run with their architec-
ture of 200 hidden units with dropout training
(“C&amp;M”). All parsing speeds were computed us-
ing the latest downloadable versions of the parsers,
on a single 3.4GHz CPU.
Our model with beam 1 (i.e. deterministic
choices of chunks) (“DINN, beam 1”) produces
state-of-the-art results while it is over 35 times
faster than the generative model with beam size
10. Moreover, we are able to achieve higher ac-
curacies using larger beams (“DINN, beam 10”).
The discriminative training of the correctness
probabilities to optimize search is crucial to these
levels of accuracy, as indicated by the relatively
poor performance of our model when this training
is removed (“Discriminative INN, no search train-
ing”). Previous deterministic shift-reduce parsers
(“MALTAE” and “C&amp;M”) are around twice as fast
</bodyText>
<page confidence="0.995437">
149
</page>
<table confidence="0.999829615384615">
Model LAA UAA wrd/sec
MALTAE 85.96 88.64 7549
C&amp;M 86.49 89.17 9589
MST 87.07 89.95 290
MALT-MST 87.45 90.22 NA
Generative INN, 77.83 81.49 1122
beam 1
beam 10 87.67 90.61 107
large beam 88.65 91.44 NA
Discriminative INN, 85.28 88.98 4012
no search training
DINN, beam 1 87.26 90.13 4035
DINN, beam 10 88.14 90.75 433
</table>
<tableCaption confidence="0.9985975">
Table 1: Labelled and unlabelled attachment accu-
racies and speeds on the test set.
</tableCaption>
<table confidence="0.998432166666667">
Model LAA German LAA Spanish LAA Czech
UAA UAA UAA
C&amp;M 82.5 86.1 81.5 85.4 58.6 70.6
MALT 80.7 83.1 82.4 86.6 67.3 77.4
MST 84.1 87.6 82.7 87.3 73.4 81.7
DINN 86.0 89.6 85.4 88.3 77.5 85.2
</table>
<tableCaption confidence="0.9789745">
Table 2: Labelled and unlabelled attachment accu-
racies on the test set of CoNLL 2009.
</tableCaption>
<bodyText confidence="0.999601307692308">
as our beam 1 model, but at the cost of significant
reductions in accuracy.
To evaluate our RNN model’s ability to induce
informative features automatically, we trained our
deterministic model, MALT, MST and C&amp;M on
three diverse languages from CoNLL 2009, using
the same features as used in the above experiments
on English (model “DINN, beam 1”). We did no
language-specific feature engineering for any of
these parsers. Table 2 shows that our RNN model
generalizes substantially better than all these mod-
els to new languages, demonstrating the power of
this model’s feature induction.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995666666667">
We propose an efficient and accurate recurrent
neural network dependency parser that uses neu-
ral network hidden representations to encode arbi-
trarily large partial parses for predicting the next
parser action. This parser uses a search strategy
that prunes to a deterministic choice at each shift
action, so we add a correctness probability to each
shift operation, and train this score to discriminate
between correct and incorrect sequences of parser
actions. All other probability estimates are trained
to optimize the conditional probability of the parse
given the sentence. We also speed up both pars-
ing and training times by only decomposing infre-
quent features, giving us both a form of backoff
smoothing and twenty times faster training.
The discriminative training for this pruning
strategy allows high accuracy to be preserved
while greatly speeding up parsing time. The re-
current neural network architecture provides pow-
erful automatic feature induction, resulting in high
accuracy on diverse languages without tuning.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866666666667">
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993424742857143">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750. Association for Compu-
tational Linguistics, October.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, ACL ’04. Associa-
tion for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing:
Deep neural networks with multitask learning. In
Proceedings of the 25th International Conference
on Machine Learning, ICML ’08, pages 160–167.
ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Proceedings of the Four-
teenth International Conference on Artificial Intel-
ligence and Statistics (AISTATS-11), volume 15,
pages 224–232. Journal of Machine Learning Re-
search - Workshop and Conference Proceedings.
Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
</reference>
<page confidence="0.991334">
150
</page>
<reference confidence="0.999346078947368">
of the 22nd International Conference on Machine
Learning, pages 169–176.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), ICML ’11, pages
513–520. ACM, June.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, CoNLL ’09, pages
1–18. Association for Computational Linguistics.
James Henderson and Ivan Titov. 2010. Incremental
sigmoid belief networks for grammar learning. J.
Mach. Learn. Res., 11:3541–3570, December.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning,
CoNLL ’08, pages 178–182. Association for Com-
putational Linguistics.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Comput. Linguist., 39(4):949–998,
December.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 24–31. Association for
Computational Linguistics.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
95–102, July.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151. Association for Computational Linguis-
tics, June.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289. Morgan Kaufmann Publishers
Inc.
Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan
ˇCernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Association (IN-
TERSPEECH 2010), volume 2010, pages 1045–
1048. International Speech Communication Associ-
ation.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Hwee Tou
Ng and Ellen Riloff, editors, HLT-NAACL 2004
Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages
49–56. Association for Computational Linguistics,
May 6 - May 7.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en
Eryiˇgit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ’06, pages 221–225. Association for
Computational Linguistics.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught
learning: Transfer learning from unlabeled data. In
Proceedings of the 24th International Conference
on Machine Learning, ICML ’07, pages 759–766.
ACM.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and
Chris Manning. 2011. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ’11, pages
129–136. ACM.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465. Association for Computational Linguis-
tics, August.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 649–652. Association for
Computational Linguistics.
</reference>
<page confidence="0.978011">
151
</page>
<reference confidence="0.999600130434783">
Ilya Sutskever, James Martens, and Geoffrey Hin-
ton. 2011. Generating text with recurrent neu-
ral networks. In Proceedings of the 28th Interna-
tional Conference on Machine Learning (ICML-11),
ICML ’11, pages 1017–1024. ACM, June.
Ivan Titov and James Henderson. 2007a. Fast and
robust multilingual dependency parsing with a gen-
erative latent variable model. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 947–951. Association for Computa-
tional Linguistics, June.
Ivan Titov and James Henderson. 2007b. A latent
variable model for generative dependency parsing.
In Proceedings of the 10th International Conference
on Parsing Technologies, IWPT ’07, pages 144–155.
Association for Computational Linguistics.
Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In INTER-
SPEECH, pages 2524–2528.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Comput. Linguist., 37(1):105–151, March.
</reference>
<page confidence="0.998128">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935948">
<title confidence="0.9988065">Incremental Recurrent Neural Network Dependency Parser with Search-based Discriminative Training</title>
<author confidence="0.99994">Majid Yazdani James Henderson</author>
<affiliation confidence="0.9926595">Computer Science Department Xerox Research Center Europe of Geneva</affiliation>
<email confidence="0.956745">majid.yazdani@unige.ch</email>
<abstract confidence="0.999786">We propose a discriminatively trained recurrent neural network (RNN) that predicts the actions for a fast and accurate shift-reduce dependency parser. The RNN uses its output-dependent model structure to compute hidden vectors that encode the preceding partial parse, and uses them to estimate probabilities of parser actions. Unlike a similar previous generative model (Henderson and Titov, 2010), the RNN is trained discriminatively to optimize a fast beam search. This beam search prunes after each shift action, so we add probability each shift action and train this score to discriminate between correct and incorrect sequences of parser actions. We also speed up parsing time by caching computations for frequent feature combinations, including during training, giving us both faster training and a form of backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1716" citStr="Bengio et al., 2003" startWordPosition="253" endWordPosition="256"> backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 1 Introduction and Motivation There has been significant interest recently in machine learning and natural language processing community in models that learn hidden multilayer representations to solve various tasks. Neural networks have been popular in this area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 20</context>
<context position="9809" citStr="Bengio et al., 2003" startWordPosition="1544" endWordPosition="1547">cases where bounded lookahead does not provide enough information to make a deterministic decision. The correctness probability is estimated by only looking at the hidden vector at its shift action, which encourages the hidden units to encode any information about the parse history that is relevant to deciding whether this is a good or bad parse, including long distance features. 1.4 Feature Decomposition and Caching Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008). As the previous work shows, this decomposition overcomes the features sparsity common in NLP tasks and also enables us to use unlabeled data effectively. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed</context>
<context position="16157" citStr="Bengio et al., 2003" startWordPosition="2648" endWordPosition="2651">F to hidden representations. Q is the sigmoid function and W (i, :) shows row i of matrix W. F and C are the only hand-coded parts of the model. The decomposition of features has attracted a lot of attention in NLP tasks, because it overcomes feature sparsity. There is transfer learning from the same word (or POS tag, Dependency label, etc.) in different positions or to similar words. Also unsupervised training of word embeddings can be used effectively within decomposed features. The use of unsupervised word embeddings in various natural language processing tasks has received much attention (Bengio et al., 2003; Collobert and Weston, 2008; Collobert, 2011). Word embeddings are real-valued feature vectors that are induced from large corpora of unlabeled text data. Using word embeddings with a large dictionary improves domain adaptation, and in the case of a small training set can improve the performance of the model. Given these advantages, we use feature decompositions to define the input-to-hidden weights WIH. WIH(f, :) = Wemb.(val(f), :)WfHH Every row in Wemb. is an embedding for a feature value, which may be a word, lemma, pos tag, or dependency relation. val(f) is the index of the value for feat</context>
<context position="26730" citStr="Bengio et al., 2003" startWordPosition="4384" endWordPosition="4387">ffectively. But it adds a huge amount of computation to both the training and decoding, since for every input feature a vector-matrix multiplication is needed. This problem is even more severe in our algorithm because at every training iteration we search for the best incorrect parse. The frequency of features follows a power law distribution; there are a few very frequent features, and a long tail of infrequent features. Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). For example in the Figure 3(b), only 2100 features are responsible of 90% of the computations among ∼400k features, so cashing these computations can have a huge impact on speed. We note that these computations also take most of the computation time during training. First, this computation is the dominant part of the forward and backward error computations. Second, at every iteration we need to decode to find the highest scoring incorrect parse. We propose to treat the cached vectors for high frequency features as parameters in their own right, u</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2394" citStr="Chen and Manning, 2014" startWordPosition="356" endWordPosition="359">works (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The more accurate parsers also use a RNN architecture (see Section 6), and use </context>
<context position="10637" citStr="Chen and Manning, 2014" startWordPosition="1676" endWordPosition="1679">ly. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed that the vector-matrix multiplication of these frequent feature combinations takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). We note that these computations also take most of the computation time during training, and that the abundance of data for these feature combinations removes the statistical motivation for decomposing them. We propose to treat the cached vectors for high frequency feature combinations as parameters in their own right, using them both during training and during testing. In summary, this paper makes several contributions to neural network parsing by considering different scales in the parse sequence and in the parametrization. We propose a discriminative recurrent neural network model of depen</context>
<context position="13230" citStr="Chen and Manning, 2014" startWordPosition="2129" endWordPosition="2132">duce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a history based approach to model these sequences of parser actions, which decomposes the conditional probability of the parse using the chain rule: P(T|S) = P(D1· · ·Dm|S) Y= t P(Dt|D1···Dt−1, S) where T is the parse tree, D1· · ·Dm is its equivalent sequence of shift-reduce parser actions and S is the input sentence. The probability of Left-Arcr and Right-Arcr include both the probability of the attachment decision and the chosen label r. But unlike in (Titov and Henderson, 20</context>
<context position="26776" citStr="Chen and Manning, 2014" startWordPosition="4392" endWordPosition="4395">omputation to both the training and decoding, since for every input feature a vector-matrix multiplication is needed. This problem is even more severe in our algorithm because at every training iteration we search for the best incorrect parse. The frequency of features follows a power law distribution; there are a few very frequent features, and a long tail of infrequent features. Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). For example in the Figure 3(b), only 2100 features are responsible of 90% of the computations among ∼400k features, so cashing these computations can have a huge impact on speed. We note that these computations also take most of the computation time during training. First, this computation is the dominant part of the forward and backward error computations. Second, at every iteration we need to decode to find the highest scoring incorrect parse. We propose to treat the cached vectors for high frequency features as parameters in their own right, using them both during training and during test</context>
<context position="30695" citStr="Chen and Manning, 2014" startWordPosition="5068" endWordPosition="5071">a large impact on parser speed. 1For this analysis, we assume that the output computation is negligable compared to the hidden representation computation, because the output computation grows with d while the hidden computation grows with d2. 6 Experimental Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009). Standard splits of training, development and test sets were used. We compare our model to the generative INN model (Titov and Henderson, 2007b), MALT parser, MST parser, and the feed-forward neural network parser of (Chen and Manning, 2014) (“C&amp;M”). All these models and our own models are trained only on the CoNLL 2009 syntactic data; they use no external word embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). Therefore, here our objective is not repeating an extensive comparison to the available parsers. Tab</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750. Association for Computational Linguistics, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23169" citStr="Collins and Roark, 2004" startWordPosition="3812" endWordPosition="3815">trings by comparing their respective correctness probabilities for those future shifts. We combine this correctness probability with the action probabilities by simply multiplying: and Henderson, 2007b). These sequences of actions provide us with positive examples. For discriminative training, we also need incorrect parses to act as the negative examples. In particular, we want negative examples that will allow us to optimize the pruning decisions made by the parser. To optimize the pruning decisions made by the parsing model, we use the parsing model itself to generate the negative examples (Collins and Roark, 2004). Using the current parameters of the model, we apply our search-based decoding strategy to find our current approximation to the highest scoring complete parse, which is the output of our current parsing model. If this parse differs from the correct one, then we train the parameters of all the correctness probabilities in each parse so as to increase the score of the correct parse and decrease the score of the incorrect output parse. By repeatedly decreasing the score of the incorrect best parse as the model parameters are learned, training will efficiently decreases the score of all incorrec</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1745" citStr="Collobert and Weston, 2008" startWordPosition="257" endWordPosition="260">he resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 1 Introduction and Motivation There has been significant interest recently in machine learning and natural language processing community in models that learn hidden multilayer representations to solve various tasks. Neural networks have been popular in this area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010</context>
<context position="9862" citStr="Collobert and Weston, 2008" startWordPosition="1552" endWordPosition="1556">e enough information to make a deterministic decision. The correctness probability is estimated by only looking at the hidden vector at its shift action, which encourages the hidden units to encode any information about the parse history that is relevant to deciding whether this is a good or bad parse, including long distance features. 1.4 Feature Decomposition and Caching Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008). As the previous work shows, this decomposition overcomes the features sparsity common in NLP tasks and also enables us to use unlabeled data effectively. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed that the vector-matrix multiplication of these frequ</context>
<context position="16185" citStr="Collobert and Weston, 2008" startWordPosition="2652" endWordPosition="2656">ations. Q is the sigmoid function and W (i, :) shows row i of matrix W. F and C are the only hand-coded parts of the model. The decomposition of features has attracted a lot of attention in NLP tasks, because it overcomes feature sparsity. There is transfer learning from the same word (or POS tag, Dependency label, etc.) in different positions or to similar words. Also unsupervised training of word embeddings can be used effectively within decomposed features. The use of unsupervised word embeddings in various natural language processing tasks has received much attention (Bengio et al., 2003; Collobert and Weston, 2008; Collobert, 2011). Word embeddings are real-valued feature vectors that are induced from large corpora of unlabeled text data. Using word embeddings with a large dictionary improves domain adaptation, and in the case of a small training set can improve the performance of the model. Given these advantages, we use feature decompositions to define the input-to-hidden weights WIH. WIH(f, :) = Wemb.(val(f), :)WfHH Every row in Wemb. is an embedding for a feature value, which may be a word, lemma, pos tag, or dependency relation. val(f) is the index of the value for feature type f, for example the </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="4880" citStr="Collobert et al., 2011" startWordPosition="748" endWordPosition="751">ural network architectures, RNNs induce complex features automatically by passing induced (hidden) features as input to other induced features in a recursive structure. This is a particular advantage for domain adaptation, multi-task learning, transfer learning, and semi-supervised learning, where hand-crafted feature engineering is often particularly difficult. The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space, which is input to another hidden layer for the target model (Henderson et al., 2013; Raina et al., 2007; Collobert et al., 2011; Glorot et al., 2011). This transferred information has proven to be particularly useful when it comes from very large datasets, such as web-scale text corpora, but learning and inference on such datasets is only practical with efficient algorithms. In this work, we propose a fast discriminative RNN model of shift-reduce dependency parsing. We choose a left-to-right shift-reduce dependency parsing architecture to benefit from efficient decoding. It also easily supports incremental interpretation in dialogue systems, or incremental language modeling for speech recognition. We choose a RNN arch</context>
<context position="9833" citStr="Collobert et al., 2011" startWordPosition="1548" endWordPosition="1551">ookahead does not provide enough information to make a deterministic decision. The correctness probability is estimated by only looking at the hidden vector at its shift action, which encourages the hidden units to encode any information about the parse history that is relevant to deciding whether this is a good or bad parse, including long distance features. 1.4 Feature Decomposition and Caching Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008). As the previous work shows, this decomposition overcomes the features sparsity common in NLP tasks and also enables us to use unlabeled data effectively. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed that the vector-matrix </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),</booktitle>
<volume>15</volume>
<pages>224--232</pages>
<contexts>
<context position="2213" citStr="Collobert, 2011" startWordPosition="330" endWordPosition="331">and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing </context>
<context position="16203" citStr="Collobert, 2011" startWordPosition="2657" endWordPosition="2658">ction and W (i, :) shows row i of matrix W. F and C are the only hand-coded parts of the model. The decomposition of features has attracted a lot of attention in NLP tasks, because it overcomes feature sparsity. There is transfer learning from the same word (or POS tag, Dependency label, etc.) in different positions or to similar words. Also unsupervised training of word embeddings can be used effectively within decomposed features. The use of unsupervised word embeddings in various natural language processing tasks has received much attention (Bengio et al., 2003; Collobert and Weston, 2008; Collobert, 2011). Word embeddings are real-valued feature vectors that are induced from large corpora of unlabeled text data. Using word embeddings with a large dictionary improves domain adaptation, and in the case of a small training set can improve the performance of the model. Given these advantages, we use feature decompositions to define the input-to-hidden weights WIH. WIH(f, :) = Wemb.(val(f), :)WfHH Every row in Wemb. is an embedding for a feature value, which may be a word, lemma, pos tag, or dependency relation. val(f) is the index of the value for feature type f, for example the particular word th</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), volume 15, pages 224–232. Journal of Machine Learning Research - Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>169--176</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proceedings of the 22nd International Conference on Machine Learning, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>1</volume>
<institution>Long Papers). Association for Computational Linguistics.</institution>
<contexts>
<context position="10612" citStr="Devlin et al., 2014" startWordPosition="1672" endWordPosition="1675">abeled data effectively. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed that the vector-matrix multiplication of these frequent feature combinations takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). We note that these computations also take most of the computation time during training, and that the abundance of data for these feature combinations removes the statistical motivation for decomposing them. We propose to treat the cached vectors for high frequency feature combinations as parameters in their own right, using them both during training and during testing. In summary, this paper makes several contributions to neural network parsing by considering different scales in the parse sequence and in the parametrization. We propose a discriminative recurrent neur</context>
<context position="26751" citStr="Devlin et al., 2014" startWordPosition="4388" endWordPosition="4391">ds a huge amount of computation to both the training and decoding, since for every input feature a vector-matrix multiplication is needed. This problem is even more severe in our algorithm because at every training iteration we search for the best incorrect parse. The frequency of features follows a power law distribution; there are a few very frequent features, and a long tail of infrequent features. Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). For example in the Figure 3(b), only 2100 features are responsible of 90% of the computations among ∼400k features, so cashing these computations can have a huge impact on speed. We note that these computations also take most of the computation time during training. First, this computation is the dominant part of the forward and backward error computations. Second, at every iteration we need to decode to find the highest scoring incorrect parse. We propose to treat the cached vectors for high frequency features as parameters in their own right, using them both during</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11,</booktitle>
<pages>513--520</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="4902" citStr="Glorot et al., 2011" startWordPosition="752" endWordPosition="755">es, RNNs induce complex features automatically by passing induced (hidden) features as input to other induced features in a recursive structure. This is a particular advantage for domain adaptation, multi-task learning, transfer learning, and semi-supervised learning, where hand-crafted feature engineering is often particularly difficult. The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space, which is input to another hidden layer for the target model (Henderson et al., 2013; Raina et al., 2007; Collobert et al., 2011; Glorot et al., 2011). This transferred information has proven to be particularly useful when it comes from very large datasets, such as web-scale text corpora, but learning and inference on such datasets is only practical with efficient algorithms. In this work, we propose a fast discriminative RNN model of shift-reduce dependency parsing. We choose a left-to-right shift-reduce dependency parsing architecture to benefit from efficient decoding. It also easily supports incremental interpretation in dialogue systems, or incremental language modeling for speech recognition. We choose a RNN architecture to benefit fr</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 513–520. ACM, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, pages 1–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Incremental sigmoid belief networks for grammar learning.</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>11--3541</pages>
<contexts>
<context position="665" citStr="Henderson and Titov, 2010" startWordPosition="85" endWordPosition="88">pendency Parser with Search-based Discriminative Training Majid Yazdani James Henderson Computer Science Department Xerox Research Center Europe University of Geneva james.henderson@xrce.xerox.com majid.yazdani@unige.ch Abstract We propose a discriminatively trained recurrent neural network (RNN) that predicts the actions for a fast and accurate shift-reduce dependency parser. The RNN uses its output-dependent model structure to compute hidden vectors that encode the preceding partial parse, and uses them to estimate probabilities of parser actions. Unlike a similar previous generative model (Henderson and Titov, 2010), the RNN is trained discriminatively to optimize a fast beam search. This beam search prunes after each shift action, so we add a correctness probability to each shift action and train this score to discriminate between correct and incorrect sequences of parser actions. We also speed up parsing time by caching computations for frequent feature combinations, including during training, giving us both faster training and a form of backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsin</context>
<context position="2345" citStr="Henderson and Titov, 2010" startWordPosition="348" endWordPosition="351">llobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The more accurate parsers also</context>
<context position="5963" citStr="Henderson and Titov, 2010" startWordPosition="912" endWordPosition="915">o easily supports incremental interpretation in dialogue systems, or incremental language modeling for speech recognition. We choose a RNN architecture to benefit from the automatic induction of informative vector representations of complex linguistic structures and the resulting reduction in the required feature engineering. This hidden vector representation is trained to encode the partial parse tree that has been built by the preceding parse, and is used to predict the next parser action conditioned on this history. As our RNN architecture, we use the neural network approximation of ISBNs (Henderson and Titov, 2010), which we refer to as an Incremental Neural Network (INN). INNs are a kind of RNN where the model structure is built incrementally as a function of the values of previous output variables. In our case, the hidden vector used to make the current parser decision is connected to the hidden vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just loca</context>
<context position="31682" citStr="Henderson and Titov, 2010" startWordPosition="5230" endWordPosition="5233"> parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). Therefore, here our objective is not repeating an extensive comparison to the available parsers. Table 1 shows the labeled and unlabeled accuracy of attachments for these models. The MALT and MST parser scores come from (Surdeanu and Manning, 2010), which compared different parsing models using CoNLL 2008 shared task dataset, which is the same as CoNLL 2009 for English syntactic parsing. The results for the generative INN with a large beam were taken from (Henderson and Titov, 2010), which uses an architecture with 80 hidden units. We replicate this setting for the other generative INN results and our discriminative INN results. The parser of (Chen and Manning, 2014) was run with their architecture of 200 hidden units with dropout training (“C&amp;M”). All parsing speeds were computed using the latest downloadable versions of the parsers, on a single 3.4GHz CPU. Our model with beam 1 (i.e. deterministic choices of chunks) (“DINN, beam 1”) produces state-of-the-art results while it is over 35 times faster than the generative model with beam size 10. Moreover, we are able to a</context>
</contexts>
<marker>Henderson, Titov, 2010</marker>
<rawString>James Henderson and Ivan Titov. 2010. Incremental sigmoid belief networks for grammar learning. J. Mach. Learn. Res., 11:3541–3570, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>178--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31193" citStr="Henderson et al., 2008" startWordPosition="5149" endWordPosition="5152">(Titov and Henderson, 2007b), MALT parser, MST parser, and the feed-forward neural network parser of (Chen and Manning, 2014) (“C&amp;M”). All these models and our own models are trained only on the CoNLL 2009 syntactic data; they use no external word embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). Therefore, here our objective is not repeating an extensive comparison to the available parsers. Table 1 shows the labeled and unlabeled accuracy of attachments for these models. The MALT and MST parser scores come from (Surdeanu and Manning, 2010), which compared different parsing models using CoNLL 2008 shared task dataset, which is the same as CoNLL 2009 for English syntactic parsing. The results for the generative INN with a large beam were taken from (Henderson and Titov, 2010), which uses an architecture with 80 hidden units. We replicate this setting for the other generative INN resul</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 178–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
<author>Gabriele Musillo</author>
</authors>
<title>Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model.</title>
<date>2013</date>
<journal>Comput. Linguist.,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="2369" citStr="Henderson et al., 2013" startWordPosition="352" endWordPosition="355">and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The more accurate parsers also use a RNN architecture </context>
<context position="4836" citStr="Henderson et al., 2013" startWordPosition="740" endWordPosition="743">ages and applications. As with other deep neural network architectures, RNNs induce complex features automatically by passing induced (hidden) features as input to other induced features in a recursive structure. This is a particular advantage for domain adaptation, multi-task learning, transfer learning, and semi-supervised learning, where hand-crafted feature engineering is often particularly difficult. The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space, which is input to another hidden layer for the target model (Henderson et al., 2013; Raina et al., 2007; Collobert et al., 2011; Glorot et al., 2011). This transferred information has proven to be particularly useful when it comes from very large datasets, such as web-scale text corpora, but learning and inference on such datasets is only practical with efficient algorithms. In this work, we propose a fast discriminative RNN model of shift-reduce dependency parsing. We choose a left-to-right shift-reduce dependency parsing architecture to benefit from efficient decoding. It also easily supports incremental interpretation in dialogue systems, or incremental language modeling </context>
<context position="8278" citStr="Henderson et al., 2013" startWordPosition="1293" endWordPosition="1296">from previous discriminative models of shift-reduce dependency parsing in the nature of the discriminative choices that are made and the way these decisions are modeled and learned. Rather than learning to make pruning decisions at each parse action, we learn to choose between sequences of actions that occur in between two shift actions. This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing (Henderson, 2003), and for synchronizing syntactic parsing and semantic role labeling in a joint model (Henderson et al., 2013). We show empirically that making discriminative parsing decisions at the scale of these chunks also provides a good balance between grouping decisions so that more context can be used to make accurate parsing decisions and dividing decisions so that the space of alternatives for each decision can be considered quickly (see Figure 4 below). 143 In line with this pruning strategy, we define a score called the correctness probability for every shift action. This score is trained discriminatively to indicate whether the entire parse prefix is correct or not. This gives us a score function that is</context>
</contexts>
<marker>Henderson, Merlo, Titov, Musillo, 2013</marker>
<rawString>James Henderson, Paola Merlo, Ivan Titov, and Gabriele Musillo. 2013. Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model. Comput. Linguist., 39(4):949–998, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>24--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2318" citStr="Henderson, 2003" startWordPosition="346" endWordPosition="347"> et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The</context>
<context position="6611" citStr="Henderson, 2003" startWordPosition="1024" endWordPosition="1026">ntal Neural Network (INN). INNs are a kind of RNN where the model structure is built incrementally as a function of the values of previous output variables. In our case, the hidden vector used to make the current parser decision is connected to the hidden vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just locality in the derivation sequence (Henderson, 2003; Titov and Henderson, 2007b). As in all deep neural network architectures, this chaining of nonlinear vector computations gives the model a very powerful mechanism to induce complex features from combinations of features in the history, which is difficult to replicate with handcoded features. 1.3 Search-based Discriminative Training We propose a discriminative model because it allows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to co</context>
<context position="8168" citStr="Henderson, 2003" startWordPosition="1277" endWordPosition="1278">ther to prune or not (Huang et al., 2012; Zhang and Clark, 2011). Our proposed model primarily differs from previous discriminative models of shift-reduce dependency parsing in the nature of the discriminative choices that are made and the way these decisions are modeled and learned. Rather than learning to make pruning decisions at each parse action, we learn to choose between sequences of actions that occur in between two shift actions. This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing (Henderson, 2003), and for synchronizing syntactic parsing and semantic role labeling in a joint model (Henderson et al., 2013). We show empirically that making discriminative parsing decisions at the scale of these chunks also provides a good balance between grouping decisions so that more context can be used to make accurate parsing decisions and dividing decisions so that the space of alternatives for each decision can be considered quickly (see Figure 4 below). 143 In line with this pruning strategy, we define a score called the correctness probability for every shift action. This score is trained discrimi</context>
<context position="19607" citStr="Henderson, 2003" startWordPosition="3232" endWordPosition="3233"> discriminate between larger sub-sequences of actions, namely the actions between two Shift actions, which we call chunks. This allows the parser to delay choosing between actions that occur early in a chunk until all the structure associated with that chunk has been built. Second, the model’s score can be used to discriminate between two parses long after they have diverged, making it appropriate for a beam search. We employ a search strategy where we prune at each shift action, but in between shift actions we consider all possible sequences of actions, similarly to the generative parser in (Henderson, 2003). The INN model is discriminatively trained to choose between these chunks of sequences of actions. The most straightforward way to model these chunk decisions would be to use unnormalized scores for the decisions in a chunk and sum these scores to make the decision, as would be done for a structured perceptron or conditional random field. Preliminary experiments applying this approach to our INN parser did not work as well as having local normalization of action decisions. We hypothesize that the main reason for this result is that updating on entire chunks does not provide a sufficiently foc</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 24–31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="20573" citStr="Henderson, 2004" startWordPosition="3391" endWordPosition="3392">ary experiments applying this approach to our INN parser did not work as well as having local normalization of action decisions. We hypothesize that the main reason for this result is that updating on entire chunks does not provide a sufficiently focused training signal. With a locally normalized action score, such as softmax, increasing the score of the correct chunk has the effect of decreasing the score of all the incorrect actions at each individual action decision. This update is an approximation to a discriminative update on all incorrect parses that continue from an incorrect decision (Henderson, 2004). Another possible reason is that local normalization prevents one action’s score from dominating the score of the whole parse, as can happen with high frequency decisions. In general, this problem can not be solved just by using norm regularization on weights. The places in a parse where the generative update and the discriminative update differ substantially are at word predictions, where the generative model considers that all words are possible but a discriminative model already knows what word comes next and so does not need to predict anything. We discriminatively train the INN to choose</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 95–102, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="7592" citStr="Huang et al., 2012" startWordPosition="1179" endWordPosition="1182"> model because it allows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to condition on words that are farther ahead in the string, and thereby avoid hypothesizing parses that are incompatible with those future words. This allows the parser to prune much more aggressively without losing accuracy. Discriminative learning further improves this aggressive pruning, because it can optimize for the discrete choice of whether to prune or not (Huang et al., 2012; Zhang and Clark, 2011). Our proposed model primarily differs from previous discriminative models of shift-reduce dependency parsing in the nature of the discriminative choices that are made and the way these decisions are modeled and learned. Rather than learning to make pruning decisions at each parse action, we learn to choose between sequences of actions that occur in between two shift actions. This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing (Henderson, 2003), and for synchronizing </context>
<context position="13154" citStr="Huang et al., 2012" startWordPosition="2117" endWordPosition="2120"> decision Right-Arcr adds an arc from s to q labeled r. The decision Reduce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a history based approach to model these sequences of parser actions, which decomposes the conditional probability of the parse using the chain rule: P(T|S) = P(D1· · ·Dm|S) Y= t P(Dt|D1···Dt−1, S) where T is the parse tree, D1· · ·Dm is its equivalent sequence of shift-reduce parser actions and S is the input sentence. The probability of Left-Arcr and Right-Arcr include both the probability of the attach</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="18521" citStr="Lafferty et al., 2001" startWordPosition="3054" endWordPosition="3057">ndependence assumptions in that words to the right of wtay are assumed to be independent of Dt. And even for words in the lookahead it can be difficult to learn dependencies with the unstructured lookahead string. The generative model first constructs a structure and then uses word prediction to test how well that matches the next word. If a discriminative model uses normalized estimates for decisions, then once a wrong decision is made there is no way for the estimates to express that this decision has lead to a structure that is incompatible with the current or future lookahead string (see (Lafferty et al., 2001) for more discussion). More generally, any discriminative model that is trained to predict individual actions has this problem. In this section we discuss how to overcome this issue. 3.1 Discriminating Correct Parse Chunks Due to this problem, discriminative parsers typically make irrevocable choices for each individual action in the parse. We propose a method for training a discriminative parser which addresses this problem in two ways. First, we train the model to discriminate between larger sub-sequences of actions, namely the actions between two Shift actions, which we call chunks. This al</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´aˇs Burget</author>
<author>Jan ˇCernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<journal>International Speech Communication Association.</journal>
<booktitle>In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010),</booktitle>
<volume>volume</volume>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, ˇCernock´y, Khudanpur, 2010</marker>
<rawString>Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), volume 2010, pages 1045– 1048. International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<journal>Association for Computational Linguistics, May</journal>
<booktitle>In Hwee Tou Ng</booktitle>
<volume>6</volume>
<pages>49--56</pages>
<editor>and Ellen Riloff, editors,</editor>
<contexts>
<context position="12752" citStr="Nivre et al., 2004" startWordPosition="2048" endWordPosition="2051">parser starts with an empty stack S and all the input words in the queue Q, and terminates when it reaches a configuration with an empty queue Q. We use an arc-eager algorithm, which has 4 actions that all manipulate the word s on top of the stack S and the word q on the front of the queue Q: The decision Left-Arcr adds a dependency arc from q to s labeled r. Word s is then popped from the stack. The decision Right-Arcr adds an arc from s to q labeled r. The decision Reduce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a his</context>
<context position="24936" citStr="Nivre et al., 2004" startWordPosition="4095" endWordPosition="4098">t) = 1 otherwise. For the shift actions, P(Correct|ht) is defined using the sigmoid function: P(Correct|ht) = Q(htWCor) In this way, the hidden representations are trained not only to choose the right action, but also to encode correctness of the partial parses. Figure 1 shows the model schematically. 3.2 Training the Parameters We want to train the correctness probabilities to discriminate correct parses from incorrect parses. The correct parses can be extracted from the training treebank by converting each dependency tree into its equivalent sequence of arc-eager shiftreduce parser actions (Nivre et al., 2004; Titov where 0 is the set of all parameters of the model (namely WHH, Wemb., WHO, and WCor), Tpos is the set of correct parses, and Toneg is the set of incorrect parses which the model 0 scores higher than their corresponding correct parses. The derivative of this objective function is the error signal that the neural network learns to minimize. This error signal is illustrated schematically in Figure 2. We optimize the above objective using stochastic gradient descent. For each parameter update, a positive tree is chosen randomly and a negative tree is built using the above strategy. The res</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 49–56. Association for Computational Linguistics, May 6 - May 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨uls¸en Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>221--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiˇgit, and Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 221–225. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Alexis Battle</author>
<author>Honglak Lee</author>
<author>Benjamin Packer</author>
<author>Andrew Y Ng</author>
</authors>
<title>Self-taught learning: Transfer learning from unlabeled data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning, ICML ’07,</booktitle>
<pages>759--766</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4856" citStr="Raina et al., 2007" startWordPosition="744" endWordPosition="747">s with other deep neural network architectures, RNNs induce complex features automatically by passing induced (hidden) features as input to other induced features in a recursive structure. This is a particular advantage for domain adaptation, multi-task learning, transfer learning, and semi-supervised learning, where hand-crafted feature engineering is often particularly difficult. The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space, which is input to another hidden layer for the target model (Henderson et al., 2013; Raina et al., 2007; Collobert et al., 2011; Glorot et al., 2011). This transferred information has proven to be particularly useful when it comes from very large datasets, such as web-scale text corpora, but learning and inference on such datasets is only practical with efficient algorithms. In this work, we propose a fast discriminative RNN model of shift-reduce dependency parsing. We choose a left-to-right shift-reduce dependency parsing architecture to benefit from efficient decoding. It also easily supports incremental interpretation in dialogue systems, or incremental language modeling for speech recogniti</context>
</contexts>
<marker>Raina, Battle, Lee, Packer, Ng, 2007</marker>
<rawString>Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. 2007. Self-taught learning: Transfer learning from unlabeled data. In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 759–766. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Ng</author>
<author>Chris Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11,</booktitle>
<pages>129--136</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2174" citStr="Socher et al., 2011" startWordPosition="322" endWordPosition="325">e been popular in this area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and Chris Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 129–136. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="2195" citStr="Socher et al., 2013" startWordPosition="326" endWordPosition="329">s area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shi</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>649--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31443" citStr="Surdeanu and Manning, 2010" startWordPosition="5189" endWordPosition="5193">d embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). Therefore, here our objective is not repeating an extensive comparison to the available parsers. Table 1 shows the labeled and unlabeled accuracy of attachments for these models. The MALT and MST parser scores come from (Surdeanu and Manning, 2010), which compared different parsing models using CoNLL 2008 shared task dataset, which is the same as CoNLL 2009 for English syntactic parsing. The results for the generative INN with a large beam were taken from (Henderson and Titov, 2010), which uses an architecture with 80 hidden units. We replicate this setting for the other generative INN results and our discriminative INN results. The parser of (Chen and Manning, 2014) was run with their architecture of 200 hidden units with dropout training (“C&amp;M”). All parsing speeds were computed using the latest downloadable versions of the parsers, o</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 649–652. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11,</booktitle>
<pages>1017--1024</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1907" citStr="Sutskever et al., 2011" startWordPosition="280" endWordPosition="283">uiring minimal feature engineering. 1 Introduction and Motivation There has been significant interest recently in machine learning and natural language processing community in models that learn hidden multilayer representations to solve various tasks. Neural networks have been popular in this area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to ind</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 1017–1024. ACM, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Fast and robust multilingual dependency parsing with a generative latent variable model.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>947--951</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="6638" citStr="Titov and Henderson, 2007" startWordPosition="1027" endWordPosition="1030">rk (INN). INNs are a kind of RNN where the model structure is built incrementally as a function of the values of previous output variables. In our case, the hidden vector used to make the current parser decision is connected to the hidden vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just locality in the derivation sequence (Henderson, 2003; Titov and Henderson, 2007b). As in all deep neural network architectures, this chaining of nonlinear vector computations gives the model a very powerful mechanism to induce complex features from combinations of features in the history, which is difficult to replicate with handcoded features. 1.3 Search-based Discriminative Training We propose a discriminative model because it allows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to condition on words that are f</context>
<context position="12862" citStr="Titov and Henderson, 2007" startWordPosition="2068" endWordPosition="2071">es a configuration with an empty queue Q. We use an arc-eager algorithm, which has 4 actions that all manipulate the word s on top of the stack S and the word q on the front of the queue Q: The decision Left-Arcr adds a dependency arc from q to s labeled r. Word s is then popped from the stack. The decision Right-Arcr adds an arc from s to q labeled r. The decision Reduce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a history based approach to model these sequences of parser actions, which decomposes the conditional probability o</context>
<context position="30596" citStr="Titov and Henderson, 2007" startWordPosition="5053" endWordPosition="5056">, JCJ is much smaller than JFJ (here JCJ=3). This is why caching input feature computations has such a large impact on parser speed. 1For this analysis, we assume that the output computation is negligable compared to the hidden representation computation, because the output computation grows with d while the hidden computation grows with d2. 6 Experimental Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009). Standard splits of training, development and test sets were used. We compare our model to the generative INN model (Titov and Henderson, 2007b), MALT parser, MST parser, and the feed-forward neural network parser of (Chen and Manning, 2014) (“C&amp;M”). All these models and our own models are trained only on the CoNLL 2009 syntactic data; they use no external word embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). T</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007a. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 947–951. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies, IWPT ’07,</booktitle>
<pages>144--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6638" citStr="Titov and Henderson, 2007" startWordPosition="1027" endWordPosition="1030">rk (INN). INNs are a kind of RNN where the model structure is built incrementally as a function of the values of previous output variables. In our case, the hidden vector used to make the current parser decision is connected to the hidden vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just locality in the derivation sequence (Henderson, 2003; Titov and Henderson, 2007b). As in all deep neural network architectures, this chaining of nonlinear vector computations gives the model a very powerful mechanism to induce complex features from combinations of features in the history, which is difficult to replicate with handcoded features. 1.3 Search-based Discriminative Training We propose a discriminative model because it allows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to condition on words that are f</context>
<context position="12862" citStr="Titov and Henderson, 2007" startWordPosition="2068" endWordPosition="2071">es a configuration with an empty queue Q. We use an arc-eager algorithm, which has 4 actions that all manipulate the word s on top of the stack S and the word q on the front of the queue Q: The decision Left-Arcr adds a dependency arc from q to s labeled r. Word s is then popped from the stack. The decision Right-Arcr adds an arc from s to q labeled r. The decision Reduce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a history based approach to model these sequences of parser actions, which decomposes the conditional probability o</context>
<context position="30596" citStr="Titov and Henderson, 2007" startWordPosition="5053" endWordPosition="5056">, JCJ is much smaller than JFJ (here JCJ=3). This is why caching input feature computations has such a large impact on parser speed. 1For this analysis, we assume that the output computation is negligable compared to the hidden representation computation, because the output computation grows with d while the hidden computation grows with d2. 6 Experimental Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009). Standard splits of training, development and test sets were used. We compare our model to the generative INN model (Titov and Henderson, 2007b), MALT parser, MST parser, and the feed-forward neural network parser of (Chen and Manning, 2014) (“C&amp;M”). All these models and our own models are trained only on the CoNLL 2009 syntactic data; they use no external word embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). T</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007b. A latent variable model for generative dependency parsing. In Proceedings of the 10th International Conference on Parsing Technologies, IWPT ’07, pages 144–155. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisheng Yao</author>
<author>Geoffrey Zweig</author>
<author>Mei-Yuh Hwang</author>
<author>Yangyang Shi</author>
<author>Dong Yu</author>
</authors>
<title>Recurrent neural networks for language understanding. In</title>
<date>2013</date>
<booktitle>INTERSPEECH,</booktitle>
<pages>2524--2528</pages>
<contexts>
<context position="1953" citStr="Yao et al., 2013" startWordPosition="287" endWordPosition="290">nd Motivation There has been significant interest recently in machine learning and natural language processing community in models that learn hidden multilayer representations to solve various tasks. Neural networks have been popular in this area as a powerful and yet efficient models. For example, feed forward neural networks were used in language modeling (Bengio et al., 2003; Collobert and Weston, 2008), and recurrent neural networks (RNNs) have yielded state-of-art results in language modeling (Mikolov et al., 2010), language generation (Sutskever et al., 2011) and language understanding (Yao et al., 2013). 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguis</context>
</contexts>
<marker>Yao, Zweig, Hwang, Shi, Yu, 2013</marker>
<rawString>Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu. 2013. Recurrent neural networks for language understanding. In INTERSPEECH, pages 2524–2528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="7616" citStr="Zhang and Clark, 2011" startWordPosition="1183" endWordPosition="1186">lows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to condition on words that are farther ahead in the string, and thereby avoid hypothesizing parses that are incompatible with those future words. This allows the parser to prune much more aggressively without losing accuracy. Discriminative learning further improves this aggressive pruning, because it can optimize for the discrete choice of whether to prune or not (Huang et al., 2012; Zhang and Clark, 2011). Our proposed model primarily differs from previous discriminative models of shift-reduce dependency parsing in the nature of the discriminative choices that are made and the way these decisions are modeled and learned. Rather than learning to make pruning decisions at each parse action, we learn to choose between sequences of actions that occur in between two shift actions. This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing (Henderson, 2003), and for synchronizing syntactic parsing and se</context>
<context position="13178" citStr="Zhang and Clark, 2011" startWordPosition="2121" endWordPosition="2124"> adds an arc from s to q labeled r. The decision Reduce pops s from the stack. The decision Shift shifts q from the queue to the stack. For more details we refer the reader to (Nivre et al., 2004). In this paper we chose the exact definition of the parse actions that are used in (Titov and Henderson, 2007b). At each step of the parse, the parser needs to choose between the set of possible next actions. To train a classifier to choose the best actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006), structured perceptron (Huang et al., 2012; Zhang and Clark, 2011), two-layer neural networks (Chen and Manning, 2014), and Incremental Sigmoid Belief Networks (ISBN) (Titov and 144 Henderson, 2007b), amongst other approaches. We take a history based approach to model these sequences of parser actions, which decomposes the conditional probability of the parse using the chain rule: P(T|S) = P(D1· · ·Dm|S) Y= t P(Dt|D1···Dt−1, S) where T is the parse tree, D1· · ·Dm is its equivalent sequence of shift-reduce parser actions and S is the input sentence. The probability of Left-Arcr and Right-Arcr include both the probability of the attachment decision and the ch</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Comput. Linguist., 37(1):105–151, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>