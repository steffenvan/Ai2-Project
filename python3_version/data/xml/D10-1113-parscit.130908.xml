<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000508">
<title confidence="0.983759">
Measuring Distributional Similarity in Context
</title>
<author confidence="0.995497">
Georgiana Dinu
</author>
<affiliation confidence="0.9977795">
Department of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.502966">
Saarbr¨ucken, Germany
</address>
<email confidence="0.982259">
dinu@coli.uni-sb.de
</email>
<author confidence="0.979341">
Mirella Lapata
</author>
<affiliation confidence="0.910330666666667">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.995088">
mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99794794117647">
The computation of meaning similarity as
operationalized by vector-based models has
found widespread use in many tasks ranging
from the acquisition of synonyms and para-
phrases to word sense disambiguation and tex-
tual entailment. Vector-based models are typ-
ically directed at representing words in isola-
tion and thus best suited for measuring simi-
larity out of context. In his paper we propose
a probabilistic framework for measuring sim-
ilarity in context. Central to our approach is
the intuition that word meaning is represented
as a probability distribution over a set of la-
tent senses and is modulated by context. Ex-
perimental results on lexical substitution and
word similarity show that our algorithm out-
performs previously proposed models.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980712244898">
The computation of meaning similarity as op-
erationalized by vector-based models has found
widespread use in many tasks within natural lan-
guage processing (NLP). These range from the ac-
quisition of synonyms (Grefenstette, 1994; Lin,
1998) and paraphrases (Lin and Pantel, 2001) to
word sense disambiguation (Schuetze, 1998), tex-
tual entailment (Clarke, 2009), and notably informa-
tion retrieval (Salton et al., 1975).
The popularity of vector-based models lies in
their unsupervised nature and ease of computation.
In their simplest incarnation, these models repre-
sent the meaning of each word as a point in a
high-dimensional space, where each component cor-
responds to some co-occurring contextual element
(Landauer and Dumais, 1997; McDonald, 2000;
Lund and Burgess, 1996). The advantage of taking
such a geometric approach is that the similarity of
word meanings can be easily quantified by measur-
ing their distance in the vector space, or the cosine
of the angle between them.
Vector-based models do not explicitly identify the
different senses of words and consequently repre-
sent their meaning invariably (i.e., irrespective of co-
occurring context). Consider for example the adjec-
tive heavy which we may associate with the gen-
eral meaning of “dense” or “massive”. However,
when attested in context, heavy may refer to an over-
weight person (e.g., She is short and heavy but she
has a heart of gold.) or an excessive cannabis user
(e.g., Some heavy users develop a psychological de-
pendence on cannabis.).
Recent work addresses this issue indirectly with
the development of specialized models that repre-
sent word meaning in context (Mitchell and Lap-
ata, 2008; Erk and Pad´o, 2008; Thater et al., 2009).
These methods first extract typical co-occurrence
vectors representing a mixture of senses and then use
vector operations to either obtain contextualized rep-
resentations of a target word (Erk and Pad´o, 2008)
or a representation for a set of words (Mitchell and
Lapata, 2009).
In this paper we propose a probabilistic frame-
work for representing word meaning and measuring
similarity in context. We model the meaning of iso-
lated words as a probability distribution over a set of
latent senses. This distribution reflects the a priori,
out-of-context likelihood of each sense. Because
sense ambiguity is taken into account directly in the
</bodyText>
<page confidence="0.958201">
1162
</page>
<note confidence="0.8168">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999840214285714">
vector construction process, contextualized meaning
can be modeled naturally as a change in the origi-
nal sense distribution. We evaluate our approach on
word similarity (Finkelstein et al., 2002) and lexical
substitution (McCarthy and Navigli, 2007) and show
improvements over competitive baselines.
In the remainder of this paper we give a brief
overview of related work, emphasizing vector-based
approaches that compute word meaning in context
(Section 2). Next, we present our probabilistic
framework and different instantiations thereof (Sec-
tions 3 and 4). Finally, we discuss our experimental
results (Sections 5 and 6) and conclude the paper
with future work.
</bodyText>
<sectionHeader confidence="0.999754" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999764677966102">
Vector composition methods construct representa-
tions that go beyond individual words (e.g., for
phrases or sentences) and thus by default obtain
word meanings in context. Mitchell and Lapata
(2008) investigate several vector composition op-
erations for representing short sentences (consist-
ing of intransitive verbs and their subjects). They
show that models performing point-wise multiplica-
tion of component vectors outperform earlier pro-
posals based on vector addition (Landauer and Du-
mais, 1997; Kintsch, 2001). They argue that multi-
plication approximates the intersection of the mean-
ing of two vectors, whereas addition their union.
Mitchell and Lapata (2009) further show that their
models yield improvements in language modeling.
Erk and Pad´o (2008) employ selectional prefer-
ences to contextualize occurrences of target words.
For example, the meaning of a verb in the presence
of its object is modeled as the multiplication of the
verb’s vector with the vector capturing the inverse
selectional preferences of the object; the latter are
computed as the centroid of the verbs that occur
with this object. Thater et al. (2009) improve on this
model by representing verbs in a second order space,
while the representation for objects remains first or-
der. The meaning of a verb boils down to restricting
its vector to the features active in the argument noun
(i.e., dimensions with value larger than zero).
More recently, Reisinger and Mooney (2010)
present a method that uses clustering to pro-
duce multiple sense-specific vectors for each word.
Specifically, a word’s contexts are clustered to pro-
duce groups of similar context vectors. An aver-
age prototype vector is then computed separately
for each cluster, producing a set of vectors for each
word. These cluster vectors can be used to determine
the semantic similarity of both isolated words and
words in context. In the second case, the distance
between prototypes is weighted by the probability
that the context belongs to the prototype’s cluster.
Erk and Pad´o (2010) propose an exemplar-based
model for capturing word meaning in context. In
contrast to the prototype-based approach, no cluster-
ing takes place, it is assumed that there are as many
senses as there are instances. The meaning of a word
in context is the set of exemplars most similar to it.
Unlike Reisinger and Mooney (2010) and Erk and
Pad´o (2010) our model is probabilistic (we repre-
sent word meaning as a distribution over a set of la-
tent senses), which makes it easy to integrate and
combine with other systems via mixture or product
models. More importantly, our approach is concep-
tually simpler as we use a single vector representa-
tion for isolated words as well as for words in con-
text. A word’s different meanings are simply mod-
eled as changes in its sense distribution. We should
also point out that our approach is not tied to a spe-
cific sense induction method and can be used with
different variants of vector-space models.
</bodyText>
<sectionHeader confidence="0.76891" genericHeader="method">
3 Meaning Representation in Context
</sectionHeader>
<bodyText confidence="0.999921058823529">
In this section we first describe how we represent
the meaning of individual words and then move on
to discuss our model of inducing meaning represen-
tations in context.
Observed Representations Most vector space
models in the literature perform computations on
a co-occurrence matrix where each row repre-
sents a target word, each column a document or
another neighboring word, and each entry their
co-occurrence frequency. The raw counts are typ-
ically mapped into the components of a vector in
some space using for example conditional probabil-
ity, the log-likelihood ratio or tf-idf weighting. Un-
der this representation, the similarity of word mean-
ings can be easily quantified by measuring their dis-
tance in the vector space, the cosine of the angle be-
tween them, or their scalar product.
</bodyText>
<page confidence="0.954184">
1163
</page>
<bodyText confidence="0.999974666666667">
Our model assumes the same type of input data,
namely a co-occurrence matrix, where rows corre-
spond to target words and columns to context fea-
tures (e.g., co-occurring neighbors). Throughout
this paper we will use the notation ti with i : 1..I
to refer to a target word and cj with j : 1..J to refer
to context features. A cell (i, j) in the matrix rep-
resents the frequency of occurrence of target ti with
context feature cj over a corpus.
</bodyText>
<subsectionHeader confidence="0.703635">
Meaning Representation over Latent Senses
</subsectionHeader>
<bodyText confidence="0.961142428571429">
We further assume that the target words ti i : 1...I
found in a corpus share a global set of meanings
or senses Z = {zk|k : 1...K}. And therefore the
meaning of individual target words can be described
as a distribution over this set of senses. More for-
mally, a target ti is represented by the following vec-
tor:
</bodyText>
<equation confidence="0.999316">
v(ti) = (P(z1|ti), ..., P(zK|ti)) (1)
</equation>
<bodyText confidence="0.999923571428571">
where component P(z1|ti) is the probability of
sense z1 given target word ti, component P(z2|ti)
the probability of sense z2 given ti and so on.
The intuition behind such a representation is that
a target word can be described by a set of core mean-
ings and by the frequency with which these are at-
tested. Note that the representation in (1) is not
fixed but parametrized with respect to an input cor-
pus (i.e., it only reflects word usage as attested in
that corpus). The senses z1 ... zK are latent and can
be seen as a means of reducing the dimensionality
of the original co-occurrence matrix.
Analogously, we can represent the meaning of a
target word given a context feature as:
</bodyText>
<equation confidence="0.963162">
v(ti, c5) = (P(z1|ti, c5), ..., P(zK|ti, c5)) (2)
</equation>
<bodyText confidence="0.999815769230769">
Here, target ti is again represented as a distribution
over senses, but is now modulated by a specific con-
text cj which reflects actual word usage. This distri-
bution is more “focused” compared to (1); the con-
text helps disambiguate the meaning of the target
word, and as a result fewer senses will share most
of the probability mass.
In order to create the context-aware representa-
tions defined in (2) we must estimate the proba-
bilities P(zk|ti, cj) which can be factorized as the
product of P(ti, zk), the joint probability of target ti
and latent sense zk, and P(cj|zk, ti), the conditional
probability of context cj given target ti and sense zk:
</bodyText>
<equation confidence="0.996295666666667">
P(ti, zk)P(cj|zk, ti)
P(zk|ti, cj) = (3)
Ek P(ti, zk)P(cj |zk, ti)
</equation>
<bodyText confidence="0.999341333333333">
Problematically, the term P(cj|zk, ti) is difficult to
estimate since it implies learning a total number of
K x I J-dimensional distributions. We will there-
fore make the simplifying assumption that target
words ti and context features cj are conditionally in-
dependent given sense zk:
</bodyText>
<equation confidence="0.998745333333333">
P(zk|ti)P(cj|zk)
P(zk|ti, cj) ^ (4)
Ek P(zk|ti)P(cj |zk)
</equation>
<bodyText confidence="0.997546647058824">
Although not true in general, the assumption is rela-
tively weak. We do not assume that words and con-
text features occur independently of each other, but
only that they are generated independently given an
assigned meaning. A variety of latent variable mod-
els can be used to obtain senses z1 ... zK and es-
timate the distributions P(zk|ti) and P(cj|zk); we
give specific examples in Section 4.
Note that we abuse terminology here, as the
senses our models obtain are not lexicographic
meaning distinctions. Rather, they denote coarse-
grained senses or more generally topics attested in
the document collections our model is trained on.
Furthermore, the senses are not word-specific but
global (i.e., shared across all words) and modulated
either within or out of context probabilistically via
estimating P(zk|ti, cj) and P(zk|ti), respectively.
</bodyText>
<sectionHeader confidence="0.99163" genericHeader="method">
4 Parametrizations
</sectionHeader>
<bodyText confidence="0.999564230769231">
The general framework outlined above can be
parametrized with respect to the input co-occurrence
matrix and the algorithm employed for inducing the
latent structure. Considerable latitude is available
when creating the co-occurrence matrix, especially
when defining its columns, i.e., the linguistic con-
texts a target word is attested with. These con-
texts can be a small number of words surrounding
the target word (Lund and Burgess, 1996; Lowe
and McDonald, 2000), entire paragraphs, documents
(Salton et al., 1975; Landauer and Dumais, 1997)
or even syntactic dependencies (Grefenstette, 1994;
Lin, 1998; Pad´o and Lapata, 2007).
</bodyText>
<page confidence="0.975129">
1164
</page>
<bodyText confidence="0.992299235294118">
Analogously, a number of probabilistic models
can be employed to induce the latent senses. Ex-
amples include Probabilistic Latent Semantic Anal-
ysis (PLSA, Hofmann (2001)), Probabilistic Prin-
cipal Components Analysis (Tipping and Bishop,
1999), non-negative matrix factorization (NMF, Lee
and Seung (2000)), and latent Dirichlet allocation
(LDA, Blei et al. (2003)). We give a more detailed
description of the latter two models as we employ
them in our experiments.
Non-negative Matrix Factorization Non-
negative matrix factorization algorithms approx-
imate a non-negative input matrix V by two
non-negative factors W and H, under a given
loss function. W and H are reduced-dimensional
matrices and their product can be regarded as a
compressed form of the data in V :
</bodyText>
<equation confidence="0.978985">
VI,J Pz� WI,KHK,J (5)
</equation>
<bodyText confidence="0.999719777777778">
where W is a basis vector matrix and H is an en-
coded matrix of the basis vectors in equation (5).
Several loss functions are possible, such as mean
squared error and Kullback-Leibler (KL) diver-
gence. In keeping with the formulation in Sec-
tion 3 we opt for a probabilistic interpretation of
NMF (Gaussier and Goutte, 2005; Ding et al., 2008)
and thus minimize the KL divergence between WH
and V .
</bodyText>
<figure confidence="0.293934">
�min Vi,j
i,j (Vi,j log − Vi,j + WHi,j) (6)
W Hi,j
</figure>
<bodyText confidence="0.979718657894737">
Specifically, we interpret matrix V as
Vij = P(ti, cj), and matrices W and H as P(ti, zk)
and P(cj|zk), respectively. We can also ob-
tain the following more detailed factorization:
P(ti, cj) = Ek P(ti)P(zk|ti)P(cj|zk).
Le WH denote the factors in a NMF decom-
position of an input matrix V and B be a diag-
onal matrix with Bkk = Ej Hkj. B−1H gives a
row-normalized version of H. Similarly, given
matrix WB, we can define a diagonal matrix A,
with Aii = Ek(WB)ik. A−1WB row-normalizes
matrix WB. The factorization WH can now be re-
written as:
WH=AA−1WBB−1H=A(A−1WB)(B−1H)
which allows us to interpret A as P(ti), A−1WB
as P(zk|ti) and B−1H as P(cj|zk). These interpre-
tations are valid since the rows of A−1WB and of
B−1H sum to 1, matrix A is diagonal with trace 1
because elements in WH sum to 1, and all entries
are non-negative.
Latent Dirichlet Allocation LDA (Blei et al.,
2003) is a probabilistic model of text generation.
Each document d is modeled as a distribution
over K topics, which are themselves characterized
by distributions over words. The individual words
in a document are generated by repeatedly sampling
a topic according to the topic distribution and then
sampling a single word from the chosen topic.
More formally, we first draw the mixing propor-
tion over topics θd from a Dirichlet prior with pa-
rameters α. Next, for each of the Nd words wdn in
document d, a topic zdn is first drawn from a multi-
nomial distribution with parameters θdn. The prob-
ability of a word token w taking on value i given
that topic z = j is parametrized using a matrix β
with bij = P (w = i|z = j). Integrating out θd’s
and zdn’s, gives P(D|α, β), the probability of a cor-
pus (or document collection):
</bodyText>
<equation confidence="0.995836666666667">
M Nd
jjfP (θd  |α)jjEP(zdn|θd)P(wdn |zdn,β) dθd
d=1 (n=1zdn
</equation>
<bodyText confidence="0.999940684210527">
The central computational problem in topic
modeling is to obtain the posterior distri-
bution P(θ, z|w, α, β) of the hidden vari-
ables z = (z1, z2, ... , zN). given a docu-
ment w = (w1, w2, ... , wN). Although this
distribution is intractable in general, a variety
of approximate inference algorithms have been
proposed in the literature. We adopt the Gibbs
sampling procedure discussed in Griffiths and
Steyvers (2004). In this model, P(w = i|z = j) is
also a Dirichlet mixture (denoted φ) with symmetric
priors (denoted β).
We use LDA to induce senses of target words
based on context words, and therefore each row ti
in the input matrix transforms into a document. The
frequency of ti occurring with context feature cj is
the number of times word cj is encountered in the
“document” associated with ti. We train the LDA
model on this data to obtain the θ and φ distribu-
</bodyText>
<page confidence="0.965341">
1165
</page>
<bodyText confidence="0.999986">
tions. 0 gives the sense distributions of each tar-
get ti: Oik = P(zk|ti) and 0 the context-word dis-
tribution for each sense zk: Okj = P(cj|zk).
</bodyText>
<sectionHeader confidence="0.99892" genericHeader="method">
5 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.94252568">
In this section we discuss the experiments we per-
formed in order to evaluate our model. We describe
the tasks on which it was applied, the corpora used
for model training and our evaluation methodology.
Tasks The probabilistic model presented in Sec-
tion 3 represents words via a set of induced senses.
We experimented with two types of semantic space
based on NMF and LDA and optimized parameters
for these models on a word similarity task. The
latter involves judging the similarity sim(ti, tz) =
sim(v(ti), v(tz)) of words ti and tz out of context,
where v(ti) and v(tz) are obtained from the output of
NMF or LDA, respectively. In our experiments we
used the data set of Finkelstein et al. (2002). It con-
tains 353 pairs of words and their similarity scores
as perceived by human subjects.
The contextualized representations were next
evaluated on lexical substitution (McCarthy and
Navigli, 2007). The task requires systems to find
appropriate substitutes for target words occurring in
context. Typically, systems are given a set of substi-
tutes, and must produce a ranking such that appro-
priate substitutes are assigned a higher rank com-
pared to non-appropriate ones. We made use of the
SemEval 2007 Lexical Substitution Task benchmark
data set. It contains 200 target words, namely nouns,
verbs, adjectives and adverbs, each of which occurs
in 10 distinct sentential contexts. The total set con-
tains 2,000 sentences. Five human annotators were
asked to provide substitutes for these target words.
Table 1 gives an example of the adjective still and
its substitutes.
Following Erk and Pad´o (2008), we pool together
the total set of substitutes for each target word.
Then, for each instance the model has to produce a
ranking for the total substitute set. We rank the can-
didate substitutes based on the similarity of the con-
textualized target and the out-of-context substitute,
sim(v(ti, cj), v(tz)), where ti is the target word, cj a
context word and t a substitute. Contextualizing
just one of the words brings higher discriminative
power to the model rather than performing compar-
Sentences Substitutes
It is important to apply the calm (5) not-windy (1)
herbicide on a still day, be- windless (1)
cause spray drift can kill
non-target plants.
A movie is a visual docu- motionless (3) unmov-
ment comprised of a series ing (2) fixed (1) sta-
of still images. tionary (1) static (1)
</bodyText>
<tableCaption confidence="0.956214333333333">
Table 1: Lexical substitution data example for the adjec-
tive still; numbers in parentheses indicate the frequency
of the substitute.
</tableCaption>
<bodyText confidence="0.99909578125">
isons with the target and its substitute embedded in
an identical context (see also Thater et al. (2010) for
a similar observation).
Model Training All the models we experimented
with use identical input data, i.e., a bag-of-words
matrix extracted from the GigaWord collection of
news text. Rows in this matrix are target words and
columns are their co-occurring neighbors, within a
symmetric window of size 5. As context words, we
used a vocabulary of the 3,000 most frequent words
in the corpus.1
We implemented the classical NMF factorization
algorithm described in Lee and Seung (2000). The
input matrix was normalized so that all elements
summed to 1. We experimented with four dimen-
sions K: [600 − 1000] with step size 200. We ran
the algorithm for 150 iterations to obtain factors W
and H which we further processes as described in
Section 4 to obtain the desired probability distribu-
tions. Since the only parameter of the NMF model
is the factorization dimension K, we performed two
independent runs with each K value and averaged
their predictions.
The parameters for the LDA model are the num-
ber of topics K and Dirichlet priors α and Q. We ex-
perimented with topics K: [600 − 1400], again with
step size 200. We fixed Q to 0.01 and tested two val-
ues for α: K (Porteous et al., 2008) and K (Griffiths
and Steyvers, 2004). We used Gibbs sampling on
the “document collection” obtained from the input
matrix and estimated the sense distributions as de-
scribed in Section 4. We ran the chains for 1000 iter-
</bodyText>
<footnote confidence="0.989538666666667">
1The GigaWord corpus contains 1.7B words; we scale down
all the counts by a factor of 70 to speed up the computation of
the LDA models. All models use this reduced size input data.
</footnote>
<page confidence="0.98875">
1166
</page>
<bodyText confidence="0.901387">
ations and averaged over five iterations [600 −1000]
at lag 100 (we observed no topic drift).
We measured similarity using the scalar prod-
uct, cosine, and inverse Jensen-Shannon (IJS) diver-
gence (see (7), (8), and (9), respectively):
</bodyText>
<equation confidence="0.999585888888889">
�sp(v, w) =&lt; v, w &gt;= viwi (7)
i
cos(v, w) = �v, w�
||v  |w ||(8)
1
IJS(v, w) = (9)
JS (v, w)
1 1
JS(v,w) = 2KL(v|m) + 2KL(w|m) (10)
</equation>
<bodyText confidence="0.982401209302326">
where m is a shorthand for 1�(v + w) and
KL the Kullback-Leibler divergence, KL(v|w) =
&amp; vilog( viwi ).
Among the above similarity measures, the scalar
product has the most straightforward interpretation
as the probability of two targets sharing a common
meaning (i.e., the sum over all possible meanings).
The scalar product assigns 1 to a pair of identi-
cal vectors if and only if P(zi) = 1 for some i
and P(zj) = 0,bj =� i. Thus, only fully disam-
biguated words receive a score of 1. Beyond similar-
ity, the measure also reflects how “focused” the dis-
tributions in question are, as very ambiguous words
are unlikely to receive high scalar product values.
Given a set of context words, we contextualize the
target using one context word at a time and compute
the overall similarity score by multiplying the indi-
vidual scores.
Baselines Our baseline models for measuring sim-
ilarity out of context are Latent Semantic Analysis
(Landauer and Dumais, 1997) and a simple seman-
tic space without any dimensionality reduction.
For LSA, we computed the UΣV SVD decompo-
sition of the original matrix to rank k = 1000. Any
decomposition of lower rank can be obtained from
this by setting rows and columns to 0. We evaluated
decompositions to ranks K: [200 − 1000], at each
100 step. Similarity computations were performed
in the lower rank approximation matrix UΣV , as
originally proposed in Deerwester et al. (1990), and
in matrix U which maps the words into the concept
space. It is common to compute SVD decomposi-
tions on matrices to which prior weighting schemes
have been applied. We experimented with tf-idf
weighting and line normalization.
Our second baseline, the simple semantic space,
was based on the original input matrix on which
we applied several weighting schemes such as point-
wise mutual information, tf-idf, and line normaliza-
tion. Again, we measured similarity using cosine,
scalar product and inverse JS divergence. In addi-
tion, we also experimented with Lin’s (1998) simi-
larity measure:
</bodyText>
<equation confidence="0.999877333333333">
�iEI(v)nI(w)(vi + wi)
lin(v, w) = (11)
&amp;EI(v) vi + ElEI(w) wi
</equation>
<bodyText confidence="0.994308757575758">
where the values in v and w are point-wise mutual
information, and I(�) gives the indices of positive
values in a vector.
Our baselines for contextualized similarity were
vector addition and vector multiplication which
we performed using the simple semantic space
(Mitchell and Lapata, 2008) and dimensionality
reduced representations obtained from NMF and
LDA. To create a ranking of the candidate substi-
tutes we compose the vector of the target with its
context and compare it with each substitute vector.
Given a set of context words, we contextualize the
target using each context word at a time and multi-
ply the individual scores.
Evaluation Method For the word similarity task
we used correlation analysis to examine the rela-
tionship between the human ratings and their cor-
responding vector-based similarity values. We re-
port Spearman’s p correlations between the simi-
larity values provided by the models and the mean
participant similarity ratings in the Finkelstein et al.
(2002) data set. For the lexical substitution task, we
compare the system ranking with the gold standard
ranking using Kendall’s 7b rank correlation (which is
adjusted for tied ranks). For all contextualized mod-
els we defined the context of a target word as the
words occurring within a symmetric context window
of size 5. We assess differences between models us-
ing stratified shuffling (Yeh, 2000).2
2Given two system outputs, the null hypothesis (i.e., that
the two predictions are indistinguishable) is tested by randomly
mixing the individual instances (in our case sentences) of the
two outputs. We ran a standard number of 10000 iterations.
</bodyText>
<page confidence="0.986634">
1167
</page>
<table confidence="0.98560225">
Model Spearman p
SVS 38.35
LSA 49.43
NMF 52.99
LDA 53.39
LSAMIX 49.76
NMFMIX 51.62
LDAMIX 51.97
</table>
<tableCaption confidence="0.981654833333333">
Table 2: Results on out of context word similarity using
a simple co-occurrence based vector space model (SVS),
latent semantic analysis, non-negative matrix factoriza-
tion and latent Dirichlet allocation as individual models
with the best parameter setting (LSA, NMF, LDA) and as
mixtures (LSAMIX, NMFMIX, LDAMIX).
</tableCaption>
<sectionHeader confidence="0.999764" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99992708">
Word Similarity Our results on word similar-
ity are summarized in Table 2. The simple co-
occurrence based vector space (SVS) performed best
with tf-idf weighting and the cosine similarity mea-
sure. With regard to LSA, we obtained best re-
sults with initial line normalization of the matrix,
K = 600 dimensions, and the scalar product sim-
ilarity measure while performing computations in
matrix U. Both NMF and LDA models are generally
better with a larger number of senses. NMF yields
best performance with K = 1000 dimensions and
the scalar product similarity measure. The best LDA
model also uses the scalar product, has K = 1200
topics, and α set to 50� .
Following Reisinger and Mooney (2010), we also
evaluated mixture models that combine the output
of models with varying parameter settings. For both
NMF and LDA we averaged the similarity scores re-
turned by all runs. For comparison, we also present
an LSA mixture model over the (best) middle in-
terval K values. As can be seen, the LSA model
improves slightly, whereas NMF and LDA perform
worse than their best individual models.3 Overall,
we observe that NMF and LDA yield significantly
(p &lt; 0.01) better correlations than LSA and the sim-
</bodyText>
<footnote confidence="0.941970166666667">
3It is difficult to relate our results to Reisinger and Mooney
(2010), due to differences in the training data and the vector rep-
resentations it gives rise to. As a comparison, a baseline config-
uration with tf-idf weighting and the cosine similarity measure
yields a correlation of 0.38 with our data and 0.49 in Reisinger
and Mooney (2010).
</footnote>
<table confidence="0.99815975">
Model Kendall’s rb
SVS 11.05
Add-SVS 12.74
Add-NMF 12.85
Add-LDA 12.33
Mult-SVS 14.41
Mult-NMF 13.20
Mult-LDA 12.90
Cont-NMF 14.95
Cont-LDA 13.71
Cont-NMFMIX 16.01
Cont-LDAMIX 15.53
</table>
<tableCaption confidence="0.993305375">
Table 3: Results on lexical substitution using a simple
semantic space model (SVS), additive and multiplicative
compositional models with vector representations based
on co-occurrences (Add-SVS, Mult-SVS), NMF (Add-
NMF, Mult-NMF), and LDA (Add-LDA, Mult-LDA) and
contextualized models based on NMF and LDA with the
best parameter setting (Cont-NMF, Cont-LDA) and as
mixtures (Cont-NMFMIX, Cont-LDAMIX).
</tableCaption>
<bodyText confidence="0.99987908">
ple semantic space, both as individual models and as
mixtures.
Lexical Substitution Our results on lexical sub-
stitution are shown in Table 3. As a baseline we
also report the performance of the simple semantic
space that does not use any contextual information.
This model returns the same ranking of the substi-
tute candidates for each instance, based solely on
their similarity with the target word. This is a rel-
atively competitive baseline as observed by Erk and
Pad´o (2008) and Thater et al. (2009).
We report results with contextualized NMF and
LDA as individual models (the best word similar-
ity settings) and as mixtures (as described above).
These are in turn compared against additive and
multiplicative compositional models. We imple-
mented an additive model with pmi weighting and
Lin’s similarity measure which is defined in an ad-
ditive fashion. The multiplicative model uses tf-
idf weighting and cosine similarity, which involves
multiplication of vector components. Other combi-
nations of weighting schemes and similarity mea-
sures delivered significantly lower results. We also
report results for these models when using the NMF
and LDA reduced representations.
</bodyText>
<page confidence="0.974454">
1168
</page>
<table confidence="0.998223166666667">
Model Adv Adj Noun Verb
SVS 22.47 14.38 09.52 7.98
Add-SVS 22.79 14.56 11.59 10.00
Mult-SVS 22.85 16.37 13.59 11.60
Cont-NMFMIX 26.13 17.10 15.16 14.18
Cont-LDAMIX 21.21 16.00 16.31 13.67
</table>
<tableCaption confidence="0.996851833333333">
Table 4: Results on lexical substitution for different parts
of speech with a simple semantic space model (SVS), two
compositional models (Add-SVS, Mult-SVS), and con-
textualized mixture models with NMF and LDA (Cont-
NMFMIX, Cont-LDAMIX), using Kendall’s Tb correlation
coefficient.
</tableCaption>
<bodyText confidence="0.999881264705882">
All models significantly (p &lt; 0.01) outperform
the context agnostic simple semantic space (see
SVS in Table 3). Mixture NMF and LDA mod-
els are significantly better than all variants of com-
positional models (p &lt; 0.01); the individual mod-
els are numerically better, however the difference
is not statistically significant. We also find that the
multiplicative model using a simple semantic space
(Mult-SVS) is the best performing compositional
model, thus corroborating the results of Mitchell and
Lapata (2009). Interestingly, dimensionality compo-
sitional models. This indicates that the better results
we obtain are due to the probabilistic formulation of
our contextualized model as a whole rather than the
use of NMF or LDA. Finally, we observe that the
Cont-NMF model is slightly better than Cont-LDA,
however the difference is not statistically significant.
To allow comparison with previous results re-
ported on this data set, we also used the General-
ized Average Precision (GAP, Kishida (2005)) as an
evaluation measure. GAP takes into account the or-
der of candidates ranked correctly by a hypothetical
system, whereas average precision is only sensitive
to their relative position. The best performing mod-
els are Cont-NMFMIX and Cont-LDAMIX obtaining
a GAP of 42.7% and 42.9%, respectively. Erk and
Pad´o (2010) report a GAP of 38.6% on this data set
with their best model.
Table 4 shows how the models perform across dif-
ferent parts of speech. While verbs and nouns seem
to be most difficult, we observe higher gains from
the use of contextualized models. Cont-LDAMIX
obtains approximately 7% absolute gain for nouns
and Cont-NMFMIX approximately 6% for verbs. All
</bodyText>
<table confidence="0.9976226">
Senses Word Distributions
TRAFFIC (0.18) road, traffic, highway, route, bridge
MUSIC (0.04) music, song, rock, band, dance, play
FAN (0.04) crowd, fan, people, wave, cheer, street
VEHICLE (0.04) car, truck, bus, train, driver, vehicle
</table>
<tableCaption confidence="0.986442666666667">
Table 5: Induced senses of jam and five most likely words
given these senses using an LDA model; sense probabili-
ties are shown in parentheses.
</tableCaption>
<bodyText confidence="0.963500222222222">
contextualized models obtain smaller improvements
for adjectives. For adverbs most models do not im-
prove over the no-context setting, with the exception
Cont-NMFMIX.
Finally, we also qualitatively examined how the
context words influence the sense distributions of
target words using examples from the lexical sub-
stitution dataset and the output of an individual
Cont-LDA model. In many cases, a target word
starts with a distribution spread over a larger number
of senses, while a context word shifts this distribu-
tion to one majority sense. Consider, for instance,
the target noun jam in the following sentence:
(1) With their transcendent, improvisational jams
and Mayan-inspired sense of a higher, meta-
physical purpose, the band’s music delivers a
spiritual sustenance that has earned them a very
devoted core following.
Table 5 shows the out-of-context senses activated
for jam together with the five most likely words as-
sociated with them.4 Sense probabilities are also
shown in parentheses. As can be seen, initially two
traffic-related and two music-related senses are acti-
vated, however with low probabilities. In the pres-
ence of the context word band, we obtain a much
more “focused” distribution, in which the MUSIC
sense has 0.88 probability. The system ranks riff
and gig as the most likely two substitutes for jam.
The gold annotation also lists session as a possible
substitute.
In a large number of cases, the target is only par-
tially disambiguated by a context word and this is
also reflected in the resulting distribution. An ex-
4Sense names are provided by the authors in an attempt to
best describe the clusters (i.e., topics for LDA) to which words
are assigned.
</bodyText>
<page confidence="0.991013">
1169
</page>
<bodyText confidence="0.996805880952381">
ample is the word bug which initially has a distribu-
tion triggering the SOFTWARE (0.09, computer, soft-
ware, microsoft, windows) and DISEASE (0.06, dis-
ease, aids, virus, cause) senses. In the context of
client, bug remains ambiguous between the senses
SECRET-AGENCY (0.34, agent, secret, intelligence,
FBI)) and SOFTWARE (0.29):
(2) We wanted to give our client more than just a
list of bugs and an invoice — we wanted to
provide an audit trail of our work along with
meaningful productivity metrics.
There are also cases where the contextualized dis-
tributions are not correct, especially when senses are
domain specific. An example is the word function
occurring in its mathematical sense with the context
word distribution. However, the senses that are trig-
gered by this pair all relate to the “service” sense of
function. This is a consequence of the newspaper
corpus we use, in which the mathematical sense of
function is rare. We also see several cases where
the target word and one of the context words are as-
signed senses that are locally correct, but invalid in
the larger context. In the following example:
(3) Check the shoulders so it hangs well, stops at
hips or below, and make sure the pants are long
enough.
The pair (check, shoulder) triggers senses IN-
JURY (0.81, injury, left, knee, shoulder) and
BALL-SPORTS (0.10, ball, shot, hit, throw). How-
ever, the sentential context ascribes a meaning that
is neither related to injury nor sports. This suggests
that our models could benefit from more principled
context feature aggregation.
Generally, verbs are not as good context words
as nouns. To give an example, we often encounter
the pair (let, know), used in the common “inform”
meaning. The senses we obtain for this pair, are,
however, rather uninformative general verb classes:
{see, know, think, do} (0.57) and {go, say, do,
can} (0.20). This type of error can be eliminated in
a space where context features are designed to best
reflect the properties of the target words.
</bodyText>
<sectionHeader confidence="0.998265" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999831522727273">
In this paper we have presented a general frame-
work for computing similarity in context. Key in this
framework is the representation of word meaning as
a distribution over a set of global senses where con-
textualized meaning is modeled as a change in this
distribution. The approach is conceptually simple,
the same vector representation is used for isolated
words and words in context without being tied to a
specific sense induction method or type of semantic
space.
We have illustrated two instantiations of this
framework using non-negative matrix factorization
and latent Dirichlet allocation for inducing the la-
tent structure, and shown experimentally that they
outperform previously proposed methods for mea-
suring similarity in context. Furthermore, both of
them benefit from mixing model predictions over a
set of different parameter choices, thus making pa-
rameter tuning redundant.
The directions for future work are many and var-
ied. Conceptually, we have defined our model in an
asymmetric fashion, i.e., by stipulating a difference
between target words and contextual features. How-
ever, in practice, we used vector representations that
do not distinguish the two: target words and con-
textual features are both words. This choice was
made to facilitate comparisons with the popular bag-
of-words vector space models. However, differen-
tiating target from context representations may be
beneficial particularly when the similarity compu-
tations are embedded within specific tasks such as
the acquisition of paraphrases, the recognition of en-
tailment relations, and thesaurus construction. Also
note that our model currently contextualizes target
words with respect to individual contexts. Ideally,
we would like to compute the collective influence of
several context words on the target. We plan to fur-
ther investigate how to select or to better aggregate
the entire set of features extracted from a context.
Acknowledgments The authors acknowledge the
support of the DFG (Dinu; International Re-
search Training Group “Language Technology and
Cognitive Systems”) and EPSRC (Lapata; grant
GR/T04540/01).
</bodyText>
<page confidence="0.989847">
1170
</page>
<sectionHeader confidence="0.983571" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998835384615385">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112–119, Athens, Greece.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41:391–407.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics &amp; Data Analysis, 52(8):3913–3927.
Katrin Erk and Sabastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92–
97, Uppsala, Sweden.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116–131.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th Annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601–602, New York, NY.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228–5235.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Machine Learn-
ing, 41(2):177–196.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173–202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato’s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211–240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In NIPS,
pages 556–562.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):342–360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768–774, Montr´eal, Canada.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd Annual Conference of the Cognitive Sci-
ence Society, pages 675–680, Philadelphia, PA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203–208.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proceedings of SemEval, pages 48–53, Prague, Czech
Republic.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, Ohio.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 430–439, Suntec, Singa-
pore.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent Dirichlet allo-
cation. In Proceeding of the 14th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 569–577, New York, NY.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 109–
117, Los Angeles, California.
G Salton, A Wang, and C Yang. 1975. A vector-space
model for information retrieval. Journal of the Ameri-
can Society for Information Science, 18:613–620.
</reference>
<page confidence="0.870358">
1171
</page>
<reference confidence="0.99984715">
Hinrich Schuetze. 1998. Automatic word sense discrim-
ination. Journal of Computational Linguistics, 24:97–
123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44–47, Suntec, Singapore.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948–957, Uppsala,
Sweden.
Michael E. Tipping and Chris M. Bishop. 1999. Prob-
abilistic principal component analysis. Journal of the
Royal Statistical Society, Series B, 61:611–622.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics,
pages 947–953, Saarbr¨ucken, Germany.
</reference>
<page confidence="0.995362">
1172
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272907">
<title confidence="0.999785">Measuring Distributional Similarity in Context</title>
<author confidence="0.996734">Georgiana</author>
<affiliation confidence="0.781655">Department of Computational Saarland</affiliation>
<address confidence="0.463823">Saarbr¨ucken,</address>
<email confidence="0.987052">dinu@coli.uni-sb.de</email>
<author confidence="0.941101">Mirella</author>
<affiliation confidence="0.974861">School of University of Edinburgh,</affiliation>
<email confidence="0.999255">mlap@inf.ed.ac.uk</email>
<abstract confidence="0.998807888888889">The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="12527" citStr="Blei et al. (2003)" startWordPosition="2003" endWordPosition="2006">words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,J Pz� WI,KHK,J (5) where W is a basis vector matrix and H is an encoded matrix of the basis vectors in equation (5). Several loss functions are possible, such as mean squared error and Kullback-</context>
<context position="14276" citStr="Blei et al., 2003" startWordPosition="2315" endWordPosition="2318">sition of an input matrix V and B be a diagonal matrix with Bkk = Ej Hkj. B−1H gives a row-normalized version of H. Similarly, given matrix WB, we can define a diagonal matrix A, with Aii = Ek(WB)ik. A−1WB row-normalizes matrix WB. The factorization WH can now be rewritten as: WH=AA−1WBB−1H=A(A−1WB)(B−1H) which allows us to interpret A as P(ti), A−1WB as P(zk|ti) and B−1H as P(cj|zk). These interpretations are valid since the rows of A−1WB and of B−1H sum to 1, matrix A is diagonal with trace 1 because elements in WH sum to 1, and all entries are non-negative. Latent Dirichlet Allocation LDA (Blei et al., 2003) is a probabilistic model of text generation. Each document d is modeled as a distribution over K topics, which are themselves characterized by distributions over words. The individual words in a document are generated by repeatedly sampling a topic according to the topic distribution and then sampling a single word from the chosen topic. More formally, we first draw the mixing proportion over topics θd from a Dirichlet prior with parameters α. Next, for each of the Nd words wdn in document d, a topic zdn is first drawn from a multinomial distribution with parameters θdn. The probability of a </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>112--119</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1384" citStr="Clarke, 2009" startWordPosition="198" endWordPosition="199">d meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of th</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="22173" citStr="Deerwester et al. (1990)" startWordPosition="3688" endWordPosition="3691"> score by multiplying the individual scores. Baselines Our baseline models for measuring similarity out of context are Latent Semantic Analysis (Landauer and Dumais, 1997) and a simple semantic space without any dimensionality reduction. For LSA, we computed the UΣV SVD decomposition of the original matrix to rank k = 1000. Any decomposition of lower rank can be obtained from this by setting rows and columns to 0. We evaluated decompositions to ranks K: [200 − 1000], at each 100 step. Similarity computations were performed in the lower rank approximation matrix UΣV , as originally proposed in Deerwester et al. (1990), and in matrix U which maps the words into the concept space. It is common to compute SVD decompositions on matrices to which prior weighting schemes have been applied. We experimented with tf-idf weighting and line normalization. Our second baseline, the simple semantic space, was based on the original input matrix on which we applied several weighting schemes such as pointwise mutual information, tf-idf, and line normalization. Again, we measured similarity using cosine, scalar product and inverse JS divergence. In addition, we also experimented with Lin’s (1998) similarity measure: �iEI(v)</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Thomas, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
</authors>
<title>On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing.</title>
<date>2008</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>52</volume>
<issue>8</issue>
<contexts>
<context position="13292" citStr="Ding et al., 2008" startWordPosition="2135" endWordPosition="2138"> matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,J Pz� WI,KHK,J (5) where W is a basis vector matrix and H is an encoded matrix of the basis vectors in equation (5). Several loss functions are possible, such as mean squared error and Kullback-Leibler (KL) divergence. In keeping with the formulation in Section 3 we opt for a probabilistic interpretation of NMF (Gaussier and Goutte, 2005; Ding et al., 2008) and thus minimize the KL divergence between WH and V . �min Vi,j i,j (Vi,j log − Vi,j + WHi,j) (6) W Hi,j Specifically, we interpret matrix V as Vij = P(ti, cj), and matrices W and H as P(ti, zk) and P(cj|zk), respectively. We can also obtain the following more detailed factorization: P(ti, cj) = Ek P(ti)P(zk|ti)P(cj|zk). Le WH denote the factors in a NMF decomposition of an input matrix V and B be a diagonal matrix with Bkk = Ej Hkj. B−1H gives a row-normalized version of H. Similarly, given matrix WB, we can define a diagonal matrix A, with Aii = Ek(WB)ik. A−1WB row-normalizes matrix WB. Th</context>
</contexts>
<marker>Ding, Li, Peng, 2008</marker>
<rawString>Chris Ding, Tao Li, and Wei Peng. 2008. On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. Computational Statistics &amp; Data Analysis, 52(8):3913–3927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sabastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Honolulu, Hawaii.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sabastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<location>Uppsala,</location>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92– 97, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="3765" citStr="Finkelstein et al., 2002" startWordPosition="568" endWordPosition="571">aning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity is taken into account directly in the 1162 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics vector construction process, contextualized meaning can be modeled naturally as a change in the original sense distribution. We evaluate our approach on word similarity (Finkelstein et al., 2002) and lexical substitution (McCarthy and Navigli, 2007) and show improvements over competitive baselines. In the remainder of this paper we give a brief overview of related work, emphasizing vector-based approaches that compute word meaning in context (Section 2). Next, we present our probabilistic framework and different instantiations thereof (Sections 3 and 4). Finally, we discuss our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentenc</context>
<context position="16891" citStr="Finkelstein et al. (2002)" startWordPosition="2783" endWordPosition="2786">te our model. We describe the tasks on which it was applied, the corpora used for model training and our evaluation methodology. Tasks The probabilistic model presented in Section 3 represents words via a set of induced senses. We experimented with two types of semantic space based on NMF and LDA and optimized parameters for these models on a word similarity task. The latter involves judging the similarity sim(ti, tz) = sim(v(ti), v(tz)) of words ti and tz out of context, where v(ti) and v(tz) are obtained from the output of NMF or LDA, respectively. In our experiments we used the data set of Finkelstein et al. (2002). It contains 353 pairs of words and their similarity scores as perceived by human subjects. The contextualized representations were next evaluated on lexical substitution (McCarthy and Navigli, 2007). The task requires systems to find appropriate substitutes for target words occurring in context. Typically, systems are given a set of substitutes, and must produce a ranking such that appropriate substitutes are assigned a higher rank compared to non-appropriate ones. We made use of the SemEval 2007 Lexical Substitution Task benchmark data set. It contains 200 target words, namely nouns, verbs,</context>
<context position="23815" citStr="Finkelstein et al. (2002)" startWordPosition="3951" endWordPosition="3954">To create a ranking of the candidate substitutes we compose the vector of the target with its context and compare it with each substitute vector. Given a set of context words, we contextualize the target using each context word at a time and multiply the individual scores. Evaluation Method For the word similarity task we used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values. We report Spearman’s p correlations between the similarity values provided by the models and the mean participant similarity ratings in the Finkelstein et al. (2002) data set. For the lexical substitution task, we compare the system ranking with the gold standard ranking using Kendall’s 7b rank correlation (which is adjusted for tied ranks). For all contextualized models we defined the context of a target word as the words occurring within a symmetric context window of size 5. We assess differences between models using stratified shuffling (Yeh, 2000).2 2Given two system outputs, the null hypothesis (i.e., that the two predictions are indistinguishable) is tested by randomly mixing the individual instances (in our case sentences) of the two outputs. We ra</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: the concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
</authors>
<title>Relation between PLSA and NMF and implications.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>601--602</pages>
<location>New York, NY.</location>
<contexts>
<context position="13272" citStr="Gaussier and Goutte, 2005" startWordPosition="2131" endWordPosition="2134">x Factorization Nonnegative matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,J Pz� WI,KHK,J (5) where W is a basis vector matrix and H is an encoded matrix of the basis vectors in equation (5). Several loss functions are possible, such as mean squared error and Kullback-Leibler (KL) divergence. In keeping with the formulation in Section 3 we opt for a probabilistic interpretation of NMF (Gaussier and Goutte, 2005; Ding et al., 2008) and thus minimize the KL divergence between WH and V . �min Vi,j i,j (Vi,j log − Vi,j + WHi,j) (6) W Hi,j Specifically, we interpret matrix V as Vij = P(ti, cj), and matrices W and H as P(ti, zk) and P(cj|zk), respectively. We can also obtain the following more detailed factorization: P(ti, cj) = Ek P(ti)P(zk|ti)P(cj|zk). Le WH denote the factors in a NMF decomposition of an input matrix V and B be a diagonal matrix with Bkk = Ej Hkj. B−1H gives a row-normalized version of H. Similarly, given matrix WB, we can define a diagonal matrix A, with Aii = Ek(WB)ik. A−1WB row-norm</context>
</contexts>
<marker>Gaussier, Goutte, 2005</marker>
<rawString>Eric Gaussier and Cyril Goutte. 2005. Relation between PLSA and NMF and implications. In Proceedings of the 28th Annual international ACM SIGIR conference on Research and development in information retrieval, pages 601–602, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1252" citStr="Grefenstette, 1994" startWordPosition="179" endWordPosition="180"> In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach i</context>
<context position="12122" citStr="Grefenstette, 1994" startWordPosition="1947" endWordPosition="1948">ely. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a n</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="15568" citStr="Griffiths and Steyvers (2004)" startWordPosition="2545" endWordPosition="2548">trized using a matrix β with bij = P (w = i|z = j). Integrating out θd’s and zdn’s, gives P(D|α, β), the probability of a corpus (or document collection): M Nd jjfP (θd |α)jjEP(zdn|θd)P(wdn |zdn,β) dθd d=1 (n=1zdn The central computational problem in topic modeling is to obtain the posterior distribution P(θ, z|w, α, β) of the hidden variables z = (z1, z2, ... , zN). given a document w = (w1, w2, ... , wN). Although this distribution is intractable in general, a variety of approximate inference algorithms have been proposed in the literature. We adopt the Gibbs sampling procedure discussed in Griffiths and Steyvers (2004). In this model, P(w = i|z = j) is also a Dirichlet mixture (denoted φ) with symmetric priors (denoted β). We use LDA to induce senses of target words based on context words, and therefore each row ti in the input matrix transforms into a document. The frequency of ti occurring with context feature cj is the number of times word cj is encountered in the “document” associated with ti. We train the LDA model on this data to obtain the θ and φ distribu1165 tions. 0 gives the sense distributions of each target ti: Oik = P(zk|ti) and 0 the context-word distribution for each sense zk: Okj = P(cj|zk)</context>
<context position="20037" citStr="Griffiths and Steyvers, 2004" startWordPosition="3310" endWordPosition="3313">0 − 1000] with step size 200. We ran the algorithm for 150 iterations to obtain factors W and H which we further processes as described in Section 4 to obtain the desired probability distributions. Since the only parameter of the NMF model is the factorization dimension K, we performed two independent runs with each K value and averaged their predictions. The parameters for the LDA model are the number of topics K and Dirichlet priors α and Q. We experimented with topics K: [600 − 1400], again with step size 200. We fixed Q to 0.01 and tested two values for α: K (Porteous et al., 2008) and K (Griffiths and Steyvers, 2004). We used Gibbs sampling on the “document collection” obtained from the input matrix and estimated the sense distributions as described in Section 4. We ran the chains for 1000 iter1The GigaWord corpus contains 1.7B words; we scale down all the counts by a factor of 70 to speed up the computation of the LDA models. All models use this reduced size input data. 1166 ations and averaged over five iterations [600 −1000] at lag 100 (we observed no topic drift). We measured similarity using the scalar product, cosine, and inverse Jensen-Shannon (IJS) divergence (see (7), (8), and (9), respectively):</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="12333" citStr="Hofmann (2001)" startWordPosition="1979" endWordPosition="1980">available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 41(2):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="4763" citStr="Kintsch, 2001" startWordPosition="716" endWordPosition="717">s our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in context. Mitchell and Lapata (2008) investigate several vector composition operations for representing short sentences (consisting of intransitive verbs and their subjects). They show that models performing point-wise multiplication of component vectors outperform earlier proposals based on vector addition (Landauer and Dumais, 1997; Kintsch, 2001). They argue that multiplication approximates the intersection of the meaning of two vectors, whereas addition their union. Mitchell and Lapata (2009) further show that their models yield improvements in language modeling. Erk and Pad´o (2008) employ selectional preferences to contextualize occurrences of target words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur with this object. T</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments.</title>
<date>2005</date>
<tech>NII Technical Report.</tech>
<contexts>
<context position="29645" citStr="Kishida (2005)" startWordPosition="4874" endWordPosition="4875">tic space (Mult-SVS) is the best performing compositional model, thus corroborating the results of Mitchell and Lapata (2009). Interestingly, dimensionality compositional models. This indicates that the better results we obtain are due to the probabilistic formulation of our contextualized model as a whole rather than the use of NMF or LDA. Finally, we observe that the Cont-NMF model is slightly better than Cont-LDA, however the difference is not statistically significant. To allow comparison with previous results reported on this data set, we also used the Generalized Average Precision (GAP, Kishida (2005)) as an evaluation measure. GAP takes into account the order of candidates ranked correctly by a hypothetical system, whereas average precision is only sensitive to their relative position. The best performing models are Cont-NMFMIX and Cont-LDAMIX obtaining a GAP of 42.7% and 42.9%, respectively. Erk and Pad´o (2010) report a GAP of 38.6% on this data set with their best model. Table 4 shows how the models perform across different parts of speech. While verbs and nouns seem to be most difficult, we observe higher gains from the use of contextualized models. Cont-LDAMIX obtains approximately 7</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments. NII Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1758" citStr="Landauer and Dumais, 1997" startWordPosition="253" endWordPosition="256">n many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them. Vector-based models do not explicitly identify the different senses of words and consequently represent their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overwei</context>
<context position="4747" citStr="Landauer and Dumais, 1997" startWordPosition="711" endWordPosition="715"> and 4). Finally, we discuss our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in context. Mitchell and Lapata (2008) investigate several vector composition operations for representing short sentences (consisting of intransitive verbs and their subjects). They show that models performing point-wise multiplication of component vectors outperform earlier proposals based on vector addition (Landauer and Dumais, 1997; Kintsch, 2001). They argue that multiplication approximates the intersection of the meaning of two vectors, whereas addition their union. Mitchell and Lapata (2009) further show that their models yield improvements in language modeling. Erk and Pad´o (2008) employ selectional preferences to contextualize occurrences of target words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur wit</context>
<context position="12071" citStr="Landauer and Dumais, 1997" startWordPosition="1939" endWordPosition="1942">tically via estimating P(zk|ti, cj) and P(zk|ti), respectively. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegat</context>
<context position="21720" citStr="Landauer and Dumais, 1997" startWordPosition="3610" endWordPosition="3613"> assigns 1 to a pair of identical vectors if and only if P(zi) = 1 for some i and P(zj) = 0,bj =� i. Thus, only fully disambiguated words receive a score of 1. Beyond similarity, the measure also reflects how “focused” the distributions in question are, as very ambiguous words are unlikely to receive high scalar product values. Given a set of context words, we contextualize the target using one context word at a time and compute the overall similarity score by multiplying the individual scores. Baselines Our baseline models for measuring similarity out of context are Latent Semantic Analysis (Landauer and Dumais, 1997) and a simple semantic space without any dimensionality reduction. For LSA, we computed the UΣV SVD decomposition of the original matrix to rank k = 1000. Any decomposition of lower rank can be obtained from this by setting rows and columns to 0. We evaluated decompositions to ranks K: [200 − 1000], at each 100 step. Similarity computations were performed in the lower rank approximation matrix UΣV , as originally proposed in Deerwester et al. (1990), and in matrix U which maps the words into the concept space. It is common to compute SVD decompositions on matrices to which prior weighting sche</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In NIPS,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="12468" citStr="Lee and Seung (2000)" startWordPosition="1994" endWordPosition="1997">rd is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,J Pz� WI,KHK,J (5) where W is a basis vector matrix and H is an encoded matrix of the basis vectors in equation (5). Several loss funct</context>
<context position="19297" citStr="Lee and Seung (2000)" startWordPosition="3173" endWordPosition="3176">e frequency of the substitute. isons with the target and its substitute embedded in an identical context (see also Thater et al. (2010) for a similar observation). Model Training All the models we experimented with use identical input data, i.e., a bag-of-words matrix extracted from the GigaWord collection of news text. Rows in this matrix are target words and columns are their co-occurring neighbors, within a symmetric window of size 5. As context words, we used a vocabulary of the 3,000 most frequent words in the corpus.1 We implemented the classical NMF factorization algorithm described in Lee and Seung (2000). The input matrix was normalized so that all elements summed to 1. We experimented with four dimensions K: [600 − 1000] with step size 200. We ran the algorithm for 150 iterations to obtain factors W and H which we further processes as described in Section 4 to obtain the desired probability distributions. Since the only parameter of the NMF model is the factorization dimension K, we performed two independent runs with each K value and averaged their predictions. The parameters for the LDA model are the number of topics K and Dirichlet priors α and Q. We experimented with topics K: [600 − 140</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1303" citStr="Lin and Pantel, 2001" startWordPosition="185" endWordPosition="188">rk for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easil</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):342–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1264" citStr="Lin, 1998" startWordPosition="181" endWordPosition="182">pose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the s</context>
<context position="12133" citStr="Lin, 1998" startWordPosition="1949" endWordPosition="1950">ons The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a non-negative</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
<author>Scott McDonald</author>
</authors>
<title>The direct route: Mediated priming in semantic space.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>675--680</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11992" citStr="Lowe and McDonald, 2000" startWordPosition="1928" endWordPosition="1931">ed across all words) and modulated either within or out of context probabilistically via estimating P(zk|ti, cj) and P(zk|ti), respectively. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models a</context>
</contexts>
<marker>Lowe, McDonald, 2000</marker>
<rawString>Will Lowe and Scott McDonald. 2000. The direct route: Mediated priming in semantic space. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 675–680, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="1799" citStr="Lund and Burgess, 1996" startWordPosition="259" endWordPosition="262">sing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them. Vector-based models do not explicitly identify the different senses of words and consequently represent their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g., She is short and heavy </context>
<context position="11966" citStr="Lund and Burgess, 1996" startWordPosition="1924" endWordPosition="1927">c but global (i.e., shared across all words) and modulated either within or out of context probabilistically via estimating P(zk|ti, cj) and P(zk|ti), respectively. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval2007 Task 10: English Lexical Substitution Task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3819" citStr="McCarthy and Navigli, 2007" startWordPosition="575" endWordPosition="578">n over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity is taken into account directly in the 1162 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics vector construction process, contextualized meaning can be modeled naturally as a change in the original sense distribution. We evaluate our approach on word similarity (Finkelstein et al., 2002) and lexical substitution (McCarthy and Navigli, 2007) and show improvements over competitive baselines. In the remainder of this paper we give a brief overview of related work, emphasizing vector-based approaches that compute word meaning in context (Section 2). Next, we present our probabilistic framework and different instantiations thereof (Sections 3 and 4). Finally, we discuss our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in contex</context>
<context position="17091" citStr="McCarthy and Navigli, 2007" startWordPosition="2812" endWordPosition="2815"> via a set of induced senses. We experimented with two types of semantic space based on NMF and LDA and optimized parameters for these models on a word similarity task. The latter involves judging the similarity sim(ti, tz) = sim(v(ti), v(tz)) of words ti and tz out of context, where v(ti) and v(tz) are obtained from the output of NMF or LDA, respectively. In our experiments we used the data set of Finkelstein et al. (2002). It contains 353 pairs of words and their similarity scores as perceived by human subjects. The contextualized representations were next evaluated on lexical substitution (McCarthy and Navigli, 2007). The task requires systems to find appropriate substitutes for target words occurring in context. Typically, systems are given a set of substitutes, and must produce a ranking such that appropriate substitutes are assigned a higher rank compared to non-appropriate ones. We made use of the SemEval 2007 Lexical Substitution Task benchmark data set. It contains 200 target words, namely nouns, verbs, adjectives and adverbs, each of which occurs in 10 distinct sentential contexts. The total set contains 2,000 sentences. Five human annotators were asked to provide substitutes for these target words</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval2007 Task 10: English Lexical Substitution Task. In Proceedings of SemEval, pages 48–53, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
</authors>
<title>Environmental Determinants of Lexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1774" citStr="McDonald, 2000" startWordPosition="257" endWordPosition="258"> language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them. Vector-based models do not explicitly identify the different senses of words and consequently represent their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g.</context>
<context position="11992" citStr="McDonald, 2000" startWordPosition="1930" endWordPosition="1931"> all words) and modulated either within or out of context probabilistically via estimating P(zk|ti, cj) and P(zk|ti), respectively. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models a</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>Scott McDonald. 2000. Environmental Determinants of Lexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2685" citStr="Mitchell and Lapata, 2008" startWordPosition="404" endWordPosition="408">ferent senses of words and consequently represent their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g., She is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likeli</context>
<context position="4448" citStr="Mitchell and Lapata (2008)" startWordPosition="670" endWordPosition="673">d show improvements over competitive baselines. In the remainder of this paper we give a brief overview of related work, emphasizing vector-based approaches that compute word meaning in context (Section 2). Next, we present our probabilistic framework and different instantiations thereof (Sections 3 and 4). Finally, we discuss our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in context. Mitchell and Lapata (2008) investigate several vector composition operations for representing short sentences (consisting of intransitive verbs and their subjects). They show that models performing point-wise multiplication of component vectors outperform earlier proposals based on vector addition (Landauer and Dumais, 1997; Kintsch, 2001). They argue that multiplication approximates the intersection of the meaning of two vectors, whereas addition their union. Mitchell and Lapata (2009) further show that their models yield improvements in language modeling. Erk and Pad´o (2008) employ selectional preferences to context</context>
<context position="23119" citStr="Mitchell and Lapata, 2008" startWordPosition="3839" endWordPosition="3842">n which we applied several weighting schemes such as pointwise mutual information, tf-idf, and line normalization. Again, we measured similarity using cosine, scalar product and inverse JS divergence. In addition, we also experimented with Lin’s (1998) similarity measure: �iEI(v)nI(w)(vi + wi) lin(v, w) = (11) &amp;EI(v) vi + ElEI(w) wi where the values in v and w are point-wise mutual information, and I(�) gives the indices of positive values in a vector. Our baselines for contextualized similarity were vector addition and vector multiplication which we performed using the simple semantic space (Mitchell and Lapata, 2008) and dimensionality reduced representations obtained from NMF and LDA. To create a ranking of the candidate substitutes we compose the vector of the target with its context and compare it with each substitute vector. Given a set of context words, we contextualize the target using each context word at a time and multiply the individual scores. Evaluation Method For the word similarity task we used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values. We report Spearman’s p correlations between the similarity values pro</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>430--439</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="3005" citStr="Mitchell and Lapata, 2009" startWordPosition="456" endWordPosition="459"> is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity is taken into account directly in the 1162 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics vector construction process, contex</context>
<context position="4913" citStr="Mitchell and Lapata (2009)" startWordPosition="737" endWordPosition="740"> representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in context. Mitchell and Lapata (2008) investigate several vector composition operations for representing short sentences (consisting of intransitive verbs and their subjects). They show that models performing point-wise multiplication of component vectors outperform earlier proposals based on vector addition (Landauer and Dumais, 1997; Kintsch, 2001). They argue that multiplication approximates the intersection of the meaning of two vectors, whereas addition their union. Mitchell and Lapata (2009) further show that their models yield improvements in language modeling. Erk and Pad´o (2008) employ selectional preferences to contextualize occurrences of target words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur with this object. Thater et al. (2009) improve on this model by representing verbs in a second order space, while the representation for objects remains first order. The</context>
<context position="29156" citStr="Mitchell and Lapata (2009)" startWordPosition="4796" endWordPosition="4799">ontextualized mixture models with NMF and LDA (ContNMFMIX, Cont-LDAMIX), using Kendall’s Tb correlation coefficient. All models significantly (p &lt; 0.01) outperform the context agnostic simple semantic space (see SVS in Table 3). Mixture NMF and LDA models are significantly better than all variants of compositional models (p &lt; 0.01); the individual models are numerically better, however the difference is not statistically significant. We also find that the multiplicative model using a simple semantic space (Mult-SVS) is the best performing compositional model, thus corroborating the results of Mitchell and Lapata (2009). Interestingly, dimensionality compositional models. This indicates that the better results we obtain are due to the probabilistic formulation of our contextualized model as a whole rather than the use of NMF or LDA. Finally, we observe that the Cont-NMF model is slightly better than Cont-LDA, however the difference is not statistically significant. To allow comparison with previous results reported on this data set, we also used the Generalized Average Precision (GAP, Kishida (2005)) as an evaluation measure. GAP takes into account the order of candidates ranked correctly by a hypothetical s</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Porteous</author>
<author>David Newman</author>
<author>Alexander Ihler</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Fast collapsed gibbs sampling for latent Dirichlet allocation.</title>
<date>2008</date>
<booktitle>In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>569--577</pages>
<location>New York, NY.</location>
<contexts>
<context position="20000" citStr="Porteous et al., 2008" startWordPosition="3304" endWordPosition="3307">ed with four dimensions K: [600 − 1000] with step size 200. We ran the algorithm for 150 iterations to obtain factors W and H which we further processes as described in Section 4 to obtain the desired probability distributions. Since the only parameter of the NMF model is the factorization dimension K, we performed two independent runs with each K value and averaged their predictions. The parameters for the LDA model are the number of topics K and Dirichlet priors α and Q. We experimented with topics K: [600 − 1400], again with step size 200. We fixed Q to 0.01 and tested two values for α: K (Porteous et al., 2008) and K (Griffiths and Steyvers, 2004). We used Gibbs sampling on the “document collection” obtained from the input matrix and estimated the sense distributions as described in Section 4. We ran the chains for 1000 iter1The GigaWord corpus contains 1.7B words; we scale down all the counts by a factor of 70 to speed up the computation of the LDA models. All models use this reduced size input data. 1166 ations and averaged over five iterations [600 −1000] at lag 100 (we observed no topic drift). We measured similarity using the scalar product, cosine, and inverse Jensen-Shannon (IJS) divergence (</context>
</contexts>
<marker>Porteous, Newman, Ihler, Asuncion, Smyth, Welling, 2008</marker>
<rawString>Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2008. Fast collapsed gibbs sampling for latent Dirichlet allocation. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569–577, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multiprototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="5703" citStr="Reisinger and Mooney (2010)" startWordPosition="866" endWordPosition="869"> words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur with this object. Thater et al. (2009) improve on this model by representing verbs in a second order space, while the representation for objects remains first order. The meaning of a verb boils down to restricting its vector to the features active in the argument noun (i.e., dimensions with value larger than zero). More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. Specifically, a word’s contexts are clustered to produce groups of similar context vectors. An average prototype vector is then computed separately for each cluster, producing a set of vectors for each word. These cluster vectors can be used to determine the semantic similarity of both isolated words and words in context. In the second case, the distance between prototypes is weighted by the probability that the context belongs to the prototype’s cluster. Erk and Pad´o (2010) propose an exemplar-ba</context>
<context position="25573" citStr="Reisinger and Mooney (2010)" startWordPosition="4238" endWordPosition="4241"> Table 2. The simple cooccurrence based vector space (SVS) performed best with tf-idf weighting and the cosine similarity measure. With regard to LSA, we obtained best results with initial line normalization of the matrix, K = 600 dimensions, and the scalar product similarity measure while performing computations in matrix U. Both NMF and LDA models are generally better with a larger number of senses. NMF yields best performance with K = 1000 dimensions and the scalar product similarity measure. The best LDA model also uses the scalar product, has K = 1200 topics, and α set to 50� . Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. For both NMF and LDA we averaged the similarity scores returned by all runs. For comparison, we also present an LSA mixture model over the (best) middle interval K values. As can be seen, the LSA model improves slightly, whereas NMF and LDA perform worse than their best individual models.3 Overall, we observe that NMF and LDA yield significantly (p &lt; 0.01) better correlations than LSA and the sim3It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the tr</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multiprototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109– 117, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wang</author>
<author>C Yang</author>
</authors>
<title>A vector-space model for information retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>18--613</pages>
<contexts>
<context position="1441" citStr="Salton et al., 1975" startWordPosition="205" endWordPosition="208">tion over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them. Vector-based models do not explicit</context>
<context position="12043" citStr="Salton et al., 1975" startWordPosition="1935" endWordPosition="1938">of context probabilistically via estimating P(zk|ti, cj) and P(zk|ti), respectively. 4 Parametrizations The general framework outlined above can be parametrized with respect to the input co-occurrence matrix and the algorithm employed for inducing the latent structure. Considerable latitude is available when creating the co-occurrence matrix, especially when defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative M</context>
</contexts>
<marker>Salton, Wang, Yang, 1975</marker>
<rawString>G Salton, A Wang, and C Yang. 1975. A vector-space model for information retrieval. Journal of the American Society for Information Science, 18:613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schuetze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>24</volume>
<pages>123</pages>
<contexts>
<context position="1349" citStr="Schuetze, 1998" startWordPosition="193" endWordPosition="194">ur approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in th</context>
</contexts>
<marker>Schuetze, 1998</marker>
<rawString>Hinrich Schuetze. 1998. Automatic word sense discrimination. Journal of Computational Linguistics, 24:97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Georgiana Dinu</author>
<author>Manfred Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2728" citStr="Thater et al., 2009" startWordPosition="413" endWordPosition="416">t their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g., She is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity</context>
<context position="5382" citStr="Thater et al. (2009)" startWordPosition="813" endWordPosition="816">). They argue that multiplication approximates the intersection of the meaning of two vectors, whereas addition their union. Mitchell and Lapata (2009) further show that their models yield improvements in language modeling. Erk and Pad´o (2008) employ selectional preferences to contextualize occurrences of target words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur with this object. Thater et al. (2009) improve on this model by representing verbs in a second order space, while the representation for objects remains first order. The meaning of a verb boils down to restricting its vector to the features active in the argument noun (i.e., dimensions with value larger than zero). More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. Specifically, a word’s contexts are clustered to produce groups of similar context vectors. An average prototype vector is then computed separately for each cluster, producing a set </context>
<context position="27503" citStr="Thater et al. (2009)" startWordPosition="4546" endWordPosition="4549">ased on NMF and LDA with the best parameter setting (Cont-NMF, Cont-LDA) and as mixtures (Cont-NMFMIX, Cont-LDAMIX). ple semantic space, both as individual models and as mixtures. Lexical Substitution Our results on lexical substitution are shown in Table 3. As a baseline we also report the performance of the simple semantic space that does not use any contextual information. This model returns the same ranking of the substitute candidates for each instance, based solely on their similarity with the target word. This is a relatively competitive baseline as observed by Erk and Pad´o (2008) and Thater et al. (2009). We report results with contextualized NMF and LDA as individual models (the best word similarity settings) and as mixtures (as described above). These are in turn compared against additive and multiplicative compositional models. We implemented an additive model with pmi weighting and Lin’s similarity measure which is defined in an additive fashion. The multiplicative model uses tfidf weighting and cosine similarity, which involves multiplication of vector components. Other combinations of weighting schemes and similarity measures delivered significantly lower results. We also report results</context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44–47, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Tipping</author>
<author>Chris M Bishop</author>
</authors>
<title>Probabilistic principal component analysis.</title>
<date>1999</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>61--611</pages>
<contexts>
<context position="12406" citStr="Tipping and Bishop, 1999" startWordPosition="1986" endWordPosition="1989">hen defining its columns, i.e., the linguistic contexts a target word is attested with. These contexts can be a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000), entire paragraphs, documents (Salton et al., 1975; Landauer and Dumais, 1997) or even syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). 1164 Analogously, a number of probabilistic models can be employed to induce the latent senses. Examples include Probabilistic Latent Semantic Analysis (PLSA, Hofmann (2001)), Probabilistic Principal Components Analysis (Tipping and Bishop, 1999), non-negative matrix factorization (NMF, Lee and Seung (2000)), and latent Dirichlet allocation (LDA, Blei et al. (2003)). We give a more detailed description of the latter two models as we employ them in our experiments. Non-negative Matrix Factorization Nonnegative matrix factorization algorithms approximate a non-negative input matrix V by two non-negative factors W and H, under a given loss function. W and H are reduced-dimensional matrices and their product can be regarded as a compressed form of the data in V : VI,J Pz� WI,KHK,J (5) where W is a basis vector matrix and H is an encoded m</context>
</contexts>
<marker>Tipping, Bishop, 1999</marker>
<rawString>Michael E. Tipping and Chris M. Bishop. 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 61:611–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="24207" citStr="Yeh, 2000" startWordPosition="4017" endWordPosition="4018"> their corresponding vector-based similarity values. We report Spearman’s p correlations between the similarity values provided by the models and the mean participant similarity ratings in the Finkelstein et al. (2002) data set. For the lexical substitution task, we compare the system ranking with the gold standard ranking using Kendall’s 7b rank correlation (which is adjusted for tied ranks). For all contextualized models we defined the context of a target word as the words occurring within a symmetric context window of size 5. We assess differences between models using stratified shuffling (Yeh, 2000).2 2Given two system outputs, the null hypothesis (i.e., that the two predictions are indistinguishable) is tested by randomly mixing the individual instances (in our case sentences) of the two outputs. We ran a standard number of 10000 iterations. 1167 Model Spearman p SVS 38.35 LSA 49.43 NMF 52.99 LDA 53.39 LSAMIX 49.76 NMFMIX 51.62 LDAMIX 51.97 Table 2: Results on out of context word similarity using a simple co-occurrence based vector space model (SVS), latent semantic analysis, non-negative matrix factorization and latent Dirichlet allocation as individual models with the best parameter s</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational Linguistics, pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>