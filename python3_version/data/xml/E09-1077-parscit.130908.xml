<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.974583">
Semi-Supervised Polarity Lexicon Induction
</title>
<author confidence="0.969423">
Delip Rao∗
</author>
<affiliation confidence="0.92684">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.883798">
Baltimore, MD
</address>
<email confidence="0.998656">
delip@cs.jhu.edu
</email>
<author confidence="0.834677">
Deepak Ravichandran
</author>
<affiliation confidence="0.788248">
Google Inc.
</affiliation>
<address confidence="0.631463">
1600 Amphitheatre Parkway
Mountain View, CA
</address>
<email confidence="0.998003">
deepakr@google.com
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976392">
We present an extensive study on the prob-
lem of detecting polarity of words. We
consider the polarity of a word to be ei-
ther positive or negative. For example,
words such as good, beautiful, and won-
derful are considered as positive words;
whereas words such as bad, ugly, and sad
are considered negative words. We treat
polarity detection as a semi-supervised la-
bel propagation problem in a graph. In
the graph, each node represents a word
whose polarity is to be determined. Each
weighted edge encodes a relation that ex-
ists between two words. Each node (word)
can have two labels: positive or negative.
We study this framework in two differ-
ent resource availability scenarios using
WordNet and OpenOffice thesaurus when
WordNet is not available. We report our
results on three different languages: En-
glish, French, and Hindi. Our results in-
dicate that label propagation improves sig-
nificantly over the baseline and other semi-
supervised learning methods like Mincuts
and Randomized Mincuts for this task.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.916906">
Opinionated texts are characterized by words or
phrases that communicate positive or negative sen-
timent. Consider the following example of two
movie reviews1 shown in Figure 1. The posi-
tive review is peppered with words such as enjoy-
able, likeable, decent, breathtakingly and the negative
∗Work done as a summer intern at Google Inc.
</bodyText>
<footnote confidence="0.8582835">
1Source: Live Free or Die Hard,
rottentomatoes.com
</footnote>
<figureCaption confidence="0.9728945">
Figure 1: Movie Reviews with positive (left) and
negative (right) sentiment.
</figureCaption>
<bodyText confidence="0.999713434782609">
comment uses words like ear-shattering, humorless,
unbearable. These terms and prior knowledge of
their polarity could be used as features in a su-
pervised classification framework to determine the
sentiment of the opinionated text (E.g., (Esuli and
Sebastiani, 2006)). Thus lexicons indicating po-
larity of such words are indispensable resources
not only in automatic sentiment analysis but also
in other natural language understanding tasks like
textual entailment. This motivation was seen in
the General Enquirer effort by Stone et al. (1966)
and several others who manually construct such
lexicons for the English language.2 While it is
possible to manually build these resources for a
language, the ensuing effort is onerous. This mo-
tivates the need for automatic language-agnostic
methods for building sentiment lexicons. The im-
portance of this problem has warranted several ef-
forts in the past, some of which will be reviewed
here.
We demonstrate the application of graph-based
semi-supervised learning for induction of polar-
ity lexicons. We try several graph-based semi-
</bodyText>
<footnote confidence="0.9873155">
2The General Inquirer tries to classify English words
along several dimensions, including polarity.
</footnote>
<note confidence="0.961831">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 675–682,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998749">
675
</page>
<bodyText confidence="0.999907083333333">
supervised learning methods like Mincuts, Ran-
domized Mincuts, and Label Propagation. In par-
ticular, we define a graph with nodes consisting
of the words or phrases to be classified either as
positive or negative. The edges between the nodes
encode some notion of similarity. In a transduc-
tive fashion, a few of these nodes are labeled us-
ing seed examples and the labels for the remaining
nodes are derived using these seeds. We explore
natural word-graph sources like WordNet and ex-
ploit different relations within WordNet like syn-
onymy and hypernymy. Our method is not just
confined to WordNet; any source listing synonyms
could be used. To demonstrate this, we show
the use of OpenOffice thesaurus – a free resource
available in several languages.3
We begin by discussing some related work in
Section 2 and briefly describe the learning meth-
ods we use, in Section 3. Section 4 details our
evaluation methodology along with detailed ex-
periments for English. In Section 5 we demon-
strate results in French and Hindi, as an example
of how the method could be easily applied to other
languages as well.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99956425">
The literature on sentiment polarity lexicon induc-
tion can be broadly classified into two categories,
those based on corpora and the ones using Word-
Net.
</bodyText>
<subsectionHeader confidence="0.99785">
2.1 Corpora based approaches
</subsectionHeader>
<bodyText confidence="0.999807388888889">
One of the earliest work on learning polarity
of terms was by Hatzivassiloglou and McKeown
(1997) who deduce polarity by exploiting con-
straints on conjoined adjectives in the Wall Street
Journal corpus. For example, the conjunction
“and” links adjectives of the same polarity while
“but” links adjectives of opposite polarity. How-
ever the applicability of this method for other im-
portant classes of sentiment terms like nouns and
verbs is yet to be demonstrated. Further they as-
sume linguistic features specific to English.
Wiebe (2000) uses Lin (1998a) style distribu-
tionally similar adjectives in a cluster-and-label
process to generate sentiment lexicon of adjec-
tives.
In a different work, Riloff et al. (2003) use man-
ually derived pattern templates to extract subjec-
tive nouns by bootstrapping.
</bodyText>
<footnote confidence="0.90187">
3http://www.openoffice.org
</footnote>
<bodyText confidence="0.989177333333333">
Another corpora based method due to Turney
and Littman (2003) tries to measure the semantic
orientation O(t) for a term t by
</bodyText>
<equation confidence="0.9987">
O(t) = � PMI(t, ti) − � PMI(t, tj)
ti∈S+ tj∈S−
</equation>
<bodyText confidence="0.999764333333333">
where S+ and S− are minimal sets of polar terms
that contain prototypical positive and negative
terms respectively, and PMI(t,ti) is the point-
wise mutual information (Lin, 1998b) between
the terms t and ti. While this method is general
enough to be applied to several languages our aim
was to develop methods that exploit more struc-
tured sources like WordNet to leverage benefits
from the rich network structure.
Kaji and Kitsuregawa (2007) outline a method
of building sentiment lexicons for Japanese us-
ing structural cues from HTML documents. Apart
from being very specific to Japanese, excessive de-
pendence on HTML structure makes their method
brittle.
</bodyText>
<subsectionHeader confidence="0.999674">
2.2 WordNet based approaches
</subsectionHeader>
<bodyText confidence="0.998183533333333">
These approaches use lexical relations defined in
WordNet to derive sentiment lexicons. A sim-
ple but high-precision method proposed by Kim
and Hovy (2006) is to add all synonyms of a po-
lar word with the same polarity and its antonyms
with reverse polarity. As demonstrated later, the
method suffers from low recall and is unsuitable in
situations when the seed polar words are too few –
not uncommon in low resource languages.
In line with Turney’s work, Kamps et. al. (2004)
try to determine sentiments of adjectives in Word-
Net by measuring relative distance of the term
from exemplars, such as “good” and “bad”. The
polarity orientation of a term t is measured as fol-
lows
</bodyText>
<equation confidence="0.968678333333333">
d(t, good) − d(t, bad)
O(t) =
d(good, bad)
</equation>
<bodyText confidence="0.999898">
where d(.) is a WordNet based relatedness mea-
sure (Pedersen et al., 2004). Again they report re-
sults for adjectives alone.
Another relevant example is the recent work by
Mihalcea et. al. (2007) on multilingual sentiment
analysis using cross-lingual projections. This is
achieved by using bridge resources like dictionar-
ies and parallel corpora to build sentence subjec-
tivity classifiers for the target language (Roma-
nian). An interesting result from their work is that
</bodyText>
<page confidence="0.996102">
676
</page>
<bodyText confidence="0.998683666666667">
only a small fraction of the lexicon entries pre-
serve their polarities under translation.
The primary contributions of this paper are :
</bodyText>
<listItem confidence="0.949561083333333">
• An application of graph-based semi-
supervised learning methods for inducing
sentiment lexicons from WordNet and other
thesauri. The label propagation method
naturally allows combining several relations
from WordNet.
• Our approach works on all classes of words
and not just adjectives
• Though we report results for English, Hindi,
and French, our methods can be easily repli-
cated for other languages where WordNet is
available.4 In the absence of WordNet, any
</listItem>
<bodyText confidence="0.8533625">
thesaurus listing synonyms could be used.
We present one such result using the OpenOf-
fice thesaurus – a freely available multilin-
gual resource scarcely used in NLP literature.
</bodyText>
<sectionHeader confidence="0.96669" genericHeader="method">
3 Graph based semi-supervised learning
</sectionHeader>
<bodyText confidence="0.999946866666667">
Most natural language data has some structure that
could be exploited even in the absence of fully an-
notated data. For instance, documents are simi-
lar in the terms they contain, words could be syn-
onyms of each other, and so on. Such informa-
tion can be readily encoded as a graph where the
presence of an edge between two nodes would in-
dicate a relationship between the two nodes and,
optionally, the weight on the edge could encode
strength of the relationship. This additional infor-
mation aids learning when very few annotated ex-
amples are present. We review three well known
graph based semi-supervised learning methods –
mincuts, randomized mincuts, and label propaga-
tion – that we use in induction of polarity lexicons.
</bodyText>
<subsectionHeader confidence="0.996487">
3.1 Mincuts
</subsectionHeader>
<bodyText confidence="0.999521555555556">
A mincut of a weighted graph G(V, E) is a par-
titioning the vertices V into V1 and V2 such that
sum of the edge weights of all edges between V1
and V2 is minimal (Figure 2).
Mincuts for semi-supervised learning proposed
by Blum and Chawla (2001) tries to classify data-
points by partitioning the similarity graph such
that it minimizes the number of similar points be-
ing labeled differently. Mincuts have been used
</bodyText>
<footnote confidence="0.9812645">
4As of this writing, WordNet is available for more than 40
world languages (http://www.globalwordnet.org)
</footnote>
<figureCaption confidence="0.9713785">
Figure 2: Semi-supervised classification using
mincuts
</figureCaption>
<bodyText confidence="0.8770865">
in semi-supervised learning for various tasks, in-
cluding document level sentiment analysis (Pang
and Lee, 2004). We explore the use of mincuts for
the task of sentiment lexicon learning.
</bodyText>
<subsectionHeader confidence="0.99704">
3.2 Randomized Mincuts
</subsectionHeader>
<bodyText confidence="0.99995975">
An improvement to the basic mincut algorithm
was proposed by Blum et. al. (2004). The deter-
ministic mincut algorithm, solved using max-flow,
produces only one of the several possible mincuts.
Some of these cuts could be skewed thereby nega-
tively effecting the results. As an extreme example
consider the graph in Figure 3a. Let the nodes with
degree one be labeled as positive and negative re-
spectively, and for the purpose of illustration let
all edges be of the same weight. The graph in Fig-
ure 3a. can be partitioned in four equal cost cuts –
two of which are shown in (b) and (c). The min-
</bodyText>
<figureCaption confidence="0.991057">
Figure 3: Problem with mincuts
</figureCaption>
<bodyText confidence="0.999846625">
cut algorithm, depending on the implementation,
will return only one of the extreme cuts (as in (b))
while the desired classification might be as shown
in Figure 3c.
The randomized mincut approach tries to ad-
dress this problem by randomly perturbing the ad-
jacency matrix by adding random noise.5 Mincut
is then performed on this perturbed graph. This is
</bodyText>
<footnote confidence="0.97689">
5We use a Gaussian noise N(0, 1).
</footnote>
<page confidence="0.994977">
677
</page>
<bodyText confidence="0.999946666666667">
repeated several times and unbalanced partitions
are discarded. Finally the remaining partitions are
used to deduce the final classification by majority
voting. In the unlikely event of the voting result-
ing in a tie, we refrain from making a decision thus
favoring precision over recall.
</bodyText>
<subsectionHeader confidence="0.998791">
3.3 Label propagation
</subsectionHeader>
<bodyText confidence="0.997840333333333">
Another semi-supervised learning method we use
is label propagation by Zhu and Ghahramani
(2002). The label propagation algorithm is a trans-
ductive learning framework which uses a few ex-
amples, or seeds, to label a large number of un-
labeled examples. In addition to the seed exam-
ples, the algorithm also uses a relation between the
examples. This relation should have two require-
ments:
</bodyText>
<listItem confidence="0.995759">
1. It should be transitive.
2. It should encode some notion of relatedness
between the examples.
</listItem>
<bodyText confidence="0.999964642857143">
To name a few, examples of such relations in-
clude, synonymy, hypernymy, and similarity in
some metric space. This relation between the ex-
amples can be easily encoded as a graph. Thus ev-
ery node in the graph is an example and the edge
represents the relation. Also associated with each
node, is a probability distribution over the labels
for the node. For the seed nodes, this distribution
is known and kept fixed. The aim is to derive the
distributions for the remaining nodes.
Consider a graph G(V, E, W) with vertices V ,
edges E, and an n x n edge weight matrix W =
[wij], where n = |V |. The label propagation algo-
rithm minimizes a quadratic energy function
</bodyText>
<equation confidence="0.9688035">
£ =2 wij(yi − yj)2
(i, j) E E
</equation>
<bodyText confidence="0.8668142">
where yi and yj are the labels assigned to the
nodes i and j respectively.6 Thus, to derive the
labels at yi, we set ∂£ = 0 to obtain the follow-
∂yi
ing update equation
</bodyText>
<equation confidence="0.9582005">
� wijyj
(i,j)∈E
� wij
(i,j)∈E
</equation>
<bodyText confidence="0.938671">
In practice, we use the following iterative algo-
rithm as noted by Zhu and Ghahramani (2002). A
</bodyText>
<footnote confidence="0.893991">
6For binary classification yk E {−1, +1}.
</footnote>
<bodyText confidence="0.656642">
n x n stochastic transition matrix T is derived by
row-normalizing W as follows:
</bodyText>
<equation confidence="0.9961985">
Tij = P(j —* i) = En wij
k=1 wkj
</equation>
<bodyText confidence="0.982167">
where Tij can be viewed as the transition probabil-
ity from node j to node i. The algorithm proceeds
as follows:
</bodyText>
<listItem confidence="0.9996776">
1. Assign a n x C matrix Y with the initial as-
signment of labels, where C is the number of
classes.
2. Propagate labels for all nodes by computing
Y=TY
3. Row-normalize Y such that each row adds up
to one.
4. Clamp the seed examples in Y to their origi-
nal values
5. Repeat 2-5 until Y converges.
</listItem>
<bodyText confidence="0.999975625">
There are several points to be noted. First, we add
a special label “DEFAULT” to existing set of la-
bels and set P(DEFAULT  |node = u) = 1 for all
unlabeled nodes u. For all the seed nodes s with
class label L we define P(L  |node = s) = 1. This
ensures nodes that cannot be labeled at all7 will re-
tain P(DEFAULT) = 1 thereby leading to a quick
convergence. Second, the algorithm produces a
probability distribution over the labels for all un-
labeled points. This makes this method specially
suitable for classifier combination approaches. For
this paper, we simply select the most likely label
as the predicted label for the point. Third, the al-
gorithm eventually converges. For details on the
proof for convergence we refer the reader to Zhu
and Ghahramani (2002).
</bodyText>
<sectionHeader confidence="0.992413" genericHeader="method">
4 Evaluation and Experiments
</sectionHeader>
<bodyText confidence="0.999986125">
We use the General Inquirer (GI)8 data for eval-
uation. General Inquirer is lexicon of English
words hand-labeled with categorical information
along several dimensions. One such dimension is
called valence, with 1915 words labeled “Positiv”
(sic) and 2291 words labeled “Negativ” for words
with positive and negative sentiments respectively.
Since we want to evaluate the performance of the
</bodyText>
<footnote confidence="0.999398">
7As an example of such a situation, consider a discon-
nected component of unlabeled nodes with no seed in it.
8http://www.wjh.harvard.edu/∼inquirer/
</footnote>
<equation confidence="0.923661">
yi =
</equation>
<page confidence="0.985203">
678
</page>
<bodyText confidence="0.95077725">
algorithms alone and not the recall issues in us-
ing WordNet, we only consider words from GI that
also occur in WordNet. This leaves us the distri-
bution of words as enumerated in Table 1.
</bodyText>
<table confidence="0.951785">
PoS type No. of Positives No. of Negatives
Nouns 517 579
Verbs 319 562
Adjectives 547 438
</table>
<tableCaption confidence="0.913546">
Table 1: English evaluation data from General In-
quirer
</tableCaption>
<bodyText confidence="0.971012125">
All experiments reported in Sections 4.1 to 4.5
use the data described above with a 50-50 split
so that the first half is used as seeds and the sec-
ond half is used for test. Note that all the exper-
iments described below did not involve any pa-
rameter tuning thus obviating the need for a sepa-
rate development test set. The effect of number of
seeds on learning is described in Section 4.6.
</bodyText>
<subsectionHeader confidence="0.998138">
4.1 Kim-Hovy method and improvements
</subsectionHeader>
<bodyText confidence="0.999978071428571">
Kim and Hovy (2006) enrich their sentiment lexi-
con from WordNet as follows. Synonyms of a pos-
itive word are positive while antonyms are treated
as negative. This basic version suffers from a very
poor recall as shown in the Figure 4 for adjectives
(see iteration 1). The recall can be improved for a
slight trade-off in precision if we re-run the above
algorithm on the output produced at the previous
level. This could be repeated iteratively until there
is no noticeable change in precision/recall. We
consider this as the best possible F1-score pro-
duced by the Kim-Hovy method. The classwise
F1 for this method is shown in Table 2. We use
these scores as our baseline.
</bodyText>
<figureCaption confidence="0.994152">
Figure 4: Kim-Hovy method
</figureCaption>
<table confidence="0.99955725">
PoS type P R F1
Nouns 92.59 21.43 34.80
Verbs 87.89 38.31 53.36
Adjectives 92.95 31.71 47.28
</table>
<tableCaption confidence="0.9807995">
Table 2: Precision/Recall/F1-scores for Kim-
Hovy method
</tableCaption>
<subsectionHeader confidence="0.999207">
4.2 Using prototypes
</subsectionHeader>
<bodyText confidence="0.978255076923077">
We now consider measuring semantic orientation
from WordNet using prototypical examples such
as “good” and “bad” similar to Kamps et al.
(2004). Kamps et. al., report results only for
adjectives though their method could be used for
other part-of-speech types. The results for us-
ing prototypes are listed in Table 3. Note that
the seed data was fully unused except for the ex-
amples “good” and “bad”. We still test on the
same test data as earlier for comparing results.
Also note that the recall need not be 100 in this
case as we refrain from making a decision when
d(t, good) = d(t, bad).
</bodyText>
<table confidence="0.997347">
PoS type P R F1
Nouns 48.03 99.82 64.86
Verbs 58.12 100.00 73.51
Adjectives 57.35 99.59 72.78
</table>
<tableCaption confidence="0.9691685">
Table 3: Precision/Recall/F1-scores for prototype
method
</tableCaption>
<subsectionHeader confidence="0.999664">
4.3 Using mincuts and randomized mincuts
</subsectionHeader>
<bodyText confidence="0.999874444444444">
We now report results for mincuts and random-
ized mincuts algorithm using the WordNet syn-
onym graph. As seen in Table 4, we only observed
a marginal improvement (for verbs) over mincuts
by using randomized mincuts.
But the overall improvement of using graph-
based semi-supervised learning methods over the
Kim-Hovy and Prototype methods is quite signifi-
cant.
</bodyText>
<subsectionHeader confidence="0.999574">
4.4 Using label propagation
</subsectionHeader>
<bodyText confidence="0.99986425">
We extract the synonym graph from WordNet with
an edge between two nodes being defined iff one
is a synonym of the other. When label propaga-
tion is performed on this graph results in Table
5 are observed. The results presented in Tables
2-5 need deeper inspection. The iterated Kim-
Hovy method suffers from poor recall. However
both mincut methods and the prototype method by
</bodyText>
<page confidence="0.997622">
679
</page>
<table confidence="0.9999106">
P R F1
Nouns
Mincut 68.25 100.00 81.13
RandMincut 68.32 99.09 80.08
Verbs
Mincut 72.34 100.00 83.95
RandMincut 73.06 99.02 84.19
Adjectives
Mincut 73.78 100.00 84.91
RandMincut 73.58 100.00 84.78
</table>
<tableCaption confidence="0.9746475">
Table 4: Precision/Recall/F1-scores using mincuts
and randomized mincuts
</tableCaption>
<table confidence="0.9998705">
PoS type P R F1
Nouns 82.55 58.58 58.53
Verbs 81.00 85.94 83.40
Adjectives 84.76 64.02 72.95
</table>
<tableCaption confidence="0.8187435">
Table 5: Precision/Recall/F1-scores for Label Pro-
pogation
</tableCaption>
<bodyText confidence="0.996112142857143">
Kamps et. al., have high recall as they end up
classifying every node as either positive or nega-
tive. Note that the recall for randomized mincut
is not 100 as we do not make a classification de-
cision when there is a tie in majority voting (refer
Section 3.2). Observe that the label propagation
method performs significantly better than previ-
ous graph based methods in precision. The rea-
son for lower recall is attributed to the lack of con-
nectivity between plausibly related nodes, thereby
not facilitating the “spread” of labels from the la-
beled seed nodes to the unlabeled nodes. We ad-
dress this problem by adding additional edges to
the synonym graph in the next section.
</bodyText>
<subsectionHeader confidence="0.997986">
4.5 Incorporating hypernyms
</subsectionHeader>
<bodyText confidence="0.993225333333333">
The main reason for low recall in label propaga-
tion is that the WordNet synonym graph is highly
disconnected. Even nodes which are logically re-
lated have paths missing between them. For exam-
ple the positive nouns compliment and laud belong
to different synonym subgraphs without a path
between them. But incorporating the hypernym
edges the two are connected by the noun praise.
So, we incorporated hypernyms of every node to
improve connectivity. Performing label propaga-
tion on this combined graph gives much better re-
sults (Table 6) with much higher recall and even
slightly better precision. In Table 6., we do not
report results for adjectives as WordNet does not
define hypernyms for adjectives. A natural ques-
</bodyText>
<table confidence="0.9992145">
PoS type P R F1
Nouns 83.88 99.64 91.08
Verbs 85.49 100.00 92.18
Adjectives N/A N/A N/A
</table>
<tableCaption confidence="0.999214">
Table 6: Effect of adding hypernyms
</tableCaption>
<bodyText confidence="0.9952015">
tion to ask is if we can use other WordNet relations
too. We will defer this until section 6.
</bodyText>
<subsectionHeader confidence="0.999493">
4.6 Effect of number of seeds
</subsectionHeader>
<bodyText confidence="0.999991285714286">
The results reported in Sections 4.1 to 4.5 fixed
the number of seeds. We now investigate the per-
formance of the various methods on the number
of seeds used. In particular, we are interested in
performance under conditions when the number of
seeds are few – which is the motivation for using
semi-supervised learning in the first place. Fig-
ure 5 presents our results for English. Observe that
Label Propagation performs much better than our
baseline even when the number of seeds is as low
as ten. Thus label propagation is especially suited
when annotation data is extremely sparse.
One reason for mincuts performing badly with
few seeds is because they generate degenrate cuts.
</bodyText>
<sectionHeader confidence="0.925707" genericHeader="method">
5 Adapting to other languages
</sectionHeader>
<bodyText confidence="0.9991298">
In order to demonstrate the ease of adaptability of
our method for other languages, we used the Hindi
WordNet9 to derive the adjective synonym graph.
We selected 489 adjectives at random from a list
of 10656 adjectives and this list was annotated by
two native speakers of the language. The anno-
tated list was then split 50-50 into seed and test
sets. Label propagation was performed using the
seed list and evaluated on the test list. The results
are listed in Table 7.
</bodyText>
<equation confidence="0.423571">
Hindi P R F1
90.99 95.10 93.00
</equation>
<tableCaption confidence="0.99395">
Table 7: Evaluation on Hindi dataset
</tableCaption>
<bodyText confidence="0.999209">
WordNet might not be freely available for all
languages or may not exist. In such cases build-
ing graph from an existing thesaurus might also
suffice. As an example, we consider French. Al-
though the French WordNet is available10, we
</bodyText>
<footnote confidence="0.958157333333333">
9http://www.cfiilt.iitb.ac.in/wordnet/webhwn/
10http://www.illc.uva.nl/EuroWordNet/consortium-
ewn.html
</footnote>
<page confidence="0.991793">
680
</page>
<figureCaption confidence="0.800896">
Figure 5: Effect of number of seeds on the F-score for Nouns, Verbs, and Adjectives. The X-axis is
number of seeds and the Y-axis is the F-score.
</figureCaption>
<bodyText confidence="0.9998449">
found the cost prohibitive to obtain it. Observe
that if we are using only the synonymy relation in
WordNet then any thesaurus can be used instead.
To demonstrate this, we consider the OpenOffice
thesaurus for French, that is freely available. The
synonym graph of French adjectives has 9707 ver-
tices and 1.6M edges. We manually annotated a
list of 316 adjectives and derived seed and test sets
using a 50-50 split. The results of label propaga-
tion on such a graph is shown in Table 8.
</bodyText>
<table confidence="0.7052705">
French P R F1
73.65 93.67 82.46
</table>
<tableCaption confidence="0.997372">
Table 8: Evaluation on French dataset
</tableCaption>
<bodyText confidence="0.9010529">
The reason for better results in Hindi compared
to French can be attributed to (1) higher inter-
annotator agreement (κ = 0.7) in Hindi compared
that in French (κ = 0.55).11 (2) The Hindi ex-
periment, like English, used WordNet while the
French experiment was performed on graphs de-
rived from the OpenOffice thesaurus due lack of
freely available French WordNet.
11We do not have κ scores for English dataset derived from
the Harvard Inquirer project.
</bodyText>
<sectionHeader confidence="0.996748" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999951166666667">
This paper demonstrated the utility of graph-based
semi-supervised learning framework for building
sentiment lexicons in a variety of resource avail-
ability situations. We explored how the struc-
ture of WordNet could be leveraged to derive
polarity lexicons. The paper combines, for the
first time, relationships like synonymy and hyper-
nymy to improve label propagation results. All
of our methods are independent of language as
shown in the French and Hindi cases. We demon-
strated applicability of our approach on alterna-
tive thesaurus-derived graphs when WordNet is
not freely available, as in the case of French.
Although our current work uses WordNet and
other thesauri, in resource poor situations when
only monolingual raw text is available we can per-
form label propagation on nearest neighbor graphs
derived directly from raw text using distributional
similarity methods. This is work in progress.
We are also currently working on the possibil-
ity of including WordNet relations other than syn-
onymy and hypernymy. One relation that is in-
teresting and useful is antonymy. Antonym edges
cannot be added in a straight-forward way to the
</bodyText>
<page confidence="0.996096">
681
</page>
<bodyText confidence="0.999751333333333">
graph for label propagation as antonymy encodes
negative similarity (or dissimilarity) and the dis-
similarity relation is not transitive.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854730769231">
[Blum and Chawla2001] Avrim Blum and Shuchi
Chawla. 2001. Learning from labeled and un-
labeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages
19–26.
[Blum et al.2004] Blum, Lafferty, Rwebangira, and
Reddy. 2004. Semi-supervised learning using ran-
domized mincuts. In Proceedings of the ICML.
[Esuli and Sebastiani2006] Andrea Esuli and Fabrizio
Sebastiani. 2006. Determining term subjectivity
and term orientation for opinion mining. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 193–200.
[Hatzivassiloglou and McKeown1997] Vasileios Hatzi-
vassiloglou and Kathleen McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the ACL, pages 174–181.
[Kaji and Kitsuregawa2007] Nobuhiro Kaji and Masaru
Kitsuregawa. 2007. Building lexicon for sentiment
analysis from massive collection of HTML docu-
ments. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075–1083.
[Kamps et al.2004] Jaap Kamps, Maarten Marx, R. ort.
Mokken, and Maarten de Rijke. 2004. Using
WordNet to measure semantic orientation of adjec-
tives. In Proceedings ofLREC-04, 4th International
Conference on Language Resources and Evaluation,
volume IV.
[Kim and Hovy2006] Soo-Min Kim and Eduard H.
Hovy. 2006. Identifying and analyzing judgment
opinions. In Proceedings of the HLT-NAACL.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval
and clustering of similar words. In Proceedings of
COLING, pages 768–774.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings
of the 15th International Conference in Machine
Learning, pages 296–304.
[Mihalcea et al.2007] Rada Mihalcea, Carmen Banea,
and Janyce Wiebe. 2007. Learning multilingual
subjective language via cross-lingual projections. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 976–
983.
[Pang and Lee2004] Bo Pang and Lillian Lee. 2004.
A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of the ACL, pages 271–278.
[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Word-
net::similarity - measuring the relatedness of con-
cepts. In Proceeding of the HLT-NAACL.
[Riloffet al.2003] Ellen Riloff, Janyce Wiebe, and
Theresa Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In Proceed-
ings of the 7th Conference on Natural Language
Learning, pages 25–32.
[Stone et al.1966] Philip J. Stone, Dexter C. Dunphy,
Marshall S. Smith, and Daniel M. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. MIT Press.
[Turney and Littman2003] Peter D. Turney and
Michael L. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from
association. ACM Transactions on Information
Systems, 21(4):315–346.
[Wiebe2000] Janyce M. Wiebe. 2000. Learning sub-
jective adjectives from corpora. In Proceedings of
the 2000 National Conference on Artificial Intelli-
gence. AAAI.
[Zhu and Ghahramani2002] Xiaojin Zhu and Zoubin
Ghahramani. 2002. Learning from labeled and un-
labeled data with label propagation. Technical Re-
port CMU-CALD-02-107, Carnegie Mellon Univer-
sity.
</reference>
<page confidence="0.997959">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844068">
<title confidence="0.926864">Semi-Supervised Polarity Lexicon Induction</title>
<affiliation confidence="0.996298">Department of Computer Science Johns Hopkins University</affiliation>
<address confidence="0.990988">Baltimore, MD</address>
<email confidence="0.999559">delip@cs.jhu.edu</email>
<author confidence="0.970736">Deepak Ravichandran</author>
<affiliation confidence="0.999395">Google Inc.</affiliation>
<address confidence="0.9921605">1600 Amphitheatre Parkway Mountain View, CA</address>
<email confidence="0.999627">deepakr@google.com</email>
<abstract confidence="0.998754807692308">We present an extensive study on the problem of detecting polarity of words. We consider the polarity of a word to be either positive or negative. For example, such as wonconsidered as positive words; words such as are considered negative words. We treat polarity detection as a semi-supervised label propagation problem in a graph. In the graph, each node represents a word whose polarity is to be determined. Each weighted edge encodes a relation that exists between two words. Each node (word) can have two labels: positive or negative. We study this framework in two different resource availability scenarios using WordNet and OpenOffice thesaurus when WordNet is not available. We report our results on three different languages: English, French, and Hindi. Our results indicate that label propagation improves significantly over the baseline and other semisupervised learning methods like Mincuts and Randomized Mincuts for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>pages</pages>
<marker>[Blum and Chawla2001]</marker>
<rawString>Avrim Blum and Shuchi Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proc. 18th International Conf. on Machine Learning, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lafferty Blum</author>
<author>Rwebangira</author>
<author>Reddy</author>
</authors>
<title>Semi-supervised learning using randomized mincuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ICML.</booktitle>
<marker>[Blum et al.2004]</marker>
<rawString>Blum, Lafferty, Rwebangira, and Reddy. 2004. Semi-supervised learning using randomized mincuts. In Proceedings of the ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining term subjectivity and term orientation for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<marker>[Esuli and Sebastiani2006]</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Determining term subjectivity and term orientation for opinion mining. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>174--181</pages>
<marker>[Hatzivassiloglou and McKeown1997]</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the ACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of HTML documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1075--1083</pages>
<marker>[Kaji and Kitsuregawa2007]</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of HTML documents. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1075–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using WordNet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC-04, 4th International Conference on Language Resources and Evaluation, volume IV.</booktitle>
<marker>[Kamps et al.2004]</marker>
<rawString>Jaap Kamps, Maarten Marx, R. ort. Mokken, and Maarten de Rijke. 2004. Using WordNet to measure semantic orientation of adjectives. In Proceedings ofLREC-04, 4th International Conference on Language Resources and Evaluation, volume IV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard H Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL.</booktitle>
<marker>[Kim and Hovy2006]</marker>
<rawString>Soo-Min Kim and Eduard H. Hovy. 2006. Identifying and analyzing judgment opinions. In Proceedings of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>768--774</pages>
<marker>[Lin1998a]</marker>
<rawString>Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In Proceedings of COLING, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An informationtheoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference in Machine Learning,</booktitle>
<pages>296--304</pages>
<marker>[Lin1998b]</marker>
<rawString>Dekang Lin. 1998b. An informationtheoretic definition of similarity. In Proceedings of the 15th International Conference in Machine Learning, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>976--983</pages>
<marker>[Mihalcea et al.2007]</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976– 983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<marker>[Pang and Lee2004]</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceeding of the HLT-NAACL.</booktitle>
<marker>[Pedersen et al.2004]</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Proceeding of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<pages>25--32</pages>
<marker>[Riloffet al.2003]</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of the 7th Conference on Natural Language Learning, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<marker>[Stone et al.1966]</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>[Turney and Littman2003]</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 National Conference on Artificial Intelligence. AAAI.</booktitle>
<marker>[Wiebe2000]</marker>
<rawString>Janyce M. Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the 2000 National Conference on Artificial Intelligence. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>[Zhu and Ghahramani2002]</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>