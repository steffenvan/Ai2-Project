<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.963272">
Empirical Exploitation of Click Data for Task Specific Ranking
</title>
<author confidence="0.860399">
Anlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui Zheng
</author>
<affiliation confidence="0.593962">
Yahoo! Labs
</affiliation>
<address confidence="0.597274">
701 First Avenue
Sunnyvale, CA 94089
</address>
<email confidence="0.650581">
{anlei,yichang,shihao,ciyaliao,xinli,zhaohui}@yahoo-inc.com
</email>
<sectionHeader confidence="0.960608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992214285714">
There have been increasing needs for task
specific rankings in web search such as
rankings for specific query segments like
long queries, time-sensitive queries, navi-
gational queries, etc; or rankings for spe-
cific domains/contents like answers, blogs,
news, etc. In the spirit of ”divide-and-
conquer”, task specific ranking may have
potential advantages over generic ranking
since different tasks have task-specific fea-
tures, data distributions, as well as feature-
grade correlations. A critical problem for
the task-specific ranking is training data
insufficiency, which may be solved by us-
ing the data extracted from click log. This
paper empirically studies how to appro-
priately exploit click data to improve rank
function learning in task-specific ranking.
The main contributions are 1) the explo-
ration on the utilities of two promising ap-
proaches for click pair extraction; 2) the
analysis of the role played by the noise
information which inevitably appears in
click data extraction; 3) the appropriate
strategy for combining training data and
click data; 4) the comparison of click data
which are consistent and inconsistent with
baseline function.
</bodyText>
<sectionHeader confidence="0.995006" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916159090909">
Learning-to-rank approaches (Liu, 2008) have
been widely applied in commercial search en-
gines, in which ranking models are learned using
labeled documents. Significant efforts have been
made in attempt to learn a generic ranking model
which can appropriately rank documents for all
queries. However, web users’ query intentions are
extremely heterogeneous, which makes it difficult
for a generic ranking model to achieve best rank-
ing results for all queries. For this reason, there
have been increasing needs for task specific rank-
ings in web search such as rankings for specific
query segments like long queries, time-sensitive
queries, navigational queries, etc; or rankings
for specific domains/contents like answers, blogs,
news, etc. Therefore, a specific ranking task usu-
ally correspond to a category of queries; when
the search engine determines that a query is be-
longing to this category, it will call the ranking
function dedicated to this ranking task. The mo-
tivation of this divide-and-conquer strategy is that,
task specific ranking may have potential advan-
tages over generic ranking since different tasks
have task-specific features, data distributions, as
well as feature-grade correlations.
Such a dedicated ranking model can be trained
using the labeled data belonging to this query cat-
egory (which is called dedicated training data).
However, the amount of training data dedicated
to a specific ranking task is usually insufficient
because human labeling is expensive and time-
consuming, not to mention there are multiple rank-
ing tasks that need to be taken care of. To deal
with the training data insufficiency problem for
task-specific ranking, we propose to extract click-
through data and incorporate it with dedicated
training data to learn a dedicated model.
In order to incorporate click data to improve the
ranking for a dedicate query category, it is critical
to fully exploit click information. We empirically
explore the related approaches for the appropriate
click data exploitation in task-specific rank func-
tion learning. Figure 1 illustrates the procedures
and critical components to be studied.
</bodyText>
<listItem confidence="0.973209571428571">
1) Click data mining: the purpose is to extract
informative and reliable users’ preference infor-
mation from click log. We employ two promis-
ing approaches: one is heuristic rule approach, the
other is sequential supervised learning approach.
2) Sample selection and combination: with la-
beled training data and unlabeled click data, how
</listItem>
<page confidence="0.369851">
1086
</page>
<note confidence="0.9992115">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1086–1095,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.3776925">
Click data mining
• Heuristic-rule-based approach
• Sequential supervised learning approach
Generic training data Generic click data
Dedicated training data Dedicated click data
Sample selection and combination
GBrank algorithm
Task-specific ranking model
</figure>
<figureCaption confidence="0.674461">
Figure 1: Framework of incorporating click-
through data with training data to improve dedi-
cated model for task-specific ranking.
</figureCaption>
<bodyText confidence="0.998884392857143">
to select and combine them so that the samples
have the best utility for learning? As the data
distribution for a specific ranking task is differ-
ent from the generic data distribution, it is nat-
ural to select those labeled training samples and
unlabeled click preference pairs which belong to
this query category, so that the data distributions
of training set and testing set are consistent for
this category. On the other hand, we should keep
in mind that: a) non-dedicated data, i.e, the data
that does not belong the specific category, might
also have similar distribution as the dedicated data.
Such distribution similarity makes non-dedicated
data also useful for task-specific rank function
learning, especially for the scenario that dedicated
training samples is insufficient. b) The quality of
dedicated click data may be not as reliable as hu-
man labeled training data. In other words, there
are some extracted click preference pairs that are
inconsistent with human labeling while we regard
human labeling as correct labeling.
3) Rank function learning algorithm: we use
GBrank (Zheng et al., 2007) algorithm for rank
function learning, which has proved to be one
of the most effective up-to-date learning-to-rank
algorithms; furthermore, GBrank algorithm also
takes preference pairs as inputs, which will be il-
lustrated with more details in the paper.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99996587037037">
Learning to rank has been a promising research
area which continuously improves web search rel-
evance (Burges et al., 2005) (Zha et al., 2006)
(Cao et al., 2007) (Freund et al., 1998) (Fried-
man, 2001) (Joachims, 2002) (Wang and Zhai,
2007) (Zheng et al., 2007). The ranking prob-
lem is usually formulated as learning a ranking
function from preference data. The basic idea
is to minimize the number of contradicted pairs
in the training data, and different algorithm cast
the preference learning problem from different
point of view, for example, RankSVM (Joachims,
2002) uses support vector machines; RankBoost
(Freund et al., 1998) applies the idea of boost-
ing from weak learners; GBrank (Zheng et al.,
2007) uses gradient boosting with decision tree;
RankNet (Burges et al., 2005) uses gradient boost-
ing with neural net-work. In (Zha et al., 2006),
query difference is taken into consideration for
learning effective retrieval function, which leads
to a multi-task learning problem using risk mini-
mization framework.
There are a few related works to apply multi-
ple ranking models for different query categories.
However, none of them takes click-through infor-
mation into consideration. In (Kang and Kim,
2003), queries are categorized into 3 types, infor-
mational, navigational and transactional, and dif-
ferent models are applied on each query category.
a KNN method is proposed to employ different
ranking models to handle different types of queries
(Geng et al., 2008). The KNN method is unsuper-
vised, and it targets to improve the overall ranking
instead of the rank-ing for a certain query cate-
gory. In addition, the KNN method requires all
feature vector to be the same.
Quite a few research papers explore how to ob-
tain useful information from click-through data,
which could benefit search relevance (Carterette
et al., 2008) (Fox et al., 2005) (Radlinski and
Joachims, 2007) (Wang and Zhai, 2007). The in-
formation can be expressed as pair-wise prefer-
ences (Chapelle and Zhang, 2009) (Ji et al., 2009)
(Radlinski et al., 2008), or represented as rank fea-
tures (Agichtein et al., 2006). Task-specific rank-
ing relies on the accuracy of query classification.
Query classification or query intention identifica-
tion has been extensively studied in (Beitzel et al.,
2007) (Lee et al., 2005) (Li et al., 2008) (Rose and
Levinson, 2004). How to combine editorial data
and click data is well discussed in (Chen et al.,
2008) (Zheng et al., 2007). In addition, how to use
click data to improve ranking are also exploited
in personalized or preference-based search (Coyle
</bodyText>
<equation confidence="0.732133">
Click log
</equation>
<page confidence="0.653252">
1087
</page>
<tableCaption confidence="0.941519">
Table 1: Statistics of click occurrences for heuris-
tic rule approach.
</tableCaption>
<bodyText confidence="0.791541714285714">
imp impression, number of occurrence of the tuple
cc number of occurrence of the tuple where two
documents both get clicked
ncc number of occurrence of the tuple where url1
is not clicked but url2 is clicked
cnc number of occurrence of the tuple where url1
is clicked but url2 is not clicked
ncnc number of occurrence of the tuple where url1
and url2 are not clicked
Table 2: Skip-above pairs count vs. human judge-
ments (e.g., the element in the third row and sec-
ond column means we have 40 skip-above pairs
with ”excellent” url1 and ”perfect” url2). P: per-
fect; E: excellent; G: good; F: fair; B: bad.
</bodyText>
<equation confidence="0.490708666666667">
P E G F B
P 13 13 12 4 0
E 40 44 16 2 2
G 27 53 103 29 8
F 10 15 43 27 5
B 4 4 11 20 14
</equation>
<bodyText confidence="0.770813">
and Smyth, 2007) (Glance, 2001) (R. Jin, 2008).
</bodyText>
<sectionHeader confidence="0.952844" genericHeader="method">
3 Technical approach
</sectionHeader>
<bodyText confidence="0.965623666666667">
This section presents the related approaches in
Figure 1. In Section 4, we will make deeper anal-
ysis based on experimental results.
</bodyText>
<subsectionHeader confidence="0.999182">
3.1 Click data mining
</subsectionHeader>
<bodyText confidence="0.999973">
We use two approaches for click data mining,
whose outputs are preference pairs. A preference
pair is defined as a tuple {&lt; xq, yq &gt; Jxq &gt;- yq},
which means for the query q, the document xq is
more relevant than yq. We need to extract infor-
mative and reliable preference pairs which can be
used to improve rank function learning.
</bodyText>
<subsectionHeader confidence="0.896994">
3.1.1 Heuristic rule approach
</subsectionHeader>
<bodyText confidence="0.995619076923077">
We use heuristic rules to extract skip-above pairs
and skip-next pairs, which are similar to Strategy
1 (click &gt; skip above) and Strategy 5 (click &gt; no-
click next) proposed in (Joachims et al., 2005). To
reduce the misleading effect of an individual click
behavior, click information from different query
sessions is aggregated before applying heuristic
rules. For a tuple (q, url1, url2, pos1, pos2) where
q is query, url1 and url2 are urls representing two
documents, pos1 and pos2 are ranking positions
for the two documents with pos1 � pos2 mean-
ing url1 has higher rank than url2, the statistics for
this tuple are listed in Table 1.
Skip-above pair extraction: if ncc is much
larger than cnc, and cc imp, ncnc
imp is much smaller
than 1, that means, when url1 is ranked higher than
url2 in query q, most users click url2 but not click
url1. In this case, we extract a skip-above pair, i.e.,
url2 is more relevant than url1. In order to have
highly accurate skip-above pairs, a set of thresh-
Table 3: Skip-next pairs vs. human judgements
(e.g., the element in the third row and second col-
umn means we have 10 skip-next pairs with ”ex-
cellent” url1 and ”perfect” url2 ). P: perfect; E:
excellent; G: good; F: fair; B: bad.
</bodyText>
<table confidence="0.947318">
P E G F B
P 126 343 225 100 35
E 10 71 84 37 12
G 6 9 116 56 21
F 1 5 17 29 14
B 1 1 1 2 5
</table>
<bodyText confidence="0.988665057692308">
olds are applied to only extract the pairs that have
high impression and ncc is larger enough than cnc.
Skip-next pair extraction: if pos1 = pos2 − 1,
cnc is much larger than ncc, and cc
imp, ncnc
imp is much
smaller than 1, that means, in most of cases when
url2 is ranked just below url1 in query q, most
users click url1 but not click url2. In this case, we
regard this tuple as a skip-next pair.
To test the accuracy of preference pairs, we
ask editors to judge some randomly selected pairs
from skip-above pairs and skip-next pairs. Edi-
tors label each query-url pair using five grades ac-
cording to relevance: perfect, excellent, good, fair,
bad. Table 2 shows skip-above pair distribution.
The diagonal elements have high values, which
are for tied pairs labeled by editors but determined
as skip-above pairs from heuristic rules. Higher
values appear in the left-bottom triangle than in
the right-top triangle, because there are more skip-
above preferences agreed with editors than dis-
agreed with editors. Summing up the tied pairs,
agreed and disagreed pairs, 44% skip-above pref-
erence judgments agree with editors, 18% skip-
above preference judgments disagree with editors,
1088
and there are 38% skip-above pairs judged as tie
pairs by editors.
Table 3 shows skip-next pair distribution. Sum-
ming up the tied pairs, agreed and disagreed pairs,
70% skip-next preference judgments agree with
editors, 4% skip-next preference judgments dis-
agree with editors, and 26% skip-next pairs judged
as tie pairs by editors.
Therefore, skip-next pairs have much higher
accuracy than skip-above. That is because in a
search engine that already has a good ranking
function, it is much easier to find a correct skip-
next pairs which are consistent with the search en-
gine than to find a correct skip-above pairs which
are contradictory to the search engine. Skip-above
and skip-next preferences provide us two kinds of
users’s feedbacks which are complementary: skip-
above preferences provide us the feedback that the
user’s vote is contradictory to the current ranking,
which implies the current relative ranking should
be reversed; skip-next preferences shows that the
user’s vote is consistent with the current ranking,
which implies the current relative ranking should
be maintained with high confidence provided by
users’ vote.
</bodyText>
<subsectionHeader confidence="0.966089">
3.1.2 Sequential supervised learning
</subsectionHeader>
<bodyText confidence="0.965420846153846">
The click modeling by sequential supervised
learning (SSL) was proposed in (Ji et al., 2009),
in which user’s sequential click information is
exploited to extract relevance information from
click-logs. This approach is reliable because 1)
the sequential click information embedded in an
aggregation of user clicks provides substantial rel-
evance information of the documents displayed in
the search results, and 2) the SSL is supervised
learning (i.e., human judgments are provided with
relevance labels for the training).
The SSL is formulated in the framework
of global ranking (Qin et al., 2008). Let
</bodyText>
<equation confidence="0.935156875">
x(q) = {x(q)
1 , x(q)
2 , ... , x(q)
n } represent the doc-
uments retrieved with a query q, and y(q) =
{y(q)
1 , y(q)
2,... , y(q)
</equation>
<bodyText confidence="0.998075384615384">
n } represent the relevance la-
bels assigned to the documents. Here n is the
number of documents retrieved with q. Without
loss of generality, we assume that n is fixed and
invariant with respect to different queries. The
SSL determines to find a function F in the form
of y(q)=F(x(q)) that takes all the documents as
its inputs, exploiting both local and global infor-
mation among the documents, and predict the rel-
evance labels of all the document jointly. This
is distinct to most of learning to rank methods
that optimize a ranking model defined on a sin-
gle document, i.e., in the form of y(q)
</bodyText>
<equation confidence="0.922801">
i = f(x(q)
i ),
</equation>
<bodyText confidence="0.9735110625">
∀ i = 1, 2, ... , n. This formulation of the SSL
is important in extracting relevance information
from user click data since users’ click decisions
among different documents displayed in a search
session tend to rely not only on the relevance judg-
ment of a single document, but also on the relative
relevance comparison among the documents dis-
played; and the global ranking framework is well-
formulated to exploit both local and global infor-
mation from an aggregation of user clicks.
The SSL aggregates all the user sessions for
the same query into a tuple &lt;query, n-document
list, and an aggregation of user clicks&gt;. Fig-
ure 2 illustrates the process of feature extrac-
tion from an aggregated session, where x(q) =
{x(q)
</bodyText>
<equation confidence="0.7162535">
1 ,x(q)
2 , ... , x(q)
</equation>
<bodyText confidence="0.995139">
n } denotes a sequence of fea-
ture vectors extracted from the aggregated ses-
sion, with x(q)
i representing the feature vector ex-
tracted for document i. Specifically, to form fea-
ture vector x(q)
i , first a feature vector x(q)
i,j is ex-
tracted from each user j’s click information, and
j ∈ {1, 2,... }, then x(q)
i is formed by averaging
over xi,j , ∀j ∈ {1,2,... }, i.e., x(q)
(q) i is actually an
aggregated feature vector for document i. Table
4 lists all the features used in the SSL modeling.
Note that some features are statistics independent
of temporal information of the clicks, such as “Po-
sition” and “Frequency”, while other features re-
ply on their surrounding documents and the click
sequences. We use 90,000 query-url pairs to train
the SSL model, and 10,000 query-url pairs for best
model selection.
With the sequential click modeling discussed
above, several sequential supervised algorithms,
including the conditional random fields (CRF)
(Lafferty et al., 2001), the sliding window method
and the recurrent sliding window method (Diet-
terich, 2002), are explored to find a global ranking
function F. We omit the details but refer one to
(Ji et al., 2009). The emphasis here is on the im-
portance to adapt these algorithms to the ranking
problem.
After training, the SSL model can be used to
predict the relevance labels of all the documents in
a new aggregated session, and thus pair-wise pref-
erence data can be extracted, with the score dif-
ference representing the confidence of preference
</bodyText>
<figure confidence="0.978303545454546">
1089
user1
o
oo
...
user2
x1
y1
o
y2
x2
</figure>
<bodyText confidence="0.951590666666667">
i = 1, 2,..., N as many as possible. The following
loss function is used to measure the risk of a given
ranking function h.
</bodyText>
<figure confidence="0.848188222222222">
Feature Extraction N (max{0, h(yi)−h(xi)+T})2, (1)
1
R(h) = 2
o
xi yi i=1
o
x10 y10
{ ...
{ ...
</figure>
<figureCaption confidence="0.868727">
Figure 2: An illustration of feature extraction for
</figureCaption>
<bodyText confidence="0.98134975">
an aggregated session for SSL approach. x(q) de-
notes an extracted sequence of feature vectors, and
y(q) denotes the corresponding label sequence that
is assigned by human judges for training.
</bodyText>
<figure confidence="0.7110145">
...
...
</figure>
<tableCaption confidence="0.988271">
Table 4: Click features used in SSL model.
</tableCaption>
<table confidence="0.998602909090909">
Position Position of the document
in the result list
ClickRank Rank of 1st click of doc. in click seq.
Frequency Average number of clicks for this doc.
FrequencyRank Rank in the list sorted by num. of clicks
IsNextClicked 1 if next position is clicked, 0 otherwise
IsPreClicked 1 if previous position is clicked,
0 otherwise
IsAboveClicked 1 if there is a click above, 0 otherwise
IsBelowClicked 1 if there is a click below, 0 otherwise
ClickDuration Time spent on the document
</table>
<bodyText confidence="0.999881666666667">
prediction. For the reason of convenience, we also
call the preference pairs contradicting with pro-
duction ranking as skip-above pairs and those con-
sistent with production ranking as skip-next pairs,
so that we can analyze these two types of prefer-
ence pairs respectively.
</bodyText>
<subsectionHeader confidence="0.999831">
3.2 Modeling algorithm
</subsectionHeader>
<bodyText confidence="0.997193592592592">
The basic idea of GBrank (Zheng et al., 2007)
is that if the ordering of a preference pair
by the ranking function is contradictory to this
preference, we need to modify the ranking
function along the direction by swapping this
prefence pair. Preferences pairs could be gen-
erated from labeled data, or could be extracted
from click data. For each preference pair &lt;
x, y &gt; in the available preference set S =
{&lt; xi, yi &gt; |xi &gt;- yi, i = 1, 2, ..., N}, x should
be ranked higher than y. In GBrank algorithm, the
problem of learning ranking functions is to com-
pute a ranking function h , so that h matches the
set of preference, i.e, h(xi) &gt; h(yi) , if x &gt;- y,
where T is the margin between the two documents
in the pair. To minimize the loss function, h(x) has
to be larger than h(y) with the margin T, which can
be chosen as constant value, or as dynamic val-
ues varying with pairs. When pair-wise judgments
are extracted from editors’ labels with different
grades, pair-wise judgments can include grade dif-
ference, which can further be used as margin T.
The GBrank algorithm is illustrated in Algorithm
1, and two parameters need to be determined: the
shrinkage factor q and the number of iteration.
Algorithm 1 GBrank algorithm.
Start with an initial guess h0, for m = 1, 2,...
</bodyText>
<listItem confidence="0.869163222222222">
1. Construct a training set: for each &lt; xi, yi &gt;E
S, derive (xi, max{0, hm−1(yi) − hm,(xi) +
T}), and
(yi, − max{0, hm−1(yi) − hm,(xi) + T}).
2. Fit hm by using a base regressor with the
above training set.
3. hm = hm−1+qsmhm(x), where sm is found
by line search to minimize the object function,
q is shrinkage factor.
</listItem>
<subsectionHeader confidence="0.987079">
3.3 Sample selection and combination
</subsectionHeader>
<bodyText confidence="0.9740355">
We use a straightforward approach to learn rank-
ing model from the combined data, which is illus-
trated in Algorithm 2.
Algorithm 2 Learn ranking model by combining
editorial data and click preference pairs.
Input:
</bodyText>
<listItem confidence="0.998147">
• Editorial absolute judgement data.
• Preference pairs from click data.
1. Extract preference pairs from labeled data
with absolute judgement.
2. Select and combine preference pairs from
click data and labeled data.
3. Learn GBrank model from the combined
preference pairs.
</listItem>
<bodyText confidence="0.910861333333333">
Absolute judgement on labeled data contains
(query, url) pairs with absolute grade values la-
beled by human. In Step 1, for each query with
</bodyText>
<page confidence="0.472488">
1090
</page>
<bodyText confidence="0.997291333333334">
nq query-url pairs with corresponding grades, {&lt;
query, urli, gradei &gt; Ii = 1, 2, ... , nq}, its prefer-
ence pairs are extracted as
{&lt; query, urli, urlj, gradei − gradej &gt; Ii, j =
1,2,...,nq,i =� j} .
When combining human-labeled pairs and click
preference pairs, we can give use different relative
weights for these two data sources. The loss func-
tion becomes
</bodyText>
<equation confidence="0.985856166666667">
w1:
R(h) = (max{0, h(yi) − h(xi) + τ})2
Nl iELabeled
1 − w 1:(max{0, h(yi) − h(xi) + τ})2
N ,(2)
c
</equation>
<bodyText confidence="0.906526975">
where w is used to control the relative weights be-
tween labeled training data and click data, Nl is
the number of training data pairs, and Nc is the
number of click pairs. The margin τ can be deter-
mined as grade difference for editor pairs, and be
a constant parameter for click pairs.
Step 2 is critical for the efficacy of the approach.
A few factors need to be considered:
1) data distribution: for the application of task-
specific ranking, our purpose is to improve ranking
for the queries belonging to this category. An im-
portant observation is that the relevance patterns
for the ranking within a specific category may
have some unique characteristics, which are differ-
ent from generic relevance ranking. Thus, it is rea-
sonable to consider only using dedicated labeled
training data and dedicated click preference data
for training. The reality is that dedicated training
data is usually insufficient, while it is possible that
non-dedicated data can also help the learning.
2) click pair quality: it is inevitable there exist
some incorrect pairs in the click preference pairs.
Such incorrect pairs may mislead the learning. So
overall, can the click preference pairs still help the
learning for task-specific ranking? By our study,
skip-above pairs usually contain more incorrect
pairs compared with skip-above pairs. Does this
mean skip-next pairs are always more helpful in
improving learning than skip-above pairs?
3) click pair utility: use labeled training data as
baseline, how much complimentary information
can click pairs bring? This is determined by the
methodology of click data mining approach.
While it is possible to achieve some learning
improvement for task-specific ranking by using
click pairs by a plausible method, we attempt to
empirically explore the above interweaving fac-
tors for deeper understanding, in order to apply the
most appropriate strategy to exploit click data on
real-world applications of task-specific ranking.
</bodyText>
<sectionHeader confidence="0.999006" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996221">
4.1 Data set
</subsectionHeader>
<bodyText confidence="0.999933111111111">
Query category: in the experiments, we use long
query ranking as an example of task-specific rank-
ing, because it is commonly known that long query
ranking has some unique relevance patterns com-
pared with generic ranking. We define the long
queries as the queries containing at least three to-
kens. The techniques and analysis proposed in this
paper can be applied to other ranking tasks, such
as rankings for specific query segments like time-
sensitive queries, navigational queries, or rankings
for specific domains/contents like answers, blogs,
news, as long as the tasks have their own charac-
teristics of data distributions and discriminant rank
features.
Labeled training data: we do experiments
based on a data set for a commercial search en-
gine, for which there are 16,797 query-url pairs
(with 1,123 different queries) that have been la-
beled by editors. The proportion of long queries
is about 35% of all queries. The data distribution
of such long queries may be different from gen-
eral data distribution, as it will be validated in the
experiments below.
The human labeled data is randomly split into
two sets: training set (8,831 query-url pairs, 589
queries), and testing set (7,966 query-url pairs,
534 queries). The training set will be combined
with click preference pairs for rank function learn-
ing, and the testing set will be used to evaluate the
efficacy of the ranking function. In the training set,
there are 3,842 long query-url pairs (229 queries).
At testing stage, the learned rank functions are ap-
plied only to the long queries in the testing data,
as our concern in this paper is how to improve
task-specific ranking, i.e., long query ranking in
the experiment. In the testing data, there are 3,210
query-url pairs (193 queries) are long query data,
which will be used to test rank functions.
Click preference pairs: using the two ap-
proaches of heuristic rule approach and sequen-
tial supervised approach, we extract click prefence
pairs from the click log of the search engine. Each
approach yields both skip-next and skip-above
pairs, which are sorted by confidence descending
order respectively.
</bodyText>
<equation confidence="0.376941">
iEClick
</equation>
<page confidence="0.48175">
1091
</page>
<tableCaption confidence="0.96397">
Table 5: Use click data by heuristic rule approach
</tableCaption>
<table confidence="0.875200307692308">
(Data Selection: ”N”: not use; ”D”: use dedicated
data; ”G”: use generic data. Data Source: ”T”:
training data; ”C”: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7822 0.7906 (1.2%) 0.7997(2.4%)
GC 0.7834 0.7908 (1.2%) 0.7950 (1.7%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6649 0.7676 (-1.6%) 0.7748 (-0.8%)
GC 0.6792 0.7656 (-2.0%) 0.7989 (2.2%)
</table>
<tableCaption confidence="0.865138">
Table 6: Use click data by SSL approach (Data
Selection: ”N”: not use; ”D”: use dedicated data;
”G”: use generic data. Data Source: ”T”: training
</tableCaption>
<table confidence="0.967880909090909">
data; ”C”: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7752 0.7933 (1.5%) 0.7936 (1.5%)
GC 0.7624 0.7844 (0.4%) 0.7914 (1.2%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6756 0.7636 (-2.2%) 0.7784 (-0.3%)
GC 0.6860 0.7717 (-1.2%) 0.7774 (-0.5%)
</table>
<subsectionHeader confidence="0.996118">
4.2 Setup and measurements
</subsectionHeader>
<bodyText confidence="0.9998125">
We try different sample selection and combination
strategies to train rank functions using GBrank al-
gorithm. For the labeled training data, we either
use generic data or dedicated data. For the click
preference pairs, we also try these two options.
Furthermore, as more click preference pairs may
bring more useful information to help the learn-
ing while on the other hand, the more incorrect
pairs may be given so that they mislead the learn-
ing, we try different amounts of these prefence
pairs: 5,000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs.
We use NDCG to evaluate ranking model,
which is defined as
</bodyText>
<equation confidence="0.977442">
En 2r(�)−1
NDCGn = Zn i�1 ����i�1�
</equation>
<bodyText confidence="0.999891">
where i is the position in the document list, r(i) is
the score of Document i, and Zn is a normalization
factor, which is used to make the NDCG of ideal
list be 1.
</bodyText>
<subsectionHeader confidence="0.965019">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999990022222222">
Table 5 and 6 show the NDCG5 results by using
heuristic rule approach and SSL approach respec-
tively. We do not present NDCG1 results due to
space limitation, but NDCG1 results have the sim-
ilar trends as NDCG5.
Baseline by training data: there are two base-
line functions by using training data sets 1) use
dedicated training data (DT), NDCG5 on the test-
ing set by the rank function is 0.7736; 2) use
generic training data (GT), NDCG5 is 0.7813. It
is reasonable that using generic training data is
better than only using dedicated training data, be-
cause the distributions of non-dedicated data and
dedicated data share some similarity. As the ded-
icated training data is insufficient, the adoption of
the extra non-dedicated data helps the learning.
We compare learning results with Baseline 2) (use
generic training data, the slot of NC + GT in the
tables), which is the higher baseline.
Baseline by click data: we then study the utili-
ties of click preference pairs by using them alone
for training without using labeled training data.
In Table 5 and 6, each of the NDCG5 results us-
ing click preference pairs is the highest NDCG5
value over the cases of using different amounts of
pairs (5000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs). The results regarding the pairs
amounts are illustrated in Figure 3, which will help
us to analyze the results more deeply.
If we only use click preference pairs for training
(the two table slots DC+NT and GC+NT, corre-
sponding to using dedicated click preference pairs
and generic click pairs respectively), the best case
is using skip-next pairs extracted by heuristic rule
approach (Table 5 (a) ). It is not surprising that
skip-next pairs outperform skip-above pairs be-
cause there are significantly lower percentage of
incorrect pairs in skip-next pairs compared with
skip-above pairs. It is a little bit surprising that
the case of DC+NT has no dominant advantage
over GC+NT as we expected. For example, in Ta-
ble 5 (a), the NDCG5 values (0.7822 and 0.7834)
are very close to each other. However, in Figure
3, we find that with the same amount of pairs,
when we use 30,000 or fewer pairs, using dedi-
</bodyText>
<page confidence="0.720376">
1092
</page>
<figureCaption confidence="0.700554">
Figure 3: Incorporate different amounts of skip-
</figureCaption>
<bodyText confidence="0.952098101694915">
next pairs by heuristic rule approach with generic
training data.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
trainging sample weight
Figure 4: The effects of using different combin-
ing weights. Skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
cated click pairs alone is always better than using
generic click pairs alone. With more click pairs
being used (&gt; 30, 000), the noise rates become
higher in the pairs, which makes the distribution
factor less important.
Combine training data and click data: we
compare the four table slots, DC+DT, GC+DT,
DC+GT, GC+GT, in Table 5 and 6, and there are
quite a few interesting observations:
1) Skip-next vs. skip-above: overall, incorporat-
ing skip-next pairs with training data is better than
incorporating skip-above pairs, due to the reason
that there are more incorrect pairs in skip-above
pairs, which may mislead the learning. The only
exception is the slot GC+GT in Table 5 (b), whose
NDCG5 improvement is as high as 2.2%. We fur-
ther track this result, and find that this is the case
by using only 5,000 generic skip-above pairs. The
noise rate of these 5,000 pairs is low because they
have the highest pair extraction confidence values.
At the same time, these 5,000 pairs may provide
good complementary signals to the generic train-
ing data, so that the learning result is good. How-
ever, in general, skip-next pairs have better utilities
than skip-above pairs.
2) Dedicated training data vs. generic train-
ing data: using generic training data is gen-
erally better than only using dedicated training
data. If training data is insufficient, the extra
non-dedicated data provides useful information
for relevance pattern learning, and the distribu-
tion dissimilarity between dedicated data and non-
dedicated data is not the most important factor.
3) Dedicated click data vs. generic click data:
using dedicated click data is more effective than
using generic click data. From Figure 3, we ob-
serve that when 30,000 or fewer pairs are incorpo-
rated into training data, using dedicate click pairs
is always better than using generic click pairs.
Figure 5: The effects of using different margin
values for click preference pairs. Skip-next pairs
by heuristic rule approach are incorporated with
generic training data.
4) Heuristic rule approach vs. SSL approach:
the preference pairs extracted by heuristic rule ap-
proach have better utilities than those extracted by
SSL approach.
5) GBrank parameters for combining training
data and click pairs: the relative weight w for
combining training data and click pairs in (2) may
also affect rank function learning. Figure 4 shows
the effects of using different combining weights,
</bodyText>
<figure confidence="0.987408605263158">
0.755
1 2 3 4 5 6 7 8 9 10
click pairs
x 104
0.8
0.795
0.79
0.785
0.78
0.775
0.77
dedicate train + dedicate click
generic train + dedicate click
dedicate click
generic click
0.765
0.76
0.81
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
0.8
0.79
0.78
0.77
0.76
τ
0.802
0.798
0.796
0.794
0.792
0.788
0.786
0.784
0 0.5 1 1.5 2 2.5
0.79
0.8
</figure>
<bodyText confidence="0.910402466666667">
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
1093
for which skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
We observe that neither over-weighting training
data or over-weighting click pairs yields good re-
sults while the two data sources are best exploited
at certain weight values when there is good bal-
ance between them. Another concern is the ap-
propriate margin value T for the click pairs in (2).
Figure 5 shows that T = 1 consistently yields good
learning results, which suggests us that click pair
provides good information at T = 1.
</bodyText>
<subsectionHeader confidence="0.981981">
4.4 Discussions
</subsectionHeader>
<bodyText confidence="0.995472050000001">
we have defactorized the related approaches for
exploiting click data to improve task-specific rank
learning. The utility of click preference pairs de-
pends on the following factors:
1) Data distribution: if click pairs have good
quality, we should use dedicated click pairs in-
stead of generic click pairs, so that the samples
for training have similar distribution to the task of
task-specific ranking.
2) The amount of dedicated training data: the
more dedicated training data, the more reliable the
task-specific rank function is; thus, the less room
for learning improvement using click data. For the
case in the experiment that dedicated training is in-
sufficient, the non-dedicated training data can also
help the learning as non-dedicated training data
share relevance pattern similarity with the dedi-
cated data distribution.
3) The quality of click pairs: if we can extract
large amount of high-quality click pairs, the learn-
ing improvement will be significant. For example,
as shown in Figure 3, at the early stage with fewer
click pairs (5,000 and 10,000 pairs) being com-
bined with training data, the learning improvement
is best. With more click pairs are used, the noise
rate in the click pairs becomes higher so that the
learning misleading factor is more important than
information complementary factor. Thus, it is im-
portant to improve the reliability of the click pairs.
4) The utility of click pairs: by our study, the
quality of click pairs extracted by SSL approach
is comparable to those extracted by heuristic rule
approach. The possible reason that heuristic-rule-
based click pairs can bring more benefit is that
these pairs provide more complementary infor-
mation compared with SSL approach. As the
methodologies of these two click data extraction
approaches are totally different, in future we will
explore the concrete reason that causes such utility
difference.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998175">
By empirically exploring the related factors in
utilizing click-through data to improve dedicated
model learning for task-specific ranking, we have
better understood the principles of using click
preference pairs appropriately, which is impor-
tant for the real-world applications in commer-
cial search engines as using click data can sig-
nificantly save human labeling costs and makes
rank function learning more efficient. In the case
that dedicated training data is limited, while non-
dedicated training data is helpful, using dedicated
skip-next pairs is the most effective way to further
improve the learning. Heuristic rule approach pro-
vides more useful click pairs compared with se-
quential supervised learning approach. The qual-
ity of click pairs is critical for the efficacy of the
approach. Therefore, an interesting topic is how
to further reduce the inconsistency between skip-
above pairs and human labeling so that such data
may also be useful for task-specific ranking.
</bodyText>
<page confidence="0.771352">
1094
</page>
<sectionHeader confidence="0.994922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999179696969697">
E. Agichtein, E. Brill, and S. Dumais. 2006. Improv-
ing web search ranking by incorporating user behav-
ior information. Proc. of ACM SIGIR Conference.
S. M. Beitzel, E. C. Jensen, A. Chowdhury, and
O. Frieder. 2007. Varying approaches to topical
web query classification. Proceedings of ACM SI-
GIR conference.
C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to rank using gradient descent. Proc. of
Intl. Conf. on Machine Learning.
Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. 2007.
Learning to rank: From pairwise approach to list-
wise. Proceedings of ICML conference.
B. Carterette, P. N. Bennett, D. M. Chickering, and S. T.
Dumais. 2008. Here or there: preference judgments
for relevance. Proc. of ECIR.
O. Chapelle and Y. Zhang. 2009. A dynamic bayesian
network click model for web search ranking. Pro-
ceedings of the 18th International World Wide Web
Conference.
K. Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun.
2008. Adapting ranking functions to user prefer-
ence. ICDE Workshops, pages 580–587.
M. Coyle and B. Smyth. 2007. Supporting intelligent
web search. ACM Transaction Internet Tech., 7(4).
T. G. Dietterich. 2002. Machine learning for sequen-
tial data: a review. Lecture Notes in Computer Sci-
ence, (2396):15–30.
S. Fox, K. Karnawat, M. Mydland, S. Dumias, and
T. White. 2005. Evaluating implicit measures to
improve web search. ACM Trans. on Information
Systems, 23(2):147–168.
Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for combin-
ing preferences. Proceedings of International Con-
ference on Machine Learning.
J. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist., 29:1189–
1232.
X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum.
2008. Query dependent ranking with k nearest
neighbor. Proceedings of ACM SIGIR Conference.
N. S. Glance. 2001. Community search assistant. In-
telligent User Interfaces, pages 91–96.
S. Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle,
G. Sun, and H. Zha. 2009. Global ranking by ex-
ploiting user clicks. In SIGIR’09, Boston, USA, July
19-23.
T. Joachims, L. Granka, B. Pan, and G Gay. 2005.
Accurately interpreting clickthough data as implicit
feedback. Proc. of ACM SIGIR Conference.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
I. Kang and G. Kim. 2003. Query type classification
for web document retrieval. Proceedings of ACM
SIGIR Conference.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML, pages
282–289.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in web search. Proceedings of
International Conference on World Wide Web.
X. Li, Y.Y Wang, and A. Acero. 2008. Learning query
intent from regularized click graphs. Proceedings of
ACM SIGIR Conference.
T. Y Liu. 2008. Learning to rank for information re-
trieval. SIGIR tutorial.
T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. 2008.
Global ranking using continuous conditional ran-
dom fields. In NIPS.
H. Li R. Jin, H. Valizadegan. 2008. Ranking re-
finement and its application to information retrieval.
Proceedings of International Conference on World
Wide Web.
F. Radlinski and T. Joachims. 2007. Active exploration
for learning rankings from clickthrough data. Proc.
of ACM SIGKDD Conference.
F. Radlinski, M. Kurup, and T. Joachims. 2008. How
does clickthrough data reflect retrieval quality? Pro-
ceedings of ACM CIKM Conference.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of International
Conference on World Wide Web.
X. Wang and C. Zhai. 2007. Learn from web search
logs to organize search results. In Proceedings of
the 30th ACM SIGIR.
H. Zha, Z. Zheng, H. Fu, and G. Sun. 2006. Incor-
porating query difference for learning retrieval func-
tions in world wide web search. Proceedings of the
15th ACM Conference on Information and Knowl-
edge Management.
Z. Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen,
and G. Sun. 2007. A general boosting method and
its application to learning ranking functions for web
search. NIPS.
</reference>
<page confidence="0.737993">
1095
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774201">
<title confidence="0.999745">Empirical Exploitation of Click Data for Task Specific Ranking</title>
<author confidence="0.990064">Anlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui</author>
<affiliation confidence="0.865474">Yahoo!</affiliation>
<address confidence="0.9150945">701 First Sunnyvale, CA</address>
<abstract confidence="0.99922624137931">There have been increasing needs for task specific rankings in web search such as rankings for specific query segments like long queries, time-sensitive queries, navigational queries, etc; or rankings for specific domains/contents like answers, blogs, news, etc. In the spirit of ”divide-andconquer”, task specific ranking may have potential advantages over generic ranking since different tasks have task-specific features, data distributions, as well as featuregrade correlations. A critical problem for the task-specific ranking is training data insufficiency, which may be solved by using the data extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-specific ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>E Brill</author>
<author>S Dumais</author>
</authors>
<title>Improving web search ranking by incorporating user behavior information.</title>
<date>2006</date>
<booktitle>Proc. of ACM SIGIR</booktitle>
<contexts>
<context position="7883" citStr="Agichtein et al., 2006" startWordPosition="1215" endWordPosition="1218">8). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence o</context>
</contexts>
<marker>Agichtein, Brill, Dumais, 2006</marker>
<rawString>E. Agichtein, E. Brill, and S. Dumais. 2006. Improving web search ranking by incorporating user behavior information. Proc. of ACM SIGIR Conference. S. M. Beitzel, E. C. Jensen, A. Chowdhury, and O. Frieder. 2007. Varying approaches to topical web query classification. Proceedings of ACM SIGIR conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burges</author>
<author>T Shaked</author>
<author>E Renshaw</author>
<author>A Lazier</author>
<author>M Deeds</author>
<author>N Hamilton</author>
<author>G Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>Proc. of Intl. Conf. on Machine Learning.</booktitle>
<contexts>
<context position="5914" citStr="Burges et al., 2005" startWordPosition="896" endWordPosition="899">n other words, there are some extracted click preference pairs that are inconsistent with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient</context>
</contexts>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. 2005. Learning to rank using gradient descent. Proc. of Intl. Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Cao</author>
<author>T Qin</author>
<author>T Liu</author>
<author>M Tsai</author>
<author>H Li</author>
</authors>
<title>Learning to rank: From pairwise approach to listwise.</title>
<date>2007</date>
<booktitle>Proceedings of ICML conference.</booktitle>
<contexts>
<context position="5952" citStr="Cao et al., 2007" startWordPosition="904" endWordPosition="907">lick preference pairs that are inconsistent with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet </context>
</contexts>
<marker>Cao, Qin, Liu, Tsai, Li, 2007</marker>
<rawString>Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. 2007. Learning to rank: From pairwise approach to listwise. Proceedings of ICML conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carterette</author>
<author>P N Bennett</author>
<author>D M Chickering</author>
<author>S T Dumais</author>
</authors>
<title>Here or there: preference judgments for relevance.</title>
<date>2008</date>
<booktitle>Proc. of ECIR.</booktitle>
<contexts>
<context position="7624" citStr="Carterette et al., 2008" startWordPosition="1171" endWordPosition="1174">3), queries are categorized into 3 types, informational, navigational and transactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et</context>
</contexts>
<marker>Carterette, Bennett, Chickering, Dumais, 2008</marker>
<rawString>B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. 2008. Here or there: preference judgments for relevance. Proc. of ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>Y Zhang</author>
</authors>
<title>A dynamic bayesian network click model for web search ranking.</title>
<date>2009</date>
<booktitle>Proceedings of the 18th International World Wide Web Conference.</booktitle>
<contexts>
<context position="7782" citStr="Chapelle and Zhang, 2009" startWordPosition="1197" endWordPosition="1200">d is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1</context>
</contexts>
<marker>Chapelle, Zhang, 2009</marker>
<rawString>O. Chapelle and Y. Zhang. 2009. A dynamic bayesian network click model for web search ranking. Proceedings of the 18th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chen</author>
<author>Y Zhang</author>
<author>Z Zheng</author>
<author>H Zha</author>
<author>G Sun</author>
</authors>
<title>Adapting ranking functions to user preference. ICDE Workshops,</title>
<date>2008</date>
<pages>580--587</pages>
<contexts>
<context position="8214" citStr="Chen et al., 2008" startWordPosition="1269" endWordPosition="1272">ce (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence of the tuple cc number of occurrence of the tuple where two documents both get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of the tuple where url1 is clicked but url2 is not clicked ncnc number of occurrence of the tuple where url1 and url2 are not clicked Ta</context>
</contexts>
<marker>Chen, Zhang, Zheng, Zha, Sun, 2008</marker>
<rawString>K. Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun. 2008. Adapting ranking functions to user preference. ICDE Workshops, pages 580–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Coyle</author>
<author>B Smyth</author>
</authors>
<title>Supporting intelligent web search.</title>
<date>2007</date>
<journal>ACM Transaction Internet Tech.,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>Coyle, Smyth, 2007</marker>
<rawString>M. Coyle and B. Smyth. 2007. Supporting intelligent web search. ACM Transaction Internet Tech., 7(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Machine learning for sequential data: a review.</title>
<date>2002</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>2396--15</pages>
<contexts>
<context position="16538" citStr="Dietterich, 2002" startWordPosition="2735" endWordPosition="2737"> 4 lists all the features used in the SSL modeling. Note that some features are statistics independent of temporal information of the clicks, such as “Position” and “Frequency”, while other features reply on their surrounding documents and the click sequences. We use 90,000 query-url pairs to train the SSL model, and 10,000 query-url pairs for best model selection. With the sequential click modeling discussed above, several sequential supervised algorithms, including the conditional random fields (CRF) (Lafferty et al., 2001), the sliding window method and the recurrent sliding window method (Dietterich, 2002), are explored to find a global ranking function F. We omit the details but refer one to (Ji et al., 2009). The emphasis here is on the importance to adapt these algorithms to the ranking problem. After training, the SSL model can be used to predict the relevance labels of all the documents in a new aggregated session, and thus pair-wise preference data can be extracted, with the score difference representing the confidence of preference 1089 user1 o oo ... user2 x1 y1 o y2 x2 i = 1, 2,..., N as many as possible. The following loss function is used to measure the risk of a given ranking functi</context>
</contexts>
<marker>Dietterich, 2002</marker>
<rawString>T. G. Dietterich. 2002. Machine learning for sequential data: a review. Lecture Notes in Computer Science, (2396):15–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fox</author>
<author>K Karnawat</author>
<author>M Mydland</author>
<author>S Dumias</author>
<author>T White</author>
</authors>
<title>Evaluating implicit measures to improve web search.</title>
<date>2005</date>
<journal>ACM Trans. on Information Systems,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="7643" citStr="Fox et al., 2005" startWordPosition="1175" endWordPosition="1178">d into 3 types, informational, navigational and transactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In add</context>
</contexts>
<marker>Fox, Karnawat, Mydland, Dumias, White, 2005</marker>
<rawString>S. Fox, K. Karnawat, M. Mydland, S. Dumias, and T. White. 2005. Evaluating implicit measures to improve web search. ACM Trans. on Information Systems, 23(2):147–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R D Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5974" citStr="Freund et al., 1998" startWordPosition="908" endWordPosition="911">rs that are inconsistent with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet (Burges et al., 2005) </context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. 1998. An efficient boosting algorithm for combining preferences. Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine.</title>
<date>2001</date>
<journal>Ann. Statist.,</journal>
<volume>29</volume>
<pages>1232</pages>
<contexts>
<context position="5991" citStr="Friedman, 2001" startWordPosition="912" endWordPosition="914">nt with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet (Burges et al., 2005) uses gradient boo</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>J. Friedman. 2001. Greedy function approximation: a gradient boosting machine. Ann. Statist., 29:1189– 1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Geng</author>
<author>T Liu</author>
<author>T Qin</author>
<author>A Arnold</author>
<author>H Li</author>
<author>H Shum</author>
</authors>
<title>Query dependent ranking with k nearest neighbor.</title>
<date>2008</date>
<booktitle>Proceedings of ACM SIGIR Conference.</booktitle>
<contexts>
<context position="7262" citStr="Geng et al., 2008" startWordPosition="1110" endWordPosition="1113">ery difference is taken into consideration for learning effective retrieval function, which leads to a multi-task learning problem using risk minimization framework. There are a few related works to apply multiple ranking models for different query categories. However, none of them takes click-through information into consideration. In (Kang and Kim, 2003), queries are categorized into 3 types, informational, navigational and transactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Ag</context>
</contexts>
<marker>Geng, Liu, Qin, Arnold, Li, Shum, 2008</marker>
<rawString>X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum. 2008. Query dependent ranking with k nearest neighbor. Proceedings of ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N S Glance</author>
</authors>
<title>Community search assistant. Intelligent User Interfaces,</title>
<date>2001</date>
<pages>91--96</pages>
<contexts>
<context position="9167" citStr="Glance, 2001" startWordPosition="1458" endWordPosition="1459">th get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of the tuple where url1 is clicked but url2 is not clicked ncnc number of occurrence of the tuple where url1 and url2 are not clicked Table 2: Skip-above pairs count vs. human judgements (e.g., the element in the third row and second column means we have 40 skip-above pairs with ”excellent” url1 and ”perfect” url2). P: perfect; E: excellent; G: good; F: fair; B: bad. P E G F B P 13 13 12 4 0 E 40 44 16 2 2 G 27 53 103 29 8 F 10 15 43 27 5 B 4 4 11 20 14 and Smyth, 2007) (Glance, 2001) (R. Jin, 2008). 3 Technical approach This section presents the related approaches in Figure 1. In Section 4, we will make deeper analysis based on experimental results. 3.1 Click data mining We use two approaches for click data mining, whose outputs are preference pairs. A preference pair is defined as a tuple {&lt; xq, yq &gt; Jxq &gt;- yq}, which means for the query q, the document xq is more relevant than yq. We need to extract informative and reliable preference pairs which can be used to improve rank function learning. 3.1.1 Heuristic rule approach We use heuristic rules to extract skip-above pai</context>
</contexts>
<marker>Glance, 2001</marker>
<rawString>N. S. Glance. 2001. Community search assistant. Intelligent User Interfaces, pages 91–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ji</author>
<author>K Zhou</author>
<author>C Liao</author>
<author>Z Zheng</author>
<author>G Xue</author>
<author>O Chapelle</author>
<author>G Sun</author>
<author>H Zha</author>
</authors>
<title>Global ranking by exploiting user clicks.</title>
<date>2009</date>
<booktitle>In SIGIR’09,</booktitle>
<location>Boston, USA,</location>
<contexts>
<context position="7800" citStr="Ji et al., 2009" startWordPosition="1201" endWordPosition="1204">ferent ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of cl</context>
<context position="13482" citStr="Ji et al., 2009" startWordPosition="2203" endWordPosition="2206">arch engine. Skip-above and skip-next preferences provide us two kinds of users’s feedbacks which are complementary: skipabove preferences provide us the feedback that the user’s vote is contradictory to the current ranking, which implies the current relative ranking should be reversed; skip-next preferences shows that the user’s vote is consistent with the current ranking, which implies the current relative ranking should be maintained with high confidence provided by users’ vote. 3.1.2 Sequential supervised learning The click modeling by sequential supervised learning (SSL) was proposed in (Ji et al., 2009), in which user’s sequential click information is exploited to extract relevance information from click-logs. This approach is reliable because 1) the sequential click information embedded in an aggregation of user clicks provides substantial relevance information of the documents displayed in the search results, and 2) the SSL is supervised learning (i.e., human judgments are provided with relevance labels for the training). The SSL is formulated in the framework of global ranking (Qin et al., 2008). Let x(q) = {x(q) 1 , x(q) 2 , ... , x(q) n } represent the documents retrieved with a query q</context>
<context position="16644" citStr="Ji et al., 2009" startWordPosition="2755" endWordPosition="2758">emporal information of the clicks, such as “Position” and “Frequency”, while other features reply on their surrounding documents and the click sequences. We use 90,000 query-url pairs to train the SSL model, and 10,000 query-url pairs for best model selection. With the sequential click modeling discussed above, several sequential supervised algorithms, including the conditional random fields (CRF) (Lafferty et al., 2001), the sliding window method and the recurrent sliding window method (Dietterich, 2002), are explored to find a global ranking function F. We omit the details but refer one to (Ji et al., 2009). The emphasis here is on the importance to adapt these algorithms to the ranking problem. After training, the SSL model can be used to predict the relevance labels of all the documents in a new aggregated session, and thus pair-wise preference data can be extracted, with the score difference representing the confidence of preference 1089 user1 o oo ... user2 x1 y1 o y2 x2 i = 1, 2,..., N as many as possible. The following loss function is used to measure the risk of a given ranking function h. Feature Extraction N (max{0, h(yi)−h(xi)+T})2, (1) 1 R(h) = 2 o xi yi i=1 o x10 y10 { ... { ... Figu</context>
</contexts>
<marker>Ji, Zhou, Liao, Zheng, Xue, Chapelle, Sun, Zha, 2009</marker>
<rawString>S. Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle, G. Sun, and H. Zha. 2009. Global ranking by exploiting user clicks. In SIGIR’09, Boston, USA, July 19-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
<author>L Granka</author>
<author>B Pan</author>
<author>G Gay</author>
</authors>
<title>Accurately interpreting clickthough data as implicit feedback.</title>
<date>2005</date>
<booktitle>Proc. of ACM SIGIR Conference.</booktitle>
<contexts>
<context position="9917" citStr="Joachims et al., 2005" startWordPosition="1588" endWordPosition="1591">r analysis based on experimental results. 3.1 Click data mining We use two approaches for click data mining, whose outputs are preference pairs. A preference pair is defined as a tuple {&lt; xq, yq &gt; Jxq &gt;- yq}, which means for the query q, the document xq is more relevant than yq. We need to extract informative and reliable preference pairs which can be used to improve rank function learning. 3.1.1 Heuristic rule approach We use heuristic rules to extract skip-above pairs and skip-next pairs, which are similar to Strategy 1 (click &gt; skip above) and Strategy 5 (click &gt; noclick next) proposed in (Joachims et al., 2005). To reduce the misleading effect of an individual click behavior, click information from different query sessions is aggregated before applying heuristic rules. For a tuple (q, url1, url2, pos1, pos2) where q is query, url1 and url2 are urls representing two documents, pos1 and pos2 are ranking positions for the two documents with pos1 � pos2 meaning url1 has higher rank than url2, the statistics for this tuple are listed in Table 1. Skip-above pair extraction: if ncc is much larger than cnc, and cc imp, ncnc imp is much smaller than 1, that means, when url1 is ranked higher than url2 in quer</context>
</contexts>
<marker>Joachims, Granka, Pan, Gay, 2005</marker>
<rawString>T. Joachims, L. Granka, B. Pan, and G Gay. 2005. Accurately interpreting clickthough data as implicit feedback. Proc. of ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="6008" citStr="Joachims, 2002" startWordPosition="915" endWordPosition="916">eling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet (Burges et al., 2005) uses gradient boosting with neural</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kang</author>
<author>G Kim</author>
</authors>
<title>Query type classification for web document retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of ACM SIGIR Conference.</booktitle>
<contexts>
<context position="7002" citStr="Kang and Kim, 2003" startWordPosition="1069" endWordPosition="1072">achines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet (Burges et al., 2005) uses gradient boosting with neural net-work. In (Zha et al., 2006), query difference is taken into consideration for learning effective retrieval function, which leads to a multi-task learning problem using risk minimization framework. There are a few related works to apply multiple ranking models for different query categories. However, none of them takes click-through information into consideration. In (Kang and Kim, 2003), queries are categorized into 3 types, informational, navigational and transactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Ca</context>
</contexts>
<marker>Kang, Kim, 2003</marker>
<rawString>I. Kang and G. Kim. 2003. Query type classification for web document retrieval. Proceedings of ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="16452" citStr="Lafferty et al., 2001" startWordPosition="2721" endWordPosition="2724">{1,2,... }, i.e., x(q) (q) i is actually an aggregated feature vector for document i. Table 4 lists all the features used in the SSL modeling. Note that some features are statistics independent of temporal information of the clicks, such as “Position” and “Frequency”, while other features reply on their surrounding documents and the click sequences. We use 90,000 query-url pairs to train the SSL model, and 10,000 query-url pairs for best model selection. With the sequential click modeling discussed above, several sequential supervised algorithms, including the conditional random fields (CRF) (Lafferty et al., 2001), the sliding window method and the recurrent sliding window method (Dietterich, 2002), are explored to find a global ranking function F. We omit the details but refer one to (Ji et al., 2009). The emphasis here is on the importance to adapt these algorithms to the ranking problem. After training, the SSL model can be used to predict the relevance labels of all the documents in a new aggregated session, and thus pair-wise preference data can be extracted, with the score difference representing the confidence of preference 1089 user1 o oo ... user2 x1 y1 o y2 x2 i = 1, 2,..., N as many as possi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Lee</author>
<author>Z Liu</author>
<author>J Cho</author>
</authors>
<title>Automatic identification of user goals in web search.</title>
<date>2005</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="8083" citStr="Lee et al., 2005" startWordPosition="1245" endWordPosition="1248">. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence of the tuple cc number of occurrence of the tuple where two documents both get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of th</context>
</contexts>
<marker>Lee, Liu, Cho, 2005</marker>
<rawString>U. Lee, Z. Liu, and J. Cho. 2005. Automatic identification of user goals in web search. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>Y Y Wang</author>
<author>A Acero</author>
</authors>
<title>Learning query intent from regularized click graphs.</title>
<date>2008</date>
<booktitle>Proceedings of ACM SIGIR Conference.</booktitle>
<contexts>
<context position="8101" citStr="Li et al., 2008" startWordPosition="1249" endWordPosition="1252">rch papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence of the tuple cc number of occurrence of the tuple where two documents both get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of the tuple where url1</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>X. Li, Y.Y Wang, and A. Acero. 2008. Learning query intent from regularized click graphs. Proceedings of ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Y Liu</author>
</authors>
<title>Learning to rank for information retrieval.</title>
<date>2008</date>
<note>SIGIR tutorial.</note>
<contexts>
<context position="1450" citStr="Liu, 2008" startWordPosition="211" endWordPosition="212">ata extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-specific ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function. 1 Introduction Learning-to-rank approaches (Liu, 2008) have been widely applied in commercial search engines, in which ranking models are learned using labeled documents. Significant efforts have been made in attempt to learn a generic ranking model which can appropriately rank documents for all queries. However, web users’ query intentions are extremely heterogeneous, which makes it difficult for a generic ranking model to achieve best ranking results for all queries. For this reason, there have been increasing needs for task specific rankings in web search such as rankings for specific query segments like long queries, time-sensitive queries, n</context>
</contexts>
<marker>Liu, 2008</marker>
<rawString>T. Y Liu. 2008. Learning to rank for information retrieval. SIGIR tutorial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qin</author>
<author>T Liu</author>
<author>X Zhang</author>
<author>D Wang</author>
<author>H Li</author>
</authors>
<title>Global ranking using continuous conditional random fields.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="13987" citStr="Qin et al., 2008" startWordPosition="2279" endWordPosition="2282">supervised learning The click modeling by sequential supervised learning (SSL) was proposed in (Ji et al., 2009), in which user’s sequential click information is exploited to extract relevance information from click-logs. This approach is reliable because 1) the sequential click information embedded in an aggregation of user clicks provides substantial relevance information of the documents displayed in the search results, and 2) the SSL is supervised learning (i.e., human judgments are provided with relevance labels for the training). The SSL is formulated in the framework of global ranking (Qin et al., 2008). Let x(q) = {x(q) 1 , x(q) 2 , ... , x(q) n } represent the documents retrieved with a query q, and y(q) = {y(q) 1 , y(q) 2,... , y(q) n } represent the relevance labels assigned to the documents. Here n is the number of documents retrieved with q. Without loss of generality, we assume that n is fixed and invariant with respect to different queries. The SSL determines to find a function F in the form of y(q)=F(x(q)) that takes all the documents as its inputs, exploiting both local and global information among the documents, and predict the relevance labels of all the document jointly. This is</context>
</contexts>
<marker>Qin, Liu, Zhang, Wang, Li, 2008</marker>
<rawString>T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. 2008. Global ranking using continuous conditional random fields. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li R Jin</author>
<author>H Valizadegan</author>
</authors>
<title>Ranking refinement and its application to information retrieval.</title>
<date>2008</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<marker>Jin, Valizadegan, 2008</marker>
<rawString>H. Li R. Jin, H. Valizadegan. 2008. Ranking refinement and its application to information retrieval. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Radlinski</author>
<author>T Joachims</author>
</authors>
<title>Active exploration for learning rankings from clickthrough data.</title>
<date>2007</date>
<booktitle>Proc. of ACM SIGKDD Conference.</booktitle>
<contexts>
<context position="7674" citStr="Radlinski and Joachims, 2007" startWordPosition="1179" endWordPosition="1182">ormational, navigational and transactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to</context>
</contexts>
<marker>Radlinski, Joachims, 2007</marker>
<rawString>F. Radlinski and T. Joachims. 2007. Active exploration for learning rankings from clickthrough data. Proc. of ACM SIGKDD Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Radlinski</author>
<author>M Kurup</author>
<author>T Joachims</author>
</authors>
<title>How does clickthrough data reflect retrieval quality?</title>
<date>2008</date>
<booktitle>Proceedings of ACM CIKM Conference.</booktitle>
<contexts>
<context position="7825" citStr="Radlinski et al., 2008" startWordPosition="1205" endWordPosition="1208">els to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuri</context>
</contexts>
<marker>Radlinski, Kurup, Joachims, 2008</marker>
<rawString>F. Radlinski, M. Kurup, and T. Joachims. 2008. How does clickthrough data reflect retrieval quality? Proceedings of ACM CIKM Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rose</author>
<author>D Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="8127" citStr="Rose and Levinson, 2004" startWordPosition="1253" endWordPosition="1256"> how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence of the tuple cc number of occurrence of the tuple where two documents both get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of the tuple where url1 is clicked but url2 is no</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>D. E. Rose and D. Levinson. 2004. Understanding user goals in web search. Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>C Zhai</author>
</authors>
<title>Learn from web search logs to organize search results.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th ACM SIGIR.</booktitle>
<contexts>
<context position="6030" citStr="Wang and Zhai, 2007" startWordPosition="917" endWordPosition="920">gard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with decision tree; RankNet (Burges et al., 2005) uses gradient boosting with neural net-work. In (Zha et </context>
<context position="7696" citStr="Wang and Zhai, 2007" startWordPosition="1183" endWordPosition="1186">ansactional, and different models are applied on each query category. a KNN method is proposed to employ different ranking models to handle different types of queries (Geng et al., 2008). The KNN method is unsupervised, and it targets to improve the overall ranking instead of the rank-ing for a certain query category. In addition, the KNN method requires all feature vector to be the same. Quite a few research papers explore how to obtain useful information from click-through data, which could benefit search relevance (Carterette et al., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are a</context>
</contexts>
<marker>Wang, Zhai, 2007</marker>
<rawString>X. Wang and C. Zhai. 2007. Learn from web search logs to organize search results. In Proceedings of the 30th ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zha</author>
<author>Z Zheng</author>
<author>H Fu</author>
<author>G Sun</author>
</authors>
<title>Incorporating query difference for learning retrieval functions in world wide web search.</title>
<date>2006</date>
<booktitle>Proceedings of the 15th ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="5933" citStr="Zha et al., 2006" startWordPosition="900" endWordPosition="903">re some extracted click preference pairs that are inconsistent with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking function from preference data. The basic idea is to minimize the number of contradicted pairs in the training data, and different algorithm cast the preference learning problem from different point of view, for example, RankSVM (Joachims, 2002) uses support vector machines; RankBoost (Freund et al., 1998) applies the idea of boosting from weak learners; GBrank (Zheng et al., 2007) uses gradient boosting with deci</context>
</contexts>
<marker>Zha, Zheng, Fu, Sun, 2006</marker>
<rawString>H. Zha, Z. Zheng, H. Fu, and G. Sun. 2006. Incorporating query difference for learning retrieval functions in world wide web search. Proceedings of the 15th ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zheng</author>
<author>H Zhang</author>
<author>T Zhang</author>
<author>O Chapelle</author>
<author>K Chen</author>
<author>G Sun</author>
</authors>
<title>A general boosting method and its application to learning ranking functions for web search.</title>
<date>2007</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="5522" citStr="Zheng et al., 2007" startWordPosition="835" endWordPosition="838">e data that does not belong the specific category, might also have similar distribution as the dedicated data. Such distribution similarity makes non-dedicated data also useful for task-specific rank function learning, especially for the scenario that dedicated training samples is insufficient. b) The quality of dedicated click data may be not as reliable as human labeled training data. In other words, there are some extracted click preference pairs that are inconsistent with human labeling while we regard human labeling as correct labeling. 3) Rank function learning algorithm: we use GBrank (Zheng et al., 2007) algorithm for rank function learning, which has proved to be one of the most effective up-to-date learning-to-rank algorithms; furthermore, GBrank algorithm also takes preference pairs as inputs, which will be illustrated with more details in the paper. 2 Related work Learning to rank has been a promising research area which continuously improves web search relevance (Burges et al., 2005) (Zha et al., 2006) (Cao et al., 2007) (Freund et al., 1998) (Friedman, 2001) (Joachims, 2002) (Wang and Zhai, 2007) (Zheng et al., 2007). The ranking problem is usually formulated as learning a ranking funct</context>
<context position="8235" citStr="Zheng et al., 2007" startWordPosition="1273" endWordPosition="1276">., 2008) (Fox et al., 2005) (Radlinski and Joachims, 2007) (Wang and Zhai, 2007). The information can be expressed as pair-wise preferences (Chapelle and Zhang, 2009) (Ji et al., 2009) (Radlinski et al., 2008), or represented as rank features (Agichtein et al., 2006). Task-specific ranking relies on the accuracy of query classification. Query classification or query intention identification has been extensively studied in (Beitzel et al., 2007) (Lee et al., 2005) (Li et al., 2008) (Rose and Levinson, 2004). How to combine editorial data and click data is well discussed in (Chen et al., 2008) (Zheng et al., 2007). In addition, how to use click data to improve ranking are also exploited in personalized or preference-based search (Coyle Click log 1087 Table 1: Statistics of click occurrences for heuristic rule approach. imp impression, number of occurrence of the tuple cc number of occurrence of the tuple where two documents both get clicked ncc number of occurrence of the tuple where url1 is not clicked but url2 is clicked cnc number of occurrence of the tuple where url1 is clicked but url2 is not clicked ncnc number of occurrence of the tuple where url1 and url2 are not clicked Table 2: Skip-above pai</context>
<context position="18354" citStr="Zheng et al., 2007" startWordPosition="3053" endWordPosition="3056"> clicks IsNextClicked 1 if next position is clicked, 0 otherwise IsPreClicked 1 if previous position is clicked, 0 otherwise IsAboveClicked 1 if there is a click above, 0 otherwise IsBelowClicked 1 if there is a click below, 0 otherwise ClickDuration Time spent on the document prediction. For the reason of convenience, we also call the preference pairs contradicting with production ranking as skip-above pairs and those consistent with production ranking as skip-next pairs, so that we can analyze these two types of preference pairs respectively. 3.2 Modeling algorithm The basic idea of GBrank (Zheng et al., 2007) is that if the ordering of a preference pair by the ranking function is contradictory to this preference, we need to modify the ranking function along the direction by swapping this prefence pair. Preferences pairs could be generated from labeled data, or could be extracted from click data. For each preference pair &lt; x, y &gt; in the available preference set S = {&lt; xi, yi &gt; |xi &gt;- yi, i = 1, 2, ..., N}, x should be ranked higher than y. In GBrank algorithm, the problem of learning ranking functions is to compute a ranking function h , so that h matches the set of preference, i.e, h(xi) &gt; h(yi) ,</context>
</contexts>
<marker>Zheng, Zhang, Zhang, Chapelle, Chen, Sun, 2007</marker>
<rawString>Z. Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen, and G. Sun. 2007. A general boosting method and its application to learning ranking functions for web search. NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>