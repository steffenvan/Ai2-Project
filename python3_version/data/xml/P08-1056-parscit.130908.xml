<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.977023">
Word Clustering and Word Selection based Feature Reduction for MaxEnt
based Hindi NER
</title>
<author confidence="0.878977">
Sujan Kumar Saha
</author>
<affiliation confidence="0.8093405">
Indian Institute of Technology
Kharagpur, West Bengal
</affiliation>
<address confidence="0.721555">
India - 721302
</address>
<email confidence="0.975981">
sujan.kr.saha@gmail.com
</email>
<author confidence="0.992479">
Pabitra Mitra
</author>
<affiliation confidence="0.880784">
Indian Institute of Technology
Kharagpur, West Bengal
</affiliation>
<address confidence="0.766052">
India - 721302
</address>
<email confidence="0.996681">
pabitra@gmail.com
</email>
<author confidence="0.988116">
Sudeshna Sarkar
</author>
<affiliation confidence="0.880074">
Indian Institute of Technology
Kharagpur, West Bengal
</affiliation>
<address confidence="0.766605">
India - 721302
</address>
<email confidence="0.997928">
shudeshna@gmail.com
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999712736842105">
Statistical machine learning methods are em-
ployed to train a Named Entity Recognizer
from annotated data. Methods like Maxi-
mum Entropy and Conditional Random Fields
make use of features for the training purpose.
These methods tend to overfit when the avail-
able training corpus is limited especially if the
number of features is large or the number of
values for a feature is large. To overcome
this we proposed two techniques for feature
reduction based on word clustering and se-
lection. A number of word similarity mea-
sures are proposed for clustering words for
the Named Entity Recognition task. A few
corpus based statistical measures are used for
important word selection. The feature reduc-
tion techniques lead to a substantial perfor-
mance improvement over baseline Maximum
Entropy technique.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999563760869565">
Named Entity Recognition (NER) involves locat-
ing and classifying the names in a text. NER is
an important task, having applications in informa-
tion extraction, question answering, machine trans-
lation and in most other Natural Language Process-
ing (NLP) applications. NER systems have been de-
veloped for English and few other languages with
high accuracy. These belong to two main cate-
gories based on machine learning (Bikel et al., 1997;
Borthwick, 1999; McCallum and Li, 2003) and lan-
guage or domain specific rules (Grishman, 1995;
Wakao et al., 1996).
In English, the names are usually capitalized
which is an important clue for identifying a name.
Absence of capitalization makes the Hindi NER task
difficult. Also, person names are more diverse in In-
dian languages, many common words being used as
names.
A pioneering work on Hindi NER is by Li and
McCallum (2003) where they used Conditional Ran-
dom Fields (CRF) and feature induction to auto-
matically construct only the features that are impor-
tant for recognition. In an effort to reduce overfit-
ting, they use a combination of a Gaussian prior and
early-stopping.
In their Maximum Entropy (MaxEnt) based ap-
proach for Hindi NER development, Saha et al.
(2008) also observed that the performance of the
MaxEnt based model often decreases when huge
number of features are used in the model. This is
due to overfitting which is a serious problem in most
of the NLP tasks in resource poor languages where
annotated data is scarce.
This paper is a study on effectiveness of word
clustering and selection as feature reduction tech-
niques for MaxEnt based NER. For clustering we
use a number of word similarities like cosine sim-
ilarity among words and co-occurrence, along with
the k-means clustering algorithm. The clusters are
then used as features instead of words. For impor-
tant word selection we use corpus based statistical
measurements to find the importance of the words in
the NER task. A significant performance improve-
ment over baseline MaxEnt was observed after using
the above feature reduction techniques.
The paper is organized as follows. The MaxEnt
</bodyText>
<page confidence="0.966422">
488
</page>
<note confidence="0.283509">
Proceedings ofACL-08: HLT, pages 488–495,
</note>
<page confidence="0.602635">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9997345">
based NER system is described in Section 2. Vari-
ous approaches for word clustering are discussed in
Section 3. Next section presents the procedure for
selecting the important words. In Section 5 experi-
mental results and related discussions are given. Fi-
nally Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.8798055" genericHeader="method">
2 Maximum Entropy Based Model for
Hindi NER
</sectionHeader>
<bodyText confidence="0.999391411764706">
Maximum Entropy (MaxEnt) principle is a com-
monly used technique which provides probability of
belongingness of a token to a class. MaxEnt com-
putes the probability p(oIh) for any o from the space
of all possible outcomes O, and for every h from
the space of all possible histories H. In NER, his-
tory can be viewed as all information derivable from
the training corpus relative to the current token. The
computation of probability (p(olh)) of an outcome
for a token in MaxEnt depends on a set of features
that are helpful in making predictions about the out-
come. The features may be binary-valued or multi-
valued. Given a set of features and a training corpus,
the MaxEnt estimation process produces a model in
which every feature fi has a weight αi. We can
compute the conditional probability as (Berger et al.,
1996):
</bodyText>
<equation confidence="0.99150025">
p(o |h) = Z1) 11 αifz(h,o) (1)
i
αifz(h,o) (2)
i
</equation>
<bodyText confidence="0.999904333333333">
The conditional probability of the outcome is the
product of the weights of all active features, normal-
ized over the products of all the features. For our
development we have used a Java based open-nlp
MaxEnt toolkit1. A beam search algorithm is used
to get the most probable class from the probabilities.
</bodyText>
<subsectionHeader confidence="0.987521">
2.1 Training Corpus
</subsectionHeader>
<bodyText confidence="0.9986095">
The training data for the Hindi NER task is com-
posed of about 243K words which is collected
from the popular daily Hindi newspaper “Dainik
Jagaran”. This corpus has been manually anno-
tated and contains about 16,491 Named Entities
(NEs). In this study we have considered 4 types
</bodyText>
<footnote confidence="0.990201">
1http://sourceforge.net/projects/maxent/
</footnote>
<table confidence="0.999573333333333">
Type Features
Word wi, wi−1, wi−2, wi+1, wi+2
NE Tag ti−1, ti−2
Digit infor- Contains digit, Only digit, Four
mation digit, Numerical word
Affix infor- Fixed length suffix, Suffix list,
mation Fixed length prefix
POS infor- POS of words, Coarse-grained
mation POS, POS based binary features
</table>
<tableCaption confidence="0.9764385">
Table 1: Features used in the MaxEnt based Hindi NER
system
</tableCaption>
<bodyText confidence="0.9995914">
of NEs, these are Person (Per), Location (Loc),
Organization (Org) and Date (Dat). To recognize
entity boundaries each name class N has 4 types
of labels: N Begin, N Continue, N End and
N Unique. For example, Kharagpur is annotated
as Loc Unique and Atal Bihari Vajpeyi is annotated
as Per Begin Per Continue Per End. Hence,
there are a total of 17 classes including one class for
not-name. The corpus contains 6298 person, 4696
location, 3652 organization and 1845 date entities.
</bodyText>
<subsectionHeader confidence="0.997585">
2.2 Feature Description
</subsectionHeader>
<bodyText confidence="0.998635227272727">
We have identified a number of candidate features
for the Hindi NER task. Several experiments were
conducted with the identified features, individually
and in combination. Some of the features are men-
tioned below. They are summarized in Table 1.
Static Word Feature: Recognition of NE is
highly dependent on contexts. So the surrounding
words of a particular word (wi) are used as fea-
tures. During our experiments different combina-
tions of previous 3 words (wi−3...wi−1) to next 3
words (wi+1...wi+3) are treated as features. This is
represented by L binary features where L is the size
of lexicon.
Dynamic NE tag: NE tags of the previous words
(ti−m...ti−1) are used as features. During decoding,
the value of this feature for a word (wi) is obtained
only after the computation of the NE tag for the pre-
vious word (wi−1).
Digit Information: If a word (wi) contains
digit(s) then the feature ContainsDigit is set to 1.
This feature is used with some modifications also.
OnlyDigit, which is set to 1 if the word contains
</bodyText>
<equation confidence="0.872111666666667">
�
Z(h) =
o
</equation>
<page confidence="0.995157">
489
</page>
<table confidence="0.999928318181818">
Feature Id Feature Per Loc Org Dat Total
F1 wi, wi−1, wi+1 61.36 68.29 52.12 88.9 67.26
F2 wi, wi−1, wi−2, wi+1, wi+2 64.10 67.81 58 92.30 69.09
F3 wi, wi−1, wi−2, wi−3, wi+1, 60.42 67.81 51.48 90.18 66.84
wi+2, wi+3
F4 wi, wi−1, wi−2, wi+1, wi+2, 66.67 73.36 58.58 89.09 71.2
ti−1, ti−2, Suffix
F5 wi, wi−1, wi+1, ti−1, Suffix 69.65 75.8 59.31 89.09 73.42
F6 wi, wi−1, wi+1, ti−1, Prefix 66.67 71 58.58 87.8 70.02
F7 wi, wi−1, wi+1, ti−1, Prefix, 70.61 71 59.31 89.09 72.5
Suffix
F8 wi, wi−1, wi+1, ti−1, Suffix, 70.61 75.8 60.54 93.8 74.26
Digit
F9 wi, wi−1, wi+1, ti−1, POS (28 64.25 71 60.54 89.09 70.39
tags)
F10 wi, wi−1, wi+1, ti−1, POS 69.65 75.8 59.31 92.82 74.16
(coarse grained)
F11 wi, wi−1, wi+1, Ti−1, Suffix, 72.26 78.6 61.36 92.82 75.6
Digit, NomPSP
F12 wi, wi−1, wi+1, wi−2, wi+2, 65.26 78.01 52.12 93.33 72.65
Ti−1, Prefix, Suffix, Digit,
NomPSP
</table>
<tableCaption confidence="0.984768">
Table 2: F-values for different features in the MaxEnt based Hindi NER system
</tableCaption>
<bodyText confidence="0.957780052631579">
only digits, 4Digit, which is set to 1 if the word
contains only 4 digits, etc. are some modifications
of the feature which are helpful.
Numerical Word: For a word (wi) if it is a nu-
merical word i.e. word denoting a number (e.g. eka2
(one), do (two), tina (three) etc.) then the feature
NumWord is set to 1.
Word Suffix: Word suffix information is helpful
to identify the NEs. Two types of suffix features
have been used. Firstly a fixed length word suffix
(set of characters occurring at the end of the word) of
the current and surrounding words are used as fea-
tures. Secondly we compiled list of common suf-
fixes of place names in Hindi. For example, pura,
bAda, nagara etc. are location suffixes. We used
binary feature corresponding to the list - whether a
given word has a suffix from the list.
Word Prefix: Prefix information of a word may
be also helpful in identifying whether it is a NE. A
</bodyText>
<footnote confidence="0.557597">
2All Hindi words are written in italics using the ‘Itrans’
transliteration.
</footnote>
<bodyText confidence="0.999785181818182">
fixed length word prefix (set of characters occur-
ring at the beginning of the word) of current and
surrounding words are treated as features. List of
important prefixes, which are used frequently in the
NEs, are also effective.
Parts-of-Speech (POS) Information: The POS
of the current word and the surrounding words are
used as feature for NER. We have used a Hindi POS
tagger developed at IIT Kharagpur, India which has
an accuracy about 90%. We have used the POS val-
ues of the current and surrounding words as features.
We realized that the detailed POS tagging is not
very relevant. Since NEs are noun phrases, the noun
tag is very relevant. Further the postposition follow-
ing a name may give a clue to the NE type. So we de-
cided to use a coarse-grained tagset with only three
tags - nominal (Nom), postposition (PSP) and other
(O).
The POS information is also used by defining sev-
eral binary features. An example is the NomPSP
binary feature. The value of this feature is defined
to be 1 if the current word is nominal and the next
</bodyText>
<page confidence="0.986839">
490
</page>
<bodyText confidence="0.803155">
word is a PSP.
</bodyText>
<subsectionHeader confidence="0.9733325">
2.3 Performance of Hindi NER using MaxEnt
Method
</subsectionHeader>
<bodyText confidence="0.9999697125">
The performance of the MaxEnt based Hindi NER
using the above mentioned features is reported here
as a baseline. We have evaluated the system us-
ing a blind test corpus of 25K words. The test
corpus contains 521 person, 728 location, 262 or-
ganization and 236 date entities. The accuracies
are measured in terms of the f-measure, which is
the weighted harmonic mean of precision and re-
call. Precision is the fraction of the correct anno-
tations and recall is the fraction of the total NEs
that are successfully annotated. The general formula
for measuring the f-measure or f-value is, Fβ _
(1+Q2) . (precision . recall) \ (Q2 . precision +
recall). Here the value of Q is taken as 1. In Table 2
we have shown the accuracy values for few feature
sets.
While experimenting with static word features,
we have observed that a window of previous and
next two words (wi−2...wi+2) gives best result
(69.09) using the word features only. But when wi−3
and wi+3 are added with it, the f-value is reduced
to 66.84. Again when wi−2 and wi+2 are deducted
from the feature set (i.e. only wi−1 and wi+1 as fea-
ture), the f-value is reduced to 67.26. This demon-
strates that wi−2 and wi+2 are helpful features in NE
identification.
When suffix, prefix and digit information are
added to the feature set, the f-value is increased upto
74.26. The value is obtained using the feature set
F8 [wi, wi−1, wi+1, ti−1, Suffix, Digit]. It is ob-
served that when wi−2 and wi+2 are added with the
feature, the accuracy decreases by 2%. It contra-
dicts the results using the word features only. An-
other interesting observation is that prefix informa-
tion are helpful features in NE identification as these
increase accuracy when separately added with the
word features (F6). Similarly the suffix information
helps in increasing the accuracy. But when both the
suffix and prefix information are used in combina-
tion along with the word features, the f-value is de-
creased. From Table 2, a f-value of 73.42 is obtained
using F5 [wi, wi−1, wi+1, ti−1, Suffix] but when
prefix information are added with it (F7), the f-value
is reduced to 72.5.
POS information are important features in NER.
In general it is observed that coarse grained POS
information performs better than the finer grained
POS information. The best accuracy (75.6 f-value)
of the baseline system is obtained using the binary
NomPSP feature along with word feature (wi−1,
wi+1), suffix and digit information. It is noted that
when wi−2, wi+2 and prefix information are added
with the best feature, the f-value is reduced to 72.65.
From the above discussion it is clear that the sys-
tem suffers from overfitting if a large number of fea-
tures are used to train the system. Note that the sur-
rounding word (wi−2, wi−1, wi+1, wi+2 etc.) fea-
tures can take any value from the lexicon and hence
are of high dimensionality. These cause the degra-
dation of performance of the system. However it is
obvious that few words in the lexicon are important
in identification of NEs.
To solve the problem of high dimensionality we
use clustering to group the words present in the cor-
pus into much smaller number of clusters. Then
the word clusters are used as features instead of
the word features (for surrounding words). For ex-
ample, our Hindi corpus contains 17,456 different
words, which are grouped into N (say 100) clusters.
Then for a particular word, it is assigned to a cluster
and the corresponding cluster-id is used as feature.
Hence the number of features is reduced to 100 in-
stead of 17,456.
Similarly, selection of important words can also
solve the problem of high dimensionality. As some
of the words in the lexicon play important role in
the NE identification process, we aim to select these
particular words. Only these important words are
used in NE identification instead of all words in the
corpus.
</bodyText>
<sectionHeader confidence="0.99312" genericHeader="method">
3 Word Clustering
</sectionHeader>
<bodyText confidence="0.9999445">
Clustering is the process of grouping together ob-
jects based on their similarity. The measure of sim-
ilarity is critical for good quality clustering. We
have experimented with some approaches to com-
pute word-word similarity. These are described in
details in the following section.
</bodyText>
<page confidence="0.997364">
491
</page>
<subsectionHeader confidence="0.783473">
3.1 Cosine Similarity based on Sentence Level
Co-occurrence
</subsectionHeader>
<bodyText confidence="0.999903666666667">
A word is represented by a binary vector of dimen-
sion same as the number of sentences in the cor-
pus. A component of the vector is 1 if the word
occurs in the corresponding sentence and zero oth-
erwise. Then we measure cosine similarity between
the word vectors. The cosine similarity between two
word vectors ( A� and B) with dimension d is mea-
This measures the number of co-occurring sen-
tences.
</bodyText>
<subsectionHeader confidence="0.8737615">
3.2 Cosine Similarity based on Proximal
Words
</subsectionHeader>
<bodyText confidence="0.999995666666667">
In this measure a word is represented by a vector
having dimension same as the lexicon size. For
ease of implementation we have taken a dimen-
sion of 2 x 200, where each component of the vec-
tor corresponds to one of the 200 most frequent
preceding and following words of a token word.
List Prev containing the most frequent (top 200)
previous words (wi−1 or wi−2 if wi is the first word
of a NE) and List Next contains 200 most frequent
next words (wi+1 or wi+2 if wi is the last word of a
NE). A particular word wk may occur several times
(say n) in the corpus. For each occurrence of wk
find if its previous word (wk−1 or wk−2) matches
any element of List Prev. If matches, then set 1 to
the corresponding position of the vector and set zero
to all other positions related to List Prev. Sim-
ilarly check the next word (wk+1 or wk+2) in the
List Next and find the values of the corresponding
positions. The final word vector Wk is obtained by
taking the average of all occurrences of wk. Then
the cosine similarity is measured between the word
vectors. This measures the similarity of the contexts
of the occurrences of the word in terms of the prox-
imal words.
</bodyText>
<subsectionHeader confidence="0.964711">
3.3 Similarity based on Proximity to NE
Categories
</subsectionHeader>
<bodyText confidence="0.9999612">
Here, for each word (wi) in the corpus four binary
vectors are defined corresponding to two preceding
and two following positions (i-1, i-2, i+1, i+2). Each
binary vector is of dimension five corresponding
to four NE classes (Cj) and one for the not-name
class. For a particular word wk, find all the words
occur in a particular position (say, +1). Measure
the fraction (Pj(wk)) of these words belonging to a
class Cj. The component of the word vector Wk for
the position corresponding to Cj is Pj(wk).
</bodyText>
<equation confidence="0.954458">
Pj(wk) = No. of times wk+1 is a NE of class Cj Total occurrence of wk in corpus
</equation>
<bodyText confidence="0.999091136363636">
The Euclidean distance is used to find the simi-
larity between the above word vectors as a similar-
ity measure. Some of the word vectors for the +1
position are given in Table 3. In this table we have
given the word vectors for a few Hindi words, which
are, sthita (located), shahara (city), jAkara (go), na-
gara (township), gA.nva (village), nivAsI (resident),
mishrA (a surname) and limiTeDa (ltd.). From the
table we observe that the word vectors are close for
sthita [0 0.478 0 0 0.522], shahara [0 0.585 0.001
0.024 0.39], nagara [0 0.507 0.019 0 0.474] and
gA.nva [0 0.551 0 0 0.449]. So these words are con-
sidered as close.
Word Per Loc Org Dat Not
sthita 0 0.478 0 0 0.522
shahara 0 0.585 0.001 0.024 0.39
jAkara 0 0.22 0 0 0.88
nagara 0 0.507 0.019 0 0.474
gA.nva 0 0.551 0 0 0.449
nivAsI 0.108 0.622 0 0 0.27
mishrA 0.889 0 0 0 0.111
limiTeDa 0 0 1 0 0
</bodyText>
<tableCaption confidence="0.8519525">
Table 3: Example of some word vectors for next (+1)
position (see text for glosses)
</tableCaption>
<subsectionHeader confidence="0.849308">
3.4 K-means Clustering
</subsectionHeader>
<bodyText confidence="0.9999765">
Using the above similarity measures we have used
the k-means algorithm. The seeds were randomly
selected. The value of k (number of clusters) was
varied till the best result is obtained.
</bodyText>
<sectionHeader confidence="0.984606" genericHeader="method">
4 Important Word Selection
</sectionHeader>
<bodyText confidence="0.9992065">
It is noted that not all words are equally important
in determining the NE category. Some of the words
</bodyText>
<equation confidence="0.6373268">
sured as:
COs5im(�A, B) = Ed AdBd
(� 2 (3)
d A2 d) 2 � (�d B2 d)1
1
</equation>
<page confidence="0.995616">
492
</page>
<bodyText confidence="0.9998572">
in the lexicon are typically associated with a partic-
ular NE category and hence have important role to
play in the classification process. We describe be-
low a few statistical techniques that has been used to
identify the important words.
</bodyText>
<subsectionHeader confidence="0.9935755">
4.1 Class Independent Important Word
Selection
</subsectionHeader>
<bodyText confidence="0.998484571428571">
We define context words as those which occur in
proximity of a NE. In other words, context words
are the words present in the wi−2, wi−1, wi+1
or wi+2 position if wi is a NE. Note that only a
subset of the lexicon are context words. For all
the context words, its N weight is calculated as
the ratio between the occurrence of the word as a
context word and its total number of occurrence in
the corpus. The context words having the higher
N weight are considered as important words for
NER. For our experiments we have considered top
500 words as important words.
N weight(wi) = Occurrence of wi as context word
Total occurrence of wi in corpus
</bodyText>
<subsectionHeader confidence="0.994984">
4.2 Important Words for Each Class
</subsectionHeader>
<bodyText confidence="0.999967818181818">
Similar to the class independent important word se-
lection from the contexts, important words are se-
lected for individual classes also. This is an exten-
sion of the previous context word considering only
NEs of a particular class. For person, location, or-
ganization and date classes we have considered top
150, 120, 50 and 50 words respectively as impor-
tant words. Four binary features are also defined for
these four classes. These are defined as having value
1 if any of the context words belongs to the impor-
tant words list for a particular class.
</bodyText>
<subsectionHeader confidence="0.998914">
4.3 Important Words for Each Position
</subsectionHeader>
<bodyText confidence="0.9998208">
Position based important words are also selected
from the corpus. Here instead of context, particu-
lar positions are considered. Four lists are compiled
for two preceding and two following positions (-2,
-1, +1 and +2).
</bodyText>
<sectionHeader confidence="0.880489" genericHeader="evaluation">
5 Evaluation of NE Recognition
</sectionHeader>
<bodyText confidence="0.999179666666667">
The following subsections contain the experimental
results using word clustering and important word se-
lection. The results demonstrate the effectiveness of
</bodyText>
<table confidence="0.999172875">
k Per Loc Org Dat Total
20 66.33 74.57 43.64 91.30 69.54
50 64.13 76.35 52 93.62 71.7
80 66.33 74.57 53.85 93.62 72.08
100 70.1 73.1 57.7 96.62 72.78
120 66.15 73.43 54.9 93.62 71.52
150 66.88 74.94 53.06 95.65 72.33
200 66.09 73.82 52 92 71.13
</table>
<tableCaption confidence="0.986664">
Table 4: Variation of MaxEnt based system accuracy de-
pending on number of clusters (k)
</tableCaption>
<bodyText confidence="0.889401">
word clustering and important word selection over
the baseline MaxEnt model.
</bodyText>
<subsectionHeader confidence="0.999394">
5.1 Using Word Clusters
</subsectionHeader>
<bodyText confidence="0.999941322580645">
To evaluate the effectiveness of the clustering ap-
proaches in Hindi NER, we have used cluster fea-
tures instead of word features. For the surrounding
words, corresponding cluster-ids are used as feature.
Choice of k : We have already mentioned that,
for k-means clustering number of classes (k) should
be determined initially. To find suitable k we had
conducted the following experiments. We have se-
lected a feature F1 (mentioned in Table 2) and ap-
plied the clusters with different k as features replac-
ing the word features. In Table 4 we have summa-
rized the experimental results, in order to find a suit-
able k for clustering, the word vectors obtained us-
ing the procedure described in Section 3.3. From
the table we observe that the best result is obtained
when k is 100. We have used k = 100 for the sub-
sequent experiments for comparing the effectiveness
of the features. Similarly when we deal with all the
words in the corpus (17,465 words), we got best re-
sults when the words are clustered into 1100 clus-
ters. Q
The details of the comparison between the base-
line word features and the reduced features obtained
using clustering are given in Table 5. In general it
is observed that clustering has improved the perfor-
mance over baseline features. Using only cluster
features the system provides a maximum f-value of
74.26 where the corresponding word features give
f-value of 69.09.
Among the various similarity measures of clus-
tering, improved results are obtained using the clus-
</bodyText>
<page confidence="0.997608">
493
</page>
<table confidence="0.99973975">
Feature Using Using Using Using
Word Clusters Clusters Clusters
Features (C1) (C2) (C3)
wz, window(-1, +1) 67.26 69.67 72.05 72.78
wz, window(-2, +2) 69.09 71.52 72.65 74.26
wz, window(-1, +1), Suffix 73.42 74.24 75.44 75.84
wz, window(-1, +1), Prefix, Suffix 72.5 74.76 75.7 76.33
wz, window(-1, +1), Prefix, Suffix, Digit 74.26 75.09 75.91 76.41
wz, window(-1, +1), Prefix, Suffix, Digit, 75.6 77.2 77.39 77.61
NomPSP
wz, window(-2, +2), Prefix, Suffix, Digit, 72.65 77.86 78.61 79.03
NomPSP
</table>
<tableCaption confidence="0.817074">
Table 5: F-values for different features in a MaxEnt based Hindi NER with clustering based feature reduction
[window(−m, +n) refers to the cluster or word features corresponding to previous m positions and next n posi-
tions; C1 is the clusters which use sentence level co-occurrence based cosine similarity (3.1), C2 denotes the clusters
which use proximal word based cosine similarity (3.2), C3 denotes the clusters for each positions related to NE (3.3)]
</tableCaption>
<bodyText confidence="0.99997475">
ters which uses the similarity measurement based on
proximity of the words to NE categories (defined in
Section 3.3).
Using clustering features the best f-value (79.03)
is obtained using clusters for previous two and next
two words along with the suffix, prefix, digit and
POS information.
It is observed that the prefix information increases
the accuracy if applied along with suffix informa-
tion when cluster features are used. More interest-
ingly, addition of cluster features for positions −2
and +2 over the feature [window(-1, +1), Suffix,
Prefix, Digit, NomPSP] increase the f-value from
77.61 to 79.03. But in the baseline system addition
of word features (wz_2 and wz+2) over the same fea-
ture decrease the f-value from 75.6 to 72.65.
</bodyText>
<subsectionHeader confidence="0.998802">
5.2 Using Important Word Selection
</subsectionHeader>
<bodyText confidence="0.999973444444445">
The details of the comparison between the word fea-
ture and the reduced features based on important
word selection are given in Table 6. For the sur-
rounding word features, find whether the particular
word (e.g. at position -1, -2 etc.) presents in the
important words list (corresponding to the particu-
lar position if position based important words are
considered). If the word occurs in the list then the
word is used as features. In general it is observed
that word selection also improves performance over
baseline features. Among the different approaches,
the best result is obtained when important words for
two preceding and two following positions (defined
in Section 4.3) are selected. Using important word
based features, the highest f-value of 79.85 is ob-
tained by using the important words for previous two
and next two positions along with the suffix, prefix,
digit and POS information.
</bodyText>
<subsectionHeader confidence="0.9732325">
5.3 Relative Effectiveness of Clustering and
Word Selection
</subsectionHeader>
<bodyText confidence="0.9999922">
In most of the cases clustering based features per-
form better then the important word based feature
reduction. But the best f-value (79.85) of the sys-
tem (using the clustering based and important word
based features separately) is obtained by using im-
portant word based features.
Next we have made an experiment by consider-
ing both the clusters and important words combined.
We have defined the combined feature as, if the word
(wz) is in the corresponding important word list then
the word is used as feature otherwise the correspond-
ing cluster-id (in which wz belongs to) is considered
as feature. Using the combined feature, we have
achieved further improvement. Here we are able to
achieve the highest f-value of 80.01.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9991205">
A hierarchical word clustering technique, where
clusters are driven automatically from large unan-
</bodyText>
<page confidence="0.99815">
494
</page>
<table confidence="0.999443916666667">
Feature Using Using Using Using
Word Words Words Words
Features (I1) (I2) (I3)
wz, window(-1, +1) 67.26 66.31 67.53 66.8
wz, window(-2, +2) 69.09 72.04 72.9 73.34
wz, window(-1, +1), Suffix 73.42 73.85 73.12 74.61
wz, window(-1, +1), Prefix, Suffix 72.5 73.52 73.94 74.87
wz, window(-1, +1), Prefix, Suffix, Digit 74.26 73.97 74.13 74.7
wz, window(-1, +1), Prefix, Suffix, Digit, 75.6 75.84 76.6 77.22
NomPSP
wz, window(-2, +2), Prefix, Suffix, Digit, 72.65 76.69 77.42 79.85
NomPSP
</table>
<tableCaption confidence="0.67021975">
Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction
[window(−m, +n) refers to the important word or baseline word features corresponding to previous m positions and
next n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2),
I3 denotes the important words for each positions (4.3)]
</tableCaption>
<bodyText confidence="0.999861166666667">
notated corpus, is used by Miller et al. (2004) for
augmenting annotated training data. Note that our
clustering approach is different, where the clusters
are obtained using some statistics derived from the
annotated corpus, and also the purpose is different
as we have used the clusters for feature reduction.
In this paper we propose two feature reduction
techniques for Hindi NER based on word cluster-
ing and word selection. A number of word similar-
ity measures are used for clustering. A few statisti-
cal approaches are used for the selection of impor-
tant words. It is observed that significant enhance-
ment of accuracy over the baseline system which use
word features is obtained. This is probably due to
reduction of overfitting. This is more important for
a resource poor languages like Hindi where there is
scarcity in annotated training data and other NER
resources (like, gazetteer lists).
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9984575">
The work is partially funded by Microsoft Research
India.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655571428572">
Berger A L, Pietra S D and Pietra V D 1996. A Maxi-
mum Entropy Approach to Natural Language Process-
ing. Computational Linguistic, 22(1):39–71.
Bikel D M, Miller S, Schwartz R and W Ralph. 1997.
Nymble: A High Performance Learning Name-finder.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 194–201.
Borthwick A. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, Computer
Science Department, New York University.
Grishman R. 1995. The New York University System
MUC-6 or Where’s the syntax? In Proceedings of the
Sixth Message Understanding Conference.
Li W and McCallum A. 2003. Rapid Development of
Hindi Named Entity Recognition using Conditional
Random Fields and Feature Induction. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 2(3):290–294.
McCallum A and Li W. 2003. Early Results for Named
Entity Recognition with Conditional Random fields,
feature induction and web-enhanced lexicons. In Pro-
ceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL.
Miller S, Guinness J and Zamanian A. 2004. Name Tag-
ging with Word Clusters and Discriminative Training.
In Proceedings of the HLT-NAACL 2004, pages 337–
342.
Saha S K, Sarkar S and Mitra P. 2008. A Hybrid Fea-
ture Set based Maximum Entropy Hindi Named En-
tity Recognition. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP-08), pages 343–349.
Wakao T, Gaizauskas R and Wilks Y. 1996. Evaluation
of an algorithm for the recognition and classification
of proper names. In Proceedings of COLING-96.
</reference>
<page confidence="0.99913">
495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521021">
<title confidence="0.968695">Word Clustering and Word Selection based Feature Reduction for MaxEnt based Hindi NER</title>
<author confidence="0.987751">Sujan Kumar Saha</author>
<affiliation confidence="0.999947">Indian Institute of Technology</affiliation>
<address confidence="0.958962">Kharagpur, West Bengal India - 721302</address>
<email confidence="0.999635">sujan.kr.saha@gmail.com</email>
<author confidence="0.983851">Pabitra Mitra</author>
<affiliation confidence="0.999948">Indian Institute of Technology</affiliation>
<address confidence="0.958808">Kharagpur, West Bengal India - 721302</address>
<email confidence="0.999429">pabitra@gmail.com</email>
<author confidence="0.990034">Sudeshna Sarkar</author>
<affiliation confidence="0.999939">Indian Institute of Technology</affiliation>
<address confidence="0.958668">Kharagpur, West Bengal India - 721302</address>
<email confidence="0.999763">shudeshna@gmail.com</email>
<abstract confidence="0.98681365">Statistical machine learning methods are employed to train a Named Entity Recognizer from annotated data. Methods like Maximum Entropy and Conditional Random Fields make use of features for the training purpose. These methods tend to overfit when the available training corpus is limited especially if the number of features is large or the number of values for a feature is large. To overcome this we proposed two techniques for feature reduction based on word clustering and selection. A number of word similarity measures are proposed for clustering words for the Named Entity Recognition task. A few corpus based statistical measures are used for important word selection. The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistic,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4609" citStr="Berger et al., 1996" startWordPosition="742" endWordPosition="745">of all possible outcomes O, and for every h from the space of all possible histories H. In NER, history can be viewed as all information derivable from the training corpus relative to the current token. The computation of probability (p(olh)) of an outcome for a token in MaxEnt depends on a set of features that are helpful in making predictions about the outcome. The features may be binary-valued or multivalued. Given a set of features and a training corpus, the MaxEnt estimation process produces a model in which every feature fi has a weight αi. We can compute the conditional probability as (Berger et al., 1996): p(o |h) = Z1) 11 αifz(h,o) (1) i αifz(h,o) (2) i The conditional probability of the outcome is the product of the weights of all active features, normalized over the products of all the features. For our development we have used a Java based open-nlp MaxEnt toolkit1. A beam search algorithm is used to get the most probable class from the probabilities. 2.1 Training Corpus The training data for the Hindi NER task is composed of about 243K words which is collected from the popular daily Hindi newspaper “Dainik Jagaran”. This corpus has been manually annotated and contains about 16,491 Named En</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger A L, Pietra S D and Pietra V D 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistic, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>W Ralph</author>
</authors>
<title>Nymble: A High Performance Learning Name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="1657" citStr="Bikel et al., 1997" startWordPosition="250" endWordPosition="253"> measures are used for important word selection. The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique. 1 Introduction Named Entity Recognition (NER) involves locating and classifying the names in a text. NER is an important task, having applications in information extraction, question answering, machine translation and in most other Natural Language Processing (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to </context>
</contexts>
<marker>Bikel, Miller, Schwartz, Ralph, 1997</marker>
<rawString>Bikel D M, Miller S, Schwartz R and W Ralph. 1997. Nymble: A High Performance Learning Name-finder. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, New York University.</institution>
<contexts>
<context position="1674" citStr="Borthwick, 1999" startWordPosition="254" endWordPosition="255">or important word selection. The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique. 1 Introduction Named Entity Recognition (NER) involves locating and classifying the names in a text. NER is an important task, having applications in information extraction, question answering, machine translation and in most other Natural Language Processing (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfittin</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Borthwick A. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, Computer Science Department, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
</authors>
<title>The New York University System MUC-6 or Where’s the syntax?</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference.</booktitle>
<contexts>
<context position="1752" citStr="Grishman, 1995" startWordPosition="267" endWordPosition="268">tial performance improvement over baseline Maximum Entropy technique. 1 Introduction Named Entity Recognition (NER) involves locating and classifying the names in a text. NER is an important task, having applications in information extraction, question answering, machine translation and in most other Natural Language Processing (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfitting, they use a combination of a Gaussian prior and early-stopping. In their Max</context>
</contexts>
<marker>Grishman, 1995</marker>
<rawString>Grishman R. 1995. The New York University System MUC-6 or Where’s the syntax? In Proceedings of the Sixth Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="2089" citStr="Li and McCallum (2003)" startWordPosition="323" endWordPosition="326"> (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfitting, they use a combination of a Gaussian prior and early-stopping. In their Maximum Entropy (MaxEnt) based approach for Hindi NER development, Saha et al. (2008) also observed that the performance of the MaxEnt based model often decreases when huge number of features are used in the model. This is due to overfitting which is a serious problem in most of the NLP tasks in resource poor languages where annotated dat</context>
</contexts>
<marker>Li, McCallum, 2003</marker>
<rawString>Li W and McCallum A. 2003. Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):290–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL.</booktitle>
<contexts>
<context position="1698" citStr="McCallum and Li, 2003" startWordPosition="256" endWordPosition="259"> selection. The feature reduction techniques lead to a substantial performance improvement over baseline Maximum Entropy technique. 1 Introduction Named Entity Recognition (NER) involves locating and classifying the names in a text. NER is an important task, having applications in information extraction, question answering, machine translation and in most other Natural Language Processing (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfitting, they use a combinatio</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>McCallum A and Li W. 2003. Early Results for Named Entity Recognition with Conditional Random fields, feature induction and web-enhanced lexicons. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name Tagging with Word Clusters and Discriminative Training.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="26335" citStr="Miller et al. (2004)" startWordPosition="4494" endWordPosition="4497">4.26 73.97 74.13 74.7 wz, window(-1, +1), Prefix, Suffix, Digit, 75.6 75.84 76.6 77.22 NomPSP wz, window(-2, +2), Prefix, Suffix, Digit, 72.65 76.69 77.42 79.85 NomPSP Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction [window(−m, +n) refers to the important word or baseline word features corresponding to previous m positions and next n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2), I3 denotes the important words for each positions (4.3)] notated corpus, is used by Miller et al. (2004) for augmenting annotated training data. Note that our clustering approach is different, where the clusters are obtained using some statistics derived from the annotated corpus, and also the purpose is different as we have used the clusters for feature reduction. In this paper we propose two feature reduction techniques for Hindi NER based on word clustering and word selection. A number of word similarity measures are used for clustering. A few statistical approaches are used for the selection of important words. It is observed that significant enhancement of accuracy over the baseline system </context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Miller S, Guinness J and Zamanian A. 2004. Name Tagging with Word Clusters and Discriminative Training. In Proceedings of the HLT-NAACL 2004, pages 337– 342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Saha</author>
<author>S Sarkar</author>
<author>P Mitra</author>
</authors>
<title>A Hybrid Feature Set based Maximum Entropy Hindi Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP-08),</booktitle>
<pages>343--349</pages>
<contexts>
<context position="2434" citStr="Saha et al. (2008)" startWordPosition="380" endWordPosition="383">ized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfitting, they use a combination of a Gaussian prior and early-stopping. In their Maximum Entropy (MaxEnt) based approach for Hindi NER development, Saha et al. (2008) also observed that the performance of the MaxEnt based model often decreases when huge number of features are used in the model. This is due to overfitting which is a serious problem in most of the NLP tasks in resource poor languages where annotated data is scarce. This paper is a study on effectiveness of word clustering and selection as feature reduction techniques for MaxEnt based NER. For clustering we use a number of word similarities like cosine similarity among words and co-occurrence, along with the k-means clustering algorithm. The clusters are then used as features instead of words</context>
</contexts>
<marker>Saha, Sarkar, Mitra, 2008</marker>
<rawString>Saha S K, Sarkar S and Mitra P. 2008. A Hybrid Feature Set based Maximum Entropy Hindi Named Entity Recognition. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP-08), pages 343–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wakao</author>
<author>R Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>Evaluation of an algorithm for the recognition and classification of proper names.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96.</booktitle>
<contexts>
<context position="1773" citStr="Wakao et al., 1996" startWordPosition="269" endWordPosition="272"> improvement over baseline Maximum Entropy technique. 1 Introduction Named Entity Recognition (NER) involves locating and classifying the names in a text. NER is an important task, having applications in information extraction, question answering, machine translation and in most other Natural Language Processing (NLP) applications. NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name. Absence of capitalization makes the Hindi NER task difficult. Also, person names are more diverse in Indian languages, many common words being used as names. A pioneering work on Hindi NER is by Li and McCallum (2003) where they used Conditional Random Fields (CRF) and feature induction to automatically construct only the features that are important for recognition. In an effort to reduce overfitting, they use a combination of a Gaussian prior and early-stopping. In their Maximum Entropy (MaxEnt)</context>
</contexts>
<marker>Wakao, Gaizauskas, Wilks, 1996</marker>
<rawString>Wakao T, Gaizauskas R and Wilks Y. 1996. Evaluation of an algorithm for the recognition and classification of proper names. In Proceedings of COLING-96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>