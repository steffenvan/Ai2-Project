<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9915305">
A noisy-channel model of rational human sentence comprehension under
uncertain input
</title>
<author confidence="0.937295">
Abstract Roger Levy
</author>
<affiliation confidence="0.9862305">
Department of Linguistics
University of California – San Diego
</affiliation>
<address confidence="0.9100115">
9500 Gilman Drive #0108
La Jolla, CA 92093-0108
</address>
<email confidence="0.855337">
rlevy@ling.ucsd.edu ∗
</email>
<sectionHeader confidence="0.888362" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999556769230769">
Language comprehension, as with all other
cases of the extraction of meaningful struc-
ture from perceptual input, takes places un-
der noisy conditions. If human language
comprehension is a rational process in the
sense of making use of all available infor-
mation sources, then we might expect uncer-
tainty at the level of word-level input to af-
fect sentence-level comprehension. However,
nearly all contemporary models of sentence
comprehension assume clean input—that is,
that the input to the sentence-level com-
prehension mechanism is a perfectly-formed,
completely certain sequence of input tokens
(words). This article presents a simple model
of rational human sentence comprehension
under noisy input, and uses the model to in-
vestigate some outstanding problems in the
psycholinguistic literature for theories of ra-
tional human sentence comprehension. We
argue that by explicitly accounting for input-
level noise in sentence processing, our model
provides solutions for these outstanding prob-
lems and broadens the scope of theories of hu-
man sentence comprehension as rational prob-
abilistic inference.
</bodyText>
<footnote confidence="0.917477222222222">
∗Part of this work has benefited from presentation at the
21st annual meeting of the CUNY Sentence Processing Confer-
ence in Chapel Hill, NC, 14 March 2008, and at a seminar at the
Center for Research on Language, UC San Diego. I am grateful
to Klinton Bicknell, Andy Kehler, and three anonymous review-
ers for comments and suggestions, Cyril Allauzen for guidance
regarding the OpenFST library, and to Mark Johnson, Mark-
Jan Nederhof, and Noah Smith for discussion of renormalizing
weighted CFGs.
</footnote>
<bodyText confidence="0.99985178125">
Considering the adversity of the conditions under
which linguistic communication takes place in ev-
eryday life—ambiguity of the signal, environmental
competition for our attention, speaker error, and so
forth—it is perhaps remarkable that we are as suc-
cessful at it as we are. Perhaps the leading expla-
nation of this success is that (a) the linguistic sig-
nal is redundant, and (b) diverse information sources
are generally available that can help us obtain infer
the intended message (or something close enough)
when comprehending an utterance (Tanenhaus et al.,
1995; Altmann and Kamide,1999; Genzel and Char-
niak, 2002, 2003; Aylett and Turk, 2004; Keller,
2004; Levy and Jaeger, 2007). Given the difficulty
of this task coupled with the availability of redun-
dancy and useful information sources, it would seem
rational for all available information to be used to
its fullest in sentence comprehension. This idea is
either implicit or explicit in several interactivist the-
ories of probabilistic language comprehension (Ju-
rafsky, 1996; Hale, 2001; Narayanan and Jurafsky,
2002; Levy, 2008). However, these theories have
implicitly assumed a partitioning of interactivity that
distinguishes the word as a fundamental level of
linguistic information processing: word recognition
is an evidential process whose output is nonethe-
less a specific “winner-takes-all” sequence of words,
which is in turn the input to an evidential sentence-
comprehension process. It is theoretically possible
that this partition is real and is an optimal solution
to the problem of language comprehension under
gross architectural constraints that favor modularity
</bodyText>
<page confidence="0.972263">
234
</page>
<note confidence="0.962137">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 234–243,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999793911764706">
(Fodor, 1983). On the other hand, it is also possible
that this partition has been a theoretical convenience
but that, in fact, evidence at the sub-word level plays
an important role in sentence processing, and that
sentence-level information can in turn affect word
recognition. If the latter is the case, then the ques-
tion arises of how we might model this type of infor-
mation flow, and what consequences it might have
for our understanding of human language compre-
hension. This article employs the well-understood
formalisms of probabilistic context-free grammars
(PCFGs) and weighted finite-state automata (wF-
SAs) to propose a novel yet simple noisy-channel
probabilistic model of sentence comprehension un-
der circumstances where there is uncertainty about
word-level representations. Section 2 introduces this
model. We use this new model to investigate two
outstanding problems for the theory of rational sen-
tence comprehension: one involving global infer-
ence—the beliefs that a human comprehender ar-
rives at regarding the meaning of a sentence after
reading it in its entirety (Section 3)—and one involv-
ing incremental inference—the beliefs that a com-
prehender forms and updates moment by moment
while reading each part of it (Section 4). The com-
mon challenge posed by each of these problems is
an apparent failure on the part of the comprehender
to use information made available in one part of a
sentence to rule out an interpretation of another part
of the sentence that is inconsistent with this informa-
tion. In each case, we will see that the introduction
of uncertainty into the input representation, coupled
with noisy-channel inference, provides a unified so-
lution within a theory of rational comprehension.
</bodyText>
<sectionHeader confidence="0.6981665" genericHeader="method">
2 Sentence comprehension under
uncertain input
</sectionHeader>
<bodyText confidence="0.9998782">
The use of generative probabilistic grammars for
parsing is well understood (e.g., Charniak, 1997;
Collins, 1999). The problem of using a probabilistic
grammar G to find the “best parse” T for a known
input string w is formulated as1
</bodyText>
<footnote confidence="0.972742333333333">
1By assumption, G is defined such that its complete pro-
ductions T completely specify the string, such that P(wJT) is
non-zero for only one value of w.
</footnote>
<equation confidence="0.511536">
arg max PG(T |w) (I)
T
</equation>
<bodyText confidence="0.97346675">
but a generative grammar directly defines the joint
distribution PG(T, w) rather than the conditional
distribution. In this case, Bayes’ rule is used to find
the posterior:
</bodyText>
<equation confidence="0.986713">
PG(T|w) = P(T(, (II)
∝ P(T, w) (III)
</equation>
<bodyText confidence="0.9987246">
If the input string is unknown, the problem
changes. Suppose we have some noisy evidence I
that determines a probability distribution over input
strings P(w|I). We can still use Bayes’ rule to ob-
tain the posterior:
</bodyText>
<equation confidence="0.928738888888889">
P(T, I)
PG(T|I) = (IV)
P (I)
∝ P(I|T, w)P(w|T)P(T) (V)
W
Likewise, if we are focused on inferring which
words were seen given an uncertain input, we have
PG(w|I) ∝ � P(I|T,w)P(w|T)P(T) (VI)
T
</equation>
<subsectionHeader confidence="0.833698">
2.1 Uncertainty for a Known Input
</subsectionHeader>
<bodyText confidence="0.999335777777778">
This paper considers situations such as controlled
psycholinguistic experiments where we (the re-
searchers) know the sentence w∗ presented to a
comprehender, but do not know the specific input I
that the comprehender obtains. In this case, if we
are, for example, interested in the expected infer-
ences of a rational comprehender about what word
string she was exposed to, the probability distribu-
tion of interest is
</bodyText>
<equation confidence="0.997846">
P(w|w∗) = J PC(w|I, w∗)PT(I|w∗) dI (VII)
I
</equation>
<bodyText confidence="0.988449">
where PC is the probability distribution used by the
comprehender to process perceived input, and PT
is the “true” probability distribution over the inputs
</bodyText>
<page confidence="0.996595">
235
</page>
<bodyText confidence="0.9986394">
that might actually be perceived given the true sen-
tence. Since the comprehender does not observe w*
we must have conditional independence between w
and w* given I. We can then apply Bayes’ rule to
(VII) to obtain
</bodyText>
<equation confidence="0.9980795">
P(w|w*) — � PC(I)
_ PC(I |w)PC(w)
� PT(I|w*) dI
= PC(w) �� PC(I|w)PT (I|w*) dI
PC(I)
a PC(w)Q(w,w*) (X)
</equation>
<bodyText confidence="0.999965115384615">
where Q(w, w*) is proportional to the integral term
in Equation (IX). The term PC(w) corresponds
to the comprehender’s prior beliefs; the integral
term is the effect of input uncertainty. If com-
prehenders model noise rationally, then we should
have PC(I|w) = PT(I|w), and thus Q(w, w*)
becomes a symmetric, non-negative function of w
and w*; hence the effect of input uncertainty can
be modeled by a kernel function on input string
pairs. (Similar conclusions result when the poste-
rior distribution of interest is over structures T.) It
is an open question which kernel functions might
best model the inferences made in human sentence
comprehension. Most obviously the kernel func-
tion should account for noise (environmental, per-
ceptual, and attentional) introduced into the signal
en route to the neural stage of abstract sentence
processing. In addition, this kernel function might
also be a natural means of accounting for modeling
error such as disfluencies (Johnson and Charniak,
2004), word/phrase swaps, and even well-formed ut-
terances that the speaker did not intend. For pur-
poses of this paper, we limit ourselves to a simple
kernel based on the Levenshtein distance LD(w, w′)
between words and constructed in the form of a
weighted finite-state automaton (Mohri, 1997).
</bodyText>
<subsectionHeader confidence="0.999533">
2.2 The Levenshtein-distance kernel
</subsectionHeader>
<bodyText confidence="0.9912002">
Suppose that the input word string w* consists of
words wi...n. We define the Levenshtein-distance
kernel as follows. Start with a weighted finite-state
automaton in the log semiring over the vocabulary
E with states 0 ... n, state 0 being the start state
</bodyText>
<figureCaption confidence="0.928236">
Figure 1: The Levenshtein-distance kernel for multi-
word string edits. KLD(w*) is shown for E =
</figureCaption>
<bodyText confidence="0.722389916666667">
{cat,sat,a}, w* = (a cat sat), and A = 1. State 0
is the start state, and State 3 is the lone (zero-cost)
final state.
and n the (zero-cost) final state. We add two types
of arcs to this automaton: (a) substitution/deletion
arcs (i — 1, w′) —* i, i E 1, ... , n, each with cost
A LD(wi, w′), for all w′ E E U {E}; and (b) in-
sertion loop arcs (j, w′) — j, j E 0, ... , n, each
with cost ALD(E, w′), for all w′ E E.2 The result-
ing wFSA KLD(w*) defines a function over w such
that the summed weight of paths through the wFSA
accepting w is log Q(w, w*). This kernel allows for
the possibility of word substitutions (represented by
the transition arcs with labels that are neither wi nor
E), word deletions (represented by the transition arcs
with E labels), and even word insertions (represented
by the loop arcs). The unnormalized probability of
each type of operation is exponential in the Leven-
shtein distance of the change induced by the oper-
ation. The term A is a free parameter, with smaller
values corresponding to noisier input. Figure 1 gives
an example of the Levenshtein-distance kernel for a
simple vocabulary and sentence.3
2For purposes of computing the Levenshtein distance be-
tween words, the epsilon label a is considered to be a zero-
length letter string.
3The Levenshtein-distance kernel can be seen to be sym-
metric in w, w∗ as follows. Any path accepting w in the
wFSA generated from w∗ involves the following non-zero-
cost transitions: insertions w′I�...i, deletions wD�...j, and substi-
tutions (w --+ w′)S�...k. For each such path P, there will be
exactly one path P′ in the wFSA generated from w that ac-
cepts w∗ with insertions wD�...j, deletions w′I�...i, and substitu-
tions (w′ --+ w)S�...k. Due to the symmetry of the Levenshtein
distance, the paths P and P′ will have identical costs. There-
fore the kernel is indeed symmetric.
</bodyText>
<figure confidence="0.99717975">
sat/3
sat/3
sat/3
sat/3
cat/3
cat/3
cat/3
cat/3
a/1
&lt;eps&gt;/1
a/1
&lt;eps&gt;/3
a/1
&lt;eps&gt;/3
a/1
0
a
cat/2
sat/2
1
a/2
cat
sat/1
2
a/2
cat/1
sat
3
</figure>
<page confidence="0.9763">
236
</page>
<subsectionHeader confidence="0.982901">
2.3 Efficient computation of posterior beliefs
</subsectionHeader>
<bodyText confidence="0.999978526315789">
The problem of finding structures or strings with
high posterior probability given a particular input
string w∗ is quite similar to the problem faced in
the parsing of speech, where the acoustic input I to
a parser can be represented as a lattice of possible
word sequences, and the edges of the lattice have
weights determined by a model of acoustic realiza-
tion of words, P(I|w) (Collins et al., 2004; Hall
and Johnson, 2003, 2004). The two major differ-
ences between lattice parsing and our problem are
(a) we have integrated out the expected effect of
noise, which is thus implicit in our choice of kernel;
and (b) the loops in the Levenshtein-distance kernel
mean that the input to parsing is no longer a lattice.
This latter difference means that some of the tech-
niques applicable to string parsing and lattice pars-
ing – notably the computation of inside probabilities
– are no longer possible using exact methods. We
return to this difference in Sections 3 and 4.
</bodyText>
<sectionHeader confidence="0.993059" genericHeader="method">
3 Global inference
</sectionHeader>
<bodyText confidence="0.999854724137931">
One clear prediction of the uncertain-input model of
(VII)–(X) is that under appropriate circumstances,
the prior expectations PC(w) of the comprehen-
der should in principle be able to override the lin-
guistic input actually presented, so that a sentence
is interpreted as meaning—and perhaps even be-
ing—something other than it actually meant or was.
At one level, it is totally clear that comprehenders
do this on a regular basis: the ability to do this
is required for someone to act as a copy editor—
that is, to notice and (crucially) correct mistakes
on the printed page. In many cases, these types
of correction happen at a level that may be below
consciousness—thus we sometimes miss a typo but
interpret the sentences as it was intended, or ignore
the disfluency of a speaker. What has not been pre-
viously proposed in a formal model, however, is that
this can happen even when an input is a completely
grammatical sentence. Here, we argue that an ef-
fect demonstrated by Christianson et al. (2001) (see
also Ferreira et al., 2002) is an example of expec-
tations overriding input. When presented sentences
of the forms in (1) using methods that did not per-
mit rereading, and asked questions of the type Did
the man hunt the deer?, experimental participants
gave affirmative responses significantly more often
for sentences of type (1a), in which the substring the
man hunted the deer appears, than for either (1b) or
(1c).
</bodyText>
<listItem confidence="0.64403">
(1) a. While the man hunted the deer ran into
</listItem>
<bodyText confidence="0.95561895">
the woods. (GARDENPATH)
b. While the man hunted the pheasant the
deer ran into the woods. (TRANSITIVE)
c. While the man hunted, the deer ran into
the woods. (COMMA)
This result was interpreted by Christianson et al.
(2001) and Ferreira et al. (2002) as reflecting (i)
the fact that there is a syntactic garden path in
(1a)—after reading the first six words of the sen-
tence, the preferred interpretation of the substring
the man hunted the deer is as a simple clause in-
dicating that the deer was hunted by the man—and
(ii) that readers were not always successful at revis-
ing away this interpretation when they saw the dis-
ambiguating verb ran, which signals that the deer
is actually the subject of the main clause, and that
hunted must therefore be intransitive. Furthermore
(and crucially), for (1a) participants also responded
affirmatively most of the time to questions of the
type Did the deer run into the woods? This result
is a puzzle for existing models of sentence compre-
hension because no grammatical analysis exists of
any substring of (1a) for which the deer is both the
object of hunted and the subject of ran. In fact, no
formal model has yet been proposed to account for
this effect.
The uncertain-input model gives us a means of
accounting for these results, because there are near
neighbors of (1a) for which there is a global gram-
matical analysis in which either the deer or a coref-
erent NP is in fact the object of the subordinate-
clause verb hunted. In particular, inserting the word
it either before or after the deer creates such a near
neighbor:
(2) a. While the man hunted the deer it ran
into the woods.
b. While the man hunted it the deer ran
into the woods.
We formalize this intuition within our model by us-
ing the wFSA representation of the Levenshtein-
</bodyText>
<page confidence="0.90761">
237
</page>
<equation confidence="0.915405052631579">
ROOT → S PUNCT. 0.0
S → SBAR S 6.3
S → SBAR PUNCT S 4.6
PUNCT S → , S 0.0
S → NP VP 0.1
SBAR → IN S 0.0
NP → DT NN 1.9
NP → NNS 4.4
NP → NNP 3.3
NP → DT NNS 4.5
NP → PRP 1.3
NP → NN 3.1
VP → VBD RB 9.7
VP → VBD PP 2.2
VP → VBD NP 1.2
VP → VBD RP 8.3
VP → VBD 2.0
VP → VBD JJ 3.4
PP → IN NP 0.0
</equation>
<figureCaption confidence="0.952418666666667">
Figure 2: The PCFG used in the global-inference
study of Section 3. Rule weights given as negative
log-probabilities in bits.
</figureCaption>
<bodyText confidence="0.958670375">
distance kernel. A probabilistic context-free gram-
mar (PCFG) representing the comprehender’s gram-
matical knowledge can be intersected with that
wFSA using well-understood techniques, generating
a new weighted CFG (Bar-Hillel et al., 1964; Neder-
hof and Satta, 2003). This intersection thus repre-
sents the unnormalized posterior PC(T, w|w∗). Be-
cause there are loops in the wFSA generated by the
Levenshtein-distance kernel, exact normalization of
the posterior is not tractable (though see Neder-
hof and Satta, 2003; Chi, 1999; Smith and John-
son, 2007 for possible approaches to approximat-
ing the normalization constant). We can, however,
use the lazy k-best algorithm of Huang and Chiang
(2005; Algorithm 3) to obtain the word-string/parse-
tree pairs with highest posterior probability.
</bodyText>
<subsectionHeader confidence="0.982709">
3.1 Experimental Verification
</subsectionHeader>
<bodyText confidence="0.9999534">
To test our account of the rational noisy-channel in-
terpretation of sentences such as (1), we defined a
small PCFG using the phrasal rules listed in Figure
2, with rule probabilities estimated from the parsed
Brown corpus.4 Lexical rewrite probabilities were
determined using relative-frequency estimation over
the entire parsed Brown corpus. For each of the sen-
tence sets like (1) used in Experiments 1a, 1b, and 2
of Christianson et al. (2001) that have complete lex-
ical coverage in the parsed Brown corpus (22 sets
in total), a noisy-input wFSA was constructed us-
ing KLD, permitting all words occurring more than
2500 times in the parsed Brown corpus as possi-
ble edit/insertion targets.5 Figure 3 shows the av-
erage proportion of parse trees among the 100 best
parses in the intersection between this PCFG and the
wFSA for each sentence for which an interpretation
is available such that the deer or a coreferent NP is
the direct object of hunted.6 The Levenshtein dis-
tance penalty A is a free parameter in the model, but
the results are consistent for a wide range of A: in-
terpretations of type (2) are more prevalent both in
terms of number mass for (1a) than for either (1b)
or (1c). Furthermore, across 9 noise values for 22
sentence sets, there were never more interpretations
of type (2) for COMMA sentences than for the cor-
responding GARDENPATH sentences, and in only
one case were there more such interpretations for
a TRANSITIVE sentence than for the corresponding
GARDENPATH sentence.
</bodyText>
<sectionHeader confidence="0.9914945" genericHeader="method">
4 Incremental comprehension and error
identification
</sectionHeader>
<bodyText confidence="0.9999865">
We begin taking up the role of input uncertainty for
incremental comprehension by posing a question:
</bodyText>
<footnote confidence="0.957826315789474">
4Counts of these rules were obtained using
tgrep2/Tregex tree-matching patterns (Rohde,
2005; Levy and Andrew, 2006), available online at
http://idiom.ucsd.edu/˜rlevy/papers/
emnlp2008/tregex_patterns. We have also in-
vestigated the use of broad-coverage PCFGs estimated using
standard treebank-based techniques, but found that the compu-
tational cost of inference with treebank-sized grammars was
prohibitive.
5The word-frequency cutoff was introduced for computa-
tional speed; we have obtained qualitatively similar results with
lower word-frequency cutoffs.
6We took a parse tree to satisfy this criterion if the NP
the deer appeared either as the matrix-clause subject or the
embedded-clause object, and a pronoun appeared in the other
position. In a finer-grained grammatical model, number/gender
agreement would be enforced between such a pronoun and the
NP in the posterior, so that the probability mass for these parses
would be concentrated on cases where the pronoun is it.
</footnote>
<page confidence="0.994744">
238
</page>
<figure confidence="0.9930752">
GardenPath
Comma
Transitive
0 5 10 15 20
Levenshtein edit distance penalty (bits)
</figure>
<figureCaption confidence="0.954017333333333">
Figure 3: Results for 100-best global inference, as
a function of the Levenshtein distance penalty λ (in
bits).
</figureCaption>
<bodyText confidence="0.9897412">
what is the optimal way to read a sentence on a page
(Legge et al., 1997)? Presumably, the goal of read-
ing is to find a good compromise between scanning
the contents of the sentence as quickly as possible
while achieving an accurate understanding of the
sentence’s meaning. To a first approximation, hu-
mans solve this problem by reading each sentence in
a document from beginning to end, regardless of the
actual layout; whether this general solution is best
understood in terms of optimality or rather as para-
sitic on spoken language comprehension is an open
question beyond the immediate scope of the present
paper. However, about 10–15% of eye movements in
reading are regressive (Rayner, 1998), and we may
usefully refine our question to when a regressive eye
movement might be a good decision. In traditional
models of sentence comprehension, the optimal an-
swer would certainly be “never”, since past observa-
tions are known with certainty. But once uncertainty
about the past is accounted for, it is clear that there
may in principle be situations in which regressive
saccades may be the best choice.
What are these situations? One possible answer
would be: when the uncertainty (e.g., measured by
entropy) about an earlier part of the sentence is high.
There are some cases in which this is probably the
correct answer: many regressive eye movements are
very small and the consensus in the eye-movement
literature is that they represent corrections for motor
error at the saccadic level. That is, the eyes over-
shoot the intended target and regress to obtain in-
formation about what was missed. However, mo-
tor error can account only for short, isolated regres-
sions, and about one-sixth of regressions are part of
a longer series back into the sentence, into a much
earlier part of the text which has already been read.
We propose that these regressive saccades might be
the best choice when the most recent observed in-
put significantly changes the comprehender’s beliefs
about the earlier parts of the sentence. To make the
discussion more concrete, we turn to another recent
result in the psycholinguistic literature that has been
argued to be problematic for rational theories of sen-
tence comprehension.
It has been shown (Tabor et al., 2004) that sen-
tences such as (3) below induce considerable pro-
cessing difficulty at the word tossed, as measured in
word-by-word reading times:
(3) The coach smiled at the player tossed a fris-
bee. (LOCALLY COHERENT)
Both intuition and controlled experiments reveal that
this difficulty seems due at least in part to the cat-
egory ambiguity of the word tossed, which is oc-
casionally used as a participial verb but is much
more frequently used as a simple-past verb. Al-
though tossed in (3) is actually a participial verb in-
troducing a reduced relative clause (and the player
is hence its recipient), most native English speakers
find it extremely difficult not to interpret tossed as a
main verb and the player as its agent—far more dif-
ficult than for corresponding sentences in which the
critical participial verb is morphologically distinct
from the simple past form ((4a), (4c); c.f. threw) or
in which the relative clause is unreduced and thus
clearly marked ((4b), (4c)).
</bodyText>
<listItem confidence="0.988327166666667">
(4) a. The coach smiled at the player thrown a
frisbee. (LOCALLY INCOHERENT)
b. The coach smiled at the player who was
tossed a frisbee.
c. The coach smiled at the player who was
thrown a frisbee.
</listItem>
<bodyText confidence="0.9999082">
The puzzle here for rational approaches to sentence
comprehension is that the preceding top-down con-
text provided by The coach smiled at... should com-
pletely rule out the possibility of seeing a main
verb immediately after player, hence a rational com-
</bodyText>
<figure confidence="0.7962115">
0 5 10 15 20 25
# Misleading parses in top 100
</figure>
<page confidence="0.996103">
239
</page>
<bodyText confidence="0.99279">
prehender should not be distracted by the part-of-
speech ambiguity.7
</bodyText>
<subsectionHeader confidence="0.998897">
4.1 An uncertain-input solution
</subsectionHeader>
<bodyText confidence="0.99983225">
The solution we pursue to this puzzle lies in the fact
that (3) has many near-neighbor sentences in which
the word tossed is in fact a simple-past tense verb.
Several possibilities are listed below in (5):
</bodyText>
<listItem confidence="0.90396675">
(5) a. The coach who smiled at the player
tossed a frisbee.
b. The coach smiled as the player tossed a
frisbee.
c. The coach smiled and the player tossed
a frisbee.
d. The coach smiled at the player who
tossed a frisbee.
e. The coach smiled at the player that
tossed a frisbee.
f. The coach smiled at the player and
tossed a frisbee.
</listItem>
<bodyText confidence="0.999779071428571">
The basic intuition we follow is that simple-past
verb tossed is much more probable where it appears
in any of (5a)-(5f) than participial tossed is in (3).
Therefore, seeing this word causes the comprehen-
der to shift her probability distribution about the ear-
lier part of the sentence away from (3), where it had
been peaked, toward its near neighbors such as the
examples in (5). This change in beliefs about the
past is treated as an error identification signal (EIS).
In reading, a sensible response to an EIS would be
a slowdown or a regressive saccade; in spoken lan-
guage comprehension, a sensible response would be
to allocate more working memory resources to the
comprehension task.
</bodyText>
<subsectionHeader confidence="0.999652">
4.2 Quantifying the Error Identification Signal
</subsectionHeader>
<bodyText confidence="0.99976">
We quantify our proposed error identification sig-
nal as follows. Consider the probability distribution
over the input up to, but not including, a position j
in a sentence w:
</bodyText>
<footnote confidence="0.9910752">
7This preceding context sharply distinguishes (3) from
better-known, traditional garden-path sentences such as The
horse raced past the barn fell, in which preceding context can-
not be used to correctly disambiguate the part of speech of the
ambiguous verb raced.
</footnote>
<equation confidence="0.820652">
P(w[0,j)) (XI)
</equation>
<bodyText confidence="0.999982222222222">
We use the subscripting [0, j) to illustrate that this
interval is “closed” through to include the beginning
of the string, but “open” at position j—that is, it in-
cludes all material before position j but does not in-
clude anything at that position or beyond. Let us
then define the posterior distribution after seeing all
input up through and including word i as Pi(w[0,j)).
We define the EIS induced by reading a word wi as
follows:
</bodyText>
<equation confidence="0.9870782">
(XII)
D (Pi(w[0,i))||Pi−1(w[0,i)))
Pi (w)
Pi (w) log (XIII)
Pi−1 (w)
</equation>
<bodyText confidence="0.999857428571429">
where D(q||p) is the Kullback-Leibler divergence,
or relative entropy, from p to q, a natural way of
quantifying the distance between probability distri-
butions (Cover and Thomas, 1991) which has also
been argued for previously in modeling attention and
surprise in both visual and linguistic cognition (Itti
and Baldi, 2005; Levy, 2008).
</bodyText>
<subsectionHeader confidence="0.997707">
4.3 Experimental Verification
</subsectionHeader>
<bodyText confidence="0.991915142857143">
As in Section 3, we use a small probabilistic gram-
mar covering the relevant structures in the problem
domain to represent the comprehender’s knowledge,
and a wFSA based on the Levenshtein-distance ker-
nel to represent noisy input. We are interested in
comparing the EIS at the word tossed in (3) versus
the EIS at the word thrown in (4a). In this case,
the interval w[0,j) contains all the material that could
possibly have come before the word tossed/thrown,
but does not contain material at or after the position
introduced by the word itself. Loops in the prob-
abilistic grammar and the Levenshtein-distance ker-
nel pose a challenge, however, to evaluating the EIS,
because the normalization constant of the resulting
grammar/input intersection is essential to evaluat-
ing Equation (XIII). To circumvent this problem,
we eliminate loops from the kernel by allowing only
one insertion per inter-word space.8 (See Section 5
for a possible alternative).
8Technically, this involves the following transformation of
a Levenshtein-distance wFSA. First, eliminate all loop arcs.
</bodyText>
<equation confidence="0.4759375">
��
w∈{w[0,o}
</equation>
<page confidence="0.932134">
240
</page>
<table confidence="0.988614307692308">
EIS
ROOT —* S 0.00
S —* S-base CC S-base 7.3
S —* S-base 0.01
S-base —* NP-base VP 0
NP —* NP-base RC 4.1
NP —* NP-base 0.5
NP —* NP-base PP 2.0
NP-base —* DT N N 4.7
NP-base —* DT N 1.9
NP-base —* DT JJ N 3.8
NP-base —* PRP 1.0
NP-base —* NNP 3.1
VP/NP —* V NP 4.0
VP/NP —* V 0.1
VP —* V PP 2.0
VP —* V NP 0.7
VP —* V 2.9
RC —* WP S/NP 0.5
RC —* VP-pass/NP 2.0
RC —* WP FinCop VP-pass/NP 4.9
PP —* IN NP 0
S/NP —* VP 0.7
S/NP —* NP-base VP/NP 1.3
VP-pass/NP —* VBN NP 2.2
VP-pass/NP —* VBN 0.4
</table>
<figureCaption confidence="0.988232">
Figure 4: The grammar used for the incremental-
</figureCaption>
<bodyText confidence="0.8440659">
inference experiment of Section 4. Rule weights
given as negative log-probabilities in bits.
Figure 4 shows the (finite-state) probabilistic
grammar used for the study, with rule probabilities
once again determined from the parsed Brown cor-
pus using relative frequency estimation. To calcu-
late the distribution over strings after exposure to
the i-th word in the sentence, we “cut” the input
wFSA such that all transitions and arcs after state
2i+2 were removed and replaced with a sequence of
states j = 2i + 3,. . . , m, with zero-cost transitions
(j −1, w′) —* j for all w′ E EUIcl, and each new j
Next, map every state i onto a state pair in a new wFSA
(2i, 2i + 1), with all incoming arcs in i being incoming into 2i,
all outgoing arcs from i being outgoing from 2i + 1, and new
transition arcs (2i, w′) --+ 2i + 1 for each w′ E E U {e} with
cost LD(e, w′). Finally, add initial state 0 to the new wFSA
with transition arcs to state 1 for all w′ E E U {e} with cost
LD(e, w′). A final state i in the old wFSA corresponds to a
final state 2i + 1 in the new wFSA.
</bodyText>
<figure confidence="0.204511">
Levenshtein edit distance penalty (bits)
</figure>
<figureCaption confidence="0.967869666666667">
Figure 5: The Error Identification Signal (EIS) for
(3) and (4a), as a function of the Levenshtein dis-
tance penalty A (in bits)
</figureCaption>
<bodyText confidence="0.999654333333333">
being a zero-cost final state.9 Because the intersec-
tion between this “cut” wFSA and the probabilistic
grammar is loop-free, it can be renormalized, and
the EIS can be calculated without difficulty. All the
computations in this section were carried out using
the OpenFST library (Allauzen et al., 2007).
Figure 5 shows the average magnitude of the EIS
for sentences (3) versus (4a) at the critical word po-
sition tossed/thrown. Once again, the Levenshtein-
distance penalty A is a free parameter in the model,
so we show model behavior as a function of A, for
the eight sentence pairs in Experiment 1 of Tabor
et al. with complete lexical and syntactic coverage
for the grammar of Figure 4. For values of A where
the EIS is non-negligible, it is consistently larger at
the critical word (tossed in (3), thrown in (4a)) in
the COHERENT condition than in the INCOHERENT
condition. Across a range of eight noise levels, 67%
of sentence pairs had a higher EIS in the COHERENT
condition than in the INCOHERENT condition. Fur-
thermore, the cases where the INCOHERENT condi-
tion had a larger EIS occurred only for noise levels
below 1.1 and above 3.6, and the maximum such EIS
was quite small, at 0.067. Overall, the model’s be-
havior is consistent with the experimental results of
Tabor et al. (2004), and can be explained through the
intuition described at the end of Section 4.1.
</bodyText>
<footnote confidence="0.962978">
9The number of states added had little effect on results, so
long as at least as many states were added as words remained in
the sentence.
</footnote>
<figure confidence="0.97628125">
Locally coherent
Locally incoherent
0 1 2 3 4 5 6 7
0.0 0.1 0.2 0.3 0.4
</figure>
<page confidence="0.990548">
241
</page>
<sectionHeader confidence="0.997681" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999922345454546">
In this paper we have outlined a simple model of ra-
tional sentence comprehension under uncertain in-
put and explored some of the consequences for out-
standing problems in the psycholinguistic literature.
The model proposed here will require further em-
pirical investigation in order to distinguish it from
other proposals that have been made in the liter-
ature, but if our proposal turns out to be correct
it has important consequences for both the theory
of language processing and cognition more gener-
ally. Most notably, it furthers the case for ratio-
nality in sentence processing; and it eliminates one
of the longest-standing modularity hypotheses im-
plicit in work on the cognitive science of language:
a partition between systems of word recognition
and sentence comprehension (Fodor, 1983). Unlike
the pessimistic picture originally painted by Fodor,
however, the interactivist picture resulting from our
model’s joint inference over possible word strings
and structures points to many rich details that still
need to be filled in. These include questions such as
what kernel functions best account for human com-
prehenders’ modeling of noise in linguistic input,
and what kinds of algorithms might allow represen-
tations with uncertain input to be computed incre-
mentally.
The present work could also be extended in sev-
eral more technical directions. Perhaps most notable
is the problem of the normalization constant for the
posterior distribution over word strings and struc-
tures; this problem was circumvented via a k-best
approach in Section 3 and by removing loops from
the Levenshtein-distance kernel in Section 4. We
believe, however, that a more satisfactory solution
may exist via sampling from the posterior distribu-
tion over trees and strings. This may be possible
either by estimating normalizing constants for the
posterior grammar using iterative weight propaga-
tion and using them to obtain proper production rule
probabilities (Chi, 1999; Smith and Johnson, 2007),
or by using reversible-jump Markov-chain Monte
Carlo (MCMC) techniques to sample from the pos-
terior (Green, 1995), and estimating the normaliz-
ing constant with annealing-based techniques (Gel-
man and Meng, 1998) or nested sampling (Skilling,
2004). Scaling the model up for use with treebank-
size grammars is another area for technical improve-
ment.
Finally, we note that the model here could poten-
tially find practical application in grammar correc-
tion. Although the noisy channel has been in use for
many years in spelling correction, our model could
be used more generally for grammar corrections, in-
cluding insertions, deletions, and (with new noise
functions) potentially changes in word order.
</bodyText>
<sectionHeader confidence="0.998081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983940047619048">
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,
and Mohri, M. (2007). OpenFst: A general
and efficient weighted finite-state transducer library.
In Proceedings of the Ninth International Confer-
ence on Implementation and Application of Au-
tomata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
http://www.openfst.org.
Altmann, G. T. and Kamide, Y. (1999). Incremental in-
terpretation at verbs: restricting the domain of subse-
quent reference. Cognition, 73(3):247–264.
Aylett, M. and Turk, A. (2004). The Smooth Sig-
nal Redundancy Hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31–56.
Bar-Hillel, Y., Perles, M., and Shamir, E. (1964). On for-
mal properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application. Addison-Wesley.
Charniak, E. (1997). Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
AAAI, pages 598–603.
Chi, Z. (1999). Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
Christianson, K., Hollingworth, A., Halliwell, J. F., and
Ferreira, F. (2001). Thematic roles assigned along the
garden path linger. Cognitive Psychology, 42:368–
407.
Collins, C., Carpenter, B., and Penn, G. (2004). Head-
driven parsing for word lattices. In Proceedings of
ACL.
Collins, M. (1999). Head-Driven Statistical Models for
Natural Language Parsing. PhD thesis, University of
Pennsylvania.
Cover, T. and Thomas, J. (1991). Elements ofInformation
Theory. John Wiley.
Ferreira, F., Ferraro, V., and Bailey, K. G. D. (2002).
Good-enough representations in language comprehen-
sion. Current Directions in Psychological Science,
11:11–15.
</reference>
<page confidence="0.963061">
242
</page>
<reference confidence="0.999790361445783">
Fodor, J. A. (1983). The Modularity ofMind. MIT Press.
Gelman, A. and Meng, X.-L. (1998). Simulating nor-
malizing constants: from importance sampling to
bridge sampling to path sampling. Statistical Science,
13(2):163–185.
Genzel, D. and Charniak, E. (2002). Entropy rate con-
stancy in text. In Proceedings ofACL.
Genzel, D. and Charniak, E. (2003). Variation of entropy
and parse trees of sentences as a function of the sen-
tence number. In Empirical Methods in Natural Lan-
guage Processing, volume 10.
Green, P. J. (1995). Reversible jump Markov chain Monte
Carlo and Bayesian model determination. Biometrika,
82:711–732.
Hale, J. (2001). A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of NAACL, vol-
ume 2, pages 159–166.
Hall, K. and Johnson, M. (2003). Language modeling
using efficient best-first bottom-up parsing. In Pro-
ceedings of the IEEE workshop on Automatic Speech
Recognition and Understanding.
Hall, K. and Johnson, M. (2004). Attention shifting for
parsing speech. In Proceedings ofACL.
Huang, L. and Chiang, D. (2005). Better k-best parsing.
In Proceedings of the International Workshop on Pars-
ing Technologies.
Itti, L. and Baldi, P. (2005). Bayesian surprise attracts
human attention. In Advances in Neural Information
Processing Systems.
Johnson, M. and Charniak, E. (2004). A TAG-based
noisy channel model of speech repairs. In Proceed-
ings ofACL.
Jurafsky, D. (1996). A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive Sci-
ence, 20(2):137–194.
Keller, F. (2004). The entropy rate principle as a pre-
dictor of processing effort: An evaluation against eye-
tracking data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 317–324, Barcelona.
Legge, G. E., Klitz, T. S., and Tjan, B. S. (1997). Mr.
Chips: An ideal-observer model of reading. Psycho-
logical Review, 104(3):524–553.
Levy, R. (2008). Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.
Levy, R. and Andrew, G. (2006). Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proceedings of the 2006 conference on Lan-
guage Resources and Evaluation.
Levy, R. and Jaeger, T. F. (2007). Speakers optimize in-
formation density through syntactic reduction. In Ad-
vances in Neural Information Processing Systems.
Mohri, M. (1997). Finite-state transducers in language
and speech processing. Computational Linguistics,
23(2):269–311.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian model
predicts human parse preference and reading time in
sentence processing. In Advances in Neural Informa-
tion Processing Systems, volume 14, pages 59–65.
Nederhof, M.-J. and Satta, G. (2003). Probabilistic pars-
ing as intersection. In Proceedings of the International
Workshop on Parsing Technologies.
Rayner, K. (1998). Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3):372–422.
Rohde, D. (2005). TGrep2 User Manual, version 1.15
edition.
Skilling, J. (2004). Nested sampling. In Fischer, R.,
Preuss, R., and von Toussaint, U., editors, Bayesian in-
ference and maximum entropy methods in science and
engineering, number 735 in AIP Conference Proceed-
ings, pages 395–405.
Smith, N. A. and Johnson, M. (2007). Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics, 33(4):477–491.
Tabor, W., Galantucci, B., and Richardson, D. (2004).
Effects of merely local syntactic coherence on sen-
tence processing. Journal of Memory and Language,
50(4):355–370.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.,
and Sedivy, J. C. (1995). Integration of visual and
linguistic information in spoken language comprehen-
sion. Science, 268:1632–1634.
</reference>
<page confidence="0.999054">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477629">
<title confidence="0.838379">A noisy-channel model of rational human sentence comprehension uncertain input</title>
<author confidence="0.994688">Abstract Roger</author>
<affiliation confidence="0.997225">Department of University of California – San</affiliation>
<address confidence="0.9219055">9500 Gilman Drive La Jolla, CA</address>
<abstract confidence="0.99683368">Language comprehension, as with all other cases of the extraction of meaningful structure from perceptual input, takes places under noisy conditions. If human language comprehension is a rational process in the sense of making use of all available information sources, then we might expect uncertainty at the level of word-level input to affect sentence-level comprehension. However, nearly all contemporary models of sentence assume is, that the input to the sentence-level comprehension mechanism is a perfectly-formed, completely certain sequence of input tokens (words). This article presents a simple model of rational human sentence comprehension under noisy input, and uses the model to investigate some outstanding problems in the psycholinguistic literature for theories of rational human sentence comprehension. We argue that by explicitly accounting for inputlevel noise in sentence processing, our model provides solutions for these outstanding problems and broadens the scope of theories of human sentence comprehension as rational probabilistic inference. of this work has benefited from presentation at the annual meeting of the CUNY Sentence Processing Conference in Chapel Hill, NC, 14 March 2008, and at a seminar at the Center for Research on Language, UC San Diego. I am grateful to Klinton Bicknell, Andy Kehler, and three anonymous reviewers for comments and suggestions, Cyril Allauzen for guidance regarding the OpenFST library, and to Mark Johnson, Mark- Jan Nederhof, and Noah Smith for discussion of renormalizing weighted CFGs. Considering the adversity of the conditions under which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that the a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 of the 2008 Conference on Empirical Methods in Natural Language pages 234–243, October Association for Computational Linguistics (Fodor, 1983). On the other hand, it is also possible that this partition has been a theoretical convenience but that, in fact, evidence at the sub-word level plays an important role in sentence processing, and that sentence-level information can in turn affect word recognition. If the latter is the case, then the question arises of how we might model this type of information flow, and what consequences it might have for our understanding of human language comprehension. This article employs the well-understood formalisms of probabilistic context-free grammars (PCFGs) and weighted finite-state automata (wF- SAs) to propose a novel yet simple noisy-channel probabilistic model of sentence comprehension under circumstances where there is uncertainty about word-level representations. Section 2 introduces this model. We use this new model to investigate two outstanding problems for the theory of rational sencomprehension: one involving inferbeliefs that a human comprehender arrives at regarding the meaning of a sentence after reading it in its entirety (Section 3)—and one involvbeliefs that a comprehender forms and updates moment by moment while reading each part of it (Section 4). The common challenge posed by each of these problems is an apparent failure on the part of the comprehender to use information made available in one part of a sentence to rule out an interpretation of another part of the sentence that is inconsistent with this informa-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Allauzen</author>
<author>M Riley</author>
<author>J Schalkwyk</author>
<author>W Skut</author>
<author>M Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer. http://www.openfst.org.</publisher>
<contexts>
<context position="29024" citStr="Allauzen et al., 2007" startWordPosition="4885" endWordPosition="4888">nsition arcs to state 1 for all w′ E E U {e} with cost LD(e, w′). A final state i in the old wFSA corresponds to a final state 2i + 1 in the new wFSA. Levenshtein edit distance penalty (bits) Figure 5: The Error Identification Signal (EIS) for (3) and (4a), as a function of the Levenshtein distance penalty A (in bits) being a zero-cost final state.9 Because the intersection between this “cut” wFSA and the probabilistic grammar is loop-free, it can be renormalized, and the EIS can be calculated without difficulty. All the computations in this section were carried out using the OpenFST library (Allauzen et al., 2007). Figure 5 shows the average magnitude of the EIS for sentences (3) versus (4a) at the critical word position tossed/thrown. Once again, the Levenshteindistance penalty A is a free parameter in the model, so we show model behavior as a function of A, for the eight sentence pairs in Experiment 1 of Tabor et al. with complete lexical and syntactic coverage for the grammar of Figure 4. For values of A where the EIS is non-negligible, it is consistently larger at the critical word (tossed in (3), thrown in (4a)) in the COHERENT condition than in the INCOHERENT condition. Across a range of eight no</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., and Mohri, M. (2007). OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G T Altmann</author>
<author>Y Kamide</author>
</authors>
<title>Incremental interpretation at verbs: restricting the domain of subsequent reference.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>73</volume>
<issue>3</issue>
<marker>Altmann, Kamide, 1999</marker>
<rawString>Altmann, G. T. and Kamide, Y. (1999). Incremental interpretation at verbs: restricting the domain of subsequent reference. Cognition, 73(3):247–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Aylett</author>
<author>A Turk</author>
</authors>
<title>The Smooth Signal Redundancy Hypothesis: A functional explanation for relationships between redundancy, prosodic prominence, and duration in spontaneous speech.</title>
<date>2004</date>
<journal>Language and Speech,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="2496" citStr="Aylett and Turk, 2004" startWordPosition="380" endWordPosition="383">of the conditions under which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information </context>
</contexts>
<marker>Aylett, Turk, 2004</marker>
<rawString>Aylett, M. and Turk, A. (2004). The Smooth Signal Redundancy Hypothesis: A functional explanation for relationships between redundancy, prosodic prominence, and duration in spontaneous speech. Language and Speech, 47(1):31–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars. In Language and Information: Selected Essays on their Theory and Application.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="16033" citStr="Bar-Hillel et al., 1964" startWordPosition="2687" endWordPosition="2690"> S → SBAR PUNCT S 4.6 PUNCT S → , S 0.0 S → NP VP 0.1 SBAR → IN S 0.0 NP → DT NN 1.9 NP → NNS 4.4 NP → NNP 3.3 NP → DT NNS 4.5 NP → PRP 1.3 NP → NN 3.1 VP → VBD RB 9.7 VP → VBD PP 2.2 VP → VBD NP 1.2 VP → VBD RP 8.3 VP → VBD 2.0 VP → VBD JJ 3.4 PP → IN NP 0.0 Figure 2: The PCFG used in the global-inference study of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC(T, w|w∗). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of </context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Bar-Hillel, Y., Perles, M., and Shamir, E. (1964). On formal properties of simple phrase structure grammars. In Language and Information: Selected Essays on their Theory and Application. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="5533" citStr="Charniak, 1997" startWordPosition="841" endWordPosition="842">mon challenge posed by each of these problems is an apparent failure on the part of the comprehender to use information made available in one part of a sentence to rule out an interpretation of another part of the sentence that is inconsistent with this information. In each case, we will see that the introduction of uncertainty into the input representation, coupled with noisy-channel inference, provides a unified solution within a theory of rational comprehension. 2 Sentence comprehension under uncertain input The use of generative probabilistic grammars for parsing is well understood (e.g., Charniak, 1997; Collins, 1999). The problem of using a probabilistic grammar G to find the “best parse” T for a known input string w is formulated as1 1By assumption, G is defined such that its complete productions T completely specify the string, such that P(wJT) is non-zero for only one value of w. arg max PG(T |w) (I) T but a generative grammar directly defines the joint distribution PG(T, w) rather than the conditional distribution. In this case, Bayes’ rule is used to find the posterior: PG(T|w) = P(T(, (II) ∝ P(T, w) (III) If the input string is unknown, the problem changes. Suppose we have some noisy</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, E. (1997). Statistical parsing with a contextfree grammar and word statistics. In Proceedings of AAAI, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="16319" citStr="Chi, 1999" startWordPosition="2734" endWordPosition="2735">ference study of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC(T, w|w∗). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation ove</context>
<context position="32271" citStr="Chi, 1999" startWordPosition="5425" endWordPosition="5426">ections. Perhaps most notable is the problem of the normalization constant for the posterior distribution over word strings and structures; this problem was circumvented via a k-best approach in Section 3 and by removing loops from the Levenshtein-distance kernel in Section 4. We believe, however, that a more satisfactory solution may exist via sampling from the posterior distribution over trees and strings. This may be possible either by estimating normalizing constants for the posterior grammar using iterative weight propagation and using them to obtain proper production rule probabilities (Chi, 1999; Smith and Johnson, 2007), or by using reversible-jump Markov-chain Monte Carlo (MCMC) techniques to sample from the posterior (Green, 1995), and estimating the normalizing constant with annealing-based techniques (Gelman and Meng, 1998) or nested sampling (Skilling, 2004). Scaling the model up for use with treebanksize grammars is another area for technical improvement. Finally, we note that the model here could potentially find practical application in grammar correction. Although the noisy channel has been in use for many years in spelling correction, our model could be used more generally</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Chi, Z. (1999). Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Christianson</author>
<author>A Hollingworth</author>
<author>J F Halliwell</author>
<author>F Ferreira</author>
</authors>
<title>Thematic roles assigned along the garden path linger. Cognitive Psychology,</title>
<date>2001</date>
<pages>42--368</pages>
<contexts>
<context position="13123" citStr="Christianson et al. (2001)" startWordPosition="2145" endWordPosition="2148"> clear that comprehenders do this on a regular basis: the ability to do this is required for someone to act as a copy editor— that is, to notice and (crucially) correct mistakes on the printed page. In many cases, these types of correction happen at a level that may be below consciousness—thus we sometimes miss a typo but interpret the sentences as it was intended, or ignore the disfluency of a speaker. What has not been previously proposed in a formal model, however, is that this can happen even when an input is a completely grammatical sentence. Here, we argue that an effect demonstrated by Christianson et al. (2001) (see also Ferreira et al., 2002) is an example of expectations overriding input. When presented sentences of the forms in (1) using methods that did not permit rereading, and asked questions of the type Did the man hunt the deer?, experimental participants gave affirmative responses significantly more often for sentences of type (1a), in which the substring the man hunted the deer appears, than for either (1b) or (1c). (1) a. While the man hunted the deer ran into the woods. (GARDENPATH) b. While the man hunted the pheasant the deer ran into the woods. (TRANSITIVE) c. While the man hunted, th</context>
<context position="17055" citStr="Christianson et al. (2001)" startWordPosition="2846" endWordPosition="2849">r, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed Brown corpus. For each of the sentence sets like (1) used in Experiments 1a, 1b, and 2 of Christianson et al. (2001) that have complete lexical coverage in the parsed Brown corpus (22 sets in total), a noisy-input wFSA was constructed using KLD, permitting all words occurring more than 2500 times in the parsed Brown corpus as possible edit/insertion targets.5 Figure 3 shows the average proportion of parse trees among the 100 best parses in the intersection between this PCFG and the wFSA for each sentence for which an interpretation is available such that the deer or a coreferent NP is the direct object of hunted.6 The Levenshtein distance penalty A is a free parameter in the model, but the results are consi</context>
</contexts>
<marker>Christianson, Hollingworth, Halliwell, Ferreira, 2001</marker>
<rawString>Christianson, K., Hollingworth, A., Halliwell, J. F., and Ferreira, F. (2001). Thematic roles assigned along the garden path linger. Cognitive Psychology, 42:368– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Collins</author>
<author>B Carpenter</author>
<author>G Penn</author>
</authors>
<title>Headdriven parsing for word lattices.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11534" citStr="Collins et al., 2004" startWordPosition="1873" endWordPosition="1876">e kernel is indeed symmetric. sat/3 sat/3 sat/3 sat/3 cat/3 cat/3 cat/3 cat/3 a/1 &lt;eps&gt;/1 a/1 &lt;eps&gt;/3 a/1 &lt;eps&gt;/3 a/1 0 a cat/2 sat/2 1 a/2 cat sat/1 2 a/2 cat/1 sat 3 236 2.3 Efficient computation of posterior beliefs The problem of finding structures or strings with high posterior probability given a particular input string w∗ is quite similar to the problem faced in the parsing of speech, where the acoustic input I to a parser can be represented as a lattice of possible word sequences, and the edges of the lattice have weights determined by a model of acoustic realization of words, P(I|w) (Collins et al., 2004; Hall and Johnson, 2003, 2004). The two major differences between lattice parsing and our problem are (a) we have integrated out the expected effect of noise, which is thus implicit in our choice of kernel; and (b) the loops in the Levenshtein-distance kernel mean that the input to parsing is no longer a lattice. This latter difference means that some of the techniques applicable to string parsing and lattice parsing – notably the computation of inside probabilities – are no longer possible using exact methods. We return to this difference in Sections 3 and 4. 3 Global inference One clear pre</context>
</contexts>
<marker>Collins, Carpenter, Penn, 2004</marker>
<rawString>Collins, C., Carpenter, B., and Penn, G. (2004). Headdriven parsing for word lattices. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5549" citStr="Collins, 1999" startWordPosition="843" endWordPosition="844">sed by each of these problems is an apparent failure on the part of the comprehender to use information made available in one part of a sentence to rule out an interpretation of another part of the sentence that is inconsistent with this information. In each case, we will see that the introduction of uncertainty into the input representation, coupled with noisy-channel inference, provides a unified solution within a theory of rational comprehension. 2 Sentence comprehension under uncertain input The use of generative probabilistic grammars for parsing is well understood (e.g., Charniak, 1997; Collins, 1999). The problem of using a probabilistic grammar G to find the “best parse” T for a known input string w is formulated as1 1By assumption, G is defined such that its complete productions T completely specify the string, such that P(wJT) is non-zero for only one value of w. arg max PG(T |w) (I) T but a generative grammar directly defines the joint distribution PG(T, w) rather than the conditional distribution. In this case, Bayes’ rule is used to find the posterior: PG(T|w) = P(T(, (II) ∝ P(T, w) (III) If the input string is unknown, the problem changes. Suppose we have some noisy evidence I that</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="25673" citStr="Cover and Thomas, 1991" startWordPosition="4273" endWordPosition="4276">is “closed” through to include the beginning of the string, but “open” at position j—that is, it includes all material before position j but does not include anything at that position or beyond. Let us then define the posterior distribution after seeing all input up through and including word i as Pi(w[0,j)). We define the EIS induced by reading a word wi as follows: (XII) D (Pi(w[0,i))||Pi−1(w[0,i))) Pi (w) Pi (w) log (XIII) Pi−1 (w) where D(q||p) is the Kullback-Leibler divergence, or relative entropy, from p to q, a natural way of quantifying the distance between probability distributions (Cover and Thomas, 1991) which has also been argued for previously in modeling attention and surprise in both visual and linguistic cognition (Itti and Baldi, 2005; Levy, 2008). 4.3 Experimental Verification As in Section 3, we use a small probabilistic grammar covering the relevant structures in the problem domain to represent the comprehender’s knowledge, and a wFSA based on the Levenshtein-distance kernel to represent noisy input. We are interested in comparing the EIS at the word tossed in (3) versus the EIS at the word thrown in (4a). In this case, the interval w[0,j) contains all the material that could possibl</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. and Thomas, J. (1991). Elements ofInformation Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ferreira</author>
<author>V Ferraro</author>
<author>K G D Bailey</author>
</authors>
<title>Good-enough representations in language comprehension.</title>
<date>2002</date>
<booktitle>Current Directions in Psychological Science,</booktitle>
<pages>11--11</pages>
<contexts>
<context position="13156" citStr="Ferreira et al., 2002" startWordPosition="2151" endWordPosition="2154">a regular basis: the ability to do this is required for someone to act as a copy editor— that is, to notice and (crucially) correct mistakes on the printed page. In many cases, these types of correction happen at a level that may be below consciousness—thus we sometimes miss a typo but interpret the sentences as it was intended, or ignore the disfluency of a speaker. What has not been previously proposed in a formal model, however, is that this can happen even when an input is a completely grammatical sentence. Here, we argue that an effect demonstrated by Christianson et al. (2001) (see also Ferreira et al., 2002) is an example of expectations overriding input. When presented sentences of the forms in (1) using methods that did not permit rereading, and asked questions of the type Did the man hunt the deer?, experimental participants gave affirmative responses significantly more often for sentences of type (1a), in which the substring the man hunted the deer appears, than for either (1b) or (1c). (1) a. While the man hunted the deer ran into the woods. (GARDENPATH) b. While the man hunted the pheasant the deer ran into the woods. (TRANSITIVE) c. While the man hunted, the deer ran into the woods. (COMMA</context>
</contexts>
<marker>Ferreira, Ferraro, Bailey, 2002</marker>
<rawString>Ferreira, F., Ferraro, V., and Bailey, K. G. D. (2002). Good-enough representations in language comprehension. Current Directions in Psychological Science, 11:11–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Fodor</author>
</authors>
<title>The Modularity ofMind.</title>
<date>1983</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3679" citStr="Fodor, 1983" startWordPosition="551" endWordPosition="552">el of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 234–243, Honolulu, October 2008.c�2008 Association for Computational Linguistics (Fodor, 1983). On the other hand, it is also possible that this partition has been a theoretical convenience but that, in fact, evidence at the sub-word level plays an important role in sentence processing, and that sentence-level information can in turn affect word recognition. If the latter is the case, then the question arises of how we might model this type of information flow, and what consequences it might have for our understanding of human language comprehension. This article employs the well-understood formalisms of probabilistic context-free grammars (PCFGs) and weighted finite-state automata (wF</context>
<context position="31111" citStr="Fodor, 1983" startWordPosition="5247" endWordPosition="5248">linguistic literature. The model proposed here will require further empirical investigation in order to distinguish it from other proposals that have been made in the literature, but if our proposal turns out to be correct it has important consequences for both the theory of language processing and cognition more generally. Most notably, it furthers the case for rationality in sentence processing; and it eliminates one of the longest-standing modularity hypotheses implicit in work on the cognitive science of language: a partition between systems of word recognition and sentence comprehension (Fodor, 1983). Unlike the pessimistic picture originally painted by Fodor, however, the interactivist picture resulting from our model’s joint inference over possible word strings and structures points to many rich details that still need to be filled in. These include questions such as what kernel functions best account for human comprehenders’ modeling of noise in linguistic input, and what kinds of algorithms might allow representations with uncertain input to be computed incrementally. The present work could also be extended in several more technical directions. Perhaps most notable is the problem of t</context>
</contexts>
<marker>Fodor, 1983</marker>
<rawString>Fodor, J. A. (1983). The Modularity ofMind. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gelman</author>
<author>X-L Meng</author>
</authors>
<title>Simulating normalizing constants: from importance sampling to bridge sampling to path sampling.</title>
<date>1998</date>
<journal>Statistical Science,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Gelman, Meng, 1998</marker>
<rawString>Gelman, A. and Meng, X.-L. (1998). Simulating normalizing constants: from importance sampling to bridge sampling to path sampling. Statistical Science, 13(2):163–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Genzel</author>
<author>E Charniak</author>
</authors>
<title>Entropy rate constancy in text.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2467" citStr="Genzel and Charniak, 2002" startWordPosition="374" endWordPosition="378"> CFGs. Considering the adversity of the conditions under which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental lev</context>
</contexts>
<marker>Genzel, Charniak, 2002</marker>
<rawString>Genzel, D. and Charniak, E. (2002). Entropy rate constancy in text. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Genzel</author>
<author>E Charniak</author>
</authors>
<title>Variation of entropy and parse trees of sentences as a function of the sentence number.</title>
<date>2003</date>
<booktitle>In Empirical Methods in Natural Language Processing,</booktitle>
<volume>10</volume>
<marker>Genzel, Charniak, 2003</marker>
<rawString>Genzel, D. and Charniak, E. (2003). Variation of entropy and parse trees of sentences as a function of the sentence number. In Empirical Methods in Natural Language Processing, volume 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Green</author>
</authors>
<title>Reversible jump Markov chain Monte Carlo and Bayesian model determination.</title>
<date>1995</date>
<journal>Biometrika,</journal>
<pages>82--711</pages>
<marker>Green, 1995</marker>
<rawString>Green, P. J. (1995). Reversible jump Markov chain Monte Carlo and Bayesian model determination. Biometrika, 82:711–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<contexts>
<context position="2894" citStr="Hale, 2001" startWordPosition="443" endWordPosition="444">t can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 Pro</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL, volume 2, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>M Johnson</author>
</authors>
<title>Language modeling using efficient best-first bottom-up parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="11558" citStr="Hall and Johnson, 2003" startWordPosition="1877" endWordPosition="1880">metric. sat/3 sat/3 sat/3 sat/3 cat/3 cat/3 cat/3 cat/3 a/1 &lt;eps&gt;/1 a/1 &lt;eps&gt;/3 a/1 &lt;eps&gt;/3 a/1 0 a cat/2 sat/2 1 a/2 cat sat/1 2 a/2 cat/1 sat 3 236 2.3 Efficient computation of posterior beliefs The problem of finding structures or strings with high posterior probability given a particular input string w∗ is quite similar to the problem faced in the parsing of speech, where the acoustic input I to a parser can be represented as a lattice of possible word sequences, and the edges of the lattice have weights determined by a model of acoustic realization of words, P(I|w) (Collins et al., 2004; Hall and Johnson, 2003, 2004). The two major differences between lattice parsing and our problem are (a) we have integrated out the expected effect of noise, which is thus implicit in our choice of kernel; and (b) the loops in the Levenshtein-distance kernel mean that the input to parsing is no longer a lattice. This latter difference means that some of the techniques applicable to string parsing and lattice parsing – notably the computation of inside probabilities – are no longer possible using exact methods. We return to this difference in Sections 3 and 4. 3 Global inference One clear prediction of the uncertain</context>
</contexts>
<marker>Hall, Johnson, 2003</marker>
<rawString>Hall, K. and Johnson, M. (2003). Language modeling using efficient best-first bottom-up parsing. In Proceedings of the IEEE workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>M Johnson</author>
</authors>
<title>Attention shifting for parsing speech.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Hall, Johnson, 2004</marker>
<rawString>Hall, K. and Johnson, M. (2004). Attention shifting for parsing speech. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="16487" citStr="Huang and Chiang (2005" startWordPosition="2759" endWordPosition="2762">ting the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC(T, w|w∗). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed Brown corpus. For each of the sentence sets like (1) used in Experiments 1a, 1b, and 2 of Christianson et al. (2001) that have complete lexical cove</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Huang, L. and Chiang, D. (2005). Better k-best parsing. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Itti</author>
<author>P Baldi</author>
</authors>
<title>Bayesian surprise attracts human attention.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="25812" citStr="Itti and Baldi, 2005" startWordPosition="4295" endWordPosition="4298">oes not include anything at that position or beyond. Let us then define the posterior distribution after seeing all input up through and including word i as Pi(w[0,j)). We define the EIS induced by reading a word wi as follows: (XII) D (Pi(w[0,i))||Pi−1(w[0,i))) Pi (w) Pi (w) log (XIII) Pi−1 (w) where D(q||p) is the Kullback-Leibler divergence, or relative entropy, from p to q, a natural way of quantifying the distance between probability distributions (Cover and Thomas, 1991) which has also been argued for previously in modeling attention and surprise in both visual and linguistic cognition (Itti and Baldi, 2005; Levy, 2008). 4.3 Experimental Verification As in Section 3, we use a small probabilistic grammar covering the relevant structures in the problem domain to represent the comprehender’s knowledge, and a wFSA based on the Levenshtein-distance kernel to represent noisy input. We are interested in comparing the EIS at the word tossed in (3) versus the EIS at the word thrown in (4a). In this case, the interval w[0,j) contains all the material that could possibly have come before the word tossed/thrown, but does not contain material at or after the position introduced by the word itself. Loops in t</context>
</contexts>
<marker>Itti, Baldi, 2005</marker>
<rawString>Itti, L. and Baldi, P. (2005). Bayesian surprise attracts human attention. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>E Charniak</author>
</authors>
<title>A TAG-based noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8421" citStr="Johnson and Charniak, 2004" startWordPosition="1324" endWordPosition="1327">nput uncertainty can be modeled by a kernel function on input string pairs. (Similar conclusions result when the posterior distribution of interest is over structures T.) It is an open question which kernel functions might best model the inferences made in human sentence comprehension. Most obviously the kernel function should account for noise (environmental, perceptual, and attentional) introduced into the signal en route to the neural stage of abstract sentence processing. In addition, this kernel function might also be a natural means of accounting for modeling error such as disfluencies (Johnson and Charniak, 2004), word/phrase swaps, and even well-formed utterances that the speaker did not intend. For purposes of this paper, we limit ourselves to a simple kernel based on the Levenshtein distance LD(w, w′) between words and constructed in the form of a weighted finite-state automaton (Mohri, 1997). 2.2 The Levenshtein-distance kernel Suppose that the input word string w* consists of words wi...n. We define the Levenshtein-distance kernel as follows. Start with a weighted finite-state automaton in the log semiring over the vocabulary E with states 0 ... n, state 0 being the start state Figure 1: The Leve</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Johnson, M. and Charniak, E. (2004). A TAG-based noisy channel model of speech repairs. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2882" citStr="Jurafsky, 1996" startWordPosition="440" endWordPosition="442">ly available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modula</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2):137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
</authors>
<title>The entropy rate principle as a predictor of processing effort: An evaluation against eyetracking data.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>317--324</pages>
<location>Barcelona.</location>
<contexts>
<context position="2510" citStr="Keller, 2004" startWordPosition="384" endWordPosition="385"> which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: wo</context>
</contexts>
<marker>Keller, 2004</marker>
<rawString>Keller, F. (2004). The entropy rate principle as a predictor of processing effort: An evaluation against eyetracking data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 317–324, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Legge</author>
<author>T S Klitz</author>
<author>B S Tjan</author>
</authors>
<title>Mr. Chips: An ideal-observer model of reading.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>3</issue>
<contexts>
<context position="19513" citStr="Legge et al., 1997" startWordPosition="3236" endWordPosition="3239"> either as the matrix-clause subject or the embedded-clause object, and a pronoun appeared in the other position. In a finer-grained grammatical model, number/gender agreement would be enforced between such a pronoun and the NP in the posterior, so that the probability mass for these parses would be concentrated on cases where the pronoun is it. 238 GardenPath Comma Transitive 0 5 10 15 20 Levenshtein edit distance penalty (bits) Figure 3: Results for 100-best global inference, as a function of the Levenshtein distance penalty λ (in bits). what is the optimal way to read a sentence on a page (Legge et al., 1997)? Presumably, the goal of reading is to find a good compromise between scanning the contents of the sentence as quickly as possible while achieving an accurate understanding of the sentence’s meaning. To a first approximation, humans solve this problem by reading each sentence in a document from beginning to end, regardless of the actual layout; whether this general solution is best understood in terms of optimality or rather as parasitic on spoken language comprehension is an open question beyond the immediate scope of the present paper. However, about 10–15% of eye movements in reading are r</context>
</contexts>
<marker>Legge, Klitz, Tjan, 1997</marker>
<rawString>Legge, G. E., Klitz, T. S., and Tjan, B. S. (1997). Mr. Chips: An ideal-observer model of reading. Psychological Review, 104(3):524–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="2937" citStr="Levy, 2008" startWordPosition="449" endWordPosition="450">ssage (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 Proceedings of the 2008 Conference on Empirica</context>
<context position="25825" citStr="Levy, 2008" startWordPosition="4299" endWordPosition="4300">ng at that position or beyond. Let us then define the posterior distribution after seeing all input up through and including word i as Pi(w[0,j)). We define the EIS induced by reading a word wi as follows: (XII) D (Pi(w[0,i))||Pi−1(w[0,i))) Pi (w) Pi (w) log (XIII) Pi−1 (w) where D(q||p) is the Kullback-Leibler divergence, or relative entropy, from p to q, a natural way of quantifying the distance between probability distributions (Cover and Thomas, 1991) which has also been argued for previously in modeling attention and surprise in both visual and linguistic cognition (Itti and Baldi, 2005; Levy, 2008). 4.3 Experimental Verification As in Section 3, we use a small probabilistic grammar covering the relevant structures in the problem domain to represent the comprehender’s knowledge, and a wFSA based on the Levenshtein-distance kernel to represent noisy input. We are interested in comparing the EIS at the word tossed in (3) versus the EIS at the word thrown in (4a). In this case, the interval w[0,j) contains all the material that could possibly have come before the word tossed/thrown, but does not contain material at or after the position introduced by the word itself. Loops in the probabilis</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="18377" citStr="Levy and Andrew, 2006" startWordPosition="3065" endWordPosition="3068">mass for (1a) than for either (1b) or (1c). Furthermore, across 9 noise values for 22 sentence sets, there were never more interpretations of type (2) for COMMA sentences than for the corresponding GARDENPATH sentences, and in only one case were there more such interpretations for a TRANSITIVE sentence than for the corresponding GARDENPATH sentence. 4 Incremental comprehension and error identification We begin taking up the role of input uncertainty for incremental comprehension by posing a question: 4Counts of these rules were obtained using tgrep2/Tregex tree-matching patterns (Rohde, 2005; Levy and Andrew, 2006), available online at http://idiom.ucsd.edu/˜rlevy/papers/ emnlp2008/tregex_patterns. We have also investigated the use of broad-coverage PCFGs estimated using standard treebank-based techniques, but found that the computational cost of inference with treebank-sized grammars was prohibitive. 5The word-frequency cutoff was introduced for computational speed; we have obtained qualitatively similar results with lower word-frequency cutoffs. 6We took a parse tree to satisfy this criterion if the NP the deer appeared either as the matrix-clause subject or the embedded-clause object, and a pronoun a</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Levy, R. and Andrew, G. (2006). Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the 2006 conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>T F Jaeger</author>
</authors>
<title>Speakers optimize information density through syntactic reduction.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2534" citStr="Levy and Jaeger, 2007" startWordPosition="386" endWordPosition="389">tic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evi</context>
</contexts>
<marker>Levy, Jaeger, 2007</marker>
<rawString>Levy, R. and Jaeger, T. F. (2007). Speakers optimize information density through syntactic reduction. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="8709" citStr="Mohri, 1997" startWordPosition="1373" endWordPosition="1374">he kernel function should account for noise (environmental, perceptual, and attentional) introduced into the signal en route to the neural stage of abstract sentence processing. In addition, this kernel function might also be a natural means of accounting for modeling error such as disfluencies (Johnson and Charniak, 2004), word/phrase swaps, and even well-formed utterances that the speaker did not intend. For purposes of this paper, we limit ourselves to a simple kernel based on the Levenshtein distance LD(w, w′) between words and constructed in the form of a weighted finite-state automaton (Mohri, 1997). 2.2 The Levenshtein-distance kernel Suppose that the input word string w* consists of words wi...n. We define the Levenshtein-distance kernel as follows. Start with a weighted finite-state automaton in the log semiring over the vocabulary E with states 0 ... n, state 0 being the start state Figure 1: The Levenshtein-distance kernel for multiword string edits. KLD(w*) is shown for E = {cat,sat,a}, w* = (a cat sat), and A = 1. State 0 is the start state, and State 3 is the lone (zero-cost) final state. and n the (zero-cost) final state. We add two types of arcs to this automaton: (a) substitut</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mohri, M. (1997). Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>14</volume>
<pages>59--65</pages>
<contexts>
<context position="2924" citStr="Narayanan and Jurafsky, 2002" startWordPosition="445" endWordPosition="448">s obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 Proceedings of the 2008 Conferenc</context>
</contexts>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>Narayanan, S. and Jurafsky, D. (2002). A Bayesian model predicts human parse preference and reading time in sentence processing. In Advances in Neural Information Processing Systems, volume 14, pages 59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="16060" citStr="Nederhof and Satta, 2003" startWordPosition="2691" endWordPosition="2695">CT S → , S 0.0 S → NP VP 0.1 SBAR → IN S 0.0 NP → DT NN 1.9 NP → NNS 4.4 NP → NNP 3.3 NP → DT NNS 4.5 NP → PRP 1.3 NP → NN 3.1 VP → VBD RB 9.7 VP → VBD PP 2.2 VP → VBD NP 1.2 VP → VBD RP 8.3 VP → VBD 2.0 VP → VBD JJ 3.4 PP → IN NP 0.0 Figure 2: The PCFG used in the global-inference study of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC(T, w|w∗). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel </context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>Nederhof, M.-J. and Satta, G. (2003). Probabilistic parsing as intersection. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rayner</author>
</authors>
<title>Eye movements in reading and information processing: 20 years of research.</title>
<date>1998</date>
<journal>Psychological Bulletin,</journal>
<volume>124</volume>
<issue>3</issue>
<contexts>
<context position="20137" citStr="Rayner, 1998" startWordPosition="3339" endWordPosition="3340">y, the goal of reading is to find a good compromise between scanning the contents of the sentence as quickly as possible while achieving an accurate understanding of the sentence’s meaning. To a first approximation, humans solve this problem by reading each sentence in a document from beginning to end, regardless of the actual layout; whether this general solution is best understood in terms of optimality or rather as parasitic on spoken language comprehension is an open question beyond the immediate scope of the present paper. However, about 10–15% of eye movements in reading are regressive (Rayner, 1998), and we may usefully refine our question to when a regressive eye movement might be a good decision. In traditional models of sentence comprehension, the optimal answer would certainly be “never”, since past observations are known with certainty. But once uncertainty about the past is accounted for, it is clear that there may in principle be situations in which regressive saccades may be the best choice. What are these situations? One possible answer would be: when the uncertainty (e.g., measured by entropy) about an earlier part of the sentence is high. There are some cases in which this is </context>
</contexts>
<marker>Rayner, 1998</marker>
<rawString>Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124(3):372–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rohde</author>
</authors>
<title>TGrep2 User Manual, version 1.15 edition.</title>
<date>2005</date>
<contexts>
<context position="18353" citStr="Rohde, 2005" startWordPosition="3063" endWordPosition="3064">ms of number mass for (1a) than for either (1b) or (1c). Furthermore, across 9 noise values for 22 sentence sets, there were never more interpretations of type (2) for COMMA sentences than for the corresponding GARDENPATH sentences, and in only one case were there more such interpretations for a TRANSITIVE sentence than for the corresponding GARDENPATH sentence. 4 Incremental comprehension and error identification We begin taking up the role of input uncertainty for incremental comprehension by posing a question: 4Counts of these rules were obtained using tgrep2/Tregex tree-matching patterns (Rohde, 2005; Levy and Andrew, 2006), available online at http://idiom.ucsd.edu/˜rlevy/papers/ emnlp2008/tregex_patterns. We have also investigated the use of broad-coverage PCFGs estimated using standard treebank-based techniques, but found that the computational cost of inference with treebank-sized grammars was prohibitive. 5The word-frequency cutoff was introduced for computational speed; we have obtained qualitatively similar results with lower word-frequency cutoffs. 6We took a parse tree to satisfy this criterion if the NP the deer appeared either as the matrix-clause subject or the embedded-clause</context>
</contexts>
<marker>Rohde, 2005</marker>
<rawString>Rohde, D. (2005). TGrep2 User Manual, version 1.15 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Skilling</author>
</authors>
<title>Nested sampling.</title>
<date>2004</date>
<booktitle>Bayesian inference and maximum entropy methods in science and engineering, number 735 in AIP Conference Proceedings,</booktitle>
<pages>395--405</pages>
<editor>In Fischer, R., Preuss, R., and von Toussaint, U., editors,</editor>
<marker>Skilling, 2004</marker>
<rawString>Skilling, J. (2004). Nested sampling. In Fischer, R., Preuss, R., and von Toussaint, U., editors, Bayesian inference and maximum entropy methods in science and engineering, number 735 in AIP Conference Proceedings, pages 395–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>M Johnson</author>
</authors>
<title>Weighted and probabilistic context-free grammars are equally expressive.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="16344" citStr="Smith and Johnson, 2007" startWordPosition="2736" endWordPosition="2740">dy of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC(T, w|w∗). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed Brown</context>
<context position="32297" citStr="Smith and Johnson, 2007" startWordPosition="5427" endWordPosition="5430">rhaps most notable is the problem of the normalization constant for the posterior distribution over word strings and structures; this problem was circumvented via a k-best approach in Section 3 and by removing loops from the Levenshtein-distance kernel in Section 4. We believe, however, that a more satisfactory solution may exist via sampling from the posterior distribution over trees and strings. This may be possible either by estimating normalizing constants for the posterior grammar using iterative weight propagation and using them to obtain proper production rule probabilities (Chi, 1999; Smith and Johnson, 2007), or by using reversible-jump Markov-chain Monte Carlo (MCMC) techniques to sample from the posterior (Green, 1995), and estimating the normalizing constant with annealing-based techniques (Gelman and Meng, 1998) or nested sampling (Skilling, 2004). Scaling the model up for use with treebanksize grammars is another area for technical improvement. Finally, we note that the model here could potentially find practical application in grammar correction. Although the noisy channel has been in use for many years in spelling correction, our model could be used more generally for grammar corrections, </context>
</contexts>
<marker>Smith, Johnson, 2007</marker>
<rawString>Smith, N. A. and Johnson, M. (2007). Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tabor</author>
<author>B Galantucci</author>
<author>D Richardson</author>
</authors>
<title>Effects of merely local syntactic coherence on sentence processing.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="21688" citStr="Tabor et al., 2004" startWordPosition="3595" endWordPosition="3598">nt only for short, isolated regressions, and about one-sixth of regressions are part of a longer series back into the sentence, into a much earlier part of the text which has already been read. We propose that these regressive saccades might be the best choice when the most recent observed input significantly changes the comprehender’s beliefs about the earlier parts of the sentence. To make the discussion more concrete, we turn to another recent result in the psycholinguistic literature that has been argued to be problematic for rational theories of sentence comprehension. It has been shown (Tabor et al., 2004) that sentences such as (3) below induce considerable processing difficulty at the word tossed, as measured in word-by-word reading times: (3) The coach smiled at the player tossed a frisbee. (LOCALLY COHERENT) Both intuition and controlled experiments reveal that this difficulty seems due at least in part to the category ambiguity of the word tossed, which is occasionally used as a participial verb but is much more frequently used as a simple-past verb. Although tossed in (3) is actually a participial verb introducing a reduced relative clause (and the player is hence its recipient), most nat</context>
<context position="30010" citStr="Tabor et al. (2004)" startWordPosition="5059" endWordPosition="5062">ammar of Figure 4. For values of A where the EIS is non-negligible, it is consistently larger at the critical word (tossed in (3), thrown in (4a)) in the COHERENT condition than in the INCOHERENT condition. Across a range of eight noise levels, 67% of sentence pairs had a higher EIS in the COHERENT condition than in the INCOHERENT condition. Furthermore, the cases where the INCOHERENT condition had a larger EIS occurred only for noise levels below 1.1 and above 3.6, and the maximum such EIS was quite small, at 0.067. Overall, the model’s behavior is consistent with the experimental results of Tabor et al. (2004), and can be explained through the intuition described at the end of Section 4.1. 9The number of states added had little effect on results, so long as at least as many states were added as words remained in the sentence. Locally coherent Locally incoherent 0 1 2 3 4 5 6 7 0.0 0.1 0.2 0.3 0.4 241 5 Conclusion In this paper we have outlined a simple model of rational sentence comprehension under uncertain input and explored some of the consequences for outstanding problems in the psycholinguistic literature. The model proposed here will require further empirical investigation in order to disting</context>
</contexts>
<marker>Tabor, Galantucci, Richardson, 2004</marker>
<rawString>Tabor, W., Galantucci, B., and Richardson, D. (2004). Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50(4):355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tanenhaus</author>
<author>M J Spivey-Knowlton</author>
<author>K Eberhard</author>
<author>J C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="2415" citStr="Tanenhaus et al., 1995" startWordPosition="367" endWordPosition="370">ah Smith for discussion of renormalizing weighted CFGs. Considering the adversity of the conditions under which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide,1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactiv</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K., and Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>