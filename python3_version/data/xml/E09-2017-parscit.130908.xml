<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012958">
<title confidence="0.993888">
A Mobile Health and Fitness Companion Demonstrator*
</title>
<author confidence="0.969594">
Olov St˚ahl1 Bj¨orn Gamb¨ack1 2 Markku Turunen3 Jaakko Hakulinen3
</author>
<affiliation confidence="0.686801666666667">
&apos;ICE / Userware 2Dpt. Computer &amp; Information Science 3Dpt. Computer Sciences
Swedish Inst. of Computer Science Norwegian Univ. of Science and Technology Univ. of Tampere
Kista, Sweden Trondheim, Norway Tampere, Finland
</affiliation>
<email confidence="0.936168">
{olovs,gamback}@sics.se gamback@idi.ntnu.no {mturunen,jh}@cs.uta.fi
</email>
<sectionHeader confidence="0.99262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992035">
Multimodal conversational spoken dia-
logues using physical and virtual agents
provide a potential interface to motivate
and support users in the domain of health
and fitness. The paper presents a multi-
modal conversational Companion system
focused on health and fitness, which has
both a stationary and a mobile component.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998156827586207">
Spoken dialogue systems have traditionally fo-
cused on task-oriented dialogues, such as mak-
ing flight bookings or providing public transport
timetables. In emerging areas, such as domain-
oriented dialogues (Dybkjaer et al., 2004), the in-
teraction with the system, typically modelled as a
conversation with a virtual anthropomorphic char-
acter, can be the main motivation for the interac-
tion. Recent research has coined the term “Com-
panions” to describe embodied multimodal con-
versational agents having a long lasting interaction
history with their users (Wilks, 2007).
Such a conversational Companion within the
Health and Fitness (H&amp;F) domain helps its users
to a healthier lifestyle. An H&amp;F Companion has
quite different motivations for use than traditional
task-based spoken dialogue systems. Instead of
helping with a single, well-defined task, it truly
aims to be a Companion to the user, providing
social support in everyday activities. The system
should thus be a peer rather than act as an expert
system in health-related issues. It is important to
stress that it is the Companion concept which is
central, rather than the fitness area as such. Thus
it is not of vital importance that the system should
be a first-rate fitness coach, but it is essential that it
The work was funded by the European Commis-
sion’s IST priority through the project COMPANIONS
(www.companions-project.org).
</bodyText>
<figureCaption confidence="0.998697">
Figure 1: H&amp;F Companion Architecture
</figureCaption>
<bodyText confidence="0.999989235294118">
should be able to take a persistent part in the user’s
life, that is, that it should be able to follow the user
in all the user’s activities. This means that the
Companion must have mobile capabilities. Not
necessarily self-mobile (as a robot), but allowing
the user to bring the system with her, like a hand-
bag or a pair of shoes — or as a mobile phone.
The paper describes such a Health and Fitness
Companion. It has a stationary (“home”) compo-
nent accounting for the main part of the user in-
teraction and a mobile component which follows
the users in actual exercise activities. Section 2
outlines the overall system and its two basic com-
ponents, and Section 3 details the implementation.
Section 4 discusses some related work, while Sec-
tion 5 describes the demonstrator set-up and plans
for future work.
</bodyText>
<sectionHeader confidence="0.473202" genericHeader="method">
2 The Health and Fitness Companion
</sectionHeader>
<bodyText confidence="0.999947428571429">
The overall system architecture of the Health and
Fitness Companion is shown in Figure 1. The
system components communicate with each other
over a regular mobile phone network. The home
system provides an exercise plan to the mobile part
and in return gets the results of the performed ex-
ercises from the mobile component.
</bodyText>
<note confidence="0.337917">
Proceedings of the EACL 2009 Demonstrations Session, pages 65–68,
Athens, Greece, 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999494">
65
</page>
<figureCaption confidence="0.999983">
Figure 3: The Mobile Companion GUI
Figure 2: Home Companion interface (Later on in the evening)
</figureCaption>
<subsectionHeader confidence="0.985523">
2.1 The Home H&amp;F Companion
</subsectionHeader>
<bodyText confidence="0.999990366666667">
The home part of the H&amp;F Companion gathers in-
formation from the user and makes suggestions
targeted for a healthy lifestyle on a daily basis.
The Companion communicates with the user in
two main dialogue phases: a planning phase where
the system talks about the coming day with the
user and a reporting phase where the user’s actual
activities are assessed with reference to what was
agreed on earlier. The Companion can discuss the
following topics: travelling to work, getting lunch,
activities to be performed before dinner, getting
dinner, and activities to be performed after din-
ner. It knows activities such as playing football,
squash, or badminton; going to the gym or shop-
ping; and watching television or reading a book.
The Home Companion interface, illustrated
in Figure 2, uses a Nabaztag/tag WLAN rabbit
(www.nabaztag.com) as the physical agent in-
terface. The Nabaztag rabbit provides audio out-
put and push-to-talk speech input, moves its ears,
and operates four coloured lights to signal its sta-
tus. In the current implementation, the user needs
to push the button located on top of the Nabaz-
tag/tag device in order to speak to it. As an alter-
native, the system supports external micro-phones
and loudspeakers. The user is also able to pick a
topic by showing RFID tags (embedded in picture
cards or other objects).
The following dialogue example demonstrates
the interaction with the Companion.
</bodyText>
<footnote confidence="0.9073544">
C1: Do you have anything arranged for today?
U1: I’m playing Squash
C2: Is that before dinner?
U2: Yes
C3: Okay. Are you going to get the train to work?
U3: Yes
C4: Right. Could you take in a packed lunch to work?
U4: Yes, that would be fine
C6: Welcome back! How did your game of Squash go?
U5: The game was cancelled, so I went to the gym
</footnote>
<listItem confidence="0.61095975">
C7: Okay. Did you take the train to work?
U6: Yes, I did
C8: Right. Did you take in a packed lunch?
U7: No. I was in a rush so I went to the cafeteria
</listItem>
<subsectionHeader confidence="0.991362">
2.2 The Mobile H&amp;F Companion
</subsectionHeader>
<bodyText confidence="0.99999115625">
The mobile part of the H&amp;F Companion runs on a
mobile handset (e.g., a PDA), and is used during
physical exercise (e.g., while running or walking)
to track the distance, pace, duration, and calories
burned. The data gathered during an exercise is
stored in the device’s record store, and can be used
to compare the results to previous runs.
The user interface of the Mobile Companion
consists of a single screen showing an image of a
Nabaztag rabbit along with some text areas where
various exercise and device status information is
displayed (Figure 3). The rabbit image is intended
to give users a sense of communicating with the
same Companion, no matter if they are using the
home or mobile system. To further the feeling of
persistence, the home and mobile parts of the H&amp;F
Companion also use the same TTS voice.
When the mobile Companion is started, it asks
the user whether it should connect to the home sys-
tem and download the current plan. Such a plan
consists of various tasks (e.g., shopping or exer-
cise tasks) that the user should try to achieve dur-
ing the day, and is generated by the home system
during a session with the user. If the user chooses
to download the plan the Companion summarizes
the content of the plan for the user, excluding all
tasks that do not involve some kind of exercise ac-
tivity. The Companion then suggests a suitable
task based on time of day and the user’s current
location. If the user chooses not to download the
plan, or rejects the suggested exercise(s), the Com-
panion instead asks the user to suggest an exercise.
</bodyText>
<page confidence="0.956074">
66
</page>
<bodyText confidence="0.999908111111111">
Once an exercise has been agreed upon, the
Companion asks the user to start the exercise and
will then track the progress (distances travelled,
time, pace and calories burned) using a built-in
GPS receiver. While exercising, the user can ask
the Companion to play music or to give reports on
how the user is doing. After the exercise, the Com-
panion will summarize the result and up-load it to
the Home system so it can be referred to later on.
</bodyText>
<sectionHeader confidence="0.996445" genericHeader="method">
3 H&amp;F Companion Implementation
</sectionHeader>
<bodyText confidence="0.999872">
This section details the actual implementation of
the Health and Fitness Companion, in terms of its
two components (the home and mobile parts).
</bodyText>
<subsectionHeader confidence="0.994662">
3.1 Home Companion Implementation
</subsectionHeader>
<bodyText confidence="0.999953862068966">
The Home Companion is implemented on top
of Jaspis, a generic agent-based architecture de-
signed for adaptive spoken dialogue systems (Tu-
runen et al., 2005). The base architecture
is extended to support interaction with virtual
and physical Companions, in particular with the
Nabaztag/tag device.
For speech inputs and outputs, the Home Com-
panion uses LoquendoTMASR and TTS compo-
nents. ASR grammars are in “Speech Recogni-
tion Grammar Specification” (W3C) format and
include semantic tags in “Semantic Interpreta-
tion for Speech Recognition (SISR) Version 1.0”
(W3C) format. Domain specific grammars were
derived from a WoZ corpus. The grammars are
dynamically selected according to the current di-
alogue state. Grammars can be precompiled for
efficiency or compiled at run-time when dynamic
grammar generation takes place in certain situa-
tions. The current system vocabulary consists of
about 1400 words and a total of 900 CFG grammar
rules in 60 grammars. Statistical language models
for the system are presently being implemented.
Language understanding relies heavily on SISR
information: given the current dialogue state, the
input is parsed into a logical notation compati-
ble with the planning implemented in a Cognitive
Model. Additionally, a reduced set of DAMSL
(Core and Allen, 1997) tags is used to mark func-
tional dialogue acts using rule-based reasoning.
Language generation is implemented as a com-
bination of canned utterances and tree adjoining
grammar-based structures. The starting point for
generation is predicate-form descriptions provided
by the dialogue manager. Further details and
contextual information are retrieved from the di-
alogue history and the user model. Finally, SSML
(Speech Synthesis Markup Language) 1.0 tags are
used for controlling the Loquendo synthesizer.
Dialogue management is based on close-
cooperation of the Dialogue Manager and the Cog-
nitive Manager. The Cognitive Manager models
the domain, i.e., knows what to recommend to the
user, what to ask from the user, and what kind
of feedback to provide on domain level issues.
In contrast, the Dialogue Manager focuses on in-
teraction level phenomena, such as confirmations,
turn taking, and initiative management.
The physical agent interface is implemented
in jNabServer software to handle communication
with Nabaztag/tags, that is, Wi-Fi enabled robotic
rabbits. A Nabaztag/tag device can handle vari-
ous forms of interaction, from voice to touch (but-
ton press), and from RFID ‘sniffing’ to ear move-
ments. It can respond by moving its ears, or by
displaying or changing the colour of its four LED
lights. The rabbit can also play sounds such as
music, synthesized speech, and other audio.
</bodyText>
<subsectionHeader confidence="0.999699">
3.2 Mobile Companion Implementation
</subsectionHeader>
<bodyText confidence="0.999920846153846">
The Mobile Companion runs on Windows Mobile-
based devices, such as the Fujitsu Siemens Pocket
LOOX T830. The system is made up of two pro-
grams, both running on the mobile device: a Java
midlet controls the main application logic (exer-
cise tracking, dialogue management, etc.) as well
as the graphical user interface; and a C++-based
speech server that performs TTS and ASR func-
tions on request by the Java midlet, such as load-
ing grammar files or voices.
The midlet is made up of Java manager classes
that provide basic services (event dispatching,
GPS input, audio play-back, TTS and ASR, etc.).
However, the main application logic and the GUI
are implemented using scripts in the Hecl script-
ing language (www.hecl.org). The script files
are read from the device’s file system and evalu-
ated in a script interpreter created by the midlet
when started. The scripts have access to a num-
ber of commands, allowing them to initiate TTS
and ASR operations, etc. Furthermore, events
produced by the Java code are dispatched to the
scripts, such as the user’s current GPS position,
GUI interactions (e.g., stylus interaction and but-
ton presses), and voice input. Scripts are also used
to control the dialogue with the user.
</bodyText>
<page confidence="0.998775">
67
</page>
<bodyText confidence="0.999892571428571">
The speech server is based on the Loquendo
Embedded ASR (speaker-independent) and TTS
software.1 The Mobile Companion uses SRGS 1.0
grammars that are pre-compiled before being in-
stalled on the mobile device. The current system
vocabulary consists of about 100 words in 10 dy-
namically selected grammars.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999838714285714">
As pointed out in the introduction, it is not the aim
of the Health and Fitness Companion system to be
a full-fledged fitness coach. There are several ex-
amples of commercial systems that aim to do that,
e.g., miCoach (www.micoach.com) from Adi-
das and NIKE+ (www.nike.com/nikeplus).
MOPET (Buttussi and Chittaro, 2008) is a
PDA-based personal trainer system supporting
outdoor fitness activities. MOPET is similar to
a Companion in that it tries to build a relation-
ship with the user, but there is no real dialogue
between the user and the system and it does not
support speech input or output. Neither does
MPTrain/TripleBeat (Oliver and Flores-Mangas,
2006; de Oliveira and Oliver, 2008), a system that
runs on a mobile phone and aims to help users
to more easily achieve their exercise goals. This
is done by selecting music indicating the desired
pace and different ways to enhance user motiva-
tion, but without an agent user interface model.
InCA (Kadous and Sammut, 2004) is a spoken
language-based distributed personal assistant con-
versational character with a 3D avatar and facial
animation. Similar to the Mobile Companion, the
architecture is made up of a GUI client running on
a PDA and a speech server, but the InCA server
runs as a back-end system, while the Companion
utilizes a stand-alone speech server.
</bodyText>
<sectionHeader confidence="0.994004" genericHeader="conclusions">
5 Demonstration and Future Work
</sectionHeader>
<bodyText confidence="0.999835444444444">
The demonstration will consist of two sequential
interactions with the H&amp;F Companion. First, the
user and the home system will agree on a plan,
consisting of various tasks that the user should try
to achieve during the day. Then the mobile system
will download the plan, and the user will have a
dialogue with the Companion, concerning the se-
lection of a suitable exercise activity, which the
user will pretend to carry out.
</bodyText>
<footnote confidence="0.925668333333333">
1As described in “Loquendo embedded technologies:
Text to speech and automatic speech recognition.”
www.loquendo.com/en/brochure/Embedded.pdf
</footnote>
<bodyText confidence="0.99921125">
Plans for future work include extending the mo-
bile platform with various sensors, for example, a
pulse sensor that gives the Companion informa-
tion about the user’s pulse while exercising, which
can be used to provide feedback such as telling
the user to speed up or slow down. We are also in-
terested in using sensors to allow users to provide
gesture-like input, in addition to the voice and but-
ton/screen click input available today.
Another modification we are considering is to
unify the two dialogue management solutions cur-
rently used by the home and the mobile compo-
nents into one. This would cause the Companion
to “behave” more consistently in its two shapes,
and make future extensions of the dialogue and the
Companion behaviour easier to manage.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990938">
Fabio Buttussi and Luca Chittaro. 2008. MOPET:
A context-aware and user-adaptive wearable sys-
tem for fitness training. Artificial Intelligence in
Medicine, 42(2):153–163.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, pages 28–35, Cambridge, Mas-
sachusetts.
Laila Dybkjaer, Niels Ole Bernsen, and Wolfgang
Minker. 2004. Evaluation and usability of multi-
modal spoken language dialogue systems. Speech
Communication, 43(1-2):33–54.
Mohammed Waleed Kadous and Claude Sammut.
2004. InCa: A mobile conversational agent. In Pro-
ceedings of the 8th Pacific Rim International Con-
ference on Artificial Intelligence, pages 644–653,
Auckland, New Zealand.
Rodrigo de Oliveira and Nuria Oliver. 2008. Triple-
Beat: Enhancing exercise performance with persua-
sion. In Proceedings of 10th International Con-
ference, on Mobile Human-Computer Interaction,
pages 255–264, Amsterdam, the Netherlands. ACM.
Nuria Oliver and Fernando Flores-Mangas. 2006.
MPTrain: A mobile, music and physiology-based
personal trainer. In Proceedings of 8th International
Conference, on Mobile Human-Computer Interac-
tion, pages 21–28, Espoo, Finland. ACM.
Markku Turunen, Jaakko Hakulinen, Kari-Jouko
R¨aih¨a, Esa-Pekka Salonen, Anssi Kainulainen, and
Perttu Prusi. 2005. An architecture and applica-
tions for speech-based accessibility systems. IBM
Systems Journal, 44(3):485–504.
Yorick Wilks. 2007. Is there progress on talking sensi-
bly to machines? Science, 318(9):927–928.
</reference>
<page confidence="0.999444">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948157">
<title confidence="0.999318">Mobile Health and Fitness Companion</title>
<author confidence="0.983392">Bj¨orn Markku Jaakko</author>
<affiliation confidence="0.98786">Userware Computer &amp; Information Science Computer Sciences Swedish Inst. of Computer Science Norwegian Univ. of Science and Technology Univ. of Tampere</affiliation>
<address confidence="0.99415">Kista, Sweden Trondheim, Norway Tampere, Finland</address>
<email confidence="0.99637">gamback@idi.ntnu.no</email>
<abstract confidence="0.999241">Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and fitness. The paper presents a multimodal conversational Companion system focused on health and fitness, which has both a stationary and a mobile component.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabio Buttussi</author>
<author>Luca Chittaro</author>
</authors>
<title>MOPET: A context-aware and user-adaptive wearable system for fitness training.</title>
<date>2008</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>42</volume>
<issue>2</issue>
<contexts>
<context position="12328" citStr="Buttussi and Chittaro, 2008" startWordPosition="2019" endWordPosition="2022">ech server is based on the Loquendo Embedded ASR (speaker-independent) and TTS software.1 The Mobile Companion uses SRGS 1.0 grammars that are pre-compiled before being installed on the mobile device. The current system vocabulary consists of about 100 words in 10 dynamically selected grammars. 4 Related Work As pointed out in the introduction, it is not the aim of the Health and Fitness Companion system to be a full-fledged fitness coach. There are several examples of commercial systems that aim to do that, e.g., miCoach (www.micoach.com) from Adidas and NIKE+ (www.nike.com/nikeplus). MOPET (Buttussi and Chittaro, 2008) is a PDA-based personal trainer system supporting outdoor fitness activities. MOPET is similar to a Companion in that it tries to build a relationship with the user, but there is no real dialogue between the user and the system and it does not support speech input or output. Neither does MPTrain/TripleBeat (Oliver and Flores-Mangas, 2006; de Oliveira and Oliver, 2008), a system that runs on a mobile phone and aims to help users to more easily achieve their exercise goals. This is done by selecting music indicating the desired pace and different ways to enhance user motivation, but without an </context>
</contexts>
<marker>Buttussi, Chittaro, 2008</marker>
<rawString>Fabio Buttussi and Luca Chittaro. 2008. MOPET: A context-aware and user-adaptive wearable system for fitness training. Artificial Intelligence in Medicine, 42(2):153–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>In AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="9050" citStr="Core and Allen, 1997" startWordPosition="1492" endWordPosition="1495">ccording to the current dialogue state. Grammars can be precompiled for efficiency or compiled at run-time when dynamic grammar generation takes place in certain situations. The current system vocabulary consists of about 1400 words and a total of 900 CFG grammar rules in 60 grammars. Statistical language models for the system are presently being implemented. Language understanding relies heavily on SISR information: given the current dialogue state, the input is parsed into a logical notation compatible with the planning implemented in a Cognitive Model. Additionally, a reduced set of DAMSL (Core and Allen, 1997) tags is used to mark functional dialogue acts using rule-based reasoning. Language generation is implemented as a combination of canned utterances and tree adjoining grammar-based structures. The starting point for generation is predicate-form descriptions provided by the dialogue manager. Further details and contextual information are retrieved from the dialogue history and the user model. Finally, SSML (Speech Synthesis Markup Language) 1.0 tags are used for controlling the Loquendo synthesizer. Dialogue management is based on closecooperation of the Dialogue Manager and the Cognitive Manag</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28–35, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laila Dybkjaer</author>
<author>Niels Ole Bernsen</author>
<author>Wolfgang Minker</author>
</authors>
<title>Evaluation and usability of multimodal spoken language dialogue systems.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>43--1</pages>
<contexts>
<context position="977" citStr="Dybkjaer et al., 2004" startWordPosition="131" endWordPosition="134">cs.se gamback@idi.ntnu.no {mturunen,jh}@cs.uta.fi Abstract Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and fitness. The paper presents a multimodal conversational Companion system focused on health and fitness, which has both a stationary and a mobile component. 1 Introduction Spoken dialogue systems have traditionally focused on task-oriented dialogues, such as making flight bookings or providing public transport timetables. In emerging areas, such as domainoriented dialogues (Dybkjaer et al., 2004), the interaction with the system, typically modelled as a conversation with a virtual anthropomorphic character, can be the main motivation for the interaction. Recent research has coined the term “Companions” to describe embodied multimodal conversational agents having a long lasting interaction history with their users (Wilks, 2007). Such a conversational Companion within the Health and Fitness (H&amp;F) domain helps its users to a healthier lifestyle. An H&amp;F Companion has quite different motivations for use than traditional task-based spoken dialogue systems. Instead of helping with a single, </context>
</contexts>
<marker>Dybkjaer, Bernsen, Minker, 2004</marker>
<rawString>Laila Dybkjaer, Niels Ole Bernsen, and Wolfgang Minker. 2004. Evaluation and usability of multimodal spoken language dialogue systems. Speech Communication, 43(1-2):33–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Waleed Kadous</author>
<author>Claude Sammut</author>
</authors>
<title>InCa: A mobile conversational agent.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Pacific Rim International Conference on Artificial Intelligence,</booktitle>
<pages>644--653</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="12986" citStr="Kadous and Sammut, 2004" startWordPosition="2130" endWordPosition="2133">ystem supporting outdoor fitness activities. MOPET is similar to a Companion in that it tries to build a relationship with the user, but there is no real dialogue between the user and the system and it does not support speech input or output. Neither does MPTrain/TripleBeat (Oliver and Flores-Mangas, 2006; de Oliveira and Oliver, 2008), a system that runs on a mobile phone and aims to help users to more easily achieve their exercise goals. This is done by selecting music indicating the desired pace and different ways to enhance user motivation, but without an agent user interface model. InCA (Kadous and Sammut, 2004) is a spoken language-based distributed personal assistant conversational character with a 3D avatar and facial animation. Similar to the Mobile Companion, the architecture is made up of a GUI client running on a PDA and a speech server, but the InCA server runs as a back-end system, while the Companion utilizes a stand-alone speech server. 5 Demonstration and Future Work The demonstration will consist of two sequential interactions with the H&amp;F Companion. First, the user and the home system will agree on a plan, consisting of various tasks that the user should try to achieve during the day. T</context>
</contexts>
<marker>Kadous, Sammut, 2004</marker>
<rawString>Mohammed Waleed Kadous and Claude Sammut. 2004. InCa: A mobile conversational agent. In Proceedings of the 8th Pacific Rim International Conference on Artificial Intelligence, pages 644–653, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Oliveira</author>
<author>Nuria Oliver</author>
</authors>
<title>TripleBeat: Enhancing exercise performance with persuasion.</title>
<date>2008</date>
<booktitle>In Proceedings of 10th International Conference, on Mobile Human-Computer Interaction,</booktitle>
<pages>255--264</pages>
<publisher>ACM.</publisher>
<location>Amsterdam, the Netherlands.</location>
<marker>de Oliveira, Oliver, 2008</marker>
<rawString>Rodrigo de Oliveira and Nuria Oliver. 2008. TripleBeat: Enhancing exercise performance with persuasion. In Proceedings of 10th International Conference, on Mobile Human-Computer Interaction, pages 255–264, Amsterdam, the Netherlands. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuria Oliver</author>
<author>Fernando Flores-Mangas</author>
</authors>
<title>MPTrain: A mobile, music and physiology-based personal trainer.</title>
<date>2006</date>
<booktitle>In Proceedings of 8th International Conference, on Mobile Human-Computer Interaction,</booktitle>
<pages>21--28</pages>
<publisher>ACM.</publisher>
<location>Espoo, Finland.</location>
<contexts>
<context position="12668" citStr="Oliver and Flores-Mangas, 2006" startWordPosition="2075" endWordPosition="2078">duction, it is not the aim of the Health and Fitness Companion system to be a full-fledged fitness coach. There are several examples of commercial systems that aim to do that, e.g., miCoach (www.micoach.com) from Adidas and NIKE+ (www.nike.com/nikeplus). MOPET (Buttussi and Chittaro, 2008) is a PDA-based personal trainer system supporting outdoor fitness activities. MOPET is similar to a Companion in that it tries to build a relationship with the user, but there is no real dialogue between the user and the system and it does not support speech input or output. Neither does MPTrain/TripleBeat (Oliver and Flores-Mangas, 2006; de Oliveira and Oliver, 2008), a system that runs on a mobile phone and aims to help users to more easily achieve their exercise goals. This is done by selecting music indicating the desired pace and different ways to enhance user motivation, but without an agent user interface model. InCA (Kadous and Sammut, 2004) is a spoken language-based distributed personal assistant conversational character with a 3D avatar and facial animation. Similar to the Mobile Companion, the architecture is made up of a GUI client running on a PDA and a speech server, but the InCA server runs as a back-end syste</context>
</contexts>
<marker>Oliver, Flores-Mangas, 2006</marker>
<rawString>Nuria Oliver and Fernando Flores-Mangas. 2006. MPTrain: A mobile, music and physiology-based personal trainer. In Proceedings of 8th International Conference, on Mobile Human-Computer Interaction, pages 21–28, Espoo, Finland. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markku Turunen</author>
</authors>
<title>Jaakko Hakulinen, Kari-Jouko R¨aih¨a, Esa-Pekka Salonen, Anssi Kainulainen, and Perttu Prusi.</title>
<date>2005</date>
<journal>IBM Systems Journal,</journal>
<volume>44</volume>
<issue>3</issue>
<marker>Turunen, 2005</marker>
<rawString>Markku Turunen, Jaakko Hakulinen, Kari-Jouko R¨aih¨a, Esa-Pekka Salonen, Anssi Kainulainen, and Perttu Prusi. 2005. An architecture and applications for speech-based accessibility systems. IBM Systems Journal, 44(3):485–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Is there progress on talking sensibly to machines?</title>
<date>2007</date>
<journal>Science,</journal>
<volume>318</volume>
<issue>9</issue>
<contexts>
<context position="1314" citStr="Wilks, 2007" startWordPosition="185" endWordPosition="186">ationary and a mobile component. 1 Introduction Spoken dialogue systems have traditionally focused on task-oriented dialogues, such as making flight bookings or providing public transport timetables. In emerging areas, such as domainoriented dialogues (Dybkjaer et al., 2004), the interaction with the system, typically modelled as a conversation with a virtual anthropomorphic character, can be the main motivation for the interaction. Recent research has coined the term “Companions” to describe embodied multimodal conversational agents having a long lasting interaction history with their users (Wilks, 2007). Such a conversational Companion within the Health and Fitness (H&amp;F) domain helps its users to a healthier lifestyle. An H&amp;F Companion has quite different motivations for use than traditional task-based spoken dialogue systems. Instead of helping with a single, well-defined task, it truly aims to be a Companion to the user, providing social support in everyday activities. The system should thus be a peer rather than act as an expert system in health-related issues. It is important to stress that it is the Companion concept which is central, rather than the fitness area as such. Thus it is not</context>
</contexts>
<marker>Wilks, 2007</marker>
<rawString>Yorick Wilks. 2007. Is there progress on talking sensibly to machines? Science, 318(9):927–928.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>