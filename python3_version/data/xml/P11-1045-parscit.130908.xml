<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032600">
<title confidence="0.99687">
Beam-Width Prediction for Efficient Context-Free Parsing
</title>
<author confidence="0.990274">
Nathan Bodenstab† Aaron Dunlop† Keith Hall$ and Brian Roark†
</author>
<affiliation confidence="0.8878685">
† Center for Spoken Language Understanding, Oregon Health &amp; Science University, Portland, OR
$Google, Inc., Zurich, Switzerland
</affiliation>
<email confidence="0.997635">
{bodensta,dunlopa,roark}@cslu.ogi.edu kbhall@google.com
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999561882352941">
Efficient decoding for syntactic parsing has
become a necessary research area as statisti-
cal grammars grow in accuracy and size and
as more NLP applications leverage syntac-
tic analyses. We review prior methods for
pruning and then present a new framework
that unifies their strengths into a single ap-
proach. Using a log linear model, we learn
the optimal beam-search pruning parameters
for each CYK chart cell, effectively predicting
the most promising areas of the model space
to explore. We demonstrate that our method
is faster than coarse-to-fine pruning, exempli-
fied in both the Charniak and Berkeley parsers,
by empirically comparing our parser to the
Berkeley parser using the same grammar and
under identical operating conditions.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954846153846">
Statistical constituent parsers have gradually in-
creased in accuracy over the past ten years. This
accuracy increase has opened the door to automati-
cally derived syntactic information within a number
of NLP tasks. Prior work incorporating parse struc-
ture into machine translation (Chiang, 2010) and Se-
mantic Role Labeling (Tsai et al., 2005; Punyakanok
et al., 2008) indicate that such hierarchical structure
can have great benefit over shallow labeling tech-
niques like chunking and part-of-speech tagging.
Although syntax is becoming increasingly impor-
tant for large-scale NLP applications, constituent
parsing is slow—too slow to scale to the size of
many potential consumer applications. The exhaus-
tive CYK algorithm has computational complexity
O(n3|G|) where n is the length of the sentence and
|G |is the number of grammar productions, a non-
negligible constant. Increases in accuracy have pri-
marily been accomplished through an increase in
the size of the grammar, allowing individual gram-
mar rules to be more sensitive to their surround-
ing context, at a considerable cost in efficiency.
Grammar transformation techniques such as linguis-
tically inspired non-terminal annotations (Johnson,
1998; Klein and Manning, 2003b) and latent vari-
able grammars (Matsuzaki et al., 2005; Petrov et al.,
2006) have increased the grammar size |G |from a
few thousand rules to several million in an explic-
itly enumerable grammar, or even more in an im-
plicit grammar. Exhaustive search for the maximum
likelihood parse tree with a state-of-the-art grammar
can require over a minute of processing for a sin-
gle sentence of 25 words, an unacceptable amount
of time for real-time applications or when process-
ing millions of sentences. Deterministic algorithms
for dependency parsing exist that can extract syntac-
tic dependency structure very quickly (Nivre, 2008),
but this approach is often undesirable as constituent
parsers are more accurate and more adaptable to new
domains (Petrov et al., 2010).
The most accurate constituent parsers, e.g., Char-
niak (2000), Petrov and Klein (2007a), make use
of approximate inference, limiting their search to
a fraction of the total search space and achieving
speeds of between one and four newspaper sen-
tences per second. The paradigm for building state-
of-the-art parsing models is to first design a model
structure that can achieve high accuracy and then,
after the model has been built, design effective ap-
proximate inference methods around that particu-
lar model; e.g., coarse-to-fine non-terminal hierar-
chies for a given model, or agenda-based methods
</bodyText>
<page confidence="0.97415">
440
</page>
<note confidence="0.9794745">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 440–449,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943357142857">
that are empirically tuned to achieve acceptable ef-
ficiency/accuracy operating points. While both of
the above mentioned papers use the CYK dynamic
programming algorithm to search through possible
solutions, their particular methods of approximate
inference are quite distinct.
In this paper, we examine a general approach to
approximate inference in constituent parsing that
learns cell-specific thresholds for arbitrary gram-
mars. For each cell in the CYK chart, we sort all
potential constituents in a local agenda, ordered by
an estimate of their posterior probability. Given fea-
tures extracted from the chart cell context – e.g.,
span width; POS-tags and words surrounding the
boundary of the cell – we train a log linear model
to predict how many constituents should be popped
from the local agenda and added to the chart. As
a special case of this approach, we simply pre-
dict whether the number to add should be zero or
greater than zero, in which case the method can be
seen as a cell-by-cell generalization of Roark and
Hollingshead’s (2008; 2009) tagger-derived Chart
Constraints. More generally, instead of a binary
classification decision, we can also use this method
to predict the desired cell population directly and
get cell closure for free when the classifier predicts
a beam-width of zero. In addition, we use a non-
symmetric loss function during optimization to ac-
count for the imbalance between over-predicting or
under-predicting the beam-width.
A key feature of our approach is that it does
not rely upon reference syntactic annotations when
learning to search. Rather, the beam-width predic-
tion model is trained to learn the rank of constituents
in the maximum likelihood trees.1 We will illus-
trate this by presenting results using a latent-variable
grammar, for which there is no “true” reference la-
tent variable parse. We simply parse sections 2-21
of the WSJ treebank and train our search models
from the output of these trees, with no prior knowl-
edge of the non-terminal set or other grammar char-
acteristics to guide the process. Hence, this ap-
</bodyText>
<footnote confidence="0.991216666666667">
1Note that we do not call this method “unsupervised” be-
cause all grammars used in this paper are induced from super-
vised data, although our framework can also accommodate un-
supervised grammars. We emphasize that we are learning to
search using only maximum likelihood trees, not that we are
doing unsupervised parsing.
</footnote>
<figureCaption confidence="0.9839555">
Figure 1: Inside (grey) and outside (white) representations of
an example chart edge Nij.
</figureCaption>
<bodyText confidence="0.999946785714286">
proach is broadly applicable to a wide range of sce-
narios, including tuning the search to new domains
where domain mismatch may yield very different ef-
ficiency/accuracy operating points.
In the next section, we present prior work on
approximate inference in parsing, and discuss how
our method to learn optimal beam-search param-
eters unite many of their strengths into a single
framework. We then explore using our approach to
open or close cells in the chart as an alternative to
Roark and Hollingshead (2008; 2009). Finally, we
present results which combine cell closure and adap-
tive beam-width prediction to achieve the most effi-
cient parser.
</bodyText>
<sectionHeader confidence="0.993571" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998114">
2.1 Preliminaries and notation
</subsectionHeader>
<bodyText confidence="0.999859090909091">
Let S = w1 ... w|S |represent an input string of
|S |words. Let wi,j denote the substring from word
wi+1 to wj; i.e., S = w0,|S|. We use the term chart
edge to refer to a non-terminal spanning a specific
substring of the input sentence. Let Ni,j denote the
edge labeled with non-terminal N spanning wi,j, for
example NP3,7. We define an edge’s figure-of-merit
(FOM) as an estimate of the product of its inside
(Q) and outside (α) scores, conceptually the relative
merit the edge has to participate in the final parse
tree (see Figure 1). More formally:
</bodyText>
<equation confidence="0.999957">
α(Ni,j) = P(w0,i, Ni,j, wj,n)
Q(Ni,j) = P(wi,j|N)
FOM(Ni,j) = �α(Ni,j) �Q(Ni,j)
</equation>
<page confidence="0.97361">
441
</page>
<bodyText confidence="0.99995975">
With bottom-up parsing, the true inside probability
is accumulated and Q(NZj) does not need to be esti-
mated, improving the FOMs ability to represent the
true inside/outside distribution.
In this paper, we use a modified version of the
Caraballo and Charniak Boundary FOM (1998)
for local edge comparison, which computes a(NZ�j)
using POS forward-backward scores and POS-to-
nonterminal constituent boundary transition proba-
bilities. Details can be found in (?).
We also note that in this paper we only use
the FOM scoring function to rank constituents in
a local agenda. Alternative approaches to rank-
ing competitors are also possible, such as Learning
as Search Optimization (Daum´e and Marcu, 2005).
The method we present in this paper to learn the op-
timal beam-search parameters is applicable to any
ranking function, and we demonstrate this by com-
puting results with both the Boundary FOM and
only the inside probability in Section 6.
</bodyText>
<subsectionHeader confidence="0.999376">
2.2 Agenda-based parsing
</subsectionHeader>
<bodyText confidence="0.999978923076923">
Agenda-based parsers maintain a global agenda of
edges, ranked by FOM score. At each iteration, the
highest-scoring edge is popped off of the agenda,
added to the chart, and combined with other edges
already in the chart. The agenda-based approach
includes best-first parsing (Bobrow, 1990) and A*
parsing (Klein and Manning, 2003a), which differ
in whether an admissible FOM estimate a(NZ�j) is
required. A* uses an admissible FOM, and thus
guarantees finding the maximum likelihood parse,
whereas an inadmissible heuristic (best-first) may
require less exploration of the search space. Much
work has been pursued in both admissible and in-
admissible heuristics for agenda parsing (Caraballo
and Charniak, 1998; Klein and Manning, 2003a;
Pauls et al., 2010).
In this paper, we also make use of agendas, but
at a local rather than a global level. We maintain an
agenda for each cell, which has two significant ben-
efits: 1) Competing edges can be compared directly,
avoiding the difficulty inherent in agenda-based ap-
proaches of comparing edges of radically differ-
ent span lengths and characteristics; and 2) Since
the agendas are very small, the overhead of agenda
maintenance—a large component of agenda-based
parse time —is minimal.
</bodyText>
<subsectionHeader confidence="0.999695">
2.3 Beam-search parsing
</subsectionHeader>
<bodyText confidence="0.999981533333333">
CYK parsing with a beam-search is a local pruning
strategy, comparing edges within the same chart cell.
The beam-width can be defined in terms of a thresh-
old in the number of edges allowed, or in terms of
a threshold on the difference in probability relative
to the highest scoring edge (Collins, 1999; Zhang et
al., 2010). For the current paper, we use both kinds
of thresholds, avoiding pathological cases that each
individual criteria is prone to encounter. Further, un-
like most beam-search approaches we will make use
of a FOM estimate of the posterior probability of an
edge, defined above, as our ranking function. Fi-
nally, we will learn log linear models to assign cell-
specific thresholds, rather than relying on a single
search parameter.
</bodyText>
<subsectionHeader confidence="0.96276">
2.4 Coarse-to-Fine Parsing
</subsectionHeader>
<bodyText confidence="0.999980482758621">
Coarse-to-fine parsing, also known as multiple pass
parsing (Goodman, 1997; Charniak, 2000; Char-
niak and Johnson, 2005), first parses the input sen-
tence with a simplified (coarse) version of the tar-
get (fine) grammar in which multiple non-terminals
are merged into a single state. Since the coarse
grammar is quite small, parsing is much faster than
with the fine grammar, and can quickly yield an es-
timate of the outside probability α(·) for use in sub-
sequent agenda or beam-search parsing with the fine
grammar. This approach can also be used iteratively
with grammars of increasing complexity (Petrov and
Klein, 2007a).
Building a coarse grammar from a fine gram-
mar is a non-trivial problem, and most often ap-
proached with detailed knowledge of the fine gram-
mar being used. For example, Goodman (1997)
suggests using a coarse grammar consisting of reg-
ular non-terminals, such as NP and VP, and then
non-terminals augmented with head-word informa-
tion for the more accurate second-pass grammar.
Such an approach is followed by Charniak (2000) as
well. Petrov and Klein (2007a) derive coarse gram-
mars in a more statistically principled way, although
the technique is closely tied to their latent variable
grammar representation.
To the extent that our cell-specific threshold clas-
sifier predicts that a chart cell should contain zero
edges or more than zero edges, it is making coarse
</bodyText>
<page confidence="0.992967">
442
</page>
<bodyText confidence="0.999096">
predictions about the unlabeled constituent structure
of the target parse tree. This aspect of our work is
can be viewed as a coarse-to-fine process, though
without considering specific grammatical categories
or rule productions.
</bodyText>
<subsectionHeader confidence="0.994775">
2.5 Chart Constraints
</subsectionHeader>
<bodyText confidence="0.9999234">
Roark and Hollingshead (2008; 2009) introduced
a pruning technique that ignores entire chart cells
based on lexical and POS features of the input sen-
tence. They train two finite-state binary taggers:
one that allows multi-word constituents to start at
a word, and one that allows constituents to end at a
word. Given these tags, it is straightforward to com-
pletely skip many chart cells during processing.
In this paper, instead of tagging word positions to
infer valid constituent spans, we classify chart cells
directly. We further generalize this cell classification
to predict the beam-width of the chart cell, where a
beam-width of zero indicates that the cell is com-
pletely closed. We discuss this in detail in the next
section.
</bodyText>
<sectionHeader confidence="0.979067" genericHeader="method">
3 Open/Closed Cell Classification
</sectionHeader>
<subsectionHeader confidence="0.999465">
3.1 Constituent Closure
</subsectionHeader>
<bodyText confidence="0.999422411764706">
We first look at the binary classification of chart cells
as either open or closed to full constituents, and pre-
dict this value from the input sentence alone. This
is the same problem that Roark and Hollingshead
(2008; 2009) solve with Chart Constraints; however,
where they classify lexical items as either beginning
or ending a constituent, we classify individual chart
cells as open or closed, an approach we call Con-
stituent Closure. Although the number of classifi-
cations scales quadratically with our approach, the
total parse time is still dominated by the 0(n3|G|)
parsing complexity and we find that the added level
of specificity reduces the search space significantly.
To learn to classify a chart cell spanning words
wz+1 ... wj of a sentence S as open or closed to full
constituents, we first map cells in the training corpus
to tuples:
</bodyText>
<equation confidence="0.995682">
-b(S,i,j) = (x, y) (1)
</equation>
<bodyText confidence="0.999960583333333">
where x is a feature-vector representation of the
chart cell and y is the target class 1 if the cell con-
tains an edge from the maximum likelihood parse
tree, 0 otherwise. The feature vector x is encoded
with the chart cell’s absolute and relative span width,
as well as unigram and bigram lexical and part-of-
speech tag items from wz_1 ... wj+2.
Given feature/target tuples (x, y) for every chart
cell in every sentence of a training corpus T, we train
a weight vector 0 using the averaged perceptron al-
gorithm (Collins, 2002) to learn an open/closed bi-
nary decision boundary:
</bodyText>
<equation confidence="0.994532">
�B = argmin
� �x��������
</equation>
<bodyText confidence="0.999783666666667">
where H(·) is the unit step function: 1 if the inner
product 0 · x &gt; 0, and 0 otherwise; and La(·, ·) is an
asymmetric loss function, defined below.
When predicting cell closure, all misclassifica-
tions are not equal. If we leave open a cell which
contains no edges in the maximum likelihood (ML)
parse, we incur the cost of additional processing, but
are still able to recover the ML tree. However, if we
close a chart cell which contains an ML edge, search
errors occur. To deal with this imbalance, we intro-
duce an asymmetric loss function La(·, ·) to penalize
false-negatives more severely during training.
</bodyText>
<equation confidence="0.64252">
La(h, y) = { 0 ifh=y (3)
1 ifh&gt;y
A ifh&lt;y
</equation>
<bodyText confidence="0.999626">
We found the value A = 102 to give the best per-
formance on our development set, and we use this
value in all of our experiments.
Figures 2a and 2b compare the pruned charts of
Chart Constraints and Constituent Closure for a sin-
gle sentence in the development set. Note that both
of these methods are predicting where a complete
constituent may be located in the chart, not partial
constituents headed by factored nonterminals within
a binarized grammar. Depending on the grammar
factorization (right or left) we can infer chart cells
that are restricted to only edges with a factored left-
hand-side non-terminal. In Figure 2 these chart cells
are colored gray. Note that Constituent Closure re-
duces the number of completely open cells consider-
ably vs. Chart Constraints, and the number of cells
open to factored categories somewhat.
</bodyText>
<equation confidence="0.991131">
La(H(0 · x), y) (2)
</equation>
<page confidence="0.995918">
443
</page>
<subsectionHeader confidence="0.99767">
3.2 Complete Closure
</subsectionHeader>
<bodyText confidence="0.999897">
Alternatively, we can predict whether a chart cell
contains any edge, either a partial or a full con-
stituent, an approach we call Complete Closure.
This is a more difficult classification problem as par-
tial constituents occur in a variety of contexts. Nev-
ertheless, learning this directly allows us to remove a
large number of internal chart cells from considera-
tion, since no additional cells need to be left open to
partial constituents. The learning algorithm is iden-
tical to Equation 2, but training examples are now
assigned a positive label if the chart cell contains any
edge from the binarized maximum likelihood tree.
Figure 2c gives a visual representation of Complete
Closure for the same sentence; the number of com-
pletely open cells increases somewhat, but the total
number of open cells (including those open to fac-
tored categories) is greatly reduced.
We compare the effectiveness of Constituent Clo-
sure, Complete Closure, and Chart Constraints, by
decreasing the percentage of chart cells closed un-
til accuracy over all sentences in our development
set start to decline. For Constituent and Complete
Closure, we also vary the loss function, adjusting
the relative penalty between a false-negative (clos-
ing off a chart cell that contains a maximum like-
lihood edge) and a false-positive. Results show that
using Chart Constrains as a baseline, we prune (skip)
33% of the total chart cells. Constituent Closure im-
proves on this baseline only slightly (36%), but we
see our biggest gains with Complete Closure, which
prunes 56% of all chart cells in the development set.
All of these open/closed cell classification meth-
ods can improve the efficiency of the exhaustive
CYK algorithm, or any of the approximate infer-
ence methods mentioned in Section 2. We empir-
ically evaluate them when applied to CYK parsing
and beam-search parsing in Section 6.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="method">
4 Beam-Width Prediction
</sectionHeader>
<bodyText confidence="0.999515714285714">
The cell-closing approaches discussed in Section 3
make binary decisions to either allow or completely
block all edges in each cell. This all-on/all-off tactic
ignores the characteristics of the local cell popula-
tion, which, given a large statistical grammar, may
contain hundred of edges, even if very improbable.
Retaining all of these partial derivations forces the
</bodyText>
<figure confidence="0.654302">
(a) Chart Constraints (Roark and Hollingshead, 2009)
(b) Constituent Closure (this paper)
(c) Complete Closure (this paper)
</figure>
<figureCaption confidence="0.981827">
Figure 2: Comparison of Chart Constraints (Roark and
Hollingshead, 2009) to Constituent and Complete Closure for a
single example sentence. Black cells are open to all edges while
grey cells only allow factored edges (incomplete constituents).
</figureCaption>
<bodyText confidence="0.998533">
search in larger spans to continue down improbable
paths, adversely affecting efficiency. We can further
improve parsing speed in these open cells by lever-
aging local pruning methods, such as beam-search.
When parsing with a beam-search, finding the op-
timal beam-width threshold(s) to balance speed and
accuracy is a necessary step. As mentioned in Sec-
</bodyText>
<page confidence="0.995826">
444
</page>
<bodyText confidence="0.999979395833334">
tion 2.3, two variations of the beam-width are of-
ten considered: a fixed number of allowed edges,
or a relative probability difference from the highest
scoring local edge. For the remainder of this pa-
per we fix the relative probability threshold for all
experiments and focus on adapting the number of
allowed edges per cell. We will refer to this number-
of-allowed-edges value as the beam-width, notated
by b, and leave adaptation of the relative probability
difference to future work.
The standard way to tune the beam-width is a sim-
ple sweep over possible values until accuracy on
a heldout data set starts to decline. The optimal
point will necessarily be very conservative, allowing
outliers (sentences or sub-phrases with above aver-
age ambiguity) to stay within the beam and produce
valid parse trees. The majority of chart cells will
require much fewer than b entries to find the max-
imum likelihood (ML) edge, yet, constrained by a
constant beam-width, the cell will continue to be
filled with unfruitful edges, exponentially increasing
downstream computation.
For example, when parsing with the Berkeley
latent-variable grammar and Boundary FOM, we
find we can reduce the global beam-width b to 15
edges in each cell before accuracy starts to decline.
However we find that 73% of the ML edges are
ranked first in their cell and 96% are ranked in the
top three. Thus, in 24 of every 25 cells, 80% of the
edges are unnecessary (12 of the top 15). Clearly,
it would be advantageous to adapt the beam-width
such that it is restrictive when we are confident in
the FOM ranking and more forgiving in ambiguous
contexts.
To address this problem, we learn the optimal
beam-width for each chart cell directly. We define
Ri,j as the rank of the ML edge in the chart cell
spanning wi+1 ... wj. If no ML edge exists in the
cell, then Ri,j = 0. Given a global maximum beam-
width b, we train b different binary classifiers, each
using separate mapping functions 4k, where the tar-
get value y produced by 4bk is 1 if Ri,j &gt; k and 0
otherwise.
The same asymmetry noted in Section 3 applies
in this task as well. When in doubt, we prefer to
over-predict the beam-width and risk an increase in
processing time opposed to under-predicting at the
expense of accuracy. Thus we use the same loss
</bodyText>
<equation confidence="0.836912666666667">
function L), this time training several classifiers:
�Bk = argmin L),(H(B · x), y) (4)
9 (X,y)EDk(T)
</equation>
<bodyText confidence="0.9660355">
Note that in Equation 4 when k = 0, we re-
cover the open/closed cell classification of Equa-
tion 2, since a beam width of 0 indicates that the
chart cell is completely closed.
During decoding, we assign the beam-width
for chart cell spanning wi+1 ... wj given models
B0, B1, ...Bb_1 by finding the lowest value k such that
the binary classifier Bk classifies Ri,j G k. If no such
k exists, Ri,j is set to the maximum beam-width
value b:
</bodyText>
<equation confidence="0.9979765">
Ri,j = argmin Bk · xi &lt; 0 (5)
k
</equation>
<bodyText confidence="0.99998162962963">
In Equation 5 we assume there are b unique clas-
sifiers, one for each possible beam-width value be-
tween 0 and b − 1, but this level of granularity is not
required. Choosing the number of classification bins
to minimize total parsing time is dependent on the
FOM function and how it ranks ML edges. With the
Boundary FOM we use in this paper, 97.8% of ML
edges have a local rank less than five and we find that
the added cost of computing b decision boundaries
for each cell is not worth the added specificity. We
searched over possible classification bins and found
that training four classifiers with beam-width deci-
sion boundaries at 0, 1, 2, and 4 is faster than 15 in-
dividual classifiers and more memory efficient, since
each model Bk has over 800,000 parameters. All
beam-width prediction results reported in this paper
use these settings.
Figure 3 is a visual representation of beam-width
prediction on a single sentence of the development
set using the Berkeley latent-variable grammar and
Boundary FOM. In this figure, the gray scale repre-
sents the relative size of the beam-width, black being
the maximum beam-width value, b, and the lightest
gray being a beam-width of size one. We can see
from this figure that very few chart cells are classi-
fied as needing the full 15 edges, apart from span-1
cells which we do not classify.
</bodyText>
<page confidence="0.999256">
445
</page>
<figureCaption confidence="0.9933475">
Figure 3: Visualization of Beam-Width Prediction for a single example sentence. The grey scale represents the size of the predicted
beam-width: white is 0 (cell is skipped) and black is the maximum value b (b=15 in this example).
</figureCaption>
<sectionHeader confidence="0.99697" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999881527777778">
We run all experiments on the WSJ treebank (Mar-
cus et al., 1999) using the standard splits: section
2-21 for training, section 22 for development, and
section 23 for testing. We preprocess the treebank
by removing empty nodes, temporal labels, and spu-
rious unary productions (X—*X), as is standard in
published works on syntactic parsing.
The pruning methods we present in this paper can
be used to parse with any grammar. To achieve state-
of-the-art accuracy levels, we parse with the Berke-
ley SM6 latent-variable grammar (Petrov and Klein,
2007b) where the original treebank non-terminals
are automatically split into subclasses to optimize
parsing accuracy. This is an explicit grammar con-
sisting of 4.3 million productions, 2.4 million of
which are lexical productions. Exhaustive CYK
parsing with the grammar takes more than a minute
per sentence.
Accuracy is computed from the 1-best Viterbi
(max) tree extracted from the chart. Alternative de-
coding methods, such as marginalizing over the la-
tent variables in the grammar or MaxRule decod-
ing (Petrov and Klein, 2007a) are certainly possible
in our framework, but it is unknown how effective
these methods will be given the heavily pruned na-
ture of the chart. We leave investigation of this to
future work. We compute the precision and recall
of constituents from the 1-best Viterbi trees using
the standard EVALB script (?), which ignores punc-
tuation and the root symbol. Accuracy results are
reported as F-measure (Fl), the harmonic mean be-
tween precision and recall.
We ran all timing tests on an Intel 3.00GHz pro-
cessor with 6MB of cache and 16GB of memory.
Our parser is written in Java and publicly available
at http://nlp.csee.ogi.edu.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999970384615385">
We empirically demonstrate the advantages of our
pruning methods by comparing the total parse time
of each system, including FOM initialization, chart
cell classification, and beam-width prediction. The
parse times reported for Chart Constraints do not in-
clude tagging times as we were provided with this
pre-tagged data, but tagging all of Section 22 takes
less than three seconds and we choose to ignore this
contribution for simplicity.
Figure 4 contains a timing comparison of the three
components of our final parser: Boundary FOM ini-
tialization (which includes the forward-backward al-
gorithm over ambiguous part-of-speech tags), beam-
</bodyText>
<page confidence="0.998861">
446
</page>
<figureCaption confidence="0.9825085">
Figure 4: Timing breakdown by sentence length for major
components of our parser.
</figureCaption>
<bodyText confidence="0.999854529411765">
width prediction, and the final beam-search, includ-
ing 1-best extraction. We bin these relative times
with respect to sentence length to see how each com-
ponent scales with the number of input words. As
expected, the O(n3|G|) beam-search begins to dom-
inate as the sentence length grows, but Boundary
FOM initialization is not cheap, and absorbs, on
average, 20% of the total parse time. Beam-width
prediction, on the other hand, is almost negligible
in terms of processing time even though it scales
quadratically with the length of the sentence.
We compare the accuracy degradation of beam-
width prediction and Chart Constraints in Figure 5
as we incrementally tighten their respective prun-
ing parameters. We also include the baseline beam-
search parser with Boundary FOM in this figure
to demonstrate the accuracy/speed trade-off of ad-
justing a global beam-width alone. In this figure
we see that the knee of the beam-width prediction
curve (Beam-Predict) extends substantially further
to the left before accuracy declines, indicating that
our pruning method is intelligently removing a sig-
nificant portion of the search space that remains un-
pruned with Chart Constraints.
In Table 1 we present the accuracy and parse time
for three baseline parsers on the development set:
exhaustive CYK parsing, beam-search parsing using
only the inside score ,(i(·), and beam-search parsing
using the Boundary FOM. We then apply our two
cell-closing methods, Constituent Closure and Com-
plete Closure, to all three baselines. As expected,
the relative speedup of these methods across the var-
ious baselines is similar since the open/closed cell
classification does not change across parsers. We
</bodyText>
<figureCaption confidence="0.9989915">
Figure 5: Time vs. accuracy curves comparing beam-width
prediction (Beam-Predict) and Chart Constraints.
</figureCaption>
<bodyText confidence="0.999894105263158">
also see that Complete Closure is between 22% and
31% faster than Constituent Closure, indicating that
the greater number of cells closed translates directly
into a reduction in parse time. We can further apply
beam-width prediction to the two beam-search base-
line parsers in Table 1. Dynamically adjusting the
beam-width for the remaining open cells decreases
parse time by an additional 25% when using the In-
side FOM, and 28% with the boundary FOM.
We apply our best model to the test set and report
results in Table 2. Beam-width prediction, again,
outperforms the baseline of a constant beam-width
by 65% and the open/closed classification of Chart
Constraints by 49%. We also compare beam-width
prediction to the Berkeley Coarse-to-Fine parser.
Both our parser and the Berkeley parser are written
in Java, both are run with Viterbi decoding, and both
parse with the same grammar, so a direct compari-
son of speed and accuracy is fair.2
</bodyText>
<sectionHeader confidence="0.961847" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99960825">
We have introduced three new pruning methods, the
best of which unites figure-of-merit estimation from
agenda-based parsing, local pruning from beam-
search parsing, and unlabeled constituent structure
</bodyText>
<footnote confidence="0.577373625">
2We run the Berkeley parser with the default search param-
eterization to achieve the fastest possible parsing time. We note
that 3 of 2416 sentences fail to parse under these settings. Using
the ‘-accurate’ option provides a valid parse for all sentences,
but increases parsing time of section 23 to 0.293 seconds per
sentence with no increase in F-score. We assume a back-off
strategy for failed parses could be implemented to parse all sen-
tences with a parsing time close to the default parameterization.
</footnote>
<page confidence="0.991414">
447
</page>
<table confidence="0.999954666666667">
Parser Sec/Sent F1
CYK 70.383 89.4
CYK + Constituent Closure 47.870 89.3
CYK + Complete Closure 32.619 89.3
Beam + Inside FOM (BI) 3.977 89.2
BI + Constituent Closure 2.033 89.2
BI + Complete Closure 1.575 89.3
BI + Beam-Predict 1.180 89.3
Beam + Boundary FOM (BB) 0.326 89.2
BB + Constituent Closure 0.279 89.2
BB + Complete Closure 0.199 89.3
BB + Beam-Predict 0.143 89.3
</table>
<tableCaption confidence="0.984457666666667">
Table 1: Section 22 development set results for CYK and
Beam-Search (Beam) parsing using the Berkeley latent-variable
grammar.
</tableCaption>
<bodyText confidence="0.999724586206896">
prediction from coarse-to-fine parsing and Chart
Constraints. Furthermore, our pruning method is
trained using only maximum likelihood trees, allow-
ing it to be tuned to specific domains without labeled
data. Using this framework, we have shown that we
can decrease parsing time by 65% over a standard
beam-search without any loss in accuracy, and parse
significantly faster than both the Berkeley parser and
Chart Constraints.
We plan to explore a number of remaining ques-
tions in future work. First, we will try combin-
ing our approach with constituent-level Coarse-to-
Fine pruning. The two methods prune the search
space in very different ways and may prove to be
complementary. On the other hand, our parser cur-
rently spends 20% of the total parse time initializing
the FOM, and adding additional preprocessing costs,
such as parsing with a coarse grammar, may not out-
weigh the benefits gained in the final search.
Second, as with Chart Constraints we do not
prune lexical or unary edges in the span-1 chart cells
(i.e., chart cells that span a single word). We ex-
pect pruning entries in these cells would notably re-
duce parse time since they cause exponentially many
chart edges to be built in larger spans. Initial work
constraining span-1 chart cells has promising results
(Bodenstab et al., 2011) and we hope to investigate
its interaction with beam-width prediction even fur-
ther.
</bodyText>
<table confidence="0.999767857142857">
Parser Sec/Sent F1
CYK 64.610 88.7
Berkeley CTF MaxRule 0.213 90.2
Berkeley CTF Viterbi 0.208 88.8
Beam + Boundary FOM (BB) 0.334 88.6
BB + Chart Constraints 0.244 88.7
BB + Beam-Predict (this paper) 0.125 88.7
</table>
<tableCaption confidence="0.9781585">
Table 2: Section 23 test set results for multiple parsers using
the Berkeley latent-variable grammar.
</tableCaption>
<bodyText confidence="0.999816833333333">
Finally, the size and structure of the grammar is
the single largest contributor to parse efficiency. In
contrast to the current paradigm, we plan to inves-
tigate new algorithms that jointly optimize accuracy
and efficiency during grammar induction, leading to
more efficient decoding.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999993888888889">
We would like to thank Kristy Hollingshead for
her valuable discussions, as well as the anony-
mous reviewers who gave very helpful feedback.
This research was supported in part by NSF Grants
#IIS-0447214, #IIS-0811745 and DARPA grant
#HR0011-09-1-0041. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or DARPA.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999289388888889">
Robert J. Bobrow. 1990. Statistical agenda parsing. In
DARPA Speech and Language Workshop, pages 222–
224.
Nathan Bodenstab, Kristy Hollingshead, and Brian
Roark. 2011. Unary constraints for efficient context-
free parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
Portland, Oregon.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24:275–298.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173–
180, Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
</reference>
<page confidence="0.993019">
448
</page>
<reference confidence="0.996534817307692">
chapter of the Association for Computational Linguis-
tics conference, pages 132–139, Seattle, Washington.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 1443–1452.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD dissertation, Uni-
versity of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1–8,
Philadelphia.
Hal Daum´e, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of the
22nd international conference on Machine learning,
ICML ’05, pages 169–176, New York, NY, USA.
Joshua Goodman. 1997. Global thresholding and
Multiple-Pass parsing. Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 11–25.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.
Dan Klein and Christopher D. Manning. 2003a. A* pars-
ing. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL ’03), pages 40–47, Edmonton, Canada.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423–430, Sap-
poro, Japan.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics - ACL ’05, pages
75–82, Ann Arbor, Michigan.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34:513–553.
Adam Pauls, Dan Klein, and Chris Quirk. 2010. Top-
down k-best a* parsing. In In proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics Short Papers, ACLShort ’10, pages 200–204,
Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404–
411, Rochester, New York.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433–440, Syd-
ney, Australia.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705–713, Cambridge, MA,
October.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, editors,
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 745–
752, Manchester, UK.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity Context-Free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647–655, Boulder, Colorado.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of ME and
SVM via integer linear programming. In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning, CONLL ’05, pages 233–236,
Morristown, NJ, USA.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast Lexicalised-Grammar parsing.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 1472–1479, Bei-
jing, China.
</reference>
<page confidence="0.999239">
449
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975993">
<title confidence="0.999979">Beam-Width Prediction for Efficient Context-Free Parsing</title>
<author confidence="0.998054">Brian</author>
<affiliation confidence="0.994695">for Spoken Language Understanding, Oregon Health &amp; Science University, Portland,</affiliation>
<address confidence="0.996989">Inc., Zurich, Switzerland</address>
<email confidence="0.999887">kbhall@google.com</email>
<abstract confidence="0.999035166666667">Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. Using a log linear model, we learn the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert J Bobrow</author>
</authors>
<title>Statistical agenda parsing.</title>
<date>1990</date>
<booktitle>In DARPA Speech and Language Workshop,</booktitle>
<pages>222--224</pages>
<contexts>
<context position="8898" citStr="Bobrow, 1990" startWordPosition="1397" endWordPosition="1398">as Learning as Search Optimization (Daum´e and Marcu, 2005). The method we present in this paper to learn the optimal beam-search parameters is applicable to any ranking function, and we demonstrate this by computing results with both the Boundary FOM and only the inside probability in Section 6. 2.2 Agenda-based parsing Agenda-based parsers maintain a global agenda of edges, ranked by FOM score. At each iteration, the highest-scoring edge is popped off of the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate a(NZ�j) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which h</context>
</contexts>
<marker>Bobrow, 1990</marker>
<rawString>Robert J. Bobrow. 1990. Statistical agenda parsing. In DARPA Speech and Language Workshop, pages 222– 224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Bodenstab</author>
<author>Kristy Hollingshead</author>
<author>Brian Roark</author>
</authors>
<title>Unary constraints for efficient contextfree parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="31188" citStr="Bodenstab et al., 2011" startWordPosition="5079" endWordPosition="5082"> On the other hand, our parser currently spends 20% of the total parse time initializing the FOM, and adding additional preprocessing costs, such as parsing with a coarse grammar, may not outweigh the benefits gained in the final search. Second, as with Chart Constraints we do not prune lexical or unary edges in the span-1 chart cells (i.e., chart cells that span a single word). We expect pruning entries in these cells would notably reduce parse time since they cause exponentially many chart edges to be built in larger spans. Initial work constraining span-1 chart cells has promising results (Bodenstab et al., 2011) and we hope to investigate its interaction with beam-width prediction even further. Parser Sec/Sent F1 CYK 64.610 88.7 Berkeley CTF MaxRule 0.213 90.2 Berkeley CTF Viterbi 0.208 88.8 Beam + Boundary FOM (BB) 0.334 88.6 BB + Chart Constraints 0.244 88.7 BB + Beam-Predict (this paper) 0.125 88.7 Table 2: Section 23 test set results for multiple parsers using the Berkeley latent-variable grammar. Finally, the size and structure of the grammar is the single largest contributor to parse efficiency. In contrast to the current paradigm, we plan to investigate new algorithms that jointly optimize acc</context>
</contexts>
<marker>Bodenstab, Hollingshead, Roark, 2011</marker>
<rawString>Nathan Bodenstab, Kristy Hollingshead, and Brian Roark. 2011. Unary constraints for efficient contextfree parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--275</pages>
<contexts>
<context position="9318" citStr="Caraballo and Charniak, 1998" startWordPosition="1458" endWordPosition="1461">ch iteration, the highest-scoring edge is popped off of the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate a(NZ�j) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competing edges can be compared directly, avoiding the difficulty inherent in agenda-based approaches of comparing edges of radically different span lengths and characteristics; and 2) Since the agendas are very small, the overhead of agenda maintenance—a large component of agenda-based parse time —is minimal. 2.3 Beam-search parsing CYK parsing with a beam-search is a local pruning st</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon A Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, 24:275–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10759" citStr="Charniak and Johnson, 2005" startWordPosition="1692" endWordPosition="1696">highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a non-trivial problem, and most often approache</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173– 180, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="3112" citStr="Charniak (2000)" startWordPosition="465" endWordPosition="467"> in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods 440 Proceedings of the 49th Annual Meeting of the Association for</context>
<context position="10730" citStr="Charniak, 2000" startWordPosition="1690" endWordPosition="1691">relative to the highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a non-trivial prob</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="1359" citStr="Chiang, 2010" startWordPosition="195" endWordPosition="196">he most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48rd Annual Meeting on Association for Computational Linguistics, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<note>PhD dissertation,</note>
<contexts>
<context position="10167" citStr="Collins, 1999" startWordPosition="1602" endWordPosition="1603">ompared directly, avoiding the difficulty inherent in agenda-based approaches of comparing edges of radically different span lengths and characteristics; and 2) Since the agendas are very small, the overhead of agenda maintenance—a large component of agenda-based parse time —is minimal. 2.3 Beam-search parsing CYK parsing with a beam-search is a local pruning strategy, comparing edges within the same chart cell. The beam-width can be defined in terms of a threshold in the number of edges allowed, or in terms of a threshold on the difference in probability relative to the highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>10</volume>
<pages>1--8</pages>
<location>Philadelphia.</location>
<contexts>
<context position="14472" citStr="Collins, 2002" startWordPosition="2308" endWordPosition="2309">onstituents, we first map cells in the training corpus to tuples: -b(S,i,j) = (x, y) (1) where x is a feature-vector representation of the chart cell and y is the target class 1 if the cell contains an edge from the maximum likelihood parse tree, 0 otherwise. The feature vector x is encoded with the chart cell’s absolute and relative span width, as well as unigram and bigram lexical and part-ofspeech tag items from wz_1 ... wj+2. Given feature/target tuples (x, y) for every chart cell in every sentence of a training corpus T, we train a weight vector 0 using the averaged perceptron algorithm (Collins, 2002) to learn an open/closed binary decision boundary: �B = argmin � �x�������� where H(·) is the unit step function: 1 if the inner product 0 · x &gt; 0, and 0 otherwise; and La(·, ·) is an asymmetric loss function, defined below. When predicting cell closure, all misclassifications are not equal. If we leave open a cell which contains no edges in the maximum likelihood (ML) parse, we incur the cost of additional processing, but are still able to recover the ML tree. However, if we close a chart cell which contains an ML edge, search errors occur. To deal with this imbalance, we introduce an asymmet</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing, volume 10, pages 1–8, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning, ICML ’05,</booktitle>
<pages>169--176</pages>
<location>New York, NY, USA.</location>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction. In Proceedings of the 22nd international conference on Machine learning, ICML ’05, pages 169–176, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and Multiple-Pass parsing.</title>
<date>1997</date>
<booktitle>Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>11--25</pages>
<contexts>
<context position="10714" citStr="Goodman, 1997" startWordPosition="1688" endWordPosition="1689">in probability relative to the highest scoring edge (Collins, 1999; Zhang et al., 2010). For the current paper, we use both kinds of thresholds, avoiding pathological cases that each individual criteria is prone to encounter. Further, unlike most beam-search approaches we will make use of a FOM estimate of the posterior probability of an edge, defined above, as our ranking function. Finally, we will learn log linear models to assign cellspecific thresholds, rather than relying on a single search parameter. 2.4 Coarse-to-Fine Parsing Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a </context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and Multiple-Pass parsing. Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 11–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2264" citStr="Johnson, 1998" startWordPosition="331" endWordPosition="332">ions, constituent parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntacti</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL ’03),</booktitle>
<pages>40--47</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2289" citStr="Klein and Manning, 2003" startWordPosition="333" endWordPosition="336">nt parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure ve</context>
<context position="8938" citStr="Klein and Manning, 2003" startWordPosition="1402" endWordPosition="1405">tion (Daum´e and Marcu, 2005). The method we present in this paper to learn the optimal beam-search parameters is applicable to any ranking function, and we demonstrate this by computing results with both the Boundary FOM and only the inside probability in Section 6. 2.2 Agenda-based parsing Agenda-based parsers maintain a global agenda of edges, ranked by FOM score. At each iteration, the highest-scoring edge is popped off of the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate a(NZ�j) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. A* parsing. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL ’03), pages 40–47, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2289" citStr="Klein and Manning, 2003" startWordPosition="333" endWordPosition="336">nt parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure ve</context>
<context position="8938" citStr="Klein and Manning, 2003" startWordPosition="1402" endWordPosition="1405">tion (Daum´e and Marcu, 2005). The method we present in this paper to learn the optimal beam-search parameters is applicable to any ranking function, and we demonstrate this by computing results with both the Boundary FOM and only the inside probability in Section 6. 2.2 Agenda-based parsing Agenda-based parsers maintain a global agenda of edges, ranked by FOM score. At each iteration, the highest-scoring edge is popped off of the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate a(NZ�j) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<location>Treebank-3, Philadelphia.</location>
<contexts>
<context position="23555" citStr="Marcus et al., 1999" startWordPosition="3854" endWordPosition="3858">cale represents the relative size of the beam-width, black being the maximum beam-width value, b, and the lightest gray being a beam-width of size one. We can see from this figure that very few chart cells are classified as needing the full 15 edges, apart from span-1 cells which we do not classify. 445 Figure 3: Visualization of Beam-Width Prediction for a single example sentence. The grey scale represents the size of the predicted beam-width: white is 0 (cell is skipped) and black is the maximum value b (b=15 in this example). 5 Experimental Setup We run all experiments on the WSJ treebank (Marcus et al., 1999) using the standard splits: section 2-21 for training, section 22 for development, and section 23 for testing. We preprocess the treebank by removing empty nodes, temporal labels, and spurious unary productions (X—*X), as is standard in published works on syntactic parsing. The pruning methods we present in this paper can be used to parse with any grammar. To achieve stateof-the-art accuracy levels, we parse with the Berkeley SM6 latent-variable grammar (Petrov and Klein, 2007b) where the original treebank non-terminals are automatically split into subclasses to optimize parsing accuracy. This</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL ’05,</booktitle>
<pages>75--82</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2344" citStr="Matsuzaki et al., 2005" startWordPosition="342" endWordPosition="345"> potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often un</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL ’05, pages 75–82, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<pages>34--513</pages>
<contexts>
<context position="2913" citStr="Nivre, 2008" startWordPosition="435" endWordPosition="436">t variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inf</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Comput. Linguist., 34:513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>Chris Quirk</author>
</authors>
<title>Topdown k-best a* parsing. In</title>
<date>2010</date>
<booktitle>In proceedings of the Annual Meeting on Association for Computational Linguistics Short Papers, ACLShort ’10,</booktitle>
<pages>200--204</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9365" citStr="Pauls et al., 2010" startWordPosition="1466" endWordPosition="1469">the agenda, added to the chart, and combined with other edges already in the chart. The agenda-based approach includes best-first parsing (Bobrow, 1990) and A* parsing (Klein and Manning, 2003a), which differ in whether an admissible FOM estimate a(NZ�j) is required. A* uses an admissible FOM, and thus guarantees finding the maximum likelihood parse, whereas an inadmissible heuristic (best-first) may require less exploration of the search space. Much work has been pursued in both admissible and inadmissible heuristics for agenda parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003a; Pauls et al., 2010). In this paper, we also make use of agendas, but at a local rather than a global level. We maintain an agenda for each cell, which has two significant benefits: 1) Competing edges can be compared directly, avoiding the difficulty inherent in agenda-based approaches of comparing edges of radically different span lengths and characteristics; and 2) Since the agendas are very small, the overhead of agenda maintenance—a large component of agenda-based parse time —is minimal. 2.3 Beam-search parsing CYK parsing with a beam-search is a local pruning strategy, comparing edges within the same chart c</context>
</contexts>
<marker>Pauls, Klein, Quirk, 2010</marker>
<rawString>Adam Pauls, Dan Klein, and Chris Quirk. 2010. Topdown k-best a* parsing. In In proceedings of the Annual Meeting on Association for Computational Linguistics Short Papers, ACLShort ’10, pages 200–204, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="3136" citStr="Petrov and Klein (2007" startWordPosition="468" endWordPosition="471">rammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods 440 Proceedings of the 49th Annual Meeting of the Association for Computational Linguisti</context>
<context position="11259" citStr="Petrov and Klein, 2007" startWordPosition="1777" endWordPosition="1780">ng Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a non-trivial problem, and most often approached with detailed knowledge of the fine grammar being used. For example, Goodman (1997) suggests using a coarse grammar consisting of regular non-terminals, such as NP and VP, and then non-terminals augmented with head-word information for the more accurate second-pass grammar. Such an approach is followed by Charniak (2000) as well. Petrov and Klein (2007a) derive coarse grammars in a more statistically principled way, although the technique is closely tied to their latent variable grammar repres</context>
<context position="24036" citStr="Petrov and Klein, 2007" startWordPosition="3932" endWordPosition="3935">nd black is the maximum value b (b=15 in this example). 5 Experimental Setup We run all experiments on the WSJ treebank (Marcus et al., 1999) using the standard splits: section 2-21 for training, section 22 for development, and section 23 for testing. We preprocess the treebank by removing empty nodes, temporal labels, and spurious unary productions (X—*X), as is standard in published works on syntactic parsing. The pruning methods we present in this paper can be used to parse with any grammar. To achieve stateof-the-art accuracy levels, we parse with the Berkeley SM6 latent-variable grammar (Petrov and Klein, 2007b) where the original treebank non-terminals are automatically split into subclasses to optimize parsing accuracy. This is an explicit grammar consisting of 4.3 million productions, 2.4 million of which are lexical productions. Exhaustive CYK parsing with the grammar takes more than a minute per sentence. Accuracy is computed from the 1-best Viterbi (max) tree extracted from the chart. Alternative decoding methods, such as marginalizing over the latent variables in the grammar or MaxRule decoding (Petrov and Klein, 2007a) are certainly possible in our framework, but it is unknown how effective</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007a. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404– 411, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Learning and inference for hierarchically split PCFGs.</title>
<date>2007</date>
<booktitle>In AAAI</booktitle>
<note>(Nectar Track).</note>
<contexts>
<context position="3136" citStr="Petrov and Klein (2007" startWordPosition="468" endWordPosition="471">rammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods 440 Proceedings of the 49th Annual Meeting of the Association for Computational Linguisti</context>
<context position="11259" citStr="Petrov and Klein, 2007" startWordPosition="1777" endWordPosition="1780">ng Coarse-to-fine parsing, also known as multiple pass parsing (Goodman, 1997; Charniak, 2000; Charniak and Johnson, 2005), first parses the input sentence with a simplified (coarse) version of the target (fine) grammar in which multiple non-terminals are merged into a single state. Since the coarse grammar is quite small, parsing is much faster than with the fine grammar, and can quickly yield an estimate of the outside probability α(·) for use in subsequent agenda or beam-search parsing with the fine grammar. This approach can also be used iteratively with grammars of increasing complexity (Petrov and Klein, 2007a). Building a coarse grammar from a fine grammar is a non-trivial problem, and most often approached with detailed knowledge of the fine grammar being used. For example, Goodman (1997) suggests using a coarse grammar consisting of regular non-terminals, such as NP and VP, and then non-terminals augmented with head-word information for the more accurate second-pass grammar. Such an approach is followed by Charniak (2000) as well. Petrov and Klein (2007a) derive coarse grammars in a more statistically principled way, although the technique is closely tied to their latent variable grammar repres</context>
<context position="24036" citStr="Petrov and Klein, 2007" startWordPosition="3932" endWordPosition="3935">nd black is the maximum value b (b=15 in this example). 5 Experimental Setup We run all experiments on the WSJ treebank (Marcus et al., 1999) using the standard splits: section 2-21 for training, section 22 for development, and section 23 for testing. We preprocess the treebank by removing empty nodes, temporal labels, and spurious unary productions (X—*X), as is standard in published works on syntactic parsing. The pruning methods we present in this paper can be used to parse with any grammar. To achieve stateof-the-art accuracy levels, we parse with the Berkeley SM6 latent-variable grammar (Petrov and Klein, 2007b) where the original treebank non-terminals are automatically split into subclasses to optimize parsing accuracy. This is an explicit grammar consisting of 4.3 million productions, 2.4 million of which are lexical productions. Exhaustive CYK parsing with the grammar takes more than a minute per sentence. Accuracy is computed from the 1-best Viterbi (max) tree extracted from the chart. Alternative decoding methods, such as marginalizing over the latent variables in the grammar or MaxRule decoding (Petrov and Klein, 2007a) are certainly possible in our framework, but it is unknown how effective</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007b. Learning and inference for hierarchically split PCFGs. In AAAI 2007 (Nectar Track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2366" citStr="Petrov et al., 2006" startWordPosition="346" endWordPosition="349">ications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the grammar, allowing individual grammar rules to be more sensitive to their surrounding context, at a considerable cost in efficiency. Grammar transformation techniques such as linguistically inspired non-terminal annotations (Johnson, 1998; Klein and Manning, 2003b) and latent variable grammars (Matsuzaki et al., 2005; Petrov et al., 2006) have increased the grammar size |G |from a few thousand rules to several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constitue</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="3050" citStr="Petrov et al., 2010" startWordPosition="455" endWordPosition="458">o several million in an explicitly enumerable grammar, or even more in an implicit grammar. Exhaustive search for the maximum likelihood parse tree with a state-of-the-art grammar can require over a minute of processing for a single sentence of 25 words, an unacceptable amount of time for real-time applications or when processing millions of sentences. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al., 2010). The most accurate constituent parsers, e.g., Charniak (2000), Petrov and Klein (2007a), make use of approximate inference, limiting their search to a fraction of the total search space and achieving speeds of between one and four newspaper sentences per second. The paradigm for building stateof-the-art parsing models is to first design a model structure that can achieve high accuracy and then, after the model has been built, design effective approximate inference methods around that particular model; e.g., coarse-to-fine non-terminal hierarchies for a given model, or agenda-based methods 440</context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1431" citStr="Punyakanok et al., 2008" startWordPosition="206" endWordPosition="209">onstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increase in the size of the gram</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In Donia Scott and Hans Uszkoreit, editors, Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>745--752</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="6828" citStr="Roark and Hollingshead (2008" startWordPosition="1058" endWordPosition="1061"> doing unsupervised parsing. Figure 1: Inside (grey) and outside (white) representations of an example chart edge Nij. proach is broadly applicable to a wide range of scenarios, including tuning the search to new domains where domain mismatch may yield very different efficiency/accuracy operating points. In the next section, we present prior work on approximate inference in parsing, and discuss how our method to learn optimal beam-search parameters unite many of their strengths into a single framework. We then explore using our approach to open or close cells in the chart as an alternative to Roark and Hollingshead (2008; 2009). Finally, we present results which combine cell closure and adaptive beam-width prediction to achieve the most efficient parser. 2 Background 2.1 Preliminaries and notation Let S = w1 ... w|S |represent an input string of |S |words. Let wi,j denote the substring from word wi+1 to wj; i.e., S = w0,|S|. We use the term chart edge to refer to a non-terminal spanning a specific substring of the input sentence. Let Ni,j denote the edge labeled with non-terminal N spanning wi,j, for example NP3,7. We define an edge’s figure-of-merit (FOM) as an estimate of the product of its inside (Q) and o</context>
<context position="12309" citStr="Roark and Hollingshead (2008" startWordPosition="1942" endWordPosition="1945">0) as well. Petrov and Klein (2007a) derive coarse grammars in a more statistically principled way, although the technique is closely tied to their latent variable grammar representation. To the extent that our cell-specific threshold classifier predicts that a chart cell should contain zero edges or more than zero edges, it is making coarse 442 predictions about the unlabeled constituent structure of the target parse tree. This aspect of our work is can be viewed as a coarse-to-fine process, though without considering specific grammatical categories or rule productions. 2.5 Chart Constraints Roark and Hollingshead (2008; 2009) introduced a pruning technique that ignores entire chart cells based on lexical and POS features of the input sentence. They train two finite-state binary taggers: one that allows multi-word constituents to start at a word, and one that allows constituents to end at a word. Given these tags, it is straightforward to completely skip many chart cells during processing. In this paper, instead of tagging word positions to infer valid constituent spans, we classify chart cells directly. We further generalize this cell classification to predict the beam-width of the chart cell, where a beam-</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Donia Scott and Hans Uszkoreit, editors, Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 745– 752, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity Context-Free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>647--655</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="18380" citStr="Roark and Hollingshead, 2009" startWordPosition="2957" endWordPosition="2960">stive CYK algorithm, or any of the approximate inference methods mentioned in Section 2. We empirically evaluate them when applied to CYK parsing and beam-search parsing in Section 6. 4 Beam-Width Prediction The cell-closing approaches discussed in Section 3 make binary decisions to either allow or completely block all edges in each cell. This all-on/all-off tactic ignores the characteristics of the local cell population, which, given a large statistical grammar, may contain hundred of edges, even if very improbable. Retaining all of these partial derivations forces the (a) Chart Constraints (Roark and Hollingshead, 2009) (b) Constituent Closure (this paper) (c) Complete Closure (this paper) Figure 2: Comparison of Chart Constraints (Roark and Hollingshead, 2009) to Constituent and Complete Closure for a single example sentence. Black cells are open to all edges while grey cells only allow factored edges (incomplete constituents). search in larger spans to continue down improbable paths, adversely affecting efficiency. We can further improve parsing speed in these open cells by leveraging local pruning methods, such as beam-search. When parsing with a beam-search, finding the optimal beam-width threshold(s) to</context>
</contexts>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity Context-Free parsing pipelines via chart constraints. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 647–655, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tzong-Han Tsai</author>
<author>Chia-Wei Wu</author>
<author>Yu-Chun Lin</author>
<author>WenLian Hsu</author>
</authors>
<title>Exploiting full parsing information to label semantic roles using an ensemble of ME and SVM via integer linear programming.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05,</booktitle>
<pages>233--236</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1405" citStr="Tsai et al., 2005" startWordPosition="202" endWordPosition="205"> to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions. 1 Introduction Statistical constituent parsers have gradually increased in accuracy over the past ten years. This accuracy increase has opened the door to automatically derived syntactic information within a number of NLP tasks. Prior work incorporating parse structure into machine translation (Chiang, 2010) and Semantic Role Labeling (Tsai et al., 2005; Punyakanok et al., 2008) indicate that such hierarchical structure can have great benefit over shallow labeling techniques like chunking and part-of-speech tagging. Although syntax is becoming increasingly important for large-scale NLP applications, constituent parsing is slow—too slow to scale to the size of many potential consumer applications. The exhaustive CYK algorithm has computational complexity O(n3|G|) where n is the length of the sentence and |G |is the number of grammar productions, a nonnegligible constant. Increases in accuracy have primarily been accomplished through an increa</context>
</contexts>
<marker>Tsai, Wu, Lin, Hsu, 2005</marker>
<rawString>Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and WenLian Hsu. 2005. Exploiting full parsing information to label semantic roles using an ensemble of ME and SVM via integer linear programming. In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05, pages 233–236, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Byung gyu Ahn</author>
<author>Stephen Clark</author>
<author>Curt Van Wyk</author>
<author>James R Curran</author>
<author>Laura Rimell</author>
</authors>
<title>Chart pruning for fast Lexicalised-Grammar parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1472--1479</pages>
<location>Beijing, China.</location>
<marker>Zhang, Ahn, Clark, Van Wyk, Curran, Rimell, 2010</marker>
<rawString>Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van Wyk, James R. Curran, and Laura Rimell. 2010. Chart pruning for fast Lexicalised-Grammar parsing. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1472–1479, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>