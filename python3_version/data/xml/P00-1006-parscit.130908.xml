<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<note confidence="0.40287275">
A Maximum Entropy/Minimum Divergence Translation Model
George Foster
RALI, Universite de Montreal
fosterAiro.umontreaLea
</note>
<sectionHeader confidence="0.799513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808833333333">
I present empirical comparisons be-
tween a linear combination of stan-
dard statistical language and trans-
lation models and an equivalent
Maximum Entropy/Minimum Di-
vergence (MEMD) model, using sev-
eral different methods for auto-
matic feature selection. The MEMD
model significantly outperforms the
standard model in test corpus per-
plexity, even though it has far fewer
parameters.
</bodyText>
<sectionHeader confidence="0.995551" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999388272727273">
Statistical Machine Translation (SMT) sys-
tems use a model of p(t Is), the probability
that a text s in the source language will trans-
late into a text t in the target language, to de-
termine the best translation for a given source
text. The standard approach to modeling this
distribution relies on a &amp;quot;noisy channel&amp;quot; de-
composition into a language model p(t) and a
translation model p (sit) , which correspond re-
spectively to prior and likelihood components
in a Bayesian formulation:
</bodyText>
<equation confidence="0.955797">
p(t&apos;s) = p(t)p(slt)/Ep(t)p(slt)
P(t)p(slt),
</equation>
<bodyText confidence="0.999952875">
where proportionality holds when searching
for the optimum target text t for a given
source text s. This equation has been called
the &amp;quot;fundamental equation of SMT&amp;quot; (Brown
et al., 1993).
In this paper, I investigate an alternate
technique for modeling p(t Is), based on a di-
rect chain-rule expansion of the form:
</bodyText>
<equation confidence="0.923610333333333">
It I
p(tis) = llp(tz It&apos; tz_i, s), (1)
z=1
</equation>
<bodyText confidence="0.998877029411765">
where tz denotes the ith token in t.1 The ob-
jects to be modeled in this case belong to the
family of conditional distributions p(w lh,$),
where w is a target word at a particular po-
sition in t, and h denotes the tokens which
precede it in t. The main motivation for
this approach is that it simplifies the &amp;quot;decod-
ing&amp;quot; problem of finding the most likely target
text according to the model. In particular, if
h is known, the problem of finding the best
word at the current position requires only a
straightforward search through the target vo-
cabulary, and simple and efficient dynamic-
programming based heuristics can be used to
extend this to sequences of words. This is very
important for applications such as TransType
(Foster et al., 1997; Langlais et al., 2000),
where the task is to make real-time predic-
tions of the text a human translator will type
next, based on the source text under transla-
tion and some prefix of the target text that
has already been typed.
The main drawback to modeling p(t Is) in
terms of p(w I h, s) is that the latter distri-
bution is conditioned on two very disparate
sources of information which are difficult to
combine in a complementary way. One sim-
ple strategy is to use a linear combination of
This ignores the issue of normalization over tar-
get texts of all possible lengths, which can be easily
enforced when desired by using a stop token or a prior
distribution over lengths.
language and translation components, of the
form:
</bodyText>
<equation confidence="0.996179">
p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2)
</equation>
<bodyText confidence="0.995926244444445">
where A E [0,1] is a combining weight. How-
ever, this is a weak model because it aver-
ages over the relative strengths of its com-
ponents; when p(wih) is likely to be a more
accurate estimate than P(wls), it is obvious
that the model should rely more heavily on
p(w Ih), and vice versa, rather than using a
fixed weight. In theory this could be partially
remedied by making A depend on h and s,
but in practice significant improvements with
this technique have proven elusive (Langlais
and Foster, 2000). The noisy channel model
avoids this problem by making predictions
based on h the responsibility of the language
model p(t), and those based on s the respon-
sibility of the translation model p(slt), and
combining the two in an optimum way. But
this comes at the cost of increased decod-
ing complexity, because the chain rule can no
longer be applied as in (1) due to the reversed
direction of the translation model. Much re-
cent research in SMT, eg (Garcia-Varea et al.,
1998; Niessen et al., 1998; Och et al., 1999;
Wang and Waibel, 1998) deals with the de-
coding problem, either directly or indirectly
because of constraints imposed on the form
of the translation model.
A statistical technique which has recently
become popular for NLP is Maximum En-
tropy/Minimum Divergence (MEMD) model-
ing (Berger et al., 1996). One of the main
strengths of MEMD is that it allows informa-
tion from different sources to be combined in a
principled and effective way, so it is a natural
choice for modeling p(wlh, s) In this paper,
I describe a MEMD model for p(wlh, s) and
compare its performance to that of an equiv-
alent linear model. I also evaluate several
different methods for MEMD feature selec-
tion, including a new algorithm due to Printz
(1998). To my knowledge, this is the first ap-
plication of MEMD to building a large-scale
translation model, and one of the few direct
comparisons between a MEMD model and an
almost exactly equivalent linear mode1.2
</bodyText>
<sectionHeader confidence="0.985354" genericHeader="introduction">
2 Models
</sectionHeader>
<subsectionHeader confidence="0.8681">
2.1 Linear Model
</subsectionHeader>
<bodyText confidence="0.998180714285714">
The baseline model is a linear combination as
in (2) of a standard interpolated trigram (Je-
linek and Mercer, 1980) for p(will) and the
IBM model 1 (IBM1) (Brown et al., 1993)
for P(wls). As originally formulated, IBM1
models the distribution p(t Is), but since tar-
get text tokens are predicted independently,
it can also be used for P(wls). The underly-
ing generative process is as follows: 1) pick a
token s at random in s, independent of the po-
sitions of w and s; 2) choose w according to a
word-for-word translation probability p(w 18).
Summing over all choices for s gives the com-
plete model:
</bodyText>
<equation confidence="0.99697">
p(wis) = Ep(wisi)/asi +1)
j=0
</equation>
<bodyText confidence="0.999982125">
where si is the jth token in s for j &gt; 0, and
so is a special null token prepended to each
source sentence to account for target words
which have no direct translations. The word-
pair parameters P(wls) can be estimated from
a bilingual corpus of aligned sentence pairs us-
ing the EM algorithm, as described in (Brown
et al., 1993).
</bodyText>
<subsectionHeader confidence="0.984184">
2.2 MEMD Model
</subsectionHeader>
<bodyText confidence="0.999236">
A MEMD model for p(wlh , s) has the general
form:
</bodyText>
<equation confidence="0.982114333333333">
q(wlh, s) exp(5 f (w , h, s))
p(wlh,$) =
Z(h, s)
</equation>
<bodyText confidence="0.989171632653062">
where q(w I h, s) is a reference distribu-
tion, f (w ,h, s) maps (w, h, s) into an n-
dimensional feature vector, is a corre-
sponding vector of feature weights (the pa-
rameters of the model), and Z(h, s) =
Ew q(w I h, s) exp(. f(w, h)) is a normalizing
factor.
2Rosenfeld (1996) reports a greater perplexity re-
duction (23% versus 10%) over a baseline trigram lan-
guage model due the use of ME versus linear word
triggers. However, since the models tested apparently
differed in other aspects, it is hard to determine how
much of this gain can be attributed to the use of ME.
It can be shown (Berger et al., 1996) that
the use of this model with maximum like-
lihood parameter estimation is justified on
information-theoretic grounds when q repre-
sents some prior knowledge about the true
distribution and when the expected values of
f in the training corpus are identical to their
true expected values.3 There is no require-
ment that the components of f represent dis-
joint or statistically independent events. This
result motivates the use of MEMD models,
but it offers only weak guidance on how to
select q or f. In practice, q is usually chosen
on the basis of efficiency considerations (when
the information it captures would be compu-
tationally expensive to represent as compo-
nents of f), and f is established using heuris-
tics such as described in the next section.
Once q and f have been chosen, the ITS algo-
rithm (Della Pietra et al., 1995) can be used
to find maximum likelihood parameter values.
In the current context, since the aim was to
compare equivalent linear and MEMD mod-
els, I used an interpolated trigram as the ref-
erence distribution q and boolean indicator
functions over bilingual word pairs as features
(ie, components of f). A pair of source,target
words (8 ,t) has a corresponding feature func-
tion:
{ 1, s E s and t = w
fst (w , h, s) =
0, else
Using the notational convention that a,,t is 0
whenever the corresponding feature fst does
not exist in the model, the final MEMD model
can be written compactly as:
</bodyText>
<equation confidence="0.9096685">
P(wlh, = q(wlh) exp(E asw)/Z(h, s).
s E s
</equation>
<bodyText confidence="0.8074265">
This model is structurally quite similar to the
one defined in the previous section:
</bodyText>
<equation confidence="0.93991725">
Isl
1 — A
p(wlh,$) = Aq(wih) + 2p(wIsi)
Is&apos; +1
</equation>
<bodyText confidence="0.980333264705883">
3Another interpretation, which has been less well
publicized in the NLP literature, is that of a single-
layer neural net with certain weight constraints and a
&amp;quot;softmax&amp;quot; output function (Bishop, 1995).
with the MEMD feature weights asw playing
the role of the IBM1 probabilities P(wls) and
the MEMD model summing over contribu-
tions from source sentence words rather than
tokens for efficiency. If there are m free pa-
rameters in the trigram and n word pairs, the
MEMD model will contain m + n free pa-
rameters and the linear model will contain
77 n ± 1 — &apos;Vs&apos; IVtl 14 free parameters, so
if the source and target vocabulary sizes IV,I
and IVt I are equal the two models will contain
precisely the same number of free parameters.
One important practical difference between
the two models is the requirement to calcu-
late the MEMD normalizing factor Z(h, s) for
each context in which this model is used. This
makes the MEMD model much more compu-
tationally expensive than the linear model, so
that it is not feasible to have it incorporate all
available word-pair features (ie all bilingual
pairs of words which cooccur in some aligned
sentence pair in the training corpus). More-
over, since the empirical expectations of fea-
tures are supposed to reflect their true values,
having a feature for every cooccurring pair in
the corpus would be theoretically inadvisable
even if it were computationally feasible. Some
method of selecting a subset of reliable fea-
tures is therefore required, as described in the
next section.
</bodyText>
<sectionHeader confidence="0.992489" genericHeader="method">
3 Feature Selection
</sectionHeader>
<bodyText confidence="0.999807">
I experimented with three methods for select-
ing bilingual word pairs for inclusion in the
models. All methods assign scores to individ-
ual pairs, so feature subsets of any desired size
can be extracted by taking the highest-ranked
pairs.
</bodyText>
<subsectionHeader confidence="0.981416">
3.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.999654">
The simplest scoring method was mutual in-
formation (MI), defined for a pair (s,t) as:
</bodyText>
<equation confidence="0.764342857142857">
p(x,y)
= E E 25(x, y) log
13(x)13(Y)&apos;
xc{s,g} y Eft,q
40ne free combining weight, one normalization
constraint per source word, and IV
P(wlso) t I — 1 free parame-
</equation>
<bodyText confidence="0.9459294">
ters from
where P(s, t) is the probability that a ran-
domly chosen pair of cooccurring source and
target tokens in the corpus is (s, t); (s, t) is
the probability that the source token is s and
the target token is not t; etc; and P (x) and
25(y) are the left and right marginals off9(x, y).
Mutual information measures the degree to
which s and t are non-independent, so it is a
reasonable choice for scoring pairs.
</bodyText>
<subsectionHeader confidence="0.998267">
3.2 MEMD Gains
</subsectionHeader>
<bodyText confidence="0.999843">
The second scoring method was an approxi-
mation of the MEMD gain for feature fst, de-
fined as the log-likelihood difference between
a MEMD model which includes this feature
and one which does not:
</bodyText>
<equation confidence="0.9959125">
1 Pst (T IS)
G8t = ITI log P(TIS)
</equation>
<bodyText confidence="0.999984416666667">
where the training corpus (5, T) consists of
a set of (statistically independent) sentence
pairs (s, t), and pst is the model which in-
cludes ht. Since MEMD models are trained
by finding the set of feature weights which
maximizes the likelihood of the training cor-
pus, it is natural to rate features according
to how much they contribute to this likeli-
hood. A powerful strategy for using gains is
to build a model iteratively by adding at each
step the feature which gives the highest gain
with respect to those already added. Berger
et al (1996) describe an efficient algorithm for
accomplishing this in which approximations
to Pst (TIS) are computed in parallel for all
(new) features ft by holding all weights in
the existing model fixed and optimizing only
over a8t. However, this method requires many
expensive passes over the corpus to optimize
the weights for the set of features under con-
sideration at each step, and it adds only one
feature per step, so it is not practical for con-
structing models containing thousands of fea-
tures or more.
In a recent paper (Printz, 1998), Printz ar-
gues that it is usually sufficient to perform
the iteration described in the previous para-
graph only once, in other words that fea-
tures can be ranked simply according to their
gain with respect to some initial model. He
also gives an algorithm for computing gains
using a numerical approximation which re-
quires only a single pass over the training cor-
pus. I adopted Printz&apos; method for computing
MEMD gains, using the reference trigram as
the initial model.
</bodyText>
<subsectionHeader confidence="0.98959">
3.3 IBM1 Gains
</subsectionHeader>
<bodyText confidence="0.9998925">
The final scoring method involved the gain of
each word-pair parameter p(tis) within IBM1.
Instead of taking gains with respect to an ini-
tial model as in the previous section, I com-
puted them with respect to a &amp;quot;full&amp;quot; model
which incorporated all available word pairs:
</bodyText>
<equation confidence="0.993148666666667">
1 p(TIS)
G8 = log
TI (TIS)
</equation>
<bodyText confidence="0.9796855">
where pwt denotes the full IBM1 model p with
the parameter p(tis) set to zero and the result-
ing distribution /9(49) renormalized. The ad-
vantage of this method is that it gives a mea-
sure of each parameter&apos;s worth in the presence
of other parameters. As is the previous sec-
tion, this is an approximation because deter-
mining the true gain would require retraining
pwt and not merely renormalizing.
A problem with IBM1 gains is that they are
not very robust. If the corpus contains a sen-
tence pair (s, t) which consists only of a single
word pair (8, t), then G,,t will contain the term
1p(t1.9)+p(t1.90)
lo_
1,7-1 6 p(tIso) , so if p(tIso) is close to zero
(as is frequently the case), G,,t will be close
to infinity, even though (s, t) may occur only
once in the training corpus. To remedy this, I
computed gains with respect to a linear com-
bination of IBM1 and a smoothing model u,
of the form Ap(w Is) ± (1— ).)u (w I h, s). In the
experiments reported below, I used a uniform
distribution for u, with A = .99.5
Smoothed IBM1 gains can be computed in
parallel in a single pass over the training cor-
pus using the algorithm in figure 1. The line
marked with an asterisk takes into account
the increase in p(tis) due to renormalizing
the distribution /9(49) after setting p(tils) to
</bodyText>
<footnote confidence="0.99696475">
5Another interesting choice for u would be the in-
terpolated trigram, which would make the method de-
scribed here more similar to the MEMD gain ranking
described in the previous section.
</footnote>
<page confidence="0.839819">
45
</page>
<figure confidence="0.950885962962963">
MI
MEMD
IBM1
40
test corpus perplexity
❯
35
30
25
20
0 5000 10000 15000 20000 25000 30000
number of features
trigram+IBM1
100 1000 10000 100000 1e+06 1e+07 1e+08
number of parameters
s
❴
test corpus perplexity
❴
58
56
54
52
50
48
46
44
</figure>
<page confidence="0.827553">
42
</page>
<bodyText confidence="0.9999634">
be the case: MEMD essentially multiplies pre-
dictive scores arising from different sources
rather than averaging them. This gives infor-
mation sources which assign either very high
or very low scores much more influence over
the final result. When such scores are based
upon reliable evidence, this will lead to better
models.
One somewhat surprising result of these ex-
periments was that the IBM1 gains feature
selection method resulted in better models
than the MEMD gains method, despite the
fact that the latter is based on a much more
direct measure of each feature&apos;s worth within
the MEMD model. A possible explanation
for this is that the gain over the reference tri-
gram is not a good predictor of the gain in
the presence of many other features; this is
borne out by the fact that, for very small fea-
ture sets (on the order of 100 words and less),
the MEMD method did outperform the IBM1
method. Another explanation is inaccura-
cies in the gain approximations computed by
Printz&apos; method, which involves many numer-
ical parameters that require tuning. Further
investigation is required into this and other
techniques for finding valid word pairs, since
all methods tested yielded significant quan-
tities of noise beyond 30,000 pairs. Because
the source vocabulary contains about 50,000
words this is obviously an unrealistically small
number of translations.
Although the main use for the model I
have described in this paper is in applica-
tions like TransType which need to make
rapid predictions of upcoming target text, it
is interesting to speculate about whether a
MEMD model for p(w I h, s) could also be use-
ful for SMT. Compared to the standard noisy
channel approach, this has the advantage of
permitting much less complex search proce-
dures; of allowing any information which is
directly observable in the training corpus to
be very easily incorporated into the model
via boolean features; and of an estimation
procedure where translation model parame-
ters can be optimized for use with an existing
language mode1.7 Disadvantages include the
high cost of training MEMD models, the fact
that p(w I h, s) is somewhat less general than
P(slt) for building realistic translation mod-
els; and the lack of a mechanism equivalent
to the EM algorithm for incorporating &amp;quot;hid-
den&amp;quot; variables into MEMD models (see (Fos-
ter, 2000) for a discussion of this problem).
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974863636363">
The problem of searching for the best target
text in statistical translation applications can
be greatly simplified if the fundamental dis-
tribution p(t Is) is expanded directly in terms
of the distribution p(w I h, s), rather than us-
ing the standard noisy-channel approach. I
compared a simple linear model for p (w I h, s)
based on IBM&apos;s model 1 with an equivalent
MEMD model, and found that the MEMD
model has over 45% lower test corpus per-
plexity, despite using two orders of magnitude
fewer parameters. I also compared several
methods for selecting MEMD word-pair fea-
tures, and found that a simple method which
ranks pairs according to their gain within
model 1 offers slightly better performance and
significantly lower computational cost than a
more general MEMD feature-selection algo-
rithm due to Printz. Finally, I suggest that it
may be fruitful to explore the idea of using a
MEMD model for p (w I h, s) as an alternative
to the noisy-channel approach to SMT.
</bodyText>
<sectionHeader confidence="0.993205" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999932142857143">
This work was carried out as part of the
TransType project at RALI, funded by the
Natural Sciences and Engineering Research
Council of Canada. I wish to thank Guy La-
palme and Andreas Eisele for comments on
the paper, and Philippe Langlais for inspiring
discussions.
</bodyText>
<sectionHeader confidence="0.996409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9769784">
Yaser Al-Onaizan, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan Melamed,
Franz-Josef Och, David Purdy, Noah A.
Smith, and David Yarowsky. 1999. Sta-
tistical machine translation: Final report,
</reference>
<footnote confidence="0.641268">
71n principle, both language and translation corn- ponents could be trained simultaneously.
</footnote>
<note confidence="0.462923">
JHU workshop 1999. Technical report,
The Center for Language and Speech Pro-
cessing, The Johns Hopkins University,
www.clsp.jhu.edu/ws99/projects/mt/final_report
</note>
<reference confidence="0.9990483">
Adam L. Berger, Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A Maximum En-
tropy approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39-71.
Christopher M. Bishop. 1995. Neural Networks
for Pattern Recognition. Oxford.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguis-
tics, 19(2):263-312, June.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, CMU.
George Foster, Pierre Isabelle, and Pierre Plam-
ondon. 1997. Target-text Mediated Interac-
tive Machine Translation. Machine Transla-
tion, 12:175-194.
George Foster. 2000. Incorporating position in-
formation into a Maximum Entropy / Mini-
mum Divergence translation model. In Pro-
ceedings of the 4th Computational Natural Lan-
guage Learning Workshop (CoNLL), Lisbon,
Portugal, September. ACL SigNLL.
Ismael Garcia-Varea, Francisco Casacuberta, and
Hermann Ney. 1998. An iterative, DP-based
search algorithm for statistical machine trans-
lation. In ICSLP-98 (ICS, 1998), pages 1135-
1138.
1998. Proceedings of the 5th International Con-
ference on Spoken Language Processing (IC-
SLP) 1998, Sydney, Australia, December.
F. Jelinek and R. L. Mercer. 1980. Interpolated
estimation of Markov source parameters from
sparse data. In E. S. Gelsema and L. N. Kanal,
editors, Pattern Recognition in Practice. North-
Holland, Amsterdam.
Ph. Langlais and G. Foster. 2000. Using context-
dependent interpolation to combine statistical
language and translation models for interactive
MT. In Content-Based Multimedia Information
Access (RIAO), Paris, France, April.
Philippe Langlais, Sebastien Saul* George Fos-
ter, Elliott Macklovitch, and Guy Lapalme.
2000. A comparison of theoretical and user-
oriented evaluation procedures of a new type of
interactive MT. In Second International Con-
ference On Language Resources and Evaluation
(LREC), pages 641-648, Athens, Greece, June.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann.
1998. A DP based search algorithm for sta-
tistical machine translation. In Proceedings
. of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL) and
17th International Conference on Computa-
tional Linguistics (COLING) 1998, pages 960-
967, Montréal, Canada, August.
Franz Josef Och, Christoph Tillmann, and Her-
mann Ney. 1999. Improved alignment models
for statistical machine translation. In Proceed-
ings of the 4nd Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
College Park, Maryland.
Harry Printz. 1998. Fast computation of Max-
imum Entropy/Minimum Divergence feature
gain. In ICSLP-98 (ICS, 1998), pages 2083-
2086.
Ronald Rosenfeld. 1996. A maximum entropy
approach to adaptive statistical language mod-
elling. Computer Speech and Language, 10:187-
228.
Michel Simard, George F. Foster, and Pierre Is-
abelle. 1992. Using cognates to align sen-
tences in bilingual corpora. In Proceedings of
the 4th Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI),
Montréal, Québec.
Ye-yi Wang and Alex Waibel. 1998. Fast decoding
for statistical machine translation. In ICSLP-
98 (ICS, 1998), pages 2775-2778.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882112">
<title confidence="0.997502">A Maximum Entropy/Minimum Divergence Translation Model</title>
<author confidence="0.998031">George Foster</author>
<affiliation confidence="0.974938">RALI, Universite de Montreal</affiliation>
<email confidence="0.991157">fosterAiro.umontreaLea</email>
<abstract confidence="0.993222692307692">empirical comparisons between a linear combination of standard statistical language and translation models and an equivalent Maximum Entropy/Minimum Divergence (MEMD) model, using several different methods for automatic feature selection. The MEMD model significantly outperforms the standard model in test corpus perplexity, even though it has far fewer parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<date>1999</date>
<note>Statistical machine translation: Final report,</note>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="4219" citStr="Berger et al., 1996" startWordPosition="720" endWordPosition="723">p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost e</context>
<context position="6554" citStr="Berger et al., 1996" startWordPosition="1154" endWordPosition="1157">,$) = Z(h, s) where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it ca</context>
<context position="11389" citStr="Berger et al (1996)" startWordPosition="2009" endWordPosition="2012">es this feature and one which does not: 1 Pst (T IS) G8t = ITI log P(TIS) where the training corpus (5, T) consists of a set of (statistically independent) sentence pairs (s, t), and pst is the model which includes ht. Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iter</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<location>Oxford.</location>
<contexts>
<context position="8343" citStr="Bishop, 1995" startWordPosition="1472" endWordPosition="1473">rresponding feature function: { 1, s E s and t = w fst (w , h, s) = 0, else Using the notational convention that a,,t is 0 whenever the corresponding feature fst does not exist in the model, the final MEMD model can be written compactly as: P(wlh, = q(wlh) exp(E asw)/Z(h, s). s E s This model is structurally quite similar to the one defined in the previous section: Isl 1 — A p(wlh,$) = Aq(wih) + 2p(wIsi) Is&apos; +1 3Another interpretation, which has been less well publicized in the NLP literature, is that of a singlelayer neural net with certain weight constraints and a &amp;quot;softmax&amp;quot; output function (Bishop, 1995). with the MEMD feature weights asw playing the role of the IBM1 probabilities P(wls) and the MEMD model summing over contributions from source sentence words rather than tokens for efficiency. If there are m free parameters in the trigram and n word pairs, the MEMD model will contain m + n free parameters and the linear model will contain 77 n ± 1 — &apos;Vs&apos; IVtl 14 free parameters, so if the source and target vocabulary sizes IV,I and IVt I are equal the two models will contain precisely the same number of free parameters. One important practical difference between the two models is the requirem</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1232" citStr="Brown et al., 1993" startWordPosition="187" endWordPosition="190">a text s in the source language will translate into a text t in the target language, to determine the best translation for a given source text. The standard approach to modeling this distribution relies on a &amp;quot;noisy channel&amp;quot; decomposition into a language model p(t) and a translation model p (sit) , which correspond respectively to prior and likelihood components in a Bayesian formulation: p(t&apos;s) = p(t)p(slt)/Ep(t)p(slt) P(t)p(slt), where proportionality holds when searching for the optimum target text t for a given source text s. This equation has been called the &amp;quot;fundamental equation of SMT&amp;quot; (Brown et al., 1993). In this paper, I investigate an alternate technique for modeling p(t Is), based on a direct chain-rule expansion of the form: It I p(tis) = llp(tz It&apos; tz_i, s), (1) z=1 where tz denotes the ith token in t.1 The objects to be modeled in this case belong to the family of conditional distributions p(w lh,$), where w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem</context>
<context position="5052" citStr="Brown et al., 1993" startWordPosition="869" endWordPosition="872">a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generative process is as follows: 1) pick a token s at random in s, independent of the positions of w and s; 2) choose w according to a word-for-word translation probability p(w 18). Summing over all choices for s gives the complete model: p(wis) = Ep(wisi)/asi +1) j=0 where si is the jth token in s for j &gt; 0, and so is a special null token prepended to each source sentence to account for target words which have no</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of Machine Translation: Parameter estimation. Computational Linguistics, 19(2):263-312, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144, CMU.</tech>
<contexts>
<context position="7383" citStr="Pietra et al., 1995" startWordPosition="1297" endWordPosition="1300">alues of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. This result motivates the use of MEMD models, but it offers only weak guidance on how to select q or f. In practice, q is usually chosen on the basis of efficiency considerations (when the information it captures would be computationally expensive to represent as components of f), and f is established using heuristics such as described in the next section. Once q and f have been chosen, the ITS algorithm (Della Pietra et al., 1995) can be used to find maximum likelihood parameter values. In the current context, since the aim was to compare equivalent linear and MEMD models, I used an interpolated trigram as the reference distribution q and boolean indicator functions over bilingual word pairs as features (ie, components of f). A pair of source,target words (8 ,t) has a corresponding feature function: { 1, s E s and t = w fst (w , h, s) = 0, else Using the notational convention that a,,t is 0 whenever the corresponding feature fst does not exist in the model, the final MEMD model can be written compactly as: P(wlh, = q(w</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<date>1997</date>
<booktitle>Target-text Mediated Interactive Machine Translation. Machine Translation,</booktitle>
<pages>12--175</pages>
<contexts>
<context position="2141" citStr="Foster et al., 1997" startWordPosition="351" endWordPosition="354">butions p(w lh,$), where w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem of finding the best word at the current position requires only a straightforward search through the target vocabulary, and simple and efficient dynamicprogramming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The main drawback to modeling p(t Is) in terms of p(w I h, s) is that the latter distribution is conditioned on two very disparate sources of information which are difficult to combine in a complementary way. One simple strategy is to use a linear combination of This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforc</context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1997</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1997. Target-text Mediated Interactive Machine Translation. Machine Translation, 12:175-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>Incorporating position information into a Maximum Entropy / Minimum Divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Computational Natural Language Learning Workshop (CoNLL),</booktitle>
<publisher>ACL SigNLL.</publisher>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="3408" citStr="Foster, 2000" startWordPosition="583" endWordPosition="584">ibution over lengths. language and translation components, of the form: p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2) where A E [0,1] is a combining weight. However, this is a weak model because it averages over the relative strengths of its components; when p(wih) is likely to be a more accurate estimate than P(wls), it is obvious that the model should rely more heavily on p(w Ih), and vice versa, rather than using a fixed weight. In theory this could be partially remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly</context>
<context position="16731" citStr="Foster, 2000" startWordPosition="2942" endWordPosition="2944">ing much less complex search procedures; of allowing any information which is directly observable in the training corpus to be very easily incorporated into the model via boolean features; and of an estimation procedure where translation model parameters can be optimized for use with an existing language mode1.7 Disadvantages include the high cost of training MEMD models, the fact that p(w I h, s) is somewhat less general than P(slt) for building realistic translation models; and the lack of a mechanism equivalent to the EM algorithm for incorporating &amp;quot;hidden&amp;quot; variables into MEMD models (see (Foster, 2000) for a discussion of this problem). 6 Conclusion The problem of searching for the best target text in statistical translation applications can be greatly simplified if the fundamental distribution p(t Is) is expanded directly in terms of the distribution p(w I h, s), rather than using the standard noisy-channel approach. I compared a simple linear model for p (w I h, s) based on IBM&apos;s model 1 with an equivalent MEMD model, and found that the MEMD model has over 45% lower test corpus perplexity, despite using two orders of magnitude fewer parameters. I also compared several methods for selectin</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. Incorporating position information into a Maximum Entropy / Minimum Divergence translation model. In Proceedings of the 4th Computational Natural Language Learning Workshop (CoNLL), Lisbon, Portugal, September. ACL SigNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
<author>Hermann Ney</author>
</authors>
<title>An iterative, DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In ICSLP-98 (ICS,</booktitle>
<pages>1135--1138</pages>
<contexts>
<context position="3881" citStr="Garcia-Varea et al., 1998" startWordPosition="665" endWordPosition="668">lly remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its perfo</context>
</contexts>
<marker>Garcia-Varea, Casacuberta, Ney, 1998</marker>
<rawString>Ismael Garcia-Varea, Francisco Casacuberta, and Hermann Ney. 1998. An iterative, DP-based search algorithm for statistical machine translation. In ICSLP-98 (ICS, 1998), pages 1135-1138.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="4643" citStr="(1998)" startWordPosition="800" endWordPosition="800">ed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
</contexts>
<marker>1998</marker>
<rawString>1998. Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998, Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Pattern Recognition in Practice. NorthHolland,</booktitle>
<editor>In E. S. Gelsema and L. N. Kanal, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="4992" citStr="Jelinek and Mercer, 1980" startWordPosition="856" endWordPosition="860">a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generative process is as follows: 1) pick a token s at random in s, independent of the positions of w and s; 2) choose w according to a word-for-word translation probability p(w 18). Summing over all choices for s gives the complete model: p(wis) = Ep(wisi)/asi +1) j=0 where si is the jth token in s for j &gt; 0, and so is a special null token prepended to ea</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In E. S. Gelsema and L. N. Kanal, editors, Pattern Recognition in Practice. NorthHolland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Langlais</author>
<author>G Foster</author>
</authors>
<title>Using contextdependent interpolation to combine statistical language and translation models for interactive MT.</title>
<date>2000</date>
<booktitle>In Content-Based Multimedia Information Access (RIAO),</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="3408" citStr="Langlais and Foster, 2000" startWordPosition="581" endWordPosition="584">a prior distribution over lengths. language and translation components, of the form: p(w1h, s) = Ap(w1h) + (1— A)p(w 1s)• (2) where A E [0,1] is a combining weight. However, this is a weak model because it averages over the relative strengths of its components; when p(wih) is likely to be a more accurate estimate than P(wls), it is obvious that the model should rely more heavily on p(w Ih), and vice versa, rather than using a fixed weight. In theory this could be partially remedied by making A depend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly</context>
</contexts>
<marker>Langlais, Foster, 2000</marker>
<rawString>Ph. Langlais and G. Foster. 2000. Using contextdependent interpolation to combine statistical language and translation models for interactive MT. In Content-Based Multimedia Information Access (RIAO), Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Sebastien Saul George Foster</author>
<author>Elliott Macklovitch</author>
<author>Guy Lapalme</author>
</authors>
<title>A comparison of theoretical and useroriented evaluation procedures of a new type of interactive MT.</title>
<date>2000</date>
<booktitle>In Second International Conference On Language Resources and Evaluation (LREC),</booktitle>
<pages>641--648</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="2165" citStr="Langlais et al., 2000" startWordPosition="355" endWordPosition="358">ere w is a target word at a particular position in t, and h denotes the tokens which precede it in t. The main motivation for this approach is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text according to the model. In particular, if h is known, the problem of finding the best word at the current position requires only a straightforward search through the target vocabulary, and simple and efficient dynamicprogramming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The main drawback to modeling p(t Is) in terms of p(w I h, s) is that the latter distribution is conditioned on two very disparate sources of information which are difficult to combine in a complementary way. One simple strategy is to use a linear combination of This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using</context>
</contexts>
<marker>Langlais, Foster, Macklovitch, Lapalme, 2000</marker>
<rawString>Philippe Langlais, Sebastien Saul* George Foster, Elliott Macklovitch, and Guy Lapalme. 2000. A comparison of theoretical and useroriented evaluation procedures of a new type of interactive MT. In Second International Conference On Language Resources and Evaluation (LREC), pages 641-648, Athens, Greece, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>A DP based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings</booktitle>
<contexts>
<context position="3903" citStr="Niessen et al., 1998" startWordPosition="669" endWordPosition="672">pend on h and s, but in practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an e</context>
</contexts>
<marker>Niessen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998. A DP based search algorithm for statistical machine translation. In Proceedings</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING)</booktitle>
<pages>960--967</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="4643" citStr="(1998)" startWordPosition="800" endWordPosition="800">ed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
</contexts>
<marker>1998</marker>
<rawString>. of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 960-967, Montréal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>College Park, Maryland.</location>
<contexts>
<context position="3921" citStr="Och et al., 1999" startWordPosition="673" endWordPosition="676">n practice significant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear m</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP), College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Printz</author>
</authors>
<title>Fast computation of Maximum Entropy/Minimum Divergence feature gain.</title>
<date>1998</date>
<booktitle>In ICSLP-98 (ICS,</booktitle>
<pages>2083--2086</pages>
<contexts>
<context position="4643" citStr="Printz (1998)" startWordPosition="799" endWordPosition="800">s imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate several different methods for MEMD feature selection, including a new algorithm due to Printz (1998). To my knowledge, this is the first application of MEMD to building a large-scale translation model, and one of the few direct comparisons between a MEMD model and an almost exactly equivalent linear mode1.2 2 Models 2.1 Linear Model The baseline model is a linear combination as in (2) of a standard interpolated trigram (Jelinek and Mercer, 1980) for p(will) and the IBM model 1 (IBM1) (Brown et al., 1993) for P(wls). As originally formulated, IBM1 models the distribution p(t Is), but since target text tokens are predicted independently, it can also be used for P(wls). The underlying generativ</context>
<context position="11924" citStr="Printz, 1998" startWordPosition="2102" endWordPosition="2103">ives the highest gain with respect to those already added. Berger et al (1996) describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. However, this method requires many expensive passes over the corpus to optimize the weights for the set of features under consideration at each step, and it adds only one feature per step, so it is not practical for constructing models containing thousands of features or more. In a recent paper (Printz, 1998), Printz argues that it is usually sufficient to perform the iteration described in the previous paragraph only once, in other words that features can be ranked simply according to their gain with respect to some initial model. He also gives an algorithm for computing gains using a numerical approximation which requires only a single pass over the training corpus. I adopted Printz&apos; method for computing MEMD gains, using the reference trigram as the initial model. 3.3 IBM1 Gains The final scoring method involved the gain of each word-pair parameter p(tis) within IBM1. Instead of taking gains wi</context>
</contexts>
<marker>Printz, 1998</marker>
<rawString>Harry Printz. 1998. Fast computation of Maximum Entropy/Minimum Divergence feature gain. In ICSLP-98 (ICS, 1998), pages 2083-2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="6221" citStr="Rosenfeld (1996)" startWordPosition="1095" endWordPosition="1096">tence to account for target words which have no direct translations. The wordpair parameters P(wls) can be estimated from a bilingual corpus of aligned sentence pairs using the EM algorithm, as described in (Brown et al., 1993). 2.2 MEMD Model A MEMD model for p(wlh , s) has the general form: q(wlh, s) exp(5 f (w , h, s)) p(wlh,$) = Z(h, s) where q(w I h, s) is a reference distribution, f (w ,h, s) maps (w, h, s) into an ndimensional feature vector, is a corresponding vector of feature weights (the parameters of the model), and Z(h, s) = Ew q(w I h, s) exp(. f(w, h)) is a normalizing factor. 2Rosenfeld (1996) reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown (Berger et al., 1996) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their t</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George F Foster</author>
<author>Pierre Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Montréal, Québec.</location>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, George F. Foster, and Pierre Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Montréal, Québec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Fast decoding for statistical machine translation.</title>
<date>1998</date>
<booktitle>In ICSLP98 (ICS,</booktitle>
<pages>2775--2778</pages>
<contexts>
<context position="3945" citStr="Wang and Waibel, 1998" startWordPosition="677" endWordPosition="680">cant improvements with this technique have proven elusive (Langlais and Foster, 2000). The noisy channel model avoids this problem by making predictions based on h the responsibility of the language model p(t), and those based on s the responsibility of the translation model p(slt), and combining the two in an optimum way. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998) deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling (Berger et al., 1996). One of the main strengths of MEMD is that it allows information from different sources to be combined in a principled and effective way, so it is a natural choice for modeling p(wlh, s) In this paper, I describe a MEMD model for p(wlh, s) and compare its performance to that of an equivalent linear model. I also evaluate se</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-yi Wang and Alex Waibel. 1998. Fast decoding for statistical machine translation. In ICSLP98 (ICS, 1998), pages 2775-2778.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>