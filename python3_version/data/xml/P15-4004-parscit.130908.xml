<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026673">
<title confidence="0.920069">
KeLP: a Kernel-based Learning Platform for Natural Language
Processing
</title>
<author confidence="0.810552">
Simone Filice(†), Giuseppe Castellucci(‡), Danilo Croce(*), Roberto Basili(*)
</author>
<affiliation confidence="0.562805">
(†) Dept. of Civil Engineering and Computer Science Engineering
(‡) Dept. of Electronic Engineering
(*) Dept. of Enterprise Engineering
University of Roma, Tor Vergata, Italy
</affiliation>
<email confidence="0.992479">
{filice,croce,basili}@info.uniroma2.it; castellucci@ing.uniroma2.it
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840380952381">
Kernel-based learning algorithms have
been shown to achieve state-of-the-art re-
sults in many Natural Language Process-
ing (NLP) tasks. We present KELP, a Java
framework that supports the implementa-
tion of both kernel-based learning algo-
rithms and kernel functions over generic
data representation, e.g. vectorial data or
discrete structures. The framework has
been designed to decouple kernel func-
tions and learning algorithms: once a new
kernel function has been implemented it
can be adopted in all the available kernel-
machine algorithms. The platform in-
cludes different Online and Batch Learn-
ing algorithms for Classification, Regres-
sion and Clustering, as well as several Ker-
nel functions, ranging from vector-based
to structural kernels. This paper will show
the main aspects of the framework by ap-
plying it to different NLP tasks.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999006511627907">
Most of the existing Machine Learning (ML) plat-
forms assume that instances are represented as
vectors in a feature space, e.g. (Joachims, 1999;
Hall et al., 2009; Chang and Lin, 2011), that must
be defined beforehand. In Natural Language Pro-
cessing (NLP) the definition of a feature space of-
ten requires a complex feature engineering phase.
Let us consider any NLP task in which syntactic
information is crucial, e.g. Boundary Detection in
Semantic Role Labeling (Carreras and M`arquez,
2005). Understanding which syntactic patterns
should be captured is non-trivial and usually the
resulting feature vector model is a poor approxi-
mation. Instead, a more natural approach is oper-
ating directly with the parse tree of sentences. Ker-
nel methods (Shawe-Taylor and Cristianini, 2004)
provide an efficient and effective solution, allow-
ing to represent data at a more abstract level, while
their computation still looks at the informative
properties of them. For instance, Tree Kernels
(Collins and Duffy, 2001) take in input two syntac-
tic parse trees, and compute a similarity measure
by looking at the shared sub-structures.
In this paper, KELP, a Java kernel based learn-
ing platform is presented. It supports the imple-
mentation of Kernel-based learning algorithms, as
well as kernel functions over generic data repre-
sentations, e.g. vectorial data or discrete struc-
tures, such as trees and sequences. The framework
has been designed to decouple data structures, ker-
nel functions and learning algorithms in order to
maximize the re-use of existing functionalities: as
an example, a new kernel can be included inherit-
ing existing algorithms and vice versa. KELP sup-
ports XML and JSON serialization of kernel func-
tions and algorithms, enabling the agile definition
of kernel-based learning systems without writing
additional lines of code. KELP can effectively
tackle a wide variety of learning problems. In par-
ticular, in this paper we will show how vectorial
and structured data can be exploited by KELP in
three NLP tasks: Twitter Sentiment Analysis, Text
Categorization and Question Classification.
</bodyText>
<sectionHeader confidence="0.959501" genericHeader="method">
2 Framework Overview
</sectionHeader>
<bodyText confidence="0.80977475">
KELP is a machine learning library completely
written in Java. The Java language has been cho-
sen in order to be compatible with many Java
NLP/IR tools that are developed by the commu-
</bodyText>
<page confidence="0.992166">
19
</page>
<note confidence="0.6950225">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 19–24,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<bodyText confidence="0.999455583333333">
nity, such as Stanford CoreNLP1, OpenNLP2 or
Lucene3. KELP is released as open source soft-
ware under the Apache 2.0 license and the source
code is available on github4. Furthermore it can
be imported via Maven. A detailed documentation
of KELP with helpful examples and use cases is
available on the website of the Semantic Analytics
Group5 of the University of Roma, Tor Vergata.
In this Section, a closer look at the implementa-
tion of different kinds of data representations, ker-
nel functions and kernel-based learning algorithms
is provided.
</bodyText>
<subsectionHeader confidence="0.996805">
2.1 Data Representations
</subsectionHeader>
<bodyText confidence="0.998721470588236">
KELP supports both vectorial and structured
data to model learning instances. For ex-
ample, SparseVector can host a Bag-of-
Words model, while DenseVector can rep-
resent data derived from low dimensional em-
beddings. TreeRepresentation can model
a parse tree and SequenceRepresentation
can be adopted to represent sequences of charac-
ters or sequences of words. Moreover, the plat-
form enables the definition of more complex forms
of data such as pairs, which are useful in model-
ing those problems where instances can be natu-
rally represented as pairs of texts, such as question
and answer in Q/A re-ranking (Severyn and Mos-
chitti, 2013), text and hypothesis in textual entail-
ment (Zanzotto et al., 2009) or sentence pairs in
paraphrasing detection (Filice et al., 2015).
</bodyText>
<subsectionHeader confidence="0.995769">
2.2 Kernels
</subsectionHeader>
<bodyText confidence="0.999962071428571">
Many ML algorithms rely on the notion of similar-
ity between examples. Kernel methods (Shawe-
Taylor and Cristianini, 2004) leverage on the
so-called kernel functions, which compute the
similarity between instances in an implicit high-
dimensional feature space without explicitly com-
puting the coordinates of the data in that space.
The kernel operation is often cheaper from a com-
putational perspective and specific kernels have
been defined for sequences, graphs, trees, texts,
images, as well as vectors.
Kernels can be combined and composed to
create richer similarity metrics, where infor-
mation from different Representations can
</bodyText>
<footnote confidence="0.9999466">
1http://nlp.stanford.edu/software/corenlp.shtml
2https://opennlp.apache.org/
3http://lucene.apache.org/
4https://github.com/SAG-KeLP
5http://sag.art.uniroma2.it/demo-software/kelp/
</footnote>
<bodyText confidence="0.999605222222222">
be exploited at the same time. This flexibil-
ity is completely supported by KELP, which is
also easy to extend with new kernels. Among
the currently available implementations of ker-
nels, there are various standard kernels, such
as LinearKernel, PolynomialKernel or
RbfKernel. A large set of kernels specifically
designed for NLP applications will be described
in the following section.
</bodyText>
<subsectionHeader confidence="0.535492">
2.2.1 Kernels for NLP
</subsectionHeader>
<bodyText confidence="0.999859073170732">
Many tasks in NLP cannot be properly tackled
considering only a Bag-of-Words approach and re-
quire the exploration of deep syntactic aspects. In
question classification the syntactic information is
crucial has largely demonstrated in (Croce et al.,
2011). In Textual Entailment Recognition or in
Paraphrase Detection a pure lexical similarity be-
tween text and hypothesis cannot capture any dif-
ference between Federer won against Nadal and
Nadal won against Federer. A manual definition
of an artificial feature set accounting for syntax
is a very expensive operation that requires a deep
knowledge of the linguistic phenomena character-
izing a specific task. Moreover, every task has
specific patterns that must be considered, making
a manual feature engineering an extremely com-
plex and not portable operation. How can linguis-
tic patterns characterizing a question be automat-
ically discovered? How can linguistic rewriting
rules in paraphrasing be learnt? How can seman-
tic and syntactic relations in textual entailment be
automatically captured? An elegant and efficient
approach to solve NLP problems involving the us-
age of syntax is provided by tree kernels (Collins
and Duffy, 2001). Instead of trying to design a
synthetic feature space, tree kernels directly oper-
ate on the parse tree of sentences evaluating the
tree fragments shared by two trees. This operation
implicitly corresponds to a dot product in the fea-
ture space of all possible tree fragments. The di-
mensionality of such space is extremely large and
operating directly on it is not viable.
Many tree kernels are implemented in KELP,
and they differ by the type of tree fragment
considered in the evaluation of the matching
structures. In the SubTreeKernel (Collins
and Duffy, 2001) valid fragments are subtrees
(ST), i.e. any node of a tree along with
all its descendants. A subset tree (SST) ex-
ploited by the SubSetTreeKernel is a more
general structure since its leaves can be non-
</bodyText>
<page confidence="0.777323">
20
</page>
<figure confidence="0.9848772">
S
VP
PP
NP
NN
</figure>
<page confidence="0.99648">
21
</page>
<bodyText confidence="0.999956333333333">
code, i.e. the algorithm description can be pro-
vided in JSON to an interpreter that will instantiate
it. Listing 1 reports a JSON example of a kernel-
based Support Vector Machine operating in a one-
vs-all schema, where a kernel linear combination
between a normalized Partial Tree Kernel and a
linear kernel is adopted. As the listing shows ker-
nels and algorithms can be easily composed and
combined in order to create new training models.
</bodyText>
<sectionHeader confidence="0.760466" genericHeader="method">
3 Case Studies in NLP
</sectionHeader>
<bodyText confidence="0.9996202">
In this Section, the functionalities and use of the
learning platform are shown. We apply KELP to
very different NLP tasks, i.e. Sentiment Analysis
in Twitter, Text Categorization and Question Clas-
sification, providing examples of kernel-based and
linear learning algorithms. Further examples are
available on the KELP website7 where it is shown
how to instantiate each algorithm or kernel via
JSON and how to add new algorithms, represen-
tations and kernels.
</bodyText>
<subsectionHeader confidence="0.999884">
3.1 Sentiment Analysis in Twitter
</subsectionHeader>
<bodyText confidence="0.999990272727273">
The task of Sentiment Analysis in Twitter has been
proposed in 2013 during the SemEval competi-
tion (Nakov et al., 2013). We built a classifier
for the subtask B, i.e. the classification of a tweet
with respect to the positive, negative and neutral
classes. The contribution of different kernel func-
tions is evaluated using the Support Vector Ma-
chine learning algorithm. As shown in Table 1, we
apply linear (Lin), polynomial (Poly) and Gaus-
sian (Rbf) kernels on two different data represen-
tations: a Bag-Of-Words model of tweets (BoW)
and a distributional representation (WS). The
last is obtained by linearly combining the distri-
butional vectors corresponding to the words of a
message; these vectors are obtained by applying a
Skip-gram model (Mikolov et al., 2013) with the
word2vec tool8 over 20 million of tweets. The lin-
ear combination of the proposed kernel functions
is also applied, e.g. PolyBow+RbfWS. The mean
F1-measure of the positive and negative classes
(pn)9 as well as of all the classes (pnn) is shown
in Table 1.
</bodyText>
<subsectionHeader confidence="0.995367">
3.2 Text Categorization
</subsectionHeader>
<bodyText confidence="0.9972085">
In order to show the scalability of the platform,
a second evaluation considers linear algorithms.
</bodyText>
<footnote confidence="0.996517">
7http://sag.art.uniroma2.it/demo-software/kelp/
8https://code.google.com/p/word2vec/
9pn was the official metric of the SemEval competition.
</footnote>
<table confidence="0.999755857142857">
Kernel MeanF1(pn) MeanF1(pnn)
LinBoW 59.72 63.53
PolyBoW 54.58 59.27
LinWS 60.79 63.94
RbfWS 61.68 65.05
LinBoW+LinWS 66.12 68.56
PolyBoW+RbfWS 64.92 68.10
</table>
<tableCaption confidence="0.999939">
Table 1: Results of Sentiment Analysis
</tableCaption>
<bodyText confidence="0.999799545454545">
We selected the Text Categorization task on the
RCV1 dataset (Lewis et al., 2004) with the setting
that can be found on the LibLinear website10. In
this version of the dataset, CCAT and ECAT are
collapsed into a positive class, while GCAT and
MCAT are the negative class, resulting in a dataset
composed by 20, 242 examples. As shown in Ta-
ble 2, we applied the LibLinear, Pegasos and Lin-
ear Passive-Aggressive implementations, comput-
ing the accuracy and the standard deviation with
respect to a 5-fold cross validation strategy.
</bodyText>
<table confidence="0.995095">
Task Accuracy Std
LibLinear 96.74% 0.0029
Pegasos 95.31% 0.0033
Passive Aggressive 96.60% 0.0024
</table>
<tableCaption confidence="0.985733">
Table 2: Text Categorization Accuracy
</tableCaption>
<subsectionHeader confidence="0.996055">
3.3 Question Classification
</subsectionHeader>
<bodyText confidence="0.999929409090909">
The third case study explores the application of
Tree Kernels to Question Classification (QC), an
inference task required in many Question Answer-
ing processes. In this problem, questions writ-
ten in natural language are assigned to different
classes. A QC system should select the correct
class given an instance question. In this setting,
Tree Kernels allow to directly model the examples
in terms of their parse trees. The reference cor-
pus is the UIUC dataset (Li and Roth, 2002), in-
cluding 5,452 questions for training and 500 ques-
tions for test11, organized in six coarse-grained
classes, such as HUMAN or LOCATION. Again,
Kernel-based SVM has been evaluated adopting
the same setup of (Croce et al., 2011). A pure lex-
ical model based on a linear kernel over a Bag-of-
Words (BoW) is considered a baseline. The con-
tribution of the syntactic information is demon-
strated by the results achieved by the Partial Tree
Kernel (PTK), the Smoothed Partial Tree Kernels
(SPTK) and the Compositionally Smoothed Par-
tial Tree Kernel (CSPTK), as shown in Table 3.
</bodyText>
<footnote confidence="0.993321333333333">
10http://www.csie.ntu.edu.tw/∼cjlin/
libsvmtools/datasets/
11http://cogcomp.cs.illinois.edu/Data/QA/QC/
</footnote>
<page confidence="0.996985">
22
</page>
<table confidence="0.999331833333333">
Kernel Accuracy
BoW 87.2%
PolyBoW 88.8%
PTK 91.6%
SPTK 94.6%
CSPTK 95.0%
</table>
<tableCaption confidence="0.997774">
Table 3: Question Classification Accuracy.
</tableCaption>
<sectionHeader confidence="0.999107" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.997347714285715">
Many software tools for computational linguis-
tic research already exist. Tools like Stan-
ford CoreNLP or OpenNLP provide a complete
pipeline for performing linguistic tasks such as
stemming, lemmatization, Part-of-Speech tagging
or parsing. They are complementary to KELP:
they can be used in the feature extraction phase,
while KELP will care about the machine learning
part. Regarding other machine learning platforms
there are plenty of available possibilities, but for
different reasons no one can provide something
close to what the proposed library offers.
Weka (Hall et al., 2009) is a collection of ma-
chine learning algorithms for data mining tasks.
The algorithms can either be applied directly to
a dataset or called from Java. It contains vari-
ous tools for different data mining activities: data
pre-processing, classification, regression, cluster-
ing and visualization.
Mallet (McCallum, 2002) is more oriented to
NLP applications. It is entirely in Java and in-
cludes feature extraction tools for converting text
into vectors and statistical analysis tools for docu-
ment classification, clustering, topic modeling, in-
formation extraction, and other machine learning
applications to text. Regarding the kernel-based
learning both Weka and Mallet leverage on Lib-
SVM, and obviously inherit its limits.
LibSVM (Chang and Lin, 2011) is a machine
learning platform focusing on Support Vector Ma-
chines. It is written in C++ language and it
includes different SVM formulations: C-svm,
Nu-svm and OneClass-svm, as well as a one-
vs-one multi classification schema. It implements
also regression support vector solvers. It has been
ported in different languages, including Java. The
batch learning part of KELP is strongly inspired
by LibSVM formulations and implementations.
LibSVM is mainly intended for plain users and
does not provide any support for extendibility. It
can operate only on sparse feature vectors via stan-
dard kernel functions. No structured representa-
tions are considered.
Another very popular Support Vector Machines
(SVM) package is SvmLight (Joachims, 1999). It
is entirely written in C language and its main fea-
ture is speed. It solves classification and regres-
sion problems, as well as ranking problems. Its
efficiency is paid in terms of extensibility: C lan-
guage does not allow a fast prototyping of new ma-
chine learning kernels or algorithms. Many times
in research contexts fast prototyping is more im-
portant than performances: the proposed platform
has been developed with extensibility in mind.
The most similar platform to ours is JKernel-
Machines (Picard et al., 2013). It is a Java based
package focused on Kernel machines. Just like the
proposed library, JKernelMachines is primary de-
signed to deal with custom kernels that cannot be
easily found in standard libraries. Standard SVM
optimization algorithms are implemented, but also
more sophisticated learning-based kernel combi-
nation methods such as Multiple Kernel Learn-
ing (MKL). However, many features covered by
KELP are not offered by JKernelMachines, just
like tree kernels, regression and clustering. More-
over, different architectural choices have been ap-
plied in KELP in order to support an easier com-
position and combination of representations, ker-
nels as well as learning algorithms.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991772727273">
This paper presented KELP, a Java framework
to support the application of Kernel-based learn-
ing methods with a particular attention to Lan-
guage Learning tasks. The library implements a
large variety of kernel functions used in NLP (such
as Tree Kernels or Sequence Kernels) as well as
many learning algorithms useful in classification,
regression, novelty detection or clustering prob-
lems. KELP can be imported via Maven but its
usage is not restricted to a Java-compliant environ-
ment as it allows to build complex kernel machine
based systems, leveraging on JSON/XML inter-
faces to instantiate classifiers. The entire frame-
work has been designed to support researchers in
the development of new kernel functions or algo-
rithms, providing a principled decoupling of the
data structures in order to maximize the re-use of
existing functionalities. The benefits of the pro-
posed environment have been shown in three NLP
tasks, where results in line with the state-of-the-art
have been reached with the simple application of
various kernel functions available in KELP.
</bodyText>
<page confidence="0.997462">
23
</page>
<sectionHeader confidence="0.989952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973503">
Paolo Annesi, Danilo Croce, and Roberto Basili. 2014.
Semantic compositionality in tree kernels. In Proc.
of CIKM 2014, pages 1029–1038, New York, NY,
USA. ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS.
Xavier Carreras and Lluis M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learn-
ing, CONLL ’05, pages 152–164, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nicol`o Cesa-Bianchi and Claudio Gentile. 2006.
Tracking the best hyperplane with a simple budget
perceptron. In In Proc. of the 19th Annual Con-
ference on Computational Learning Theory, pages
483–498. Springer-Verlag.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551–585,
December.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In EMNLP,
Edinburgh.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2015. Structural representations for
learning relations between pairs of texts. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics, Beijing, China,
July. Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
sigkdd explor., 11(1).
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning, pages 169–184. MIT Press.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. J. Mach. Learn. Res.,
5:361–397, December.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL ’02, COLING ’02, pages 1–
7, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, Berlin, Germany, September.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in twitter. In Proceedings of SemEval 2013, pages
312–320, Atlanta, USA. ACL.
David Picard, Nicolas Thome, and Matthieu Cord.
2013. Jkernelmachines: A simple framework for
kernel machines. Journal of Machine Learning Re-
search, 14:1417–1421.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense
of one-vs-all classification. J. Mach. Learn. Res.,
5:101–141, December.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on EMNLP, pages 458–467, Seattle, USA. ACL.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007.
Pegasos: Primal estimated sub–gradient solver for
SVM. In Proc. of ICML.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Libin Shen and Aravind K. Joshi. 2003. An svm based
voting algorithm with application to parse reranking.
In In Proc. of CoNLL 2003, pages 9–16.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Fabio massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learn-
ing approach to textual entailment recognition. Nat.
Lang. Eng., 15(4):551–582, October.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842122">
<title confidence="0.996422">KeLP: a Kernel-based Learning Platform for Natural Language Processing</title>
<author confidence="0.99738">Giuseppe Danilo Roberto</author>
<affiliation confidence="0.9769995">Dept. of Civil Engineering and Computer Science Dept. of Electronic (*) Dept. of Enterprise University of Roma, Tor Vergata, Italy</affiliation>
<email confidence="0.986965">castellucci@ing.uniroma2.it</email>
<abstract confidence="0.997625">Kernel-based learning algorithms have been shown to achieve state-of-the-art results in many Natural Language Process- (NLP) tasks. We present a Java framework that supports the implementation of both kernel-based learning algorithms and kernel functions over generic data representation, e.g. vectorial data or discrete structures. The framework has been designed to decouple kernel functions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classification, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paolo Annesi</author>
<author>Danilo Croce</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic compositionality in tree kernels.</title>
<date>2014</date>
<booktitle>In Proc. of CIKM 2014,</booktitle>
<pages>1029--1038</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Annesi, Croce, Basili, 2014</marker>
<rawString>Paolo Annesi, Danilo Croce, and Roberto Basili. 2014. Semantic compositionality in tree kernels. In Proc. of CIKM 2014, pages 1029–1038, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. Subsequence kernels for relation extraction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05,</booktitle>
<pages>152--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05, pages 152–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Claudio Gentile</author>
</authors>
<title>Tracking the best hyperplane with a simple budget perceptron. In</title>
<date>2006</date>
<booktitle>In Proc. of the 19th Annual Conference on Computational Learning Theory,</booktitle>
<pages>483--498</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Cesa-Bianchi, Gentile, 2006</marker>
<rawString>Nicol`o Cesa-Bianchi and Claudio Gentile. 2006. Tracking the best hyperplane with a simple budget perceptron. In In Proc. of the 19th Annual Conference on Computational Learning Theory, pages 483–498. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="1435" citStr="Chang and Lin, 2011" startWordPosition="203" endWordPosition="206"> algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classification, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks. 1 Introduction Most of the existing Machine Learning (ML) platforms assume that instances are represented as vectors in a feature space, e.g. (Joachims, 1999; Hall et al., 2009; Chang and Lin, 2011), that must be defined beforehand. In Natural Language Processing (NLP) the definition of a feature space often requires a complex feature engineering phase. Let us consider any NLP task in which syntactic information is crucial, e.g. Boundary Detection in Semantic Role Labeling (Carreras and M`arquez, 2005). Understanding which syntactic patterns should be captured is non-trivial and usually the resulting feature vector model is a poor approximation. Instead, a more natural approach is operating directly with the parse tree of sentences. Kernel methods (Shawe-Taylor and Cristianini, 2004) pro</context>
<context position="13974" citStr="Chang and Lin, 2011" startWordPosition="2144" endWordPosition="2147">or called from Java. It contains various tools for different data mining activities: data pre-processing, classification, regression, clustering and visualization. Mallet (McCallum, 2002) is more oriented to NLP applications. It is entirely in Java and includes feature extraction tools for converting text into vectors and statistical analysis tools for document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on LibSVM, and obviously inherit its limits. LibSVM (Chang and Lin, 2011) is a machine learning platform focusing on Support Vector Machines. It is written in C++ language and it includes different SVM formulations: C-svm, Nu-svm and OneClass-svm, as well as a onevs-one multi classification schema. It implements also regression support vector solvers. It has been ported in different languages, including Java. The batch learning part of KELP is strongly inspired by LibSVM formulations and implementations. LibSVM is mainly intended for plain users and does not provide any support for extendibility. It can operate only on sparse feature vectors via standard kernel fun</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language. In NIPS.</title>
<date>2001</date>
<contexts>
<context position="2257" citStr="Collins and Duffy, 2001" startWordPosition="329" endWordPosition="332">ntactic information is crucial, e.g. Boundary Detection in Semantic Role Labeling (Carreras and M`arquez, 2005). Understanding which syntactic patterns should be captured is non-trivial and usually the resulting feature vector model is a poor approximation. Instead, a more natural approach is operating directly with the parse tree of sentences. Kernel methods (Shawe-Taylor and Cristianini, 2004) provide an efficient and effective solution, allowing to represent data at a more abstract level, while their computation still looks at the informative properties of them. For instance, Tree Kernels (Collins and Duffy, 2001) take in input two syntactic parse trees, and compute a similarity measure by looking at the shared sub-structures. In this paper, KELP, a Java kernel based learning platform is presented. It supports the implementation of Kernel-based learning algorithms, as well as kernel functions over generic data representations, e.g. vectorial data or discrete structures, such as trees and sequences. The framework has been designed to decouple data structures, kernel functions and learning algorithms in order to maximize the re-use of existing functionalities: as an example, a new kernel can be included </context>
<context position="7427" citStr="Collins and Duffy, 2001" startWordPosition="1115" endWordPosition="1118">ation that requires a deep knowledge of the linguistic phenomena characterizing a specific task. Moreover, every task has specific patterns that must be considered, making a manual feature engineering an extremely complex and not portable operation. How can linguistic patterns characterizing a question be automatically discovered? How can linguistic rewriting rules in paraphrasing be learnt? How can semantic and syntactic relations in textual entailment be automatically captured? An elegant and efficient approach to solve NLP problems involving the usage of syntax is provided by tree kernels (Collins and Duffy, 2001). Instead of trying to design a synthetic feature space, tree kernels directly operate on the parse tree of sentences evaluating the tree fragments shared by two trees. This operation implicitly corresponds to a dot product in the feature space of all possible tree fragments. The dimensionality of such space is extremely large and operating directly on it is not viable. Many tree kernels are implemented in KELP, and they differ by the type of tree fragment considered in the evaluation of the matching structures. In the SubTreeKernel (Collins and Duffy, 2001) valid fragments are subtrees (ST), </context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="6496" citStr="Croce et al., 2011" startWordPosition="972" endWordPosition="975">bility is completely supported by KELP, which is also easy to extend with new kernels. Among the currently available implementations of kernels, there are various standard kernels, such as LinearKernel, PolynomialKernel or RbfKernel. A large set of kernels specifically designed for NLP applications will be described in the following section. 2.2.1 Kernels for NLP Many tasks in NLP cannot be properly tackled considering only a Bag-of-Words approach and require the exploration of deep syntactic aspects. In question classification the syntactic information is crucial has largely demonstrated in (Croce et al., 2011). In Textual Entailment Recognition or in Paraphrase Detection a pure lexical similarity between text and hypothesis cannot capture any difference between Federer won against Nadal and Nadal won against Federer. A manual definition of an artificial feature set accounting for syntax is a very expensive operation that requires a deep knowledge of the linguistic phenomena characterizing a specific task. Moreover, every task has specific patterns that must be considered, making a manual feature engineering an extremely complex and not portable operation. How can linguistic patterns characterizing </context>
<context position="12057" citStr="Croce et al., 2011" startWordPosition="1861" endWordPosition="1864">QC), an inference task required in many Question Answering processes. In this problem, questions written in natural language are assigned to different classes. A QC system should select the correct class given an instance question. In this setting, Tree Kernels allow to directly model the examples in terms of their parse trees. The reference corpus is the UIUC dataset (Li and Roth, 2002), including 5,452 questions for training and 500 questions for test11, organized in six coarse-grained classes, such as HUMAN or LOCATION. Again, Kernel-based SVM has been evaluated adopting the same setup of (Croce et al., 2011). A pure lexical model based on a linear kernel over a Bag-ofWords (BoW) is considered a baseline. The contribution of the syntactic information is demonstrated by the results achieved by the Partial Tree Kernel (PTK), the Smoothed Partial Tree Kernels (SPTK) and the Compositionally Smoothed Partial Tree Kernel (CSPTK), as shown in Table 3. 10http://www.csie.ntu.edu.tw/∼cjlin/ libsvmtools/datasets/ 11http://cogcomp.cs.illinois.edu/Data/QA/QC/ 22 Kernel Accuracy BoW 87.2% PolyBoW 88.8% PTK 91.6% SPTK 94.6% CSPTK 95.0% Table 3: Question Classification Accuracy. 4 Related Work Many software tools</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In EMNLP, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Filice</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural representations for learning relations between pairs of texts.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="5010" citStr="Filice et al., 2015" startWordPosition="769" endWordPosition="772">seVector can represent data derived from low dimensional embeddings. TreeRepresentation can model a parse tree and SequenceRepresentation can be adopted to represent sequences of characters or sequences of words. Moreover, the platform enables the definition of more complex forms of data such as pairs, which are useful in modeling those problems where instances can be naturally represented as pairs of texts, such as question and answer in Q/A re-ranking (Severyn and Moschitti, 2013), text and hypothesis in textual entailment (Zanzotto et al., 2009) or sentence pairs in paraphrasing detection (Filice et al., 2015). 2.2 Kernels Many ML algorithms rely on the notion of similarity between examples. Kernel methods (ShaweTaylor and Cristianini, 2004) leverage on the so-called kernel functions, which compute the similarity between instances in an implicit highdimensional feature space without explicitly computing the coordinates of the data in that space. The kernel operation is often cheaper from a computational perspective and specific kernels have been defined for sequences, graphs, trees, texts, images, as well as vectors. Kernels can be combined and composed to create richer similarity metrics, where in</context>
</contexts>
<marker>Filice, Martino, Moschitti, 2015</marker>
<rawString>Simone Filice, Giovanni Da San Martino, and Alessandro Moschitti. 2015. Structural representations for learning relations between pairs of texts. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update. sigkdd explor.,</title>
<date>2009</date>
<pages>11--1</pages>
<contexts>
<context position="1413" citStr="Hall et al., 2009" startWordPosition="199" endWordPosition="202">ctions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classification, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks. 1 Introduction Most of the existing Machine Learning (ML) platforms assume that instances are represented as vectors in a feature space, e.g. (Joachims, 1999; Hall et al., 2009; Chang and Lin, 2011), that must be defined beforehand. In Natural Language Processing (NLP) the definition of a feature space often requires a complex feature engineering phase. Let us consider any NLP task in which syntactic information is crucial, e.g. Boundary Detection in Semantic Role Labeling (Carreras and M`arquez, 2005). Understanding which syntactic patterns should be captured is non-trivial and usually the resulting feature vector model is a poor approximation. Instead, a more natural approach is operating directly with the parse tree of sentences. Kernel methods (Shawe-Taylor and </context>
<context position="13224" citStr="Hall et al., 2009" startWordPosition="2032" endWordPosition="2035">cation Accuracy. 4 Related Work Many software tools for computational linguistic research already exist. Tools like Stanford CoreNLP or OpenNLP provide a complete pipeline for performing linguistic tasks such as stemming, lemmatization, Part-of-Speech tagging or parsing. They are complementary to KELP: they can be used in the feature extraction phase, while KELP will care about the machine learning part. Regarding other machine learning platforms there are plenty of available possibilities, but for different reasons no one can provide something close to what the proposed library offers. Weka (Hall et al., 2009) is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from Java. It contains various tools for different data mining activities: data pre-processing, classification, regression, clustering and visualization. Mallet (McCallum, 2002) is more oriented to NLP applications. It is entirely in Java and includes feature extraction tools for converting text into vectors and statistical analysis tools for document classification, clustering, topic modeling, information extraction, and other machine learning application</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. sigkdd explor., 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1394" citStr="Joachims, 1999" startWordPosition="197" endWordPosition="198">ouple kernel functions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classification, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks. 1 Introduction Most of the existing Machine Learning (ML) platforms assume that instances are represented as vectors in a feature space, e.g. (Joachims, 1999; Hall et al., 2009; Chang and Lin, 2011), that must be defined beforehand. In Natural Language Processing (NLP) the definition of a feature space often requires a complex feature engineering phase. Let us consider any NLP task in which syntactic information is crucial, e.g. Boundary Detection in Semantic Role Labeling (Carreras and M`arquez, 2005). Understanding which syntactic patterns should be captured is non-trivial and usually the resulting feature vector model is a poor approximation. Instead, a more natural approach is operating directly with the parse tree of sentences. Kernel methods</context>
<context position="14715" citStr="Joachims, 1999" startWordPosition="2258" endWordPosition="2259">VM formulations: C-svm, Nu-svm and OneClass-svm, as well as a onevs-one multi classification schema. It implements also regression support vector solvers. It has been ported in different languages, including Java. The batch learning part of KELP is strongly inspired by LibSVM formulations and implementations. LibSVM is mainly intended for plain users and does not provide any support for extendibility. It can operate only on sparse feature vectors via standard kernel functions. No structured representations are considered. Another very popular Support Vector Machines (SVM) package is SvmLight (Joachims, 1999). It is entirely written in C language and its main feature is speed. It solves classification and regression problems, as well as ranking problems. Its efficiency is paid in terms of extensibility: C language does not allow a fast prototyping of new machine learning kernels or algorithms. Many times in research contexts fast prototyping is more important than performances: the proposed platform has been developed with extensibility in mind. The most similar platform to ours is JKernelMachines (Picard et al., 2013). It is a Java based package focused on Kernel machines. Just like the proposed </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>5--361</pages>
<contexts>
<context position="10737" citStr="Lewis et al., 2004" startWordPosition="1649" endWordPosition="1652">egative classes (pn)9 as well as of all the classes (pnn) is shown in Table 1. 3.2 Text Categorization In order to show the scalability of the platform, a second evaluation considers linear algorithms. 7http://sag.art.uniroma2.it/demo-software/kelp/ 8https://code.google.com/p/word2vec/ 9pn was the official metric of the SemEval competition. Kernel MeanF1(pn) MeanF1(pnn) LinBoW 59.72 63.53 PolyBoW 54.58 59.27 LinWS 60.79 63.94 RbfWS 61.68 65.05 LinBoW+LinWS 66.12 68.56 PolyBoW+RbfWS 64.92 68.10 Table 1: Results of Sentiment Analysis We selected the Text Categorization task on the RCV1 dataset (Lewis et al., 2004) with the setting that can be found on the LibLinear website10. In this version of the dataset, CCAT and ECAT are collapsed into a positive class, while GCAT and MCAT are the negative class, resulting in a dataset composed by 20, 242 examples. As shown in Table 2, we applied the LibLinear, Pegasos and Linear Passive-Aggressive implementations, computing the accuracy and the standard deviation with respect to a 5-fold cross validation strategy. Task Accuracy Std LibLinear 96.74% 0.0029 Pegasos 95.31% 0.0033 Passive Aggressive 96.60% 0.0024 Table 2: Text Categorization Accuracy 3.3 Question Clas</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361–397, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL ’02, COLING ’02,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11828" citStr="Li and Roth, 2002" startWordPosition="1824" endWordPosition="1827">ar 96.74% 0.0029 Pegasos 95.31% 0.0033 Passive Aggressive 96.60% 0.0024 Table 2: Text Categorization Accuracy 3.3 Question Classification The third case study explores the application of Tree Kernels to Question Classification (QC), an inference task required in many Question Answering processes. In this problem, questions written in natural language are assigned to different classes. A QC system should select the correct class given an instance question. In this setting, Tree Kernels allow to directly model the examples in terms of their parse trees. The reference corpus is the UIUC dataset (Li and Roth, 2002), including 5,452 questions for training and 500 questions for test11, organized in six coarse-grained classes, such as HUMAN or LOCATION. Again, Kernel-based SVM has been evaluated adopting the same setup of (Croce et al., 2011). A pure lexical model based on a linear kernel over a Bag-ofWords (BoW) is considered a baseline. The contribution of the syntactic information is demonstrated by the results achieved by the Partial Tree Kernel (PTK), the Smoothed Partial Tree Kernels (SPTK) and the Compositionally Smoothed Partial Tree Kernel (CSPTK), as shown in Table 3. 10http://www.csie.ntu.edu.tw</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In Proceedings of ACL ’02, COLING ’02, pages 1– 7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="13541" citStr="McCallum, 2002" startWordPosition="2081" endWordPosition="2082"> in the feature extraction phase, while KELP will care about the machine learning part. Regarding other machine learning platforms there are plenty of available possibilities, but for different reasons no one can provide something close to what the proposed library offers. Weka (Hall et al., 2009) is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from Java. It contains various tools for different data mining activities: data pre-processing, classification, regression, clustering and visualization. Mallet (McCallum, 2002) is more oriented to NLP applications. It is entirely in Java and includes feature extraction tools for converting text into vectors and statistical analysis tools for document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on LibSVM, and obviously inherit its limits. LibSVM (Chang and Lin, 2011) is a machine learning platform focusing on Support Vector Machines. It is written in C++ language and it includes different SVM formulations: C-svm, Nu-svm and OneCla</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="9932" citStr="Mikolov et al., 2013" startWordPosition="1533" endWordPosition="1536"> the subtask B, i.e. the classification of a tweet with respect to the positive, negative and neutral classes. The contribution of different kernel functions is evaluated using the Support Vector Machine learning algorithm. As shown in Table 1, we apply linear (Lin), polynomial (Poly) and Gaussian (Rbf) kernels on two different data representations: a Bag-Of-Words model of tweets (BoW) and a distributional representation (WS). The last is obtained by linearly combining the distributional vectors corresponding to the words of a message; these vectors are obtained by applying a Skip-gram model (Mikolov et al., 2013) with the word2vec tool8 over 20 million of tweets. The linear combination of the proposed kernel functions is also applied, e.g. PolyBow+RbfWS. The mean F1-measure of the positive and negative classes (pn)9 as well as of all the classes (pnn) is shown in Table 1. 3.2 Text Categorization In order to show the scalability of the platform, a second evaluation considers linear algorithms. 7http://sag.art.uniroma2.it/demo-software/kelp/ 8https://code.google.com/p/word2vec/ 9pn was the official metric of the SemEval competition. Kernel MeanF1(pn) MeanF1(pnn) LinBoW 59.72 63.53 PolyBoW 54.58 59.27 Li</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML,</booktitle>
<location>Berlin, Germany,</location>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>312--320</pages>
<publisher>ACL.</publisher>
<location>Atlanta, USA.</location>
<contexts>
<context position="9284" citStr="Nakov et al., 2013" startWordPosition="1429" endWordPosition="1432">in NLP In this Section, the functionalities and use of the learning platform are shown. We apply KELP to very different NLP tasks, i.e. Sentiment Analysis in Twitter, Text Categorization and Question Classification, providing examples of kernel-based and linear learning algorithms. Further examples are available on the KELP website7 where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, representations and kernels. 3.1 Sentiment Analysis in Twitter The task of Sentiment Analysis in Twitter has been proposed in 2013 during the SemEval competition (Nakov et al., 2013). We built a classifier for the subtask B, i.e. the classification of a tweet with respect to the positive, negative and neutral classes. The contribution of different kernel functions is evaluated using the Support Vector Machine learning algorithm. As shown in Table 1, we apply linear (Lin), polynomial (Poly) and Gaussian (Rbf) kernels on two different data representations: a Bag-Of-Words model of tweets (BoW) and a distributional representation (WS). The last is obtained by linearly combining the distributional vectors corresponding to the words of a message; these vectors are obtained by a</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of SemEval 2013, pages 312–320, Atlanta, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Picard</author>
<author>Nicolas Thome</author>
<author>Matthieu Cord</author>
</authors>
<title>Jkernelmachines: A simple framework for kernel machines.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--1417</pages>
<contexts>
<context position="15235" citStr="Picard et al., 2013" startWordPosition="2343" endWordPosition="2346"> considered. Another very popular Support Vector Machines (SVM) package is SvmLight (Joachims, 1999). It is entirely written in C language and its main feature is speed. It solves classification and regression problems, as well as ranking problems. Its efficiency is paid in terms of extensibility: C language does not allow a fast prototyping of new machine learning kernels or algorithms. Many times in research contexts fast prototyping is more important than performances: the proposed platform has been developed with extensibility in mind. The most similar platform to ours is JKernelMachines (Picard et al., 2013). It is a Java based package focused on Kernel machines. Just like the proposed library, JKernelMachines is primary designed to deal with custom kernels that cannot be easily found in standard libraries. Standard SVM optimization algorithms are implemented, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL). However, many features covered by KELP are not offered by JKernelMachines, just like tree kernels, regression and clustering. Moreover, different architectural choices have been applied in KELP in order to support an easier composit</context>
</contexts>
<marker>Picard, Thome, Cord, 2013</marker>
<rawString>David Picard, Nicolas Thome, and Matthieu Cord. 2013. Jkernelmachines: A simple framework for kernel machines. Journal of Machine Learning Research, 14:1417–1421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>5--101</pages>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-vs-all classification. J. Mach. Learn. Res., 5:101–141, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic feature engineering for answer selection and extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on EMNLP,</booktitle>
<pages>458--467</pages>
<publisher>ACL.</publisher>
<location>Seattle, USA.</location>
<contexts>
<context position="4877" citStr="Severyn and Moschitti, 2013" startWordPosition="747" endWordPosition="751">LP supports both vectorial and structured data to model learning instances. For example, SparseVector can host a Bag-ofWords model, while DenseVector can represent data derived from low dimensional embeddings. TreeRepresentation can model a parse tree and SequenceRepresentation can be adopted to represent sequences of characters or sequences of words. Moreover, the platform enables the definition of more complex forms of data such as pairs, which are useful in modeling those problems where instances can be naturally represented as pairs of texts, such as question and answer in Q/A re-ranking (Severyn and Moschitti, 2013), text and hypothesis in textual entailment (Zanzotto et al., 2009) or sentence pairs in paraphrasing detection (Filice et al., 2015). 2.2 Kernels Many ML algorithms rely on the notion of similarity between examples. Kernel methods (ShaweTaylor and Cristianini, 2004) leverage on the so-called kernel functions, which compute the similarity between instances in an implicit highdimensional feature space without explicitly computing the coordinates of the data in that space. The kernel operation is often cheaper from a computational perspective and specific kernels have been defined for sequences,</context>
</contexts>
<marker>Severyn, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic feature engineering for answer selection and extraction. In Proceedings of the 2013 Conference on EMNLP, pages 458–467, Seattle, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub–gradient solver for SVM.</title>
<date>2007</date>
<booktitle>In Proc. of ICML.</booktitle>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: Primal estimated sub–gradient solver for SVM. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2031" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="294" endWordPosition="297">Hall et al., 2009; Chang and Lin, 2011), that must be defined beforehand. In Natural Language Processing (NLP) the definition of a feature space often requires a complex feature engineering phase. Let us consider any NLP task in which syntactic information is crucial, e.g. Boundary Detection in Semantic Role Labeling (Carreras and M`arquez, 2005). Understanding which syntactic patterns should be captured is non-trivial and usually the resulting feature vector model is a poor approximation. Instead, a more natural approach is operating directly with the parse tree of sentences. Kernel methods (Shawe-Taylor and Cristianini, 2004) provide an efficient and effective solution, allowing to represent data at a more abstract level, while their computation still looks at the informative properties of them. For instance, Tree Kernels (Collins and Duffy, 2001) take in input two syntactic parse trees, and compute a similarity measure by looking at the shared sub-structures. In this paper, KELP, a Java kernel based learning platform is presented. It supports the implementation of Kernel-based learning algorithms, as well as kernel functions over generic data representations, e.g. vectorial data or discrete structures, such as tr</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An svm based voting algorithm with application to parse reranking. In</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL</booktitle>
<pages>9--16</pages>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An svm based voting algorithm with application to parse reranking. In In Proc. of CoNLL 2003, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="4944" citStr="Zanzotto et al., 2009" startWordPosition="759" endWordPosition="762">s. For example, SparseVector can host a Bag-ofWords model, while DenseVector can represent data derived from low dimensional embeddings. TreeRepresentation can model a parse tree and SequenceRepresentation can be adopted to represent sequences of characters or sequences of words. Moreover, the platform enables the definition of more complex forms of data such as pairs, which are useful in modeling those problems where instances can be naturally represented as pairs of texts, such as question and answer in Q/A re-ranking (Severyn and Moschitti, 2013), text and hypothesis in textual entailment (Zanzotto et al., 2009) or sentence pairs in paraphrasing detection (Filice et al., 2015). 2.2 Kernels Many ML algorithms rely on the notion of similarity between examples. Kernel methods (ShaweTaylor and Cristianini, 2004) leverage on the so-called kernel functions, which compute the similarity between instances in an implicit highdimensional feature space without explicitly computing the coordinates of the data in that space. The kernel operation is often cheaper from a computational perspective and specific kernels have been defined for sequences, graphs, trees, texts, images, as well as vectors. Kernels can be c</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Nat. Lang. Eng., 15(4):551–582, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>