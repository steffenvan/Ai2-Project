<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.053112">
<title confidence="0.952202">
Scoring Algorithms for Wordspotting Systems
</title>
<note confidence="0.888681333333333">
Robert W. Morris and Jon A. Arrowood and Peter S. Cardillo
Nexidia Inc.
3060 Peachtree Rd Suite 730
</note>
<address confidence="0.542117">
Atlanta, Georgia 30305-2240
</address>
<email confidence="0.932942">
{rmorris,jarrowood,pcardillo}@nexidia.com
</email>
<author confidence="0.824715">
Mark A. Clements
</author>
<affiliation confidence="0.9154805">
Center for Signal &amp; Image Processing
Georgia Institute of Technology
</affiliation>
<address confidence="0.959957">
Atlanta, Georgia 30332-0250
</address>
<email confidence="0.999468">
clements@ece.gatech.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999378545454546">
When evaluating wordspotting systems, one
normally compares receiver operating charac-
teristic curves and different measures of accu-
racy. However, there are many other factors
that are relevant to the system’s usability for
searching speech. In this paper, we discuss
both measures of quality for confidence scores
and propose algorithms for producing scores
that are optimal with respect to these criteria.
algorithms for producing scores that are optimal with re-
spect to these criteria.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="keywords">
2 Assumptions
</sectionHeader>
<bodyText confidence="0.9997645">
In order to derive a scoring algorithm, a key assumption
must be made by the wordspotting algorithm: each match
must have a numeric score associated with it. In addition,
there must be some theoretical basis for an additive de-
composition of this score. This decomposition is given
by
</bodyText>
<equation confidence="0.9602075">
1 Introduction R(q) = XL R(q)
l=1 l , (1)
</equation>
<bodyText confidence="0.9997475625">
In order to evaluate any system, it is useful to have objec-
tive quality measures that can be automatically applied to
systems for comparison. For wordspotting systems, these
measures are oriented towards recall accuracy. Most of
these measures are based on receiver operating character-
istic (ROC) curves and functions of these curves. How-
ever, there are many other factors that are relevant to the
systems usability.
When a user enters a query to the Nexidia wordspot-
ter (Clements et al., 2001), the system returns a sorted re-
sult list that marks the times where the query matches the
audio. In addition, scores are associated with each result.
These scores are related to the likelihood that the tagged
audio matches the query. Although this score gives an
indication of the strength of the match, users have had
difficulty interpreting the scores.
We found that most users want to use the score in one
of two ways. The first application is to provide a score
threshold for monitoring applications. Alternatively, peo-
ple also assume that the score reflects the probability that
the tagged audio segment is actually a match.
However, without any objective quality measure of
these scores, it was difficult to evaluate different score
generation algorithms. In this paper, we discuss both
measures of quality for confidence scores and propose
where R(q) is the score returned by query q, and R(q)
l is
the score associated with the lth phoneme in the query.
With this assumption, we also assume that these compo-
nents can be modeled with a Gaussian distribution with
dependence on whether the match is truly a hit or a miss.
The distributions are then given by
</bodyText>
<equation confidence="0.9930605">
R(q) |Hit — JV 3µH 3S 1W , aH2´ (2)
R(q) |Miss — JV 3µM 3S(q) , a2M´ , (3)
</equation>
<bodyText confidence="0.9264192">
where S(q)
l is the lth phoneme in query q. In this model,
the means, µ are dependent on the phoneme, but the vari-
ance, a2, is not. Using the additive model, the raw scores
are distributed by
</bodyText>
<equation confidence="0.9265255">
R(q)|Miss — JV Ã XµM 3Slq)´ ,Lam!. (5)
l=1
</equation>
<sectionHeader confidence="0.994159" genericHeader="introduction">
3 Performance Measures
</sectionHeader>
<bodyText confidence="0.999974">
We propose two scoring evaluation measures. In each of
these methods, the raw score is modified by some scoring
</bodyText>
<equation confidence="0.9664734">
Ã L
R(q) |Hit — JVXµH 3S(q) ,La2 (4)
l H
l=1
L
</equation>
<bodyText confidence="0.9999355">
function F(). The first measure evaluates a scoring algo-
rithms usefulness for setting detection thresholds. This
method assumes that the scoring function calculates the
cdf of the missed score distributions. The measurement
is based on the Kolmogorov-Smirnov test statistic, which
is given by
</bodyText>
<equation confidence="0.982744333333333">
KS = max ¯¯3´ ¯¯¯¯
i ¯R(i) − i (6)
¯F M N
</equation>
<bodyText confidence="0.99970175">
where R(i)M are the raw scores for the false alarms in de-
scending order.
A metric for measuring scoring algorithms based on
result confidence is given by
</bodyText>
<equation confidence="0.989460333333333">
1 NH 1r ll21NM hF 3RM) ´ i 2
B = NH X 1−F 3RHn))J +NM E (7)
n=1
</equation>
<bodyText confidence="0.99959075">
where NM and NH are the number of hits and misses.
This value is equal to zero when all hits are scored to one
and all misses are scored as zero. On the other hand, B is
equal to 0.5 if F(R) is set to 0.5 regardless of the input.
</bodyText>
<sectionHeader confidence="0.996063" genericHeader="method">
4 Algorithms
</sectionHeader>
<bodyText confidence="0.99989025">
If one is interested in setting a detection threshold based
on false alarms per hour, then one can set the score using
the cumulative density function of the misses. This yields
the score
</bodyText>
<equation confidence="0.9988576">
3R(q)´ 3x &lt; R(q)´
FC = Pr
&amp;quot; 1 L
ÃR(q) − X µM 3Slq)´, (8)
VLσM l=1
</equation>
<bodyText confidence="0.999978333333333">
where Q is the cdf of the unit normal distribution. To set a
threshold for K false alarms per hour, then the threshold
should be set to
</bodyText>
<equation confidence="0.887552">
K (9)
KT,
</equation>
<bodyText confidence="0.99984">
where KT is the range of false alarms per hour that the
miss model is trained.
If one is looking at a list of scores, one might be inter-
ested in the probability that the score was generated by a
true match. By Bayes law, the conditional probability can
be calculated by
</bodyText>
<equation confidence="0.9956255">
3R(q)´ 3Hit|R(q)´
FB = Pr
PHp(R(q)|Hit) =(10)
PHp(R(q)  |Hit) + (1−PH)p(R(q)  |Miss)
</equation>
<bodyText confidence="0.991085">
where PH is the prior probability of a hit.
</bodyText>
<sectionHeader confidence="0.958073" genericHeader="method">
5 Model Training
</sectionHeader>
<bodyText confidence="0.9999307">
Each of the scoring methods described above require
models of how the phonemes relate to the scores through
the parameters: µM, µH, σ2M, and σ2H. For this purpose,
a series of hits and misses over the desired range of false
alarms rates must be collected from the wordspotter. With
these scores, it is possible to train the miss and hit mod-
els independently. For this reason, only the miss model
training is described here.
Given the model in Equation 5, the following distribu-
tion holds with N observations:
</bodyText>
<equation confidence="0.9975255">
p(R|S, µM, σ2M) =
L
Ar ÃR (n) −XµM 3Sl n)´ ,LσM. (11)
l=1
</equation>
<bodyText confidence="0.995364904761905">
The maximum likelihood solution for µM and σ2M is a
difficult optimization problem. However, if the phoneme
components R(n)
l from Equation 1, the distribution sim-
plifies to observations of the Gaussian components. By
using the Expectation Maximization (EM) algorithm, the
overall likelihood in Equation 11 can be iteratively max-
imized (Dempster et al., 1977).
Similarly, the training problem can also be viewed in a
Bayesian framework, where a Minimum Mean Squared
Error (MMSE) estimate can be calculated. Like the
maximum likelihood estimate, this requires an iterative
method where the components of the score are generated.
This can be computed by a Gibbs sampler (Gamerman,
1997).
In addition to providing a mechanism for creating
meaningful scores, these models can be useful for other
purposes. For example, one can analyze the mean vectors
to determine which phonemes provide better discrimina-
tion for wordspotting. These can also be used to diagnose
problems in performance that are phoneme specific.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999982928571428">
The experiments for this algorithm were conducted us-
ing the Nexidia wordspotting system trained on broadcast
quality North American English speech. The effect of us-
ing different scoring algorithms was accomplished using
a nine hour subset of the HUB-4 1996 North American
English broadcast corpus. This data was chosen since this
corpus is widely available and is disjoint from the train-
ing data used for the wordspotter. From this corpus, 8500
search terms were randomly selected from the transcripts.
These queries were equally distributed in length from 4
to 20 phonemes, and then split into a testing and training
set. For each search term, results ranging from the top
score down to the 90th false alarm were collected. The
results from the training terms were then used to train the
</bodyText>
<equation confidence="0.9883948">
= Q
α = 1.0 −
N
Y
n=1
</equation>
<bodyText confidence="0.999763952380953">
score models using both the EM algorithm and a Gibbs
sampler.
These trained models were then then used to gener-
ate both FB and FC for all of the test queries. In addi-
tion, the “Standard” scores were generated. These scores
are what the Nexidia wordspotting product reveals to the
users, and are calculated by scaling the raw scores by
the number of phonemes and mapping these from zero
to one.
The resulting scores from these tests are listed in Ta-
ble 1. As expected, the CFAR based score performed
well on the KS metric, while the Bayesian score was
more accurate on the B measure. Both of these methods
performed much better than the previous ad-hoc “Stan-
dard” method. However, performance improvements on
one measure resulted in very poor scores on the other.
This is due to the fact that the objective of each mea-
sure is very different. In addition, the estimation scheme
had little effect on the overall scores. Since the EM al-
gorithm requires a small fraction of the computation that
the Gibbs sampler requires, this method is preferable.
</bodyText>
<tableCaption confidence="0.9844705">
Table 1: Comparison of different scoring algorithms
based on two scoring measurements
</tableCaption>
<table confidence="0.996936714285714">
Algorithm Performance Measure
KS B
Gibbs CFAR 0.312 0.350
Bayes 0.790 0.197
EM CFAR 0.322 0.351
Bayes 0.789 0.196
Standard 0.633 0.496
</table>
<bodyText confidence="0.999898785714286">
To illustrate the differences between the three scoring
algorithms, the hits and misses were also collected and
plotted in Figure 6. In each subplot, there are histograms
of the hits and misses. In all three cases, most of the hits
tend to have scores close to one. However, the misses
in the standard scoring scheme are concentrated from 0.5
to 0.8. When the Bayes scoring method is used, half of
the hits are very close to 1.0, while half of the misses are
very close to 0.0. The other half of the scores are dis-
tributed along the score range. Finally, the misses from
the CFAR scoring algorithm are distributed evenly along
entire range of scores. Because the normal score assump-
tion does not strictly hold, this distribution is not perfectly
flat at the start and the end, but it is fairly close.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9998109">
Several methods for for both generating and evaluating
scores from wordspotting systems have been proposed.
These methods can operate on any system that generates
scores where an additive model based on phonemes is
valid. The scores that are produced by the algorithms
described can be used to both give intuitive confidence
levels, as well as provide a simple mechanisms for setting
thresholds in monitoring environments. These methods
have been shown to provide superior performance when
compared to their relevant metrics.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977767818181818">
M. A. Clements, P. S. Cardillo, and M. S. Miller. Pho-
netic searching vs. LVCSR: How to find what you re-
ally want in audio archives, in AVIOS 2001.
Dani Gamerman. 1997. Markov Chain Monte Carlo:
Stochastic Simulation for Bayesian Inference, vol-
ume 1. Chapman &amp; Hall, Boca Raton, FL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1–38.
Histogram of hits and misses using standard scoring
</reference>
<figure confidence="0.997090333333333">
Misses
Hits
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.3
0.2
0.1
0
Score
Histogram of hits and misses using Bayes scoring
Score
Score
Misses
Hits
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.4
0.3
0.2
0.1
0
Histogram of hits and misses using CFAR scoring
Misses
Hits
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.9962745">
Figure 1: Comparison of different scoring methods on Broadcast English queries. Scores are derived from results
ranging from zero to ten false alarms per hour.
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348537">
<title confidence="0.99999">Scoring Algorithms for Wordspotting Systems</title>
<author confidence="0.986498">Robert W Morris</author>
<author confidence="0.986498">Jon A Arrowood</author>
<author confidence="0.986498">S Peter</author>
<affiliation confidence="0.57954">Nexidia</affiliation>
<address confidence="0.9855065">3060 Peachtree Rd Suite Atlanta, Georgia</address>
<author confidence="0.976537">A Mark</author>
<affiliation confidence="0.9991395">Center for Signal &amp; Image Georgia Institute of</affiliation>
<address confidence="0.90561">Atlanta, Georgia</address>
<email confidence="0.999872">clements@ece.gatech.edu</email>
<abstract confidence="0.998636888888889">When evaluating wordspotting systems, one normally compares receiver operating characteristic curves and different measures of accuracy. However, there are many other factors that are relevant to the system’s usability for searching speech. In this paper, we discuss both measures of quality for confidence scores and propose algorithms for producing scores that are optimal with respect to these criteria. algorithms for producing scores that are optimal with respect to these criteria. 2 Assumptions In order to derive a scoring algorithm, a key assumption must be made by the wordspotting algorithm: each match must have a numeric score associated with it. In addition, there must be some theoretical basis for an additive decomposition of this score. This decomposition is given</abstract>
<intro confidence="0.703251">by</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M A Clements</author>
<author>P S Cardillo</author>
<author>M S Miller</author>
</authors>
<title>Phonetic searching vs. LVCSR: How to find what you really want in audio archives, in AVIOS</title>
<date>2001</date>
<contexts>
<context position="1683" citStr="Clements et al., 2001" startWordPosition="255" endWordPosition="258">cal basis for an additive decomposition of this score. This decomposition is given by 1 Introduction R(q) = XL R(q) l=1 l , (1) In order to evaluate any system, it is useful to have objective quality measures that can be automatically applied to systems for comparison. For wordspotting systems, these measures are oriented towards recall accuracy. Most of these measures are based on receiver operating characteristic (ROC) curves and functions of these curves. However, there are many other factors that are relevant to the systems usability. When a user enters a query to the Nexidia wordspotter (Clements et al., 2001), the system returns a sorted result list that marks the times where the query matches the audio. In addition, scores are associated with each result. These scores are related to the likelihood that the tagged audio matches the query. Although this score gives an indication of the strength of the match, users have had difficulty interpreting the scores. We found that most users want to use the score in one of two ways. The first application is to provide a score threshold for monitoring applications. Alternatively, people also assume that the score reflects the probability that the tagged audi</context>
</contexts>
<marker>Clements, Cardillo, Miller, 2001</marker>
<rawString>M. A. Clements, P. S. Cardillo, and M. S. Miller. Phonetic searching vs. LVCSR: How to find what you really want in audio archives, in AVIOS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Gamerman</author>
</authors>
<title>Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference,</title>
<date>1997</date>
<volume>1</volume>
<publisher>Chapman &amp; Hall,</publisher>
<location>Boca Raton, FL.</location>
<contexts>
<context position="6201" citStr="Gamerman, 1997" startWordPosition="1075" endWordPosition="1076">ation problem. However, if the phoneme components R(n) l from Equation 1, the distribution simplifies to observations of the Gaussian components. By using the Expectation Maximization (EM) algorithm, the overall likelihood in Equation 11 can be iteratively maximized (Dempster et al., 1977). Similarly, the training problem can also be viewed in a Bayesian framework, where a Minimum Mean Squared Error (MMSE) estimate can be calculated. Like the maximum likelihood estimate, this requires an iterative method where the components of the score are generated. This can be computed by a Gibbs sampler (Gamerman, 1997). In addition to providing a mechanism for creating meaningful scores, these models can be useful for other purposes. For example, one can analyze the mean vectors to determine which phonemes provide better discrimination for wordspotting. These can also be used to diagnose problems in performance that are phoneme specific. 6 Results The experiments for this algorithm were conducted using the Nexidia wordspotting system trained on broadcast quality North American English speech. The effect of using different scoring algorithms was accomplished using a nine hour subset of the HUB-4 1996 North A</context>
</contexts>
<marker>Gamerman, 1997</marker>
<rawString>Dani Gamerman. 1997. Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference, volume 1. Chapman &amp; Hall, Boca Raton, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5876" citStr="Dempster et al., 1977" startWordPosition="1022" endWordPosition="1025">ssible to train the miss and hit models independently. For this reason, only the miss model training is described here. Given the model in Equation 5, the following distribution holds with N observations: p(R|S, µM, σ2M) = L Ar ÃR (n) −XµM 3Sl n)´ ,LσM. (11) l=1 The maximum likelihood solution for µM and σ2M is a difficult optimization problem. However, if the phoneme components R(n) l from Equation 1, the distribution simplifies to observations of the Gaussian components. By using the Expectation Maximization (EM) algorithm, the overall likelihood in Equation 11 can be iteratively maximized (Dempster et al., 1977). Similarly, the training problem can also be viewed in a Bayesian framework, where a Minimum Mean Squared Error (MMSE) estimate can be calculated. Like the maximum likelihood estimate, this requires an iterative method where the components of the score are generated. This can be computed by a Gibbs sampler (Gamerman, 1997). In addition to providing a mechanism for creating meaningful scores, these models can be useful for other purposes. For example, one can analyze the mean vectors to determine which phonemes provide better discrimination for wordspotting. These can also be used to diagnose </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="false">
<title>Histogram of hits and misses using standard scoring</title>
<marker></marker>
<rawString>Histogram of hits and misses using standard scoring</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>