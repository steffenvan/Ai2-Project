<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002494">
<title confidence="0.9953795">
University of Warwick: SENTIADAPTRON - A Domain Adaptable
Sentiment Analyser for Tweets - Meets SemEval
</title>
<author confidence="0.997647">
Richard Townsend Aaron Kalair Ojas Kulkarni Rob Procter
</author>
<affiliation confidence="0.999032">
University of Warwick University of Warwick University of Warwick University of Warwick
</affiliation>
<email confidence="0.572453">
Richard.Townsend@warwick.ac.uk Aaron.Kalair@warwick.ac.uk Ojas.Kulkarni@warwick.ac.uk Rob.Procter@warwick.ac.uk
</email>
<author confidence="0.988029">
Maria Liakata
</author>
<affiliation confidence="0.996806">
University of Warwick
</affiliation>
<email confidence="0.995453">
M.Liakata@warwick.ac.uk
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818">
We give a brief overview of our system,
SentiAdaptron, a domain-sensitive and do-
main adaptable system for twitter analy-
sis in tweets, and discuss performance on
SemEval (in both the constrained and un-
constrained scenarios), as well as implica-
tions arising from comparing the intra- and
inter- domain performance on our twitter
corpus.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988408">
A domain is broadly defined as a set of documents
demonstrating a similar distribution of words and
linguistic patterns. Task 9 of SemEval treats Twit-
ter as a single domain with respect to sentiment
analysis. However previous research has argued
for the topic-specific treatment of sentiment given
domain-specific nuances and the over-generality
of current sentiment analysis systems with respect
to applications in the social sciences (Thelwall and
Buckley, 2013). Thelwal’s method - manually ex-
tending a sentiment lexicon for a particular topic
or domain - highlights that expression of senti-
ment varies from one domain to another. Rather
than relying on the manual extension of lexica, we
developed an approach to Twitter sentiment clas-
sification that is domain sensitive. To this effect
we gathered tweets from three primary domains -
financial news, political opinion, technology com-
panies and their products - and trained our system
on one domain while adapting to the other. Us-
ing this methodology we obtained both intrinsic
as well as extrinsic evaluation of the system on
real world applications with promising results. As
our approach to sentiment analysis has been influ-
enced by the task description of SemEval 2013 we
</bodyText>
<footnote confidence="0.53211875">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<figure confidence="0.832414375">
Positive Negative Neutral Objective
SemEval 11610 6332 905 189887
Our corpus 10725 17837 3514 36904
(a) Contextual polarity.
Positive Negative Neutral Objective
SemEval 4215 1798 4082 1243
Our corpus 1090 1711 1191 -
(b) Message polarity.
</figure>
<tableCaption confidence="0.968521">
Table 1: The distribution of sentiment classes be-
</tableCaption>
<bodyText confidence="0.971049142857143">
tween SemEval and our corpus at word and tweet
level.
decided to also evaluate the system on SemEval’s
data, since it provides a well established bench-
mark. In the following we briefly describe our
system and corpus and discuss our approach for
the SemEval submission.
</bodyText>
<sectionHeader confidence="0.7970325" genericHeader="introduction">
2 A Corpus of Three Domains: Source of
Unconstrained Data
</sectionHeader>
<bodyText confidence="0.9999479">
Our goal in developing SentiAdaptron was domain
adaptive tweet-level classification. We decided to
follow SemEval 2013 and collect both word-level
as well as message level annotations. We prepared
a corpus of 4000 tweets with a balanced cover-
age of financial, political and technology related
tweets. Tweets were collected using keywords
from domain-specific websites: the final list was
chosen after evaluating each candidate keyword’s
popularity using a third-party service1. Each tweet
is tagged with multiple candidate domains, based
on a hierarchy of terms generated from the original
keyword list and tweets are filtered using a clus-
tering methodology based on the DBSCAN clus-
tering algorithm to remove robotic and repetitive
tweets. We performed domain disambiguation for
annotation through keyword filtering and also by
picking a number of synsets from WordNet and
computing the tweet’s mean semantic distance us-
ing the NLTK toolkit. Tweets which didn’t contain
</bodyText>
<footnote confidence="0.998155">
1http://topsy.com
</footnote>
<page confidence="0.921671">
768
</page>
<note confidence="0.7364525">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 768–772,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999964952380953">
any words associated with a score of less than 0.3
in SentiWordNet were removed, in a manner sim-
ilar to Nakov et al (2013). After further manual
relevance checks, the remaining tweets were sub-
mitted to Amazon’s Mechanical Turk service for
message and phrase-level annotation. We initially
used the form demonstrated by Nakov et al., al-
though we later redesigned it with a dramatic im-
provement in annotator performance and annota-
tion quality. Each tweet was annotated by four
workers and annotation sparsity at the phrase level
was addressed by taking the majority of annota-
tions following the precedence neutral &gt; pos &gt;
neg &gt; other. This is in contrast to the approach
used by Nakov et al. for SemEval which intersects
annotations. Annotations at the tweet level were
aggregated using a majority vote. We found that
using the proportion of positive, negative and neu-
tral words in a tweet is a surprisingly robust fea-
ture for cross-domain classification, and boosted
our performance when using bigrams.
</bodyText>
<sectionHeader confidence="0.587216" genericHeader="method">
3 Subjectivity Detection and Contextual
Polarity Disambiguation (Subtask A)
</sectionHeader>
<bodyText confidence="0.999987052631579">
Sentiment lexicons such as SentiWordNet (Esuli
and Sebastiani, 2006), the NRC emotions lexicon
(Mohammad and Turney, 2010), the MPQA lex-
icon (Wilson et al., 2005) and the Bing Liu Lex-
icon (Hu and Liu, 2004) have been used for de-
termining whether a phrase should be labelled as
positive, negative or neutral within a tweet or sen-
tence (contextual polarity). However, lexical re-
sources are by nature non-contextual and may not
have good coverage over a given domain. We in-
stead considered how to infer contextual polarity
purely from the data available.
To address the problem of class imbalance in
the tweets, we break the problem of contextual po-
larity detection into two stages: (i) we first deter-
mine whether a given word should be assigned a
positive, negative or neutral annotation (subjectiv-
ity detection) and (ii) distinguish subjective tweets
into positive, negative neutral.
</bodyText>
<subsectionHeader confidence="0.998726">
3.1 Contextual Subjectivity Detection
</subsectionHeader>
<bodyText confidence="0.999878488372093">
Task A asks participants to predict the contextual
subjectivity annotation of a text span at a given
offset: our extrinsic applications don’t have this
fundamental structure, so we considered whether
it was possible to automatically separate the con-
tent of input documents into those regions which
should be assigned an annotation and those which
should not. We considered a unigram and a bigram
baseline using a naive Bayes classifier, which gave
an F1 score of 0.640 and 0.520 respectively on our
in domain data (under 10-fold cross-validation).
We followed a number of approaches to subjec-
tivity detection to try and improve on the baseline
including sequential modelling using linear-chain
Conditional Random Fields (CRFs) with CRF-
suite (Okazaki, 2007) and lexical inference us-
ing semantically disambiguated WordNet (Miller,
1995) synsets in conjunction with their occurrence
in a subjective context.
We found that the observed subjective pro-
portion of a given word alongside its successor
and predecessor2 was a viable feature engineer-
ing scheme, which we call neighbouring subjec-
tivity proportions. This gave the best subjectivity
performance on our in-domain data when fed to a
voted perceptron (Freund and Schapire, 1999), an
ensemble approach which assigns particularly pre-
dictive iterations of an incrementally trained per-
ceptron a greater weight when deciding the final
classification, and offers wide-margin classifica-
tion akin to support vector machines whilst also
requiring less parameter exploration. We used the
implementation provided by the WEKA machine-
learning environment (Hall et al., 2009), which
achieved an F1-score of 0.740 (again under 10-
fold cross validation) for our in-domain data, but
performance dropped to an F1-score of 0.323 on
the SemEval 2013 training and development data.
Table 1 indicates that the proportion of objective
features in SemEval is much greater than that seen
within our own corpus, likely due to the differ-
ences in the way we processed annotations (out-
lined in Section 2).
</bodyText>
<subsectionHeader confidence="0.999555">
3.2 Contextual Polarity
</subsectionHeader>
<bodyText confidence="0.846172230769231">
We considered a naive Bayes unigram baseline,
(similar approaches have proven popular with Se-
mEval 2013 participants for Task A) and achieved
an F1-measure of 0.662 when training using Se-
mEval’s 2014 training and development data and
evaluating on SemEval’s 2013 gold-standard an-
notations. However we could not detect the neu-
tral class, and the test did not consider the objec-
tive class.
2As an example, if we observe 14 total occurrences of the
word “heartbreaking”, and 13 of them appear with a positive,
negative or neutral label, the subjective proportion computed
would be 13/14.
</bodyText>
<page confidence="0.975275">
769
</page>
<table confidence="0.998965125">
F1-score Precision Recall
P N E Overall P N E P N E
Task A (constrained) 85.5 49.0 9.84 67.3 91.1 41.4 3.8 80.6 60.2 16.7
Task A (unconstrained) 85.1 49.2 12.2 67.2 89.8 43.4 8.0 80.9 59.8 25.9
Finance 78.0 83.1 51.2 78.0 77.3 81.7 58.7 78.7 45.5 84.6
Politics 67.3 83.3 42.1 74.3 70.9 79.7 50.3 64.1 87.2 36.3
Technology 75.1 85.6 47.4 77.8 77.8 81.8 56.7 89.8 89.8 40.7
(a) Performance on word-level contextual annotation tasks.
F1-score Precision Recall
P N E Overall P N E P N E
Task B (constrained) 57.1 34.0 57.0 45.6 46.1 42.6 68.8 75.0 28.3 48.7
Task B (unconstrained) 57.2 33.0 57.7 45.1 46.2 36.1 72.1 74.9 30.4 47.9
Finance 78.7 78.9 64.6 73.8 77.9 79.5 64.8 78.3 79.5 64.5
Politics 80.8 75.0 61.9 72.3 78.4 74.8 63.7 83.3 75.1 60.2
Technology 73.2 78.2 58.0 70.1 69.7 76.3 62.9 76.9 80.2 53.7
(b) Performance on document-level annotation tasks.
</table>
<tableCaption confidence="0.844048">
Table 2: Classifier metrics obtained from 4-fold intra-domain cross-validation (using reference annota-
tions) and results for subtasks A and B of SemEval task 9 (computed using reference scorer).
</tableCaption>
<table confidence="0.9999638">
Tech Politics Finance
Unigrams 0.53 0.35 0.53
Bigrams 0.38 0.31 0.52
Bigrams + SP 0.77 0.65 0.78
Unigrams + SP 0.68 0.62 0.76
</table>
<tableCaption confidence="0.997305">
Table 3: F1-scores achieved for each domain on
</tableCaption>
<bodyText confidence="0.99438375">
our corpus (naive Bayes, 10-fold cross valida-
tion) using reference annotations with and without
subjective (positive, negative, neutral) proportions
(SP).
We improved on this baseline by combining un-
igrams with information from the wider context of
the tweet. The algorithm first runs subjectivity de-
tection on the entire document and then, for each
word we need to classify (or otherwise each word
detected as subjective), effectively generates two
bags of words consisting of the subjective words
before and after that word (we also included any
adverbs as annotated by the Gimpel tagger (2011)
in this bag to improve robustness). We output the
word itself as a further feature, and use a random
forest classifier (10 trees, loge N + 1 features) to
generate the annotation. We found this approach
outperformed the other approaches we tried (in-
cluding Naive Bayes and OneR) and also gave us
better F1-scores on the neutral class. Results from
this approach for our in-domain data and the Se-
mEval 2014 data can be seen in Table 2a. The
drop in performance from our in-domain data to
SemEval 2014 can be explained by the different
class distribution observed in SemEval (Table 1).
Subjectivity detection was used to generate fea-
tures for subtask B, but not subtask A, where the
target subjective phrases are already given.
</bodyText>
<sectionHeader confidence="0.9419995" genericHeader="method">
4 Message Polarity Classification
(Subtask B)
</sectionHeader>
<bodyText confidence="0.999963066666667">
We tried various different combinations of fea-
tures to discover the best intra-domain classifica-
tion approach for our corpus and found that the
proportion of positive, negative and neutral words
within a tweet boosted performance using bigram
binary features (Table 3). This involves first run-
ning the contextual polarity detection component
as described in Section 3 and feeding in the results
as features (together with bigrams) into a naive
Bayes classifier for tweet level sentiment detec-
tion. However, one of our hypotheses was that do-
main adaptation could help improve performance
when moving from one domain to another, effec-
tively allowing us to port our classifier from our
own corpus to SemEval.
</bodyText>
<subsectionHeader confidence="0.972095">
4.1 Cross-Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999915684210526">
Our research in domain adaptation uses and ex-
tends the technique described by Blitzer and
Pereira (2007) called Structural Correspondence
Learning (SCL), which derives a relationship (or
correspondence) between features from two dif-
ferent domains. This is done via pivot features se-
lected from the intersection of features from both
domains which have been ranked according to mu-
tual information. The technique then uses N pivot
features from both the seen and unseen domains
to learn a set of binary problems corresponding to
whether a given pivot exists in a target document.
A perceptron is then used to train each of the bi-
nary problems, giving a matrix of weights (where
a weight represents covariance of non pivot fea-
tures with pivot features). We extended Blitzer’s
technique to encompass the neutral class and gave
a wider notion of domain than previously found in
the literature. As an example Liu et al. (2013) use
</bodyText>
<page confidence="0.988927">
770
</page>
<table confidence="0.99945725">
Training domains Test Accuracy Precision Recall F-measure
Tech &amp; Finance Politics 0.76 0.52 0.40 0.45
Tech &amp; Politics Finance 0.69 0.51 0.78 0.61
Finance &amp; Politics Tech 0.63 0.53 0.58 0.55
</table>
<tableCaption confidence="0.984732">
Table 4: Binary class metrics with structural correspondence learning on our own corpus.
</tableCaption>
<table confidence="0.997461916666667">
F-measure Precision Recall
Train Test Accuracy SP Baseline(Bigrams) Loss SP Baseline (Bigrams) Loss SP Baseline (Bigrams) Loss
(SP)
Tech Politics 69.74% 0.67 0.24 0.43 0.64 0.19 0.45 0.71 0.33 0.38
Tech Finance 72.28% 0.78 0.20 0.58 0.74 0.15 0.59 0.83 0.33 0.50
Politics Tech 72.40% 0.76 0.21 0.55 0.71 0.25 0.46 0.82 0.33 0.49
Politics Finance 73.06% 0.79 0.22 0.57 0.77 0.16 0.61 0.82 0.33 0.49
Finance Politics 68.39% 0.66 0.23 0.43 0.67 0.18 0.49 0.66 0.33 0.33
Finance Tech 71.92% 0.76 0.22 0.54 0.71 0.16 0.55 0.82 0.33 0.49
Tech &amp; Finance Politics 69.17% 0.63 0.23 0.40 0.61 0.18 0.43 0.67 0.33 0.34
Tech &amp; Politics Finance 70.49% 0.63 0.20 0.43 0.61 0.15 0.46 0.64 0.33 0.31
Finance &amp; Politics Tech 72.88% 0.77 0.22 0.55 0.72 0.16 0.56 0.84 0.33 0.51
</table>
<tableCaption confidence="0.8538555">
Table 5: Classifier metrics from training and testing on different domains, with and without proportions
of positive, negative and neutral phrases from the source domain (Subjective Proportions SP).
</tableCaption>
<bodyText confidence="0.999987828571428">
an SVM-derived technique to adapt on domains
containing terms relevant to Google and Twitter,
which are both considered part of the technology
domain in our corpus, whereas we attempted to
adapt from technology topics to financial news and
political opinions.
We found that the amount of mutual informa-
tion in our three domains was very low and was
practically zero for the three class version of our
problem. The results for the binary version of the
classifier generated poorer results (Table 4) than
those produced by our back-up classifier (based
on the naive Bayes bigrams and subjective phrase
proportions from the source domain, see Table
5, last three rows). Therefore we generated our
submission to SemEval 2014 based on bigrams
and subjective proportions rather than SCL, since
we found that the proportion of pos/neg/neutral
phrases is a robust feature across domains (as long
as it can be reliably predicted during the contex-
tual polarity prediction stage, which was the case
for our data). Our results for SemEval task B us-
ing the subjective phrase proportions can be found
in Table 2b. Our unconstrained performance in-
dicates that whilst this classifier provides reason-
able cross domain performance for our own data
(Table 5), it is very sensitive to the performance
of subjectivity and contextual polarity detection,
which is lower for SemEval than it is for our own
corpus. Presumably the reason for this is the dif-
ferent assumptions in annotations in the two cases
and the differences in the class distributions be-
tween SemEval and our own data. This meant that
our performance was lower than systems that had
specifically trained on the SemEval data.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999349">
Our goal was to demonstrate the potential of do-
main sensitivity and domain adaptability for senti-
ment analysis in tweets - a task which brings chal-
lenges defying the use of fixed lexica. We found
that the proportions of positive, negative and neu-
tral tweets are quite robust cross-domain features,
although we do think that domain adaptation tech-
niques such as Structural Correspondence Learn-
ing merit further investigation in the context of
sentiment analysis for Twitter.
</bodyText>
<sectionHeader confidence="0.696359" genericHeader="method">
Access to the source code of this
submission
</sectionHeader>
<bodyText confidence="0.99990225">
The source code of the applications used to
gather and prepare our corpus, conduct CRF-
suite and structural correspondence learning,
and the Java-based environment used to gen-
erate our final submission are available at
https://github.com/Sentimentron/
Nebraska-public3 and https://
github.com/Sentimentron/PRJ90814.
</bodyText>
<sectionHeader confidence="0.994984" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9971728">
Warwick Research Development Fund grant
RD13129 provided funding for crowdsourced an-
notations. We thank our partners at CUSP, NYU
for enabling us to use Amazon Mechanical Turk
for this process.
</bodyText>
<footnote confidence="0.9972">
3http://dx.doi.org/10.5281/zenodo.9906
4http://dx.doi.org/10.5281/zenodo.9904
</footnote>
<page confidence="0.995387">
771
</page>
<sectionHeader confidence="0.989874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853157142857">
Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xi-
aowei Xu. 1996. A density-based algorithm for
discovering clusters in large spatial databases with
noise. In KDD, volume 96, pages 226–231.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC, volume 6,
pages 417–422.
Yoav Freund and Robert E Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine learning, 37(3):277–296.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ’11, pages 42–47.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
Mark Dredze John Blitzer and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 440–
447.
Shenghua Liu, Fuxin Li, Fangtao Li, Xueqi Cheng, and
Huawei Shen. 2013. Adaptive co-training SVM for
sentiment classification on tweets. In Proceedings
of the 22nd ACM international conference on Con-
ference on information &amp; knowledge management,
pages 2079–2088.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Saif M Mohammad and Peter D Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26–34.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. pages 312–320, June.
Naoaki Okazaki. 2007. CRFsuite: a fast
implementation of Conditional Random Fields
(CRFs). [ONLINE] http://www.chokkan.
org/software/crfsuite/ (Retrieved: July
16, 2014).
Mike Thelwall and Kevan Buckley. 2013. Topic-based
sentiment analysis for the social web: The role of
mood and issue-related words. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 64(8):1608–1617.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. OpinionFinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34–35.
</reference>
<page confidence="0.997412">
772
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420397">
<title confidence="0.95403">University of Warwick: SENTIADAPTRON - A Domain Sentiment Analyser for Tweets - Meets SemEval</title>
<author confidence="0.999831">Richard Townsend Aaron Kalair Ojas Kulkarni Rob Procter</author>
<affiliation confidence="0.998992">University of Warwick University of Warwick University of Warwick University of Warwick</affiliation>
<title confidence="0.478191">Richard.Townsend@warwick.ac.uk Aaron.Kalair@warwick.ac.uk Ojas.Kulkarni@warwick.ac.uk Rob.Procter@warwick.ac.uk</title>
<author confidence="0.9923">Maria Liakata</author>
<affiliation confidence="0.999731">University of Warwick</affiliation>
<email confidence="0.998397">M.Liakata@warwick.ac.uk</email>
<abstract confidence="0.9960926">We give a brief overview of our system, SentiAdaptron, a domain-sensitive and domain adaptable system for twitter analysis in tweets, and discuss performance on SemEval (in both the constrained and unconstrained scenarios), as well as implications arising from comparing the intraand interdomain performance on our twitter corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Martin Ester</author>
<author>Hans-Peter Kriegel</author>
<author>J¨org Sander</author>
<author>Xiaowei Xu</author>
</authors>
<title>A density-based algorithm for discovering clusters in large spatial databases with noise.</title>
<date>1996</date>
<booktitle>In KDD,</booktitle>
<volume>96</volume>
<pages>226--231</pages>
<marker>Ester, Kriegel, Sander, Xu, 1996</marker>
<rawString>Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, volume 96, pages 226–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>417--422</pages>
<contexts>
<context position="5111" citStr="Esuli and Sebastiani, 2006" startWordPosition="766" endWordPosition="769">e level was addressed by taking the majority of annotations following the precedence neutral &gt; pos &gt; neg &gt; other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of LREC, volume 6, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="7193" citStr="Freund and Schapire, 1999" startWordPosition="1094" endWordPosition="1097">ction to try and improve on the baseline including sequential modelling using linear-chain Conditional Random Fields (CRFs) with CRFsuite (Okazaki, 2007) and lexical inference using semantically disambiguated WordNet (Miller, 1995) synsets in conjunction with their occurrence in a subjective context. We found that the observed subjective proportion of a given word alongside its successor and predecessor2 was a viable feature engineering scheme, which we call neighbouring subjectivity proportions. This gave the best subjectivity performance on our in-domain data when fed to a voted perceptron (Freund and Schapire, 1999), an ensemble approach which assigns particularly predictive iterations of an incrementally trained perceptron a greater weight when deciding the final classification, and offers wide-margin classification akin to support vector machines whilst also requiring less parameter exploration. We used the implementation provided by the WEKA machinelearning environment (Hall et al., 2009), which achieved an F1-score of 0.740 (again under 10- fold cross validation) for our in-domain data, but performance dropped to an F1-score of 0.323 on the SemEval 2013 training and development data. Table 1 indicate</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E Schapire. 1999. Large margin classification using the perceptron algorithm. Machine learning, 37(3):277–296.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="7576" citStr="Hall et al., 2009" startWordPosition="1148" endWordPosition="1151">and predecessor2 was a viable feature engineering scheme, which we call neighbouring subjectivity proportions. This gave the best subjectivity performance on our in-domain data when fed to a voted perceptron (Freund and Schapire, 1999), an ensemble approach which assigns particularly predictive iterations of an incrementally trained perceptron a greater weight when deciding the final classification, and offers wide-margin classification akin to support vector machines whilst also requiring less parameter exploration. We used the implementation provided by the WEKA machinelearning environment (Hall et al., 2009), which achieved an F1-score of 0.740 (again under 10- fold cross validation) for our in-domain data, but performance dropped to an F1-score of 0.323 on the SemEval 2013 training and development data. Table 1 indicates that the proportion of objective features in SemEval is much greater than that seen within our own corpus, likely due to the differences in the way we processed annotations (outlined in Section 2). 3.2 Contextual Polarity We considered a naive Bayes unigram baseline, (similar approaches have proven popular with SemEval 2013 participants for Task A) and achieved an F1-measure of </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="5249" citStr="Hu and Liu, 2004" startWordPosition="792" endWordPosition="795">h used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into two stages: (i) we first determine whether a given word should be assigned a positive, negative or neutral annotation (subjectivity detect</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze John Blitzer</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="12037" citStr="Blitzer and Pereira (2007)" startWordPosition="1889" endWordPosition="1892">osted performance using bigram binary features (Table 3). This involves first running the contextual polarity detection component as described in Section 3 and feeding in the results as features (together with bigrams) into a naive Bayes classifier for tweet level sentiment detection. However, one of our hypotheses was that domain adaptation could help improve performance when moving from one domain to another, effectively allowing us to port our classifier from our own corpus to SemEval. 4.1 Cross-Domain Adaptation Our research in domain adaptation uses and extends the technique described by Blitzer and Pereira (2007) called Structural Correspondence Learning (SCL), which derives a relationship (or correspondence) between features from two different domains. This is done via pivot features selected from the intersection of features from both domains which have been ranked according to mutual information. The technique then uses N pivot features from both the seen and unseen domains to learn a set of binary problems corresponding to whether a given pivot exists in a target document. A perceptron is then used to train each of the binary problems, giving a matrix of weights (where a weight represents covarian</context>
</contexts>
<marker>Blitzer, Pereira, 2007</marker>
<rawString>Mark Dredze John Blitzer and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440– 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shenghua Liu</author>
<author>Fuxin Li</author>
<author>Fangtao Li</author>
<author>Xueqi Cheng</author>
<author>Huawei Shen</author>
</authors>
<title>Adaptive co-training SVM for sentiment classification on tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>2079--2088</pages>
<contexts>
<context position="12853" citStr="Liu et al. (2013)" startWordPosition="2024" endWordPosition="2027">section of features from both domains which have been ranked according to mutual information. The technique then uses N pivot features from both the seen and unseen domains to learn a set of binary problems corresponding to whether a given pivot exists in a target document. A perceptron is then used to train each of the binary problems, giving a matrix of weights (where a weight represents covariance of non pivot features with pivot features). We extended Blitzer’s technique to encompass the neutral class and gave a wider notion of domain than previously found in the literature. As an example Liu et al. (2013) use 770 Training domains Test Accuracy Precision Recall F-measure Tech &amp; Finance Politics 0.76 0.52 0.40 0.45 Tech &amp; Politics Finance 0.69 0.51 0.78 0.61 Finance &amp; Politics Tech 0.63 0.53 0.58 0.55 Table 4: Binary class metrics with structural correspondence learning on our own corpus. F-measure Precision Recall Train Test Accuracy SP Baseline(Bigrams) Loss SP Baseline (Bigrams) Loss SP Baseline (Bigrams) Loss (SP) Tech Politics 69.74% 0.67 0.24 0.43 0.64 0.19 0.45 0.71 0.33 0.38 Tech Finance 72.28% 0.78 0.20 0.58 0.74 0.15 0.59 0.83 0.33 0.50 Politics Tech 72.40% 0.76 0.21 0.55 0.71 0.25 0.4</context>
</contexts>
<marker>Liu, Li, Li, Cheng, Shen, 2013</marker>
<rawString>Shenghua Liu, Fuxin Li, Fangtao Li, Xueqi Cheng, and Huawei Shen. 2013. Adaptive co-training SVM for sentiment classification on tweets. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 2079–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="6798" citStr="Miller, 1995" startWordPosition="1035" endWordPosition="1036">tomatically separate the content of input documents into those regions which should be assigned an annotation and those which should not. We considered a unigram and a bigram baseline using a naive Bayes classifier, which gave an F1 score of 0.640 and 0.520 respectively on our in domain data (under 10-fold cross-validation). We followed a number of approaches to subjectivity detection to try and improve on the baseline including sequential modelling using linear-chain Conditional Random Fields (CRFs) with CRFsuite (Okazaki, 2007) and lexical inference using semantically disambiguated WordNet (Miller, 1995) synsets in conjunction with their occurrence in a subjective context. We found that the observed subjective proportion of a given word alongside its successor and predecessor2 was a viable feature engineering scheme, which we call neighbouring subjectivity proportions. This gave the best subjectivity performance on our in-domain data when fed to a voted perceptron (Freund and Schapire, 1999), an ensemble approach which assigns particularly predictive iterations of an incrementally trained perceptron a greater weight when deciding the final classification, and offers wide-margin classification</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="5165" citStr="Mohammad and Turney, 2010" startWordPosition="774" endWordPosition="777">ions following the precedence neutral &gt; pos &gt; neg &gt; other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into two stages: (i) we first determine whether a given wor</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M Mohammad and Peter D Turney. 2010. Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<date>2013</date>
<booktitle>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</booktitle>
<pages>312--320</pages>
<contexts>
<context position="4087" citStr="Nakov et al (2013)" startWordPosition="606" endWordPosition="609">a clustering methodology based on the DBSCAN clustering algorithm to remove robotic and repetitive tweets. We performed domain disambiguation for annotation through keyword filtering and also by picking a number of synsets from WordNet and computing the tweet’s mean semantic distance using the NLTK toolkit. Tweets which didn’t contain 1http://topsy.com 768 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 768–772, Dublin, Ireland, August 23-24, 2014. any words associated with a score of less than 0.3 in SentiWordNet were removed, in a manner similar to Nakov et al (2013). After further manual relevance checks, the remaining tweets were submitted to Amazon’s Mechanical Turk service for message and phrase-level annotation. We initially used the form demonstrated by Nakov et al., although we later redesigned it with a dramatic improvement in annotator performance and annotation quality. Each tweet was annotated by four workers and annotation sparsity at the phrase level was addressed by taking the majority of annotations following the precedence neutral &gt; pos &gt; neg &gt; other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects ann</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. pages 312–320, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields</title>
<date>2007</date>
<note>(CRFs). [ONLINE] http://www.chokkan. org/software/crfsuite/ (Retrieved:</note>
<contexts>
<context position="6720" citStr="Okazaki, 2007" startWordPosition="1025" endWordPosition="1026">have this fundamental structure, so we considered whether it was possible to automatically separate the content of input documents into those regions which should be assigned an annotation and those which should not. We considered a unigram and a bigram baseline using a naive Bayes classifier, which gave an F1 score of 0.640 and 0.520 respectively on our in domain data (under 10-fold cross-validation). We followed a number of approaches to subjectivity detection to try and improve on the baseline including sequential modelling using linear-chain Conditional Random Fields (CRFs) with CRFsuite (Okazaki, 2007) and lexical inference using semantically disambiguated WordNet (Miller, 1995) synsets in conjunction with their occurrence in a subjective context. We found that the observed subjective proportion of a given word alongside its successor and predecessor2 was a viable feature engineering scheme, which we call neighbouring subjectivity proportions. This gave the best subjectivity performance on our in-domain data when fed to a voted perceptron (Freund and Schapire, 1999), an ensemble approach which assigns particularly predictive iterations of an incrementally trained perceptron a greater weight</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs). [ONLINE] http://www.chokkan. org/software/crfsuite/ (Retrieved: July 16, 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
</authors>
<title>Topic-based sentiment analysis for the social web: The role of mood and issue-related words.</title>
<date>2013</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>64</volume>
<issue>8</issue>
<contexts>
<context position="1243" citStr="Thelwall and Buckley, 2013" startWordPosition="166" endWordPosition="169">rained and unconstrained scenarios), as well as implications arising from comparing the intra- and inter- domain performance on our twitter corpus. 1 Introduction A domain is broadly defined as a set of documents demonstrating a similar distribution of words and linguistic patterns. Task 9 of SemEval treats Twitter as a single domain with respect to sentiment analysis. However previous research has argued for the topic-specific treatment of sentiment given domain-specific nuances and the over-generality of current sentiment analysis systems with respect to applications in the social sciences (Thelwall and Buckley, 2013). Thelwal’s method - manually extending a sentiment lexicon for a particular topic or domain - highlights that expression of sentiment varies from one domain to another. Rather than relying on the manual extension of lexica, we developed an approach to Twitter sentiment classification that is domain sensitive. To this effect we gathered tweets from three primary domains - financial news, political opinion, technology companies and their products - and trained our system on one domain while adapting to the other. Using this methodology we obtained both intrinsic as well as extrinsic evaluation </context>
</contexts>
<marker>Thelwall, Buckley, 2013</marker>
<rawString>Mike Thelwall and Kevan Buckley. 2013. Topic-based sentiment analysis for the social web: The role of mood and issue-related words. Journal of the American Society for Information Science and Technology, 64(8):1608–1617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="5205" citStr="Wilson et al., 2005" startWordPosition="782" endWordPosition="785">neg &gt; other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into two stages: (i) we first determine whether a given word should be assigned a positive, negativ</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. OpinionFinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34–35.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>