<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016721">
<title confidence="0.994463">
Discourse Connectors for Latent Subjectivity in Sentiment Analysis
</title>
<author confidence="0.995847">
Rakshit Trivedi
</author>
<affiliation confidence="0.99898">
College of Computing
Georgia Institute of Technology
</affiliation>
<address confidence="0.940239">
Atlanta, GA 30308, USA
</address>
<email confidence="0.998981">
rtrivedi6@gatech.edu
</email>
<author confidence="0.997272">
Jacob Eisenstein
</author>
<affiliation confidence="0.999162">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<address confidence="0.94031">
Atlanta, GA 30308, USA
</address>
<email confidence="0.999613">
jacobe@gatech.edu
</email>
<sectionHeader confidence="0.998014" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.841290636363636">
Document-level sentiment analysis can bene-
fit from fine-grained subjectivity, so that sen-
timent polarity judgments are based on the
relevant parts of the document. While fine-
grained subjectivity annotations are rarely
available, encouraging results have been ob-
tained by modeling subjectivity as a latent
variable. However, latent variable models
fail to capitalize on our linguistic knowledge
about discourse structure. We present a new
method for injecting linguistic knowledge into
latent variable subjectivity modeling, using
discourse connectors. Connector-augmented
transition features allow the latent variable
model to learn the relevance of discourse con-
nectors for subjectivity transitions, without
subjectivity annotations. This yields signif-
icantly improved performance on document-
level sentiment analysis in English and Span-
ish. We also describe a simple heuristic for
automatically identifying connectors when no
predefined list is available.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998675">
Document-level sentiment analysis can benefit from
consideration of discourse structure. Voll and
Taboada (2007) show that adjective-based sentiment
classification is improved by examining topicality
(whether each sentence is central to the overall
point); Yessenalina et al. (2010b) show that bag-of-
ngrams sentiment classification is improved by ex-
amining subjectivity (whether a sentence expresses
a subjective opinion or objective fact). However, it
is unclear how best to obtain the appropriate dis-
course analyses. Voll and Taboada (2007) find that
domain-independent discourse parsing (Soricut and
Marcu, 2003) offers little improvement for senti-
ment analysis, so they resort to training a domain-
specific model for identifying topic sentences in re-
views. But this requires a labeled dataset of topic
sentences, imposing a substantial additional cost.
Yessenalina et al. (2010b) treat sentence level
subjectivity as a latent variable, automatically in-
ducing the “annotator rationale” (Zaidan et al., 2007;
Yessenalina et al., 2010a) for each training sen-
tence so as to focus sentiment learning on the sub-
jective parts of the document. This yields sig-
nificant improvements over bag-of-ngrams super-
vised sentiment classification. Latent variable sub-
jectivity analysis is attractive because it requires
neither subjectivity annotations nor an accurate
domain-independent discourse parser. But while the
“knowledge-free” nature of this approach is appeal-
ing, it is unsatisfying that it fails to exploit decades
of research on discourse structure.
In this paper, we explore a lightweight approach
to injecting linguistic knowledge into latent variable
models of subjectivity. The entry point is a set of
discourse connectors: words and phrases that signal
a shift or continuation in the discourse structure.
Such connectors have been the subject of exten-
sive study in the creation of the Penn Discourse
Treebank (PDTB: Prasad et al. 2008). The role
of discourse connectors in sentiment analysis can
be clearly seen in examples, such as “It’s hard to
imagine the studios hiring another manic German
maverick to helm a cop thriller. But that’s exactly
why the movie is unmissable.” (Huddleston, 2010)
</bodyText>
<page confidence="0.967164">
808
</page>
<subsectionHeader confidence="0.294636">
Proceedings of NAACL-HLT 2013, pages 808–813,
</subsectionHeader>
<bodyText confidence="0.988500903225807">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
We present a new approach to incorporate
discourse connectors in a latent subjectivity
model (Yessenalina et al., 2010b). This approach
requires no manually-specified information about
the meaning of the connectors, just the connectors
themselves. Our approach builds on proximity
features, which give the latent variable model a way
to prefer or disprefer subjectivity and sentiment
transitions, usually with the goal of encouraging
smoothness across the document. By taking
the cross-product of these features with a set of
discourse connectors, we obtain a new set of
connector-augmented transition features, which
capture the way discourse connectors are used to
indicate subjectivity and sentiment transitions. The
model is thus able to learn that subjectivity shifts
are likely to be accompanied by connectors such as
however or nonetheless.
We present experiments in both English and Span-
ish showing that this method of incorporating dis-
course connectors yields significant improvements
in document-level sentiment analysis. In case no
list of connectors is available, we describe a sim-
ple heuristic for automatically identifying candidate
connector words. The automatically identified con-
nectors do not perform as well as the expert-defined
lists, but they still outperform a baseline method
that ignores discourse connectors (in English). This
demonstrates both the robustness of the approach
and the value of linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.986983" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999965090909091">
Given accurate labels of the subjectivity of each
sentence, a document-level sentiment analyzer
could safely ignore the sentences marked as non-
subjective.1 This would be beneficial for training as
well as prediction, because the learning algorithm
would not be confused by sentences that contradict
the document label. But in general we cannot rely on
having access to sentence-level subjectivity annota-
tions. Instead, we treat subjectivity as a latent vari-
able, and ask the learner to impute its value. Given
document-level sentiment annotations and an initial
</bodyText>
<footnote confidence="0.86181675">
1Discourse parsing often focuses on sub-sentence elemen-
tary discourse units (Mann and Thompson, 1988). For sim-
plicity, we consider units at the sentence level only, and leave
finer-grained analysis for future work.
</footnote>
<bodyText confidence="0.999580583333333">
model, the learner can mark as non-subjective those
sentences whose analysis disagrees with the docu-
ment label.
More formally, each document has a label y ∈
{−1,1}, a set of sentences x, and a set of per-
sentence subjectivity judgments h ∈ {0,1}S, where
S is the number of sentences. We compute a set
of features on these variables, and score each in-
stance by a weighted combination of the features,
wTf(y, x, h). At prediction time, we seek a label
y which achieves a high score given the observed x
and the ideal h.
</bodyText>
<equation confidence="0.803635">
CmaxwTf(y, x, h)1 . (1)
h
At training time, we seek weights w which
achieve a high score given all training examples
{x, y}t,
�w� = arg max
w
t
</equation>
<bodyText confidence="0.999983037037037">
We can decompose the feature vector into two
parts: polarity features fpol(y, x, h), and subjectiv-
ity features fsubj(x, h). The basic feature set decom-
poses across sentences, though the polarity features
involve the document-level polarity. For sentence i,
we have fpol(y, xi, hi) = yhixi: the bag-of-words
features for sentence i are multiplied by the docu-
ment polarity y ∈ {−1, 1} and the sentence sub-
jectivity hi ∈ {0, 1}. The weights wpol capture the
sentiment polarity of each possible word. As for the
subjectivity features, we simply have fsubj(xi, hi) =
hixi. The weights wsubj capture the subjectivity of
each word, with large values indicate positive sub-
jectivity.
However, these features do not capture transi-
tions between the subjectivity and sentiment of ad-
jacent sentences. For this reason, Yessenalina et al.
(2010b) introduce an additional set of proximity fea-
tures, fprox(hi, hi−1), which are parametrized by the
subjectivity of both the current sentence i and the
previous sentence i − 1. The effect of these features
will be to learn a preference for consistency in the
subjectivity of adjacent sentences.
By augmenting the transition features with the
text xi, we allow this preference for consistency
to be modulated by discourse connectors. We de-
sign the transition feature vector ftrans(xi, hi, hi−1)
</bodyText>
<equation confidence="0.9937754">
y� = arg max
y
wTf(yt, xt, h). (2)
max
h
</equation>
<page confidence="0.984541">
809
</page>
<bodyText confidence="0.999840571428572">
to contain two elements for every discourse connec-
tor, one for hi = hi−1, and one for hi =� hi−1. For
example, the feature (moreover, CONTINUE) fires
when sentence i starts with moreover and hi−1 =
hi,i. We would expect to learn a positive weight for
this feature, and negative weights for features such
as (moreover, SHIFT) and (however, CONTINUE).
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99991525">
To evaluate the utility of adding discourse connec-
tors to latent subjectivity sentiment analysis, we
compare several models on movie review datasets
in English and Spanish.
</bodyText>
<subsectionHeader confidence="0.996475">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999912">
We use two movie review datasets:
</bodyText>
<listItem confidence="0.729688363636364">
• 50,000 English-language movie reviews (Maas
et al., 2011). Each review has a rating from
1-10; we marked ratings of 5 or greater as pos-
itive. Half the dataset is used for test and half
for training. Parameter tuning is performed by
cross-validation.
• 5,000 Spanish-language movie reviews (Cruz
et al., 2008). Each review has a rating from
1-5; we marked 3-5 as positive. We randomly
created a 60/20/20 split for training, validation,
and test.
</listItem>
<subsectionHeader confidence="0.99809">
3.2 Connectors
</subsectionHeader>
<bodyText confidence="0.9996401875">
We first consider single-word discourse connectors:
in English, we use a list of all 57 one-word con-
nectors from the Penn Discourse Tree Bank (Prasad
et al., 2008); in Spanish, we selected 25 one-word
connectors from a Spanish language education web-
site.2 We also consider multi-word connectors. Us-
ing the same sources, this expands the English set to
93 connectors, and Spanish set to 80 connectors.
In case no list of discourse connectors is avail-
able, we propose a simple technique for automati-
cally identifying potential connectors. We use a x2
test to select words which are especially likely to ini-
tiate sentences. The top K words (with the lowest p
values) were added as potential connectors, where
K is equal to the number of “true” connectors pro-
vided by the gold-standard resource.
</bodyText>
<footnote confidence="0.5688805">
2russell.famaf.unc.edu.ar/-laura/
shallowdisc4summ/discmar/
</footnote>
<bodyText confidence="0.998262333333333">
Finally, we consider a model with connector-
augmented transition features for all words in the
vocabulary. Thus, there are four connector sets:
</bodyText>
<listItem confidence="0.998961">
• true-unigram-connectors: unigram connec-
tors from the Penn Discourse Treebank and the
Spanish language education website
• true-multiword-connectors: unigram and
multiword connectors from these same re-
sources
• auto-unigram-connectors: automatically-
selected connectors using the x2 test
• all-unigram-connectors: all words are poten-
tial connectors
</listItem>
<subsectionHeader confidence="0.991616">
3.3 Systems
</subsectionHeader>
<bodyText confidence="0.998019333333333">
The connector-augmented transition features are in-
corporated into a latent variable support vector ma-
chine (SVM). We also consider two baselines:
</bodyText>
<listItem confidence="0.9881314">
• no-connectors: the same latent variable SVM,
but without the connector features. This is
identical to the prior work of Yessenalina et al.
(2010b).
• SVM: a standard SVM binary classifier
</listItem>
<bodyText confidence="0.9996235">
The latent variable models require an initial guess
for the subjectivity of each sentence. Yessenalina et
al. (2010b) compare several initializations and find
the best results using OpinionFinder (Wilson et al.,
2005). For the Spanish data, we performed initial
subjectivity analysis by matching against a publicly-
available full-strength Spanish lexicon set (Rosas et
al., 2012).
</bodyText>
<subsectionHeader confidence="0.991103">
3.4 Implementation details
</subsectionHeader>
<bodyText confidence="0.999970857142857">
Both our implementation and the baselines are
built on the latent structural SVM (Yu and
Joachims, 2009; http://www.cs.cornell.
edu/-cnyu/latentssvm/), which is in turn
built on the SVM-Light distribution (http://
svmlight.joachims.org/). The regulariza-
tion parameter C was chosen by cross-validation.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999055666666667">
Table 1 shows the sentiment analysis accuracy with
each system and feature set. The best overall re-
sults in both language are given by the models with
</bodyText>
<page confidence="0.983768">
810
</page>
<figure confidence="0.996555565217391">
system
true-multiword-connectors
true-unigram-connectors
auto-connectors
all-unigram-connectors
No-connectors
SVM
English Spanish
91.25 79.80
91.36 77.50
90.22 76.90
87.60 74.30
88.21 76.42
84.79 69.44
English
auto-unigram
all-unigram
no-connectors
SVM
0.84 0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92
sentiment analysis accuracy
0.70 0.75 0.80
sentiment analysis accuracy
</figure>
<figureCaption confidence="0.994532333333333">
Figure 1: Document-level sentiment analysis accuracy.
The 95% confidence intervals are estimated from the cu-
mulative density function of the binomial distribution.
</figureCaption>
<bodyText confidence="0.998323058823529">
connector-augmented transition features. In En-
glish, the multiword and unigram connectors per-
form equally well, and significantly outperform all
alternatives at p &lt; .05. The connector-based fea-
tures reduce the error rate of the latent subjectivity
SVM by 25%. In Spanish, the picture is less clear
because the smaller test set yields larger confidence
intervals, so that only the comparison with the SVM
classifier is significant at p &lt; .05. Nonetheless,
the connector-augmented transition features give the
best accuracy, with an especially large improvement
obtained by the multiword connectors.
Next, we investigated the quality of the
automatically-induced discourse connectors.
The x2 heuristic for selecting candidate connectors
gave results that were significantly better than the
no-connector baseline in English, though the
</bodyText>
<figureCaption confidence="0.9985495">
Figure 2: Precision-Recall curve for top-K discovered
connectors when compared with PDTB connector set
</figureCaption>
<bodyText confidence="0.998626333333334">
difference in Spanish was minimal. However, when
every word is included as a potential connectors, the
performance suffers, dropping below the accuracy
of the no-connector baseline. This shows that the
improvement in accuracy offered by the connector
features is not simply due to the increased flexibility
of the model, but depends on identifying a small set
of likely discourse connectors.
For a qualitative evalatuation, we ranked all
English-language unigram connectors by their fea-
ture weights, and list the top ten for each subjectivity
transition:
</bodyText>
<listItem confidence="0.9960285">
• SHIFT: however; though; but; if; unlike; al-
though; while; overall; nevertheless; still
• CONTINUATION: as; there; now; even; in; af-
ter; once; almost; because; so
</listItem>
<bodyText confidence="0.9998651875">
Overall these word lists cohere with our intu-
itions, particularly the words associated with SHIFT
transitions: however, but, and nevertheless. As one
of the reviewers noted, some of the words associ-
ated with CONTINUATION transitions are better seen
as discourse cues rather than connectors, such as
now. Other words seem to connect two subsequent
clauses, e.g., if Nicholas Cage had played every role,
the film might have reached its potential. Incorporat-
ing such connectors must be left for future work.
Finally, in learning weights for each connector
feature, our model can be seen as discovering dis-
course connectors. We compare the highly weighted
discovered connectors from the all-unigram and
auto-unigram settings with the one-word connec-
tors from the Penn Discourse Tree Bank. The results
</bodyText>
<figure confidence="0.996542888888889">
true-multiword
true-unigram
auto-unigram
all-unigram
no-connectors
SVM
Spanish
true-multiword
true-unigram
</figure>
<page confidence="0.994736">
811
</page>
<bodyText confidence="0.999958866666667">
of this comparison are shown in Figure 2, which
traces a precision-recall curve by taking the top K
connectors for various values of K. The auto-
unigram model is able to identify many true con-
nectors from the Penn Discourse Treebank, while
the all-unigram model achieves low precision. This
graph helps to explain the large performance gap
between the auto-unigram and all-unigram fea-
ture sets; the all-unigram set includes too many
weak features, and the learning algorithm is not able
to distinguish the true discourse connectors. The
Spanish discourse connectors identified by this ap-
proach were extremely poor, possibly because so
many more of the Spanish connectors include mul-
tiple words.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999973444444445">
Polanyi and Zaenen (2006) noted the importance of
accounting for valence shifters in sentiment analy-
sis, identifying relevant connectors at the sentence
and discourse levels. They propose a heuristic ap-
proach to use shifters to modify the contributions
of sentiment words. There have been several sub-
sequent efforts to model within-sentence valence
shifts, including compositional grammar (Moilanen
and Pulman, 2007), matrix-vector products across
the sentence (Yessenalina and Cardie, 2011), and
methods that reason about polarity shifters within
the parse tree (Socher et al., 2012; Sayeed et al.,
2012). The value of discourse structure towards pre-
dicting opinion polarity has also demonstrated in the
context of multi-party dialogues (Somasundaran et
al., 2009). Our approach functions at the discourse
level within single-author documents, so it is com-
plementary to this prior work.
Voll and Taboada (2007) investigate various tech-
niques for focusing sentiment analysis on sentences
that are central to the main topic. They obtain
negative results with the general-purpose SPADE
discourse parser (Soricut and Marcu, 2003), but
find that training a decision tree classifier to iden-
tify topic-central sentences yields positive results.
Wiebe (1994) argues that in coherent narratives, ob-
jectivity and subjectivity are usually consistent be-
tween adjacent sentences, an insight exploited by
Pang and Lee (2004) in a supervised system for
subjectivity analysis. Later work employed struc-
tured graphical models to model the flow of sub-
jectivity and sentiment over the course of the doc-
ument (Mao and Lebanon, 2006; McDonald et al.,
2007). All of these approaches depend on labeled
training examples of subjective and objective sen-
tences, but Yessenalina et al. (2010b) show that sub-
jectivity can be modeled as a latent variable, using a
latent variable version of the structured support vec-
tor machine (Yu and Joachims, 2009).
Our work can be seen as a combination of the
machine learning approach of Yessenalina et al.
(2010b) with the insight of Polanyi and Zaenen
(2006) that connectors play a key role in transitions
between subjectivity and sentiment. Eisenstein and
Barzilay (2008) incorporated discourse connectors
into an unsupervised model of topic segmentation,
but this work only considered the role of such mark-
ers to differentiate adjoining segments of text, and
not to identify their roles with respect to one an-
other. That work was also not capable of learning
from supervised annotations in a downstream task.
In contrast, our approach uses document-level senti-
ment annotations to learn about the role of discourse
connectors in sentence-level subjectivity.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999887533333334">
Latent variable machine learning is a powerful
tool for inducing linguistic structure directly from
data. However, adding a small amount of linguistic
knowledge can substantially improve performance.
We have presented a simple technique for combin-
ing a latent variable support vector machine with
a list of discourse connectors, by creating an aug-
mented feature set that combines the connectors
with pairwise subjectivity transition features. This
improves accuracy, even with a noisy list of connec-
tors that has been identified automatically. Possible
directions for future work include richer representa-
tions of discourse structure, and the combination of
discourse-level and sentence-level valence and sub-
jectivity shifters.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.956100333333333">
Thanks to the anonymous reviewers for their help-
ful feedback. This work was supported by a Google
Faculty Research Award.
</bodyText>
<page confidence="0.995606">
812
</page>
<sectionHeader confidence="0.995162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997132709302326">
Fermin L. Cruz, Jose A. Troyano, Fernando Enriquez,
and Javier Ortega. 2008. Clasificaci´on de documen-
tos basada en la opini´on: experimentos con un cor-
pus de crıticas de cine en espanol. Procesamiento de
Lenguaje Natural, 41.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Tom Huddleston. 2010. Review of The Bad Lieutenant:
Port of Call New Orleans. Time Out, May 18.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3).
Yi Mao and Guy Lebanon. 2006. Isotonic condi-
tional random fields and local sentiment flow. In
B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
ACL.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifters. Computing attitude and affect in text:
Theory and applications.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Veronica Perez Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in spanish. In
Proceedings of LREC.
Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In Proceedings of
NAACL.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP-CoNLL.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
EMNLP.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL.
Kimberly Voll and Maite Taboada. 2007. Not all words
are created equal: Extracting semantic orientation as
a function of adjective relevance. In Proceedings of
Australian Conference on Artificial Intelligence.
Janyce M. Wiebe. 1994. Tracking point of view in nar-
rative. Computational Linguistics, 20(2).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT-EMNLP: Interactive Demonstra-
tions.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP.
Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010a.
Automatically generating annotator rationales to im-
prove sentiment classification. In Proceedings ofACL:
Short Papers.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010b. Multi-Level structured models for Document-
Level sentiment classification. In Proceedings of
EMNLP.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of ICML.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ”annotator rationales” to improve ma-
chine learning for text categorization. In Proceedings
of HLT-NAACL.
</reference>
<page confidence="0.999177">
813
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798258">
<title confidence="0.996966">Discourse Connectors for Latent Subjectivity in Sentiment Analysis</title>
<author confidence="0.908313">Rakshit</author>
<affiliation confidence="0.993346">College of Georgia Institute of</affiliation>
<address confidence="0.931723">Atlanta, GA 30308,</address>
<email confidence="0.999233">rtrivedi6@gatech.edu</email>
<author confidence="0.999968">Jacob Eisenstein</author>
<affiliation confidence="0.999892">School of Interactive Computing Georgia Institute of Technology</affiliation>
<address confidence="0.999886">Atlanta, GA 30308, USA</address>
<email confidence="0.99986">jacobe@gatech.edu</email>
<abstract confidence="0.997788130434783">Document-level sentiment analysis can benefit from fine-grained subjectivity, so that sentiment polarity judgments are based on the relevant parts of the document. While finegrained subjectivity annotations are rarely available, encouraging results have been obtained by modeling subjectivity as a latent variable. However, latent variable models fail to capitalize on our linguistic knowledge about discourse structure. We present a new method for injecting linguistic knowledge into latent variable subjectivity modeling, using connectors. features the latent variable model to learn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fermin L Cruz</author>
<author>Jose A Troyano</author>
<author>Fernando Enriquez</author>
<author>Javier Ortega</author>
</authors>
<title>Clasificaci´on de documentos basada en la opini´on: experimentos con un corpus de crıticas de cine en espanol.</title>
<date>2008</date>
<booktitle>Procesamiento de Lenguaje Natural,</booktitle>
<volume>41</volume>
<contexts>
<context position="8760" citStr="Cruz et al., 2008" startWordPosition="1338" endWordPosition="1341">ative weights for features such as (moreover, SHIFT) and (however, CONTINUE). 3 Experiments To evaluate the utility of adding discourse connectors to latent subjectivity sentiment analysis, we compare several models on movie review datasets in English and Spanish. 3.1 Data We use two movie review datasets: • 50,000 English-language movie reviews (Maas et al., 2011). Each review has a rating from 1-10; we marked ratings of 5 or greater as positive. Half the dataset is used for test and half for training. Parameter tuning is performed by cross-validation. • 5,000 Spanish-language movie reviews (Cruz et al., 2008). Each review has a rating from 1-5; we marked 3-5 as positive. We randomly created a 60/20/20 split for training, validation, and test. 3.2 Connectors We first consider single-word discourse connectors: in English, we use a list of all 57 one-word connectors from the Penn Discourse Tree Bank (Prasad et al., 2008); in Spanish, we selected 25 one-word connectors from a Spanish language education website.2 We also consider multi-word connectors. Using the same sources, this expands the English set to 93 connectors, and Spanish set to 80 connectors. In case no list of discourse connectors is avai</context>
</contexts>
<marker>Cruz, Troyano, Enriquez, Ortega, 2008</marker>
<rawString>Fermin L. Cruz, Jose A. Troyano, Fernando Enriquez, and Javier Ortega. 2008. Clasificaci´on de documentos basada en la opini´on: experimentos con un corpus de crıticas de cine en espanol. Procesamiento de Lenguaje Natural, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17469" citStr="Eisenstein and Barzilay (2008)" startWordPosition="2637" endWordPosition="2640">t over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of text, and not to identify their roles with respect to one another. That work was also not capable of learning from supervised annotations in a downstream task. In contrast, our approach uses document-level sentiment annotations to learn about the role of discourse connectors in sentence-level subjectivity. 6 Conclusion Latent variable machine learning is a powerful tool for inducing linguistic structure directly from </context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Huddleston</author>
</authors>
<title>Review of The Bad Lieutenant: Port of Call New Orleans. Time Out,</title>
<date>2010</date>
<contexts>
<context position="3480" citStr="Huddleston, 2010" startWordPosition="495" endWordPosition="496">weight approach to injecting linguistic knowledge into latent variable models of subjectivity. The entry point is a set of discourse connectors: words and phrases that signal a shift or continuation in the discourse structure. Such connectors have been the subject of extensive study in the creation of the Penn Discourse Treebank (PDTB: Prasad et al. 2008). The role of discourse connectors in sentiment analysis can be clearly seen in examples, such as “It’s hard to imagine the studios hiring another manic German maverick to helm a cop thriller. But that’s exactly why the movie is unmissable.” (Huddleston, 2010) 808 Proceedings of NAACL-HLT 2013, pages 808–813, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics We present a new approach to incorporate discourse connectors in a latent subjectivity model (Yessenalina et al., 2010b). This approach requires no manually-specified information about the meaning of the connectors, just the connectors themselves. Our approach builds on proximity features, which give the latent variable model a way to prefer or disprefer subjectivity and sentiment transitions, usually with the goal of encouraging smoothness across the document. </context>
</contexts>
<marker>Huddleston, 2010</marker>
<rawString>Tom Huddleston. 2010. Review of The Bad Lieutenant: Port of Call New Orleans. Time Out, May 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8509" citStr="Maas et al., 2011" startWordPosition="1295" endWordPosition="1298">ts for every discourse connector, one for hi = hi−1, and one for hi =� hi−1. For example, the feature (moreover, CONTINUE) fires when sentence i starts with moreover and hi−1 = hi,i. We would expect to learn a positive weight for this feature, and negative weights for features such as (moreover, SHIFT) and (however, CONTINUE). 3 Experiments To evaluate the utility of adding discourse connectors to latent subjectivity sentiment analysis, we compare several models on movie review datasets in English and Spanish. 3.1 Data We use two movie review datasets: • 50,000 English-language movie reviews (Maas et al., 2011). Each review has a rating from 1-10; we marked ratings of 5 or greater as positive. Half the dataset is used for test and half for training. Parameter tuning is performed by cross-validation. • 5,000 Spanish-language movie reviews (Cruz et al., 2008). Each review has a rating from 1-5; we marked 3-5 as positive. We randomly created a 60/20/20 split for training, validation, and test. 3.2 Connectors We first consider single-word discourse connectors: in English, we use a list of all 57 one-word connectors from the Penn Discourse Tree Bank (Prasad et al., 2008); in Spanish, we selected 25 one-w</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3).</tech>
<contexts>
<context position="5726" citStr="Mann and Thompson, 1988" startWordPosition="821" endWordPosition="824">ty of each sentence, a document-level sentiment analyzer could safely ignore the sentences marked as nonsubjective.1 This would be beneficial for training as well as prediction, because the learning algorithm would not be confused by sentences that contradict the document label. But in general we cannot rely on having access to sentence-level subjectivity annotations. Instead, we treat subjectivity as a latent variable, and ask the learner to impute its value. Given document-level sentiment annotations and an initial 1Discourse parsing often focuses on sub-sentence elementary discourse units (Mann and Thompson, 1988). For simplicity, we consider units at the sentence level only, and leave finer-grained analysis for future work. model, the learner can mark as non-subjective those sentences whose analysis disagrees with the document label. More formally, each document has a label y ∈ {−1,1}, a set of sentences x, and a set of persentence subjectivity judgments h ∈ {0,1}S, where S is the number of sentences. We compute a set of features on these variables, and score each instance by a weighted combination of the features, wTf(y, x, h). At prediction time, we seek a label y which achieves a high score given t</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Isotonic conditional random fields and local sentiment flow.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 19.</booktitle>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<contexts>
<context position="16895" citStr="Mao and Lebanon, 2006" startWordPosition="2544" endWordPosition="2547">t are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse co</context>
</contexts>
<marker>Mao, Lebanon, 2006</marker>
<rawString>Yi Mao and Guy Lebanon. 2006. Isotonic conditional random fields and local sentiment flow. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16919" citStr="McDonald et al., 2007" startWordPosition="2548" endWordPosition="2551">in topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsuper</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="15699" citStr="Moilanen and Pulman, 2007" startWordPosition="2363" endWordPosition="2366">h the true discourse connectors. The Spanish discourse connectors identified by this approach were extremely poor, possibly because so many more of the Spanish connectors include multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment composition. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16694" citStr="Pang and Lee (2004)" startWordPosition="2510" endWordPosition="2513"> the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual valence shifters. Computing attitude and affect in text: Theory and applications.</title>
<date>2006</date>
<contexts>
<context position="15308" citStr="Polanyi and Zaenen (2006)" startWordPosition="2307" endWordPosition="2310">s for various values of K. The autounigram model is able to identify many true connectors from the Penn Discourse Treebank, while the all-unigram model achieves low precision. This graph helps to explain the large performance gap between the auto-unigram and all-unigram feature sets; the all-unigram set includes too many weak features, and the learning algorithm is not able to distinguish the true discourse connectors. The Spanish discourse connectors identified by this approach were extremely poor, possibly because so many more of the Spanish connectors include multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of disco</context>
<context position="17355" citStr="Polanyi and Zaenen (2006)" startWordPosition="2621" endWordPosition="2624">vity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of text, and not to identify their roles with respect to one another. That work was also not capable of learning from supervised annotations in a downstream task. In contrast, our approach uses document-level sentiment annotations to learn about the role of discourse connectors in sentence-level subjectivity.</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2006. Contextual valence shifters. Computing attitude and affect in text: Theory and applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="3220" citStr="Prasad et al. 2008" startWordPosition="451" endWordPosition="454">tivity annotations nor an accurate domain-independent discourse parser. But while the “knowledge-free” nature of this approach is appealing, it is unsatisfying that it fails to exploit decades of research on discourse structure. In this paper, we explore a lightweight approach to injecting linguistic knowledge into latent variable models of subjectivity. The entry point is a set of discourse connectors: words and phrases that signal a shift or continuation in the discourse structure. Such connectors have been the subject of extensive study in the creation of the Penn Discourse Treebank (PDTB: Prasad et al. 2008). The role of discourse connectors in sentiment analysis can be clearly seen in examples, such as “It’s hard to imagine the studios hiring another manic German maverick to helm a cop thriller. But that’s exactly why the movie is unmissable.” (Huddleston, 2010) 808 Proceedings of NAACL-HLT 2013, pages 808–813, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics We present a new approach to incorporate discourse connectors in a latent subjectivity model (Yessenalina et al., 2010b). This approach requires no manually-specified information about the meaning of the co</context>
<context position="9075" citStr="Prasad et al., 2008" startWordPosition="1391" endWordPosition="1394">000 English-language movie reviews (Maas et al., 2011). Each review has a rating from 1-10; we marked ratings of 5 or greater as positive. Half the dataset is used for test and half for training. Parameter tuning is performed by cross-validation. • 5,000 Spanish-language movie reviews (Cruz et al., 2008). Each review has a rating from 1-5; we marked 3-5 as positive. We randomly created a 60/20/20 split for training, validation, and test. 3.2 Connectors We first consider single-word discourse connectors: in English, we use a list of all 57 one-word connectors from the Penn Discourse Tree Bank (Prasad et al., 2008); in Spanish, we selected 25 one-word connectors from a Spanish language education website.2 We also consider multi-word connectors. Using the same sources, this expands the English set to 93 connectors, and Spanish set to 80 connectors. In case no list of discourse connectors is available, we propose a simple technique for automatically identifying potential connectors. We use a x2 test to select words which are especially likely to initiate sentences. The top K words (with the lowest p values) were added as potential connectors, where K is equal to the number of “true” connectors provided by</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronica Perez Rosas</author>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning sentiment lexicons in spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="10982" citStr="Rosas et al., 2012" startWordPosition="1673" endWordPosition="1676">tor machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/-cnyu/latentssvm/), which is in turn built on the SVM-Light distribution (http:// svmlight.joachims.org/). The regularization parameter C was chosen by cross-validation. 4 Results Table 1 shows the sentiment analysis accuracy with each system and feature set. The best overall results in both language are given by the models with 810 system true-multiword-connectors true-unigram-connectors auto-connectors all-unigram-connectors No-conne</context>
</contexts>
<marker>Rosas, Banea, Mihalcea, 2012</marker>
<rawString>Veronica Perez Rosas, Carmen Banea, and Rada Mihalcea. 2012. Learning sentiment lexicons in spanish. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asad B Sayeed</author>
<author>Jordan Boyd-Graber</author>
<author>Bryan Rusk</author>
<author>Amy Weinberg</author>
</authors>
<title>Grammatical structures for word-level sentiment detection.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="15888" citStr="Sayeed et al., 2012" startWordPosition="2391" endWordPosition="2394">5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central s</context>
</contexts>
<marker>Sayeed, Boyd-Graber, Rusk, Weinberg, 2012</marker>
<rawString>Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy Weinberg. 2012. Grammatical structures for word-level sentiment detection. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="15866" citStr="Socher et al., 2012" startWordPosition="2387" endWordPosition="2390">lude multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to id</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16048" citStr="Somasundaran et al., 2009" startWordPosition="2414" endWordPosition="2417"> the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentence</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1898" citStr="Soricut and Marcu, 2003" startWordPosition="251" endWordPosition="254">oduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b) treat sentence level subjectivity as a latent variable, automatically inducing the “annotator rationale” (Zaidan et al., 2007; Yessenalina et al., 2010a) for each training sentence so as to focus sentiment learning on the subjective parts of the document. This yields significant improvements over bag-of-ngrams supervised sentiment</context>
<context position="16409" citStr="Soricut and Marcu, 2003" startWordPosition="2468" endWordPosition="2471">ds that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective </context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimberly Voll</author>
<author>Maite Taboada</author>
</authors>
<title>Not all words are created equal: Extracting semantic orientation as a function of adjective relevance.</title>
<date>2007</date>
<booktitle>In Proceedings of Australian Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1395" citStr="Voll and Taboada (2007)" startWordPosition="180" endWordPosition="183">nguistic knowledge into latent variable subjectivity modeling, using discourse connectors. Connector-augmented transition features allow the latent variable model to learn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available. 1 Introduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific mo</context>
<context position="16194" citStr="Voll and Taboada (2007)" startWordPosition="2437" endWordPosition="2440"> several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models t</context>
</contexts>
<marker>Voll, Taboada, 2007</marker>
<rawString>Kimberly Voll and Maite Taboada. 2007. Not all words are created equal: Extracting semantic orientation as a function of adjective relevance. In Proceedings of Australian Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="16534" citStr="Wiebe (1994)" startWordPosition="2488" endWordPosition="2489"> towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Janyce M. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP: Interactive Demonstrations.</booktitle>
<contexts>
<context position="10821" citStr="Wilson et al., 2005" startWordPosition="1650" endWordPosition="1653">unigram-connectors: all words are potential connectors 3.3 Systems The connector-augmented transition features are incorporated into a latent variable support vector machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/-cnyu/latentssvm/), which is in turn built on the SVM-Light distribution (http:// svmlight.joachims.org/). The regularization parameter C was chosen by cross-validation. 4 Results Table 1 shows the sentiment analysis accuracy with each system and feature set. The best overall r</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceedings of HLT-EMNLP: Interactive Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15774" citStr="Yessenalina and Cardie, 2011" startWordPosition="2372" endWordPosition="2375">fied by this approach were extremely poor, possibly because so many more of the Spanish connectors include multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discour</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Automatically generating annotator rationales to improve sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL: Short Papers.</booktitle>
<contexts>
<context position="1564" citStr="Yessenalina et al. (2010" startWordPosition="203" endWordPosition="206">arn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available. 1 Introduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b</context>
<context position="3732" citStr="Yessenalina et al., 2010" startWordPosition="528" endWordPosition="531">ve been the subject of extensive study in the creation of the Penn Discourse Treebank (PDTB: Prasad et al. 2008). The role of discourse connectors in sentiment analysis can be clearly seen in examples, such as “It’s hard to imagine the studios hiring another manic German maverick to helm a cop thriller. But that’s exactly why the movie is unmissable.” (Huddleston, 2010) 808 Proceedings of NAACL-HLT 2013, pages 808–813, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics We present a new approach to incorporate discourse connectors in a latent subjectivity model (Yessenalina et al., 2010b). This approach requires no manually-specified information about the meaning of the connectors, just the connectors themselves. Our approach builds on proximity features, which give the latent variable model a way to prefer or disprefer subjectivity and sentiment transitions, usually with the goal of encouraging smoothness across the document. By taking the cross-product of these features with a set of discourse connectors, we obtain a new set of connector-augmented transition features, which capture the way discourse connectors are used to indicate subjectivity and sentiment transitions. Th</context>
<context position="7330" citStr="Yessenalina et al. (2010" startWordPosition="1099" endWordPosition="1102">nvolve the document-level polarity. For sentence i, we have fpol(y, xi, hi) = yhixi: the bag-of-words features for sentence i are multiplied by the document polarity y ∈ {−1, 1} and the sentence subjectivity hi ∈ {0, 1}. The weights wpol capture the sentiment polarity of each possible word. As for the subjectivity features, we simply have fsubj(xi, hi) = hixi. The weights wsubj capture the subjectivity of each word, with large values indicate positive subjectivity. However, these features do not capture transitions between the subjectivity and sentiment of adjacent sentences. For this reason, Yessenalina et al. (2010b) introduce an additional set of proximity features, fprox(hi, hi−1), which are parametrized by the subjectivity of both the current sentence i and the previous sentence i − 1. The effect of these features will be to learn a preference for consistency in the subjectivity of adjacent sentences. By augmenting the transition features with the text xi, we allow this preference for consistency to be modulated by discourse connectors. We design the transition feature vector ftrans(xi, hi, hi−1) y� = arg max y wTf(yt, xt, h). (2) max h 809 to contain two elements for every discourse connector, one f</context>
<context position="10560" citStr="Yessenalina et al. (2010" startWordPosition="1611" endWordPosition="1614">igram connectors from the Penn Discourse Treebank and the Spanish language education website • true-multiword-connectors: unigram and multiword connectors from these same resources • auto-unigram-connectors: automaticallyselected connectors using the x2 test • all-unigram-connectors: all words are potential connectors 3.3 Systems The connector-augmented transition features are incorporated into a latent variable support vector machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/-cnyu/latentssvm/</context>
<context position="17048" citStr="Yessenalina et al. (2010" startWordPosition="2568" endWordPosition="2571"> training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of </context>
</contexts>
<marker>Yessenalina, Choi, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010a. Automatically generating annotator rationales to improve sentiment classification. In Proceedings ofACL: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-Level structured models for DocumentLevel sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1564" citStr="Yessenalina et al. (2010" startWordPosition="203" endWordPosition="206">arn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available. 1 Introduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b</context>
<context position="3732" citStr="Yessenalina et al., 2010" startWordPosition="528" endWordPosition="531">ve been the subject of extensive study in the creation of the Penn Discourse Treebank (PDTB: Prasad et al. 2008). The role of discourse connectors in sentiment analysis can be clearly seen in examples, such as “It’s hard to imagine the studios hiring another manic German maverick to helm a cop thriller. But that’s exactly why the movie is unmissable.” (Huddleston, 2010) 808 Proceedings of NAACL-HLT 2013, pages 808–813, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics We present a new approach to incorporate discourse connectors in a latent subjectivity model (Yessenalina et al., 2010b). This approach requires no manually-specified information about the meaning of the connectors, just the connectors themselves. Our approach builds on proximity features, which give the latent variable model a way to prefer or disprefer subjectivity and sentiment transitions, usually with the goal of encouraging smoothness across the document. By taking the cross-product of these features with a set of discourse connectors, we obtain a new set of connector-augmented transition features, which capture the way discourse connectors are used to indicate subjectivity and sentiment transitions. Th</context>
<context position="7330" citStr="Yessenalina et al. (2010" startWordPosition="1099" endWordPosition="1102">nvolve the document-level polarity. For sentence i, we have fpol(y, xi, hi) = yhixi: the bag-of-words features for sentence i are multiplied by the document polarity y ∈ {−1, 1} and the sentence subjectivity hi ∈ {0, 1}. The weights wpol capture the sentiment polarity of each possible word. As for the subjectivity features, we simply have fsubj(xi, hi) = hixi. The weights wsubj capture the subjectivity of each word, with large values indicate positive subjectivity. However, these features do not capture transitions between the subjectivity and sentiment of adjacent sentences. For this reason, Yessenalina et al. (2010b) introduce an additional set of proximity features, fprox(hi, hi−1), which are parametrized by the subjectivity of both the current sentence i and the previous sentence i − 1. The effect of these features will be to learn a preference for consistency in the subjectivity of adjacent sentences. By augmenting the transition features with the text xi, we allow this preference for consistency to be modulated by discourse connectors. We design the transition feature vector ftrans(xi, hi, hi−1) y� = arg max y wTf(yt, xt, h). (2) max h 809 to contain two elements for every discourse connector, one f</context>
<context position="10560" citStr="Yessenalina et al. (2010" startWordPosition="1611" endWordPosition="1614">igram connectors from the Penn Discourse Treebank and the Spanish language education website • true-multiword-connectors: unigram and multiword connectors from these same resources • auto-unigram-connectors: automaticallyselected connectors using the x2 test • all-unigram-connectors: all words are potential connectors 3.3 Systems The connector-augmented transition features are incorporated into a latent variable support vector machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/-cnyu/latentssvm/</context>
<context position="17048" citStr="Yessenalina et al. (2010" startWordPosition="2568" endWordPosition="2571"> training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of </context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010b. Multi-Level structured models for DocumentLevel sentiment classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural svms with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="11114" citStr="Yu and Joachims, 2009" startWordPosition="1693" endWordPosition="1696">es. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/-cnyu/latentssvm/), which is in turn built on the SVM-Light distribution (http:// svmlight.joachims.org/). The regularization parameter C was chosen by cross-validation. 4 Results Table 1 shows the sentiment analysis accuracy with each system and feature set. The best overall results in both language are given by the models with 810 system true-multiword-connectors true-unigram-connectors auto-connectors all-unigram-connectors No-connectors SVM English Spanish 91.25 79.80 91.36 77.50 90.22 76.90 87.60 74.30 88.21 76.42 84.79 69.44 English auto-unigram all-unigram n</context>
<context position="17207" citStr="Yu and Joachims, 2009" startWordPosition="2595" endWordPosition="2598">d subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed structured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of text, and not to identify their roles with respect to one another. That work was also not capable of learning from supervised annotations in a downstream task.</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural svms with latent variables. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using ”annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2291" citStr="Zaidan et al., 2007" startWordPosition="310" endWordPosition="313"> expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b) treat sentence level subjectivity as a latent variable, automatically inducing the “annotator rationale” (Zaidan et al., 2007; Yessenalina et al., 2010a) for each training sentence so as to focus sentiment learning on the subjective parts of the document. This yields significant improvements over bag-of-ngrams supervised sentiment classification. Latent variable subjectivity analysis is attractive because it requires neither subjectivity annotations nor an accurate domain-independent discourse parser. But while the “knowledge-free” nature of this approach is appealing, it is unsatisfying that it fails to exploit decades of research on discourse structure. In this paper, we explore a lightweight approach to injecting</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ”annotator rationales” to improve machine learning for text categorization. In Proceedings of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>