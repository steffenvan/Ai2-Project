<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002848">
<title confidence="0.998892">
Deepfix: Statistical Post-editing of Statistical Machine Translation Using
Deep Syntactic Analysis
</title>
<author confidence="0.997116">
Rudolf Rosa and David Mareˇcek and Aleˇs Tamchyna
</author>
<affiliation confidence="0.9946925">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.668097">
Malostransk´e n´amˇest´ı 25, Prague
</address>
<email confidence="0.998983">
{rosa,marecek,tamchyna}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977">
Deepfix is a statistical post-editing sys-
tem for improving the quality of statis-
tical machine translation outputs. It at-
tempts to correct errors in verb-noun va-
lency using deep syntactic analysis and a
simple probabilistic model of valency. On
the English-to-Czech translation pair, we
show that statistical post-editing of statis-
tical machine translation leads to an im-
provement of the translation quality when
helped by deep linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99882509375">
Statistical machine translation (SMT) is the cur-
rent state-of-the-art approach to machine transla-
tion – see e.g. Callison-Burch et al. (2011). How-
ever, its outputs are still typically significantly
worse than human translations, containing vari-
ous types of errors (Bojar, 2011b), both in lexical
choices and in grammar.
As shown by many researchers, e.g. Bojar
(2011a), incorporating deep linguistic knowledge
directly into a translation system is often hard to
do, and seldom leads to an improvement of trans-
lation output quality. It has been shown that it is
often easier to correct the machine translation out-
puts in a second-stage post-processing, which is
usually referred to as automatic post-editing.
Several types of errors can be fixed by employ-
ing rule-based post-editing (Rosa et al., 2012b),
which can be seen as being orthogonal to the sta-
tistical methods employed in SMT and thus can
capture different linguistic phenomena easily.
But there are still other errors that cannot be cor-
rected with hand-written rules, as there exist many
linguistic phenomena that can never be fully de-
scribed manually – they need to be handled statis-
tically by automatically analyzing large-scale text
corpora. However, to the best of our knowledge,
English Czech
go to the doctor j´ıt k doktorovi dative case
go to the centre j´ıt do centra genitive case
go to a concert j´ıt na koncert accusative case
go for a drink j´ıt na drink accusative case
go up the hill j´ıt na kopec accusative case
</bodyText>
<tableCaption confidence="0.78997">
Table 1: Examples of valency of the verb ‘to go’
and ‘j´ıt’. For Czech, the morphological cases of
the nouns are also indicated.
</tableCaption>
<table confidence="0.968297625">
Source: The government spends on the middle
schools.
Moses: Vl´ada utr´acistˇredn´ı ˇskoly.
Meaning: The government destroys the middle
schools.
Reference: Vl´ada utr´aciza stˇredn´ı ˇskoly.
Meaning: The government spends on the middle
schools.
</table>
<tableCaption confidence="0.8853045">
Table 2: Example of a valency error in output of
Moses SMT system.
</tableCaption>
<bodyText confidence="0.999314">
there is very little successful research in statistical
post-editing (SPE) of SMT (see Section 2).
In our paper, we describe a statistical approach
to correcting one particular type of English-to-
Czech SMT errors – errors in the verb-noun va-
lency. The term valency stands for the way in
which verbs and their arguments are used together,
usually together with prepositions and morpholog-
ical cases, and is described in Section 4. Several
examples of the valency of the English verb ‘to go’
and the corresponding Czech verb ‘j´ıt’ are shown
in Table 1.
We conducted our experiments using a state-of-
the-art SMT system Moses (Koehn et al., 2007).
An example of Moses making a valency error is
translating the sentence ‘The government spends
on the middle schools.’, adapted from our devel-
opment data set. As shown in Table 2, Moses
translates the sentence incorrectly, making an er-
ror in the valency of the ‘utr´acet – ˇskola’ (‘spend –
school’) pair. The missing preposition changes the
meaning dramatically, as the verb ‘utr´acet’ is pol-
</bodyText>
<page confidence="0.964323">
172
</page>
<note confidence="0.896956">
Proceedings of the ACL Student Research Workshop, pages 172–179,
</note>
<page confidence="0.440774">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.9998794">
ysemous and can mean ‘to spend (esp. money)’ as
well as ‘to kill, to destroy (esp. animals)’.
Our approach is to use deep linguistic analysis
to automatically determine the structure of each
sentence, and to detect and correct valency errors
using a simple statistical valency model. We de-
scribe our approach in detail in Section 5.
We evaluate and discuss our experiments in
Section 6. We then conclude the paper and pro-
pose areas to be researched in future in Section 7.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999850794871795">
The first reported results of automatic post-editing
of machine translation outputs are (Simard et al.,
2007) where the authors successfully performed
statistical post-editing (SPE) of rule-based ma-
chine translation outputs. To perform the post-
editing, they used a phrase-based SMT system in a
monolingual setting, trained on the outputs of the
rule-based system as the source and the human-
provided reference translations as the target, to
achieve massive translation quality improvements.
The authors also compared the performance of the
post-edited rule-based system to directly using the
SMT system in a bilingual setting, and reported
that the SMT system alone performed worse than
the post-edited rule-based system. They then tried
to post-edit the bilingual SMT system with another
monolingual instance of the same SMT system,
but concluded that no improvement in quality was
observed.
The first known positive results in SPE of SMT
are reported by Oflazer and El-Kahlout (2007)
on English to Turkish machine translation. The
authors followed a similar approach to Simard
et al. (2007), training an SMT system to post-
edit its own output. They use two iterations of
post-editing to get an improvement of 0.47 BLEU
points (Papineni et al., 2002). The authors used
a rather small training set and do not discuss the
scalability of their approach.
To the best of our knowledge, the best results re-
ported so far for SPE of SMT are by B´echara et al.
(2011) on French-to-English translation. The au-
thors start by using a similar approach to Oflazer
and El-Kahlout (2007), getting a statistically sig-
nificant improvement of 0.65 BLEU points. They
then further improve the performance of their
system by adding information from the source
side into the post-editing system by concatenat-
ing some of the translated words with their source
</bodyText>
<table confidence="0.974856">
Direction Baseline SPE Context SPE
en→cs 10.85±0.47 10.70±0.44 10.73±0.49
cs→en 17.20±0.53 17.11±0.52 17.18±0.54
</table>
<tableCaption confidence="0.8893415">
Table 3: Results of SPE approach of B´echara et al.
(2011) evaluated on English-Czech SMT.
</tableCaption>
<bodyText confidence="0.999092857142857">
words, eventually reaching an improvement of
2.29 BLEU points. However, similarly to Oflazer
and El-Kahlout (2007), the training data used are
very small, and it is not clear how their method
scales on larger training data.
In our previous work (Rosa et al., 2012b), we
explored a related but substantially different area
of rule-based post-editing of SMT. The resulting
system, Depfix, manages to significantly improve
the quality of several SMT systems outputs, using
a set of hand-written rules that detect and correct
grammatical errors, such as agreement violations.
Depfix can be easily combined with Deepfix,1 as
it is able to correct different types of errors.
</bodyText>
<sectionHeader confidence="0.986549" genericHeader="method">
3 Evaluation of Existing SPE
Approaches
</sectionHeader>
<bodyText confidence="0.99998325">
First, we evaluated the utility of the approach of
B´echara et al. (2011) for the English-Czech lan-
guage pair. We used 1 million sentence pairs from
CzEng 1.0 (Bojar et al., 2012b), a large English-
Czech parallel corpus. Identically to the paper, we
split the training data into 10 parts, trained 10 sys-
tems (each on nine tenths of the data) and used
them to translate the remaining part. The second
step was then trained on the concatenation of these
translations and the target side of CzEng. We also
implemented the contextual variant of SPE where
words in the intermediate language are annotated
with corresponding source words if the alignment
strength is greater than a given threshold. We lim-
ited ourselves to the threshold value 0.8, for which
the best results are reported in the paper. We tuned
all systems on the dataset of WMT11 (Callison-
Burch et al., 2011) and evaluated on the WMT12
dataset (Callison-Burch et al., 2012).
Table 3 summarizes our results. The reported
confidence intervals were estimated using boot-
strap resampling (Koehn, 2004). SPE did not lead
to any improvements of BLEU in our experiments.
In fact, SPE even slightly decreased the score (but
</bodyText>
<footnote confidence="0.7863375">
1Depfix (Rosa et al., 2012b) performs rule-based post-
editing on shallow-syntax dependency trees, while Deepfix
(described in this paper) is a statistical post-editing system
operating on deep-syntax dependency trees.
</footnote>
<page confidence="0.9978">
173
</page>
<bodyText confidence="0.9998005">
the difference is statistically insignificant in all
cases).
We conclude that this method does not improve
English-Czech translation, possibly because our
training data is too large for this method to bring
any benefit. We therefore proceed with a more
complex approach which relies on deep linguistic
knowledge.
</bodyText>
<sectionHeader confidence="0.9092945" genericHeader="method">
4 Deep Dependency Syntax, Formemes,
and Valency
</sectionHeader>
<subsectionHeader confidence="0.999891">
4.1 Tectogrammatical dependency trees
</subsectionHeader>
<bodyText confidence="0.9979025">
Tectogrammatical trees are deep syntactic depen-
dency trees based on the Functional Generative
Description (Sgall et al., 1986). Each node in
a tectogrammatical tree corresponds to a content
word, such as a noun, a full verb or an adjec-
tive; the node consists of the lemma of the con-
tent word and several other attributes. Functional
words, such as prepositions or auxiliary verbs, are
not directly present in the tectogrammatical tree,
but are represented by attributes of the respective
content nodes. See Figure 1 for an example of two
tectogrammatical trees (for simplicity, most of the
attributes are not shown).
In our work, we only use one of the
many attributes of tectogrammatical nodes, called
formeme (Duˇsek et al., 2012). A formeme is a
string representation of selected morpho-syntactic
features of the content word and selected auxiliary
words that belong to the content word, devised to
be used as a simple and efficient representation of
the node.
A noun formeme, which we are most interested
in, consists of three parts (examples taken from
Figure 1):
</bodyText>
<listItem confidence="0.992059384615385">
1. The syntactic part-of-speech – n for nouns.
2. The preposition if the noun has one (empty
otherwise), as in n:on+X or n:za+4.
3. A form specifier.
• In English, it typically marks the subject
or object, as in n:subj. In case of a
noun accompanied by a preposition, the
third part is always X, as in n:on+X.
• In Czech, it denotes the morphologi-
cal case of the noun, represented by
its number (from 1 to 7 as there are
seven cases in Czech), as in n:1 and
n:za+4.
</listItem>
<figureCaption confidence="0.8539015">
Figure 1: Tectogrammatical trees for the sentence
‘The government spends on the middle schools.’ –
‘Vl´ada utr´aciza stˇredniˇskoly.’; only lemmas and
formemes of the nodes are shown.
</figureCaption>
<bodyText confidence="0.999570166666667">
Adjectives and nouns can also have the
adj:attr and n:attr formemes, respectively,
meaning that the node is in morphological agree-
ment with its parent. This is especially important
in Czech, where this means that the word bears the
same morphological case as its parent node.
</bodyText>
<subsectionHeader confidence="0.97907">
4.2 Valency
</subsectionHeader>
<bodyText confidence="0.999927222222222">
The notion of valency (Tesni`ere and Fourquet,
1959) is semantic, but it is closely linked to syn-
tax. In the theory of valency, each verb has one
or more valency frames. Each valency frame de-
scribes a meaning of the verb, together with argu-
ments (usually nouns) that the verb must or can
have, and each of the arguments has one or several
fixed forms in which it must appear. These forms
can typically be specified by prepositions and mor-
phological cases to be used with the noun, and thus
can be easily expressed by formemes.
For example, the verb ‘to go’, shown in Ta-
ble 1, has a valency frame that can be expressed
as n:subj go n:to+X, meaning that the sub-
ject goes to some place.
The valency frames of the verbs ‘spend’
and ‘utr´acet’ in Figure 1 can be written as
n:subj spend n:on+X and n:1 utr´acet
n:za+4; the subject (in Czech this is a noun in
nominative case) spends on an object (in Czech,
the preposition ‘za’ plus a noun in accusative
case).
In our work, we have extended our scope also
to noun-noun valency, i.e. the parent node can be
either a verb or a noun, while the arguments are al-
ways nouns. Practice has proven this extension to
be useful, although the majority of the corrections
</bodyText>
<figure confidence="0.9969856">
t-tree
zone=en
t-tree
zone=cs
middle
adj:attr
střední
adj:attr
spend
v:fin
utrácet
v:fin
government
n:subj
school
n:on+X
škola
n:za+4
vláda
n:1
</figure>
<page confidence="0.849557">
174
</page>
<bodyText confidence="0.8287845">
1. the sentences are tokenized, tagged and lem-
matized (a lemma and a morphological tag is
assigned to each word)
performed are still of the verb-noun valency type.
Still, we keep the traditional notion of verb-noun
valency throughout the text, especially to be able
to always refer to the parent as “the verb” and to
the child as “the noun”.
2. corresponding English and Czech words are
aligned based on their lemmas
</bodyText>
<sectionHeader confidence="0.977625" genericHeader="method">
5 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.969549">
5.1 Valency models
</subsectionHeader>
<bodyText confidence="0.9988375">
To be able to detect and correct valency errors, we
created statistical models of verb-noun valency.
We model the conditional probability of the noun
argument formeme based on several features of the
verb-noun pair. We decided to use the following
two models:
</bodyText>
<equation confidence="0.9997385">
P(fn|lv, fEN) (1)
P(fn|lv, ln, fEN) (2)
</equation>
<bodyText confidence="0.692496">
where:
</bodyText>
<listItem confidence="0.993448">
• fn is the formeme of the Czech noun argu-
ment, which is being modelled
• lv is the lemma of the Czech parent verb
• ln is the lemma of the Czech noun argument
• fEN is the formeme of the English noun
aligned to the Czech noun argument
</listItem>
<bodyText confidence="0.999642823529412">
The input is first processed by the model (1),
which performs more general fixes, in situations
where the (lv, fEN) pair rather unambiguously de-
fines the valency frame required.
Then model (2) is applied, correcting some er-
rors of the model (1), in cases where the noun
argument requires a different valency frame than
is usual for the (lv, fEN) pair, and making some
more fixes in cases where the correct valency
frame required for the (lv, fEN) pair was too am-
biguous to make a correction according to model
(1), but the decision can be made once information
about ln is added.
We computed the models on the full training set
of CzEng 1.0 (Bojar et al., 2012b) (roughly 15 mil-
lion sentences), and smoothed the estimated prob-
abilities with add-one smoothing.
</bodyText>
<subsectionHeader confidence="0.969873">
5.2 Deepfix
</subsectionHeader>
<bodyText confidence="0.998789">
We introduce a new statistical post-editing system,
Deepfix, whose input is a pair of an English sen-
tence and its Czech machine translation, and the
output is the Czech sentence with verb-noun va-
lency errors corrected.
The Deepfix pipeline consists of several steps:
</bodyText>
<listItem confidence="0.9783837">
3. deep-syntax dependency parse trees of the
sentences are built, the nodes in the trees are
labelled with formemes
4. improbable noun formemes are replaced with
correct formemes according to the valency
model
5. the words are regenerated according to the
new formemes
6. the regenerating continues recursively to chil-
dren of regenerated nodes if they are in
</listItem>
<bodyText confidence="0.946212416666667">
morphological agreement with their parents
(which is typical for adjectives)
To decide whether the formeme of the noun is
incorrect, we query the valency model for all pos-
sible formemes and their probabilities. If an alter-
native formeme probability exceeds a fixed thresh-
old, we assume that the original formeme is incor-
rect, and we use the alternative formeme instead.
For our example sentence, ‘The government
spends on the middle schools.’ – ‘Vl´ada utr´aciza
stˇredniˇskoly.’, we query the model (2) and get the
following probabilities:
</bodyText>
<listItem confidence="0.99641625">
• P(n:4  |utr´acet, ˇskola, n:on+X) = 0.07
(the original formeme)
• P(n:za+4  |utr´acet, ˇskola, n:on+X) = 0.89
(the most probable formeme)
</listItem>
<bodyText confidence="0.99987875">
The threshold for this change type is 0.86, is
exceeded by the n:za+4 formeme and thus the
change is performed: ‘ˇskoly’ is replaced by ‘za
ˇskoly’.
</bodyText>
<subsectionHeader confidence="0.997429">
5.3 Tuning the Thresholds
</subsectionHeader>
<bodyText confidence="0.999991">
We set the thresholds differently for different types
of changes. The values of the thresholds that we
used are listed in Table 4 and were estimated man-
ually. We distinguish changes where only the
morphological case of the noun is changed from
changes to the preposition. There are three possi-
ble types of a change to a preposition: switching
one preposition to another, adding a new preposi-
tion, and removing an existing preposition. The
</bodyText>
<page confidence="0.996466">
175
</page>
<table confidence="0.999754833333333">
Correction type Thresholds for models
(1) (2)
Changing the noun case only 0.55 0.78
Changing the preposition 0.90 0.84
Adding a new preposition – 0.86
Removing the preposition – –
</table>
<tableCaption confidence="0.997795">
Table 4: Deepfix thresholds
</tableCaption>
<bodyText confidence="0.999956421052632">
change to the preposition can also involve chang-
ing the morphological case of the noun, as each
preposition typically requires a certain morpho-
logical case.
For some combinations of a change type and a
model, as in case of the preposition removing, we
never perform a fix because we observed that it
nearly never improves the translation. E.g., if a
verb-noun pair can be correct both with and with-
out a preposition, the preposition-less variant is
usually much more frequent than the prepositional
variant (and thus is assigned a much higher prob-
ability by the model). However, the preposition
often bears a meaning that is lost by removing it
– in Czech, which is a relatively free-word-order
language, the semantic roles of verb arguments
are typically distinguished by prepositions, as op-
posed to English, where they can be determined
by their relative position to the verb.
</bodyText>
<subsectionHeader confidence="0.979426">
5.4 Implementation
</subsectionHeader>
<bodyText confidence="0.999708583333333">
The whole Deepfix pipeline is implemented in
Treex, a modular NLP framework (Popel and
ˇZabokrtsk´y, 2010) written in Perl, which provides
wrappers for many state-of-the-art NLP tools. For
the analysis of the English sentence, we use the
Morˇce tagger (Spoustov´a et al., 2007) and the
MST parser (McDonald et al., 2005). The Czech
sentence is analyzed by the Featurama tagger2 and
the RUR parser (Rosa et al., 2012a) – a parser
adapted to parsing of SMT outputs. The word
alignment is created by GIZA++ (Och and Ney,
2003); the intersection symmetrization is used.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999748">
6.1 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.996981">
We evaluated our method on three datasets:
WMT10 (2489 parallel sentences), WMT11 (3003
parallel sentences), and WMT12 (3003 parallel
sentences) by Callison-Burch et al. (2010; 2011;
2012). For evaluation, we used outputs of a
state-of-the-art SMT system, Moses (Koehn et al.,
</bodyText>
<footnote confidence="0.827529">
2http://featurama.sourceforge.net/
</footnote>
<bodyText confidence="0.996671785714286">
2007), tuned for English-to-Czech translation (Bo-
jar et al., 2012a). We used the WMT10 dataset
and its Moses translation as our development data
to tune the thresholds. In Table 5, we report the
achieved BLEU scores (Papineni et al., 2002),
NIST scores (Doddington, 2002), and PER (Till-
mann et al., 1997).
The improvements in automatic scores are low
but consistently positive, which suggests that
Deepfix does improve the translation quality.
However, the changes performed by Deepfix are
so small that automatic evaluation is unable to re-
liably assess whether they are positive or negative
– it can only be taken as an indication.
</bodyText>
<subsectionHeader confidence="0.999675">
6.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.99992148">
To reliably assess the performance of Deepfix,
we performed manual evaluation on the WMT12
dataset translated by the Moses system.
The dataset was evenly split into 4 parts and
each of the parts was evaluated by one of two an-
notators (denoted “A” and “B”). For each sentence
that was modified by Deepfix, the annotator de-
cided whether the Deepfix correction had a posi-
tive (“improvement”) or negative (“degradation”)
effect on the translation quality, or concluded that
this cannot be decided (“indefinite”) – either be-
cause both of the sentences are correct variants, or
because both are incorrect.3
The results in Table 6 prove that the overall ef-
fect of Deepfix is positive: it modifies about 20%
of the sentence translations (569 out of 3003 sen-
tences), improving over a half of them while lead-
ing to a degradation in only a quarter of the cases.
We measured the inter-annotator agreement on
100 sentences which were annotated by both an-
notators. For 60 sentence pairs, both of the anno-
tators were able to select which sentence is better,
i.e. none of the annotators used the “indefinite”
marker. The inter-annotator agreement on these
60 sentence pairs was 97%.4
</bodyText>
<footnote confidence="0.9979911">
3The evaluation was done in a blind way, i.e. the annota-
tors did not know which sentence is before Deepfix and which
is after Deepfix. They were also provided with the source En-
glish sentences and the reference human translations.
4If all 100 sentence pairs are taken into account, requiring
that the annotators also agree on the “indefinite” marker, the
inter-annotator agreement is only 65%. This suggests that
deciding whether the translation quality differs significantly
is much harder than deciding which translation is of a higher
quality.
</footnote>
<page confidence="0.989748">
176
</page>
<table confidence="0.9997816">
Dataset BLEU score (higher is better) NIST score (higher is better) PER (lower is better)
Baseline Deepfix Difference Baseline Deepfix Difference Baseline Deepfix Difference
WMT10* 15.66 15.74 +0.08 5.442 5.470 +0.028 58.44% 58.26% -0.18
WMT11 16.39 16.42 +0.03 5.726 5.737 +0.011 57.17% 57.09% -0.08
WMT12 13.81 13.85 +0.04 5.263 5.283 +0.020 60.04% 59.91% -0.13
</table>
<tableCaption confidence="0.8939385">
Table 5: Automatic evaluation of Deepfix on outputs of the Moses system on WMT10, WMT11 and
WMT12 datasets. *Please note that WMT10 was used as the development dataset.
</tableCaption>
<table confidence="0.999954166666667">
Part Annotator Changed sentences Improvement Degradation Indefinite
1 A 126 57 (45%) 35 (28%) 34 (27%)
2 B 112 62 (55%) 29 (26%) 21 (19%)
3 A 150 88 (59%) 29 (19%) 33 (22%)
4 B 181 114 (63%) 42 (23%) 25 (14%)
Total 569 321 (56%) 135 (24%) 113 (20%)
</table>
<tableCaption confidence="0.99687">
Table 6: Manual evaluation of Deepfix on outputs of Moses Translate system on WMT12 dataset.
</tableCaption>
<subsectionHeader confidence="0.989374">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999916444444444">
When a formeme change was performed, it was
usually either positive or at least not harmful (sub-
stituting one correct variant for another correct
variant).
However, we also observed a substantial
amount of cases where the change of the formeme
was incorrect. Manual inspection of a sample of
these cases showed that there can be several rea-
sons for a formeme change to be incorrect:
</bodyText>
<listItem confidence="0.989817">
• incorrect analysis of the Czech sentence
• incorrect analysis of the English sentence
• the original formeme is a correct but very rare
variant
</listItem>
<bodyText confidence="0.999942666666667">
The most frequent issue is the first one. This is
to be expected, as the Czech sentence is often er-
roneous, whereas the NLP tools that we used are
trained on correct sentences; in many cases, it is
not even clear what a correct analysis of an incor-
rect sentence should be.
</bodyText>
<sectionHeader confidence="0.979036" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975967741935">
On the English-Czech pair, we have shown that
statistical post-editing of statistical machine trans-
lation outputs is possible, even when translating
from a morphologically poor to a morphologi-
cally rich language, if it is grounded by deep lin-
guistic knowledge. With our tool, Deepfix, we
have achieved improvements on outputs of two
state-of-the-art SMT systems by correcting verb-
noun valency errors, using two simple probabilis-
tic valency models computed on large-scale data.
The improvements have been confirmed by man-
ual evaluation.
We encountered many cases where the per-
formance of Deepfix was hindered by errors of
the underlying tools, especially the taggers, the
parsers and the aligner. Because the use of the
RUR parser (Rosa et al., 2012a), which is partially
adapted to SMT outputs parsing, lead to a reduc-
tion of the number of parser errors, we find the ap-
proach of adapting the tools for this specific kind
of data to be promising.
We believe that our method can be adapted
to other language pairs, provided that there is a
pipeline that can analyze at least the target lan-
guage up to deep syntactic trees. Because we only
use a small subset of information that a tectogram-
matical tree provides, it is sufficient to use only
simplified tectogrammatical trees. These could be
created by a small set of rules from shallow-syntax
dependency trees, which can be obtained for many
languages using already existing parsers.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999910846153846">
This research has been supported by the 7th FP
project of the EC No. 257528 and the project
7E11042 of the Ministry of Education, Youth and
Sports of the Czech Republic.
Data and some tools used as a prerequisite
for the research described herein have been pro-
vided by the LINDAT/CLARIN Large Infrastruc-
tural project, No. LM2010013 of the Ministry of
Education, Youth and Sports of the Czech Repub-
lic.
We would like to thank two anonymous review-
ers for many useful comments on the manuscript
of this paper.
</bodyText>
<page confidence="0.996569">
177
</page>
<sectionHeader confidence="0.996365" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810383928571">
Hanna B´echara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308–315.
Ond&amp;quot;rej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a taxonomy of factored phrase-
based models. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 253–
260, Montr´eal, Canada. Association for Computa-
tional Linguistics.
Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Pe-
tra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;r´ı
Mar&amp;quot;s´ık, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tam-
chyna. 2012b. The joy of parallelism with CzEng
1.0. In Proceedings of the 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012), pages 3921–3928, ˙Istanbul, Turkey. Euro-
pean Language Resources Association.
Ond&amp;quot;rej Bojar. 2011a. Rich morphology and what
can we expect from hybrid approaches to MT. In-
vited talk at International Workshop on Using Lin-
guistic Information for Hybrid Machine Translation
(LIHMT-2011), November.
Ond&amp;quot;rej Bojar. 2011b. Analyzing error types in
English-Czech machine translation. Prague Bulletin
of Mathematical Linguistics, 95:63–76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17–53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22–64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138–145. Morgan Kauf-
mann Publishers Inc.
Ond&amp;quot;rej Du&amp;quot;sek, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Martin Popel, Mar-
tin Majli&amp;quot;s, Michal Nov´ak, and David Mare&amp;quot;cek.
2012. Formemes in English-Czech deep syntac-
tic MT. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 267–274,
Montr´eal, Canada. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177–180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91–98. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, pages 25–32. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311–
318, Philadelphia, Pennsylvania.
Martin Popel and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL’10, pages 293–
304, Berlin, Heidelberg. Springer-Verlag.
Rudolf Rosa, Ond&amp;quot;rej Du&amp;quot;sek, David Mare&amp;quot;cek, and Mar-
tin Popel. 2012a. Using parallel features in pars-
ing of machine-translated sentences for correction of
grammatical errors. In Proceedings of Sixth Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation (SSST-6), ACL, pages 39–48, Jeju,
Korea. ACL.
Rudolf Rosa, David Mare&amp;quot;cek, and Ond&amp;quot;rej Du&amp;quot;sek.
2012b. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
</reference>
<page confidence="0.980205">
178
</page>
<reference confidence="0.999839857142857">
pages 362–368, Montr´eal, Canada. Association for
Computational Linguistics.
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics; Proceedings of the
Main Conference, pages 508–515, Rochester, New
York, April. Association for Computational Linguis-
tics.
Drahomira Spoustov´a, Jan Hajiˇc, Jan Votrubec, Pavel
Krbec, and Pavel Kvˇetoˇn. 2007. The best of
two worlds: Cooperation of statistical and rule-
based taggers for Czech. In Proceedings of the
Workshop on Balto-Slavonic Natural Language Pro-
cessing 2007, pages 67–74, Praha, Czechia. Uni-
verzita Karlova v Praze, Association for Computa-
tional Linguistics.
Lucien Tesni`ere and Jean Fourquet. 1959. El´ements de
syntaxe structurale. ´Editions Klincksieck, Paris.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conf. on Speech Communication and
Technology, pages 2667–2670.
</reference>
<page confidence="0.998797">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284393">
<title confidence="0.997644">Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</title>
<author confidence="0.999132">Rosa Mareˇcek</author>
<affiliation confidence="0.8559005">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.30452">Malostransk´e n´amˇest´ı 25,</address>
<abstract confidence="0.995712166666667">Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hanna B´echara</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
</authors>
<title>Statistical post-editing for a statistical MT system. MT Summit XIII,</title>
<date>2011</date>
<pages>308--315</pages>
<marker>B´echara, Ma, van Genabith, 2011</marker>
<rawString>Hanna B´echara, Yanjun Ma, and Josef van Genabith. 2011. Statistical post-editing for a statistical MT system. MT Summit XIII, pages 308–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Bushra Jawaid</author>
<author>Amir Kamran</author>
</authors>
<title>Probes in a taxonomy of factored phrasebased models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>253--260</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7337" citStr="Bojar et al., 2012" startWordPosition="1158" endWordPosition="1161"> explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng 1.0 (Bojar et al., 2012b), a large EnglishCzech parallel corpus. Identically to the paper, we split the training data into 10 parts, trained 10 systems (each on nine tenths of the data) and used them to translate the remaining part. The second step was then trained on the concatenation of these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are report</context>
<context position="13967" citStr="Bojar et al., 2012" startWordPosition="2291" endWordPosition="2294">(1), which performs more general fixes, in situations where the (lv, fEN) pair rather unambiguously defines the valency frame required. Then model (2) is applied, correcting some errors of the model (1), in cases where the noun argument requires a different valency frame than is usual for the (lv, fEN) pair, and making some more fixes in cases where the correct valency frame required for the (lv, fEN) pair was too ambiguous to make a correction according to model (1), but the decision can be made once information about ln is added. We computed the models on the full training set of CzEng 1.0 (Bojar et al., 2012b) (roughly 15 million sentences), and smoothed the estimated probabilities with add-one smoothing. 5.2 Deepfix We introduce a new statistical post-editing system, Deepfix, whose input is a pair of an English sentence and its Czech machine translation, and the output is the Czech sentence with verb-noun valency errors corrected. The Deepfix pipeline consists of several steps: 3. deep-syntax dependency parse trees of the sentences are built, the nodes in the trees are labelled with formemes 4. improbable noun formemes are replaced with correct formemes according to the valency model 5. the word</context>
<context position="18089" citStr="Bojar et al., 2012" startWordPosition="2950" endWordPosition="2954">turama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the changes performed by Deepfix are so small that automatic evaluation is unable to reliably assess whether they are positive or negative – it can only be taken as an indication. 6.2 Manual Evaluation To reliabl</context>
</contexts>
<marker>Bojar, Jawaid, Kamran, 2012</marker>
<rawString>Ond&amp;quot;rej Bojar, Bushra Jawaid, and Amir Kamran. 2012a. Probes in a taxonomy of factored phrasebased models. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 253– 260, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Zdenek Zabokrtsk´y</author>
<author>Ondrej Dusek</author>
<author>Petra Galusc´akov´a</author>
<author>Martin Majlis</author>
<author>David Marecek</author>
<author>Jir´ı Mars´ık</author>
<author>Michal Nov´ak</author>
<author>Martin Popel</author>
<author>Ales Tamchyna</author>
</authors>
<title>The joy of parallelism with CzEng 1.0.</title>
<date>2012</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<pages>3921--3928</pages>
<location>Istanbul,</location>
<marker>Bojar, Zabokrtsk´y, Dusek, Galusc´akov´a, Majlis, Marecek, Mars´ık, Nov´ak, Popel, Tamchyna, 2012</marker>
<rawString>Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Petra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;r´ı Mar&amp;quot;s´ık, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tamchyna. 2012b. The joy of parallelism with CzEng 1.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), pages 3921–3928, ˙Istanbul, Turkey. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2011a</author>
</authors>
<title>Rich morphology and what can we expect from hybrid approaches to MT.</title>
<date></date>
<booktitle>Invited talk at International Workshop on Using Linguistic Information for Hybrid Machine Translation (LIHMT-2011),</booktitle>
<contexts>
<context position="949" citStr="(2011)" startWordPosition="130" endWordPosition="130"> Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge. 1 Introduction Statistical machine translation (SMT) is the current state-of-the-art approach to machine translation – see e.g. Callison-Burch et al. (2011). However, its outputs are still typically significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. As shown by many researchers, e.g. Bojar (2011a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. Several types of errors can be fi</context>
<context position="5868" citStr="(2011)" startWordPosition="932" endWordPosition="932"> no improvement in quality was observed. The first known positive results in SPE of SMT are reported by Oflazer and El-Kahlout (2007) on English to Turkish machine translation. The authors followed a similar approach to Simard et al. (2007), training an SMT system to postedit its own output. They use two iterations of post-editing to get an improvement of 0.47 BLEU points (Papineni et al., 2002). The authors used a rather small training set and do not discuss the scalability of their approach. To the best of our knowledge, the best results reported so far for SPE of SMT are by B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source Direction Baseline SPE Context SPE en→cs 10.85±0.47 10.70±0.44 10.73±0.49 cs→en 17.20±0.53 17.11±0.52 17.18±0.54 Table 3: Results of SPE approach of B´echara et al. (2011) evaluated on English-Czech SMT. words, eventually </context>
<context position="7232" citStr="(2011)" startWordPosition="1142" endWordPosition="1142">ow their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng 1.0 (Bojar et al., 2012b), a large EnglishCzech parallel corpus. Identically to the paper, we split the training data into 10 parts, trained 10 systems (each on nine tenths of the data) and used them to translate the remaining part. The second step was then trained on the concatenation of these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than </context>
<context position="17900" citStr="(2010; 2011; 2012)" startWordPosition="2927" endWordPosition="2929">NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the changes performed b</context>
</contexts>
<marker>2011a, </marker>
<rawString>Ond&amp;quot;rej Bojar. 2011a. Rich morphology and what can we expect from hybrid approaches to MT. Invited talk at International Workshop on Using Linguistic Information for Hybrid Machine Translation (LIHMT-2011), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
</authors>
<title>Analyzing error types in English-Czech machine translation.</title>
<date>2011</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<pages>95--63</pages>
<contexts>
<context position="1084" citStr="Bojar, 2011" startWordPosition="149" endWordPosition="150"> correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge. 1 Introduction Statistical machine translation (SMT) is the current state-of-the-art approach to machine translation – see e.g. Callison-Burch et al. (2011). However, its outputs are still typically significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. As shown by many researchers, e.g. Bojar (2011a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. Several types of errors can be fixed by employing rule-based post-editing (Rosa et al., 2012b), which can be seen as being orthogonal to the statistical methods employe</context>
</contexts>
<marker>Bojar, 2011</marker>
<rawString>Ond&amp;quot;rej Bojar. 2011b. Analyzing error types in English-Czech machine translation. Prague Bulletin of Mathematical Linguistics, 95:63–76, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="17887" citStr="Callison-Burch et al. (2010" startWordPosition="2924" endWordPosition="2927">many state-of-the-art NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the change</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="949" citStr="Callison-Burch et al. (2011)" startWordPosition="127" endWordPosition="130">l.mff.cuni.cz Abstract Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge. 1 Introduction Statistical machine translation (SMT) is the current state-of-the-art approach to machine translation – see e.g. Callison-Burch et al. (2011). However, its outputs are still typically significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. As shown by many researchers, e.g. Bojar (2011a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. Several types of errors can be fi</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="8092" citStr="Callison-Burch et al., 2012" startWordPosition="1286" endWordPosition="1289">s (each on nine tenths of the data) and used them to translate the remaining part. The second step was then trained on the concatenation of these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, pos</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="18295" citStr="Doddington, 2002" startWordPosition="2987" endWordPosition="2988">aluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the changes performed by Deepfix are so small that automatic evaluation is unable to reliably assess whether they are positive or negative – it can only be taken as an indication. 6.2 Manual Evaluation To reliably assess the performance of Deepfix, we performed manual evaluation on the WMT12 dataset translated by the Moses system. The dataset was evenly split into 4 parts and each of the parts was evaluated by one </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Dusek</author>
<author>Zdenek Zabokrtsk´y</author>
<author>Martin Popel</author>
<author>Martin Majlis</author>
<author>Michal Nov´ak</author>
<author>David Marecek</author>
</authors>
<title>Formemes in English-Czech deep syntactic MT.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>267--274</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<marker>Dusek, Zabokrtsk´y, Popel, Majlis, Nov´ak, Marecek, 2012</marker>
<rawString>Ond&amp;quot;rej Du&amp;quot;sek, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Martin Popel, Martin Majli&amp;quot;s, Michal Nov´ak, and David Mare&amp;quot;cek. 2012. Formemes in English-Czech deep syntactic MT. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 267–274, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3377" citStr="Koehn et al., 2007" startWordPosition="525" endWordPosition="528">successful research in statistical post-editing (SPE) of SMT (see Section 2). In our paper, we describe a statistical approach to correcting one particular type of English-toCzech SMT errors – errors in the verb-noun valency. The term valency stands for the way in which verbs and their arguments are used together, usually together with prepositions and morphological cases, and is described in Section 4. Several examples of the valency of the English verb ‘to go’ and the corresponding Czech verb ‘j´ıt’ are shown in Table 1. We conducted our experiments using a state-ofthe-art SMT system Moses (Koehn et al., 2007). An example of Moses making a valency error is translating the sentence ‘The government spends on the middle schools.’, adapted from our development data set. As shown in Table 2, Moses translates the sentence incorrectly, making an error in the valency of the ‘utr´acet – ˇskola’ (‘spend – school’) pair. The missing preposition changes the meaning dramatically, as the verb ‘utr´acet’ is pol172 Proceedings of the ACL Student Research Workshop, pages 172–179, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ysemous and can mean ‘to spend (esp. money)’ as well a</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="8215" citStr="Koehn, 2004" startWordPosition="1305" endWordPosition="1306"> these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, possibly because our training data is too large for this method to bring any benefit. We therefore proceed with a more complex</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17428" citStr="McDonald et al., 2005" startWordPosition="2852" endWordPosition="2855">ften bears a meaning that is lost by removing it – in Czech, which is a relatively free-word-order language, the semantic roles of verb arguments are typically distinguished by prepositions, as opposed to English, where they can be determined by their relative position to the verb. 5.4 Implementation The whole Deepfix pipeline is implemented in Treex, a modular NLP framework (Popel and ˇZabokrtsk´y, 2010) written in Perl, which provides wrappers for many state-of-the-art NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17631" citStr="Och and Ney, 2003" startWordPosition="2889" endWordPosition="2892">glish, where they can be determined by their relative position to the verb. 5.4 Implementation The whole Deepfix pipeline is implemented in Treex, a modular NLP framework (Popel and ˇZabokrtsk´y, 2010) written in Perl, which provides wrappers for many state-of-the-art NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLE</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-to-Turkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5395" citStr="Oflazer and El-Kahlout (2007)" startWordPosition="844" endWordPosition="847"> system as the source and the humanprovided reference translations as the target, to achieve massive translation quality improvements. The authors also compared the performance of the post-edited rule-based system to directly using the SMT system in a bilingual setting, and reported that the SMT system alone performed worse than the post-edited rule-based system. They then tried to post-edit the bilingual SMT system with another monolingual instance of the same SMT system, but concluded that no improvement in quality was observed. The first known positive results in SPE of SMT are reported by Oflazer and El-Kahlout (2007) on English to Turkish machine translation. The authors followed a similar approach to Simard et al. (2007), training an SMT system to postedit its own output. They use two iterations of post-editing to get an improvement of 0.47 BLEU points (Papineni et al., 2002). The authors used a rather small training set and do not discuss the scalability of their approach. To the best of our knowledge, the best results reported so far for SPE of SMT are by B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a st</context>
</contexts>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-to-Turkish statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="5660" citStr="Papineni et al., 2002" startWordPosition="889" endWordPosition="892">nd reported that the SMT system alone performed worse than the post-edited rule-based system. They then tried to post-edit the bilingual SMT system with another monolingual instance of the same SMT system, but concluded that no improvement in quality was observed. The first known positive results in SPE of SMT are reported by Oflazer and El-Kahlout (2007) on English to Turkish machine translation. The authors followed a similar approach to Simard et al. (2007), training an SMT system to postedit its own output. They use two iterations of post-editing to get an improvement of 0.47 BLEU points (Papineni et al., 2002). The authors used a rather small training set and do not discuss the scalability of their approach. To the best of our knowledge, the best results reported so far for SPE of SMT are by B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source Direction Base</context>
<context position="18263" citStr="Papineni et al., 2002" startWordPosition="2981" endWordPosition="2984">rsection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the changes performed by Deepfix are so small that automatic evaluation is unable to reliably assess whether they are positive or negative – it can only be taken as an indication. 6.2 Manual Evaluation To reliably assess the performance of Deepfix, we performed manual evaluation on the WMT12 dataset translated by the Moses system. The dataset was evenly split into 4 parts and each of</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311– 318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Popel</author>
<author>Zdenek Zabokrtsk´y</author>
</authors>
<title>TectoMT: modular NLP framework.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th international conference on Advances in natural language processing, IceTAL’10,</booktitle>
<pages>293--304</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Popel, Zabokrtsk´y, 2010</marker>
<rawString>Martin Popel and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2010. TectoMT: modular NLP framework. In Proceedings of the 7th international conference on Advances in natural language processing, IceTAL’10, pages 293– 304, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Rosa</author>
<author>Ondrej Dusek</author>
<author>David Marecek</author>
<author>Martin Popel</author>
</authors>
<title>Using parallel features in parsing of machine-translated sentences for correction of grammatical errors.</title>
<date>2012</date>
<booktitle>In Proceedings of Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6), ACL,</booktitle>
<pages>39--48</pages>
<publisher>ACL.</publisher>
<location>Jeju,</location>
<contexts>
<context position="1608" citStr="Rosa et al., 2012" startWordPosition="231" endWordPosition="234"> significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. As shown by many researchers, e.g. Bojar (2011a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. Several types of errors can be fixed by employing rule-based post-editing (Rosa et al., 2012b), which can be seen as being orthogonal to the statistical methods employed in SMT and thus can capture different linguistic phenomena easily. But there are still other errors that cannot be corrected with hand-written rules, as there exist many linguistic phenomena that can never be fully described manually – they need to be handled statistically by automatically analyzing large-scale text corpora. However, to the best of our knowledge, English Czech go to the doctor j´ıt k doktorovi dative case go to the centre j´ıt do centra genitive case go to a concert j´ıt na koncert accusative case go</context>
<context position="6713" citStr="Rosa et al., 2012" startWordPosition="1059" endWordPosition="1062"> their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source Direction Baseline SPE Context SPE en→cs 10.85±0.47 10.70±0.44 10.73±0.49 cs→en 17.20±0.53 17.11±0.52 17.18±0.54 Table 3: Results of SPE approach of B´echara et al. (2011) evaluated on English-Czech SMT. words, eventually reaching an improvement of 2.29 BLEU points. However, similarly to Oflazer and El-Kahlout (2007), the training data used are very small, and it is not clear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng</context>
<context position="8360" citStr="Rosa et al., 2012" startWordPosition="1329" endWordPosition="1332"> are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, possibly because our training data is too large for this method to bring any benefit. We therefore proceed with a more complex approach which relies on deep linguistic knowledge. 4 Deep Dependency Syntax, Formemes, and Valency 4.1 Tectogrammatical dependency trees Tectog</context>
<context position="17523" citStr="Rosa et al., 2012" startWordPosition="2869" endWordPosition="2872">anguage, the semantic roles of verb arguments are typically distinguished by prepositions, as opposed to English, where they can be determined by their relative position to the verb. 5.4 Implementation The whole Deepfix pipeline is implemented in Treex, a modular NLP framework (Popel and ˇZabokrtsk´y, 2010) written in Perl, which provides wrappers for many state-of-the-art NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and </context>
<context position="22867" citStr="Rosa et al., 2012" startWordPosition="3747" endWordPosition="3750"> when translating from a morphologically poor to a morphologically rich language, if it is grounded by deep linguistic knowledge. With our tool, Deepfix, we have achieved improvements on outputs of two state-of-the-art SMT systems by correcting verbnoun valency errors, using two simple probabilistic valency models computed on large-scale data. The improvements have been confirmed by manual evaluation. We encountered many cases where the performance of Deepfix was hindered by errors of the underlying tools, especially the taggers, the parsers and the aligner. Because the use of the RUR parser (Rosa et al., 2012a), which is partially adapted to SMT outputs parsing, lead to a reduction of the number of parser errors, we find the approach of adapting the tools for this specific kind of data to be promising. We believe that our method can be adapted to other language pairs, provided that there is a pipeline that can analyze at least the target language up to deep syntactic trees. Because we only use a small subset of information that a tectogrammatical tree provides, it is sufficient to use only simplified tectogrammatical trees. These could be created by a small set of rules from shallow-syntax depende</context>
</contexts>
<marker>Rosa, Dusek, Marecek, Popel, 2012</marker>
<rawString>Rudolf Rosa, Ond&amp;quot;rej Du&amp;quot;sek, David Mare&amp;quot;cek, and Martin Popel. 2012a. Using parallel features in parsing of machine-translated sentences for correction of grammatical errors. In Proceedings of Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6), ACL, pages 39–48, Jeju, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Rosa</author>
<author>David Marecek</author>
<author>Ondrej Dusek</author>
</authors>
<title>DEPFIX: A system for automatic correction of Czech MT outputs.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>362--368</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1608" citStr="Rosa et al., 2012" startWordPosition="231" endWordPosition="234"> significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. As shown by many researchers, e.g. Bojar (2011a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. Several types of errors can be fixed by employing rule-based post-editing (Rosa et al., 2012b), which can be seen as being orthogonal to the statistical methods employed in SMT and thus can capture different linguistic phenomena easily. But there are still other errors that cannot be corrected with hand-written rules, as there exist many linguistic phenomena that can never be fully described manually – they need to be handled statistically by automatically analyzing large-scale text corpora. However, to the best of our knowledge, English Czech go to the doctor j´ıt k doktorovi dative case go to the centre j´ıt do centra genitive case go to a concert j´ıt na koncert accusative case go</context>
<context position="6713" citStr="Rosa et al., 2012" startWordPosition="1059" endWordPosition="1062"> their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source Direction Baseline SPE Context SPE en→cs 10.85±0.47 10.70±0.44 10.73±0.49 cs→en 17.20±0.53 17.11±0.52 17.18±0.54 Table 3: Results of SPE approach of B´echara et al. (2011) evaluated on English-Czech SMT. words, eventually reaching an improvement of 2.29 BLEU points. However, similarly to Oflazer and El-Kahlout (2007), the training data used are very small, and it is not clear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng</context>
<context position="8360" citStr="Rosa et al., 2012" startWordPosition="1329" endWordPosition="1332"> are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, possibly because our training data is too large for this method to bring any benefit. We therefore proceed with a more complex approach which relies on deep linguistic knowledge. 4 Deep Dependency Syntax, Formemes, and Valency 4.1 Tectogrammatical dependency trees Tectog</context>
<context position="17523" citStr="Rosa et al., 2012" startWordPosition="2869" endWordPosition="2872">anguage, the semantic roles of verb arguments are typically distinguished by prepositions, as opposed to English, where they can be determined by their relative position to the verb. 5.4 Implementation The whole Deepfix pipeline is implemented in Treex, a modular NLP framework (Popel and ˇZabokrtsk´y, 2010) written in Perl, which provides wrappers for many state-of-the-art NLP tools. For the analysis of the English sentence, we use the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005). The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al., 2012a) – a parser adapted to parsing of SMT outputs. The word alignment is created by GIZA++ (Och and Ney, 2003); the intersection symmetrization is used. 6 Evaluation 6.1 Automatic Evaluation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and </context>
<context position="22867" citStr="Rosa et al., 2012" startWordPosition="3747" endWordPosition="3750"> when translating from a morphologically poor to a morphologically rich language, if it is grounded by deep linguistic knowledge. With our tool, Deepfix, we have achieved improvements on outputs of two state-of-the-art SMT systems by correcting verbnoun valency errors, using two simple probabilistic valency models computed on large-scale data. The improvements have been confirmed by manual evaluation. We encountered many cases where the performance of Deepfix was hindered by errors of the underlying tools, especially the taggers, the parsers and the aligner. Because the use of the RUR parser (Rosa et al., 2012a), which is partially adapted to SMT outputs parsing, lead to a reduction of the number of parser errors, we find the approach of adapting the tools for this specific kind of data to be promising. We believe that our method can be adapted to other language pairs, provided that there is a pipeline that can analyze at least the target language up to deep syntactic trees. Because we only use a small subset of information that a tectogrammatical tree provides, it is sufficient to use only simplified tectogrammatical trees. These could be created by a small set of rules from shallow-syntax depende</context>
</contexts>
<marker>Rosa, Marecek, Dusek, 2012</marker>
<rawString>Rudolf Rosa, David Mare&amp;quot;cek, and Ond&amp;quot;rej Du&amp;quot;sek. 2012b. DEPFIX: A system for automatic correction of Czech MT outputs. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 362–368, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The meaning of the sentence in its semantic and pragmatic aspects.</title>
<date>1986</date>
<publisher>Springer.</publisher>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The meaning of the sentence in its semantic and pragmatic aspects. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Statistical phrase-based post-editing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>508--515</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="4520" citStr="Simard et al., 2007" startWordPosition="711" endWordPosition="714">putational Linguistics ysemous and can mean ‘to spend (esp. money)’ as well as ‘to kill, to destroy (esp. animals)’. Our approach is to use deep linguistic analysis to automatically determine the structure of each sentence, and to detect and correct valency errors using a simple statistical valency model. We describe our approach in detail in Section 5. We evaluate and discuss our experiments in Section 6. We then conclude the paper and propose areas to be researched in future in Section 7. 2 Related Work The first reported results of automatic post-editing of machine translation outputs are (Simard et al., 2007) where the authors successfully performed statistical post-editing (SPE) of rule-based machine translation outputs. To perform the postediting, they used a phrase-based SMT system in a monolingual setting, trained on the outputs of the rule-based system as the source and the humanprovided reference translations as the target, to achieve massive translation quality improvements. The authors also compared the performance of the post-edited rule-based system to directly using the SMT system in a bilingual setting, and reported that the SMT system alone performed worse than the post-edited rule-ba</context>
</contexts>
<marker>Simard, Goutte, Isabelle, 2007</marker>
<rawString>Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007. Statistical phrase-based post-editing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 508–515, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>The best of two worlds: Cooperation of statistical and rulebased taggers for Czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing</booktitle>
<pages>67--74</pages>
<location>Praha, Czechia.</location>
<marker>Spoustov´a, Hajiˇc, Votrubec, Krbec, Kvˇetoˇn, 2007</marker>
<rawString>Drahomira Spoustov´a, Jan Hajiˇc, Jan Votrubec, Pavel Krbec, and Pavel Kvˇetoˇn. 2007. The best of two worlds: Cooperation of statistical and rulebased taggers for Czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing 2007, pages 67–74, Praha, Czechia. Univerzita Karlova v Praze, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
<author>Jean Fourquet</author>
</authors>
<title>El´ements de syntaxe structurale. ´Editions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, Fourquet, 1959</marker>
<rawString>Lucien Tesni`ere and Jean Fourquet. 1959. El´ements de syntaxe structurale. ´Editions Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
<author>Hassan Sawaf</author>
</authors>
<title>Accelerated dp based search for statistical translation.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<contexts>
<context position="18328" citStr="Tillmann et al., 1997" startWordPosition="2991" endWordPosition="2995">ation We evaluated our method on three datasets: WMT10 (2489 parallel sentences), WMT11 (3003 parallel sentences), and WMT12 (3003 parallel sentences) by Callison-Burch et al. (2010; 2011; 2012). For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al., 2http://featurama.sourceforge.net/ 2007), tuned for English-to-Czech translation (Bojar et al., 2012a). We used the WMT10 dataset and its Moses translation as our development data to tune the thresholds. In Table 5, we report the achieved BLEU scores (Papineni et al., 2002), NIST scores (Doddington, 2002), and PER (Tillmann et al., 1997). The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. However, the changes performed by Deepfix are so small that automatic evaluation is unable to reliably assess whether they are positive or negative – it can only be taken as an indication. 6.2 Manual Evaluation To reliably assess the performance of Deepfix, we performed manual evaluation on the WMT12 dataset translated by the Moses system. The dataset was evenly split into 4 parts and each of the parts was evaluated by one of two annotators (denoted “A” an</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex Zubiaga, and Hassan Sawaf. 1997. Accelerated dp based search for statistical translation. In European Conf. on Speech Communication and Technology, pages 2667–2670.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>