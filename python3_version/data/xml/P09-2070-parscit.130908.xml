<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020019">
<title confidence="0.992982">
Parsing Speech Repair without Specialized Grammar Symbols*
</title>
<author confidence="0.997877">
Tim Miller Luan Nguyen William Schuler
</author>
<affiliation confidence="0.999519">
University of Minnesota University of Minnesota University of Minnesota
</affiliation>
<email confidence="0.997788">
tmill@cs.umn.edu lnguyen@cs.umn.edu schuler@cs.umn.edu
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982">
This paper describes a parsing model for
speech with repairs that makes a clear sep-
aration between linguistically meaningful
symbols in the grammar and operations
specific to speech repair in the operation of
the parser. This system builds a model of
how unfinished constituents in speech re-
pairs are likely to finish, and finishes them
probabilistically with placeholder struc-
ture. These modified repair constituents
and the restarted replacement constituent
are then recognized together in the same
way that two coordinated phrases of the
same type are recognized.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963132652173913">
Speech repair is a phenomenon in spontaneous
spoken language in which a speaker decides to
interrupt the flow of speech, replace some of the
utterance (the “reparandum”), and continues on
(with the “alteration”) in a way that makes the
whole sentence as transcribed grammatical only
if the reparandum is ignored. As Ferreira et al.
(2004) note, speech repairs1 are the most disrup-
tive type of disfluency, as they seem to require
that a listener first incrementally build up syntac-
tic and semantic structure, then subsequently re-
move it and rebuild when the repair is made. This
difficulty combines with their frequent occurrence
to make speech repair a pressing problem for ma-
chine recognition of spontaneous speech.
This paper introduces a model for dealing with
one part of this problem, constructing a syntac-
tic analysis based on a transcript of spontaneous
spoken language. The model introduced here dif-
fers from other models attempting to solve the
This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
</bodyText>
<footnote confidence="0.476801">
1Ferreira et al. use the term ‘revisions’.
</footnote>
<bodyText confidence="0.999980157894737">
same problem, by completely separating the fluent
grammar from the operations of the parser. The
grammar thus has no representation of disfluency
or speech repair, such as the “EDITED” category
used to represent a reparandum in the Switchboard
corpus, as such categories are seemingly at odds
with the typical nature of a linguistic constituent.
Rather, the approach presented here uses a
grammar that explicitly represents incomplete
constituents being processed, and repair is rep-
resented by rules which allow incomplete con-
stituents to be prematurely merged with existing
structure. While this model is interesting for its
elegance in representation, there is also reason
to hypothesize improved performance, since this
processing model requires no additional grammar
symbols, and only one additional operation to ac-
count for speech repair, and thus makes better use
of limited data resources.
</bodyText>
<sectionHeader confidence="0.971151" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9996369">
Previous work on parsing of speech with repairs
has shown that syntactic cues can be used to in-
crease accuracy of detection of reparanda, which
can increase overall parsing accuracy. The first
source of structure used to recognize repair is what
Levelt (1983) called the “Well-formedness Rule.”
This rule essentially states that a speech repair acts
like a conjunction; that is, the reparandum and the
alteration must be of the same syntactic category.
Of course, the reparandum is often unfinished, so
the Well-formedness Rule allows for the reparan-
dum category to be inferred.
This source of structure has been used by two
related approaches, that of Hale et al. (2006) and
Miller (2009). Hale and colleagues exploit this
structure by adding contextual information to the
standard reparandum label “EDITED”. In their
terminology, daughter annotation takes the (pos-
sibly unfinished) constituent label of the reparan-
dum and appends it to the EDITED label. This
</bodyText>
<page confidence="0.989996">
277
</page>
<note confidence="0.9260605">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277–280,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999722571428571">
allows a learned probabilistic context-free gram-
mar to represent the likelihood of a reparandum of
a certain type being a sibling with a finished con-
stituent of the same type.
Miller’s approach exploited the same source of
structure, but changed the representation to use
a REPAIRED label for alterations instead of an
EDITED label for reparanda. The rationale for
that change is the fact that a speech repair does not
really begin until the interruption point, at which
point the alteration is started and the reparandum
is retroactively labelled as such. Thus, the argu-
ment goes, no special syntactic rules or symbols
should be necessary until the alteration begins.
</bodyText>
<sectionHeader confidence="0.998453" genericHeader="method">
3 Model Description
</sectionHeader>
<subsectionHeader confidence="0.9985">
3.1 Right-corner transform
</subsectionHeader>
<bodyText confidence="0.98818765">
This work first uses a right-corner transform,
which turns right-branching structure into left-
branching structure, using category labels that use
a “slash” notation α/,y to represent an incomplete
constituent of type α “looking for” a constituent
of type -y in order to complete itself.
This transform first requires that trees be bina-
rized. This binarization is done in a similar way to
Johnson (1998) and Klein and Manning (2003).
Rewrite rules for the right-corner transform are
as follows, first flattening right-branching struc-
ture:2
then replacing it with left-branching structure:
One problem with this notation is the represen-
tation given to unfinished constituents, as seen in
Figures 1 and 2. The standard representation of
2Here, all Ai denote nonterminal symbols, and αi denote
subtrees; the notation A1:α0 indicates a subtree α0 with label
Al; and all rewrites are applied recursively, from leaves to
root.
</bodyText>
<figure confidence="0.711918">
S
india
</figure>
<figureCaption confidence="0.996556333333333">
Figure 1: Section of interest of a standard phrase
structure tree containing speech repair with unfin-
ished noun phrase (NP).
</figureCaption>
<figure confidence="0.839771">
PP
</figure>
<figureCaption confidence="0.897371">
Figure 2: Right-corner transformed version of the
fragment above. This tree requires several special
symbols to represent the reparandum that starts
this fragment.
</figureCaption>
<bodyText confidence="0.9999593">
an unfinished constituent in the Switchboard cor-
pus is to append the -UNF label to the lowest un-
finished constituent (see Figure 1). Since one goal
of this work is separation of linguistic knowledge
from language processing mechanisms, the -UNF
tag should not be an explicit part of the gram-
mar. In theory, the incomplete category notation
induced by the right-corner transform is perfectly
suited to this purpose. For instance, the category
NP-UNF is a stand in category for several incom-
plete constituents, for example NP/NN, NP/NNS,
etc. However, since the sub-trees with -UNF la-
bels in the original corpus are by definition unfin-
ished, the label to the right of the slash (NN in
this case) is not defined. As a result, transformed
trees with unfinished structure have the represen-
tation of Figure 2, which gives away the positive
benefits of the right-corner transform in represent-
ing repair by propagating a special repair symbol
(EDITED) through the grammar.
</bodyText>
<subsectionHeader confidence="0.99998">
3.2 Approximating unfinished constituents
</subsectionHeader>
<bodyText confidence="0.999917333333333">
It is possible to represent -UNF categories as stan-
dard unfinished constituents, and account for un-
finished constituents by having the parser prema-
</bodyText>
<figure confidence="0.995922970149254">
a3
α1 α2
α2
A1
α1 A2
α2 A3
A1
A1/A2
A3
A2/A3
α1
α2
a3
⇒
A1
α1 A2
A2/A3
. . .
⇒
A1/A2
A2/A3
. . .
A1
A1
A1/A2:α1 A2/A3
α3 ... ⇒
A1
A1/A3
α3 ...
α2 A1/ A2:α1 α2
as
NP
PP-LOC
IN
as
DT
NP-UNF
DT
a
IN
NN
NP
NNP
in
a westerner
... EDITED
PP
PP
IN
. . .
NP
PP/NP NP
PP/PP IN india
in
PP/NP
NP
PP/PP
NP/NN
IN
NN
EDITEDPP/NP-UNF NP-UNF a
IN DT
as a
DT
as
EDITEDPP
westerner
</figure>
<page confidence="0.985824">
278
</page>
<bodyText confidence="0.999975129032258">
turely end the processing of a given constituent.
However, in the example given above, this would
require predicting ahead of time that the NP-UNF
was only missing a common noun – NN (for ex-
ample). This problem is addressed in this work
by probabilistically filling in placeholder final cat-
egories of unfinished constituents in the standard
phrase structure trees, before applying the right-
corner transform.
In order to fill in the placeholder with realistic
items, phrase completions are learned from cor-
pus statistics. First, this algorithm identifies an
unfinished constituent to be finished as well as its
existing children (in the continuing example, NP-
UNF with child labelled DT). Next, the corpus is
searched for fluent subtrees with matching root la-
bels and child labels (NP and DT), and a distri-
bution is computed of the actual completions of
those subtrees. In the model used in this work,
the most common completions are NN, NNS, and
NNP. The original NP-UNF subtree is then given a
placeholder completion by sampling from the dis-
tribution of completions computed above.
After this addition is complete, the UNF and
EDITED labels are removed from the reparandum
subtree, and if a restarted constituent of the same
type is a sibling of the reparandum (e.g. another
NP), the two subtrees are made siblings under a
new subtree with the same category label (NP).
See Figure 3 for a simple visual example of how
this works.
</bodyText>
<note confidence="0.698102">
PP
</note>
<figureCaption confidence="0.988899">
Figure 4: Right-corner transformed tree with
placeholder finished phrase.
</figureCaption>
<figure confidence="0.666344">
PP
</figure>
<figureCaption confidence="0.783691">
Figure 5: Final right-corner transformed state af-
</figureCaption>
<bodyText confidence="0.925813111111111">
ter excising placeholder completions to unfinished
constituents. The bolded label indicates the signal
of an unfinished category reparandum.
child subtree having an unfinished constituent type
(a slash category, e.g. PP/NN in Figure 5) at its
root represents a reparandum with an unfinished
category. The tree then represents and processes
the rest of the repair in the same way as a coordi-
nation.
</bodyText>
<sectionHeader confidence="0.905149" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<figure confidence="0.999233926470588">
DT
NN
IN
NN
NP
DT
as
... EDITED
PP
. . .
NP
PP
IN
PP-LOC
IN
NP
as
NP
S
DT
westerner
DT
eli
PP/PP
IN
india
PP/NP
PP/PP
PP
NP
in
IN
as
NP/NN
NN
PP/NN
PP/NP
IN
as
NN
a
a
NNP
PP/NNP
DT
westerner
as
a
PP/PP
IN
india
PP/NP
PP/PP
PP/NN
PP/NP
IN
as
NP
in
IN
DT
a
NP/NN
NN
NNP
PP/NNP
a eli a westerner in NNP
india
</figure>
<figureCaption confidence="0.875494">
Figure 3: Same tree as in Figure 1, with the un-
finished noun phrase now given a placeholder NN
completion (both bolded).
</figureCaption>
<bodyText confidence="0.999696708333333">
Next, these trees are modified using the right-
corner transform as shown in Figure 4. This tree
still contains placeholder words that will not be
in the text stream of an observed input sentence.
Thus, in the final step of the preprocessing algo-
rithm, the finished category label and the place-
holder right child are removed where found in a
right-corner tree. This results in a right-corner
transformed tree in which a unary child or right
This model was evaluated on the Switchboard cor-
pus (Godfrey et al., 1992) of conversational tele-
phone speech between two human interlocuters.
The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. For comparison to other similar systems,
the system was given the gold standard part of
speech for each input word as well. The standard
train/test breakdown was used, with sections 2 and
3 used for training, and subsections 0 and 1 of sec-
tion 4 used for testing. Several sentences from the
end of section 4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
</bodyText>
<page confidence="0.99284">
279
</page>
<bodyText confidence="0.999903833333333">
and partial words – anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
</bodyText>
<table confidence="0.996684">
System Configuration Parseval-F Edited-F
Baseline CYK 71.05 18.03
Hale et al. 68.48 37.94
Plain RC Trees 69.07 30.89
Elided RC Trees 67.91 24.80
Merged RC Trees 68.88 27.63
</table>
<tableCaption confidence="0.99983">
Table 1: Results
</tableCaption>
<bodyText confidence="0.99929085">
Results of the testing can be seen in Ta-
ble 1. The first line (“Baseline CYK”) indi-
cates the results using a standard probabilistic
CYK parser, trained on the standardized input
trees. The following two lines are results from re-
implementations of the systems from Hale et al.
(2006) and Miller (2009). The line marked ‘Elided
trees’ gives current results. Surprisingly, this re-
sult proves to be lower than the previous results.
Two observations in the output of the parser on
the development set gave hints as to the reasons
for this performance loss.
First, repairs using the slash categories (for un-
finished reparanda) were rare (relative to finished
reparanda). This led to the suspicion that there
was a state-splitting phenomenon, where cate-
gories previously lumped together as EDITED-NP
were divided into several unfinished categories
(NP/NN, NP/NNS, etc.). To test this suspicion, an-
other experiment was performed where all unary
child and right child subtrees with unfinished cat-
egory labels X/Y were replaced with EDITED-X.
This result is shown in line five of Table 1. This
result improves on the elided version, and sug-
gests that the state-splitting effect is most likely
one cause of decreased performance.
The second effect in the parser output was the
presence of several very long reparanda (more
than ten words), which are highly unlikely in nor-
mal speech. This phenomenon does not occur
in the ‘Plain RC Trees’ condition. One explana-
tion for this effect is that plain RC trees use the
EDITED label in each rule of the reparandum (see
Figure 2 for a short real-world example). This
essentially creates a reparandum rule set, mak-
ing expansion of a reparandum difficult due to the
likelihood of a long chain eventually requiring a
reparandum rule that was not found in the train-
ing data, or was not learned correctly in the much
smaller set of reparandum-specific training data.
</bodyText>
<sectionHeader confidence="0.993716" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999515">
In conclusion, this paper has presented a new
model for speech containing repairs that enforces
a clean separation between linguistic categories
and parsing operations. Performance was below
expectations, but analysis of the interesting rea-
sons for these results suggests future directions. A
model which explicitly represents the distance that
a speaker backtracks when making a repair would
prevent the parser from hypothesizing the unlikely
reparanda of great length.
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991375">
Fernanda Ferreira, Ellen F. Lau, and Karl G.D. Bai-
ley. 2004. Disfluencies, language comprehension,
and Tree Adjoining Grammars. Cognitive Science,
28:721–749.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517–520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613–
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.
Willem J.M. Levelt. 1983. Monitoring and self-repair
in speech. Cognition, 14:41–104.
Tim Miller. 2009. Improved syntactic models for pars-
ing speech with repairs. In Proceedings of the North
American Association for Computational Linguis-
tics, Boulder, CO.
</reference>
<page confidence="0.997137">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992949">
<title confidence="0.999012">Speech Repair without Specialized Grammar</title>
<author confidence="0.999924">Tim Miller Luan Nguyen William Schuler</author>
<affiliation confidence="0.999984">University of Minnesota University of Minnesota University of Minnesota</affiliation>
<email confidence="0.999474">tmill@cs.umn.edulnguyen@cs.umn.eduschuler@cs.umn.edu</email>
<abstract confidence="0.999628866666667">This paper describes a parsing model for speech with repairs that makes a clear separation between linguistically meaningful symbols in the grammar and operations specific to speech repair in the operation of the parser. This system builds a model of how unfinished constituents in speech repairs are likely to finish, and finishes them probabilistically with placeholder structure. These modified repair constituents and the restarted replacement constituent are then recognized together in the same way that two coordinated phrases of the same type are recognized.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fernanda Ferreira</author>
<author>Ellen F Lau</author>
<author>Karl G D Bailey</author>
</authors>
<title>Disfluencies, language comprehension, and Tree Adjoining Grammars. Cognitive Science,</title>
<date>2004</date>
<pages>28--721</pages>
<contexts>
<context position="1154" citStr="Ferreira et al. (2004)" startWordPosition="166" endWordPosition="169">re likely to finish, and finishes them probabilistically with placeholder structure. These modified repair constituents and the restarted replacement constituent are then recognized together in the same way that two coordinated phrases of the same type are recognized. 1 Introduction Speech repair is a phenomenon in spontaneous spoken language in which a speaker decides to interrupt the flow of speech, replace some of the utterance (the “reparandum”), and continues on (with the “alteration”) in a way that makes the whole sentence as transcribed grammatical only if the reparandum is ignored. As Ferreira et al. (2004) note, speech repairs1 are the most disruptive type of disfluency, as they seem to require that a listener first incrementally build up syntactic and semantic structure, then subsequently remove it and rebuild when the repair is made. This difficulty combines with their frequent occurrence to make speech repair a pressing problem for machine recognition of spontaneous speech. This paper introduces a model for dealing with one part of this problem, constructing a syntactic analysis based on a transcript of spontaneous spoken language. The model introduced here differs from other models attempti</context>
</contexts>
<marker>Ferreira, Lau, Bailey, 2004</marker>
<rawString>Fernanda Ferreira, Ellen F. Lau, and Karl G.D. Bailey. 2004. Disfluencies, language comprehension, and Tree Adjoining Grammars. Cognitive Science, 28:721–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="10310" citStr="Godfrey et al., 1992" startWordPosition="1695" endWordPosition="1698">me tree as in Figure 1, with the unfinished noun phrase now given a placeholder NN completion (both bolded). Next, these trees are modified using the rightcorner transform as shown in Figure 4. This tree still contains placeholder words that will not be in the text stream of an observed input sentence. Thus, in the final step of the preprocessing algorithm, the finished category label and the placeholder right child are removed where found in a right-corner tree. This results in a right-corner transformed tree in which a unary child or right This model was evaluated on the Switchboard corpus (Godfrey et al., 1992) of conversational telephone speech between two human interlocuters. The input to this system is the gold standard word transcriptions, segmented into individual utterances. For comparison to other similar systems, the system was given the gold standard part of speech for each input word as well. The standard train/test breakdown was used, with sections 2 and 3 used for training, and subsections 0 and 1 of section 4 used for testing. Several sentences from the end of section 4 were used during development. For training, the data set was first standardized by removing punctuation, empty categor</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proc. ICASSP, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
<author>Izhak Shafran</author>
<author>Lisa Yung</author>
<author>Bonnie Dorr</author>
<author>Mary Harper</author>
<author>Anna Krasnyanskaya</author>
<author>Matthew Lease</author>
<author>Yang Liu</author>
<author>Brian Roark</author>
<author>Matthew Snover</author>
<author>Robin Stewart</author>
</authors>
<title>PCFGs with syntactic and prosodic indicators of speech repairs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</booktitle>
<contexts>
<context position="3518" citStr="Hale et al. (2006)" startWordPosition="540" endWordPosition="543">hown that syntactic cues can be used to increase accuracy of detection of reparanda, which can increase overall parsing accuracy. The first source of structure used to recognize repair is what Levelt (1983) called the “Well-formedness Rule.” This rule essentially states that a speech repair acts like a conjunction; that is, the reparandum and the alteration must be of the same syntactic category. Of course, the reparandum is often unfinished, so the Well-formedness Rule allows for the reparandum category to be inferred. This source of structure has been used by two related approaches, that of Hale et al. (2006) and Miller (2009). Hale and colleagues exploit this structure by adding contextual information to the standard reparandum label “EDITED”. In their terminology, daughter annotation takes the (possibly unfinished) constituent label of the reparandum and appends it to the EDITED label. This 277 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277–280, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP allows a learned probabilistic context-free grammar to represent the likelihood of a reparandum of a certain type being a sibling with a finished constituent of the same type. </context>
<context position="11978" citStr="Hale et al. (2006)" startWordPosition="1972" endWordPosition="1975">s edited if they are dominated by a node labeled EDITED, and measures the F-score of the hypothesized edited words relative to the gold standard. System Configuration Parseval-F Edited-F Baseline CYK 71.05 18.03 Hale et al. 68.48 37.94 Plain RC Trees 69.07 30.89 Elided RC Trees 67.91 24.80 Merged RC Trees 68.88 27.63 Table 1: Results Results of the testing can be seen in Table 1. The first line (“Baseline CYK”) indicates the results using a standard probabilistic CYK parser, trained on the standardized input trees. The following two lines are results from reimplementations of the systems from Hale et al. (2006) and Miller (2009). The line marked ‘Elided trees’ gives current results. Surprisingly, this result proves to be lower than the previous results. Two observations in the output of the parser on the development set gave hints as to the reasons for this performance loss. First, repairs using the slash categories (for unfinished reparanda) were rare (relative to finished reparanda). This led to the suspicion that there was a state-splitting phenomenon, where categories previously lumped together as EDITED-NP were divided into several unfinished categories (NP/NN, NP/NNS, etc.). To test this suspi</context>
</contexts>
<marker>Hale, Shafran, Yung, Dorr, Harper, Krasnyanskaya, Lease, Liu, Roark, Snover, Stewart, 2006</marker>
<rawString>John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary Harper, Anna Krasnyanskaya, Matthew Lease, Yang Liu, Brian Roark, Matthew Snover, and Robin Stewart. 2006. PCFGs with syntactic and prosodic indicators of speech repairs. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>632</pages>
<contexts>
<context position="5060" citStr="Johnson (1998)" startWordPosition="784" endWordPosition="785">nd the reparandum is retroactively labelled as such. Thus, the argument goes, no special syntactic rules or symbols should be necessary until the alteration begins. 3 Model Description 3.1 Right-corner transform This work first uses a right-corner transform, which turns right-branching structure into leftbranching structure, using category labels that use a “slash” notation α/,y to represent an incomplete constituent of type α “looking for” a constituent of type -y in order to complete itself. This transform first requires that trees be binarized. This binarization is done in a similar way to Johnson (1998) and Klein and Manning (2003). Rewrite rules for the right-corner transform are as follows, first flattening right-branching structure:2 then replacing it with left-branching structure: One problem with this notation is the representation given to unfinished constituents, as seen in Figures 1 and 2. The standard representation of 2Here, all Ai denote nonterminal symbols, and αi denote subtrees; the notation A1:α0 indicates a subtree α0 with label Al; and all rewrites are applied recursively, from leaves to root. S india Figure 1: Section of interest of a standard phrase structure tree containi</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representation. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="5089" citStr="Klein and Manning (2003)" startWordPosition="787" endWordPosition="790">s retroactively labelled as such. Thus, the argument goes, no special syntactic rules or symbols should be necessary until the alteration begins. 3 Model Description 3.1 Right-corner transform This work first uses a right-corner transform, which turns right-branching structure into leftbranching structure, using category labels that use a “slash” notation α/,y to represent an incomplete constituent of type α “looking for” a constituent of type -y in order to complete itself. This transform first requires that trees be binarized. This binarization is done in a similar way to Johnson (1998) and Klein and Manning (2003). Rewrite rules for the right-corner transform are as follows, first flattening right-branching structure:2 then replacing it with left-branching structure: One problem with this notation is the representation given to unfinished constituents, as seen in Figures 1 and 2. The standard representation of 2Here, all Ai denote nonterminal symbols, and αi denote subtrees; the notation A1:α0 indicates a subtree α0 with label Al; and all rewrites are applied recursively, from leaves to root. S india Figure 1: Section of interest of a standard phrase structure tree containing speech repair with unfinis</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Monitoring and self-repair in speech.</title>
<date>1983</date>
<journal>Cognition,</journal>
<pages>14--41</pages>
<contexts>
<context position="3106" citStr="Levelt (1983)" startWordPosition="475" endWordPosition="476">rged with existing structure. While this model is interesting for its elegance in representation, there is also reason to hypothesize improved performance, since this processing model requires no additional grammar symbols, and only one additional operation to account for speech repair, and thus makes better use of limited data resources. 2 Background Previous work on parsing of speech with repairs has shown that syntactic cues can be used to increase accuracy of detection of reparanda, which can increase overall parsing accuracy. The first source of structure used to recognize repair is what Levelt (1983) called the “Well-formedness Rule.” This rule essentially states that a speech repair acts like a conjunction; that is, the reparandum and the alteration must be of the same syntactic category. Of course, the reparandum is often unfinished, so the Well-formedness Rule allows for the reparandum category to be inferred. This source of structure has been used by two related approaches, that of Hale et al. (2006) and Miller (2009). Hale and colleagues exploit this structure by adding contextual information to the standard reparandum label “EDITED”. In their terminology, daughter annotation takes t</context>
</contexts>
<marker>Levelt, 1983</marker>
<rawString>Willem J.M. Levelt. 1983. Monitoring and self-repair in speech. Cognition, 14:41–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Miller</author>
</authors>
<title>Improved syntactic models for parsing speech with repairs.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics,</booktitle>
<location>Boulder, CO.</location>
<contexts>
<context position="3536" citStr="Miller (2009)" startWordPosition="545" endWordPosition="546">s can be used to increase accuracy of detection of reparanda, which can increase overall parsing accuracy. The first source of structure used to recognize repair is what Levelt (1983) called the “Well-formedness Rule.” This rule essentially states that a speech repair acts like a conjunction; that is, the reparandum and the alteration must be of the same syntactic category. Of course, the reparandum is often unfinished, so the Well-formedness Rule allows for the reparandum category to be inferred. This source of structure has been used by two related approaches, that of Hale et al. (2006) and Miller (2009). Hale and colleagues exploit this structure by adding contextual information to the standard reparandum label “EDITED”. In their terminology, daughter annotation takes the (possibly unfinished) constituent label of the reparandum and appends it to the EDITED label. This 277 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277–280, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP allows a learned probabilistic context-free grammar to represent the likelihood of a reparandum of a certain type being a sibling with a finished constituent of the same type. Miller’s approach </context>
<context position="11996" citStr="Miller (2009)" startWordPosition="1977" endWordPosition="1978">minated by a node labeled EDITED, and measures the F-score of the hypothesized edited words relative to the gold standard. System Configuration Parseval-F Edited-F Baseline CYK 71.05 18.03 Hale et al. 68.48 37.94 Plain RC Trees 69.07 30.89 Elided RC Trees 67.91 24.80 Merged RC Trees 68.88 27.63 Table 1: Results Results of the testing can be seen in Table 1. The first line (“Baseline CYK”) indicates the results using a standard probabilistic CYK parser, trained on the standardized input trees. The following two lines are results from reimplementations of the systems from Hale et al. (2006) and Miller (2009). The line marked ‘Elided trees’ gives current results. Surprisingly, this result proves to be lower than the previous results. Two observations in the output of the parser on the development set gave hints as to the reasons for this performance loss. First, repairs using the slash categories (for unfinished reparanda) were rare (relative to finished reparanda). This led to the suspicion that there was a state-splitting phenomenon, where categories previously lumped together as EDITED-NP were divided into several unfinished categories (NP/NN, NP/NNS, etc.). To test this suspicion, another expe</context>
</contexts>
<marker>Miller, 2009</marker>
<rawString>Tim Miller. 2009. Improved syntactic models for parsing speech with repairs. In Proceedings of the North American Association for Computational Linguistics, Boulder, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>