<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9989155">
Tree-Adjoining Grammar Parsing and
Boolean Matrix Multiplication
</title>
<author confidence="0.969394">
Giorgio Satta*f
</author>
<affiliation confidence="0.513544">
Universita di Venezia
</affiliation>
<bodyText confidence="0.99905525">
The computational problem of parsing a sentence in a tree-adjoining language is investigated. An
interesting relation is studied between this problem and the well-known computational problem of
Boolean matrix multiplication: it is shown that any algorithm for the solution of the former problem
can easily be converted into an algorithm for the solution of the latter problem. This result bears
on at least two important computational issues. First, we realize that a straightforward method
that improves the known upper bound for tree-adjoining grammar parsing is hard to find. Second,
we understand which features of the tree-adjoining grammar parsing problem are responsible for
the claimed difficulty.
</bodyText>
<sectionHeader confidence="0.990277" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999884458333333">
Among formalisms for the computation of syntactic description of natural language
sentences, Tree-Adjoining Grammars (TAG) play a major role. The class of TAG&apos;s was
first introduced in Joshi, Levy, and Takahashi (1975) and Joshi (1985); since then, formal
and computational properties of this class have been extensively investigated, and the
linguistic relevance of TAGs has been discussed in the literature as well. The reader
who is interested in these topics is referred to some of the most recent works, for
example Schabes (1990) and Frank (1992), and to the references therein.
Both in a theoretical vein and in view of possible natural language processing
applications, the recognition and parsing problems for TAGs have been extensively
studied and many algorithms have been proposed for their solution. On the basis of
tabular techniques, the least time upper bound that has been attested is 0(1 G11w16) for
the random-access model of computation, 1G1 being the size of the input grammar and
1w1 the length of the input string. In recent years, improvement of such a worst-case
running time has been a common goal for many researchers, but up to the present
time the TAG parsing problem has strongly resisted all such attempts. Because of the
record of all these efforts, the task of improving the above upper bound is actually
regarded as a difficult one by many researchers.
In support of such a common feeling, in this paper we restate the TAG parsing
problem as a search problem and relate it to the well-known computational prob-
lem of Boolean matrix multiplication. This is done in such a way that time upper
bounds for TAG parsing can be transferred to time upper bounds for the latter prob-
lem. More precisely, we show that any algorithm for TAG parsing that improves the
0(1 G 11w 16) time upper bound can be converted into an algorithm for Boolean ma-
trix multiplication running in less than 0(m3) time, m being the order of the input
</bodyText>
<footnote confidence="0.913489">
* Universita di Venezia, Scienze dell&apos;Informazione, via Torino, 155, 30172 Mestre—Venezia, Italy. E-mail:
satta@moo.dsi.unive.it.
t This research was done while the author was a post-doctoral fellow at the Institute for Research in
Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Philadelphia, PA 19104-6228, USA.
</footnote>
<note confidence="0.8752225">
© 1994 Association for Computational Linguistics
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.99975125">
matrices. Crucially, Boolean matrix multiplication has been the object of investigation
for many years: methods that are asymptotically faster than 0(m3) are known, but
the more considerable the improvement turned out to be, the more complex the in-
volved computation was found to be. At the present time, the asymptotically fastest
algorithms for Boolean matrix multiplication are considered to be only of theoretical
interest, because the huge constants involved in the running time of these methods
render prohibitive any practical application, given current computer hardware.
As a matter of fact, the design of practical algorithms for Boolean matrix multi-
plication that considerably improve the cubic time upper bound is regarded as a very
difficult enterprise. A consequence of the results presented in this paper is that TAG
parsing should also be considered as having the status of a problem that is &amp;quot;hard to
improve,&amp;quot; and there is enough evidence to think that methods for TAG parsing that
are asymptotically faster than 0(1G11w16) are unlikely to be of any practical interest,
i.e., will involve very complex computations.
The remaining part of this paper is organized as follows. The next section presents
the definition of tree-adjoining grammar and introduces the two computational prob-
lems that are to be related. Section 3 establishes the main result. Section 4 draws on
the computational consequences of such a result and reports some discussion. Finally,
Section 5 concludes by indicating how similar results can be found for variants of the
TAG parsing problem that have been recently discussed in the literature.
</bodyText>
<sectionHeader confidence="0.979833" genericHeader="keywords">
2. Preliminaries
</sectionHeader>
<bodyText confidence="0.999906333333333">
This section introduces the Boolean matrix multiplication problem and the tree-adjoin-
ing grammar parsing problem, along with the definition of tree-adjoining grammar.
The notation presented here will be used throughout the paper.
</bodyText>
<subsectionHeader confidence="0.998131">
2.1 Boolean Matrix Multiplication
</subsectionHeader>
<bodyText confidence="0.9753827">
Let B = {0, 1} be the set of truth values. The logical symbols V, A are defined as usual.
The set of Boolean square matrices B„„ m &gt; 1, is defined as the set of all m x m square
matrices whose elements belong to B. Given a matrix A E Bin, we say that A has order
m; the element in the ith row and jth column of A is denoted by au. In B„,, the product
of A and B, written A x B, is a Boolean matrix C such that:
= V aik A bki, 1 &lt; i,j &lt; m. (1)
An instance of the Boolean matrix multiplication problem is therefore a pair (A, B)
and the solution to such an instance consists of the matrix C such that C = A x B. In
what follows BMM will denote the set of all possible instances of the Boolean matrix
multiplication problem.
</bodyText>
<subsectionHeader confidence="0.999378">
2.2 Tree-Adjoining Grammars
</subsectionHeader>
<bodyText confidence="0.999939">
The definition of TAG and the associated notion of derivation are briefly introduced
in the following; the reader is also referred to the standard literature (see, for instance,
Vijay-Shanker and Joshi [1985] or Joshi, Vijay-Shanker, and Weir [19911).
A tree-adjoining grammar is a tree rewriting system denoted by a tuple G =
(VN, VT, S, I, A), where VN and VT are finite, disjoint sets of nonterminal and terminal
symbols respectively, S e VN is a distinguished symbol, and I and A are finite sets of
elementary trees. Trees in I and A are called initial and auxiliary trees respectively
and meet the following specifications. Internal (nonleaf) nodes in an elementary tree
</bodyText>
<page confidence="0.993101">
174
</page>
<figure confidence="0.999025111111111">
Giorgio Satta Tree-Adjoining Grammar Parsing
auxiliary tree
initial tree
z.S
terminal nodes
(a)
&gt;
A
(b)
</figure>
<figureCaption confidence="0.6923225">
Figure 1
Definitions of (a) initial and auxiliary trees and (b) adjunction operation.
</figureCaption>
<bodyText confidence="0.999620363636363">
are labeled by symbols in VN. An initial tree has a root labeled by S and leaf nodes
labeled by symbols in VT U {E}. An auxiliary tree has leaf nodes labeled by symbols
in VT U {e} with the addition of one node, called the foot node, having the same
nonterminal label as the root node (see Figure la). We define the size of G, written
I G I, to be the total number of nodes in all the trees in the set I U A. In what follows
we will also denote by TAG the class of all tree-adjoining grammars.
In TAG, the notion of derivation is based on a composition operation called ad-
junction, defined in the following way. Let -y be an auxiliary tree having its root (and
foot node) labeled by A e VN. Let also 7&apos; be any tree containing a node 7/ labeled by
A, and let 7 be the subtree of -y&apos; rooted in n. The adjunction of -y into -y&apos; at node n
results in a tree specified as follows (see Figure lb):
</bodyText>
<listItem confidence="0.8435842">
(i) the subtree 7 is excised from -y&apos;;
(ii) the auxiliary tree -y replaces 7 in -y&apos;, with the root of -y replacing the
excised node n;
(iii) the subtree 7 is attached to the resulting tree, with the foot node of -y
replacing n in 7.
</listItem>
<bodyText confidence="0.99751675">
In TAG a derivation is the process of recursive composition of elementary trees using
the adjunction operation; the resulting trees are called derived trees. Since adjunctions
at different nodes can be performed in any order, we can adjoin derived trees into
derived trees without affecting our arguments.
</bodyText>
<page confidence="0.993757">
175
</page>
<figure confidence="0.730952">
Computational Linguistics Volume 20, Number 2
</figure>
<figureCaption confidence="0.990167">
Figure 2
</figureCaption>
<bodyText confidence="0.8497515">
Parse tree -y&apos; is included in a parse tree of string w zixz2yz3 in L(G). We say that the
derivation of string pair (x, y) is a subderivation of a sentential derivation of w.
</bodyText>
<subsectionHeader confidence="0.999977">
2.3 Tree-Adjoining Grammar Parsing
</subsectionHeader>
<bodyText confidence="0.998826076923077">
In order to introduce the definition of the TAG parsing problem on which our results
are based, we define in the following the string language derived from a TAG and
discuss the notion of parse forest along with the important issue of its representation.
Given an alphabet V. we denote by V* the set of all finite strings over V (null string
e included).
Although TAG is a class of tree rewriting systems, a derivation relation can be
defined on strings in the following way. Let 7 be an elementary tree and let -y&apos; be
a tree obtained from 7 by means of zero or more adjunction operations. If the yield
of 7&apos; is a string x E WI% that is -y E I, we say that -y derives x in G. If the yield of
7&apos; is a string xAy e Vil7NV&apos;4,, that is -y E A, we say that -y derives the pair (x, y) in
G. In particular, the set of all strings in VI. that can be derived in G is denoted by
L(G). In this perspective then, an elementary or a derived tree is seen as a structural
description of a string (a pair of strings) derived by the grammar; such a description
is called a parse tree. The space of all parse trees associated with a given string by the
grammar is called a parse forest.
We introduce now the notion of subderivation. Let w be a sentence in L(G) and
let 7 E A derive the pair (x, y) in G, x, y E Vi% with an associated parse tree -y&apos;. If -y&apos; is
included in a parse tree of w, we have that w = zi xz2yz3 for some z1, z2, z3 E V:;, (see
Figure 2). Parse tree 7&apos; represents the contribution of auxiliary tree -y to a derivation
of w; we say therefore that the derivation of (x, y) from -y is a subderivation of a
sentential derivation of w. As a consequence of the definition of parse forest, we have
that all subderivations of the sentential derivations of w can be read off from the parse
forest of w. Part of this information will be used to establish our result, as precisely
stated in the next definition. We need some additional notation. Let w = d1d2 • dn.
n &gt; 0, be a string over some alphabet; symbol pwq denotes the substring dpdp±i • • • dq
for 1 &lt; p &lt; q &lt; n and is undefined otherwise.
</bodyText>
<subsectionHeader confidence="0.509982">
Definition 1
</subsectionHeader>
<bodyText confidence="0.456252666666667">
Let G = (VN, VT S, I, A) be a tree-adjoining grammar and w c V; be an input string,
1w 1= n, n &gt; 0. A parse relation Rp C A x {1..04 associated with the pair (G,w) is
specified as follows. For every auxiliary tree -y in G and for natural numbers p, q, r and
</bodyText>
<equation confidence="0.392062">
1 7A
Giorgio Satta Tree-Adjoining Grammar Parsing
s,1&lt;p&lt;q&lt;r&lt;s&lt; n, R(-y,p,q,r,$) holds if and only if:
</equation>
<listItem confidence="0.7641695">
(i) the pair (pwq, rws) can be derived by in G, and
(ii) the derivation in (i) is a subderivation of a sentential derivation of w in G.
</listItem>
<bodyText confidence="0.999979076923077">
The goal of a parsing algorithm for TAG is one of constructing a &amp;quot;suitable&amp;quot; rep-
resentation for the parse forest of a given string, with respect to a given grammar.
However, there is no common agreement in the literature on the requirements that
such a representation should meet; therefore the issue of the representation of a parse
forest deserves some discussion here.
There seems to be a trade-off between computational time• and space in choosing
among different representations of a parse forest. Note that, from an extreme perspec-
tive, the input itself can be considered as a highly compressed representation of the
parse forest—one that needs a time-expensive process for parse tree retrieval.&apos; More
explicit representations offer the advantage of time-efficient retrieval of parse trees,
at the cost of an increase in storage resources. In practice, most commonly used al-
gorithms solve the parsing problem for TAGs by computing a superset of a parse
relation (defined as above) and by representing it in such a way that its instances
can be tested in constant time; such a condition is satisfied by the methods reported
in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei
(1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and
Weir (1993). From such a representation, time-efficient computations can be used later
to retrieve parse structures of the input string.
On the basis of the previous observation, we assume in the following that the
solution of the parsing problem involves (at least) the computation of a representation
for Rp such that its instances can be tested in constant time: we base our results on
such an assumption. More precisely, an input instance of the tree-adjoining grammar
parsing problem is defined to be any pair (G, w), and the unique solution of such an
instance is provided by an explicit representation of relation (set) Rp associated with
(G, w) as in Definition 1. In what follows, TGP will represent the set of all instances
of the tree-adjoining grammar parsing problem.
</bodyText>
<sectionHeader confidence="0.819898" genericHeader="introduction">
3. Technical Part
</sectionHeader>
<bodyText confidence="0.99991675">
In this section the Boolean matrix multiplication problem is related to the tree-adjoining
grammar parsing problem, establishing the major result of this paper. A precise spec-
ification of the studied reduction is preceded by an informal discussion of the general
idea underlying the construction.
</bodyText>
<subsectionHeader confidence="0.999953">
3.1 The Basic Approach
</subsectionHeader>
<bodyText confidence="0.999798285714286">
Two maps .F and g will be studied in this section. Map Y establishes a correspondence
between the set BMM and a proper subset of TGP containing, in some sense, its most
difficult instances. Conversely, map g is defined on the set of solutions of all TGP
problems in the image of .F, and gives values in the set of Boolean square matrices.
Maps .F and g are defined in such a way that, given any algorithm for the solution
of the TGP problem, we can effectively construct an algorithm for the solution of the
BMM problem using the commutative diagram shown in Figure 3.
</bodyText>
<footnote confidence="0.722886">
1 I owe this observation to Bernard Lang (personal communication).
</footnote>
<page confidence="0.960722">
177
</page>
<figure confidence="0.942536571428571">
Computational Linguistics Volume 20, Number 2
multiplication
&lt;A, B&gt; in BMM C = AYB
5- I
1
parsing
&lt;G, w&gt; in TGP Rp
</figure>
<figureCaption confidence="0.73061">
Figure 3
Maps F and g define a commutative diagram with respect to any algorithm for Boolean
matrix multiplication and any parsing algorithm for tree-adjoining grammars.
</figureCaption>
<bodyText confidence="0.9999116">
Both the BMM and the TGP problems are viewed here as search problems whose
solutions are obtained by exploring a search space of elementary combinations. In
the case of the BMM problem, the elementary combinations are the combinations
of elements of the input matrices. If m is the order of these matrices, the solution
of the problem requires the specification of 0(m2) elements of the product matrix,
where each element depends upon 0(m) elementary combinations (see relation (1)).
Therefore the problem involves a search in a space of 0(m3) different combinations.
On the other hand, in the TGP problem the elementary combinations are taken to
be single applications of the adjunction operation. In parsing a string w of length n
according to a tree-adjoining grammar G, we have to construct a parse relation of
size 0(1G ln4) (see Definition 1), and there are 0(n2) distinguishable combinations in
which each element of the relation can be obtained. In the general case then, a number
0(1G II/6) of distinguishable combinations are involved in the parsing problem, and
we have to perform a search within an abstract space of this size.
In order to achieve our result, we then establish a size preserving correspondence
between the two search spaces above. There is no way of representing matrices A
and B within string w without blowing up the search space associated with the target
parsing problem. Our choice will then be to represent the input matrices by means
of grammar G, which fixes IG I to a quantity 0(m2). This forces the choice of n to a
quantity 0(m4), obtaining therefore the desired relation I G In6 = 0(m3).
The general idea underlying the construction is the following one. Observe that
non-null elements aik and bej in the input matrices force element c,j to value 1 in the
product matrix if and only if k = k&apos;. The check of such a condition can be transferred
to the computation of an adjunction operation in the target parsing problem using the
following encoding method. We fix a positive integer b to a (rounded) quantity mi.
Then we encode each index i of the input matrices by means of positive integers ii,
i2, and i3, such that i1 is 0(0) and i2, i3 are 0(b). Condition k = k&apos; above is therefore
reduced to the three tests kh = kh&apos;, 1 &lt;h &lt;3, which can be performed independently.
The test k1 = k is precompiled into some auxiliary tree of G; the tests k2 = k and
k3 =k&apos;3 are performed by the parser using the input string, as explained below.
Map .T constructs a string w of distinguishable symbols by concatenating six
&amp;quot;slices&amp;quot; w(h ), 1 &lt; h &lt; 6, each slice of length 0(b). Map .F also encodes the input
matrices A and B within the target grammar G; it does so by transforming each non-
null element in the input matrices into an auxiliary tree of G in the following way.
Non-null element a,k is mapped into an auxiliary tree 1yi having its root (and foot node)
</bodyText>
<page confidence="0.96044">
178
</page>
<figure confidence="0.9873022">
Giorgio Satta Tree-Adjoining Grammar Parsing
x2 Y2
1-4-1III&apos; I L.. II.. I w
k3+1 I. k2 k2+1
33 32
</figure>
<figureCaption confidence="0.994473">
Figure 4
</figureCaption>
<bodyText confidence="0.999566689655172">
String w is composed of six slices, and auxiliary trees corresponding to non-null elements a,k
and be, derive string pairs (xi, yl) and (x2, y2) matching the slices of w as shown above;
integers are used to indicate the position within a single slice of the boundary symbols in
strings x, y, x2, and y2. The figure depicts the case k = k&apos;, resulting in the exact nesting of the
two derived trees.
labeled by a symbol including integers i1 and kl. Moreover, 71 will eventually derive
a string pair (xi, yl) with the following property. String x1 is the smallest substring of
w including the symbol in the i3th position within slice w(1) and the symbol in the
k3th position within slice w(2). Furthermore, string yi is the smallest substring of w
including the symbol in the (k2 + 1)-th position within slice w(5) and the symbol in the
izth position within slice 06). This is schematically shown in Figure 4.
At the same time F maps non-null element biej into an auxiliary tree 72 having
its root labeled by a symbol including integers k and h. Crucial to our construction,
-y2 will derive a pair of strings (x2, y2) with the following property. String x2 is the
smallest substring of w including the symbol in the (1e3 + 1)-th position within slice
w(2) and the symbol in the j3th position within slice w(3). Furthermore, string y2 is the
smallest substring of w including the symbol in the j2th position within slice w(4) and
the symbol in the le2th position within slice w(5). Let us call and the derived trees
obtained from and -y2 as above. Observe that k2 = k and k3 = k&apos;3 if and only if the
yields of and are exactly nested within w; see again Figure 4.
To complete the construction of G, map .F provides an auxiliary tree 73 with the
following property. Tree -y3 can contribute to a sentential derivation of w in G if and
only if and can be adjoined to it. This is in turn possible just in case integer ki in
the root of 71. and integer k in the root of 72 coincide, as specified by the adjunction
sites in 73, and the yields of and are exactly nested within w. It follows that, by
deciding whether -y3 contributes to a sentential derivation of w, the parser is able to
perform the required test k = k&apos;. Finally, index k in its coded form is discarded in the
derivation process above, while indices i and j are preserved in such a way that map
g can eventually recover non-null element c1 by reading off the parse relation.
</bodyText>
<page confidence="0.995315">
179
</page>
<note confidence="0.512587">
Computational Linguistics Volume 20, Number 2
</note>
<tableCaption confidence="0.771834">
Table 1
Values of f (b) (i) for b = 3 and 1 &lt; i &lt; 15.
</tableCaption>
<bodyText confidence="0.82138475">
i 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
.fib)(i) 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2
.f.1&apos;)(i) 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2
.fib)(i) 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3
The next section presents a detailed specification of maps .T and g and proves
the above claimed properties. As we will see, the search space defined by the result-
ing instance of the TAG parsing problem includes the solution of the source matrix
multiplication problem.
</bodyText>
<subsectionHeader confidence="0.999944">
3.2 The Two Maps
</subsectionHeader>
<bodyText confidence="0.996353545454545">
The goal of this subsection is to establish a mapping between comparisons of matrix
indices in Boolean matrix multiplication and instances of the adjunction operation in
tree-adjoining grammar parsing. As already mentioned in the previous subsection,
this result is achieved by encoding natural numbers using three positive integers. The
encoding is then used to chop off matrix indices into smaller numbers that will be
processed independently. This is explained in detail in the following.
For pairs of integers i and b, let qn(i, b) and rm(i, b) be the quotient and the
remainder respectively of the integer division of i by b. We define qn(i, b) = qn(i, b) +1
whenever rm(i, b) 0 and qn+(i, b) = qn(i, b) otherwise; we also define rm+(i, b) =
rm(i, b) whenever rm(i, b) 0, and rm+(i, b) = b otherwise. Note that, for i &gt; 1,
qn(i, b) &gt; 1 and 1 &lt; rm+(i, b) &lt; b.
</bodyText>
<subsectionHeader confidence="0.76984">
Definition 2
</subsectionHeader>
<bodyText confidence="0.999547">
Let b&gt; 1 be an integer. We associate with b a function f (b) defined on the set of positive
natural numbers, specified as follows:
</bodyText>
<equation confidence="0.708002">
f (b) (i) (fib) (0 , f?) (0 (i)),
</equation>
<bodyText confidence="0.815176">
where
</bodyText>
<equation confidence="0.988349">
(b)
fl (1)
c(b)
I 2 I
c(b)
J 3 111 = rm+ (i, b).
</equation>
<tableCaption confidence="0.620433">
Table 1 shows some values of f (b) for the case b = 3.
</tableCaption>
<bodyText confidence="0.998294">
Observe that, for i &gt; 1, _Kb) and .e) give values in the range {1..b}, while f1(b)
can give any positive integer. It is not difficult to see that function f(&apos;) establishes a
one-to-one correspondence between the set N of positive natural numbers and the
set N x {1..b} x {1..b}. In an informal way, we will often refer to value f(i) as the
most significant digit corresponding to i, and to values .Kb) (i) and .e) (i) as the least
significant digits corresponding to i. In the following, the superscript in PO will be
omitted whenever b can be understood from the context.
We are now in a position to define in detail the maps F and g involved in the
diagram of Figure 3 discussed in Section 3.1. As a first step, we study map T that
takes as input an instance of BMM and returns an instance of TGP.
</bodyText>
<equation confidence="0.566133">
qn(i, b2),
qn+ (rm+ (i, b2), b),
</equation>
<page confidence="0.652242">
180
</page>
<figure confidence="0.999394090909091">
Giorgio Satta Tree-Adjoining Grammar Parsing
(n)
F3.
&lt;C, U, V&gt;
&lt;A, u, t&gt;
&lt;C, u, v&gt;
&lt;B, t, v&gt;
(n) (n)
T5
dp
dp
</figure>
<figureCaption confidence="0.764155333333333">
Figure 5
Definition of families of auxiliary trees n &gt; 1 and 1 &lt; h &lt; 6. Each tree in some class is
specified by the values of the integer parameters corresponding to that class.
</figureCaption>
<bodyText confidence="0.8521242">
Let n &gt; 1 be an integer. In the following we will refer to sets of terminal symbols
14n) = op 1 &lt;p &lt;6(n + 1)1,
and to sets of nonterminal symbols
14,7) = {(A, u, v), (B,u,v),(C,u,v) 1 &lt; u &lt; v &lt; n41 U {C,D,S}.
Based on these sets, Figure 5 defines families of auxiliary trees F/Y), 1 &lt; h &lt; 6. For
</bodyText>
<equation confidence="0.604054">
(n)
</equation>
<bodyText confidence="0.9362307">
example, an auxiliary tree -y(p,q,r,s,u,v) E riwill be specified by providing actual
values for the integer parameters p, q, r, s, u, and v, consistently with the definitions
of sets Vrn) and 14,7). In the following we will also use the initial tree -y, depicted in
Figure 6.
The next definition introduces map .F, which is the core component of the pro-
posed reduction. The definition is rather technical: it will be followed by a more
intuitive example.
Definition 3
Let (A, B) be an instance of BMM, m the order of matrices A and B. Let also n = m]+1
and a = n + 1. A map F is specified in such a way that .F((A,B)) = (G, w), where
</bodyText>
<equation confidence="0.785372">
G =- (Vi(1) ,17-(rn) S I A(n)) and w = d1d2 ••d6a. Set I contains the only initial tree 7, ; set
</equation>
<page confidence="0.967357">
181
</page>
<figure confidence="0.67572">
Computational Linguistics Volume 20, Number 2
</figure>
<figureCaption confidence="0.707091">
Figure 6
</figureCaption>
<bodyText confidence="0.147906">
Definition of initial tree &apos;ye.
</bodyText>
<equation confidence="0.863272">
A(n) contains all and only the following elementary trees (fin) = fh, 1 &lt; h &lt; 3):
(i) for every aq = 1 in A, the auxiliary tree
(p , q, r, s, u, v) E
belongs to A(n) , where
p= f3(i), q= a+f3(j1
r = ±f2(j) + 1, s = 5cr ±f2(),
u= MO, v= fl (i);
(ii) for every b = 1 in B, the auxiliary tree
-y(p, q, r, s, u, v) E
belongs to A(n) , where
p = o- + f3(i) + 1, q= 2o- + f3 (j),
r= 3o- + f2(i), s= 4o- + f2(i),
u= v=
(iii) for every pair (A, u, t), (B, t, E V, the auxiliary trees
t , v) e
7(u, v) E r4(n)
belong to
(iv) for every 1 &lt;p &lt;6o-, the auxiliary trees
7(P) E 1(n)
-y(p) Er&apos;n)
</equation>
<bodyText confidence="0.930777833333333">
belong to
In order to have a better understanding of map ,T and of the idea underlying grammar
G and string w, we discuss in the following a simple example, adding more details to
the informal discussion presented in Section 3.1.
Let us define a Boolean matrix by specifying only its non-null elements. Assume
then that an input instance (A, B) of the BMM problem consists of two matrices of
</bodyText>
<figure confidence="0.976103814814815">
Ys:
182
Giorgio Satta Tree-Adjoining Grammar Parsing
a2,15
A :
mapping
B :
b15,7
Imultiplication
C:
C2,2
&lt;A,1,2&gt;
d2
d44.3 &lt;4,1,2&gt;
Y2:
&lt;B,2,1&gt;
4+3+1
d84.1 &lt;B,2,1&gt;
y3 &lt;C,i/,/&gt;
d2c4.2
d16+2+1 &lt;A,i1,2&gt;
&lt;B,2,1&gt;
&lt;C,1,1&gt;
d16+2
d12+3
Iadjunction
(a) (b)
</figure>
<figureCaption confidence="0.99951">
Figure 7
</figureCaption>
<bodyText confidence="0.9091824">
Part (a) shows how non-null elements (12,15 and 1215,7 in matrices A and B combine together,
forcing element c2,7 in matrix C to value 1; each array element is represented as an arc in a
directed graph. Correspondingly, trees -Y2, -y3 and 74 in (b) are introduced in G by map F.
These trees can be composed, using adjunction, with trees in IT) and IT), in such a way that
a derived tree is obtained that matches string w and encodes the indices of c2,7.
</bodyText>
<equation confidence="0.866951">
order m = 64, specified as
A = {a2,15}, B = 015,71.
</equation>
<bodyText confidence="0.999712642857143">
The multiplication of matrices A and B results in matrix C consisting of the only non-
null element c2,7. As already mentioned in Section 3.1, the multiplication process can
be seen as a test for equality performed on the second index of a2,15 and the first index
of b15,7; in the following these indices will be called &amp;quot;intermediate&amp;quot; indices. Element
c2,7 in the product matrix is forced to value 1 if this test succeeds, and the intermediate
indices are discarded in the process. See Figure 7a for a schematic representation of
such an operation.
In performing an adjunction operation, two requirements must be satisfied. First,
the nonterminal label at the adjunction site must match the nonterminal label at the
root (and foot node) of the adjoined tree; and second, adjunction must compose trees
in such a way that the derived string is compatible with w. In the proposed reduc-
tion, each test for equality performed on some pair of intermediate indices by matrix
multiplication is transferred to an adjunction operation by map .F, using as targets the
two requirements just described. This is exemplified in the following.
</bodyText>
<page confidence="0.997897">
183
</page>
<note confidence="0.755194">
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.999059135135135">
According to Definition 3, we find n = 3 and a = 4. Map .7- then constructs
a string w = c/2 • • • d24, which can be thought of as composed of six slices
4(h-1)+1W4hr 1 &lt; h &lt; 6. Each element in a single slice will be used as a placeholder
to record information about matrix indices. Furthermore, map .F exploits function f (3)
(see Table 1) in order to map each non-null element of the input matrices to an auxiliary
tree in I&apos; or F3) (steps (i) and (ii) in Definition 3). More specifically, each index of a
non-null element is converted into three digits: the most significative one is encoded
as part of the nonterminal symbols, and the two least significative digits are encoded
by the terminal symbols in the target tree. Trees and 72 obtained in this way from
non-null elements a2,15 and b15,7 respectively have been depicted in Figure 7b. Two
additional trees 73 E F3) and 74 e IT) have been reported in the figure, that are also
added to G by F (step (iii) in Definition 3).
Crucial to our construction, the test on the intermediate indices of elements a2,15
and b15,7 has been reduced to three independent tests involving smaller integers. More
precisely, the equality test on the most significative digits obtained from the intermedi-
ate indices has been transferred to the requirement on the matching of the nonterminal
labels of the nodes involved in the adjunction. In fact, 71 and 72 can be adjoined into
73 just in case such a test is satisfied. At the same time, the equality test on the
least significative digits obtained from the intermediate indices has been transferred
to the requirement on the matching of the derived string with w. In fact, after the
adjunction of 71 and 72 into 73 takes place, no terminal symbol can intervene between
the internal boundaries of the yield of 71 and the external boundaries of the yield
of 72 in slices w(2) and w(5) (see again Figure 7b). Then 73 can participate in a sen-
tential derivation of w just in case all three equality tests above are simultaneously
satisfied.
The choice of the order of I w I has been dictated by general considerations on
the size of the search spaces associated with the two problems at hand, as already
discussed in Section 3.1. As a note, we observe that slices W(2) and w(5) are used in the
above construction to pair together least significative digits obtained from intermediate
indices. The fact that these indices have range in {1..n} forces the choice of the length
of these slices to a = n + 1; the example in Figure 7 actually uses the (n + 1)-th symbol
of w(2). For uniformity, this value has then been extended to all other slices, fixing I w I
to 6u.
In Lemma 1 below we will state in a more precise way the above arguments, and
we will also show how derivations of the kind outlined above are the only derivations
in G than can match string w, proving therefore the correctness of the reduction. To
complete the diagram of Figure 3, we now turn to the specification of map g.
</bodyText>
<subsectionHeader confidence="0.811402">
Definition 4
</subsectionHeader>
<bodyText confidence="0.993973">
Let (G, w) be an instance of TGP in the image of map F, and let m, n, and a be as in
Definition 3. Let also Rp be the parse relation that solves instance (G, w). A map g is
specified in such a way that g(R) = C, C a Boolean matrix of order m, and element cu
is non-null if and only if Rp(-y,p,q,r, s) holds for an auxiliary tree -y(u, v) E lnin), where
</bodyText>
<equation confidence="0.9975906">
r(n)
Vh =fh, 1 &lt;h &lt;3)
P = f3(i), q = 2a + f3(j),
r = 3a + f2(j), s = 5a + f2(i),
u= v =
</equation>
<bodyText confidence="0.97993">
In the above definition, function f (n) is used to retrieve the indices of non-null elements
</bodyText>
<page confidence="0.988798">
184
</page>
<figure confidence="0.977305666666667">
6iorgio Satta iree-Aajoining urammar rarsing
,k1 k1
derive
I &apos;vs&amp;quot; Hug&apos; 1114&apos;3) II Iv(t&apos;ll wP (6)
it w:I
k2+1
k2
i3
./3
</figure>
<figureCaption confidence="0.858644">
Figure 8
Non-null elements a,k and big are mapped into auxiliary trees -yl and -y2 in G, and trees 71 and
&amp;quot;YZ can successively be obtained compatibly with string w. As a convention, symbols öft,
6 E fi,k,j1 and 1 &lt;h &lt;3, denote integers fh(6), which indicate either positions within each
single slice or components of nonterminal symbols labeling tree nodes.
</figureCaption>
<bodyText confidence="0.9948089">
of matrix C. In this case also, the most significative digits associated with the retrieved
indices are encoded within the nonterminal symbols of the auxiliary tree -y(u,v), while
the two least significative digits are encoded by the position of the yield boundaries
of the string derived from -y(u,v) consistently with the input string w. To conclude our
previous example, we see that if we apply the relations in Definition 4 to the derived
tree at the bottom of Figure 7b, we get indices 2 and 7 of the only non-null element
in C.
The following result shows that any algorithm for the solution of a generic instance
of TGP can be converted into an algorithm for the solution of the BMM problem, via
the computation of maps .T and g. This concludes the present section.
</bodyText>
<subsectionHeader confidence="0.769292">
Lemma 1
</subsectionHeader>
<bodyText confidence="0.879797666666667">
Let (A, B) be an instance of BMM and let (G,w) = ,F((A,B)). Let also Rp be the parse
relation that solves (C, w). Then we have
A x B = g(Rp).
</bodyText>
<subsectionHeader confidence="0.751692">
Proof
</subsectionHeader>
<bodyText confidence="0.9209054">
Assume that in is the order of the matrices A and B, n is the natural number associated
with m as in Definition 3, and a = n +1. Let C=AxB and C&apos; = g(Rp).
To prove Cu = 1 implies 4 = 1, we go through a sentential derivation of w in G
and then apply the definition of G. If cij = 1, then there exists k, 1 &lt;k &lt; m, such that
a,k = bki = 1. Let and -y2 be the unique auxiliary trees in G associated by map .F
</bodyText>
<page confidence="0.991821">
185
</page>
<figure confidence="0.9694185">
Computational Linguistics Volume 20, Number 2
,ki
</figure>
<figureCaption confidence="0.992471333333333">
Figure 9
Tree is derived from trees and auxiliary trees in G. We use the same conventions as in
Figure 8.
</figureCaption>
<bodyText confidence="0.9984742">
with a,k and bkj respectively (steps (i) and (ii) in Definition 3). Tree has root (and
foot node) labeled by nonterminal (A,fi(i),fi(k)); furthermore, the terminal symbols
in the yield of Yi are (from left to right) df,(,), da+f3(k), d4,±f2(0+1 and d5,+,6(). The only
pair of substrings of w that -yl can derive, by means of zero or more adjunctions of
trees in and F (6n), is
</bodyText>
<equation confidence="0.80521">
(i3(i)Wu-Ff3(k))4a-f-f2(k)+1W50-d-f2(0) •
</equation>
<bodyText confidence="0.9860645">
Call -4 a parse tree associated with such a derivation (see Figure 8). In a similar way,
auxiliary tree 72 has root labeled by nonterminal (B,fi(k),fi(j)) and derives pair
</bodyText>
<equation confidence="0.696078">
(a +f3(k)+1W2cr H13(j) 5 30--Pf2(i)W40--1-f2(0)
</equation>
<bodyText confidence="0.920661">
of substrings of w. Call &apos;)4 a parse tree associated with the derivation (see again Fig-
ure 8).
According to step (iii) in Definition 3, grammar G also includes auxiliary trees
</bodyText>
<equation confidence="0.660415">
1/3 = (i),f1 (k),f1 (i) r&apos;n.) and 74 = -y(fi (j)) c rin). Note that the yields of trees
</equation>
<bodyText confidence="0.99789325">
and 14 are exactly nested within w; moreover, the root (and the foot) nodes of
and 72 have been preserved in the derivation. Therefore 71 and 14 can be adjoined into
73 and the resulting tree can in turn be adjoined into -y4 . In this way, -y4 derives the
pair of substrings of w
</bodyText>
<equation confidence="0.580597">
(f3(0w2o-d-h(j),30-d-hu) w50-+f2(i)).
</equation>
<bodyText confidence="0.77310575">
Call -y!, the resulting derived tree (see Figure 9). Since derived tree -4 can be adjoined
into 7s in G and a tree can be eventually derived for the input string w, we have
Rp (74 ,f3 (i), 2o- + f3 (j),30- +f2(j),50&amp;quot; +f2(i) )
and from the definition of g we get c = 1.
</bodyText>
<page confidence="0.995116">
186
</page>
<note confidence="0.285847">
Giorgio Satta Tree-Adjoining Grammar Parsing
</note>
<bodyText confidence="0.995666">
Assuming c;j -= 1, we now prove cu = 1; this is done by arguing that the only
sentential derivations for w that are allowed by G are those of the kind outlined
above. From the definition of g we have that
</bodyText>
<equation confidence="0.965875">
Rp (-y4,f3 + f3(j), 30- + f2(j) , 50- + f2(i))
</equation>
<bodyText confidence="0.998077">
holds for the auxiliary tree 74 = (i), (j)) E lt). Equivalently, there exists at least
one derivation from 74 of strings
</bodyText>
<equation confidence="0.7710905">
(X, y) = (h(ow27:,+h(o, 3cr+f2 (j) W5a -112 (i) (2)
that participates in a sentential derivation of w. Fix such a derivation.
</equation>
<bodyText confidence="0.998961666666667">
We first observe that, in order to derive any terminal symbol from 74, auxiliary
trees in 117), l&apos;n) and 1-&apos;n) must be used. Any tree in lin) can only derive symbols
in slices Oh) , h E {1,2,5,6}, and any tree in 1-&apos;n) can only derive symbols in slices
w(h), h E {2,3,4,5}. Therefore at least one tree in lin) and at least one tree in
must be used in the derivation of (x, y) since (x, y) includes terminal symbols from
every slice of w. Furthermore, if more than one tree in Fl(n) is used in a derivation in
G, the resulting string cannot match w. The same argument applies to trees in F.
We must then conclude that exactly one tree in r.(In), one tree in 1-&apos;n), and one tree
in 1-&apos;n) have been used in the derivation of (x, y) from 74. Call the above trees 71 =
</bodyText>
<equation confidence="0.976548">
(n)(n)
1/(p, k3, k2+1,s, u, ki) e F,72 = -y (e3+1, q , r , kip v) e F2 , and -y3 = -y(u&apos; , t, v&apos;) e F.
</equation>
<bodyText confidence="0.946188444444444">
.
As a second step, we observe that 73 can be adjoined into 74 only if u&apos; = fi (i) and
v&apos; = fi (j) and 73 can host 71 and 72 just in case u&apos; -= u, v&apos; = v, and k1 = k = t. We
also observe that, after these adjunctions take place, the leftmost terminal symbol in
the yield of 74 will be the leftmost terminal symbol in the yield of 71, that is dp. From
relation (2) we then conclude that p = h(t). Similarly, we can argue that q -= 2o- + f3(j),
r = 3o- + f2(j) and s = 50- + f2(i). Finally, adjunction of 71 and 112 into -y3 can match w
just in case k3 = lc; and k2 = k12.
From the relations inferred above, we conclude that we can rewrite 71 as 7V3 (i), k3,
</bodyText>
<equation confidence="0.910296">
k2 +1,50- + f2 (0,ki) E 17(in) and 72 as 7(k3 + 1, + f3(j), 3°- + (j)) E F.
</equation>
<bodyText confidence="0.968888666666667">
Since f is one-to-one and k2, k3 E {1..0, there exists k such that f (k) = (k1,k2,k3). From
steps (i) and (ii) in Definition 3, we then have that aik and bk./ are non-null and then
cif = 1.
</bodyText>
<sectionHeader confidence="0.760172" genericHeader="method">
4. Computational Consequences
</sectionHeader>
<bodyText confidence="0.9999665">
The results presented in the previous section are developed here under a computa-
tional perspective. Some interesting computational consequences will then be drawn
for the tree-adjoining grammar parsing problem. The following analysis assumes the
random-access machine as the model of computation.
</bodyText>
<subsectionHeader confidence="0.999957">
4.1 Transferring of Time Upper Bounds
</subsectionHeader>
<bodyText confidence="0.9998855">
We show in the following how time upper bounds for the TGP problem can be trans-
ferred to time upper bounds for the BMM problem using the commutative diagram
studied in the previous section.
Let (A, B) be an instance of BMM and let (G, w) = ((A , B)); m and n are specified
as in Definition 3. Observe that, since n6 &gt; m, function f (n ) maps set {Lin} into product
set {1..n4} x {1..n} x {1..n}, in other words we have 1 &lt; (i) &lt;n4 and 1 &lt; f2(i),f3(i) &lt;n
</bodyText>
<page confidence="0.993031">
187
</page>
<note confidence="0.817135">
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.999540842105263">
for 1 &lt; i &lt; m. From the definition of F, we see that G contains 0(m2) auxiliary trees
from each of the classes Fr,F and l-nn). This determines the size of G and we have
= 0(m2), since lw1 = 0(n). Each auxiliary tree introduced in G at steps (i)
and (ii) of Definition 3 requires the computation of a constant number of instances of
function f (n) on some integer i, 1 &lt; i &lt; m. Such a computation can be carried out in an
amount of time 0(log2(m)) using standard algorithms for integer division. Summing
up, the entire computation of ,F on an instance (A, B) takes time 0(m2 log2(m)).
Let Rp be the parse relation that solves (G, w) = .F ((A, B)). From Definition 1 and
the above observations we have that 1Rpl = 0(m2n4), that is 1 Rp = 0(m2+ ). We can
compute C = c(R) in the following way. For every element c1 we compute PO (i)
and PO (j) and then check Rp according to Definition 4. (Recall also our assumption
that an instance of Rp can be tested in constant time.) Again we find that the entire
computation takes an amount of time 0(m2 log2(m)). We observe that the computation
of T and g takes an amount of time (asymptotically) very close to the one needed to
store (A, B) or C.
As a consequence of the above discussion and of Lemma 1, we have that any time
upper bound for the TGP problem can be transferred to an upper bound for the BMM
problem, down to the time needed for the computation of transformations F and G.
The following statement gives an example.
</bodyText>
<subsectionHeader confidence="0.747414">
Theorem 1
</subsectionHeader>
<bodyText confidence="0.984958">
Let Ap be an algorithm for the solution of the TGP problem having running time
0(1G 1111W lq). Then any instance of BMM can be solved in time 0(max{m2P+4,
m2 log2(m)}), where m is the order of the input matrices.
</bodyText>
<subsectionHeader confidence="0.922868">
Proof
</subsectionHeader>
<bodyText confidence="0.999863714285714">
From Lemma 1 and from the previous discussion we have that two Boolean matrices
of order m can be multiplied in time 0(1G IP w lq + m2 log2(m)), where 1 G1 = 0(m2)
and 1 w 1 = 0(m4&apos;). 0
Observe that, according to our definition, the TGP problem has a trivial time lower
bound 0(1Rp1), since this is the amount of time needed in the worst case to store a
representation for Rp that can be accessed in constant time. In practice this means that
the upper bound transfer stated by the above result is effective down to 0(m2+i ).
</bodyText>
<subsectionHeader confidence="0.999952">
4.2 Time Upper Bounds for TGP
</subsectionHeader>
<bodyText confidence="0.992596615384615">
In previous sections we have related the complexity of tree-adjoining grammar pars-
ing to the complexity of Boolean matrix multiplication. Here we speculate on the
consequences of the presented result.
As a computational problem, Boolean matrix multiplication has been an object of
investigation for many years. Researchers have tried to improve the well-known 0(m3)
time upper bound, m the order of the input matrices, and methods were found that
work asymptotically faster than the standard cubic time algorithm. Strassen&apos;s divide
and conquer algorithm that runs in time 0(m2.81) (see for instance Cormen, Leiserson,
and Rivest [1990]) has been the first one in the series, and the best time upper bound
known to date is approximately 0(m2.376), as reported in Coppersmith and Winograd
(1990). It is worth noting here that the closer researchers have come to the 0(m2) trivial
time lower bound, the more complex the computation involved in these methods has
become. In fact, if Strassen&apos;s algorithm outperforms the 0(m3) standard algorithm only
</bodyText>
<page confidence="0.989661">
188
</page>
<note confidence="0.271126">
Giorgio Satta Tree-Adjoining Grammar Parsing
</note>
<bodyText confidence="0.99736104">
for input matrices of order greater than 45 or so (see again Cormen, Leiserson, and
Rivest [1990]), recently discovered methods that are asymptotically faster are definitely
prohibitive, given current computer hardware. At present, no straightforward method
is known for Boolean matrix multiplication that considerably improves the cubic upper
bound and that can be used in practical cases. Also, there is enough evidence that, if
such a method exists, its discovery should be a very difficult enterprise.
Let us now turn to the TAG parsing problem. Many algorithms have been pro-
posed for its solution and an 0(1G11I U A11w16) time upper bound has been given
in the literature; see for instance Schabes (1990). We remark here that the depen-
dency on the grammar size can be further improved using techniques similar to the
one proposed in Graham, Harrison, and Ruzzo (1980) for the context-free grammar
recognition/parsing problem: this results in an 0(1 G 11 w16) time upper bound for the
general case. Theorem 1 can be used to transfer this upper bound to an upper bound
for Boolean matrix multiplication, finding the already mentioned 0(m3) result.
More interestingly, Theorem 1 implies that any method for the solution of the
tree-adjoining grammar parsing problem having running time 0(1G11w15) will give us
a method for Boolean matrix multiplication having running time 0(m2.83). Likewise,
any 0(1 G 11 w 14) time method for the former problem will result in an 0(m2.6) time
method for Boolean matrix multiplication. Even if the involved constants hidden in
the studied construction are large, the resulting methods will still be competitive with
known methods for Boolean matrix multiplication that improve the cubic time upper
bound. We conclude then that the TAG parsing problem should also be considered
as having the status of a problem that is &amp;quot;difficult&amp;quot; to improve, and we have enough
evidence to think that methods for TAG parsing that are asymptotically faster than
0(1G11w16) are unlikely to be practical, i.e., will involve rather complex computations.
</bodyText>
<sectionHeader confidence="0.950481" genericHeader="conclusions">
5. Remarks and Conclusion
</sectionHeader>
<bodyText confidence="0.999949095238095">
Polynomial time reductions between decision/search problems are commonly used
in providing hardness results for complexity classes not known to be included in P
(P is the class of all languages decidable in deterministic polynomial time). We have
studied here a polynomial time reduction between Boolean matrix multiplication and
TAG parsing, two problems already known to be in P. However, the choice of the
mapping allows one to transfer upper bounds from the first problem to the other. In
this way TAG parsing inherits from Boolean matrix multiplication the reputation of
being a problem tough to improve. We comment in the following on the significance
of this result.
As already discussed, the notion of the parse forest is an informal one, and there
is no common agreement on which specifications such a structure should meet. The
obtained results are based on the assumption that a parsing algorithm for TAG should
be able to provide a representation for a parse forest such that instances of the parse
relation Rp in Definition 1 can be retrieved in constant time. Whatever the specifications
of the output parse forest structure will be, it seems quite reasonable to require that an
explicit representation of relation Rp can be extracted from the output in linear time
with respect to the size of the output itself, therefore without affecting the overall
running time of the method. This requirement is satisfied by all TAG parsers that
have been presented to date in the literature.
As a second point, the studied construction provides an interesting insight into
the structure of the TAG parsing problem. We see for instance that the major source of
</bodyText>
<page confidence="0.995722">
189
</page>
<note confidence="0.799933">
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.999936731707317">
complexity derives from cases of properly nested adjunction operations. Such cases are
responsible for a bounded amount of nondeterminism in the computation: to detect
how a string divides into subparts according to the adjunction of a derived tree into
another, we have to consider many possibilities in general, as much as we do to detect
a non-null element within a product Boolean matrix. A closer look at the studied
construction reveals also that the parsing problem for linear TAG does not seem easier
than the general case, since ,F maps instances of BMM to instances of TGP restricted
to such a class (a linear TAG is a TAG whose elementary trees allow adjunction only
into nodes along a single spine). This contrasts with the related case of context-free
grammar parsing, where the restriction of the problem to linear grammars can be
solved in time 0(1G11w12) but no method is known for the general case working with
this bound. As expected from our result, the techniques that are used for linear context-
free grammar parsing cannot be easily generalized to improve the parsing problem
for linear TAGs with respect to the general case.
Finally, we want to discuss here an interesting extension of the studied result.
The TAG parsing problem can be generalized to cases in which the input is a lattice
representation of a string of terminal symbols along with a partially specified parse
relation associated with it. This has many applications for ill-formed input and error-
correcting parsing. The TAG lattice parsing problem can still be solved in 0(1G11w16)
time: the general parsing method provided in Lang (1992) can be used to this purpose,
and already known tabular methods for TAG parsing can be easily adapted as well.
Without giving the technical details of the argument, we sketch here how Boolean
matrix multiplication can be related to TAG lattice parsing. For order m matrices, one
can use an encoding function f1(n) , where n = Lin1/2 + 1, mapping set {1..m} into
product set {1..n} x {1..n}. This allows a direct encoding of any instance (A, B) of
the BMM problem into a word lattice w1 consisting of 6(n + 1) nodes and 0(m2) arcs,
where some arcs involve four nodes and represent a derived tree corresponding to
a non-null element in either A or B. Then we can use a grammar G in the target
instance of the TAG lattice problem that is defined independently of (A, B) and there-
fore has constant size. (Such a grammar can be obtained from families F and lin)
defined in Section 3 by deleting the integer components in each nonterminal sym-
bol.) The construction obtained in this way relates therefore the BMM problem to the
fixed grammar parsing problem, and provides a result even stronger than the one
presented in Theorem 1. We have in fact that any algorithm for TAG lattice parsing
having running time 0(1 G I w lq ) can be converted into an algorithm for Boolean ma-
trix multiplication running in time 0(max{m , m2 log2(m)}), independently of p. As
an example, 0(1G w14) for TAG lattice parsing becomes 0(m2log2(m)) for matrix
multiplication, for any p. Since many tabular methods for TAG parsing can be easily
extended to TAG lattice parsing, this means that the chances of getting an 0(1 G IP w 14)
time upper bound for the TAG parsing problem itself by means of these techniques
are really small.
</bodyText>
<sectionHeader confidence="0.949045" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999633352941177">
I am indebted to Yves Schabes who
suggested to me the original idea of relating
a standard computational problem to the
tree-adjoining grammar parsing problem. I
want to thank Bernard Lang, Owen
Rambow, and Yves Schabes, who have
provided, directly or indirectly, important
suggestions for the development of the
ideas in this paper. Comments from two
anonymous referees have also been very
helpful in improving the exposition of the
results reported in this paper. Finally, I am
grateful to Aravind Joshi for his support in
this research. None of these people is
responsible for any error in this work. This
research was partially funded by the
following grants: ARO grant DAAL
</bodyText>
<page confidence="0.980417">
190
</page>
<figure confidence="0.468299">
Giorgio Satta Tree-Adjoining Grammar Parsing
</figure>
<reference confidence="0.990112094117647">
03-89-C-0031, DARPA grant
N00014-90+1863, NSF grant IRI 90-16592,
and Ben Franklin grant 91S.3078C-1.
References
Coppersmith, D., and Winograd, S. (1990).
&amp;quot;Matrix multiplication via arithmetic
progression.&amp;quot; Journal of Symbolic
Computation, 9(3), 251-280. Special Issue
on Computational Algebraic Complexity.
Cormen, T. H.; Leiserson, C. E.; and Rivest,
R. L. (1990). Introduction to Algorithms. The
MIT Press.
Frank, R. (1992). Syntactic locality and tree
adjoining grammar: grammatical, acquisition
and processing perspectives. Doctoral
dissertation, University of Pennsylvania.
Graham, S. L.; Harrison, M. A.; and Ruzzo,
W. L. (1980). &amp;quot;An improved context-free
recognizer.&amp;quot; ACM Transactions on
Programming Languages and Systems, 2(3),
415-462.
Joshi, A. K. (1985). &amp;quot;How much
context-sensitivity is necessary for
characterizing structural
descriptions—tree adjoining grammars. In
Natural Language Processing—Theoretical,
Computational and Psychological Perspectives,
edited by D. Dowty, L. Karttunen, and
A. Zwicky, 206-250. Cambridge
University Press. Originally presented in
a Workshop on Natural Language Parsing
at Ohio State University, Columbus, Ohio,
May 1983.
Joshi, A. K.; Levy, L. S.; and Takahashi, M.
(1975). &amp;quot;Tree adjunct grammars.&amp;quot; Journal
of Computer System and Science, 10(1),
136-163.
Joshi, A.; Vijay-Shanker, K.; and Weir, D.
(1991). &amp;quot;The convergence of mildly
context-sensitive grammatical
formalisms.&amp;quot; In Foundational Issues in
Natural Language Processing, edited by
S. Shieber and T. Wasow, 31-82. MIT
Press.
Lang, B. (1992). &amp;quot;Recognition can be harder
than parsing.&amp;quot; Abstract submitted to the
Second TAG Workshop, June 1992.
Lavelli, A., and Satta, G. (1991).
&amp;quot;Bidirectional parsing of lexicalized tree
adjoining grammars.&amp;quot; In Proceedings, Fifth
Conference of the European Chapter of the
Association for Computational Linguistics,
Berlin, 1991, 27-32.
Palis, M. A.; Shende, S.; and Wei, D. S. L.
(1990). &amp;quot;An optimal linear-time parallel
parser for tree-adjoining languages.&amp;quot;
SIAM Journal on Computing, 19(1), 1-31.
Schabes, Y. (1990). Mathematical and
computational aspects of lexicalized grammars.
Doctoral dissertation, University of
Pennsylvania. Available as technical
report (MS-CIS-90-48, LINC LAB179) from
the Department of Computer Science.
Schabes, Y. (1991). &amp;quot;The valid prefix
property and left to right parsing of
tree-adjoining grammar.&amp;quot; In Proceedings,
Second International Workshop on Parsing
Technologies, Cancun, Mexico, February
1991, 21-30.
Schabes, Y., and Joshi, A. K. (1988). &amp;quot;An
Earley-type parsing algorithm for tree
adjoining grammars.&amp;quot; In Proceedings, 26th
Meeting of the Association for Computational
Linguistics, Buffalo, June 1988, 258-269.
Vijay-Shanker, K., and Joshi, A. K. (1985).
&amp;quot;Some computational properties of tree
adjoining grammars.&amp;quot; In Proceedings, 23rd
Meeting of the Association for Computational
Linguistics, Chicago, July 1985, 82-93.
Vijay-Shanker, K., and Weir, D. J. (1993).
&amp;quot;The use of shared forests in TAG
parsing.&amp;quot; In Proceedings, Sixth Conference of
the European Chapter of the Association for
Computational Linguistics, Utrecht, 1993,
384-393.
</reference>
<page confidence="0.998476">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594016">
<title confidence="0.9718435">Tree-Adjoining Grammar Parsing and Boolean Matrix Multiplication</title>
<author confidence="0.750737">Universita di_Venezia</author>
<abstract confidence="0.97174475">The computational problem of parsing a sentence in a tree-adjoining language is investigated. An interesting relation is studied between this problem and the well-known computational problem of Boolean matrix multiplication: it is shown that any algorithm for the solution of the former problem can easily be converted into an algorithm for the solution of the latter problem. This result bears on at least two important computational issues. First, we realize that a straightforward method that improves the known upper bound for tree-adjoining grammar parsing is hard to find. Second, we understand which features of the tree-adjoining grammar parsing problem are responsible for the claimed difficulty.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>03-89-C-0031</author>
</authors>
<title>DARPA grant N00014-90+1863, NSF grant IRI 90-16592, and Ben Franklin grant 91S.3078C-1.</title>
<marker>03-89-C-0031, </marker>
<rawString>03-89-C-0031, DARPA grant N00014-90+1863, NSF grant IRI 90-16592, and Ben Franklin grant 91S.3078C-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Coppersmith</author>
<author>S Winograd</author>
</authors>
<title>Matrix multiplication via arithmetic progression.&amp;quot;</title>
<date>1990</date>
<journal>Journal of Symbolic Computation,</journal>
<booktitle>Special Issue on Computational Algebraic Complexity.</booktitle>
<volume>9</volume>
<issue>3</issue>
<pages>251--280</pages>
<contexts>
<context position="39908" citStr="Coppersmith and Winograd (1990)" startWordPosition="7330" endWordPosition="7333"> consequences of the presented result. As a computational problem, Boolean matrix multiplication has been an object of investigation for many years. Researchers have tried to improve the well-known 0(m3) time upper bound, m the order of the input matrices, and methods were found that work asymptotically faster than the standard cubic time algorithm. Strassen&apos;s divide and conquer algorithm that runs in time 0(m2.81) (see for instance Cormen, Leiserson, and Rivest [1990]) has been the first one in the series, and the best time upper bound known to date is approximately 0(m2.376), as reported in Coppersmith and Winograd (1990). It is worth noting here that the closer researchers have come to the 0(m2) trivial time lower bound, the more complex the computation involved in these methods has become. In fact, if Strassen&apos;s algorithm outperforms the 0(m3) standard algorithm only 188 Giorgio Satta Tree-Adjoining Grammar Parsing for input matrices of order greater than 45 or so (see again Cormen, Leiserson, and Rivest [1990]), recently discovered methods that are asymptotically faster are definitely prohibitive, given current computer hardware. At present, no straightforward method is known for Boolean matrix multiplicati</context>
</contexts>
<marker>Coppersmith, Winograd, 1990</marker>
<rawString>Coppersmith, D., and Winograd, S. (1990). &amp;quot;Matrix multiplication via arithmetic progression.&amp;quot; Journal of Symbolic Computation, 9(3), 251-280. Special Issue on Computational Algebraic Complexity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>The MIT Press.</publisher>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, T. H.; Leiserson, C. E.; and Rivest, R. L. (1990). Introduction to Algorithms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frank</author>
</authors>
<title>Syntactic locality and tree adjoining grammar: grammatical, acquisition and processing perspectives. Doctoral dissertation,</title>
<date>1992</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1377" citStr="Frank (1992)" startWordPosition="203" endWordPosition="204">are responsible for the claimed difficulty. 1. Introduction Among formalisms for the computation of syntactic description of natural language sentences, Tree-Adjoining Grammars (TAG) play a major role. The class of TAG&apos;s was first introduced in Joshi, Levy, and Takahashi (1975) and Joshi (1985); since then, formal and computational properties of this class have been extensively investigated, and the linguistic relevance of TAGs has been discussed in the literature as well. The reader who is interested in these topics is referred to some of the most recent works, for example Schabes (1990) and Frank (1992), and to the references therein. Both in a theoretical vein and in view of possible natural language processing applications, the recognition and parsing problems for TAGs have been extensively studied and many algorithms have been proposed for their solution. On the basis of tabular techniques, the least time upper bound that has been attested is 0(1 G11w16) for the random-access model of computation, 1G1 being the size of the input grammar and 1w1 the length of the input string. In recent years, improvement of such a worst-case running time has been a common goal for many researchers, but up</context>
</contexts>
<marker>Frank, 1992</marker>
<rawString>Frank, R. (1992). Syntactic locality and tree adjoining grammar: grammatical, acquisition and processing perspectives. Doctoral dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Graham</author>
<author>M A Harrison</author>
<author>W L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.&amp;quot;</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>415--462</pages>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, S. L.; Harrison, M. A.; and Ruzzo, W. L. (1980). &amp;quot;An improved context-free recognizer.&amp;quot; ACM Transactions on Programming Languages and Systems, 2(3), 415-462.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A K Joshi</author>
</authors>
<title>How much context-sensitivity is necessary for characterizing structural descriptions—tree adjoining grammars.</title>
<date>1985</date>
<booktitle>In Natural Language Processing—Theoretical, Computational and Psychological Perspectives, edited</booktitle>
<pages>206--250</pages>
<publisher>Cambridge University</publisher>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1060" citStr="Joshi (1985)" startWordPosition="152" endWordPosition="153">solution of the latter problem. This result bears on at least two important computational issues. First, we realize that a straightforward method that improves the known upper bound for tree-adjoining grammar parsing is hard to find. Second, we understand which features of the tree-adjoining grammar parsing problem are responsible for the claimed difficulty. 1. Introduction Among formalisms for the computation of syntactic description of natural language sentences, Tree-Adjoining Grammars (TAG) play a major role. The class of TAG&apos;s was first introduced in Joshi, Levy, and Takahashi (1975) and Joshi (1985); since then, formal and computational properties of this class have been extensively investigated, and the linguistic relevance of TAGs has been discussed in the literature as well. The reader who is interested in these topics is referred to some of the most recent works, for example Schabes (1990) and Frank (1992), and to the references therein. Both in a theoretical vein and in view of possible natural language processing applications, the recognition and parsing problems for TAGs have been extensively studied and many algorithms have been proposed for their solution. On the basis of tabula</context>
<context position="12198" citStr="Joshi (1985)" startWordPosition="2130" endWordPosition="2131">ut itself can be considered as a highly compressed representation of the parse forest—one that needs a time-expensive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoinin</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, A. K. (1985). &amp;quot;How much context-sensitivity is necessary for characterizing structural descriptions—tree adjoining grammars. In Natural Language Processing—Theoretical, Computational and Psychological Perspectives, edited by D. Dowty, L. Karttunen, and A. Zwicky, 206-250. Cambridge University Press. Originally presented in a Workshop on Natural Language Parsing at Ohio State University, Columbus, Ohio, May 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.&amp;quot;</title>
<date>1975</date>
<journal>Journal of Computer System and Science,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>136--163</pages>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A. K.; Levy, L. S.; and Takahashi, M. (1975). &amp;quot;Tree adjunct grammars.&amp;quot; Journal of Computer System and Science, 10(1), 136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>The convergence of mildly context-sensitive grammatical formalisms.&amp;quot;</title>
<date>1991</date>
<booktitle>In Foundational Issues in Natural Language Processing,</booktitle>
<pages>31--82</pages>
<publisher>MIT Press.</publisher>
<note>edited by</note>
<marker>Joshi, Vijay-Shanker, Weir, 1991</marker>
<rawString>Joshi, A.; Vijay-Shanker, K.; and Weir, D. (1991). &amp;quot;The convergence of mildly context-sensitive grammatical formalisms.&amp;quot; In Foundational Issues in Natural Language Processing, edited by S. Shieber and T. Wasow, 31-82. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Recognition can be harder than parsing.&amp;quot; Abstract submitted to the Second TAG Workshop,</title>
<date>1992</date>
<contexts>
<context position="12311" citStr="Lang (1992)" startWordPosition="2147" endWordPosition="2148">ive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem is defined to be any pair (G, w), and the unique solution of such an instance is provid</context>
<context position="45646" citStr="Lang (1992)" startWordPosition="8261" endWordPosition="8262">tfree grammar parsing cannot be easily generalized to improve the parsing problem for linear TAGs with respect to the general case. Finally, we want to discuss here an interesting extension of the studied result. The TAG parsing problem can be generalized to cases in which the input is a lattice representation of a string of terminal symbols along with a partially specified parse relation associated with it. This has many applications for ill-formed input and errorcorrecting parsing. The TAG lattice parsing problem can still be solved in 0(1G11w16) time: the general parsing method provided in Lang (1992) can be used to this purpose, and already known tabular methods for TAG parsing can be easily adapted as well. Without giving the technical details of the argument, we sketch here how Boolean matrix multiplication can be related to TAG lattice parsing. For order m matrices, one can use an encoding function f1(n) , where n = Lin1/2 + 1, mapping set {1..m} into product set {1..n} x {1..n}. This allows a direct encoding of any instance (A, B) of the BMM problem into a word lattice w1 consisting of 6(n + 1) nodes and 0(m2) arcs, where some arcs involve four nodes and represent a derived tree corre</context>
</contexts>
<marker>Lang, 1992</marker>
<rawString>Lang, B. (1992). &amp;quot;Recognition can be harder than parsing.&amp;quot; Abstract submitted to the Second TAG Workshop, June 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavelli</author>
<author>G Satta</author>
</authors>
<title>Bidirectional parsing of lexicalized tree adjoining grammars.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>27--32</pages>
<location>Berlin,</location>
<contexts>
<context position="12298" citStr="Lavelli and Satta (1991)" startWordPosition="2143" endWordPosition="2146">e that needs a time-expensive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem is defined to be any pair (G, w), and the unique solution of such an insta</context>
</contexts>
<marker>Lavelli, Satta, 1991</marker>
<rawString>Lavelli, A., and Satta, G. (1991). &amp;quot;Bidirectional parsing of lexicalized tree adjoining grammars.&amp;quot; In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics, Berlin, 1991, 27-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Palis</author>
<author>S Shende</author>
<author>D S L Wei</author>
</authors>
<title>An optimal linear-time parallel parser for tree-adjoining languages.&amp;quot;</title>
<date>1990</date>
<journal>SIAM Journal on Computing,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>1--31</pages>
<marker>Palis, Shende, Wei, 1990</marker>
<rawString>Palis, M. A.; Shende, S.; and Wei, D. S. L. (1990). &amp;quot;An optimal linear-time parallel parser for tree-adjoining languages.&amp;quot; SIAM Journal on Computing, 19(1), 1-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Mathematical and computational aspects of lexicalized grammars. Doctoral dissertation,</title>
<date>1990</date>
<institution>University of Pennsylvania.</institution>
<note>Available as technical report (MS-CIS-90-48, LINC LAB179) from the</note>
<contexts>
<context position="1360" citStr="Schabes (1990)" startWordPosition="200" endWordPosition="201">ar parsing problem are responsible for the claimed difficulty. 1. Introduction Among formalisms for the computation of syntactic description of natural language sentences, Tree-Adjoining Grammars (TAG) play a major role. The class of TAG&apos;s was first introduced in Joshi, Levy, and Takahashi (1975) and Joshi (1985); since then, formal and computational properties of this class have been extensively investigated, and the linguistic relevance of TAGs has been discussed in the literature as well. The reader who is interested in these topics is referred to some of the most recent works, for example Schabes (1990) and Frank (1992), and to the references therein. Both in a theoretical vein and in view of possible natural language processing applications, the recognition and parsing problems for TAGs have been extensively studied and many algorithms have been proposed for their solution. On the basis of tabular techniques, the least time upper bound that has been attested is 0(1 G11w16) for the random-access model of computation, 1G1 being the size of the input grammar and 1w1 the length of the input string. In recent years, improvement of such a worst-case running time has been a common goal for many re</context>
<context position="40919" citStr="Schabes (1990)" startWordPosition="7491" endWordPosition="7492">0]), recently discovered methods that are asymptotically faster are definitely prohibitive, given current computer hardware. At present, no straightforward method is known for Boolean matrix multiplication that considerably improves the cubic upper bound and that can be used in practical cases. Also, there is enough evidence that, if such a method exists, its discovery should be a very difficult enterprise. Let us now turn to the TAG parsing problem. Many algorithms have been proposed for its solution and an 0(1G11I U A11w16) time upper bound has been given in the literature; see for instance Schabes (1990). We remark here that the dependency on the grammar size can be further improved using techniques similar to the one proposed in Graham, Harrison, and Ruzzo (1980) for the context-free grammar recognition/parsing problem: this results in an 0(1 G 11 w16) time upper bound for the general case. Theorem 1 can be used to transfer this upper bound to an upper bound for Boolean matrix multiplication, finding the already mentioned 0(m3) result. More interestingly, Theorem 1 implies that any method for the solution of the tree-adjoining grammar parsing problem having running time 0(1G11w15) will give </context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes, Y. (1990). Mathematical and computational aspects of lexicalized grammars. Doctoral dissertation, University of Pennsylvania. Available as technical report (MS-CIS-90-48, LINC LAB179) from the Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>The valid prefix property and left to right parsing of tree-adjoining grammar.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Second International Workshop on Parsing Technologies,</booktitle>
<pages>21--30</pages>
<location>Cancun, Mexico,</location>
<contexts>
<context position="12272" citStr="Schabes (1991)" startWordPosition="2141" endWordPosition="2142"> parse forest—one that needs a time-expensive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem is defined to be any pair (G, w), and the unique</context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Schabes, Y. (1991). &amp;quot;The valid prefix property and left to right parsing of tree-adjoining grammar.&amp;quot; In Proceedings, Second International Workshop on Parsing Technologies, Cancun, Mexico, February 1991, 21-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A K Joshi</author>
</authors>
<title>An Earley-type parsing algorithm for tree adjoining grammars.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>258--269</pages>
<location>Buffalo,</location>
<contexts>
<context position="12224" citStr="Schabes and Joshi (1988)" startWordPosition="2132" endWordPosition="2135">be considered as a highly compressed representation of the parse forest—one that needs a time-expensive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem </context>
</contexts>
<marker>Schabes, Joshi, 1988</marker>
<rawString>Schabes, Y., and Joshi, A. K. (1988). &amp;quot;An Earley-type parsing algorithm for tree adjoining grammars.&amp;quot; In Proceedings, 26th Meeting of the Association for Computational Linguistics, Buffalo, June 1988, 258-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>82--93</pages>
<location>Chicago,</location>
<contexts>
<context position="12198" citStr="Vijay-Shanker and Joshi (1985)" startWordPosition="2128" endWordPosition="2131">rspective, the input itself can be considered as a highly compressed representation of the parse forest—one that needs a time-expensive process for parse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoinin</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>Vijay-Shanker, K., and Joshi, A. K. (1985). &amp;quot;Some computational properties of tree adjoining grammars.&amp;quot; In Proceedings, 23rd Meeting of the Association for Computational Linguistics, Chicago, July 1985, 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The use of shared forests in TAG parsing.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Sixth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>384--393</pages>
<location>Utrecht,</location>
<contexts>
<context position="12346" citStr="Vijay-Shanker and Weir (1993)" startWordPosition="2150" endWordPosition="2153">arse tree retrieval.&apos; More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem is defined to be any pair (G, w), and the unique solution of such an instance is provided by an explicit representation of</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>Vijay-Shanker, K., and Weir, D. J. (1993). &amp;quot;The use of shared forests in TAG parsing.&amp;quot; In Proceedings, Sixth Conference of the European Chapter of the Association for Computational Linguistics, Utrecht, 1993, 384-393.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>