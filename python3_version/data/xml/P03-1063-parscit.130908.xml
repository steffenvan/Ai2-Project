<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9966375">
Text Chunking by Combining Hand-Crafted Rules and Memory-Based
Learning
</title>
<author confidence="0.99174">
Seong-Bae Park Byoung-Tak Zhang
</author>
<affiliation confidence="0.9802715">
School of Computer Science and Engineering
Seoul National University
</affiliation>
<address confidence="0.926899">
Seoul 151-744, Korea
</address>
<email confidence="0.9996">
{sbpark,btzhang}@bi.snu.ac.kr
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949947368421">
This paper proposes a hybrid of hand-
crafted rules and a machine learning
method for chunking Korean. In the par-
tially free word-order languages such as
Korean and Japanese, a small number
of rules dominate the performance due
to their well-developed postpositions and
endings. Thus, the proposed method is
primarily based on the rules, and then the
residual errors are corrected by adopting a
memory-based machine learning method.
Since the memory-based learning is an
efficient method to handle exceptions in
natural language processing, it is good at
checking whether the estimates are excep-
tional cases of the rules and revising them.
An evaluation of the method yields the im-
provement in F-score over the rules or var-
ious machine learning methods alone.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972159090909">
Text chunking has been one of the most interest-
ing problems in natural language learning commu-
nity since the first work of (Ramshaw and Marcus,
1995) using a machine learning method. The main
purpose of the machine learning methods applied to
this task is to capture the hypothesis that best deter-
mine the chunk type of a word, and such methods
have shown relatively high performance in English
(Kudo and Matsumoto, 2000; Zhang et. al, 2001).
In order to do it, various kinds of information, such
as lexical information, part-of-speech and grammat-
ical relation, of the neighboring words is used. Since
the position of a word plays an important role as a
syntactic constraint in English, the methods are suc-
cessful even with local information.
However, these methods are not appropriate for
chunking Korean and Japanese, because such lan-
guages have a characteristic of partially free word-
order. That is, there is a very weak positional con-
straint in these languages. Instead of positional con-
straints, they have overt postpositions that restrict
the syntactic relation and composition of phrases.
Thus, unless we concentrate on the postpositions,
we must enlarge the neighboring window to get
a good hypothesis. However, enlarging the win-
dow size will cause the curse of dimensionality
(Cherkassky and Mulier, 1998), which results in the
deficiency in the generalization performance.
Especially in Korean, the postpositions and the
endings provide important information for noun
phrase and verb phrase chunking respectively. With
only a few simple rules using such information,
the performance of chunking Korean is as good
as the rivaling other inference models such as ma-
chine learning algorithms and statistics-based meth-
ods (Shin, 1999). Though the rules are approxi-
mately correct for most cases drawn from the do-
main on which the rules are based, the knowledge
in the rules is not necessarily well-represented for
any given set of cases. Since chunking is usually
processed in the earlier step of natural language pro-
cessing, the errors made in this step have a fatal in-
fluence on the following steps. Therefore, the ex-
ceptions that are ignored by the rules must be com-
</bodyText>
<figure confidence="0.346255">
Training Phase
</figure>
<figureCaption confidence="0.985979">
Figure 1: The structure of Korean chunking model. This figure describes a sentence-based learning and
classification.
</figureCaption>
<figure confidence="0.988731541666667">
For Each W ord wi
w 1 ... w N
(PO S1 ...PO SN)
Yes
Correctly
D eterm ined?
Finish
N o
E rror C ase Library
Rule Base
Find Error Type
Rule Based
D eterm ination
Classification Phase For Each W ord wi
w 1 ... w N
(PO S1 ...PO SN)
Rule B ased
D eterm ination
R ule B ase
M em ory B ased
D eterm ination
E rror Case Library
Com bination
C 1 ... C N
</figure>
<bodyText confidence="0.994116466666667">
pensated for by some special treatments of them for
higher performance.
To solve this problem, we have proposed a com-
bining method of the rules and the k-nearest neigh-
bor (k-NN) algorithm (Park and Zhang, 2001). The
problem in this method is that it has redundant k-
NNs because it maintains a separate k-NN for each
kind of errors made by the rules. In addition, be-
cause it applies a k-NN and the rules to each exam-
ples, it requires more computations than other infer-
ence methods.
The goal of this paper is to provide a new method
for chunking Korean by combining the hand-crafted
rules and a machine learning method. The chunk
type of a word in question is determined by the rules,
and then verified by the machine learning method.
The role of the machine learning method is to de-
termine whether the current context is an exception
of the rules. Therefore, a memory-based learning
(MBL) is used as a machine learning method that
can handle exceptions efficiently (Daelemans et. al,
1999).
The rest of the paper is organized as follows. Sec-
tion 2 explains how the proposed method works.
Section 3 describes the rule-based method for
chunking Korean and Section 4 explains chunking
by memory-based learning. Section 5 presents the
experimental results. Section 6 introduces the issues
for applying the proposed method to other problems.
Finally, Section 7 draws conclusions.
</bodyText>
<sectionHeader confidence="0.907113" genericHeader="method">
2 Chunking Korean
</sectionHeader>
<bodyText confidence="0.999314769230769">
Figure 1 shows the structure of the chunking model
for Korean. The main idea of this model is to apply
rules to determine the chunk type of a word wi in a
sentence, and then to refer to a memory based clas-
sifier in order to check whether it is an exceptional
case of the rules. In the training phase, each sentence
is analyzed by the rules and the predicted chunk type
is compared with the true chunk type. In case of mis-
prediction, the error type is determined according to
the true chunk type and the predicted chunk type.
The mispredicted chunks are stored in the error case
library with their true chunk types. Since the error
case library accumulates only the exceptions of the
rules, the number of cases in the library is small if
the rules are general enough to represent the instance
space well.
The classification phase in Figure 1 is expressed
as a procedure in Figure 2. It determines the chunk
type of a word wi given with the context Ci. First of
all, the rules are applied to determine the chunk type.
Then, it is checked whether Ci is an exceptional case
of the rules. If it is, the chunk type determined by
the rules is discarded and is determined again by the
memory based reasoning. The condition to make a
decision of exceptional case is whether the similar-
ity between Ci and the nearest instance in the error
</bodyText>
<figure confidence="0.366098444444444">
Procedure Combine
Input: a word wi, a context Ci, and the threshold t
Output: a chunk type c
[Step 1] c = Determine the chunk type of wi using rules.
[Step 2] e = Get the nearest instance of Ci in error case
library.
[Step 3] If Similarity(Ci, e) ≥ t,
then c = Determine chunk type of wi by memory-
based learning.
</figure>
<figureCaption confidence="0.991356">
Figure 2: The procedure for combining the rules and
memory based learning.
</figureCaption>
<bodyText confidence="0.98653275">
case library is larger than the threshold t. Since the
library contains only the exceptional cases, the more
similar is Ci to the nearest instance, the more prob-
able is it an exception of the rules.
</bodyText>
<sectionHeader confidence="0.901699" genericHeader="method">
3 Chunking by Rules
</sectionHeader>
<bodyText confidence="0.99997175">
There are four basic phrases in Korean: noun phrase
(NP), verb phrase (VP), adverb phrase (ADVP), and
independent phrase (IP). Thus, chunking by rules is
divided into largely four components.
</bodyText>
<subsectionHeader confidence="0.997353">
3.1 Noun Phrase Chunking
</subsectionHeader>
<bodyText confidence="0.998974">
When the part-of-speech of wi is one of determiner,
noun, and pronoun, there are only seven rules to
determine the chunk type of wi due to the well-
developed postpositions of Korean.
</bodyText>
<listItem confidence="0.989135846153846">
1. If POS(wi−1) = determiner and wi−1 does not have a
postposition Then yi = I-NP.
2. Else If POS(wi−1) = pronoun and wi−1 does not have
a postposition Then yi = I-NP.
3. Else If POS(wi−1) = noun and wi−1 does not have a
postposition Then yi = I-NP.
4. Else If POS(wi−1) = noun and wi−1 has a possessive
postposition Then yi = I-NP.
5. Else If POS(wi−1) =noun and wi−1 has a relative post-
fix Then yi = I-NP.
6. Else If POS(wi−1) = adjective and wi−1 has a relative
ending Then yi = I-NP.
7. Else yi = B-NP.
</listItem>
<bodyText confidence="0.999797352941177">
Here, POS(wi_1) is the part-of-speech of wi_1.
B-NP represents the first word of a noun phrase,
while I-NP is given to other words in the noun
phrase.
Since determiners, nouns and pronouns play the
similar syntactic role in Korean, they form a noun
phrase when they appear in succession without post-
position (Rule 1–3). The words with postpositions
become the end of a noun phrase, but there are only
two exceptions. When the type of a postposition
is possessive, it is still in the mid of noun phrase
(Rule 4). The other exception is a relative postfix
‘�a� (jeok)’ (Rule 5). Rule 6 states that a simple rela-
tive clause with no sub-constituent also constitutes a
noun phrase. Since the adjectives of Korean have no
definitive usage, this rule corresponds to the defini-
tive usage of the adjectives in English.
</bodyText>
<subsectionHeader confidence="0.998343">
3.2 Verb Phrase Chunking
</subsectionHeader>
<bodyText confidence="0.999982">
The verb phrase chunking has been studied for a
long time under the name of compound verb pro-
cessing in Korean and shows relatively high accu-
racy. Shin used a finite state automaton for verb
phrase chunking (Shin, 1999), while K.-C. Kim used
knowledge-based rules (Kim et. al, 1995). For the
consistency with noun phrase chunking, we use the
rules in this paper. The rules used are the ones pro-
posed by (Kim et. al, 1995) and the further explana-
tion on the rules is skipped. The number of the rules
used is 29.
</bodyText>
<subsectionHeader confidence="0.999109">
3.3 Adverb Phrase Chunking
</subsectionHeader>
<bodyText confidence="0.999882769230769">
When the adverbs appear in succession, they have a
great tendency to form an adverb phrase. Though an
adverb sequence is not always one adverb phrase, it
usually forms one phrase. Table 1 shows this empiri-
cally. The usage of the successive adverbs is investi-
gated from STEP 2000 dataset1 where 270 cases are
observed. The 189 cases among them form a phrase
whereas the remaining 81 cases form two phrases in-
dependently. Thus, it can be said that the possibility
that an adverb sequence forms a phrase is far higher
than the possibility that it forms two phrases.
When the part-of-speech of wi is an adjective, its
chunk type is determined by the following rule.
</bodyText>
<listItem confidence="0.769015333333333">
1. If POS(wi−1) = adverb Then yi = I-ADVP.
2. Else yi = B-ADVP.
1This dataset will be explained in Section 5.1.
</listItem>
<tableCaption confidence="0.912197">
Table 1: The probability that an adverb sequence
forms a chunk.
</tableCaption>
<subsectionHeader confidence="0.969356">
3.4 Independent Phrase Chunking
</subsectionHeader>
<bodyText confidence="0.999951">
There is no special rule for independent phrase
chunking. It can be done only through knowledge
base that stores the cases where independent phrases
take place. We designed 12 rules for independent
phrases.
</bodyText>
<sectionHeader confidence="0.922822" genericHeader="method">
4 Chunking by Memory-Based Learning
</sectionHeader>
<bodyText confidence="0.9994359">
Memory-based learning is a direct descent of the
k-Nearest Neighbor (k-NN) algorithm (Cover and
Hart, 1967). Since many natural language process-
ing (NLP) problems have constraints of a large num-
ber of examples and many attributes with different
relevance, memory-based learning uses more com-
plex data structure and different speedup optimiza-
tion from the k-NN.
It can be viewed with two components: a learning
component and a similarity-based performance com-
ponent. The learning component involves adding
training examples to memory, where all examples
are assumed to be fixed-length vectors of n at-
tributes. The similarity between an instance x and
all examples y in memory is computed using a dis-
tance metric, ∆(x, y). The chunk type of x is then
determined by assigning the most frequent category
within the k most similar examples of x.
The distance from x and y, ∆(x, y) is defined to
be
</bodyText>
<equation confidence="0.989344">
n
∆(x, y) ≡ αiδ(xi, yi),
i=1
</equation>
<bodyText confidence="0.784894">
where αi is the weight of i-th attribute and
� 0 if xi = yi,
</bodyText>
<equation confidence="0.846546">
δ(xi, yi) = 1 if xi =� yi.
</equation>
<bodyText confidence="0.998158733333333">
When αi is determined by information gain (Quin-
lan, 1993), the k-NN algorithm with this metric is
called IB1-IG (Daelemans et. al, 2001). All the ex-
periments performed by memory-based learning in
this paper are done with IB1-IG.
Table 2 shows the attributes of IB1-IG for chunk-
ing Korean. To determine the chunk type of a word
wi, the lexicons, POS tags, and chunk types of
surrounding words are used. For the surrounding
words, three words of left context and three words
of right context are used for lexicons and POS tags,
while two words of left context are used for chunk
types. Since chunking is performed sequentially, the
chunk types of the words in right context are not
known in determining the chunk type of wi.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.605268">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999967380952381">
For the evaluation of the proposed method, all exper-
iments are performed on STEP 2000 Korean Chunk-
ing dataset (STEP 2000 dataset)2. This dataset is
derived from the parsed corpus, which is a product
of STEP 2000 project supported by Korean govern-
ment. The corpus consists of 12,092 sentences with
111,658 phrases and 321,328 words, and the vocab-
ulary size is 16,808. Table 3 summarizes the infor-
mation on the dataset.
The format of the dataset follows that of CoNLL-
2000 dataset (CoNLL, 2000). Figure 3 shows an ex-
ample sentence in the dataset3. Each word in the
dataset has two additional tags, which are a part-of-
speech tag and a chunk tag. The part-of-speech tags
are based on KAIST tagset (Yoon and Choi, 1999).
Each phrase can have two kinds of chunk types: B-
XP and I-XP. In addition to them, there is O chunk
type that is used for words which are not part of any
chunk. Since there are four types of phrases and
one additional chunk type O, there exist nine chunk
types.
</bodyText>
<subsectionHeader confidence="0.999451">
5.2 Performance of Chunking by Rules
</subsectionHeader>
<bodyText confidence="0.999615875">
Table 4 shows the chunking performance when only
the rules are applied. Using only the rules gives
97.99% of accuracy and 91.87 of F-score. In spite
of relatively high accuracy, F-score is somewhat low.
Because the important unit of the work in the appli-
cations of text chunking is a phrase, F-score is far
more important than accuracy. Thus, we have much
room to improve in F-score.
</bodyText>
<footnote confidence="0.998971">
2The STEP 2000 Korean Chunking dataset is available in
http://bi.snu.ac.kr/∼sbpark/Step2000.
3The last column of this figure, the English annotation, does
</footnote>
<table confidence="0.695667666666667">
No. of Cases Probability
One Phrase
Two Phrases
189 0.70
81 0.30
Attribute Explanation Attribute Explanation
Wi−3 word of wi−3 POSi−3 POS of wi−3
Wi−2 word of wi−2 POSi−2 POS of wi−2
Wi−1 word of wi−1 POSi−1 POS of wi−1
Wi word of wi POSi POS of wi
Wi+1 word of wi+1 POSi+1 POS of wi+1
Wi+2 word of wi+2 POSi+2 POS of wi+2
Wi+3 word of wi+3 POSi+3 POS of wi+3
Ci−3 chunk of wi−3 Ci−2 chunk of wi−2
Ci−1 chunk of wi−1
</table>
<tableCaption confidence="0.879002666666667">
Table 2: The attributes of IB1-IG for chunking Korean.
Table 3: The simple statistics on STEP 2000 Korean
Chunking dataset.
</tableCaption>
<table confidence="0.999923764705882">
Information Value
Vocabulary Size 16,838
Number of total words 321,328
Number of chunk types 9
Number of POS tags 52
Number of sentences 12,092
Number of phrases 112,658
Error Type No. of Errors Ratio (%)
B-ADVP I-ADVP 89 1.38
B-ADVP I-NP 9 0.14
B-IP B-NP 9 0.14
I-IP I-NP 2 0.03
B-NP I-NP 2,376 36.76
I-NP B-NP 2,376 36.76
B-VP I-VP 3 0.05
I-VP B-VP 1,599 24.74
All 6,463 100.00
</table>
<tableCaption confidence="0.8763475">
Table 5: The error distribution according to the mis-
labeled chunk type.
</tableCaption>
<table confidence="0.999751578947368">
a� nq B-NP Korea
Æ� jcm I-NP Postposition : POSS
Allo nq I-NP Sejong
;7171 ncn I-NP base
4} jcj I-NP and
a mmd I-NP the
T* ncn I-NP surrounding
;7171 ncn I-NP base
��� jxt I-NP Postposition: TOPIC
A���~- ncn B-NP western South Pole
L-1-~ ncn B-NP south
Tai= � nq I-NP Shetland
u-
Æ� jcm I-NP Postposition : POSS
ofX71Al� nq I-NP King George Island
oil jca I-NP Postposition : LOCA
V paa B-VP is located
�� ef I-VP Ending: DECL
. sf O
</table>
<figureCaption confidence="0.551701">
Figure 3: An example of STEP 2000 dataset.
</figureCaption>
<table confidence="0.999881833333333">
Type Precision Recall F-score
ADVP 98.67% 97.23% 97.94
IP 100.00% 99.63% 99.81
NP 88.96% 88.93% 88.94
VP 92.89% 96.35% 94.59
All 91.28% 92.47% 91.87
</table>
<tableCaption confidence="0.9749165">
Table 4: The experimental results when the rules are
only used.
</tableCaption>
<bodyText confidence="0.983200142857143">
Table 5 shows the error types by the rules and
their distribution. For example, the error type ‘B-
ADVP I-ADVP’ contains the errors whose true la-
bel is B-ADVP and that are mislabeled by I-ADVP.
There are eight error types, but most errors are re-
lated with noun phrases. We found two reasons for
this:
</bodyText>
<listItem confidence="0.975531153846154">
1. It is difficult to find the beginning of noun
phrases. All nouns appearing successively
without postpositions are not a single noun
phrase. But, they are always predicted to be
single noun phrase by the rules, though they
can be more than one noun phrase.
2. The postposition representing a noun coordi-
nation, ‘4 (wa)’ is very ambiguous. When
‘4 (wa)’ is representing the coordination, the
chunk types of it and its next word should be
“I-NP I-NP”. But, when it is just an adverbial
postposition that implies ‘with’ in English, the
chunk types should be “I-NP B-NP”.
</listItem>
<table confidence="0.9998468">
Decision Tree SVM MBL
Accuracy 97.95±0.24% 98.15±0.20% 97.79±0.29%
Precision 92.29±0.94% 93.63±0.81% 91.41±1.24%
Recall 90.45±0.80% 91.48±0.70% 91.43±0.87%
F-score 91.36±0.85 92.54±0.72 91.38±1.01
</table>
<tableCaption confidence="0.959165">
Table 6: The experimental results of various ma-
chine learning algorithms.
</tableCaption>
<subsectionHeader confidence="0.9862215">
5.3 Performance of Machine Learning
Algorithms
</subsectionHeader>
<bodyText confidence="0.999918111111111">
Table 6 gives the 10-fold cross validation result of
three machine learning algorithms. In each fold, the
corpus is divided into three parts: training (80%),
held-out (10%), test (10%). Since held-out set is
used only to find the best value for the threshold t
in the combined model, it is not used in measuring
the performance of machine learning algorithms.
The machine learning algorithms tested are (i)
memory-based learning (MBL), (ii) decision tree,
and (iii) support vector machines (SVM). We use
C4.5 release 8 (Quinlan, 1993) for decision tree in-
duction and SV Mlight (Joachims, 1998) for support
vector machines, while TiMBL (Daelemans et. al,
2001) is adopted for memory-based learning. De-
cision trees and SVMs use the same attributes with
memory-based learning (see Table 2). Two of the al-
gorithms, memory-based learning and decision tree,
show worse performance than the rules. The F-
scores of memory-based learning and decision tree
are 91.38 and 91.36 respectively, while that of the
rules is 91.87 (see Table 4). On the other hand, sup-
port vector machines present a slightly better perfor-
mance than the rules. The F-score of support vector
machine is 92.54, so the improvement over the rules
is just 0.67.
Table 7 shows the weight of attributes when
only memory-based learning is used. Each value
in this table corresponds to αi in calculating
∆(x, y). The more important is an attribute, the
larger is the weight of it. Thus, the most im-
portant attribute among 17 attributes is Ci−1, the
chunk type of the previous word. On the other
hand, the least important attributes are Wi−3 and
Ci−3. Because the words make less influence
on determining the chunk type of ui in ques-
tion as they become more distant from ui. That
</bodyText>
<table confidence="0.937400727272727">
not exist in the dataset. It is given for the explanation.
Attribute Weight Attribute Weight
Wi−3 0.03 POSi−3 0.04
Wi−2 0.07 POSi−2 0.11
Wi−1 0.17 POSi−1 0.28
Wi 0.22 POSi 0.38
Wi+1 0.14 POSi+1 0.22
Wi+2 0.06 POSi+2 0.09
Wi+3 0.04 POSi+3 0.05
Ci−3 0.03 Ci−2 0.11
Ci−1 0.43
</table>
<tableCaption confidence="0.995755">
Table 7: The weights of the attributes in IB1-IG. The
total sum of the weights is 2.48.
</tableCaption>
<table confidence="0.998962916666667">
fold Precision (%) Recall (%) F-score t
1 94.87 94.12 94.49 1.96
2 93.52 93.85 93.68 1.98
3 95.25 94.72 94.98 1.95
4 95.30 94.32 94.81 1.95
5 92.91 93.54 93.22 1.87
6 94.49 94.50 94.50 1.92
7 95.88 94.35 95.11 1.94
8 94.25 94.18 94.21 1.94
9 92.96 91.97 92.46 1.91
10 95.24 94.02 94.63 1.97
Avg. 94.47±1.04 93.96±0.77 94.21±0.84 1.94
</table>
<tableCaption confidence="0.987712">
Table 8: The final result of the proposed method by
</tableCaption>
<bodyText confidence="0.994150941176471">
combining the rules and the memory-based learning.
The average accuracy is 98.21±0.43.
is, the order of important lexical attributes is
(Wi, Wi−1, Wi+1, Wi−2, Wi+2, Wi+3, Wi−3). The
same phenomenon is found in part-of-speech
(POS) and chunk type (C). In comparing the part-
of-speech information with the lexical information,
we find out that the part-of-speech is more impor-
tant. One possible explanation for this is that the
lexical information is too sparse.
The best performance on English reported is
94.13 in F-score (Zhang et. al, 2001). The reason
why the performance on Korean is lower than that
on English is the curse of dimensionality. That is,
the wider context is required to compensate for the
free order of Korean, but it hurts the performance
(Cherkassky and Mulier, 1998).
</bodyText>
<subsectionHeader confidence="0.996328">
5.4 Performance of the Hybrid Method
</subsectionHeader>
<bodyText confidence="0.999635333333333">
Table 8 shows the final result of the proposed
method. The F-score is 94.21 on the average which
is improvement of 2.34 over the rules only, 1.67 over
support vector machines, and 2.83 over memory-
based learning. In addition, this result is as high as
the performance on English (Zhang et. al, 2001).
</bodyText>
<figure confidence="0.969961">
Rule Only
Hybrid
AoVe Ie Ne Ve
ehrases
</figure>
<figureCaption confidence="0.79219">
Figure 4: The improvement for each kind of phrases
by combining the rules and MBL.
</figureCaption>
<bodyText confidence="0.99994">
The threshold t is set to the value which produces
the best performance on the held-out set. The total
sum of all weights in Table 7 is 2.48. This implies
that when we set t &gt; 2.48, only the rules are ap-
plied since there is no exception with this threshold.
When t = 0.00, only the memory-based learning is
used. Since the memory-based learning determines
the chunk type of wz based on the exceptional cases
of the rules in this case. the performance is poor with
t = 0.00. The best performance is obtained when t
is near 1.94.
Figure 4 shows how much F-score is improved for
each kind of phrases. The average F-score of noun
phrase is 94.54 which is far improved over that of the
rules only. This implies that the exceptional cases of
the rules for noun phrase are well handled by the
memory-based learning. The performance is much
improved for noun phrase and verb phrase, while it
remains same for adverb phrases and independent
phrases. This result can be attributed to the fact that
there are too small number of exceptions for adverb
phrases and independent phrases. Because the ac-
curacy of the rules for these phrases is already high
enough, most cases are covered by the rules. Mem-
ory based learning treats only the exceptions of the
rules, so the improvement by the proposed method
is low for the phrases.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999457">
In order to make the proposed method practical and
applicable to other NLP problems, the following is-
sues are to be discussed:
</bodyText>
<listItem confidence="0.8716645">
1. Why are the rules applied before the
memory-based learning?
</listItem>
<bodyText confidence="0.92863844">
When the rules are efficient and accurate
enough to begin with, it is reasonable to ap-
ply the rules first (Golding and Rosenbloom,
1996). But, if they were deficient in some
way, we should have applied the memory-based
learning first.
2. Why don’t we use all data for the machine
learning method?
In the proposed method, memory-based learn-
ing is used not to find a hypothesis for inter-
preting whole data space but to handle the ex-
ceptions of the rules. If we use all data for both
the rules and memory-based learning, we have
to weight the methods to combine them. But, it
is difficult to know the weights of the methods.
3. Why don’t we convert the memory-based
learning to the rules?
Converting between the rules and the cases in
the memory-based learning tends to yield inef-
ficient or unreliable representation of rules.
The proposed method can be directly applied to
the problems other than chunking Korean if the
proper rules are prepared. The proposed method will
show better performance than the rules or machine
learning methods alone.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999135705882353">
In this paper we have proposed a new method
to learn chunking Korean by combining the hand-
crafted rules and a memory-based learning. Our
method is based on the rules, and the estimates on
chunks by the rules are verified by a memory-based
learning. Since the memory-based learning is an
efficient method to handle exceptional cases of the
rules, it supports the rules by making decisions only
for the exceptions of the rules. That is, the memory-
based learning enhances the rules by efficiently han-
dling the exceptional cases of the rules.
The experiments on STEP 2000 dataset showed
that the proposed method improves the F-score of
the rules by 2.34 and of the memory-based learn-
ing by 2.83. Even compared with support vector
machines, the best machine learning algorithm in
text chunking, it achieved the improvement of 1.67.
</bodyText>
<figure confidence="0.962791818181818">
l-����� 100
98
96
94
92
90
88
86
84
82
80
</figure>
<bodyText confidence="0.9998095">
The improvement was made mainly in noun phrases
among four kinds of phrases in Korean. This is
because the errors of the rules are mostly related
with noun phrases. With relatively many instances
for noun phrases, the memory-based learning could
compensate for the errors of the rules. We also em-
pirically found the threshold value t used to deter-
mine when to apply the rules and when to apply
memory-based learning.
We also discussed some issues in combining a
rule-based method and a memory-based learning.
These issues will help to understand how the method
works and to apply the proposed method to other
problems in natural language processing. Since the
method is general enough, it can be applied to other
problems such as POS tagging and PP attachment.
The memory-based learning showed good perfor-
mance in these problems, but did not reach the state-
of-the-art. We expect that the performance will be
improved by the proposed method.
</bodyText>
<sectionHeader confidence="0.980398" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9997005">
This research was supported by the Korean Ministry
of Education under the BK21-IT program and by the
Korean Ministry of Science and Technology under
NRL and BrainTech programs.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880306122449">
V. Cherkassky and F. Mulier. 1998. Learningfrom Data:
Concepts, Theory, and Methods, John Wiley &amp; Sons,
Inc.
CoNLL. 2000. Shared Task for Computational
Natural Language Learning (CoNLL), http://lcg-
www.uia.ac.be/conll2000/chunking.
T. Cover and P. Hart. 1967. Nearest Neighbor Pat-
tern Classification, IEEE Transactions on Information
Theory, Vol. 13, pp. 21–27.
W. Daelemans, A. Bosch and J. Zavrel. 1999. Forgetting
Exceptions is Harmful in Language Learning, Ma-
chine Learning, Vol. 34, No. 1, pp. 11–41.
W. Daelemans, J. Zavrel, K. Sloot and A. Bosch. 2001.
TiMBL: Tilburg Memory Based Learner, version 4.1,
Reference Guide, ILK 01-04, Tilburg University.
A. Golding and P. Rosenbloom. 1996. Improving Accu-
racy by Combining Rule-based and Case-based Rea-
soning, Artificial Intelligence, Vol. 87, pp. 215–254.
T. Joachims. 1998. Making Large-Scale SVM Learning
Practical, LS8, Universitaet Dortmund.
K.-C. Kim, K.-O. Lee, and Y.-S. Lee. 1995. Korean
Compound Verbals Processing driven by Morpholog-
ical Analysis, Journal of KISS, Vol. 22, No. 9, pp.
1384–1393.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Learning for Chunk Identification, In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning, pp. 142–144.
S.-B. Park and B.-T. Zhang. 2001. Combining a Rule-
based Method and a k-NN for Chunking Korean Text,
In Proceedings of the 19th International Conference
on Computer Processing of Oriental Languages, pp.
225–230.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing, Morgan Kaufmann Publishers.
L. Ramshaw and M. Marcus. 1995. Text Chunking Us-
ing Transformation-Based Learning, In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pp. 82–94.
H.-P. Shin. 1999. Maximally Efficient Syntatic Parsing
with Minimal Resources, In Proceedings of the Con-
ference on Hangul and Korean Language Infomration
Processing, pp. 242–244.
J.-T. Yoon and K.-S. Choi. 1999. Study on KAIST Cor-
pus, CS-TR-99-139, KAIST CS.
T. Zhang, F. Damerau and D. Johnson. 2001. Text
Chunking Using Regularized Winnow, In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics, pp. 539–546.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915258">
<title confidence="0.9993445">Text Chunking by Combining Hand-Crafted Rules and Memory-Based Learning</title>
<author confidence="0.991088">Seong-Bae Park Byoung-Tak Zhang</author>
<affiliation confidence="0.99724">School of Computer Science and Engineering Seoul National University</affiliation>
<address confidence="0.963246">Seoul 151-744, Korea</address>
<abstract confidence="0.99778165">This paper proposes a hybrid of handcrafted rules and a machine learning method for chunking Korean. In the partially free word-order languages such as Korean and Japanese, a small number of rules dominate the performance due to their well-developed postpositions and endings. Thus, the proposed method is primarily based on the rules, and then the residual errors are corrected by adopting a memory-based machine learning method. Since the memory-based learning is an efficient method to handle exceptions in natural language processing, it is good at checking whether the estimates are exceptional cases of the rules and revising them. An evaluation of the method yields the improvement in F-score over the rules or various machine learning methods alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Cherkassky</author>
<author>F Mulier</author>
</authors>
<title>Learningfrom Data: Concepts, Theory, and Methods,</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="2320" citStr="Cherkassky and Mulier, 1998" startWordPosition="360" endWordPosition="363"> the methods are successful even with local information. However, these methods are not appropriate for chunking Korean and Japanese, because such languages have a characteristic of partially free wordorder. That is, there is a very weak positional constraint in these languages. Instead of positional constraints, they have overt postpositions that restrict the syntactic relation and composition of phrases. Thus, unless we concentrate on the postpositions, we must enlarge the neighboring window to get a good hypothesis. However, enlarging the window size will cause the curse of dimensionality (Cherkassky and Mulier, 1998), which results in the deficiency in the generalization performance. Especially in Korean, the postpositions and the endings provide important information for noun phrase and verb phrase chunking respectively. With only a few simple rules using such information, the performance of chunking Korean is as good as the rivaling other inference models such as machine learning algorithms and statistics-based methods (Shin, 1999). Though the rules are approximately correct for most cases drawn from the domain on which the rules are based, the knowledge in the rules is not necessarily well-represented </context>
<context position="19823" citStr="Cherkassky and Mulier, 1998" startWordPosition="3441" endWordPosition="3444">Wi+2, Wi+3, Wi−3). The same phenomenon is found in part-of-speech (POS) and chunk type (C). In comparing the partof-speech information with the lexical information, we find out that the part-of-speech is more important. One possible explanation for this is that the lexical information is too sparse. The best performance on English reported is 94.13 in F-score (Zhang et. al, 2001). The reason why the performance on Korean is lower than that on English is the curse of dimensionality. That is, the wider context is required to compensate for the free order of Korean, but it hurts the performance (Cherkassky and Mulier, 1998). 5.4 Performance of the Hybrid Method Table 8 shows the final result of the proposed method. The F-score is 94.21 on the average which is improvement of 2.34 over the rules only, 1.67 over support vector machines, and 2.83 over memorybased learning. In addition, this result is as high as the performance on English (Zhang et. al, 2001). Rule Only Hybrid AoVe Ie Ne Ve ehrases Figure 4: The improvement for each kind of phrases by combining the rules and MBL. The threshold t is set to the value which produces the best performance on the held-out set. The total sum of all weights in Table 7 is 2.4</context>
</contexts>
<marker>Cherkassky, Mulier, 1998</marker>
<rawString>V. Cherkassky and F. Mulier. 1998. Learningfrom Data: Concepts, Theory, and Methods, John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<title>Shared Task for Computational Natural Language Learning (CoNLL),</title>
<date>2000</date>
<location>http://lcgwww.uia.ac.be/conll2000/chunking.</location>
<contexts>
<context position="12626" citStr="CoNLL, 2000" startWordPosition="2194" endWordPosition="2195">e chunk types of the words in right context are not known in determining the chunk type of wi. 5 Experiments 5.1 Dataset For the evaluation of the proposed method, all experiments are performed on STEP 2000 Korean Chunking dataset (STEP 2000 dataset)2. This dataset is derived from the parsed corpus, which is a product of STEP 2000 project supported by Korean government. The corpus consists of 12,092 sentences with 111,658 phrases and 321,328 words, and the vocabulary size is 16,808. Table 3 summarizes the information on the dataset. The format of the dataset follows that of CoNLL2000 dataset (CoNLL, 2000). Figure 3 shows an example sentence in the dataset3. Each word in the dataset has two additional tags, which are a part-ofspeech tag and a chunk tag. The part-of-speech tags are based on KAIST tagset (Yoon and Choi, 1999). Each phrase can have two kinds of chunk types: BXP and I-XP. In addition to them, there is O chunk type that is used for words which are not part of any chunk. Since there are four types of phrases and one additional chunk type O, there exist nine chunk types. 5.2 Performance of Chunking by Rules Table 4 shows the chunking performance when only the rules are applied. Using </context>
</contexts>
<marker>CoNLL, 2000</marker>
<rawString>CoNLL. 2000. Shared Task for Computational Natural Language Learning (CoNLL), http://lcgwww.uia.ac.be/conll2000/chunking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>P Hart</author>
</authors>
<title>Nearest Neighbor Pattern Classification,</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<pages>21--27</pages>
<contexts>
<context position="10483" citStr="Cover and Hart, 1967" startWordPosition="1815" endWordPosition="1818">adjective, its chunk type is determined by the following rule. 1. If POS(wi−1) = adverb Then yi = I-ADVP. 2. Else yi = B-ADVP. 1This dataset will be explained in Section 5.1. Table 1: The probability that an adverb sequence forms a chunk. 3.4 Independent Phrase Chunking There is no special rule for independent phrase chunking. It can be done only through knowledge base that stores the cases where independent phrases take place. We designed 12 rules for independent phrases. 4 Chunking by Memory-Based Learning Memory-based learning is a direct descent of the k-Nearest Neighbor (k-NN) algorithm (Cover and Hart, 1967). Since many natural language processing (NLP) problems have constraints of a large number of examples and many attributes with different relevance, memory-based learning uses more complex data structure and different speedup optimization from the k-NN. It can be viewed with two components: a learning component and a similarity-based performance component. The learning component involves adding training examples to memory, where all examples are assumed to be fixed-length vectors of n attributes. The similarity between an instance x and all examples y in memory is computed using a distance met</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. Cover and P. Hart. 1967. Nearest Neighbor Pattern Classification, IEEE Transactions on Information Theory, Vol. 13, pp. 21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting Exceptions is Harmful</title>
<date>1999</date>
<booktitle>in Language Learning, Machine Learning,</booktitle>
<volume>34</volume>
<pages>11--41</pages>
<marker>Daelemans, Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. Bosch and J. Zavrel. 1999. Forgetting Exceptions is Harmful in Language Learning, Machine Learning, Vol. 34, No. 1, pp. 11–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Sloot</author>
<author>A Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 4.1, Reference Guide, ILK 01-04,</title>
<date>2001</date>
<institution>Tilburg University.</institution>
<marker>Daelemans, Zavrel, Sloot, Bosch, 2001</marker>
<rawString>W. Daelemans, J. Zavrel, K. Sloot and A. Bosch. 2001. TiMBL: Tilburg Memory Based Learner, version 4.1, Reference Guide, ILK 01-04, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Golding</author>
<author>P Rosenbloom</author>
</authors>
<title>Improving Accuracy by Combining Rule-based and Case-based Reasoning,</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<volume>87</volume>
<pages>215--254</pages>
<contexts>
<context position="21938" citStr="Golding and Rosenbloom, 1996" startWordPosition="3818" endWordPosition="3821">ons for adverb phrases and independent phrases. Because the accuracy of the rules for these phrases is already high enough, most cases are covered by the rules. Memory based learning treats only the exceptions of the rules, so the improvement by the proposed method is low for the phrases. 6 Discussion In order to make the proposed method practical and applicable to other NLP problems, the following issues are to be discussed: 1. Why are the rules applied before the memory-based learning? When the rules are efficient and accurate enough to begin with, it is reasonable to apply the rules first (Golding and Rosenbloom, 1996). But, if they were deficient in some way, we should have applied the memory-based learning first. 2. Why don’t we use all data for the machine learning method? In the proposed method, memory-based learning is used not to find a hypothesis for interpreting whole data space but to handle the exceptions of the rules. If we use all data for both the rules and memory-based learning, we have to weight the methods to combine them. But, it is difficult to know the weights of the methods. 3. Why don’t we convert the memory-based learning to the rules? Converting between the rules and the cases in the </context>
</contexts>
<marker>Golding, Rosenbloom, 1996</marker>
<rawString>A. Golding and P. Rosenbloom. 1996. Improving Accuracy by Combining Rule-based and Case-based Reasoning, Artificial Intelligence, Vol. 87, pp. 215–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<date>1998</date>
<booktitle>Making Large-Scale SVM Learning Practical, LS8,</booktitle>
<institution>Universitaet Dortmund.</institution>
<contexts>
<context position="17148" citStr="Joachims, 1998" startWordPosition="2981" endWordPosition="2982">e Learning Algorithms Table 6 gives the 10-fold cross validation result of three machine learning algorithms. In each fold, the corpus is divided into three parts: training (80%), held-out (10%), test (10%). Since held-out set is used only to find the best value for the threshold t in the combined model, it is not used in measuring the performance of machine learning algorithms. The machine learning algorithms tested are (i) memory-based learning (MBL), (ii) decision tree, and (iii) support vector machines (SVM). We use C4.5 release 8 (Quinlan, 1993) for decision tree induction and SV Mlight (Joachims, 1998) for support vector machines, while TiMBL (Daelemans et. al, 2001) is adopted for memory-based learning. Decision trees and SVMs use the same attributes with memory-based learning (see Table 2). Two of the algorithms, memory-based learning and decision tree, show worse performance than the rules. The Fscores of memory-based learning and decision tree are 91.38 and 91.36 respectively, while that of the rules is 91.87 (see Table 4). On the other hand, support vector machines present a slightly better performance than the rules. The F-score of support vector machine is 92.54, so the improvement o</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Making Large-Scale SVM Learning Practical, LS8, Universitaet Dortmund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-C Kim</author>
<author>K-O Lee</author>
<author>Y-S Lee</author>
</authors>
<title>Korean Compound Verbals Processing driven by Morphological Analysis,</title>
<date>1995</date>
<journal>Journal of KISS,</journal>
<volume>22</volume>
<pages>1384--1393</pages>
<marker>Kim, Lee, Lee, 1995</marker>
<rawString>K.-C. Kim, K.-O. Lee, and Y.-S. Lee. 1995. Korean Compound Verbals Processing driven by Morphological Analysis, Journal of KISS, Vol. 22, No. 9, pp. 1384–1393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of Support Vector Learning for Chunk Identification,</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="1426" citStr="Kudo and Matsumoto, 2000" startWordPosition="219" endWordPosition="222"> the estimates are exceptional cases of the rules and revising them. An evaluation of the method yields the improvement in F-score over the rules or various machine learning methods alone. 1 Introduction Text chunking has been one of the most interesting problems in natural language learning community since the first work of (Ramshaw and Marcus, 1995) using a machine learning method. The main purpose of the machine learning methods applied to this task is to capture the hypothesis that best determine the chunk type of a word, and such methods have shown relatively high performance in English (Kudo and Matsumoto, 2000; Zhang et. al, 2001). In order to do it, various kinds of information, such as lexical information, part-of-speech and grammatical relation, of the neighboring words is used. Since the position of a word plays an important role as a syntactic constraint in English, the methods are successful even with local information. However, these methods are not appropriate for chunking Korean and Japanese, because such languages have a characteristic of partially free wordorder. That is, there is a very weak positional constraint in these languages. Instead of positional constraints, they have overt pos</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of Support Vector Learning for Chunk Identification, In Proceedings of the Fourth Conference on Computational Natural Language Learning, pp. 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-B Park</author>
<author>B-T Zhang</author>
</authors>
<title>Combining a Rulebased Method and a k-NN for Chunking Korean Text,</title>
<date>2001</date>
<booktitle>In Proceedings of the 19th International Conference on Computer Processing of Oriental Languages,</booktitle>
<pages>225--230</pages>
<contexts>
<context position="3867" citStr="Park and Zhang, 2001" startWordPosition="637" endWordPosition="640">is figure describes a sentence-based learning and classification. For Each W ord wi w 1 ... w N (PO S1 ...PO SN) Yes Correctly D eterm ined? Finish N o E rror C ase Library Rule Base Find Error Type Rule Based D eterm ination Classification Phase For Each W ord wi w 1 ... w N (PO S1 ...PO SN) Rule B ased D eterm ination R ule B ase M em ory B ased D eterm ination E rror Case Library Com bination C 1 ... C N pensated for by some special treatments of them for higher performance. To solve this problem, we have proposed a combining method of the rules and the k-nearest neighbor (k-NN) algorithm (Park and Zhang, 2001). The problem in this method is that it has redundant kNNs because it maintains a separate k-NN for each kind of errors made by the rules. In addition, because it applies a k-NN and the rules to each examples, it requires more computations than other inference methods. The goal of this paper is to provide a new method for chunking Korean by combining the hand-crafted rules and a machine learning method. The chunk type of a word in question is determined by the rules, and then verified by the machine learning method. The role of the machine learning method is to determine whether the current co</context>
</contexts>
<marker>Park, Zhang, 2001</marker>
<rawString>S.-B. Park and B.-T. Zhang. 2001. Combining a Rulebased Method and a k-NN for Chunking Korean Text, In Proceedings of the 19th International Conference on Computer Processing of Oriental Languages, pp. 225–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<date>1993</date>
<booktitle>C4.5: Programs for Machine Learning,</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="11443" citStr="Quinlan, 1993" startWordPosition="1988" endWordPosition="1990">e component. The learning component involves adding training examples to memory, where all examples are assumed to be fixed-length vectors of n attributes. The similarity between an instance x and all examples y in memory is computed using a distance metric, ∆(x, y). The chunk type of x is then determined by assigning the most frequent category within the k most similar examples of x. The distance from x and y, ∆(x, y) is defined to be n ∆(x, y) ≡ αiδ(xi, yi), i=1 where αi is the weight of i-th attribute and � 0 if xi = yi, δ(xi, yi) = 1 if xi =� yi. When αi is determined by information gain (Quinlan, 1993), the k-NN algorithm with this metric is called IB1-IG (Daelemans et. al, 2001). All the experiments performed by memory-based learning in this paper are done with IB1-IG. Table 2 shows the attributes of IB1-IG for chunking Korean. To determine the chunk type of a word wi, the lexicons, POS tags, and chunk types of surrounding words are used. For the surrounding words, three words of left context and three words of right context are used for lexicons and POS tags, while two words of left context are used for chunk types. Since chunking is performed sequentially, the chunk types of the words in</context>
<context position="17089" citStr="Quinlan, 1993" startWordPosition="2971" endWordPosition="2972">ous machine learning algorithms. 5.3 Performance of Machine Learning Algorithms Table 6 gives the 10-fold cross validation result of three machine learning algorithms. In each fold, the corpus is divided into three parts: training (80%), held-out (10%), test (10%). Since held-out set is used only to find the best value for the threshold t in the combined model, it is not used in measuring the performance of machine learning algorithms. The machine learning algorithms tested are (i) memory-based learning (MBL), (ii) decision tree, and (iii) support vector machines (SVM). We use C4.5 release 8 (Quinlan, 1993) for decision tree induction and SV Mlight (Joachims, 1998) for support vector machines, while TiMBL (Daelemans et. al, 2001) is adopted for memory-based learning. Decision trees and SVMs use the same attributes with memory-based learning (see Table 2). Two of the algorithms, memory-based learning and decision tree, show worse performance than the rules. The Fscores of memory-based learning and decision tree are 91.38 and 91.36 respectively, while that of the rules is 91.87 (see Table 4). On the other hand, support vector machines present a slightly better performance than the rules. The F-sco</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs for Machine Learning, Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning,</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="1155" citStr="Ramshaw and Marcus, 1995" startWordPosition="173" endWordPosition="176">ethod is primarily based on the rules, and then the residual errors are corrected by adopting a memory-based machine learning method. Since the memory-based learning is an efficient method to handle exceptions in natural language processing, it is good at checking whether the estimates are exceptional cases of the rules and revising them. An evaluation of the method yields the improvement in F-score over the rules or various machine learning methods alone. 1 Introduction Text chunking has been one of the most interesting problems in natural language learning community since the first work of (Ramshaw and Marcus, 1995) using a machine learning method. The main purpose of the machine learning methods applied to this task is to capture the hypothesis that best determine the chunk type of a word, and such methods have shown relatively high performance in English (Kudo and Matsumoto, 2000; Zhang et. al, 2001). In order to do it, various kinds of information, such as lexical information, part-of-speech and grammatical relation, of the neighboring words is used. Since the position of a word plays an important role as a syntactic constraint in English, the methods are successful even with local information. Howeve</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text Chunking Using Transformation-Based Learning, In Proceedings of the Third ACL Workshop on Very Large Corpora, pp. 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-P Shin</author>
</authors>
<title>Maximally Efficient Syntatic Parsing with Minimal Resources,</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Hangul and Korean Language Infomration Processing,</booktitle>
<pages>242--244</pages>
<contexts>
<context position="2745" citStr="Shin, 1999" startWordPosition="425" endWordPosition="426">he postpositions, we must enlarge the neighboring window to get a good hypothesis. However, enlarging the window size will cause the curse of dimensionality (Cherkassky and Mulier, 1998), which results in the deficiency in the generalization performance. Especially in Korean, the postpositions and the endings provide important information for noun phrase and verb phrase chunking respectively. With only a few simple rules using such information, the performance of chunking Korean is as good as the rivaling other inference models such as machine learning algorithms and statistics-based methods (Shin, 1999). Though the rules are approximately correct for most cases drawn from the domain on which the rules are based, the knowledge in the rules is not necessarily well-represented for any given set of cases. Since chunking is usually processed in the earlier step of natural language processing, the errors made in this step have a fatal influence on the following steps. Therefore, the exceptions that are ignored by the rules must be comTraining Phase Figure 1: The structure of Korean chunking model. This figure describes a sentence-based learning and classification. For Each W ord wi w 1 ... w N (PO</context>
<context position="8944" citStr="Shin, 1999" startWordPosition="1551" endWordPosition="1552">ition is possessive, it is still in the mid of noun phrase (Rule 4). The other exception is a relative postfix ‘�a� (jeok)’ (Rule 5). Rule 6 states that a simple relative clause with no sub-constituent also constitutes a noun phrase. Since the adjectives of Korean have no definitive usage, this rule corresponds to the definitive usage of the adjectives in English. 3.2 Verb Phrase Chunking The verb phrase chunking has been studied for a long time under the name of compound verb processing in Korean and shows relatively high accuracy. Shin used a finite state automaton for verb phrase chunking (Shin, 1999), while K.-C. Kim used knowledge-based rules (Kim et. al, 1995). For the consistency with noun phrase chunking, we use the rules in this paper. The rules used are the ones proposed by (Kim et. al, 1995) and the further explanation on the rules is skipped. The number of the rules used is 29. 3.3 Adverb Phrase Chunking When the adverbs appear in succession, they have a great tendency to form an adverb phrase. Though an adverb sequence is not always one adverb phrase, it usually forms one phrase. Table 1 shows this empirically. The usage of the successive adverbs is investigated from STEP 2000 da</context>
</contexts>
<marker>Shin, 1999</marker>
<rawString>H.-P. Shin. 1999. Maximally Efficient Syntatic Parsing with Minimal Resources, In Proceedings of the Conference on Hangul and Korean Language Infomration Processing, pp. 242–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-T Yoon</author>
<author>K-S Choi</author>
</authors>
<date>1999</date>
<journal>Study on KAIST Corpus, CS-TR-99-139, KAIST CS.</journal>
<contexts>
<context position="12848" citStr="Yoon and Choi, 1999" startWordPosition="2233" endWordPosition="2236">Chunking dataset (STEP 2000 dataset)2. This dataset is derived from the parsed corpus, which is a product of STEP 2000 project supported by Korean government. The corpus consists of 12,092 sentences with 111,658 phrases and 321,328 words, and the vocabulary size is 16,808. Table 3 summarizes the information on the dataset. The format of the dataset follows that of CoNLL2000 dataset (CoNLL, 2000). Figure 3 shows an example sentence in the dataset3. Each word in the dataset has two additional tags, which are a part-ofspeech tag and a chunk tag. The part-of-speech tags are based on KAIST tagset (Yoon and Choi, 1999). Each phrase can have two kinds of chunk types: BXP and I-XP. In addition to them, there is O chunk type that is used for words which are not part of any chunk. Since there are four types of phrases and one additional chunk type O, there exist nine chunk types. 5.2 Performance of Chunking by Rules Table 4 shows the chunking performance when only the rules are applied. Using only the rules gives 97.99% of accuracy and 91.87 of F-score. In spite of relatively high accuracy, F-score is somewhat low. Because the important unit of the work in the applications of text chunking is a phrase, F-score </context>
</contexts>
<marker>Yoon, Choi, 1999</marker>
<rawString>J.-T. Yoon and K.-S. Choi. 1999. Study on KAIST Corpus, CS-TR-99-139, KAIST CS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text Chunking Using Regularized Winnow,</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>539--546</pages>
<marker>Zhang, Damerau, Johnson, 2001</marker>
<rawString>T. Zhang, F. Damerau and D. Johnson. 2001. Text Chunking Using Regularized Winnow, In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pp. 539–546.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>