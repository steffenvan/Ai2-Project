<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996873">
Efficient Parsing for Transducer Grammars
</title>
<author confidence="0.999456">
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein
</author>
<affiliation confidence="0.9991585">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.991509">
{denero, mbansal, adpauls, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997318" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932095238095">
The tree-transducer grammars that arise in
current syntactic machine translation systems
are large, flat, and highly lexicalized. We ad-
dress the problem of parsing efficiently with
such grammars in three ways. First, we
present a pair of grammar transformations
that admit an efficient cubic-time CKY-style
parsing algorithm despite leaving most of the
grammar in n-ary form. Second, we show
how the number of intermediate symbols gen-
erated by this transformation can be substan-
tially reduced through binarization choices.
Finally, we describe a two-pass coarse-to-fine
parsing approach that prunes the search space
using predictions from a subset of the origi-
nal grammar. In all, parsing time reduces by
81%. We also describe a coarse-to-fine prun-
ing scheme for forest-based language model
reranking that allows a 100-fold increase in
beam size while reducing decoding time. The
resulting translations improve by 1.3 BLEU.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994985106383">
Current approaches to syntactic machine translation
typically include two statistical models: a syntac-
tic transfer model and an n-gram language model.
Recent innovations have greatly improved the effi-
ciency of language model integration through multi-
pass techniques, such as forest reranking (Huang
and Chiang, 2007), local search (Venugopal et al.,
2007), and coarse-to-fine pruning (Petrov et al.,
2008; Zhang and Gildea, 2008). Meanwhile, trans-
lation grammars have grown in complexity from
simple inversion transduction grammars (Wu, 1997)
to general tree-to-string transducers (Galley et al.,
2004) and have increased in size by including more
synchronous tree fragments (Galley et al., 2006;
Marcu et al., 2006; DeNeefe et al., 2007). As a result
of these trends, the syntactic component of machine
translation decoding can now account for a substan-
tial portion of total decoding time. In this paper,
we focus on efficient methods for parsing with very
large tree-to-string grammars, which have flat n-ary
rules with many adjacent non-terminals, as in Fig-
ure 1. These grammars are sufficiently complex that
the purely syntactic pass of our multi-pass decoder is
the compute-time bottleneck under some conditions.
Given that parsing is well-studied in the mono-
lingual case, it is worth asking why MT grammars
are not simply like those used for syntactic analy-
sis. There are several good reasons. The most im-
portant is that MT grammars must do both analysis
and generation. To generate, it is natural to mem-
orize larger lexical chunks, and so rules are highly
lexicalized. Second, syntax diverges between lan-
guages, and each divergence expands the minimal
domain of translation rules, so rules are large and
flat. Finally, we see most rules very few times, so
it is challenging to subcategorize non-terminals to
the degree done in analytic parsing. This paper de-
velops encodings, algorithms, and pruning strategies
for such grammars.
We first investigate the qualitative properties of
MT grammars, then present a sequence of parsing
methods adapted to their broad characteristics. We
give normal forms which are more appropriate than
Chomsky normal form, leaving the rules mostly flat.
We then describe a CKY-like algorithm which ap-
plies such rules efficiently, working directly over the
n-ary forms in cubic time. We show how thoughtful
</bodyText>
<page confidence="0.965223">
227
</page>
<note confidence="0.908401">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227–235,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.993544647058823">
S !
NNP1 did not slap DT2 green NN3
NNP1 no daba una bofetada a DT2 NN3 verde
S ! NNP no daba una bofetada a DT NN verde
1 2 3 4 5 6 7+
iginal g
90,000
60,000
NP !
xical no
30,000
!
0
S
NNP DT NN
Mary did not slap the green witch
Maria no daba una bofetada a la bruja verde
</figure>
<figureCaption confidence="0.89240775">
Figure 1: (a) A synchronous transducer rule has co-
indexed non-terminals on the source and target side. In-
ternal grammatical structure of the target side has been
omitted. (b) The source-side projection of the rule is a
monolingual source-language rule with target-side gram-
mar symbols. (c) A training sentence pair is annotated
with a target-side parse tree and a word alignment, which
license this rule to be extracted.
</figureCaption>
<bodyText confidence="0.866836375">
binarization can further increase parsing speed, and
we present a new coarse-to-fine scheme that uses
,
rule subsets rather than symbol clustering to build
a coarsegrammar projection. These techniques re-
52500
duce parsing time by 81% in aggregate. Finally,
we demonstrate that we can accelerate forest-based
</bodyText>
<equation confidence="0.321662">
35,000
</equation>
<bodyText confidence="0.81479825">
reranking with a language model by pruning with
1500
information from the parsing pass. This approach
enables a 100-fold increase in maximum beam size,
</bodyText>
<equation confidence="0.9604755">
0
1 3 5 6+
</equation>
<bodyText confidence="0.999385">
improving translation quality by 1.3 BLEU while
decreasing total decoding time.
</bodyText>
<sectionHeader confidence="0.935203" genericHeader="introduction">
2 Tree Transducer Grammars
</sectionHeader>
<bodyText confidence="0.999869466666667">
Tree-to-string transducer grammars consist of
weighted rules like the one depicted in Figure 1.
Each n-ary rule consists of a root symbol, a se-
quence of lexical items and non-terminals on the
source-side, and a fragment of a syntax tree on
the target side. Each non-terminal on the source
side corresponds to a unique one on the target side.
Aligned non-terminals share a grammar symbol de-
rived from a target-side monolingual grammar.
These grammars are learned from word-aligned
sentence pairs annotated with target-side phrase
structure trees. Extraction proceeds by using word
alignments to find correspondences between target-
side constituents and source-side word spans, then
discovering transducer rules that match these con-
</bodyText>
<figureCaption confidence="0.98986">
Figure 2: Transducer grammars are composed of very flat
</figureCaption>
<subsubsectionHeader confidence="0.36997">
Required symbols Sequences to build
</subsubsectionHeader>
<bodyText confidence="0.636498">
rules. Above, the histogram shows rule counts for each
rule size among the 332,000 rules that apply to an indi-
</bodyText>
<equation confidence="0.450309">
TNN NN T,NN
vidual 30-word sentence. The size of a rule is the total
NN NNP NP S DT,NN,NNS
</equation>
<bodyText confidence="0.894968">
number of non-terminals and lexical items in its source-
side yield.
</bodyText>
<equation confidence="0.812587666666667">
Mma
cho LNF mtio
stituent alignments (Galley et al., 2004). Given this
n 67500 b DN er
correspondence, an array of extraction procedures
yields rules that are well-suited to machine trans-
DT+NN ! DT NN NP ! DT+NN NNS
,0
lation (Galley et al., 2006; DeNeefe et al., 2007;
S ! NNP S\NNP
Marcu et al., 006). Rule weights are estimated
22500
</equation>
<bodyText confidence="0.997383">
by discriminatively combining relative frequency
counts and other rule features.
</bodyText>
<equation confidence="0.916189666666667">
0
A transducer grammar G can be projected onto its
1 2 3 4 5 6 7 8 9 10
</equation>
<bodyText confidence="0.999817529411765">
source language, inducing a monolingual grammar.
If we weight each rule by the maximum weight of its
projecting synchronous rules, then parsing with this
projected grammar maximizes the translation model
score for a source sentence. We need not even con-
sider the target side of transducer rules until integrat-
ing an n-gram language model or other non-local
features of the target language.
We conduct experiments with a grammar ex-
tracted from 220 million words of Arabic-English
bitext, extracting rules with up to 6 non-terminals. A
histogram of the size of rules applicable to a typical
30-word sentence appears in Figure 2. The grammar
includes 149 grammatical symbols, an augmentation
of the Penn Treebank symbol set. To evaluate, we
decoded 300 sentences of up to 40 words in length
from the NIST05 Arabic-English test set.
</bodyText>
<sectionHeader confidence="0.984838" genericHeader="method">
3 Efficient Grammar Encodings
</sectionHeader>
<bodyText confidence="0.999277">
Monolingual parsing with a source-projected trans-
ducer grammar is a natural first pass in multi-pass
decoding. These grammars are qualitatively dif-
ferent from syntactic analysis grammars, such as
the lexicalized grammars of Charniak (1997) or the
heavily state-split grammars of Petrov et al. (2006).
</bodyText>
<page confidence="0.995416">
228
</page>
<bodyText confidence="0.916894">
In thissection, we develop an appropriate grammar
</bodyText>
<equation confidence="0.8073185">
NNP1 di not slap DT2 green NN3
S
</equation>
<bodyText confidence="0.80349375">
encoding that enles efficient prsing.
NNP1 no daba una bofetada a DT2 N
It is problematic to convert these grammars into
Chomsky norml form, which CKY requirs. Be-
</bodyText>
<equation confidence="0.6230035">
S ! NNP no daba una bfetada a DT NN verde
cause transducer rules are very flat and contain spe-
cific lexical items, binarization introduces a large
S
</equation>
<bodyText confidence="0.997289">
number of intermediate grammar symbols. Rule size
and lexicalization affect parsing complexity whether
</bodyText>
<sectionHeader confidence="0.370847" genericHeader="method">
NNP DT NN
</sectionHeader>
<bodyText confidence="0.9775955">
the grammar is binarized explicitly (Zhang et al.,
2006) or implicitly binarized using Early-style inter-
</bodyText>
<subsectionHeader confidence="0.669806">
Mary did not slap the green wtch
</subsectionHeader>
<bodyText confidence="0.956865375">
mediate symbols (Zollmann et al., 2006). Moreover,
the resulting binary rules cannot be Markovized to
Maria no daba una bofetada a la bruj verde
merge symbols, as in Klein and Manning (2003), be-
cause each rule is associated with a target-side tree
that cannot be abstracted.
We also do not restrict the form of rules in the
grammar, a common technique in syntactic machine
</bodyText>
<figure confidence="0.257612090909091">
g ,09
eft-branchig
translation. For instance, Zollmann et al. (2006)
,81
follow Chiang (2005) in disallowing adjacent non-
Geedy 1,101
terminals. Watanabe et al. (2006) limit grammars
ptimal (ILP) 443
to Griebach-Normal form. However, general tree
transducer grammars provide excellent translation
0 3,000 6,000 9,000
</figure>
<bodyText confidence="0.9792655">
performance (Galley et al., 2006), and so we focus
on parsing with all available rules.
</bodyText>
<subsectionHeader confidence="0.999305">
3.1 Lexical Normal Form
</subsectionHeader>
<bodyText confidence="0.91314275">
Sequences of consecutive non-terminals complicate
parsing because they require a search over non-
35000
terminal boundaries when applied to a sentence
span. We transform the grammar to ensure that all
1750
rules containing lexical items (lexical rules) do not
contain sequences of non-terminals. We allow both
</bodyText>
<equation confidence="0.942443">
0
unary and binary non-lexical rules.
1 2 3 4
</equation>
<bodyText confidence="0.99880875">
Let L be the set of lexical items and V the set
of non-terminal symbols in the original grammar.
Then, lexical normal form (LNF) limits productions
to two forms:
</bodyText>
<equation confidence="0.999341">
Non-lexical: X → X1(X2)
Lexical: X → (X1)α(X2)
α = w+(Xiw+)∗
</equation>
<bodyText confidence="0.9970145">
Above, all Xi E V and w+ E L+. Symbols in
parentheses are optional. The nucleus α of lexical
rules is a mixed sequence that has lexical items on
each end and no adjacent non-terminals.
Converting a grammar into LNF requires two
steps. In the sequence elimination step, for every
</bodyText>
<figure confidence="0.978296">
Original grammar rules are flat and lexical
S ! NNP no daba una bofetada a DT NN verde
NP ! DT NN NNS
LNF replaces non-terminal sequences in lexical rules
S ! NNP no daba una bofetada a DT+NN verde
DT+NN ! DT NN
Non-lexical rules are binarized using few symbols
Non-lexical rules before binarization:
NP ! DT NN NNS DT+NN ! DT NN
Equivalent binary rules, minimizing symbol count:
NP ! DT+NN NNS DT+NN ! DT NN
Anchored LNF rules are bounded by lexical items
S\NNP ! no daba una bofetada a DT+NN verde
NP ! DT+NN NNS DT+NN ! DT NN
S ! NNP S\NNP
</figure>
<figureCaption confidence="0.6442534">
Figure 3: We transform the original grammar by first
eliminating non-terminal sequences in lexical rules.
Next, we binarize, adding a minimal number of inter-
mediate grammar symbols and binary non-lexical rules.
Finally, anchored LNF further transforms lexical rules
</figureCaption>
<figure confidence="0.326187">
90,000
</figure>
<figureCaption confidence="0.3876495">
to begin and end with lexical items by introducing ad-
ditional symbols.
</figureCaption>
<bodyText confidence="0.852588666666667">
67,500
lexical rule we replace each sequence of consecutive
non-terminals X1 ... Xn with the intermediate sym-
22500
bol X1+...+Xn (abbreviated X1:n) and introduce a
non-lexical rule X1+...+Xn → X1 ... Xn. In the
</bodyText>
<equation confidence="0.719165">
0
binarizationstep, we introduce further intermediate
1 2 3 4 5 6 7+
</equation>
<bodyText confidence="0.8275625">
symbols and rules to binarize all non-lexical rules
0,000
in the grammar, including those added by sequence
elimination.
</bodyText>
<subsectionHeader confidence="0.8410185">
3.2 Non-terminal Binarization
5,000
</subsectionHeader>
<bodyText confidence="0.932279166666667">
Exactly how we binarize non-lexical rules affects the
total number of intermediate symbols introduced by
2,500
the LNF transformation.
Binarization involves selecting a set of symbols
that will allow us to assemble the right-hand side
</bodyText>
<equation confidence="0.870951">
1 2 5 6 7 8 9 10+
</equation>
<bodyText confidence="0.9901885">
X1 ... Xn of every non-lexical rule using binary
productions. This symbol set must at least include
the left-hand side of every rule in the grammar
(lexical and non-lexical), including the intermediate
</bodyText>
<page confidence="0.990679">
229
</page>
<figure confidence="0.995519">
0 3,000 6,000 9,000
</figure>
<figureCaption confidence="0.871646333333333">
Figure 4: The number of non-terminal symbols intro-
ducedto the grammar through LNF binarization depends
70,000
</figureCaption>
<bodyText confidence="0.905934428571429">
upon the policy for binarizing type sequences. This ex-
periment shows results from transforming a grammar that
52,500
has already been filtered for a particular short sentence.
Both the greedy and optimal binarizations use far fewer
35,000
symbols than naive binarizations.
</bodyText>
<equation confidence="0.630945333333333">
symbols X1:n introduced by sequence elimination.
0
3 4
</equation>
<bodyText confidence="0.997728133333334">
To ensure that a symbol sequence X1 ... Xn can
be constructed, we select a split point k and add in-
termediate types X1:k and Xk+1:n to the grammar.
We must also ensure that the sequences X1 ... Xk
and Xk+1 ... Xn can be constructed. As baselines,
we used left-branching (where k = 1 always) and
right-branching (where k = n − 1) binarizations.
We also tested a greedy binarization approach,
choosing k to minimize the number of grammar
symbols introduced. We first try to select k such that
both X1:k and Xk+1:n are already in the grammar.
If no such k exists, we select k such that one of the
intermediate types generated is already used. If no
such k exists again, we choose k = 112n]. This pol-
icy only creates new intermediate types when nec-
essary. Song et al. (2008) propose a similar greedy
approach to binarization that uses corpus statistics to
select common types rather than explicitly reusing
types that have already been introduced.
Finally, we computed an optimal binarization that
explicitly minimizes the number of symbols in the
resulting grammar. We cast the minimization as an
integer linear program (ILP). Let V be the set of
all base non-terminal symbols in the grammar. We
introduce an indicator variable TY for each symbol
Y E V + to indicate that Y is used in the grammar.
Y can be either a base non-terminal symbol Xi or
an intermediate symbol X1:n. We also introduce in-
dicators AY,Z for each pairs of symbols, indicating
that both Y and Z are used in the grammar. Let
</bodyText>
<listItem confidence="0.669246666666667">
L C_ V + be the set of left-hand side symbols for
all lexical and non-lexical rules already in the gram-
mar. Let R be the set of symbol sequences on the
</listItem>
<subsectionHeader confidence="0.427778">
Aco LF rs re d b lc ms
</subsectionHeader>
<bodyText confidence="0.690373">
right-hand side of all non-lexical rules. Then, the
ILP takes the form:
</bodyText>
<table confidence="0.983754777777778">
S\NN � o d
min TY
Y ∈V +
s.t. TY = 1 b Y E L
1 G1: AX1:k,Xk+1:n b X1 ... Xn E R
k
1: TX1:n G AX1:k,Xk+1:n b X1:n
k
AY,Z G TY , AY,Z G TZ b Y, Z
</table>
<bodyText confidence="0.890624">
The solution to this ILP indicates which symbols
appear in a minimal binarization. Equation 1 explic-
</bodyText>
<equation confidence="0.647671">
0
234567+
</equation>
<bodyText confidence="0.816086285714286">
itly minimizes the number of symbols. Equation 2
ensures that all symbols already in the grammar re-
000
main in the grammar.
00Equation 3 does not require that a symbol repre-
sent the entire right-hand side of each non-lexical
rule, but does ensure that each right-hand side se-
</bodyText>
<figure confidence="0.663498">
,000
quence can be built from two subsequence symbols.
,500
Equation 4 ensures that any included intermediate
type can also be built from two subsequence types.
0
</figure>
<bodyText confidence="0.948478692307692">
Finally, Equation 5 ensures that if a pair is used, each
2 3 4 5 6 7 8 9 10+
member of the pair is included. This program can be
optimized with an off-the-shelf ILP solver.1
Figure 4 shows the number of intermediate gram-
mar symbols needed for the four binarization poli-
cies described above for a short sentence. Our ILP
solver could only find optimal solutions for very
short sentences (which have small grammars after
relativization). Because greedy requires very little
time to compute and generates symbol counts that
are close to optimal when both can be computed, we
use it for our remaining experiments.
</bodyText>
<subsectionHeader confidence="0.998582">
3.3 Anchored Lexical Normal Form
</subsectionHeader>
<bodyText confidence="0.999956">
We also consider a further grammar transformation,
anchored lexical normal form (ALNF), in which the
yield of lexical rules must begin and end with a lex-
ical item. As shown in the following section, ALNF
improves parsing performance over LNF by shifting
work from lexical rule applications to non-lexical
</bodyText>
<footnote confidence="0.94697">
1We used lp solve: http://sourceforge.net/projects/lpsolve.
</footnote>
<table confidence="0.579324875">
Right-branching
Left-branching
Greedy
Optimal (ILP)
8,095
1,101
443
5,871
</table>
<page confidence="0.982344">
230
</page>
<bodyText confidence="0.997779">
rule applications. ALNF consists of rules with the
following two forms:
</bodyText>
<equation confidence="0.998023">
Non-lexical: X —* X1(X2)
Lexical: X —* w+(Xiw+)∗
</equation>
<bodyText confidence="0.999947">
To convert a grammar into ALNF, we first transform
it into LNF, then introduce additional binary rules
that split off non-terminal symbols from the ends of
lexical rules, as shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="method">
4 Efficient CKY Parsing
</sectionHeader>
<bodyText confidence="0.999933857142857">
We now describe a CKY-style parsing algorithm for
grammars in LNF. The dynamic program is orga-
nized into spans Sij and computes the Viterbi score
w(i, j, X) for each edge Sij[X], the weight of the
maximum parse over words i+1 to j, rooted at sym-
bol X. For each Sij, computation proceeds in three
phases: binary, lexical, and unary.
</bodyText>
<subsectionHeader confidence="0.998193">
4.1 Applying Non-lexical Binary Rules
</subsectionHeader>
<bodyText confidence="0.99988075">
For a span Sij, we first apply the binary non-lexical
rules just as in standard CKY, computing an interme-
diate Viterbi score wb(i, j, X). Let ωr be the weight
of rule r. Then, wb(i, j, X) =
</bodyText>
<equation confidence="0.93403">
w(i, k, X1) · w(k, j, X2).
</equation>
<bodyText confidence="0.998871666666667">
The quantities w(i, k, X1) and w(k, j, X2) will have
already been computed by the dynamic program.
The work in this phase is cubic in sentence length.
</bodyText>
<subsectionHeader confidence="0.998872">
4.2 Applying Lexical Rules
</subsectionHeader>
<bodyText confidence="0.927194857142857">
On the other hand, lexical rules in LNF can be ap-
plied without binarization, because they only apply
to particular spans that contain the appropriate lexi-
cal items. For a given Sij, we first compute all the le-
gal mappings of each rule onto the span. A mapping
consists of a correspondence between non-terminals
in the rule and subspans of Sij. In practice, there
is typically only one way that a lexical rule in LNF
can map onto a span, because most lexical items will
appear only once in the span.
Let m be a legal mapping and r its corresponding
rule. Let S(i)
k� [X] be the edge mapped to the ith non-
terminal of r under m, and ωr the weight of r. Then,
</bodyText>
<equation confidence="0.9509115">
wl(i, j, X) = max
m
</equation>
<bodyText confidence="0.98892625">
Again, w(k, `, X) will have been computed by the
dynamic program. Assuming only a constant num-
ber of mappings per rule per span, the work in this
phase is quadratic. We can then merge wl and wb:
</bodyText>
<equation confidence="0.579176">
w(i, j, X) = max(wl(i, j, X), wb(i, j, X)).
</equation>
<bodyText confidence="0.99995775">
To efficiently compute mappings, we store lexi-
cal rules in a trie (or suffix array) – a searchable
graph that indexes rules according to their sequence
of lexical items and non-terminals. This data struc-
ture has been used similarly to index whole training
sentences for efficient retrieval (Lopez, 2007). To
find all rules that map onto a span, we traverse the
trie using depth-first search.
</bodyText>
<subsectionHeader confidence="0.999544">
4.3 Applying Unary Rules
</subsectionHeader>
<bodyText confidence="0.994043">
Unary non-lexical rules are applied after lexical
rules and non-lexical binary rules.
</bodyText>
<equation confidence="0.9923025">
w(i, j, X) = max
r:r=X→X1
</equation>
<bodyText confidence="0.9997542">
While this definition is recursive, we allow only one
unary rule application per symbol X at each span
to prevent infinite derivations. This choice does not
limit the generality of our algorithm: chains of unar-
ies can always be collapsed via a unary closure.
</bodyText>
<subsectionHeader confidence="0.999692">
4.4 Bounding Split Points for Binary Rules
</subsectionHeader>
<bodyText confidence="0.999704636363636">
Non-lexical binary rules can in principle apply to
any span Sij where j − i &gt; 2, using any split point
k such that i &lt; k &lt; j. In practice, however, many
rules cannot apply to many (i, k, j) triples because
the symbols for their children have not been con-
structed successfully over the subspans Sik and Skj.
Therefore, the precise looping order over rules and
split points can influence computation time.
We found the following nested looping order for
the binary phase of processing an edge Sij[X] gave
the fastest parsing times for these grammars:
</bodyText>
<listItem confidence="0.9809345">
1. Loop over symbols X1 for the left child
2. Loop over all rules X —* X1X2 containing X1
3. Loop over split points k : i &lt; k &lt; j
4. Update wb(i, j, X) as necessary
</listItem>
<bodyText confidence="0.989695">
This looping order allows for early stopping via
additional bookkeeping in the algorithm. We track
the following statistics as we parse:
</bodyText>
<equation confidence="0.6753341">
max
r=X→X1X2
ωr
j−1
max
k=i+1
�ωr w(k, `, X).
S(i)
k� [X]
ωrw(i, j, X1).
</equation>
<page confidence="0.979061">
231
</page>
<table confidence="0.99894025">
Grammar Bound checks Parsing time
LNF no 264
LNF yes 181
ALNF yes 104
</table>
<tableCaption confidence="0.8632124">
Table 1: Adding bound checks to CKY and transforming
the grammar from LNF to anchored LNF reduce parsing
time by 61% for 300 sentences of length 40 or less. No
approximations have been applied, so all three scenarios
produce no search errors. Parsing time is in minutes.
</tableCaption>
<bodyText confidence="0.998537">
minEND(i, X), maxEND(i, X): The minimum and
maximum position k for which symbol X was
successfully built over SZk.
minSTART(j, X), maxSTART(j, X): The minimum
and maximum position k for which symbol X
was successfully built over Skj.
We then bound k by mink and maxk in the inner
loop using these statistics. If ever mink &gt; maxk,
then the loop is terminated early.
</bodyText>
<listItem confidence="0.951187111111111">
1. set mink = i + 1, maxk = j − 1
2. loop over symbols X1 for the left child
mink = max(mink, minEND(i, X1))
maxk = min(maxk, maxEND(i, X1))
3. loop over rules X → X1X2
mink = max(mink, minSTART(j, X2))
maxk = min(maxk, maxSTART(j, X2))
4. loop over split points k : mink G k G maxk
5. update wb(i, j, X) as necessary
</listItem>
<bodyText confidence="0.998428666666667">
In this way, we eliminate unnecessary work by
avoiding split points that we know beforehand can-
not contribute to wb(i, j, X).
</bodyText>
<subsectionHeader confidence="0.999752">
4.5 Parsing Time Results
</subsectionHeader>
<bodyText confidence="0.999876">
Table 1 shows the decrease in parsing time from in-
cluding these bound checks, as well as switching
from lexical normal form to anchored LNF.
Using ALNF rather than LNF increases the num-
ber of grammar symbols and non-lexical binary
rules, but makes parsing more efficient in three
ways. First, it decreases the number of spans for
which a lexical rule has a legal mapping. In this way,
ALNF effectively shifts work from the lexical phase
to the binary phase. Second, ALNF reduces the time
spent searching the trie for mappings, because the
first transition into the trie must use an edge with a
lexical item. Finally, ALNF improves the frequency
that, when a lexical rule matches a span, we have
successfully built every edge Skt[X] in the mapping
for that rule. This frequency increases from 45% to
96% with ALNF.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="method">
5 Coarse-to-Fine Search
</sectionHeader>
<bodyText confidence="0.999990954545455">
We now consider two coarse-to-fine approximate
search procedures for parsing with these grammars.
Our first approach clusters grammar symbols to-
gether during the coarse parsing pass, following
work in analytic parsing (Charniak and Caraballo,
1998; Petrov and Klein, 2007). We collapse all
intermediate non-terminal grammar symbols (e.g.,
NP) to a single coarse symbol X, while pre-terminal
symbols (e.g., NN) are hand-clustered into 7 classes
(nouns, verbals, adjectives, punctuation, etc.). We
then project the rules of the original grammar into
this simplified symbol set, weighting each rule of
the coarse grammar by the maximum weight of any
rule that mapped onto it.
In our second and more successful approach, we
select a subset of grammar symbols. We then in-
clude only and all rules that can be built using those
symbols. Because the grammar includes many rules
that are compositions of smaller rules, parsing with
a subset of the grammar still provides meaningful
scores that can be used to prune base grammar sym-
bols while parsing under the full grammar.
</bodyText>
<subsectionHeader confidence="0.998042">
5.1 Symbol Selection
</subsectionHeader>
<bodyText confidence="0.9998815">
To compress the grammar, we select a small sub-
set of symbols that allow us to retain as much of
the original grammar as possible. We use a voting
scheme to select the symbol subset. After conver-
sion to LNF (or ALNF), each lexical rule in the orig-
inal grammar votes for the symbols that are required
to build it. A rule votes as many times as it was ob-
served in the training data to promote frequent rules.
We then select the top nl symbols by vote count and
include them in the coarse grammar C.
We would also like to retain as many non-lexical
rules from the original grammar as possible, but the
right-hand side of each rule can be binarized in many
ways. We again use voting, but this time each non-
</bodyText>
<page confidence="0.983555">
232
</page>
<table confidence="0.99839175">
Pruning Minutes Model score BLEU
No pruning 104 60,179 44.84
Clustering 79 60,179 44.84
Subsets 50 60,163 44.82
</table>
<tableCaption confidence="0.909516166666667">
Table 2: Coarse-to-fine pruning speeds up parsing time
with minimal effect on either model score or translation
quality. The coarse grammar built using symbol subsets
outperforms clustering grammar symbols, reducing pars-
ing time by 52%. These experiments do not include a
language model.
</tableCaption>
<bodyText confidence="0.9998756">
lexical rule votes for its yield, a sequence of sym-
bols. We select the top nu symbol sequences as the
set R of right-hand sides.
Finally, we augment the symbol set of C with in-
termediate symbols that can construct all sequences
in R, using only binary rules. This step again re-
quires choosing a binarization for each sequence,
such that a minimal number of additional symbols is
introduced. We use the greedy approach from Sec-
tion 3.2. We then include in C all rules from the
original grammar that can be built from the symbols
we have chosen. Surprisingly, we are able to re-
tain 76% of the grammar rules while excluding 92%
of the grammar symbols2, which speeds up parsing
substantially.
</bodyText>
<subsectionHeader confidence="0.999619">
5.2 Max Marginal Thresholding
</subsectionHeader>
<bodyText confidence="0.99995025">
We parse first with the coarse grammar to find the
Viterbi derivation score for each edge Sij[X]. We
then perform a Viterbi outside pass over the chart,
like a standard outside pass but replacing E with
max (Goodman, 1999). The product of an edge’s
Viterbi score and its Viterbi outside score gives a
max marginal, the score of the maximal parse that
uses the edge.
We then prune away regions of the chart that de-
viate in their coarse max marginal from the global
Viterbi score by a fixed margin tuned on a develop-
ment set. Table 2 shows that both methods of con-
structing a coarse grammar are effective in pruning,
but selecting symbol subsets outperformed the more
typical clustering approach, reducing parsing time
by an additional factor of 2.
</bodyText>
<footnote confidence="0.9199355">
2We used n, of 500 and nv. of 4000 for experiments. These
parameters were tuned on a development set.
</footnote>
<sectionHeader confidence="0.965198" genericHeader="method">
6 Language Model Integration
</sectionHeader>
<bodyText confidence="0.99997415">
Large n-gram language models (LMs) are critical
to the performance of machine translation systems.
Recent innovations have managed the complexity
of LM integration using multi-pass architectures.
Zhang and Gildea (2008) describes a coarse-to-fine
approach that iteratively increases the order of the
LM. Petrov et al. (2008) describes an additional
coarse-to-fine hierarchy over language projections.
Both of these approaches integrate LMs via bottom-
up dynamic programs that employ beam search. As
an alternative, Huang and Chiang (2007) describes a
forest-based reranking algorithm called cube grow-
ing, which also employs beam search, but focuses
computation only where necessary in a top-down
pass through a parse forest.
In this section, we show that the coarse-to-fine
idea of constraining each pass using marginal pre-
dictions of the previous pass also applies effectively
to cube growing. Max marginal predictions from the
parse can substantially reduce LM integration time.
</bodyText>
<subsectionHeader confidence="0.933102">
6.1 Language Model Forest Reranking
</subsectionHeader>
<bodyText confidence="0.999988782608696">
Parsing produces a forest of derivations, where each
edge in the forest holds its Viterbi (or one-best)
derivation under the transducer grammar. In forest
reranking via cube growing, edges in the forest pro-
duce k-best lists of derivations that are scored by
both the grammar and an n-gram language model.
Using ALNF, each edge must first generate a k-best
list of derivations that are not scored by the language
model. These derivations are then flattened to re-
move the binarization introduced by ALNF, so that
the resulting derivations are each rooted by an n-
ary rule r from the original grammar. The leaves of
r correspond to sub-edges in the chart, which are
recursively queried for their best language-model-
scored derivations. These sub-derivations are com-
bined by r, and new n-grams at the edges of these
derivations are scored by the language model.
The language-model-scored derivations for the
edge are placed on a priority queue. The top of
the priority queue is repeatedly removed, and its
successors added back on to the queue, until k
language-model-scored derivations have been dis-
covered. These k derivations are then sorted and
</bodyText>
<page confidence="0.995349">
233
</page>
<table confidence="0.998130166666667">
Pruning Max BLEU TM LM Total Inside Outside LM Total
strategy beam score score score time time time time
No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346
CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239
CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241
CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253
</table>
<tableCaption confidence="0.99804425">
Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220
million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.
Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar. Coarse-to-fine reranking used
max marginals to constrain the reranking pass. Coarse-to-fine parse + rerank employed both of these approximations.
</tableCaption>
<bodyText confidence="0.600448">
supplied to parent edges upon request.3
</bodyText>
<subsectionHeader confidence="0.908657">
6.2 Coarse-to-Fine Parsing
</subsectionHeader>
<bodyText confidence="0.999976461538462">
Even with this efficient reranking algorithm, inte-
grating a language model substantially increased de-
coding time and memory use. As a baseline, we
reranked using a small fixed-size beam of 20 deriva-
tions at each edge. Larger beams exceeded the mem-
ory of our hardware. Results appear in Table 3.
Coarse-to-fine parsing before LM integration sub-
stantially improved language model reranking time.
By pruning the chart with max marginals from the
coarse symbol subset grammar from Section 5, we
were able to rerank with beams of length 200, lead-
ing to a 0.8 BLEU increase and a 31% reduction in
total decoding time.
</bodyText>
<subsectionHeader confidence="0.976744">
6.3 Coarse-to-Fine Forest Reranking
</subsectionHeader>
<bodyText confidence="0.990509481481481">
We realized similar performance and speed bene-
fits by instead pruning with max marginals from the
full grammar. We found that LM reranking explored
many edges with low max marginals, but used few
of them in the final decoder output. Following the
coarse-to-fine paradigm, we restricted the reranker
to edges with a max marginal above a fixed thresh-
old. Furthermore, we varied the beam size of each
edge based on the parse. Let Δ,,t be the ratio of
the max marginal for edge m to the global Viterbi
derivation for the sentence. We used a beam of size
[k · 2ln Δ_] for each edge.
Computing max marginals under the full gram-
mar required an additional outside pass over the full
parse forest, adding substantially to parsing time.
3Huang and Chiang (2007) describes the cube growing al-
gorithm in further detail, including the precise form of the suc-
cessor function for derivations.
However, soft coarse-to-fine pruning based on these
max marginals also allowed for beams up to length
200, yielding a 1.0 BLEU increase over the baseline
and a 30% reduction in total decoding time.
We also combined the coarse-to-fine parsing ap-
proach with this soft coarse-to-fine reranker. Tiling
these approximate search methods allowed another
10-fold increase in beam size, further improving
BLEU while only slightly increasing decoding time.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999959375">
As translation grammars increase in complexity
while innovations drive down the computational cost
of language model integration, the efficiency of the
parsing phase of machine translation decoding is be-
coming increasingly important. Our grammar nor-
mal form, CKY improvements, and symbol subset
coarse-to-fine procedure reduced parsing time for
large transducer grammars by 81%.
These techniques also improved forest-based lan-
guage model reranking. A full decoding pass with-
out any of our innovations required 511 minutes us-
ing only small beams. Coarse-to-fine pruning in
both the parsing and language model passes allowed
a 100-fold increase in beam size, giving a perfor-
mance improvement of 1.3 BLEU while decreasing
total decoding time by 50%.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994706">
This work was enabled by the Information Sci-
ences Institute Natural Language Group, primarily
through the invaluable assistance of Jens Voeckler,
and was supported by the National Science Founda-
tion (NSF) under grant IIS-0643742.
</bodyText>
<page confidence="0.996576">
234
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850906976744">
Eugene Charniak and Sharon Caraballo. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
In Computational Linguistics.
Eugene Charniak. 1997. Statistical techniques for natu-
ral language parsing. In National Conference on Arti-
ficial Intelligence.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
The Annual Conference of the Association for Compu-
tational Linguistics.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the Association for
Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In The Conference on Empiri-
cal Methods in Natural Language Processing.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In The Annual Conference of
the Association for Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In The Conference on Empirical
Methods in Natural Language Processing.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In The Con-
ference on Empirical Methods in Natural Language
Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In The Annual Conference
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In The Annual Conference of the Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In North American Chapter of the Associ-
ation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In The Statistical Machine Translation
Workshop at the North American Association for Com-
putational Linguistics Conference.
</reference>
<page confidence="0.998528">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.855901">
<title confidence="0.999575">Efficient Parsing for Transducer Grammars</title>
<author confidence="0.991097">Mohit Bansal DeNero</author>
<author confidence="0.991097">Adam Pauls</author>
<affiliation confidence="0.9999495">Computer Science University of California,</affiliation>
<email confidence="0.952342">mbansal,adpauls,</email>
<abstract confidence="0.995704681818182">The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the in form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Caraballo</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="21810" citStr="Charniak and Caraballo, 1998" startWordPosition="3707" endWordPosition="3710"> Second, ALNF reduces the time spent searching the trie for mappings, because the first transition into the trie must use an edge with a lexical item. Finally, ALNF improves the frequency that, when a lexical rule matches a span, we have successfully built every edge Skt[X] in the mapping for that rule. This frequency increases from 45% to 96% with ALNF. 5 Coarse-to-Fine Search We now consider two coarse-to-fine approximate search procedures for parsing with these grammars. Our first approach clusters grammar symbols together during the coarse parsing pass, following work in analytic parsing (Charniak and Caraballo, 1998; Petrov and Klein, 2007). We collapse all intermediate non-terminal grammar symbols (e.g., NP) to a single coarse symbol X, while pre-terminal symbols (e.g., NN) are hand-clustered into 7 classes (nouns, verbals, adjectives, punctuation, etc.). We then project the rules of the original grammar into this simplified symbol set, weighting each rule of the coarse grammar by the maximum weight of any rule that mapped onto it. In our second and more successful approach, we select a subset of grammar symbols. We then include only and all rules that can be built using those symbols. Because the gramm</context>
</contexts>
<marker>Charniak, Caraballo, 1998</marker>
<rawString>Eugene Charniak and Sharon Caraballo. 1998. New figures of merit for best-first probabilistic chart parsing. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical techniques for natural language parsing.</title>
<date>1997</date>
<booktitle>In National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7648" citStr="Charniak (1997)" startWordPosition="1223" endWordPosition="1224">xt, extracting rules with up to 6 non-terminals. A histogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2. The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set. To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set. 3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding. These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al. (2006). 228 In thissection, we develop an appropriate grammar NNP1 di not slap DT2 green NN3 S encoding that enles efficient prsing. NNP1 no daba una bofetada a DT2 N It is problematic to convert these grammars into Chomsky norml form, which CKY requirs. BeS ! NNP no daba una bfetada a DT NN verde cause transducer rules are very flat and contain specific lexical items, binarization introduces a large S number of intermediate grammar symbols. Rule size and lexicalization affect parsing complexity whether NNP DT NN the grammar is binarized ex</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical techniques for natural language parsing. In National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8824" citStr="Chiang (2005)" startWordPosition="1421" endWordPosition="1422">NNP DT NN the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using Early-style interMary did not slap the green wtch mediate symbols (Zollmann et al., 2006). Moreover, the resulting binary rules cannot be Markovized to Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Watanabe et al. (2006) limit grammars ptimal (ILP) 443 to Griebach-Normal form. However, general tree transducer grammars provide excellent translation 0 3,000 6,000 9,000 performance (Galley et al., 2006), and so we focus on parsing with all available rules. 3.1 Lexical Normal Form Sequences of consecutive non-terminals complicate parsing because they require a search over non35000 terminal boundaries when applied to a sentence span. We transform the grammar to ensure that all 1750 rules containing lexical items (lexical rules) do not contain</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1892" citStr="DeNeefe et al., 2007" startWordPosition="272" endWordPosition="275">del and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1. These grammars are sufficiently complex that the purely syntactic pass of our multi-pass decoder is the compute-time bottleneck under some conditions. Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those</context>
<context position="6328" citStr="DeNeefe et al., 2007" startWordPosition="1006" endWordPosition="1009">: Transducer grammars are composed of very flat Required symbols Sequences to build rules. Above, the histogram shows rule counts for each rule size among the 332,000 rules that apply to an indiTNN NN T,NN vidual 30-word sentence. The size of a rule is the total NN NNP NP S DT,NN,NNS number of non-terminals and lexical items in its sourceside yield. Mma cho LNF mtio stituent alignments (Galley et al., 2004). Given this n 67500 b DN er correspondence, an array of extraction procedures yields rules that are well-suited to machine transDT+NN ! DT NN NP ! DT+NN NNS ,0 lation (Galley et al., 2006; DeNeefe et al., 2007; S ! NNP S\NNP Marcu et al., 006). Rule weights are estimated 22500 by discriminatively combining relative frequency counts and other rule features. 0 A transducer grammar G can be projected onto its 1 2 3 4 5 6 7 8 9 10 source language, inducing a monolingual grammar. If we weight each rule by the maximum weight of its projecting synchronous rules, then parsing with this projected grammar maximizes the translation model score for a source sentence. We need not even consider the target side of transducer rules until integrating an n-gram language model or other non-local features of the targe</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1756" citStr="Galley et al., 2004" startWordPosition="249" endWordPosition="252">U. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1. These grammars are sufficiently complex that the purely syntactic pass of our multi-pass decoder is the compute-time bottleneck under s</context>
<context position="6118" citStr="Galley et al., 2004" startWordPosition="967" endWordPosition="970">e structure trees. Extraction proceeds by using word alignments to find correspondences between targetside constituents and source-side word spans, then discovering transducer rules that match these conFigure 2: Transducer grammars are composed of very flat Required symbols Sequences to build rules. Above, the histogram shows rule counts for each rule size among the 332,000 rules that apply to an indiTNN NN T,NN vidual 30-word sentence. The size of a rule is the total NN NNP NP S DT,NN,NNS number of non-terminals and lexical items in its sourceside yield. Mma cho LNF mtio stituent alignments (Galley et al., 2004). Given this n 67500 b DN er correspondence, an array of extraction procedures yields rules that are well-suited to machine transDT+NN ! DT NN NP ! DT+NN NNS ,0 lation (Galley et al., 2006; DeNeefe et al., 2007; S ! NNP S\NNP Marcu et al., 006). Rule weights are estimated 22500 by discriminatively combining relative frequency counts and other rule features. 0 A transducer grammar G can be projected onto its 1 2 3 4 5 6 7 8 9 10 source language, inducing a monolingual grammar. If we weight each rule by the maximum weight of its projecting synchronous rules, then parsing with this projected gram</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1849" citStr="Galley et al., 2006" startWordPosition="264" endWordPosition="267">atistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1. These grammars are sufficiently complex that the purely syntactic pass of our multi-pass decoder is the compute-time bottleneck under some conditions. Given that parsing is well-studied in the monolingual case, it is worth askin</context>
<context position="6306" citStr="Galley et al., 2006" startWordPosition="1002" endWordPosition="1005">tch these conFigure 2: Transducer grammars are composed of very flat Required symbols Sequences to build rules. Above, the histogram shows rule counts for each rule size among the 332,000 rules that apply to an indiTNN NN T,NN vidual 30-word sentence. The size of a rule is the total NN NNP NP S DT,NN,NNS number of non-terminals and lexical items in its sourceside yield. Mma cho LNF mtio stituent alignments (Galley et al., 2004). Given this n 67500 b DN er correspondence, an array of extraction procedures yields rules that are well-suited to machine transDT+NN ! DT NN NP ! DT+NN NNS ,0 lation (Galley et al., 2006; DeNeefe et al., 2007; S ! NNP S\NNP Marcu et al., 006). Rule weights are estimated 22500 by discriminatively combining relative frequency counts and other rule features. 0 A transducer grammar G can be projected onto its 1 2 3 4 5 6 7 8 9 10 source language, inducing a monolingual grammar. If we weight each rule by the maximum weight of its projecting synchronous rules, then parsing with this projected grammar maximizes the translation model score for a source sentence. We need not even consider the target side of transducer rules until integrating an n-gram language model or other non-local</context>
<context position="9080" citStr="Galley et al., 2006" startWordPosition="1455" endWordPosition="1458">Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Watanabe et al. (2006) limit grammars ptimal (ILP) 443 to Griebach-Normal form. However, general tree transducer grammars provide excellent translation 0 3,000 6,000 9,000 performance (Galley et al., 2006), and so we focus on parsing with all available rules. 3.1 Lexical Normal Form Sequences of consecutive non-terminals complicate parsing because they require a search over non35000 terminal boundaries when applied to a sentence span. We transform the grammar to ensure that all 1750 rules containing lexical items (lexical rules) do not contain sequences of non-terminals. We allow both 0 unary and binary non-lexical rules. 1 2 3 4 Let L be the set of lexical items and V the set of non-terminal symbols in the original grammar. Then, lexical normal form (LNF) limits productions to two forms: Non-l</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing. Computational Linguistics.</title>
<date>1999</date>
<contexts>
<context position="24695" citStr="Goodman, 1999" startWordPosition="4209" endWordPosition="4210">such that a minimal number of additional symbols is introduced. We use the greedy approach from Section 3.2. We then include in C all rules from the original grammar that can be built from the symbols we have chosen. Surprisingly, we are able to retain 76% of the grammar rules while excluding 92% of the grammar symbols2, which speeds up parsing substantially. 5.2 Max Marginal Thresholding We parse first with the coarse grammar to find the Viterbi derivation score for each edge Sij[X]. We then perform a Viterbi outside pass over the chart, like a standard outside pass but replacing E with max (Goodman, 1999). The product of an edge’s Viterbi score and its Viterbi outside score gives a max marginal, the score of the maximal parse that uses the edge. We then prune away regions of the chart that deviate in their coarse max marginal from the global Viterbi score by a fixed margin tuned on a development set. Table 2 shows that both methods of constructing a coarse grammar are effective in pruning, but selecting symbol subsets outperformed the more typical clustering approach, reducing parsing time by an additional factor of 2. 2We used n, of 500 and nv. of 4000 for experiments. These parameters were t</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1470" citStr="Huang and Chiang, 2007" startWordPosition="209" endWordPosition="212">s from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus o</context>
<context position="25888" citStr="Huang and Chiang (2007)" startWordPosition="4398" endWordPosition="4401">ments. These parameters were tuned on a development set. 6 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing. Max marginal predictions from the parse can substantially reduce LM integration time. 6.1 Language Model Forest Reranking Parsing produces a forest of derivations, where each edge in the forest holds its Viterbi (or one-best) derivation under </context>
<context position="29780" citStr="Huang and Chiang (2007)" startWordPosition="5041" endWordPosition="5044"> reranking explored many edges with low max marginals, but used few of them in the final decoder output. Following the coarse-to-fine paradigm, we restricted the reranker to edges with a max marginal above a fixed threshold. Furthermore, we varied the beam size of each edge based on the parse. Let Δ,,t be the ratio of the max marginal for edge m to the global Viterbi derivation for the sentence. We used a beam of size [k · 2ln Δ_] for each edge. Computing max marginals under the full grammar required an additional outside pass over the full parse forest, adding substantially to parsing time. 3Huang and Chiang (2007) describes the cube growing algorithm in further detail, including the precise form of the successor function for derivations. However, soft coarse-to-fine pruning based on these max marginals also allowed for beams up to length 200, yielding a 1.0 BLEU increase over the baseline and a 30% reduction in total decoding time. We also combined the coarse-to-fine parsing approach with this soft coarse-to-fine reranker. Tiling these approximate search methods allowed another 10-fold increase in beam size, further improving BLEU while only slightly increasing decoding time. 7 Conclusion As translatio</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8548" citStr="Klein and Manning (2003)" startWordPosition="1373" endWordPosition="1376">y norml form, which CKY requirs. BeS ! NNP no daba una bfetada a DT NN verde cause transducer rules are very flat and contain specific lexical items, binarization introduces a large S number of intermediate grammar symbols. Rule size and lexicalization affect parsing complexity whether NNP DT NN the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using Early-style interMary did not slap the green wtch mediate symbols (Zollmann et al., 2006). Moreover, the resulting binary rules cannot be Markovized to Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Watanabe et al. (2006) limit grammars ptimal (ILP) 443 to Griebach-Normal form. However, general tree transducer grammars provide excellent translation 0 3,000 6,000 9,000 performance (Galley et al., 2006), and so we focus on parsing with all available rules. 3.1 Lexical N</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18094" citStr="Lopez, 2007" startWordPosition="3049" endWordPosition="3050">minal of r under m, and ωr the weight of r. Then, wl(i, j, X) = max m Again, w(k, `, X) will have been computed by the dynamic program. Assuming only a constant number of mappings per rule per span, the work in this phase is quadratic. We can then merge wl and wb: w(i, j, X) = max(wl(i, j, X), wb(i, j, X)). To efficiently compute mappings, we store lexical rules in a trie (or suffix array) – a searchable graph that indexes rules according to their sequence of lexical items and non-terminals. This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). To find all rules that map onto a span, we traverse the trie using depth-first search. 4.3 Applying Unary Rules Unary non-lexical rules are applied after lexical rules and non-lexical binary rules. w(i, j, X) = max r:r=X→X1 While this definition is recursive, we allow only one unary rule application per symbol X at each span to prevent infinite derivations. This choice does not limit the generality of our algorithm: chains of unaries can always be collapsed via a unary closure. 4.4 Bounding Split Points for Binary Rules Non-lexical binary rules can in principle apply to any span Sij where j </context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1869" citStr="Marcu et al., 2006" startWordPosition="268" endWordPosition="271">yntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1. These grammars are sufficiently complex that the purely syntactic pass of our multi-pass decoder is the compute-time bottleneck under some conditions. Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars ar</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21835" citStr="Petrov and Klein, 2007" startWordPosition="3711" endWordPosition="3714"> spent searching the trie for mappings, because the first transition into the trie must use an edge with a lexical item. Finally, ALNF improves the frequency that, when a lexical rule matches a span, we have successfully built every edge Skt[X] in the mapping for that rule. This frequency increases from 45% to 96% with ALNF. 5 Coarse-to-Fine Search We now consider two coarse-to-fine approximate search procedures for parsing with these grammars. Our first approach clusters grammar symbols together during the coarse parsing pass, following work in analytic parsing (Charniak and Caraballo, 1998; Petrov and Klein, 2007). We collapse all intermediate non-terminal grammar symbols (e.g., NP) to a single coarse symbol X, while pre-terminal symbols (e.g., NN) are hand-clustered into 7 classes (nouns, verbals, adjectives, punctuation, etc.). We then project the rules of the original grammar into this simplified symbol set, weighting each rule of the coarse grammar by the maximum weight of any rule that mapped onto it. In our second and more successful approach, we select a subset of grammar symbols. We then include only and all rules that can be built using those symbols. Because the grammar includes many rules th</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7708" citStr="Petrov et al. (2006)" startWordPosition="1231" endWordPosition="1234">ogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2. The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set. To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set. 3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding. These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al. (2006). 228 In thissection, we develop an appropriate grammar NNP1 di not slap DT2 green NN3 S encoding that enles efficient prsing. NNP1 no daba una bofetada a DT2 N It is problematic to convert these grammars into Chomsky norml form, which CKY requirs. BeS ! NNP no daba una bfetada a DT NN verde cause transducer rules are very flat and contain specific lexical items, binarization introduces a large S number of intermediate grammar symbols. Rule size and lexicalization affect parsing complexity whether NNP DT NN the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1558" citStr="Petrov et al., 2008" startWordPosition="222" endWordPosition="225">be a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat</context>
<context position="25675" citStr="Petrov et al. (2008)" startWordPosition="4368" endWordPosition="4371">rammar are effective in pruning, but selecting symbol subsets outperformed the more typical clustering approach, reducing parsing time by an additional factor of 2. 2We used n, of 500 and nv. of 4000 for experiments. These parameters were tuned on a development set. 6 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing. Max marginal predictions from </context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinying Song</author>
<author>Shilin Ding</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Better binarization for the CKY parsing.</title>
<date>2008</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13026" citStr="Song et al. (2008)" startWordPosition="2132" endWordPosition="2135">ust also ensure that the sequences X1 ... Xk and Xk+1 ... Xn can be constructed. As baselines, we used left-branching (where k = 1 always) and right-branching (where k = n − 1) binarizations. We also tested a greedy binarization approach, choosing k to minimize the number of grammar symbols introduced. We first try to select k such that both X1:k and Xk+1:n are already in the grammar. If no such k exists, we select k such that one of the intermediate types generated is already used. If no such k exists again, we choose k = 112n]. This policy only creates new intermediate types when necessary. Song et al. (2008) propose a similar greedy approach to binarization that uses corpus statistics to select common types rather than explicitly reusing types that have already been introduced. Finally, we computed an optimal binarization that explicitly minimizes the number of symbols in the resulting grammar. We cast the minimization as an integer linear program (ILP). Let V be the set of all base non-terminal symbols in the grammar. We introduce an indicator variable TY for each symbol Y E V + to indicate that Y is used in the grammar. Y can be either a base non-terminal symbol Xi or an intermediate symbol X1:</context>
</contexts>
<marker>Song, Ding, Lin, 2008</marker>
<rawString>Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better binarization for the CKY parsing. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT. In</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.</booktitle>
<contexts>
<context position="1509" citStr="Venugopal et al., 2007" startWordPosition="215" endWordPosition="218">. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with ve</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Stephan Vogel. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8897" citStr="Watanabe et al. (2006)" startWordPosition="1430" endWordPosition="1433">6) or implicitly binarized using Early-style interMary did not slap the green wtch mediate symbols (Zollmann et al., 2006). Moreover, the resulting binary rules cannot be Markovized to Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Watanabe et al. (2006) limit grammars ptimal (ILP) 443 to Griebach-Normal form. However, general tree transducer grammars provide excellent translation 0 3,000 6,000 9,000 performance (Galley et al., 2006), and so we focus on parsing with all available rules. 3.1 Lexical Normal Form Sequences of consecutive non-terminals complicate parsing because they require a search over non35000 terminal boundaries when applied to a sentence span. We transform the grammar to ensure that all 1750 rules containing lexical items (lexical rules) do not contain sequences of non-terminals. We allow both 0 unary and binary non-lexical</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="1696" citStr="Wu, 1997" startWordPosition="243" endWordPosition="244">me. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1. These grammars are sufficiently complex that the purely syntactic pass of o</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1583" citStr="Zhang and Gildea, 2008" startWordPosition="226" endWordPosition="229">runing scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007). As a result of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time. In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many ad</context>
<context position="25570" citStr="Zhang and Gildea (2008)" startWordPosition="4352" endWordPosition="4355">ore by a fixed margin tuned on a development set. Table 2 shows that both methods of constructing a coarse grammar are effective in pruning, but selecting symbol subsets outperformed the more typical clustering approach, reducing parsing time by an additional factor of 2. 2We used n, of 500 and nv. of 4000 for experiments. These parameters were tuned on a development set. 6 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal </context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<journal>In North American Chapter of the Association for Computational Linguistics.</journal>
<contexts>
<context position="8277" citStr="Zhang et al., 2006" startWordPosition="1328" endWordPosition="1331">avily state-split grammars of Petrov et al. (2006). 228 In thissection, we develop an appropriate grammar NNP1 di not slap DT2 green NN3 S encoding that enles efficient prsing. NNP1 no daba una bofetada a DT2 N It is problematic to convert these grammars into Chomsky norml form, which CKY requirs. BeS ! NNP no daba una bfetada a DT NN verde cause transducer rules are very flat and contain specific lexical items, binarization introduces a large S number of intermediate grammar symbols. Rule size and lexicalization affect parsing complexity whether NNP DT NN the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using Early-style interMary did not slap the green wtch mediate symbols (Zollmann et al., 2006). Moreover, the resulting binary rules cannot be Markovized to Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Wa</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In The Statistical Machine Translation Workshop at the North American Association for Computational Linguistics Conference.</booktitle>
<contexts>
<context position="8397" citStr="Zollmann et al., 2006" startWordPosition="1347" endWordPosition="1350"> slap DT2 green NN3 S encoding that enles efficient prsing. NNP1 no daba una bofetada a DT2 N It is problematic to convert these grammars into Chomsky norml form, which CKY requirs. BeS ! NNP no daba una bfetada a DT NN verde cause transducer rules are very flat and contain specific lexical items, binarization introduces a large S number of intermediate grammar symbols. Rule size and lexicalization affect parsing complexity whether NNP DT NN the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using Early-style interMary did not slap the green wtch mediate symbols (Zollmann et al., 2006). Moreover, the resulting binary rules cannot be Markovized to Maria no daba una bofetada a la bruj verde merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the grammar, a common technique in syntactic machine g ,09 eft-branchig translation. For instance, Zollmann et al. (2006) ,81 follow Chiang (2005) in disallowing adjacent nonGeedy 1,101 terminals. Watanabe et al. (2006) limit grammars ptimal (ILP) 443 to Griebach-Normal form. However, general tree transducer grammars </context>
</contexts>
<marker>Zollmann, Venugopal, Vogel, 2006</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, and Stephan Vogel. 2006. Syntax augmented machine translation via chart parsing. In The Statistical Machine Translation Workshop at the North American Association for Computational Linguistics Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>