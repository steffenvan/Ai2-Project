<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000643">
<title confidence="0.9983205">
Labeling the Languages of Words in Mixed-Language Documents using
Weakly Supervised Methods
</title>
<author confidence="0.997912">
Ben King
</author>
<affiliation confidence="0.997827">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.948178">
Ann Arbor, MI
</address>
<email confidence="0.998524">
benking@umich.edu
</email>
<author confidence="0.996035">
Steven Abney
</author>
<affiliation confidence="0.9979595">
Department of Linguistics
University of Michigan
</affiliation>
<address confidence="0.94833">
Ann Arbor, MI
</address>
<email confidence="0.999035">
abney@umich.edu
</email>
<sectionHeader confidence="0.99867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993078545454545">
In this paper we consider the problem of label-
ing the languages of words in mixed-language
documents. This problem is approached in a
weakly supervised fashion, as a sequence la-
beling problem with monolingual text sam-
ples for training data. Among the approaches
evaluated, a conditional random field model
trained with generalized expectation criteria
was the most accurate and performed consis-
tently as the amount of training data was var-
ied.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876259259259">
Language identification is a well-studied problem
(Hughes et al., 2006), but it is typically only studied
in its canonical text-classification formulation, iden-
tifying a document’s language given sample texts
from a few different languages. But there are sev-
eral other interesting and useful formulations of the
problem that have received relatively little attention.
Here, we focus on the problem of labeling the lan-
guages of individual words within a multilingual
document. To our knowledge, this is the first paper
to specifically address this problem.
Our own motivation for studying this problem
stems from issues encountered while attempting to
build language resources for minority languages. In
trying to extend parts of Kevin Scannell’s Cr´ubad´an
project (Scannell, 2007), which automatically builds
minority language corpora from the Web, we found
that the majority of webpages that contain text in
a minority language also contain text in other lan-
guages. Since Scannell’s method builds these cor-
pora by bootstrapping from the pages that were re-
trieved, the corpus-building process can go disas-
trously wrong without accounting for this problem.
And any resources, such as a lexicon, created from
the corpus will also be incorrect.
In this paper, we explore techniques for per-
forming language identification at the word level in
mixed language documents. Our results show that
one can do better than independent word language
classification, as there are clues in a word’s context:
words of one language are frequently surrounded by
words in the same language, and many documents
have patterns that may be marked by the presence of
certain words or punctuation. The methods in this
paper also outperform sentence-level language iden-
tification, which is too coarse to capture most of the
shifts between language.
To evaluate our methods, we collected and man-
ually annotated a corpus of over 250,000 words
of bilingual (though mostly non-parallel) text from
the web. After running several different weakly-
supervised learning methods, we found that a condi-
tional random field model trained with generalized
expectation criteria is the most accurate and per-
forms quite consistently as the amount of training
data is varied.
In section 2, we review the related work. In sec-
tion 3, we define the task and describe the data and
its annotation. Because the task of language identi-
fication for individual words has not been explicitly
studied in the literature, and because of its impor-
tance to the overall task, we examine the features
and methods that work best for independent word
language identification in section 4. We begin to ex-
</bodyText>
<page confidence="0.931276">
1110
</page>
<note confidence="0.469904">
Proceedings of NAACL-HLT 2013, pages 1110–1119,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999819">
amine the larger problem of labeling the language
of words in context in section 5 by describing our
methods. In section 6, we describe the evaluation
and present the results. We present our error analy-
sis in section 7 and conclude in section 8.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999805683333333">
Language identification is one of the older NLP
problems (Beesley, 1988), especially in regards to
spoken language (House and Neuburg, 1977), and
has received a fair share of attention through the
years (Hughes et al., 2006). In its standard formu-
lation, language identification assumes monolingual
documents and attempts to classify each document
according to its language from some closed set of
known languages.
Many approaches have been proposed, such as
Markov models (Dunning, 1994), Monte Carlo
methods (Poutsma, 2002), and more recently sup-
port vector machines with string kernels, but nearly
all approaches use the n-gram features first sug-
gested by (Cavnar and Trenkle, 1994). Performance
of language identification is generally very high with
large documents, usually in excess of 99% accuracy,
but Xia et al. (2009) mention that current methods
still can perform quite poorly when the class of po-
tential languages is very large or the texts to be clas-
sified are very short.
This paper attempts to address three of the on-
going issues specifically mentioned by Hughes et
al. (2006) in their survey of textual language iden-
tification: supporting minority languages, sparse or
impoverished training data, and multilingual docu-
ments.
A number of methods have been proposed in re-
cent years to apply to the problems of unsuper-
vised and weakly-supervised learning. Excluding
self- and co-training methods, these methods can
be categorized into two broad classes: those which
bootstrap from a small number of tokens (some-
times called prototypes) (Collins and Singer, 1999;
Haghighi and Klein, 2006), and those which impose
constraints on the underlying unsupervised learning
problem (Chang et al., 2007; Bellare et al., 2009;
Druck et al., 2008; Ganchev et al., 2010).
Constraint-based weakly supervised learning has
been applied to some sequence labeling problems,
through such methods as contrastive estimation
(Smith and Eisner, 2005), generalized expectation
criteria (Mann and McCallum, 2008), alternating
projections (Singh et al., 2010), and posterior reg-
ularization (Ganchev et al., 2010).
Perhaps the work that is most similar to this work
is the study of code-switching within NLP literature.
Most of the work done has been on automatically
identifying code-switch points (Joshi, 1982; Solorio
and Liu, 2008). The problem of identifying lan-
guage in the presence of code-switching has seen
the most attention in the realm of speech process-
ing (Chu et al., 2007; Lyu and Lyu, 2008), among
many others. Though code-switching has been well-
studied linguistically, it is only one possible rea-
son to explain why a document contains multiple
languages, and is actually one of the less common
causes observed in our corpus. For that reason, we
approach this problem more generally, assuming no
specific generative process behind multilingual text.
</bodyText>
<sectionHeader confidence="0.993301" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.99997308">
The task we describe in this paper is a sequence
labeling problem, labeling a word in running text
according to the language to which it belongs. In
the interest of being able to produce reliable hu-
man annotations, we limit ourselves to texts with
exactly two languages represented, though the tech-
niques developed in this paper would certainly be
applicable to documents with more than two lan-
guages. The two languages represented in the paper
are known a priori by the labeler and the only train-
ing data available to the labeler is a small amount
of sample text in each of the two languages repre-
sented.
In most NLP sequence labeling problems, the re-
searchers can safely assume that each sequence (but
not each item in the sequence) is independent and
identically distributed (iid) according to some un-
derlying distribution common to all the documents.
For example, it is safe to assume that a sentence
drawn from WSJ section 23 can be labeled by a
model trained on the other sections. With the task
of this paper we cannot assume that sequences from
different documents are iid, (e.g. One document
may have 90% of its words in Basque, while another
only has 20%), but we do make the simplifying as-
</bodyText>
<page confidence="0.988633">
1111
</page>
<bodyText confidence="0.999837566666667">
sumption that sequences within the same document
are iid.
Because of this difference, the labeler is presented
each document separately and must label its words
independently of any other document. And the train-
ing data for this task is not in the form of labeled
sequences. Rather, the models in this task are given
two monolingual example texts which are used only
to learn a model for individual instances. Any se-
quential dependencies between words must be boot-
strapped from the document. It is this aspect of
the problem that makes it well-suited for weakly-
supervised learning.
It is worth considering whether this problem is
best approached at the word level, or if perhaps
sentence- or paragraph-level language identification
would suffice for this task. In those cases, we could
easily segment the text at the sentence or paragraph
level and feed those segments to an existing lan-
guage identifier. To answer this question we seg-
mented our corpus into sentences by splitting at ev-
ery period, exclamation point, or question mark (an
overly agressive approximation of sentence segmen-
tation). Even if every sentence was given the cor-
rect majority label under this sentence segmentation,
the maximum possible word-level accuracy that a
sentence-level classifier could achieve is 85.8%, and
even though this number reflects quite optimistic
conditions, it is still much lower than the methods
of this paper are able to achieve.
</bodyText>
<subsectionHeader confidence="0.999032">
3.1 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999718625">
To build a corpus of mixed language documents, we
used the BootCat tool (Baroni and Bernardini, 2004)
seeded with words from a minority language. Boot-
Cat is designed to automatically collect webpages
on a specific topic by repeatedly searching for key-
words from a topic-specific set of seed words. We
found that this method works equally well for lan-
guages as for topics, when seeded with words from
a specific language. Once BootCat returned a col-
lection of documents, we manually identified docu-
ments from the set that contained text in both the tar-
get language and in English, but did not contain text
in any other languages. Since the problem becomes
trivial when the languages do not share a character
set, we limited ourselves to languages with a Latin
orthography.
</bodyText>
<table confidence="0.9996278125">
Language # words Language # words
Azerbaijani 4114 Lingala 1359
Banjar 10485 Lombard 18512
Basque 5488 Malagasy 6779
Cebuano 17994 Nahuatl 1133
Chippewa 15721 Ojibwa 24974
Cornish 2284 Oromo 28636
Croatian 17318 Pular 3648
Czech 886 Serbian 2457
Faroese 8307 Slovak 8403
Fulfulde 458 Somali 11613
Hausa 2899 Sotho 8198
Hungarian 9598 Tswana 879
Igbo 11828 Uzbek 43
Kiribati 2187 Yoruba 4845
Kurdish 531 Zulu 20783
</table>
<tableCaption confidence="0.998542">
Table 1: Languages present in the corpus and their
</tableCaption>
<bodyText confidence="0.991268230769231">
number of words before separating out English text.
We found that there was an important balance to
be struck concerning the popularity of a language. If
a language is not spoken widely enough, then there
is little chance of finding any text in that language on
the Web. Conversely if a language is too widely spo-
ken, then it is difficult to find mixed-language pages
for it. The list of languages present in the corpus
and the number of words in each language reflects
this balance as seen in Table 1.
For researchers who wish to make use this data,
the set of annotations used in this paper is available
from the first author’s website1.
</bodyText>
<subsectionHeader confidence="0.999384">
3.2 Annotation
</subsectionHeader>
<bodyText confidence="0.99995375">
Before the human annotators were presented with
the mixed-language documents fetched by Boot-
Cat, the documents were first stripped of all HTML
markup, converted to Unicode, and had HTML es-
cape sequences replaced with the proper Unicode
characters. Documents that had any encoding er-
rors (e.g. original page used a mixture of encodings)
were excluded from the corpus.
</bodyText>
<footnote confidence="0.993119333333333">
1http://www-personal.umich.edu/˜benking/
resources/mixed-language-annotations-
release-v1.0.tgz
</footnote>
<page confidence="0.993071">
1112
</page>
<bodyText confidence="0.7575427">
ENG: because of LUTARU.Thank you ntate T.T! Sevice...
SOT: Retselisitsoemonethi ekare jwale hotla sebetswa ...
ENG: Lesotho is heading 4 development #big-ups Mr ...
SOT: Basotho bare monoana hao its’upe.
ENG: Just do the job and lets see what you are made ...
SOT: Malerato Mokoena Ntate Thabane, molimo ...
ENG: It is God who reigns and if God is seen in your ...
SOT: Mathabo Letsie http://www.facebook.com/taole....
ENG: As Zuma did he should introduce a way of we can ...
SOT: Msekhotho Matona a rona ha a hlomamisoe, re ...
</bodyText>
<tableCaption confidence="0.785728">
Table 2: An example of text from an annotated
English-Sotho web page.
</tableCaption>
<bodyText confidence="0.999930366666666">
Since there are many different reasons that the
language in a document may change (e.g. code-
switching, change of authors, borrowing) and many
variations thereof, we attempted to create a broad
set of annotation rules that would cover many cases,
rather than writing a large number of very specific
rules. In cases when the language use was ambigu-
ous, the annotators were instructed simply to make
their best guess. Table 2 shows an example of an
annotated document.
Generally, only well-digested English loanwords
and borrowings were to be marked as belonging to
the foreign language. If a word appeared in the con-
text of both languages, it was permissible for that
word to receive different labels at different times,
depending on its context.
Ordinary proper names (like “John Williams” or
“Chicago”) were to be marked as belonging to the
language of the context in which they appear. This
rule also applied to abbreviations (like “FIFA” or
“BBC”). The exception to this rule was proper
names composed of common nouns (like “Stairway
to Heaven” or “American Red Cross”) and to abbre-
viations that spelled out English words, which were
to be marked as belonging to the language of the
words they were composed of.
The annotators were instructed not to assign la-
bels to numbers or punctuation, but they were al-
lowed to use numbers as punctuation as clues for as-
signing other labels.
</bodyText>
<subsectionHeader confidence="0.99901">
3.3 Human Agreement
</subsectionHeader>
<bodyText confidence="0.998174">
To verify that the annotation rules were reasonable
and led to a problem that could potentially be solved
by a computer, we had each of the annotators mark
</bodyText>
<table confidence="0.999654823529412">
Language # words Language # words
Azerbaijani 211 Lingala 1816
Banjar 450 Lombard 2955
Basque 1378 Malagasy 4038
Cebuano 1898 Nahuatl 3544
Chippewa 92 Ojibwa 167
Cornish 2096 Oromo 1443
Croatian 1505 Pular 1285
Czech 1503 Serbian 1515
English 16469 Slovak 1504
Faroese 1585 Somali 1871
Fulfulde 1097 Sotho 2154
Hausa 2677 Tswana 2191
Hungarian 1541 Uzbek 1533
Igbo 2079 Yoruba 2454
Kiribati 1891 Zulu 1075
Kurdish 1674
</table>
<tableCaption confidence="0.8754265">
Table 3: Number of total words of training data for
each language.
</tableCaption>
<bodyText confidence="0.999614">
up a small shared set of a few hundred words from
each of eight documents, in order to measure the
inter-annotator agreement.
The average actual agreement was 0.988, with 0.5
agreement expected by chance for a kappa of 0.975.
</bodyText>
<subsectionHeader confidence="0.987517">
3.4 Training Data
</subsectionHeader>
<bodyText confidence="0.999893">
Following Scannell (2007), we collected small
monolingual samples of 643 languages from four
sources: the Universal Declaration of Human
Rights2, non-English Wikipedias3, the Jehovah’s
Witnesses website4, and the Rosetta project (Lands-
bergen, 1989).
Only 30 of these languages ended up being used
in experiments. Table 3 shows the sizes of the mono-
lingual samples of the languages used in this paper.
</bodyText>
<footnote confidence="0.9980313">
2The Universal Declaration of Human Rights is a document
created by the United Nations and translated into many lan-
guages. As of February 2011 there were 365 versions available
from http://www.unicode.org/udhr/
3As of February 2011, there were 113 Wikipedias in differ-
ent languages. Current versions of Wikipedia can be accessed
from http://meta.wikimedia.org/wiki/List of
Wikipedias
4As of February 2011, there were 310 versions of the site
available at http://www.watchtower.org
</footnote>
<page confidence="0.96819">
1113
</page>
<figure confidence="0.9968278">
Sampled Words
Accuracy
0 200 400 600 800 1,000
0.9
0.8
0.7
logistic regression
decision tree
winnow2
naive Bayes
</figure>
<bodyText confidence="0.999231285714286">
They range from 92 for Chippewa to 16469 for En-
glish. Most of the languages have between 1300 and
1600 words in their example text. To attempt to mit-
igate variation caused by the sizes of these language
samples, we sample an equal number of words with
replacement from each of English and a second lan-
guage to create the training data.
</bodyText>
<sectionHeader confidence="0.982343" genericHeader="method">
4 Word-level Language Classification
</sectionHeader>
<bodyText confidence="0.9999951">
We shift our attention momentarily to a subproblem
of the overall task: independent word-level language
classification. While the task of language identifica-
tion has been studied extensively at the document,
sentence, and query level, little or no work has been
done at the level of an individual word. For this rea-
son, we feel it is prudent to formally evaluate the fea-
tures and classifiers which perform most effectively
at the task of word language classification (ignoring
any sequential dependencies at this point).
</bodyText>
<subsectionHeader confidence="0.759888">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999874111111111">
We used a logistic regression classifier to experiment
with combinations of the following features: charac-
ter unigrams, bigrams, trigrams, 4-grams, 5-grams,
and the full word. For these experiments, the train-
ing data consisted of 1000 words sampled uniformly
with replacement from the sample text in the appro-
priate languages. Table 4 shows the accuracies that
the classifier achieved when using different sets of
features averaged over 10 independent runs.
</bodyText>
<table confidence="0.99787325">
Features Accuracy
Unigrams 0.8056
Bigrams 0.8783
Trigrams 0.8491
4-grams 0.7846
5-grams 0.6977
{1,2,3,4,5}-grams 0.8817
{1,2,3,4,5}-grams, word 0.8819
</table>
<tableCaption confidence="0.982028">
Table 4: Logistic regression accuracy when trained
using varying features.
</tableCaption>
<bodyText confidence="0.98178175">
The use of all available features seems to be the
best option, and we use the full set of features in
all proceeding experiments. This result also concurs
with the findigs of (Cavnar and Trenkle, 1994), who
</bodyText>
<figureCaption confidence="0.9156246">
Figure 1: Learning curves for logistic regression,
naive Bayes, decision tree, and Winnow2 on the in-
dependent word classification problem as the num-
ber of sampled words in each training example
changes from 10 to 1000.
</figureCaption>
<bodyText confidence="0.9208555">
found 1-5-grams to be most effective for document
language classification.
</bodyText>
<subsectionHeader confidence="0.950762">
4.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.9998805">
Using all available features, we compare four MAL-
LET (McCallum, 2002) classifiers: logistic regres-
sion, naive Bayes, decision tree, and Winnow2. Fig-
ure 1 shows the learning curves for each classifier as
the number of sampled words comprising each train-
ing example is varied from 10 to 1000.
Since a naive Bayes classifier gave the best per-
formance in most experiments, we use naive Bayes
as a representative word classifier for the rest of the
paper.
</bodyText>
<sectionHeader confidence="0.999211" genericHeader="method">
5 Methods
</sectionHeader>
<bodyText confidence="0.9999067">
Moving onto the main task of this paper, labeling
sequences of words in documents according to their
languages, we use this section to describe our meth-
ods.
Since training data for this task is limited and is
of a different type than the evaluation data (labeled
instances from monolingual example texts vs. la-
beled sequences from the multilingual document),
we approach the problem with weakly- and semi-
supervised methods.
</bodyText>
<page confidence="0.980624">
1114
</page>
<bodyText confidence="0.9983346">
The sequence labeling methods are presented
with a few new sequence-relevant features, which
are not applicable to independent word classification
(since these features do not appear in the training
data):
</bodyText>
<listItem confidence="0.981867166666667">
• a feature for the presence of each possible non-
word character (punctuation or digit) between
the previous and the current words
• a feature for the presence of each possible non-
word character between the current and next
words
</listItem>
<bodyText confidence="0.999879571428571">
In addition to independent word classification,
which was covered in section 4, we also imple-
ment a conditional random field model trained with
generalized expectation criteria, a hidden Markov
model (HMM) trained with expectation maximiza-
tion (EM), and a logistic regression model trained
with generalized expectation criteria.
We had also considered that a semi-Markov CRF
(Sarawagi and Cohen, 2004) could be useful if
we could model segment lengths (a non-Markovian
feature), but we found that gold-standard segment
lengths did not seem to be distributed according to
any canonical distribution, and we did not have a re-
liable way to estimate these segment lengths.
</bodyText>
<subsectionHeader confidence="0.561188">
5.1 Conditional Random Field Model trained
with Generalized Expectation
</subsectionHeader>
<bodyText confidence="0.9976674">
Generalized expectation (GE) criteria (Druck et al.,
2008) are terms added to the objective function of
a learning algorithm which specify preferences for
the learned model. When the model is a linear
chain conditional random field (CRF) model, we can
straightforwardly express these criteria in the objec-
tive function with a KL-divergence term between the
expected values of the current model p� and the pre-
ferred model p� (Mann and McCallum, 2008).
Practically, to compute these expectations, we
produce the smoothed MLE on the output label dis-
tribution for every feature observed in the training
data. For example, the trigram “ter” may occur
27 times in the English sample text and 34 times
in the other sample text, leading to an MLE of
p(eng|ter) ,: 0.44.
Because we do not expect the true marginal label
distribution to be uniform (i.e. the document may
not have equal numbers of words in each language),
we first estimate the expected marginal label distri-
bution by classifying each word in the document in-
dependently using naive Bayes and taking the result-
ing counts of labels produced by the classifier as an
MLE estimate for it: p(eng) and p(non).
We use these terms to bias the expected label dis-
tributions over each feature. Let Feng and Fnon re-
spectively be the collections of all training data fea-
tures with the two labels. For every label l E G =
{eng, non} and every feature f E Feng UFnon, we
calculate
</bodyText>
<equation confidence="0.991046">
count(f, F�) + δ p(l)
p(l|f) = x puniform(l),
count (f , Ui Fi) + δ|G |
</equation>
<bodyText confidence="0.9997649">
the biased maximum likelihood expected output
label distribution. To avoid having p(l|f) = 0,
which can cause the KL-divergence to be undefined,
we perform additive smoothing with δ = 0.5 on the
counts before multiplying with the biasing term.
We use the implementation of CRF with GE cri-
teria from MALLET (McCallum, 2002), which uses
a gradient descent algorithm to optimize the objec-
tive function. (Mann and McCallum, 2008; Druck,
2011)
</bodyText>
<subsectionHeader confidence="0.996027">
5.2 Hidden Markov Model trained with
Expectation Maximization
</subsectionHeader>
<bodyText confidence="0.999977153846154">
A second method we used was a hidden Markov
model (HMM) trained iteratively using the Expec-
tation Maximization algorithm (Dempster et al.,
1977). Here an HMM is preferable to a CRF be-
cause it is a generative model and therefore uses pa-
rameters with simple interpretations. In the case of
an HMM, it is easy to estimate emission and transi-
tion probabilities using an external method and then
set these directly.
To initialize the HMM, we use a uniform distri-
bution for transition probabilities, and produce the
emission probabilities by using a naive Bayes clas-
sifier trained over the two small language samples.
</bodyText>
<equation confidence="0.9756645">
� log pO(y(d) |x(d)) − 2k θk
O(θ; D, U) = o,2
d
− λD(p||pe)
</equation>
<page confidence="0.933393">
1115
</page>
<bodyText confidence="0.999970210526316">
In the expectation step, we simply pass the docu-
ment through the HMM and record the labels it pro-
duces for each word in the document.
In the maximization step, we produce maximum-
likelihood estimates for transition probabilities from
the transitions between the labels produced. To
estimate emission probabilities, we retrain a naive
Bayes classifier on the small language samples along
the set of words from the document that were labeled
as being in the respective language. We iterated this
process until convergence, which usually took fewer
than 10 iterations.
We additionally experimented with a naive Bayes
classifier trained by EM in the same fashion, except
that it had no transition probabilities to update. This
classifier’s performance was almost identical to that
of the GE-trained MaxEnt method mentioned in the
following section, so we omit it from the results and
analysis for that reason.
</bodyText>
<subsectionHeader confidence="0.9866135">
5.3 Logistic Regression trained with
Generalized Expectation
</subsectionHeader>
<bodyText confidence="0.9999303125">
GE criteria can also be straightforwardly applied to
the weakly supervised training of logistic regression
models. The special case where the constraints spec-
ified are over marginal label distributions, is called
label regularization.
As with the CRF constraint creation, here we first
use an ordinary supervised naive Bayes classifier in
order to estimate the marginal label distributions for
the document, which can be used to create more ac-
curate output label expectations that are biased to
the marginal label distributions over all words in the
document.
We use the MALLET implementation of a GE-
trained logistic regression classifier, which opti-
mizes the objective function using a gradient descent
algorithm.
</bodyText>
<subsectionHeader confidence="0.96466">
5.4 Word-level Classification
</subsectionHeader>
<bodyText confidence="0.999864">
Our fourth method served as a baseline and did
not involve any sequence labeling, only independent
classification of words. Since naive Bayes was the
best performer among word classification methods,
we use that the representative of independent word
classification methods. The implementation of the
naive Bayes classifier is from MALLET.
</bodyText>
<subsubsectionHeader confidence="0.43042">
Sampled Words
</subsubsectionHeader>
<figureCaption confidence="0.6823356">
Figure 2: Learning curves for naive Bayes, logistic
regression trained with GE, HMM trained with EM,
and CRF trained with GE as the number of sampled
words in each training example changes from 10 to
1000.
</figureCaption>
<bodyText confidence="0.9999265">
We also implemented a self-trained CRF, initially
trained on the output of this naive Bayes classifier,
and trained on its own output in subsequent itera-
tions. This method was not able to consistently out-
perform the naive Bayes classifier after any number
of iterations.
</bodyText>
<sectionHeader confidence="0.988174" genericHeader="evaluation">
6 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.9999755">
We evaluated each method using simple token-level
accuracy, i.e. whether the correct label was assigned
to a word in the document. Word boundaries were
defined by punctuation or whitespace, and no tokens
containing a digit were included. Figure 2 displays
the accuracy for each method as the number of sam-
pled words from each language example is varied
from 10 to 1000.
In all the cases we tested, CRF trained with GE
is clearly the most accurate option among the meth-
ods examined, though the EM-trained HMM seemed
to be approaching a similar accuracy with large
amounts of training data. With a slight edge in ef-
ficiency also in its favor, we think the GE+CRF ap-
proach, rather than EM+HMM, is the best approach
for this problem because of its consistent perfor-
mance across a wide range of training data sizes.
In its favor, the EM+HMM approach has a slightly
</bodyText>
<figure confidence="0.999491833333333">
0 200 400 600 800 1,000
0.95
0.85
0.75
0.9
0.8
0.7
GE-trained logistic regression
EM-trained HMM
GE-trained CRF
naive Bayes
Accuracy
</figure>
<page confidence="0.993197">
1116
</page>
<bodyText confidence="0.999768428571429">
lower variance in its performance across different
files, though not at a statistically significant level.
Contrary to most of the results in (Mann and Mc-
Callum, 2010), a logistic regression classifier trained
with GE did not outperform a standard supervised
naive Bayes classifier. We suspect that this is due
to the different nature of this problem as compared
to most other sequence labeling problems, with the
classifier bootstrapping over a single document only.
In the problems studied by Mann and McCallum, the
GE-trained classifier was able to train over the entire
training set, which was on average about 50,000 in-
stances, far more than the number of words in the
average document in this set (2,500).
</bodyText>
<sectionHeader confidence="0.998454" genericHeader="evaluation">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999855545454546">
In order to analyze the types of mistakes that the
models made we performed an error analysis on ten
randomly selected files, looking at each mislabeled
word and classifying the error according to its type.
The results of this analysis are in Table 5. The three
classes of errors are (1) named entity errors, when
a named entity is given a label that does not match
the label it was given in the original annotation, (2)
shared word errors, when a word that could belong
to either language is classified incorrectly, and (3)
other, a case that covers all other types of errors.
</bodyText>
<table confidence="0.9966894">
Method NE SW Other
GE+CRF 41% 10% 49%
EM+HMM 50% 14% 35%
GE+MaxEnt 37% 12% 51%
Naive Bayes 42% 17% 40%
</table>
<tableCaption confidence="0.962086">
Table 5: Types of errors and their proportions among
</tableCaption>
<bodyText confidence="0.985463730769231">
the different methods. NE stands for Named Entity,
SW stands for Shared Word, and Other covers all
other types of errors.
Our annotation rules for named entities specified
that named entities should be given a label match-
ing their context, but this was rather arbitrary, and
not explicitly followed by any of the methods, which
treat a named entity as if it was any other token. This
was the one of most frequent types of error made by
each of the methods and in our conclusion in sec-
tion 8, we discuss ways to improve it.
In a regression analysis to determine which fac-
tors had the greatest correlations with the GE-
trained CRF performance, the estimated proportion
of named entities in the document had by far the
greatest correlation with CRF accuracy of anything
we measured. Following that in decreasing order of
correlation strength were the cosine similarity be-
tween English and the document’s second language,
the number of words in the monolingual example
text (even though we sampled from it), and the aver-
age length of gold-standard monolingual sequences
in the document.
The learning curve for GE-trained CRF in Fig-
ure 2 is somewhat atypical as far as most machine
learning methods are concerned: performance is
typically non-decreasing as more training data is
made available.
We believe that the model is becoming over-
constrained as more words are used to create the
constraints. The GE method does not have a way
to specify that some of the soft constraints (for the
labels observed most frequently in the sample text)
should be more important than other constraints
(those observed less frequently). When we mea-
sure the KL-divergence between the label distribu-
tions predicted by the constraints and the true la-
bel distribution, we find that this divergence seems
to reach its minimum value between 600 and 800
words, which is where the GE+CRF also seems to
reach its maximum performance.
The step with a naive Bayes classifier estimating
the marginal label distribution ended up being quite
important overall. Without it, the accuracy dropped
by more than a full percentage point absolute. But
the problem of inaccurate constraint estimation is
one that needs further consideration. Some possible
ways to address it may be to prune the constraints
according to their frequency or perhaps according to
a metric like entropy, or to vary the GE-criteria coef-
ficient in the objective function in order to penalize
the model less for varying from the expected model.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99872875">
This paper addresses three of the ongoing issues
specifically mentioned by Hughes et al. (2006) in
their survey of textual language identification. Our
approach is able to support minority languages; in
</bodyText>
<page confidence="0.98426">
1117
</page>
<bodyText confidence="0.999974088888889">
fact, almost all of the languages we tested on would
be considered minority languages. We also address
the issue of sparse or impoverished training data.
Because we use weakly-supervised methods, we are
able to successfully learn to recognize a language
with as few as 10 words of training data5. The last
and most obvious point we address is that of multi-
lingual documents, which is the focus of the paper.
We present a weakly-supervised system for iden-
tifying the languages of individual words in mixed-
language documents. We found that across a broad
range of training data sizes, a CRF model trained
with GE criteria is an accurate sequence classifier
and is preferable to other methods for several rea-
sons.
One major issue to be improved upon in future
work is how named entities are handled. A straight-
forward way to approach this may be to create an-
other label for named entities, which (for the pur-
poses of evaluation) would be considered not to be-
long to any of the languages in the document. We
could simply choose not to evaluate a system on the
named entity tokens in a document. Alternatively,
the problem of language-independent named entity
recognition has received some attention in the past
(Tjong Kim Sang and De Meulder, 2003), and it may
be beneficial to incorporate such a system in a robust
word-level language identification system.
Going forward, an issue that needs to be ad-
dressed with this method is its dependence on know-
ing the set of possible languages a priori. Because
we don’t see an easy way to adapt this method to ac-
curately label words in documents from a possible
set of thousands of languages when the document
itself may only contain two or three languages, we
would propose the following future work.
We propose a two-step approach to general word-
level language identification. The first step would be
to examine a multilingual document, and with high
accuracy, list the languages that are present in the
document. The second step would be identical to the
approach described in this paper (but with the two-
language restriction lifted), and would be responsi-
ble for labeling the languages of individual words,
using the set of languages provided by the first step.
</bodyText>
<footnote confidence="0.9724945">
5With only 10 words of each language as training data, the
CRF approach correctly labels 88% of words
</footnote>
<sectionHeader confidence="0.928937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992155056603774">
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of the Fourth International Conference
on Langauge Resources and Evaluation (LREC 2004),
volume 4, pages 1313–1316, Lisbon, Portugal.
Kenneth R. Beesley. 1988. Language identifier: A com-
puter program for automatic natural-language identifi-
cation of on-line text. In Proceedings of the 29th An-
nual Conference of the American Translators Associa-
tion, volume 47, page 54.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
pages 43–50. AUAI Press.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Annual Symposium on Document Analysis and
Information (SDAIR 94), pages 161–175, Las Vegas,
Nevada.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280–287, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chyng-Leei Chu, Dau-cheng Lyu, and Ren-yuan Lyu.
2007. Language identification on code-switching
speech. In Proceedings of ROCLING.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 100–110.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1–38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR 2008), pages 595–602. ACM.
Gregory Druck. 2011. Generalized Expectation Criteria
for Lightly Supervised Learning. Ph.D. thesis, Univer-
sity of Massachusetts Amherst.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical report.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001–2049.
</reference>
<page confidence="0.870194">
1118
</page>
<reference confidence="0.999736166666667">
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320–327, New York
City, USA, June. Association for Computational Lin-
guistics.
A.S. House and E.P. Neuburg. 1977. Toward automatic
identification of the language of an utterance. i. pre-
liminary methodological considerations. The Journal
of the Acoustical Society ofAmerica, 62:708.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. International Conference on Lan-
guage Resources and Evaluation, pages 485–488.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of the
9th conference on Computational linguistics-Volume
1, pages 145–150. Academia Praha.
Jan Landsbergen. 1989. The rosetta project. pages 82–
87, Munich, Germany.
Dau-Cheng Lyu and Ren-Yuan Lyu. 2008. Language
identification on code-switching utterances using mul-
tiple cues. In Ninth Annual Conference of the Interna-
tional Speech Communication Association.
Gideon S. Mann and Andrew McCallum. 2008. Gener-
alized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings of
ACL-08: HLT, pages 870–878, Columbus, Ohio, June.
Association for Computational Linguistics.
Gideon S. Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. The Journal of Machine
Learning Research, pages 955–984.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45(1):179–189.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems (NIPS 2004), 17:1185–1192.
Kevin P. Scannell. 2007. The cr´ubad´an project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, volume 4, pages 5–15,
Louvain-la-Neuve, Belgium.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 73–81, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 354–362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973–981, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceed-
ings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, pages 142–147.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data
off the web. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 870–878, Athens, Greece, March. Association
for Computational Linguistics.
</reference>
<page confidence="0.997181">
1119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.326682">
<title confidence="0.993113">Labeling the Languages of Words in Mixed-Language Documents Weakly Supervised Methods</title>
<author confidence="0.997879">Ben</author>
<affiliation confidence="0.993799">Department of University of</affiliation>
<author confidence="0.660112">Ann Arbor</author>
<email confidence="0.999701">benking@umich.edu</email>
<author confidence="0.994613">Steven</author>
<affiliation confidence="0.9935315">Department of University of</affiliation>
<author confidence="0.71482">Ann Arbor</author>
<email confidence="0.999311">abney@umich.edu</email>
<abstract confidence="0.976261166666667">In this paper we consider the problem of labeling the languages of words in mixed-language documents. This problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data. Among the approaches evaluated, a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Bootcat: Bootstrapping corpora and terms from the web.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Langauge Resources and Evaluation (LREC 2004),</booktitle>
<volume>4</volume>
<pages>1313--1316</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="9396" citStr="Baroni and Bernardini, 2004" startWordPosition="1498" endWordPosition="1501">estion we segmented our corpus into sentences by splitting at every period, exclamation point, or question mark (an overly agressive approximation of sentence segmentation). Even if every sentence was given the correct majority label under this sentence segmentation, the maximum possible word-level accuracy that a sentence-level classifier could achieve is 85.8%, and even though this number reflects quite optimistic conditions, it is still much lower than the methods of this paper are able to achieve. 3.1 Evaluation Data To build a corpus of mixed language documents, we used the BootCat tool (Baroni and Bernardini, 2004) seeded with words from a minority language. BootCat is designed to automatically collect webpages on a specific topic by repeatedly searching for keywords from a topic-specific set of seed words. We found that this method works equally well for languages as for topics, when seeded with words from a specific language. Once BootCat returned a collection of documents, we manually identified documents from the set that contained text in both the target language and in English, but did not contain text in any other languages. Since the problem becomes trivial when the languages do not share a char</context>
</contexts>
<marker>Baroni, Bernardini, 2004</marker>
<rawString>Marco Baroni and Silvia Bernardini. 2004. Bootcat: Bootstrapping corpora and terms from the web. In Proceedings of the Fourth International Conference on Langauge Resources and Evaluation (LREC 2004), volume 4, pages 1313–1316, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
</authors>
<title>Language identifier: A computer program for automatic natural-language identification of on-line text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 29th Annual Conference of the American Translators Association,</booktitle>
<volume>47</volume>
<pages>54</pages>
<contexts>
<context position="3843" citStr="Beesley, 1988" startWordPosition="602" endWordPosition="603">to the overall task, we examine the features and methods that work best for independent word language identification in section 4. We begin to ex1110 Proceedings of NAACL-HLT 2013, pages 1110–1119, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics amine the larger problem of labeling the language of words in context in section 5 by describing our methods. In section 6, we describe the evaluation and present the results. We present our error analysis in section 7 and conclude in section 8. 2 Related Work Language identification is one of the older NLP problems (Beesley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and T</context>
</contexts>
<marker>Beesley, 1988</marker>
<rawString>Kenneth R. Beesley. 1988. Language identifier: A computer program for automatic natural-language identification of on-line text. In Proceedings of the 29th Annual Conference of the American Translators Association, volume 47, page 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Alternating projections for learning with expectation constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>43--50</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="5498" citStr="Bellare et al., 2009" startWordPosition="859" endWordPosition="862"> of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The </context>
</contexts>
<marker>Bellare, Druck, McCallum, 2009</marker>
<rawString>Kedar Bellare, Gregory Druck, and Andrew McCallum. 2009. Alternating projections for learning with expectation constraints. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 43–50. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of the Third Annual Symposium on Document Analysis and Information (SDAIR 94),</booktitle>
<pages>161--175</pages>
<location>Las Vegas, Nevada.</location>
<contexts>
<context position="4456" citStr="Cavnar and Trenkle, 1994" startWordPosition="694" endWordPosition="697">esley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally very high with large documents, usually in excess of 99% accuracy, but Xia et al. (2009) mention that current methods still can perform quite poorly when the class of potential languages is very large or the texts to be classified are very short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in re</context>
<context position="17332" citStr="Cavnar and Trenkle, 1994" startWordPosition="2787" endWordPosition="2790">ith replacement from the sample text in the appropriate languages. Table 4 shows the accuracies that the classifier achieved when using different sets of features averaged over 10 independent runs. Features Accuracy Unigrams 0.8056 Bigrams 0.8783 Trigrams 0.8491 4-grams 0.7846 5-grams 0.6977 {1,2,3,4,5}-grams 0.8817 {1,2,3,4,5}-grams, word 0.8819 Table 4: Logistic regression accuracy when trained using varying features. The use of all available features seems to be the best option, and we use the full set of features in all proceeding experiments. This result also concurs with the findigs of (Cavnar and Trenkle, 1994), who Figure 1: Learning curves for logistic regression, naive Bayes, decision tree, and Winnow2 on the independent word classification problem as the number of sampled words in each training example changes from 10 to 1000. found 1-5-grams to be most effective for document language classification. 4.2 Classifiers Using all available features, we compare four MALLET (McCallum, 2002) classifiers: logistic regression, naive Bayes, decision tree, and Winnow2. Figure 1 shows the learning curves for each classifier as the number of sampled words comprising each training example is varied from 10 to</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Proceedings of the Third Annual Symposium on Document Analysis and Information (SDAIR 94), pages 161–175, Las Vegas, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraintdriven learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>280--287</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5476" citStr="Chang et al., 2007" startWordPosition="855" endWordPosition="858">006) in their survey of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solori</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraintdriven learning. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280–287, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chyng-Leei Chu</author>
</authors>
<title>Dau-cheng Lyu, and Ren-yuan Lyu.</title>
<date>2007</date>
<booktitle>In Proceedings of ROCLING.</booktitle>
<marker>Chu, 2007</marker>
<rawString>Chyng-Leei Chu, Dau-cheng Lyu, and Ren-yuan Lyu. 2007. Language identification on code-switching speech. In Proceedings of ROCLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="5345" citStr="Collins and Singer, 1999" startWordPosition="836" endWordPosition="839">ts to be classified are very short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-s</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>1--38</pages>
<contexts>
<context position="21852" citStr="Dempster et al., 1977" startWordPosition="3530" endWordPosition="3533">lihood expected output label distribution. To avoid having p(l|f) = 0, which can cause the KL-divergence to be undefined, we perform additive smoothing with δ = 0.5 on the counts before multiplying with the biasing term. We use the implementation of CRF with GE criteria from MALLET (McCallum, 2002), which uses a gradient descent algorithm to optimize the objective function. (Mann and McCallum, 2008; Druck, 2011) 5.2 Hidden Markov Model trained with Expectation Maximization A second method we used was a hidden Markov model (HMM) trained iteratively using the Expectation Maximization algorithm (Dempster et al., 1977). Here an HMM is preferable to a CRF because it is a generative model and therefore uses parameters with simple interpretations. In the case of an HMM, it is easy to estimate emission and transition probabilities using an external method and then set these directly. To initialize the HMM, we use a uniform distribution for transition probabilities, and produce the emission probabilities by using a naive Bayes classifier trained over the two small language samples. � log pO(y(d) |x(d)) − 2k θk O(θ; D, U) = o,2 d − λD(p||pe) 1115 In the expectation step, we simply pass the document through the HM</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR</booktitle>
<pages>595--602</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5518" citStr="Druck et al., 2008" startWordPosition="863" endWordPosition="866">dentification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifyi</context>
<context position="19772" citStr="Druck et al., 2008" startWordPosition="3174" endWordPosition="3177">hidden Markov model (HMM) trained with expectation maximization (EM), and a logistic regression model trained with generalized expectation criteria. We had also considered that a semi-Markov CRF (Sarawagi and Cohen, 2004) could be useful if we could model segment lengths (a non-Markovian feature), but we found that gold-standard segment lengths did not seem to be distributed according to any canonical distribution, and we did not have a reliable way to estimate these segment lengths. 5.1 Conditional Random Field Model trained with Generalized Expectation Generalized expectation (GE) criteria (Druck et al., 2008) are terms added to the objective function of a learning algorithm which specify preferences for the learned model. When the model is a linear chain conditional random field (CRF) model, we can straightforwardly express these criteria in the objective function with a KL-divergence term between the expected values of the current model p� and the preferred model p� (Mann and McCallum, 2008). Practically, to compute these expectations, we produce the smoothed MLE on the output label distribution for every feature observed in the training data. For example, the trigram “ter” may occur 27 times in </context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR 2008), pages 595–602. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
</authors>
<title>Generalized Expectation Criteria for Lightly Supervised Learning.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context position="21645" citStr="Druck, 2011" startWordPosition="3501" endWordPosition="3502">ith the two labels. For every label l E G = {eng, non} and every feature f E Feng UFnon, we calculate count(f, F�) + δ p(l) p(l|f) = x puniform(l), count (f , Ui Fi) + δ|G | the biased maximum likelihood expected output label distribution. To avoid having p(l|f) = 0, which can cause the KL-divergence to be undefined, we perform additive smoothing with δ = 0.5 on the counts before multiplying with the biasing term. We use the implementation of CRF with GE criteria from MALLET (McCallum, 2002), which uses a gradient descent algorithm to optimize the objective function. (Mann and McCallum, 2008; Druck, 2011) 5.2 Hidden Markov Model trained with Expectation Maximization A second method we used was a hidden Markov model (HMM) trained iteratively using the Expectation Maximization algorithm (Dempster et al., 1977). Here an HMM is preferable to a CRF because it is a generative model and therefore uses parameters with simple interpretations. In the case of an HMM, it is easy to estimate emission and transition probabilities using an external method and then set these directly. To initialize the HMM, we use a uniform distribution for transition probabilities, and produce the emission probabilities by u</context>
</contexts>
<marker>Druck, 2011</marker>
<rawString>Gregory Druck. 2011. Generalized Expectation Criteria for Lightly Supervised Learning. Ph.D. thesis, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<tech>Technical report.</tech>
<contexts>
<context position="4259" citStr="Dunning, 1994" startWordPosition="665" endWordPosition="666"> the evaluation and present the results. We present our error analysis in section 7 and conclude in section 8. 2 Related Work Language identification is one of the older NLP problems (Beesley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally very high with large documents, usually in excess of 99% accuracy, but Xia et al. (2009) mention that current methods still can perform quite poorly when the class of potential languages is very large or the texts to be classified are very short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (20</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="5372" citStr="Haghighi and Klein, 2006" startWordPosition="840" endWordPosition="843">ry short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literat</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 320–327, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S House</author>
<author>E P Neuburg</author>
</authors>
<title>Toward automatic identification of the language of an utterance. i. preliminary methodological considerations.</title>
<date>1977</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<pages>62--708</pages>
<contexts>
<context position="3911" citStr="House and Neuburg, 1977" startWordPosition="610" endWordPosition="613">hat work best for independent word language identification in section 4. We begin to ex1110 Proceedings of NAACL-HLT 2013, pages 1110–1119, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics amine the larger problem of labeling the language of words in context in section 5 by describing our methods. In section 6, we describe the evaluation and present the results. We present our error analysis in section 7 and conclude in section 8. 2 Related Work Language identification is one of the older NLP problems (Beesley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally v</context>
</contexts>
<marker>House, Neuburg, 1977</marker>
<rawString>A.S. House and E.P. Neuburg. 1977. Toward automatic identification of the language of an utterance. i. preliminary methodological considerations. The Journal of the Acoustical Society ofAmerica, 62:708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Jeremy Nicholson</author>
<author>Andrew MacKinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proc. International Conference on Language Resources and Evaluation,</booktitle>
<pages>485--488</pages>
<contexts>
<context position="806" citStr="Hughes et al., 2006" startWordPosition="116" endWordPosition="119">y Department of Linguistics University of Michigan Ann Arbor, MI abney@umich.edu Abstract In this paper we consider the problem of labeling the languages of words in mixed-language documents. This problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data. Among the approaches evaluated, a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied. 1 Introduction Language identification is a well-studied problem (Hughes et al., 2006), but it is typically only studied in its canonical text-classification formulation, identifying a document’s language given sample texts from a few different languages. But there are several other interesting and useful formulations of the problem that have received relatively little attention. Here, we focus on the problem of labeling the languages of individual words within a multilingual document. To our knowledge, this is the first paper to specifically address this problem. Our own motivation for studying this problem stems from issues encountered while attempting to build language resou</context>
<context position="3995" citStr="Hughes et al., 2006" startWordPosition="625" endWordPosition="628">110 Proceedings of NAACL-HLT 2013, pages 1110–1119, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics amine the larger problem of labeling the language of words in context in section 5 by describing our methods. In section 6, we describe the evaluation and present the results. We present our error analysis in section 7 and conclude in section 8. 2 Related Work Language identification is one of the older NLP problems (Beesley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally very high with large documents, usually in excess of 99% accuracy, but Xia et al. (20</context>
<context position="29998" citStr="Hughes et al. (2006)" startWordPosition="4880" endWordPosition="4883"> label distribution ended up being quite important overall. Without it, the accuracy dropped by more than a full percentage point absolute. But the problem of inaccurate constraint estimation is one that needs further consideration. Some possible ways to address it may be to prune the constraints according to their frequency or perhaps according to a metric like entropy, or to vary the GE-criteria coefficient in the objective function in order to penalize the model less for varying from the expected model. 8 Conclusion This paper addresses three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual language identification. Our approach is able to support minority languages; in 1117 fact, almost all of the languages we tested on would be considered minority languages. We also address the issue of sparse or impoverished training data. Because we use weakly-supervised methods, we are able to successfully learn to recognize a language with as few as 10 words of training data5. The last and most obvious point we address is that of multilingual documents, which is the focus of the paper. We present a weakly-supervised system for identifying the languages of individu</context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, MacKinlay, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006. Reconsidering language identification for written language resources. In Proc. International Conference on Language Resources and Evaluation, pages 485–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Processing of sentences with intra-sentential code-switching.</title>
<date>1982</date>
<booktitle>In Proceedings of the 9th conference on Computational linguistics-Volume 1,</booktitle>
<pages>145--150</pages>
<location>Academia Praha.</location>
<contexts>
<context position="6068" citStr="Joshi, 1982" startWordPosition="944" endWordPosition="945">(Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has been wellstudied linguistically, it is only one possible reason to explain why a document contains multiple languages, and is actually one of the less common causes observed in our corpus. For that reason, we approach this problem more generally, assuming no specific generative process behind multilingual text. 3 Task Definition The task we describe in thi</context>
</contexts>
<marker>Joshi, 1982</marker>
<rawString>Aravind K. Joshi. 1982. Processing of sentences with intra-sentential code-switching. In Proceedings of the 9th conference on Computational linguistics-Volume 1, pages 145–150. Academia Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Landsbergen</author>
</authors>
<title>The rosetta project.</title>
<date>1989</date>
<pages>82--87</pages>
<location>Munich, Germany.</location>
<contexts>
<context position="14788" citStr="Landsbergen, 1989" startWordPosition="2389" endWordPosition="2391">Igbo 2079 Yoruba 2454 Kiribati 1891 Zulu 1075 Kurdish 1674 Table 3: Number of total words of training data for each language. up a small shared set of a few hundred words from each of eight documents, in order to measure the inter-annotator agreement. The average actual agreement was 0.988, with 0.5 agreement expected by chance for a kappa of 0.975. 3.4 Training Data Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights2, non-English Wikipedias3, the Jehovah’s Witnesses website4, and the Rosetta project (Landsbergen, 1989). Only 30 of these languages ended up being used in experiments. Table 3 shows the sizes of the monolingual samples of the languages used in this paper. 2The Universal Declaration of Human Rights is a document created by the United Nations and translated into many languages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/ 3As of February 2011, there were 113 Wikipedias in different languages. Current versions of Wikipedia can be accessed from http://meta.wikimedia.org/wiki/List of Wikipedias 4As of February 2011, there were 310 versions of the site avail</context>
</contexts>
<marker>Landsbergen, 1989</marker>
<rawString>Jan Landsbergen. 1989. The rosetta project. pages 82– 87, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dau-Cheng Lyu</author>
<author>Ren-Yuan Lyu</author>
</authors>
<title>Language identification on code-switching utterances using multiple cues.</title>
<date>2008</date>
<booktitle>In Ninth Annual Conference of the International Speech Communication Association.</booktitle>
<contexts>
<context position="6263" citStr="Lyu and Lyu, 2008" startWordPosition="977" endWordPosition="980">gh such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has been wellstudied linguistically, it is only one possible reason to explain why a document contains multiple languages, and is actually one of the less common causes observed in our corpus. For that reason, we approach this problem more generally, assuming no specific generative process behind multilingual text. 3 Task Definition The task we describe in this paper is a sequence labeling problem, labeling a word in running text according to the language to which it belongs. In the interest of being able to produce reliable human annotations, we limi</context>
</contexts>
<marker>Lyu, Lyu, 2008</marker>
<rawString>Dau-Cheng Lyu and Ren-Yuan Lyu. 2008. Language identification on code-switching utterances using multiple cues. In Ninth Annual Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5771" citStr="Mann and McCallum, 2008" startWordPosition="896" endWordPosition="899">ng self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has been wellstudied linguistically, it is only one possible reas</context>
<context position="20163" citStr="Mann and McCallum, 2008" startWordPosition="3238" endWordPosition="3241">g to any canonical distribution, and we did not have a reliable way to estimate these segment lengths. 5.1 Conditional Random Field Model trained with Generalized Expectation Generalized expectation (GE) criteria (Druck et al., 2008) are terms added to the objective function of a learning algorithm which specify preferences for the learned model. When the model is a linear chain conditional random field (CRF) model, we can straightforwardly express these criteria in the objective function with a KL-divergence term between the expected values of the current model p� and the preferred model p� (Mann and McCallum, 2008). Practically, to compute these expectations, we produce the smoothed MLE on the output label distribution for every feature observed in the training data. For example, the trigram “ter” may occur 27 times in the English sample text and 34 times in the other sample text, leading to an MLE of p(eng|ter) ,: 0.44. Because we do not expect the true marginal label distribution to be uniform (i.e. the document may not have equal numbers of words in each language), we first estimate the expected marginal label distribution by classifying each word in the document independently using naive Bayes and t</context>
<context position="21631" citStr="Mann and McCallum, 2008" startWordPosition="3497" endWordPosition="3500"> training data features with the two labels. For every label l E G = {eng, non} and every feature f E Feng UFnon, we calculate count(f, F�) + δ p(l) p(l|f) = x puniform(l), count (f , Ui Fi) + δ|G | the biased maximum likelihood expected output label distribution. To avoid having p(l|f) = 0, which can cause the KL-divergence to be undefined, we perform additive smoothing with δ = 0.5 on the counts before multiplying with the biasing term. We use the implementation of CRF with GE criteria from MALLET (McCallum, 2002), which uses a gradient descent algorithm to optimize the objective function. (Mann and McCallum, 2008; Druck, 2011) 5.2 Hidden Markov Model trained with Expectation Maximization A second method we used was a hidden Markov model (HMM) trained iteratively using the Expectation Maximization algorithm (Dempster et al., 1977). Here an HMM is preferable to a CRF because it is a generative model and therefore uses parameters with simple interpretations. In the case of an HMM, it is easy to estimate emission and transition probabilities using an external method and then set these directly. To initialize the HMM, we use a uniform distribution for transition probabilities, and produce the emission prob</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL-08: HLT, pages 870–878, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>955--984</pages>
<contexts>
<context position="26117" citStr="Mann and McCallum, 2010" startWordPosition="4224" endWordPosition="4228">r accuracy with large amounts of training data. With a slight edge in efficiency also in its favor, we think the GE+CRF approach, rather than EM+HMM, is the best approach for this problem because of its consistent performance across a wide range of training data sizes. In its favor, the EM+HMM approach has a slightly 0 200 400 600 800 1,000 0.95 0.85 0.75 0.9 0.8 0.7 GE-trained logistic regression EM-trained HMM GE-trained CRF naive Bayes Accuracy 1116 lower variance in its performance across different files, though not at a statistically significant level. Contrary to most of the results in (Mann and McCallum, 2010), a logistic regression classifier trained with GE did not outperform a standard supervised naive Bayes classifier. We suspect that this is due to the different nature of this problem as compared to most other sequence labeling problems, with the classifier bootstrapping over a single document only. In the problems studied by Mann and McCallum, the GE-trained classifier was able to train over the entire training set, which was on average about 50,000 instances, far more than the number of words in the average document in this set (2,500). 7 Error Analysis In order to analyze the types of mista</context>
</contexts>
<marker>Mann, McCallum, 2010</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research, pages 955–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="17717" citStr="McCallum, 2002" startWordPosition="2849" endWordPosition="2850">ined using varying features. The use of all available features seems to be the best option, and we use the full set of features in all proceeding experiments. This result also concurs with the findigs of (Cavnar and Trenkle, 1994), who Figure 1: Learning curves for logistic regression, naive Bayes, decision tree, and Winnow2 on the independent word classification problem as the number of sampled words in each training example changes from 10 to 1000. found 1-5-grams to be most effective for document language classification. 4.2 Classifiers Using all available features, we compare four MALLET (McCallum, 2002) classifiers: logistic regression, naive Bayes, decision tree, and Winnow2. Figure 1 shows the learning curves for each classifier as the number of sampled words comprising each training example is varied from 10 to 1000. Since a naive Bayes classifier gave the best performance in most experiments, we use naive Bayes as a representative word classifier for the rest of the paper. 5 Methods Moving onto the main task of this paper, labeling sequences of words in documents according to their languages, we use this section to describe our methods. Since training data for this task is limited and is</context>
<context position="21529" citStr="McCallum, 2002" startWordPosition="3483" endWordPosition="3484">abel distributions over each feature. Let Feng and Fnon respectively be the collections of all training data features with the two labels. For every label l E G = {eng, non} and every feature f E Feng UFnon, we calculate count(f, F�) + δ p(l) p(l|f) = x puniform(l), count (f , Ui Fi) + δ|G | the biased maximum likelihood expected output label distribution. To avoid having p(l|f) = 0, which can cause the KL-divergence to be undefined, we perform additive smoothing with δ = 0.5 on the counts before multiplying with the biasing term. We use the implementation of CRF with GE criteria from MALLET (McCallum, 2002), which uses a gradient descent algorithm to optimize the objective function. (Mann and McCallum, 2008; Druck, 2011) 5.2 Hidden Markov Model trained with Expectation Maximization A second method we used was a hidden Markov model (HMM) trained iteratively using the Expectation Maximization algorithm (Dempster et al., 1977). Here an HMM is preferable to a CRF because it is a generative model and therefore uses parameters with simple interpretations. In the case of an HMM, it is easy to estimate emission and transition probabilities using an external method and then set these directly. To initial</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Applying monte carlo techniques to language identification.</title>
<date>2002</date>
<journal>Language and Computers,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="4296" citStr="Poutsma, 2002" startWordPosition="670" endWordPosition="671">ts. We present our error analysis in section 7 and conclude in section 8. 2 Related Work Language identification is one of the older NLP problems (Beesley, 1988), especially in regards to spoken language (House and Neuburg, 1977), and has received a fair share of attention through the years (Hughes et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally very high with large documents, usually in excess of 99% accuracy, but Xia et al. (2009) mention that current methods still can perform quite poorly when the class of potential languages is very large or the texts to be classified are very short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual langua</context>
</contexts>
<marker>Poutsma, 2002</marker>
<rawString>Arjen Poutsma. 2002. Applying monte carlo techniques to language identification. Language and Computers, 45(1):179–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS 2004),</booktitle>
<pages>17--1185</pages>
<contexts>
<context position="19374" citStr="Sarawagi and Cohen, 2004" startWordPosition="3113" endWordPosition="3116">eature for the presence of each possible nonword character (punctuation or digit) between the previous and the current words • a feature for the presence of each possible nonword character between the current and next words In addition to independent word classification, which was covered in section 4, we also implement a conditional random field model trained with generalized expectation criteria, a hidden Markov model (HMM) trained with expectation maximization (EM), and a logistic regression model trained with generalized expectation criteria. We had also considered that a semi-Markov CRF (Sarawagi and Cohen, 2004) could be useful if we could model segment lengths (a non-Markovian feature), but we found that gold-standard segment lengths did not seem to be distributed according to any canonical distribution, and we did not have a reliable way to estimate these segment lengths. 5.1 Conditional Random Field Model trained with Generalized Expectation Generalized expectation (GE) criteria (Druck et al., 2008) are terms added to the objective function of a learning algorithm which specify preferences for the learned model. When the model is a linear chain conditional random field (CRF) model, we can straight</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. Advances in Neural Information Processing Systems (NIPS 2004), 17:1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Scannell</author>
</authors>
<title>The cr´ubad´an project: Corpus building for under-resourced languages. In Building and Exploring Web Corpora:</title>
<date>2007</date>
<booktitle>Proceedings of the 3rd Web as Corpus Workshop,</booktitle>
<volume>4</volume>
<pages>5--15</pages>
<location>Louvain-la-Neuve, Belgium.</location>
<contexts>
<context position="1516" citStr="Scannell, 2007" startWordPosition="224" endWordPosition="225">g a document’s language given sample texts from a few different languages. But there are several other interesting and useful formulations of the problem that have received relatively little attention. Here, we focus on the problem of labeling the languages of individual words within a multilingual document. To our knowledge, this is the first paper to specifically address this problem. Our own motivation for studying this problem stems from issues encountered while attempting to build language resources for minority languages. In trying to extend parts of Kevin Scannell’s Cr´ubad´an project (Scannell, 2007), which automatically builds minority language corpora from the Web, we found that the majority of webpages that contain text in a minority language also contain text in other languages. Since Scannell’s method builds these corpora by bootstrapping from the pages that were retrieved, the corpus-building process can go disastrously wrong without accounting for this problem. And any resources, such as a lexicon, created from the corpus will also be incorrect. In this paper, we explore techniques for performing language identification at the word level in mixed language documents. Our results sho</context>
<context position="14565" citStr="Scannell (2007)" startWordPosition="2360" endWordPosition="2361">hippewa 92 Ojibwa 167 Cornish 2096 Oromo 1443 Croatian 1505 Pular 1285 Czech 1503 Serbian 1515 English 16469 Slovak 1504 Faroese 1585 Somali 1871 Fulfulde 1097 Sotho 2154 Hausa 2677 Tswana 2191 Hungarian 1541 Uzbek 1533 Igbo 2079 Yoruba 2454 Kiribati 1891 Zulu 1075 Kurdish 1674 Table 3: Number of total words of training data for each language. up a small shared set of a few hundred words from each of eight documents, in order to measure the inter-annotator agreement. The average actual agreement was 0.988, with 0.5 agreement expected by chance for a kappa of 0.975. 3.4 Training Data Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights2, non-English Wikipedias3, the Jehovah’s Witnesses website4, and the Rosetta project (Landsbergen, 1989). Only 30 of these languages ended up being used in experiments. Table 3 shows the sizes of the monolingual samples of the languages used in this paper. 2The Universal Declaration of Human Rights is a document created by the United Nations and translated into many languages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/ 3As of Februa</context>
</contexts>
<marker>Scannell, 2007</marker>
<rawString>Kevin P. Scannell. 2007. The cr´ubad´an project: Corpus building for under-resourced languages. In Building and Exploring Web Corpora: Proceedings of the 3rd Web as Corpus Workshop, volume 4, pages 5–15, Louvain-la-Neuve, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Dustin Hillard</author>
<author>Chris Leggetter</author>
</authors>
<title>Minimally-supervised extraction of entities from text advertisements.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>73--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="5817" citStr="Singh et al., 2010" startWordPosition="902" endWordPosition="905"> be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has been wellstudied linguistically, it is only one possible reason to explain why a document contains multiple</context>
</contexts>
<marker>Singh, Hillard, Leggetter, 2010</marker>
<rawString>Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010. Minimally-supervised extraction of entities from text advertisements. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 73–81, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>354--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5711" citStr="Smith and Eisner, 2005" startWordPosition="889" endWordPosition="892">ems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can be categorized into two broad classes: those which bootstrap from a small number of tokens (sometimes called prototypes) (Collins and Singer, 1999; Haghighi and Klein, 2006), and those which impose constraints on the underlying unsupervised learning problem (Chang et al., 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has b</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 354–362, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Learning to predict code-switching points.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>973--981</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="6092" citStr="Solorio and Liu, 2008" startWordPosition="946" endWordPosition="949">, 2007; Bellare et al., 2009; Druck et al., 2008; Ganchev et al., 2010). Constraint-based weakly supervised learning has been applied to some sequence labeling problems, through such methods as contrastive estimation (Smith and Eisner, 2005), generalized expectation criteria (Mann and McCallum, 2008), alternating projections (Singh et al., 2010), and posterior regularization (Ganchev et al., 2010). Perhaps the work that is most similar to this work is the study of code-switching within NLP literature. Most of the work done has been on automatically identifying code-switch points (Joshi, 1982; Solorio and Liu, 2008). The problem of identifying language in the presence of code-switching has seen the most attention in the realm of speech processing (Chu et al., 2007; Lyu and Lyu, 2008), among many others. Though code-switching has been wellstudied linguistically, it is only one possible reason to explain why a document contains multiple languages, and is actually one of the less common causes observed in our corpus. For that reason, we approach this problem more generally, assuming no specific generative process behind multilingual text. 3 Task Definition The task we describe in this paper is a sequence la</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008. Learning to predict code-switching points. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973–981, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>142--147</pages>
<editor>Miles Osborne, editors,</editor>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William Lewis</author>
<author>Hoifung Poon</author>
</authors>
<title>Language ID in the context of harvesting language data off the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="4598" citStr="Xia et al. (2009)" startWordPosition="716" endWordPosition="719"> et al., 2006). In its standard formulation, language identification assumes monolingual documents and attempts to classify each document according to its language from some closed set of known languages. Many approaches have been proposed, such as Markov models (Dunning, 1994), Monte Carlo methods (Poutsma, 2002), and more recently support vector machines with string kernels, but nearly all approaches use the n-gram features first suggested by (Cavnar and Trenkle, 1994). Performance of language identification is generally very high with large documents, usually in excess of 99% accuracy, but Xia et al. (2009) mention that current methods still can perform quite poorly when the class of potential languages is very large or the texts to be classified are very short. This paper attempts to address three of the ongoing issues specifically mentioned by Hughes et al. (2006) in their survey of textual language identification: supporting minority languages, sparse or impoverished training data, and multilingual documents. A number of methods have been proposed in recent years to apply to the problems of unsupervised and weakly-supervised learning. Excluding self- and co-training methods, these methods can</context>
</contexts>
<marker>Xia, Lewis, Poon, 2009</marker>
<rawString>Fei Xia, William Lewis, and Hoifung Poon. 2009. Language ID in the context of harvesting language data off the web. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 870–878, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>