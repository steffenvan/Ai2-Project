<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002904">
<title confidence="0.997946">
Random Graph Model Simulations of Semantic Networks for
Associative Concept Dictionaries
</title>
<author confidence="0.994948">
Hiroyuki Akama
</author>
<affiliation confidence="0.999356">
Tokyo Institute of Technology
</affiliation>
<address confidence="0.8620085">
2-12-1 O-okayama Meguro-ku
Tokyo 152-8550, Japan
</address>
<email confidence="0.99771">
akama@dp.hum.titech.ac.jp
</email>
<author confidence="0.997307">
Terry Joyce
</author>
<affiliation confidence="0.999141">
Tama University
</affiliation>
<address confidence="0.9152645">
802 Engyo Fujisawa-shi
Kanagawa-ken 252-0805, Japan
</address>
<email confidence="0.997449">
terry@tama.ac.jp
</email>
<author confidence="0.991355">
Jaeyoung Jung
</author>
<affiliation confidence="0.99832">
Tokyo Institute of Technology
</affiliation>
<address confidence="0.861565">
2-12-1 O-okayama Meguro-ku
Tokyo 152-8550, Japan
</address>
<email confidence="0.997614">
catherina@dp.hum.titech.ac.jp
</email>
<author confidence="0.993117">
Maki Miyake
</author>
<affiliation confidence="0.997146">
Osaka University
</affiliation>
<address confidence="0.832136">
1-8 Machikaneyama-cho Toyonaka-shi
Osaka 560-0043, Japan
</address>
<email confidence="0.998383">
mmiyake@lang.osaka-u.ac.jp
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999812181818182">
Word association data in dictionary form
can be simulated through the combina-
tion of three components: a bipartite
graph with an imbalance in set sizes; a
scale-free graph of the Barab‡si-Albert
model; and a normal distribution con-
necting the two graphs. Such a model
makes it possible to simulate the complex
features in degree distributions and the
interesting graph clustering results that
are typically observed for real data.
</bodyText>
<sectionHeader confidence="0.880981" genericHeader="method">
1 Modeling background
</sectionHeader>
<bodyText confidence="0.999771647058824">
Associative Concept Dictionaries (ACDs) consist
of word pair data based on psychological ex-
periments where the participants are typically
asked to provide the semantically-related re-
sponse word that comes to mind on presentation
of a stimulus word. Two well-known ACDs for
English are the University of South Florida word
association, rhyme and word fragment norms
(Nelson et al., 1998) and the Edinburgh Word
Association Thesaurus of English (EAT; Kiss et
al., 1973). Two ACDs for Japanese are Ishizaki’s
Associative Concept Dictionary (IACD) (Oka-
moto and Ishizaki, 2001) and the Japanese Word
Association Database (JWAD) (Joyce, 2005,
2006, 2007).
While there are a number of practical applica-
tions for ACDs, three are singled out for mention
</bodyText>
<footnote confidence="0.9875345">
© 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<bodyText confidence="0.999931736842105">
here. The first is in the area of artificial intelli-
gence, where ACDs can contribute to the devel-
opment of intelligent information retrieval sys-
tems for societies requiring increasingly sophisti-
cated navigation methods. A second application
is in the field of medicine, where ACDs could be
used in developing systems that seek to prevent
dementia by checking higher brain functions
with a brain dock. Finally, within educational
settings, ACDs can greatly facilitate language
learning through the manifestation of inherent
cultural modes of thinking.
The typical format of an ACD is to list the
stimulus words (cue words) and their response
words together with some statistics relating to the
word pairing. The stimulus words are generally
basic words determined in advance by the ex-
perimenter, while the response words are seman-
tically associated words provided by respondents
on presentation of the stimulus word. The statis-
tics for the word pairing include, for example,
measured or calculated indices of distance or
perhaps some classification of the semantic rela-
tionship between the pair of words.
In order to mathematically analyze the struc-
ture of ACDs, the raw association data is often
transformed into some form of graph or complex
network representation, where the vertices stand
for words and the edges indicate an associative
relationship (Joyce and Miyake, 2007). However,
to our knowledge, there have been no attempts at
mathematically simulating an ACD as a way of
determining in advance the architectural design
of a dictionary. One reason is that it is a major
challenge to compute maximum likelihood esti-
mations (MLEs) or Monte-Carlo simulations for
graph data (Snijder, 2005). Thus, it is extremely
difficult to predict dependences for unknown
</bodyText>
<page confidence="0.991271">
57
</page>
<note confidence="0.604082">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57–60
</note>
<bodyText confidence="0.926474">
Manchester, August 2008
factors such as the lexical distribution across a
predetermined and controllable dictionary
framework starting simply from a list of basic
words. Accordingly, we propose an easier and
more basic approach to constructing an ACD
model by combining random graph models to
simulate graph features in terms of degree distri-
butions and clustering results.
</bodyText>
<sectionHeader confidence="0.912565" genericHeader="method">
2 Degree distributions for ACDs
</sectionHeader>
<subsectionHeader confidence="0.990024">
2.1 Typical local skew
</subsectionHeader>
<bodyText confidence="0.987956866666667">
It is widely known that Barab‡si and Albert
(1999) have suggested that the degree
distributions of scale-free network structures
correspond to a power law, expressed as
P(x = d) = d-r (where d stands for degree and
r is a small number, such as 2 or 3). This type of
distribution is also known as Zipf&apos;s law
describing the typical frequency distribution of
words in a document and plots on a log scale as a
falling diagonal stroke. However, in the degree
distribution of ACDs, there is always a local
skew, as a local peak or bump with a low
hemline. Figure 1 presents two degree
distributions; for the IACD (upper) ( r = 1.8) and
the JWAD (lower) ( r = 2.3).
</bodyText>
<figure confidence="0.871922857142857">
k
1
0.1
0.01
0.001
0.0001
k
</figure>
<figureCaption confidence="0.998097">
Figure 1. Degree distributions for actual data
</figureCaption>
<bodyText confidence="0.9999555">
The plots indicate a combination of heteroge-
neous distributions, consisting of a single degree
distribution represented as a bell form with a
steep slope on the right side. However, what is
most interesting here is that throughout the dis-
tribution range the curves remain regular and
continuous, with an absence of any ruptures or
fractures both before and after the local peaks.
When actual ACD data is examined, one finds
that as response words are not linked together,
almost all the words located in the skewed part
are stimulus words (which we refer to as peak
words in this study), while the items before the
local peak are less frequent response words that
have a strong tendency to conform to a decaying
distribution. It is therefore relatively natural to
divide all word pairs into two types of graph:
either a bipartite graph for new response words
that are not already part of the stimulus list and a
graph that conforms to Zipf&apos;s law for the fre-
quencies of response words that are already pre-
sent in the stimulus list. For the first type, new
response words are represented as nodes only
with incoming links, generating a bipartite graph
with two sets of different sizes. This bipartite
graph would exhibit the decaying distribution
due to low-frequency response words prior to the
local peak. In the second type of graph, response
words are represented as nodes with both incom-
ing and outgoing links. This second type is simi-
lar to a scale-free graph, such as that incorpo-
rated within the Barab‡si-Albert (BA) model.
</bodyText>
<subsectionHeader confidence="0.999665">
2.2 Bipartite Graph and BA Model
</subsectionHeader>
<bodyText confidence="0.999963368421053">
A bipartite graph is a graph consisting of vertices
that are divided into two independent sets, S and
R, such that every edge connects to one S vertex
and one R vertex. The graph can be represented
by an adjacency matrix with diagonal zero sub-
matrices, where the values of the lower right sub-
matrices would all be zero were it not for the
appearances of some stimulus words as response
words. The lower right section is exactly where
the extremely high degrees of hubs are produced,
which far exceed the average numbers of
response words.
Thus, we adopt an approach to generating a
scale-free graph that reflects Zipf&apos;s law for fre-
quency distributions. According to the BA model,
the probability that a node receives an additional
link is proportional to its degree. Here, we im-
plement the principle of preferential attachment
formulated by Bollob‡s (2003):
</bodyText>
<equation confidence="0.812277666666667">
t
P(x∀Nx+1)=mdt(x) /( )
!= dt T (1),
T1
P(k)
P(k)
</equation>
<figure confidence="0.984345071428571">
0
0.00001
1
1 10 100 1000
0.1
0.01
0.001
0.0001
1 10 100 1000
data
k^(-r)
0.00001
data
k^(-r)
</figure>
<page confidence="0.995181">
58
</page>
<bodyText confidence="0.999841707317073">
with the addition of one condition that is specific
to ACDs, which we explain below. The BA
model starts with a small number, mo of vertices,
and at each time step, T , a new vertex with in
edges is added and linked to in different vertices
that already exist in the graph. Nt + 1 represents a
random set of in early vertices, dt (i) the degree
of vertex i in the process at time t . The
probability that a new vertex will be connected to
a vertex i depends on the connectivity of that
vertex, as expressed by Equation (1). However,
we specifically assume that in is a random
natural number that is smaller than mo , because
in actual data the ratio of stimulus words among
all responses words for each stimulus word is
obviously far from constant.
Moreover, the graph for the BA model here
should be regarded as being a directed graph,
because the very reason that hubs emerge within
semantic network representations of ACDs is
that the number of incoming edges is much larger
than the expected number of nodes for each
possible in-degree. In contrast, out-degree is
limited by the number of responses for each
stimulus word i , which is represented as c(i) .
Let c(i) follow a normal distribution with a
mean cm and a small variance value 2
! (which is
not constant but nearly so) to smoothly combine
the distribution of the bipartite graph and the
power distribution. If a directed adjacency matrix
for the network exclusively between stimulus
words is expressed as D(Bij) , then the sum of the
non-zero values for each row in a random
bipartite graph introducing new response words
D(Bij) (The vertices of stimulus
words with the subscript j are linked with the
vertex of the stimulus word i). Thus, new
response words—words that are not stimulus
words—will be randomly allocated within a
bipartite graph according to Equation (2):
</bodyText>
<equation confidence="0.994467">
P i l
(( , ) =1) = r-1 (c(i) -!D(Bij )) (2),
i∀
</equation>
<bodyText confidence="0.999958875">
where r is the approximate number of such
words. Equation (2) will yield the lower left and
the upper right sections of the complete adja-
cency matrix A for the ACD model. The subse-
quent sub-matrix t
P refers to the transposition
of the prior sub-matrix P . The adjacency matrix
in Equation (3) represents a pseudo bipartite
graph structure where the upper left section is a
zero sub-matrix (because there are no intercon-
nections among new response words), but the
lower right section is not. Here, Bij (not D(Bij) ,
but the undirected counterpart to it), which corre-
sponds to the BA model, is taken as a subsection
of the adjacency matrix that must be non-
directed for the whole composition.
</bodyText>
<equation confidence="0.945182333333333">
&amp;O Pt #
A =
P B (3)BY
</equation>
<bodyText confidence="0.9997784">
The key to understanding Equation (3) is to real-
ize that P is conditionally dependent on Bij ,
because we assume a normal distribution for the
number of non-zero values at each row in the
lower section of A.
</bodyText>
<subsectionHeader confidence="0.999601">
2.3 Simulation Results
</subsectionHeader>
<bodyText confidence="0.999361625">
Taking into account the approximate numbers of
possible new response words, in other words, the
balance in sizes between the two sets in the
bipartite graph, we built a composition of partial
random graphs that could represent an adjacency
matrix of the ACD model. Figure 2 presents one
of the results obtained for the following
conditions:
</bodyText>
<equation confidence="0.774195">
t=90,m0 =1 0,m=3,cm =5,!=1,r=3000.
</equation>
<bodyText confidence="0.999901857142857">
As the Figure shows, the local peak and the
accompanying hemline in the degree distribution
are clearly simulated by the complex
combination of random graphs.
The degree distribution for the artificial net-
work is consistent with the features observed for
actual ACD data, where more than 96% of the
stimulus words in each data set are distributed
across the peak section of the degree distribution,
which is why we have referred to them as peak
words. Moreover, it is easy to verify that without
the assumption of a normal distribution for c(i) ,
distinct fractures emerge in the artificial curve
where new response words in the bipartite struc-
</bodyText>
<figure confidence="0.993917666666667">
10 20 30 40
-2
Local peak
-4
-6
-8
</figure>
<figureCaption confidence="0.983425">
Figure 2. Degree distribution of an ACD model
</figureCaption>
<equation confidence="0.604700666666667">
will be !∀
C(i) #
i
</equation>
<page confidence="0.990791">
59
</page>
<bodyText confidence="0.997821">
ture would be distinguished from stimulus words
located at initial points of the local peak.
</bodyText>
<sectionHeader confidence="0.993073" genericHeader="method">
3 Markov Clustering of ACDs
</sectionHeader>
<subsectionHeader confidence="0.923693">
3.1 MCL
</subsectionHeader>
<bodyText confidence="0.999950692307692">
This section introduces the graph clustering
method that is applied to both the real and artifi-
cial ACD data in order to compare them. Markov
Clustering (MCL) proposed by Van Dongen
(2001) is well known as a scalable unsupervised
cluster algorithm for graphs that decomposes a
whole graph into small coherent groups by simu-
lating the probability movements of a random
walker across the graph. It is believed that when
MCL is applied to semantic networks, it yields
clusters of words that share certain similarities in
meaning or appear to be related to common con-
cepts.
</bodyText>
<subsectionHeader confidence="0.994244">
3.2 MCL Results
</subsectionHeader>
<bodyText confidence="0.999978516129032">
The clustering results for the ACD model created
by combining random graphs reveal that each of
the resultant clusters contains only one stimulus
word surrounded by several response words. This
result is somewhat strange because there are
dense connections between stimulus words,
which would lead us to assume that clusters
would have multiple stimulus word. However,
the results of applying MCL clustering to the
graph for the ACD model are in reality highly
influenced by the sub-structure of the bipartite
graph and less dependent on the scale-free
structure.
Nevertheless, the result is quite similar to
results observed with real data. On examining
MCL clustering results for different ACD
semantic networks, we have observed that MCL
clusters tend to consist of one word node with a
relatively high degree and some other words with
relatively low degrees. On closer inspection of
the graph, it is possible to see several supporter
nodes that gather around one leader node,
forming a kind of small conceptual community.
This suggests that the highest degree word for
each cluster becomes a representative for that
particular cluster consisting of some other low
degree words. In short, MCL clustering is
executed based on such high degree words that
tend to have relatively low curvature values
(Dorow, 2005) compared to their high average
degree values.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999047">
In this paper, we have proposed a basic approach
to simulating word association dictionary data
through the application of graph methodologies.
This modeling is expected not only to provide
insights into the structures of real ACD data, but
also to predict, by manipulating the model pa-
rameters, possible forms for future ACDs. Future
research will focus on constructing an exponen-
tial random graph model for ACDs based on
Markov Chain Monte Carlo (MCMC) methods.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981587804878">
Barab‡si, Albert-L‡szl— and Reka Albert. 1999.
Emergence of scaling in random networks, Science.
286:509-512.
Bollob‡s, Béla. 2003. Mathematical Results on Scale-
free Random Graphs, http://www.stat.berk
eley.edu/~aldous/Networks/boll1.pdf
Dorow, Beate et al. 2005. Using Curvature and
Markov Clustering in Graphs for Lexical Acquisi-
tion and Word Sense Discrimination, MEANING-
2005,2nd Workshop organized by the MEANING
Project, February,3rd-4th.
Joyce, Terry and Maki Miyake. 2007. Capturing the
Structures in Association Knowledge: Application
of Network Analyses to Large-Scale Databases of
Japanese Word Associations, Large-Scale Knowl-
edge Resources. Construction and Application,
Springer Verlag:116-131.
Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J.
1973. An associative thesaurus of English and its
computer analysis, In Aitken, A.J., Bailey, R.W.
and Hamilton-Smith, N. (Eds.), The Computer and
Literary Studies, Edinburgh University Press.
Nelson, Douglas L., Cathy L. McEvoy, &amp; Thomas A.
Schreiber. 1998. The University of South Florida
word association, rhyme, and word fragment
norms, Retrieved August 31, 2005, from
http://www.usf.edu/FreeAssociation
Okamato, Jun and Shun Ishizaki. 2001. Associative
Concept Dictionary and its Comparison Electronic
Concept Dictionaries. PACLING2001-4th Confer-
ence of the Pacific Association for Computational
Linguistics:214-220.
Snijders, Tom A.B., Philippa E. Pattison, Garry
L.Robins, Mark S. Handcock, 2005. New Specifi-
cations for Exponential Random Graph Models,
http://stat.gamma.rug.nl/SnijdersPattisonRobinsHa
ndcock2006.pdf
Steyvers, Mark and Josh Tenenbaum. 2005. The
Large-Scale Structure of Semantic Networks, Sta-
tistical Analyses and a Model of Semantic Growth,
Cognitive Science. 29 (1):41-78.
</reference>
<page confidence="0.9981915">
60
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9978785">Random Graph Model Simulations of Semantic Networks Associative Concept Dictionaries</title>
<author confidence="0.884855">Hiroyuki</author>
<affiliation confidence="0.991234">Tokyo Institute of</affiliation>
<address confidence="0.8128695">2-12-1 O-okayama Tokyo 152-8550,</address>
<email confidence="0.833971">akama@dp.hum.titech.ac.jp</email>
<author confidence="0.887004">Terry</author>
<affiliation confidence="0.718851">Tama</affiliation>
<address confidence="0.8668135">802 Engyo Kanagawa-ken 252-0805, Japan</address>
<email confidence="0.89077">terry@tama.ac.jp</email>
<affiliation confidence="0.6334025">Jaeyoung Tokyo Institute of</affiliation>
<address confidence="0.5497295">2-12-1 O-okayama Tokyo 152-8550,</address>
<email confidence="0.486738">catherina@dp.hum.titech.ac.jp</email>
<note confidence="0.73899225">Maki Osaka 1-8 Machikaneyama-cho Osaka 560-0043, Japan</note>
<email confidence="0.782538">mmiyake@lang.osaka-u.ac.jp</email>
<abstract confidence="0.960548205479452">Word association data in dictionary form can be simulated through the combination of three components: a bipartite graph with an imbalance in set sizes; a scale-free graph of the Barab‡si-Albert model; and a normal distribution connecting the two graphs. Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological experiments where the participants are typically asked to provide the semantically-related response word that comes to mind on presentation of a stimulus word. Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al., 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al., 1973). Two ACDs for Japanese are Ishizaki’s Associative Concept Dictionary (IACD) (Okamoto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007). While there are a number of practical applications for ACDs, three are singled out for mention 2008. Licensed under the Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. here. The first is in the area of artificial intelligence, where ACDs can contribute to the development of intelligent information retrieval systems for societies requiring increasingly sophisticated navigation methods. A second application is in the field of medicine, where ACDs could be used in developing systems that seek to prevent dementia by checking higher brain functions with a brain dock. Finally, within educational settings, ACDs can greatly facilitate language learning through the manifestation of inherent cultural modes of thinking. The typical format of an ACD is to list the stimulus words (cue words) and their response words together with some statistics relating to the word pairing. The stimulus words are generally basic words determined in advance by the experimenter, while the response words are semantically associated words provided by respondents on presentation of the stimulus word. The statistics for the word pairing include, for example, measured or calculated indices of distance or perhaps some classification of the semantic relationship between the pair of words. In order to mathematically analyze the structure of ACDs, the raw association data is often transformed into some form of graph or complex network representation, where the vertices stand for words and the edges indicate an associative relationship (Joyce and Miyake, 2007). However, to our knowledge, there have been no attempts at mathematically simulating an ACD as a way of determining in advance the architectural design of a dictionary. One reason is that it is a major challenge to compute maximum likelihood estimations (MLEs) or Monte-Carlo simulations for graph data (Snijder, 2005). Thus, it is extremely difficult to predict dependences for unknown 57</abstract>
<address confidence="0.6361495">2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, 57–60 Manchester, August 2008</address>
<abstract confidence="0.983626264367816">factors such as the lexical distribution across a predetermined and controllable dictionary framework starting simply from a list of basic words. Accordingly, we propose an easier and more basic approach to constructing an ACD model by combining random graph models to simulate graph features in terms of degree distributions and clustering results. 2 Degree distributions for ACDs 2.1 Typical local skew It is widely known that Barab‡si and Albert (1999) have suggested that the degree distributions of scale-free network structures correspond to a power law, expressed as for degree and a small number, such as 2 or 3). This type of distribution is also known as Zipf&apos;s law describing the typical frequency distribution of words in a document and plots on a log scale as a falling diagonal stroke. However, in the degree distribution of ACDs, there is always a local skew, as a local peak or bump with a low hemline. Figure 1 presents two degree for the IACD (upper) ( 1.8) and JWAD (lower) ( 2.3). k 1 0.1 0.01 0.001 0.0001 k Figure 1. Degree distributions for actual data The plots indicate a combination of heterogeneous distributions, consisting of a single degree distribution represented as a bell form with a steep slope on the right side. However, what is most interesting here is that throughout the distribution range the curves remain regular and continuous, with an absence of any ruptures or fractures both before and after the local peaks. When actual ACD data is examined, one finds that as response words are not linked together, almost all the words located in the skewed part are stimulus words (which we refer to as peak words in this study), while the items before the local peak are less frequent response words that have a strong tendency to conform to a decaying distribution. It is therefore relatively natural to divide all word pairs into two types of graph: either a bipartite graph for new response words that are not already part of the stimulus list and a graph that conforms to Zipf&apos;s law for the frequencies of response words that are already present in the stimulus list. For the first type, new response words are represented as nodes only with incoming links, generating a bipartite graph with two sets of different sizes. This bipartite graph would exhibit the decaying distribution due to low-frequency response words prior to the local peak. In the second type of graph, response words are represented as nodes with both incoming and outgoing links. This second type is similar to a scale-free graph, such as that incorporated within the Barab‡si-Albert (BA) model. 2.2 Bipartite Graph and BA Model A bipartite graph is a graph consisting of vertices that are divided into two independent sets, S and R, such that every edge connects to one S vertex and one R vertex. The graph can be represented by an adjacency matrix with diagonal zero submatrices, where the values of the lower right submatrices would all be zero were it not for the appearances of some stimulus words as response words. The lower right section is exactly where the extremely high degrees of hubs are produced, which far exceed the average numbers of response words. Thus, we adopt an approach to generating a scale-free graph that reflects Zipf&apos;s law for frequency distributions. According to the BA model, the probability that a node receives an additional link is proportional to its degree. Here, we imthe principle of attachment formulated by Bollob‡s (2003): t /( )</abstract>
<note confidence="0.510721875">P(k) P(k) 0 0.00001 1 1 10 100 1000 0.1 0.01</note>
<date confidence="0.762567">0.001 0.0001</date>
<phone confidence="0.856522">1 10 100 1000</phone>
<abstract confidence="0.99256545398773">k^(-r) 0.00001 data k^(-r) 58 with the addition of one condition that is specific to ACDs, which we explain below. The BA starts with a small number, vertices, at each time step, a new vertex with is added and linked to vertices already exist in the graph. a set of vertices, degree vertex the process at time The probability that a new vertex will be connected to vertex on the connectivity of that vertex, as expressed by Equation (1). However, specifically assume that a random number that is smaller than because in actual data the ratio of stimulus words among all responses words for each stimulus word is obviously far from constant. Moreover, the graph for the BA model here should be regarded as being a directed graph, because the very reason that hubs emerge within semantic network representations of ACDs is the number of is much larger than the expected number of nodes for each In contrast, limited by the number of responses for each word which is represented as a normal distribution with a a small variance value is not constant but nearly so) to smoothly combine the distribution of the bipartite graph and the distribution. If a matrix for the network exclusively between stimulus is expressed as then the sum of the non-zero values for each row in a random bipartite graph introducing new response words vertices of stimulus with the subscript linked with the of the stimulus word Thus, new response words—words that are not stimulus words—will be randomly allocated within a bipartite graph according to Equation (2): P i l , ) the approximate number of such words. Equation (2) will yield the lower left and the upper right sections of the complete adjamatrix the ACD model. The subsesub-matrix to the transposition the prior sub-matrix The adjacency matrix in Equation (3) represents a pseudo bipartite graph structure where the upper left section is a zero sub-matrix (because there are no interconnections among new response words), but the right section is not. Here, but the undirected counterpart to it), which corresponds to the BA model, is taken as a subsection the adjacency matrix that must be nonthe whole composition. B The key to understanding Equation (3) is to realthat conditionally dependent on because we assume a normal distribution for the number of non-zero values at each row in the section of 2.3 Simulation Results Taking into account the approximate numbers of possible new response words, in other words, the balance in sizes between the two sets in the bipartite graph, we built a composition of partial random graphs that could represent an adjacency matrix of the ACD model. Figure 2 presents one of the results obtained for the following conditions: As the Figure shows, the local peak and the accompanying hemline in the degree distribution are clearly simulated by the complex combination of random graphs. The degree distribution for the artificial network is consistent with the features observed for actual ACD data, where more than 96% of the stimulus words in each data set are distributed across the peak section of the degree distribution, which is why we have referred to them as peak words. Moreover, it is easy to verify that without assumption of a normal distribution for distinct fractures emerge in the artificial curve new response words in the bipartite struc- 10 20 30 40 -2 Local peak -4 -6 -8 Figure 2. Degree distribution of an ACD model be i 59 ture would be distinguished from stimulus words located at initial points of the local peak. 3 Markov Clustering of ACDs 3.1 MCL This section introduces the graph clustering method that is applied to both the real and artificial ACD data in order to compare them. Markov Clustering (MCL) proposed by Van Dongen (2001) is well known as a scalable unsupervised cluster algorithm for graphs that decomposes a whole graph into small coherent groups by simulating the probability movements of a random walker across the graph. It is believed that when MCL is applied to semantic networks, it yields clusters of words that share certain similarities in meaning or appear to be related to common concepts. 3.2 MCL Results The clustering results for the ACD model created by combining random graphs reveal that each of the resultant clusters contains only one stimulus word surrounded by several response words. This result is somewhat strange because there are dense connections between stimulus words, which would lead us to assume that clusters would have multiple stimulus word. However, the results of applying MCL clustering to the graph for the ACD model are in reality highly influenced by the sub-structure of the bipartite graph and less dependent on the scale-free structure. Nevertheless, the result is quite similar to results observed with real data. On examining MCL clustering results for different ACD semantic networks, we have observed that MCL clusters tend to consist of one word node with a relatively high degree and some other words with relatively low degrees. On closer inspection of the graph, it is possible to see several supporter nodes that gather around one leader node, forming a kind of small conceptual community. This suggests that the highest degree word for each cluster becomes a representative for that particular cluster consisting of some other low degree words. In short, MCL clustering is executed based on such high degree words that tend to have relatively low curvature values (Dorow, 2005) compared to their high average degree values. 4 Conclusion In this paper, we have proposed a basic approach to simulating word association dictionary data through the application of graph methodologies. This modeling is expected not only to provide insights into the structures of real ACD data, but also to predict, by manipulating the model parameters, possible forms for future ACDs. Future research will focus on constructing an exponential random graph model for ACDs based on Markov Chain Monte Carlo (MCMC) methods.</abstract>
<note confidence="0.93778375">References Barab‡si, Albert-L‡szl— and Reka Albert. 1999. of scaling in random Science. 286:509-512. Béla. 2003. Results on Scale- Random http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf Beate et al. 2005. Curvature and</note>
<title confidence="0.579765">Markov Clustering in Graphs for Lexical Acquisiand Word Sense MEANING-</title>
<note confidence="0.748827545454545">2005,2nd Workshop organized by the MEANING Project, February,3rd-4th. Terry and Maki Miyake. 2007. the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Word Large-Scale Knowledge Resources. Construction and Application, Springer Verlag:116-131. Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. associative thesaurus of English and its In Aitken, A.J., Bailey, R.W.</note>
<affiliation confidence="0.815208">and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies, Edinburgh University Press.</affiliation>
<address confidence="0.775867">Nelson, Douglas L., Cathy L. McEvoy, &amp; Thomas A. 1998. University of South Florida</address>
<note confidence="0.845336">word association, rhyme, and word fragment Retrieved August 31, 2005, from</note>
<web confidence="0.840561">http://www.usf.edu/FreeAssociation</web>
<note confidence="0.6330136">Jun and Shun Ishizaki. 2001. Concept Dictionary and its Comparison Electronic Dictionaries. Conference of the Pacific Association for Computational Linguistics:214-220.</note>
<title confidence="0.396026">Snijders, Tom A.B., Philippa E. Pattison, Garry</title>
<author confidence="0.953607">Mark S Handcock</author>
<email confidence="0.281679">forExponentialRandomGraph</email>
<web confidence="0.981214">http://stat.gamma.rug.nl/SnijdersPattisonRobinsHa</web>
<note confidence="0.718890714285714">ndcock2006.pdf Steyvers, Mark and Josh Tenenbaum. 2005. The Large-Scale Structure of Semantic Networks, Statistical Analyses and a Model of Semantic Growth, Cognitive Science. 29 (1):41-78. 60 61</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert-L‡szl— Barab‡si</author>
<author>Reka Albert</author>
</authors>
<title>Emergence of scaling in random networks,</title>
<date>1999</date>
<journal>Science.</journal>
<pages>286--509</pages>
<marker>Barab‡si, Albert, 1999</marker>
<rawString>Barab‡si, Albert-L‡szl— and Reka Albert. 1999. Emergence of scaling in random networks, Science. 286:509-512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Béla Bollob‡s</author>
</authors>
<date>2003</date>
<booktitle>Mathematical Results on Scalefree Random Graphs, http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf</booktitle>
<marker>Bollob‡s, 2003</marker>
<rawString>Bollob‡s, Béla. 2003. Mathematical Results on Scalefree Random Graphs, http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
</authors>
<title>Using Curvature and Markov Clustering</title>
<date>2005</date>
<booktitle>in Graphs for Lexical Acquisition and Word Sense Discrimination, MEANING2005,2nd Workshop organized by the MEANING Project, February,3rd-4th.</booktitle>
<marker>Dorow, 2005</marker>
<rawString>Dorow, Beate et al. 2005. Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination, MEANING2005,2nd Workshop organized by the MEANING Project, February,3rd-4th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Joyce</author>
<author>Maki Miyake</author>
</authors>
<title>Capturing the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Japanese Word Associations, Large-Scale Knowledge Resources. Construction and Application,</title>
<date>2007</date>
<pages>116--131</pages>
<publisher>Springer</publisher>
<contexts>
<context position="3303" citStr="Joyce and Miyake, 2007" startWordPosition="475" endWordPosition="478">termined in advance by the experimenter, while the response words are semantically associated words provided by respondents on presentation of the stimulus word. The statistics for the word pairing include, for example, measured or calculated indices of distance or perhaps some classification of the semantic relationship between the pair of words. In order to mathematically analyze the structure of ACDs, the raw association data is often transformed into some form of graph or complex network representation, where the vertices stand for words and the edges indicate an associative relationship (Joyce and Miyake, 2007). However, to our knowledge, there have been no attempts at mathematically simulating an ACD as a way of determining in advance the architectural design of a dictionary. One reason is that it is a major challenge to compute maximum likelihood estimations (MLEs) or Monte-Carlo simulations for graph data (Snijder, 2005). Thus, it is extremely difficult to predict dependences for unknown 57 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57–60 Manchester, August 2008 factors such as the lexical distribution across a predetermined</context>
</contexts>
<marker>Joyce, Miyake, 2007</marker>
<rawString>Joyce, Terry and Maki Miyake. 2007. Capturing the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Japanese Word Associations, Large-Scale Knowledge Resources. Construction and Application, Springer Verlag:116-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Kiss</author>
<author>C Armstrong</author>
<author>R Milroy</author>
<author>J Piper</author>
</authors>
<title>An associative thesaurus of English and its computer analysis,</title>
<date>1973</date>
<journal>In</journal>
<publisher>University Press.</publisher>
<location>Edinburgh</location>
<contexts>
<context position="1471" citStr="Kiss et al., 1973" startWordPosition="199" endWordPosition="202">complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological experiments where the participants are typically asked to provide the semantically-related response word that comes to mind on presentation of a stimulus word. Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al., 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al., 1973). Two ACDs for Japanese are Ishizaki’s Associative Concept Dictionary (IACD) (Okamoto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007). While there are a number of practical applications for ACDs, three are singled out for mention © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. here. The first is in the area of artificial intelligence, where ACDs can contribute to the development of intelligent information retrieval syst</context>
</contexts>
<marker>Kiss, Armstrong, Milroy, Piper, 1973</marker>
<rawString>Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. 1973. An associative thesaurus of English and its computer analysis, In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies, Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida word association, rhyme, and word fragment norms,</title>
<date>1998</date>
<location>Retrieved</location>
<note>from http://www.usf.edu/FreeAssociation</note>
<contexts>
<context position="1390" citStr="Nelson et al., 1998" startWordPosition="186" endWordPosition="189">ribution connecting the two graphs. Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological experiments where the participants are typically asked to provide the semantically-related response word that comes to mind on presentation of a stimulus word. Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al., 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al., 1973). Two ACDs for Japanese are Ishizaki’s Associative Concept Dictionary (IACD) (Okamoto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007). While there are a number of practical applications for ACDs, three are singled out for mention © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. here. The first is in the area of artificial intelligence, where</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Nelson, Douglas L., Cathy L. McEvoy, &amp; Thomas A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms, Retrieved August 31, 2005, from http://www.usf.edu/FreeAssociation</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Okamato</author>
<author>Shun Ishizaki</author>
</authors>
<title>Associative Concept Dictionary and its Comparison Electronic Concept Dictionaries.</title>
<date>2001</date>
<booktitle>PACLING2001-4th Conference of the Pacific Association for Computational Linguistics:214-220.</booktitle>
<marker>Okamato, Ishizaki, 2001</marker>
<rawString>Okamato, Jun and Shun Ishizaki. 2001. Associative Concept Dictionary and its Comparison Electronic Concept Dictionaries. PACLING2001-4th Conference of the Pacific Association for Computational Linguistics:214-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom A B Snijders</author>
<author>Philippa E Pattison</author>
<author>Garry L Robins</author>
<author>Mark S Handcock</author>
</authors>
<title>New Specifications for Exponential Random Graph Models,</title>
<date>2005</date>
<note>http://stat.gamma.rug.nl/SnijdersPattisonRobinsHa ndcock2006.pdf</note>
<marker>Snijders, Pattison, Robins, Handcock, 2005</marker>
<rawString>Snijders, Tom A.B., Philippa E. Pattison, Garry L.Robins, Mark S. Handcock, 2005. New Specifications for Exponential Random Graph Models, http://stat.gamma.rug.nl/SnijdersPattisonRobinsHa ndcock2006.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Josh Tenenbaum</author>
</authors>
<title>The Large-Scale Structure of Semantic Networks, Statistical Analyses and a Model of Semantic Growth,</title>
<date>2005</date>
<journal>Cognitive Science.</journal>
<volume>29</volume>
<pages>1--41</pages>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Steyvers, Mark and Josh Tenenbaum. 2005. The Large-Scale Structure of Semantic Networks, Statistical Analyses and a Model of Semantic Growth, Cognitive Science. 29 (1):41-78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>