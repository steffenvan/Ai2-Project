<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009861">
<title confidence="0.76228">
Corpus-Based Lexical Choice in Natural Language Generation
</title>
<author confidence="0.693046">
Srinivas Bangalore and Owen Rambow
</author>
<affiliation confidence="0.595728">
AT&amp;T Labs — Research
</affiliation>
<address confidence="0.9099705">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.998633">
fsrini,rambowl@research.att.com
</email>
<sectionHeader confidence="0.980093" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982675">
Choosing the best lexeme to realize
a meaning in natural language gen-
eration is a hard task. We inves-
tigate different tree-based stochas-
tic models for lexical choice. Be-
cause of the difficulty of obtaining
a sense-tagged corpus, we generalize
the notion of synonymy. We show
that a tree-based model can achieve
a word-bag based accuracy of 90%,
representing an improvement over
the baseline.
</bodyText>
<sectionHeader confidence="0.93351" genericHeader="method">
1 Introduction: Lexical Choice
</sectionHeader>
<bodyText confidence="0.992553824561404">
Sentence planning in natural language gener-
ation (NLG) can be characterized as the col-
lection of tasks related to the choice of (ab-
stract) linguistic resources in order to achieve
elementary communicative goals. These tasks
include lexical and syntactic choice. Lexical
choice is the choice of meaning-bearing lex-
emes. (Function words express grammatical
meaning such as tense and aspect features, or
are entirely grammatically determined, and
therefore are usually chosen not during sen-
tence planning, but during linguistic realiza-
tion.) Syntactic choice is the choice of how
meaning-bearing lexemes are combined. It is
clear that lexical and syntactic choice are not
independent. For example, if we choose the
verb give, we can choose between a nominal
or prepositional realization of the goal argu-
ment (John gave the organization a car or
John gave a car to the organization), while if
we choose donate, only the prepositional real-
ization is possible.
In many systems, lexical choice is han-
dled in a dictionary mapping from domain
concepts (or, in machine translation (MT),
source language words to target language
words). These dictionaries are typically hand-
crafted.&apos; While such an approach is per-
fectly reasonable for many applications, es-
pecially those in which linguistic variation in
the target texts is restricted, there are appli-
cations (including many MT applications) in
which the range of linguistic variation of the
texts to be generated is far too vast to allow
for hand-coding of dictionaries. Furthermore,
even for domains with a limited range of lin-
guistic variation, the task of hand-coding the
necessary resources can be formidable when a
generation system must be ported to a new
domain, a new genre, or especially a new lan-
guage.
In seminal work,
Langkilde and Knight (1998) have used
a language model as a way of choosing
among different lexical and syntactic options.
In (Bangalore and Rambow, 2000), we
present a model of syntactic choice which,
like Langkilde and Knight (1998) relies on a
linear language model, but unlike their ap-
proach, also uses a tree-based representation
of syntactic structure, a tree model, and an
independently hand-crafted grammar. We
show that the addition of the tree-based
model improves the performance of the
syntactic choice module. The system, called
FERGUS (Flexible Rationalist-Empiricist
Generation Using Syntax), does not, how-
</bodyText>
<footnote confidence="0.805163">
&apos;There have been some advances in automatic
transfer dictionary building for MT, but the automat-
ically extracted dictionaries do not allow for very fine-
grained control in lexical choice.
</footnote>
<figure confidence="0.99810725">
repeat
Indians
many
a of
experience
that
fear
that
γ1
experience
1
α
α
1
Indians
repeat
many
1
a of
γ3
γ
1
γ
1
α
Semi−specified
Dependency Tree
Word
Lattice
Semantic Predicate Representation
String
Syntax
Chooser
Tree
Tree
Language
XTAG
Grammar
Model
Model
Model
fear
α
2
Unraveler
LP Chooser
Lexeme
Chooser
</figure>
<bodyText confidence="0.994007388235295">
ner. If we had a corpus in which each word
were annotated with its sense (i.e., a Word-
Net synset), then we could learn a mapping
from meanings to lexeme. Unfortunately, no
extensive sense-tagged corpus currently ex-
ists, though there are efforts underway to cre-
ate one. Nonetheless, it is clear that sense-
tagging is more complex a task than syntac-
tic tagging, and therefore it will in the fore-
seeable future be a formidable task to sense-
tag a new corpus. Recall that one of the
motivations for stochastic NLG is the abil-
ity to quickly port to new domains or even
languages.
Therefore, we have chosen to represent
meaning in a more sloppy manner, namely by
using the union of all the synsets of a lexeme.
We refer to these sets as supersynsets. For ex-
ample, the supersynset for estimate contains
all the nouns given above in the table:
Supersynset for verb fear: fear,
dread, reverence, revere, venerate
If we don&apos;t assume we know the part-of-
speech of a lexeme, we get the union of all
synsets of the lexeme in all possible parts-of-
speech:
Supersynset for POS-unspecified
fear: fear, dread, reverence, revere,
venerate, fearfulness, fright, con-
cern, care
While a synset represents a meaning, a su-
persynset represents meaning potential: it en-
codes all possible meanings of a lexeme. Un-
like synsets, supersynsets do not form a parti-
tion of anything useful. Each lexeme is asso-
ciated with exactly one supersynset; a super-
synset may be associated with more than one
lexeme, but need not be, even if it contains
more than one lexeme. However, if lexeme /1
is in the supersynset of lexeme /2, then /2 is
of course also a member of the supersynset of
/1. Clearly, it is straightforward (and deter-
ministic) to map from a representation based
on lexemes to one based on supersynsets.
Since, for practical reasons, we must work
with supersynsets and not synsets, we are
framing the lexical choice question not as the
task of choosing lexical items to convey a
meaning specified in the input, but rather
as a task of choosing the most appropriate
synonym for a lexeme specified in the input
(where the lexeme represents its own mean-
ing potential). Thus, the input to the Lexeme
Chooser is formally the same as the input to
the Syntax Chooser, namely the tree shown
in Figure 1; however, these node labels rep-
resent meaning potentials rather than actual
lexemes, and Lexeme Chooser may or may
not change the label when it determines the
lexeme. Note that we can use the approach
presented in this paper in order to perform
the lexical choice task as it is usually framed:
if we are given synsets (i.e., meanings) in the
input representation, we can choose a mem-
ber of that synset and then proceed in the
manner described in this paper. However, we
are not guaranteed to obtain a synonym from
the original synset.
We assume that the structural relation be-
tween the lexemes is not changed during lexi-
cal or syntactic choice. Since the input repre-
sentation may be underspecified with respect
to the type of dependency relation (adjunct
or argument, which type of argument), all
that remains fixed is the the choice of heads
and dependents. But it is well known that
there are synonymous realizations in which
meanings are distributed in different struc-
tural configurations, for example we walked
across the state and we traversed the state
on foot.4 The issue of dealing with such
structural paraphrases is a complex one and
presumably requires corpora which are anno-
tated in more complex ways than the syntacti-
cally annotated corpora available to us today.
</bodyText>
<sectionHeader confidence="0.9536145" genericHeader="method">
4 Lexical Choice: Basic
Architecture
</sectionHeader>
<bodyText confidence="0.99974975">
In the architecture diagram in Figure 2, lex-
ical choice happens before syntactic choice.
However, a priori there are other architec-
tural options as well. In this section, we inves-
</bodyText>
<footnote confidence="0.661568666666667">
4These kinds of examples are known as structural
divergences when found in the context of machine
translation (Dorr, 1994).
</footnote>
<bodyText confidence="0.999879956521739">
tigate three basic options of integrating lexi-
cal with syntactic choice: lexical choice before
syntactic choice; integrated simultaneous lex-
ical and syntactic choice; and lexical choice
after syntactic choice. We observe that these
three possible architectures do not accomplish
exactly the same tasks. If we perform lexical
choice after syntactic choice, then the range of
possible synonyms is reduced: not only must
we choose a member of the supersynset of the
input lexeme, but it must also be compati-
ble with the syntactic choice already made.
For example, if the input lexeme is give, we
can choose donate as a synonym. However, if
we first choose a double-object construction
(give a car to the organization) as the syntac-
tic realization of the lexeme yet to be chosen,
then donate is excluded as a lexical choice.
The task in the syntactic-before-lexical-choice
architecture is thus easier than the task of
the lexical-before-syntactic-choice (or lexical-
with-syntactic-choice) architecture, since the
choice is more constrained.
The first task — that of the lexical-before-
syntactic-choice (or lexical-with-syntactic-
choice) architecture — is thus to take an in-
put tree of the form shown in Figure 1, and
to choose lexemes from the supersynsets asso-
ciated with each lexeme, and then (or simul-
taneously) to choose supertags for the cho-
sen lexemes. The second task — that of the
syntactic-before-lexical-choice architecture —
is to take a tree of the form shown in Fig-
ure 3, and to choose lexemes from the super-
synsets associated with each lexeme which are
compatible with the already made syntactic
choices.
For the first task, we distinguish two types
of architectures, lexical-before-syntactic-
choice and lexical-with-syntactic-choice. In
both architectures, we use top-down algo-
rithms in which decisions are first made for a
mother node before her daughters are dealt
with. For the first lexical-before-syntactic-
choice architecture, we use four different
models.
</bodyText>
<listItem confidence="0.951959047619048">
• LexSyn: We choose the lexeme of the
daughter node based on her mother&apos;s lex-
eme and supertag. More precisely, given
a mother&apos;s lexeme tm and supertag
we find the daughter lexeme /d that max-
imizes P(iditm,
• Lex: We choose the lexeme of the daugh-
ter node based on her mother&apos;s lexeme
only. More precisely, we find the daugh-
ter lexeme /d that maximizes P(1clulm).
• Syn: We choose the lexeme of the daugh-
ter node based on her mother&apos;s supertag
only. More precisely, we find the daugh-
ter lexeme /d that maximizes *id ism).
• DtrLex: We choose the lexeme of the
daughter node based on the simple lex-
ical frequency of the lexemes in the
daughter&apos;s supersynset, without regard
to the mother. More precisely, we find
the daughter lexeme /d that maximizes
P(10.
</listItem>
<bodyText confidence="0.9500851">
Since sparseness of data is a major problem,
we combine these models, backing off from
one to a less specific one as needed. For ex-
ample, LexSyn-DtrLex is a model in which
we take each member of the daughter&apos;s super-
synset and choose the one with the highest
in
ll
(la,
value for P Sm). If no data is available
for any lexemes of the daughter&apos;s supersynset,
then we backoff and choose the lexeme from
the supersynset which is most frequent in the
training corpus. In all models, if the model
does not result in a lexical choice (because of
unseen data), we randomly choose a lexeme
from the supersynset. (Note that DtrLex al-
most always results in a choice.)
For the lexical-with-syntactic-choice archi-
tecture, we use a single model:
</bodyText>
<listItem confidence="0.94851275">
• LexSynJoint: We choose the lexeme
and the supertag of the daughter node
based on her mother&apos;s lexeme and su-
pertag. More precisely, given a mother&apos;s
</listItem>
<bodyText confidence="0.992036463414634">
5Note that this model implies that lexical and syn-
tactic choice are performed interleaved: for a given
node, first the lexeme is chosen, then its supertag.
Then attention shifts to the lexeme&apos;s daughters, and
for each daughter, when the lexeme is chosen, the
mother&apos;s supertag is already available. Interleaved ex-
ecution of lexical and syntactic choice should not be
confused with combined execution of lexical and syn-
tactic choice, discussed below.
lexeme 1,,, and supertag sm, we find the
daughter lexeme /d and the daughter su-
pertag sd that maximize P(1d, S dilm, Sm).
Here, the only backoff we consider is to
DtrLex, with subsequent independent syn-
tactic choice.
For the second task — that of the syntactic-
before-lexical-choice architecture — we define
two types of algorithms. In the tree al-
gorithm, we choose the lexemes in the tree
model after we have chosen the syntax, in a
second pass through the tree. Each lexeme is
chosen based on its own supertag, its mother
node&apos;s lexeme, and its mother&apos;s supertag. If
no data is available, we first back off to consid-
ering its own supertag and its mother node&apos;s
lexeme, then just its own supertag, and finally
to the frequency of the daughter lexemes. In
the linear algorithm, we include all the al-
ternative lexemes in the lattice passed to the
LP Chooser, where the language model im-
poses a choice of lexeme. For this task, it was
necessary to change the backoff parameters in
the language model in such a way that unseen
words were weighted very low, unlike cases in
which the language model is used for recog-
nition or parsing. For each of these two al-
gorithms, we can consider all members of the
supersynset of the lexeme, or only those with
the part-of-speech as specified in the input
representation. This gives us four algorithms,
tree-all, tree-pos, linear-all, linear-pos.
</bodyText>
<sectionHeader confidence="0.988181" genericHeader="method">
5 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999771873015873">
The evaluation is based on comparison with
a test corpus of 100 sentences randomly cho-
sen from the Penn Tree Bank WSJ corpus
(not used in training, of course). The average
sentence length is 16.69 words, and the aver-
age lexical ambiguity per word is 4.8. (The
maximum lexical ambiguity is 69 for take.)
The premise of the evaluation is that the goal
of generation is to match the gold standard
as closely as possible, even if other results
would also be acceptable or even appropriate;
in some applications, variation may be use-
ful, in which case this evaluation may not be
appropriate.
For the evaluation, we use two different
metrics. The first metric, string accuracy,
is based on similar metrics used in machine
translation. We compare the output of FER-
GUS to the gold standard sentence and count
the number of move and substitution oper-
ations that need to be performed to trans-
form the actual output into the gold standard.
This number is subtracted from and then di-
vided by the length of the sentence, yielding
a number between 0 and 1, with 1 the best
score. For a more detailed discussion of the
issue of metrics in evaluation, of the metrics
we have used, and of an experiment relating
human judgments to these metrics, see (Ban-
galore et al., 2000). String accuracy measures
the performance of the entire FERGUS system.
For the second metric, bag accuracy, we
disregard linear order and calculate recall and
precision on the bag (i.e., multiset) of lex-
ernes in the generated string as compared to
the bag of lexemes in the gold standard sen-
tence. Since the number of words in the two
sentences is usually the same, recall and pre-
cision are very close, and we report as set
accuracy their average, which can be inter-
preted as the percentage of correctly gener-
ated lexemes. Bag accuracy measures the per-
formance of the Lexeme Chooser only.
The results for the first task (the
lexical-before-syntactic-choice and lexical-
with-syntactic-choice architectures) are sum-
marized in the table in Figure 4. As we can
see, DtrLex on its own far outperforms any
other algorithm that does not back off to it.
All of the algorithms that have one backoff to
DtrLex perform at the same level, as does
LexSyn-Syn-DtrLex, while LexSyn-Lex-
DtrLex performs slightly better.
The results for the second task (choosing
lexemes after syntactic choice) are summa-
rized in the table in Figure 5. Because of the
different tasks, the results cannot be mean-
ingfully compared to Figure 4, but we can see
that, as expected, FERGUS performs better on
this task than on the first task. Without part-
of-speech information, the Tree model per-
form better than the linear model, but the
advantage is lost when part-of-speech infor-
</bodyText>
<table confidence="0.999221571428571">
Model String Bag
Accu- Accu-
racy racy
random 0.34 0.69
LexS yn 0.34 0.69
LexS yn-Lex 0.48 0.79
LexSyn-Lex-DtrLex 0.63 0.88
LexSyn-Syn-DtrLex 0.62 0.87
LexSyn-DtrLex 0.62 0.87
Lex 0.48 0.79
Lex-DtrLex 0.62 0.87
LexSynJoint 0.45 0.77
LexSynJoint-DtrLex 0.61 0.87
DtrLex 0.59 0.85
</table>
<figureCaption confidence="0.990014">
Figure 4: Summary of results for the
</figureCaption>
<bodyText confidence="0.911277565217391">
lexical-before-syntactic-choice and lexical-
with-syntactic-choice architectures for the
first task
mation is used as well.
We now draw tentative conclusions for fu-
ture work from these results. Clearly, using
information from the mother node increases
performance, if only slightly. However, there
is no reliable evidence that the lexical-before-
syntactic-choice architecture outperforms the
lexical-with-syntactic-choice architectures or
vice versa. There is also no evidence that
the mother&apos;s supertag affects lexical choice
in the daughter.6 As a result, we conclude
that for the first task we can restrict our at-
tention to a simplified architecture in which
lexical choice for the entire tree occurs before
syntactic choice for the entire tree (i.e., the
two choices are neither simultaneous nor in-
terleaved). Furthermore, the importance of
the backoff DtrLex model shows the impor-
tance of the sparse data problem. Thus, the
model we will concentrate on is Lex-DtrLex.
</bodyText>
<sectionHeader confidence="0.993588" genericHeader="method">
6 Context in Lexical Choice
</sectionHeader>
<bodyText confidence="0.924314714285714">
We now investigate how much context in the
syntax tree is needed in order to optimize lex-
ical choice. In the experiments reported in
6However, we have not used information about the
(semantic or syntactic) role of the daughter, and it
stands to reason that this information can help in lex-
ical choice.
</bodyText>
<table confidence="0.995918333333333">
Model String Ac- Bag Accu-
curacy racy
Tree-All 0.69 0.93
Tree-Pos 0.68 0.93
Linear-All 0.67 0.90
Linear-Pos 0.70 0.93
</table>
<figureCaption confidence="0.949718666666667">
Figure 5: Summary of results for the
syntactic-before-lexical-choice algorithms for
the second task
</figureCaption>
<bodyText confidence="0.999962972972973">
the previous section, a weakness quickly be-
comes apparent: while lexical choice depends
on the mother node, the root lexeme can-
not be chosen in this manner. In the exper-
iments reported above we simply use lexical
frequency at the root. A wrong choice at the
root, however, can lead to a cascade of sub-
sequent wrong choices at lower nodes as they
all depend on previously made wrong choices.
We therefore will base lexical choice on an ex-
tended notion of tree context which includes
the daughters.
Because the number of daughters in a de-
pendency tree is not a priori bound, it is im-
possible to create a probability model of the
type P(iiidi, • • • , /4), where / is the node&apos;s lex-
eme and /di is the lexeme of the ith daughter.
Instead, we make an independence assump-
tion for the daughters and calculate the node-
daughter probabilities separately. Note that
the daughters&apos; lexemes have not actually been
chosen yet, so we use the daughters&apos; meaning
potential rather than their lexemes, as dis-
cussed in Section 3.
The results are shown in Figure 6. Here,
we have renamed the Lex model to Node, to
reflect the fact that only information about
the node whose lexeme is being chosen is
taken into account. Model Mother-node
is the Lex-DtrLex model from Figure 5:
we determine a node&apos;s lexeme based on the
mother&apos;s lexeme. In model Node-daughters
the choice is based on the daughters only. Fi-
nally, model Mother-node-daughters con-
siders both the mother&apos;s lexeme and the
daughters&apos; (excluding, of course, the mother
in case of the root node, and daughters in case
</bodyText>
<table confidence="0.997656625">
Model String Bag
Accu- Accu-
racy racy
Random 0.34 0.69
Node 0.59 0.85
Mother-node 0.62 0.87
Node-daughters 0.63 0.88
Mother-node-daughters 0.67 0.90
</table>
<figureCaption confidence="0.9960635">
Figure 6: Summary of results for different
syntactic context usage
</figureCaption>
<bodyText confidence="0.9922695">
of leaf nodes).
As we can see, extending the context in
the tree from which we draw information can
significantly improve performance beyond the
baseline of choosing the most frequent lexeme
from a supersynset (model Node in Figure 6).
</bodyText>
<sectionHeader confidence="0.986104" genericHeader="method">
7 Using POS Information
</sectionHeader>
<bodyText confidence="0.999117461538461">
For our sample sentence in Figure 3, the
Mother-node-daughters model chooses
concern for the root node. The verb con-
cern is not a synonym of the verb fear, but
the noun concern is a synonym of the noun
fear. For many applications, it is quite rea-
sonable to assume that we know the lexical
class of some or all labels in the input struc-
ture. We therefore investigated the perfor-
mance of our best model so far, Mother-
node-daughters, if we restrict our choice
to those lexemes that have the same part-of-
speech as the input meaning potential. Since
the supersynsets associated with each input
lexeme are now smaller, the performance can-
not be inferior than in the general case (at
least as measured by bag accuracy). Indeed,
for our example sentence, we now obtain the
desired choice, namely fear. However, to our
surprise, overall performance on our test set
increased only very slightly, to a string accu-
racy of 0.68 (from 0.67) and to a bag accuracy
of 0.91 (from 0.90). We conjecture that most
errors are within the correct lexical class, and
thus not saved by taking part-of-speech into
account.
</bodyText>
<sectionHeader confidence="0.995664" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999594">
We have seen that the performance of lexical
choice can be improved over the baseline of
choosing the most frequent member of a su-
persynset by taking syntactic context in the
dependency tree into account. Since sparse-
ness of data is a major problem, we intend to
train the models on the new Brown corpus of
30,000,000 words.
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998683914285714">
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237-266.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for
generation. In Proceedings of the 18th Inter-
national Conference on Computational Linguis-
tics (COLING 2000), Saarbriicken, Germany.
Srinivas Bangalore, Owen Rambow, and Steve
Whittaker. 2000. Evaluation metrics for gen-
eration. In Proceedings of the First Interna-
tional Natural Language Generation Confer-
ence (INLG2000), Mitzpe Ramon, Israel.
Bonnie J. Dorr. 1994. Machine translation diver-
gences: A formal description and proposed so-
lution. Computational Linguistics, 20(4):597—
635.
Christiane Fellbaum. 1997. WordNet: An Elec-
tronic Lexical Database. MITPress, Cambridge,
MA.
Aravind K. Joshi. 1987. An introduction to Tree
Adjoining Grammars. In A. Manaster-Ramer,
editor, Mathematics of Language, pages 87-115.
John Benjamins, Amsterdam.
Irene Langkilde and Kevin Knight. 1998. Gen-
eration that exploits corpus-based statistical
knowledge. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th In-
ternational Conference on Computational Lin-
guistics (COLING-ACL &apos;98), pages 704-710,
Montréal, Canada.
The XTAG-Group. 1999. A lexicalized Tree Ad-
joining Grammar for English. Technical re-
port, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937130">
<title confidence="0.997459">Corpus-Based Lexical Choice in Natural Language Generation</title>
<author confidence="0.96864">Bangalore Rambow</author>
<affiliation confidence="0.99902">AT&amp;T Labs — Research</affiliation>
<address confidence="0.998833">180 Park Avenue Florham Park, NJ 07932</address>
<email confidence="0.999922">fsrini,rambowl@research.att.com</email>
<abstract confidence="0.997631307692308">Choosing the best lexeme to realize a meaning in natural language generation is a hard task. We investigate different tree-based stochastic models for lexical choice. Because of the difficulty of obtaining a sense-tagged corpus, we generalize the notion of synonymy. We show that a tree-based model can achieve a word-bag based accuracy of 90%, representing an improvement over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--2</pages>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="2542" citStr="Bangalore and Rambow, 2000" startWordPosition="397" endWordPosition="400">target texts is restricted, there are applications (including many MT applications) in which the range of linguistic variation of the texts to be generated is far too vast to allow for hand-coding of dictionaries. Furthermore, even for domains with a limited range of linguistic variation, the task of hand-coding the necessary resources can be formidable when a generation system must be ported to a new domain, a new genre, or especially a new language. In seminal work, Langkilde and Knight (1998) have used a language model as a way of choosing among different lexical and syntactic options. In (Bangalore and Rambow, 2000), we present a model of syntactic choice which, like Langkilde and Knight (1998) relies on a linear language model, but unlike their approach, also uses a tree-based representation of syntactic structure, a tree model, and an independently hand-crafted grammar. We show that the addition of the tree-based model improves the performance of the syntactic choice module. The system, called FERGUS (Flexible Rationalist-Empiricist Generation Using Syntax), does not, how&apos;There have been some advances in automatic transfer dictionary building for MT, but the automatically extracted dictionaries do not </context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
<author>Steve Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference (INLG2000),</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="14104" citStr="Bangalore et al., 2000" startWordPosition="2354" endWordPosition="2358">s. The first metric, string accuracy, is based on similar metrics used in machine translation. We compare the output of FERGUS to the gold standard sentence and count the number of move and substitution operations that need to be performed to transform the actual output into the gold standard. This number is subtracted from and then divided by the length of the sentence, yielding a number between 0 and 1, with 1 the best score. For a more detailed discussion of the issue of metrics in evaluation, of the metrics we have used, and of an experiment relating human judgments to these metrics, see (Bangalore et al., 2000). String accuracy measures the performance of the entire FERGUS system. For the second metric, bag accuracy, we disregard linear order and calculate recall and precision on the bag (i.e., multiset) of lexernes in the generated string as compared to the bag of lexemes in the gold standard sentence. Since the number of words in the two sentences is usually the same, recall and precision are very close, and we report as set accuracy their average, which can be interpreted as the percentage of correctly generated lexemes. Bag accuracy measures the performance of the Lexeme Chooser only. The result</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>Srinivas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proceedings of the First International Natural Language Generation Conference (INLG2000), Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Machine translation divergences: A formal description and proposed solution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>635</pages>
<contexts>
<context position="7383" citStr="Dorr, 1994" startWordPosition="1219" endWordPosition="1220"> we walked across the state and we traversed the state on foot.4 The issue of dealing with such structural paraphrases is a complex one and presumably requires corpora which are annotated in more complex ways than the syntactically annotated corpora available to us today. 4 Lexical Choice: Basic Architecture In the architecture diagram in Figure 2, lexical choice happens before syntactic choice. However, a priori there are other architectural options as well. In this section, we inves4These kinds of examples are known as structural divergences when found in the context of machine translation (Dorr, 1994). tigate three basic options of integrating lexical with syntactic choice: lexical choice before syntactic choice; integrated simultaneous lexical and syntactic choice; and lexical choice after syntactic choice. We observe that these three possible architectures do not accomplish exactly the same tasks. If we perform lexical choice after syntactic choice, then the range of possible synonyms is reduced: not only must we choose a member of the supersynset of the input lexeme, but it must also be compatible with the syntactic choice already made. For example, if the input lexeme is give, we can c</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>Bonnie J. Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20(4):597— 635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database. MITPress,</title>
<date>1997</date>
<location>Cambridge, MA.</location>
<marker>Fellbaum, 1997</marker>
<rawString>Christiane Fellbaum. 1997. WordNet: An Electronic Lexical Database. MITPress, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to Tree Adjoining Grammars. In</title>
<date>1987</date>
<booktitle>Mathematics of Language,</booktitle>
<pages>87--115</pages>
<editor>A. Manaster-Ramer, editor,</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam.</location>
<marker>Joshi, 1987</marker>
<rawString>Aravind K. Joshi. 1987. An introduction to Tree Adjoining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language, pages 87-115. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98),</booktitle>
<pages>704--710</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="2415" citStr="Langkilde and Knight (1998)" startWordPosition="376" endWordPosition="379">.&apos; While such an approach is perfectly reasonable for many applications, especially those in which linguistic variation in the target texts is restricted, there are applications (including many MT applications) in which the range of linguistic variation of the texts to be generated is far too vast to allow for hand-coding of dictionaries. Furthermore, even for domains with a limited range of linguistic variation, the task of hand-coding the necessary resources can be formidable when a generation system must be ported to a new domain, a new genre, or especially a new language. In seminal work, Langkilde and Knight (1998) have used a language model as a way of choosing among different lexical and syntactic options. In (Bangalore and Rambow, 2000), we present a model of syntactic choice which, like Langkilde and Knight (1998) relies on a linear language model, but unlike their approach, also uses a tree-based representation of syntactic structure, a tree model, and an independently hand-crafted grammar. We show that the addition of the tree-based model improves the performance of the syntactic choice module. The system, called FERGUS (Flexible Rationalist-Empiricist Generation Using Syntax), does not, how&apos;There</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98), pages 704-710, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG-Group</author>
</authors>
<title>A lexicalized Tree Adjoining Grammar for English.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<marker>XTAG-Group, 1999</marker>
<rawString>The XTAG-Group. 1999. A lexicalized Tree Adjoining Grammar for English. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>