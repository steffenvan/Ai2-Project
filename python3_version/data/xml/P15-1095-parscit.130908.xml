<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002274">
<title confidence="0.979537">
Robust Subgraph Generation Improves Abstract Meaning Representation
Parsing
</title>
<author confidence="0.989299">
Keenon Werling
</author>
<affiliation confidence="0.982876">
Stanford University
</affiliation>
<email confidence="0.991217">
keenon@stanford.edu
</email>
<author confidence="0.95436">
Gabor Angeli
</author>
<affiliation confidence="0.957569">
Stanford University
</affiliation>
<email confidence="0.989407">
angeli@stanford.edu
</email>
<author confidence="0.982782">
Christopher D. Manning
</author>
<affiliation confidence="0.977648">
Stanford University
</affiliation>
<email confidence="0.996342">
manning@stanford.edu
</email>
<sectionHeader confidence="0.993667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993420578947369">
The Abstract Meaning Representation
(AMR) is a representation for open-
domain rich semantics, with potential use
in fields like event extraction and machine
translation. Node generation, typically
done using a simple dictionary lookup, is
currently an important limiting factor in
AMR parsing. We propose a small set
of actions that derive AMR subgraphs by
transformations on spans of text, which
allows for more robust learning of this
stage. Our set of construction actions
generalize better than the previous ap-
proach, and can be learned with a sim-
ple classifier. We improve on the previ-
ous state-of-the-art result for AMR pars-
ing, boosting end-to-end performance by
3 F1 on both the LDC2013E117 and
LDC2014T12 datasets.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998991722222222">
The Abstract Meaning Representation (AMR)
(Banarescu et al., 2013) is a rich, graph-based lan-
guage for expressing semantics over a broad do-
main. The formalism is backed by a large data-
labeling effort, and it holds promise for enabling a
new breed of natural language applications rang-
ing from semantically aware MT to rich broad-
domain QA over text-based knowledge bases. Fig-
ure 1 shows an example AMR for “he gleefully ran
to his dog Rover,” and we give a brief introduction
to AMR in Section 2. This paper focuses on AMR
parsing, the task of mapping a natural language
sentence into an AMR graph.
We follow previous work (Flanigan et al., 2014)
in dividing AMR parsing into two steps. The
first step is concept identification, which generates
AMR nodes from text, and which we’ll refer to as
NER++ (Section 3.1). The second step is relation
</bodyText>
<figureCaption confidence="0.651809666666667">
Figure 1: The AMR graph for He gleefully ran to
his dog Rover. We show that improving the gen-
eration of low level subgraphs (e.g., Rover gener-
</figureCaption>
<bodyText confidence="0.990350217391305">
ating name :op1 −−→ “Rover”) significantly improves
end-to-end performance.
identification, which adds arcs to link these nodes
into a fully connected AMR graph, which we’ll
call SRL++ (Section 3.2).
We observe that SRL++ is not the hard part of
AMR parsing; rather, much of the difficulty in
AMR is generating high accuracy concept sub-
graphs from the NER++ component. For example,
when the existing AMR parser JAMR (Flanigan
et al., 2014) is given a gold NER++ output, and
must only perform SRL++ over given subgraphs
it scores 80 F1 – nearly the inter-annotator agree-
ment of 83 F1, and far higher than its end to end
accuracy of 59 F1.
SRL++ within AMR is relatively easy given a
perfect NER++ output, because so much pressure
is put on the output of NER++ to carry meaningful
information. For example, there’s a strong type-
check feature for the existence and type of any arc
just by looking at its end-points, and syntactic de-
pendency features are very informative for remov-
ing any remaining ambiguity. If a system is con-
</bodyText>
<page confidence="0.920071">
982
</page>
<note confidence="0.988386666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 982–991,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.936735666666667">
Figure 2: A graphical explanation of our method. We represent the derivation process for He gleefully
ran to his dog Rover. First the tokens in the sentence are labeled with derivation actions, then these
actions are used to generate AMR subgraphs, which are then stitched together to form a coherent whole.
</figureCaption>
<bodyText confidence="0.999643609756098">
sidering how to link the node run-01 in Figure 1,
the verb-sense frame for “run-01” leaves very little
uncertainty for what we could assign as an ARG0
arc. It must be a noun, which leaves either he or
dog, and this is easily decided in favor of he by
looking for an nsubj arc in the dependency parse.
The primary contribution of this paper is a novel
approach to the NER++ task, illustrated in Fig-
ure 2. We notice that the subgraphs aligned to lexi-
cal items can often be generated from a small set of
generative actions which generalize across tokens.
For example, most verbs generate an AMR node
corresponding to the verb sense of the appropri-
ate PropBank frame – e.g., run generates run-01
in Figure 1. This allows us to frame the NER++
task as the task of classifying one of a small num-
ber of actions for each token, rather than choosing
a specific AMR subgraph for every token in the
sentence.
Our approach to the end-to-end AMR parsing
task is therefore as follows: we define an action
space for generating AMR concepts, and create
a classifier for classifying lexical items into one
of these actions (Section 4). This classifier is
trained from automatically generated alignments
between the gold AMR trees and their associated
sentences (Section 5), using an objective which fa-
vors alignment mistakes which are least harmful to
the NER++ component. Finally, the concept sub-
graphs are combined into a coherent AMR parse
using the maximum spanning connected subgraph
algorithm of Flanigan et al. (2014).
We show that our approach provides a large
boost to recall over previous approaches, and that
end to end performance is improved from 59 to
62 smatch (an F1 measure of correct AMR arcs;
see Cai and Knight (2013)) when incorporated into
the SRL++ parser of Flanigan et al. (2014). When
evaluating the performance of our action classifier
in isolation, we obtain an action classification ac-
curacy of 84.1%.
</bodyText>
<sectionHeader confidence="0.964656" genericHeader="introduction">
2 The AMR Formalism
</sectionHeader>
<bodyText confidence="0.999957666666667">
AMR is a language for expressing semantics as
a rooted, directed, and potentially cyclic graph,
where nodes represent concepts and arcs are re-
lationships between concepts. AMR is based
on neo-Davidsonian semantics, (Davidson, 1967;
Parsons, 1990). The nodes (concepts) in an AMR
graph do not have to be explicitly grounded in the
source sentence, and while such an alignment is
often generated to train AMR parsers, it is not pro-
vided in the training corpora. The semantics of
nodes can represent lexical items (e.g., dog), sense
tagged lexical items (e.g., run-01), type markers
(e.g., date-entity), and a host of other phenomena.
The edges (relationships) in AMR describe one
of a number of semantic relationships between
concepts. The most salient of these is seman-
tic role labels, such as the ARG0 and destination
arcs in Figure 2. However, often these arcs define
</bodyText>
<page confidence="0.999203">
983
</page>
<figureCaption confidence="0.920438666666667">
Figure 3: AMR representation of the word sailor,
which is notable for breaking the word up into
a self-contained multi-node unit unpacking the
derivational morphology of the word.
Figure 4: AMR representation of the span Jan-
uary 1, 2008, an example of how AMR can rep-
</figureCaption>
<bodyText confidence="0.998510689655172">
resent structured data by creating additional nodes
such as date-entity to signify the presence of spe-
cial structure.
a semantics more akin to syntactic dependencies
(e.g., mod standing in for adjective and adverbial
modification), or take on domain-specific mean-
ing (e.g., the month, day, and year arcs of a date-
entity).
To introduce AMR and its notation in more de-
tail, we’ll unpack the translation of the sentence
“he gleefully ran to his dog Rover.” We show in
Figure 1 the interpretation of this sentence as an
AMR graph.
The root node of the graph is labeled run-01,
corresponding to the PropBank (Palmer et al.,
2005) definition of the verb ran. run-01 has an
outgoing ARG0 arc to a node he, with the usual
PropBank semantics. The outgoing mod edge
from run-01 to glee takes a general purpose se-
mantics corresponding to adjective, adverbial, or
other modification of the governor by the depen-
dent. We note that run-01 has a destination arc to
dog. The label for destination is taken from a finite
set of special arc sense tags similar to the prepo-
sition senses found in (Srikumar, 2013). The last
portion of the figure parses dog to a node which
serves as a type marker similar to named entity
types, and Rover into the larger subgraph indicat-
ing a concept with name “Rover.”
</bodyText>
<subsectionHeader confidence="0.989637">
2.1 AMR Subgraphs
</subsectionHeader>
<bodyText confidence="0.999943096774194">
The mapping from tokens of a sentence to AMR
nodes is not one-to-one. A single token or span
of tokens can generate a subgraph of AMR con-
sisting of multiple nodes. These subgraphs can
logically be considered the expression of a single
concept, and are useful to treat as such (e.g., see
Section 3.1).
Many of these multi-node subgraphs capture
structured data such as time expressions, as in Fig-
ure 4. In this example, a date-entity node is cre-
ated to signify that this cluster of nodes is part of
a structured sub-component representing a date,
where the nodes and arcs within the component
have specific semantics. This illustrates a broader
recurring pattern in AMR: an artificial node may,
based on its title, have expected children with spe-
cial semantics. A particularly salient example of
this pattern is the name node (see “Rover” in Fig-
ure 1) which signifies that all outgoing arcs with
label op comprise the tokens of a name object.
The ability to decouple the meaning representa-
tion of a lexical item from its surface form allows
for rich semantic interpretations of certain con-
cepts in a sentence. For example, the token sailor
is represented in Figure 3 by a concept graph rep-
resenting a person who performs the action sail-
01. Whereas often the AMR node aligned to a
span of text is a straightforward function of the
text, these cases remain difficult to capture in a
principled way beyond memorizing mappings be-
tween tokens and subgraphs.
</bodyText>
<sectionHeader confidence="0.990822" genericHeader="method">
3 Task Decomposition
</sectionHeader>
<bodyText confidence="0.999909230769231">
To the best of our knowledge, the JAMR parser
is the only published end-to-end AMR parser at
the time of publication. An important insight in
JAMR is that AMR parsing can be broken into
two distinct tasks: (1) NER++ (concept identifi-
cation): the task of interpreting what entities are
being referred to in the text, realized by gener-
ating the best AMR subgraphs for a given set of
tokens, and (2) SRL++ (relation identification):
the task of discovering what relationships exist be-
tween entities, realized by taking the disjoint sub-
graphs generated by NER++ and creating a fully-
connected graph. We describe both tasks in more
</bodyText>
<page confidence="0.99435">
984
</page>
<bodyText confidence="0.697425">
detail below.
</bodyText>
<subsectionHeader confidence="0.968026">
3.1 NER++
</subsectionHeader>
<bodyText confidence="0.999938913043478">
Much of the difficulty of parsing to AMR lies in
generating local subgraphs representing the mean-
ing of token spans. For instance, the formalism
implicitly demands rich notions of NER, lemma-
tization, word sense disambiguation, number nor-
malization, and temporal parsing; among others.
To illustrate, a correct parse of the sentence in Fig-
ure 2 requires lemmatization (gleefully → glee),
word sense tagging (run → run-01), and open do-
main NER (i.e., Rover), Furthermore, many of the
generated subgraphs (e.g., sailor in Figure 3) have
rich semantics beyond those produced by standard
NLP systems.
Formally, NER++ is the task of generating a
disjoint set of subgraphs representing the mean-
ings of localized spans of words in the sentence.
For NER++, JAMR uses a simple Viterbi sequence
model to directly generate AMR-subgraphs from
memorized mappings of text spans to subgraphs.
This paper’s main contribution, presented in Sec-
tion 4, is to make use of generative actions to gen-
erate these subgraphs, rather than appealing to a
memorized mapping.
</bodyText>
<subsectionHeader confidence="0.999386">
3.2 SRL++
</subsectionHeader>
<bodyText confidence="0.999962666666667">
The second stage of the AMR decomposition con-
sists of generating a coherent graph from the set of
disjoint subgraphs produced by NER++. Whereas
NER++ produces subgraphs whose arcs encode
domain-specific semantics (e.g., month), the arcs
in SRL++ tend to have generally applicable se-
mantics. For example, the many arcs encode con-
ventional semantic roles (e.g., ARG0 and desti-
nation in Figure 2), or a notion akin to syntac-
tic dependencies (e.g., mod and poss in Figure 2).
For SRL++, JAMR uses a variation of the max-
imum spanning connected graph algorithm aug-
mented by dual decomposition to impose linguis-
tically motivated constraints on a maximum likeli-
hood objective.
</bodyText>
<sectionHeader confidence="0.997044" genericHeader="method">
4 A Novel NER++ Method
</sectionHeader>
<bodyText confidence="0.999965529411765">
The training sets currently available for AMR are
not large. To illustrate, 38% of the words in the
LDC2014E113 dev set are unseen during training
time. With training sets this small, memorization-
based approaches are extremely brittle. We re-
move much of the necessity to memorize map-
pings in NER++ by partitioning the AMR sub-
graph search space in terms of the actions needed
to derive a node from its aligned token. At test
time we do a sequence labeling of input tokens
with these actions, and then deterministically de-
rive the AMR subgraphs from spans of tokens
by applying the transformation decreed by their
actions. We explain in Section 4.1 how exactly
we manage this partition, and in Section 4.3 how
we create training data from existing resources to
setup and train an action-type classifier.
</bodyText>
<subsectionHeader confidence="0.998277">
4.1 Derivation actions
</subsectionHeader>
<bodyText confidence="0.99992375">
We partition the AMR subgraph space into a set of
9 actions, each corresponding to an action that will
be taken by the NER++ system if a token receives
this classification.
IDENTITY This action handles the common
case that the title of the node corresponding to a
token is identical to the source token. To execute
the action, we take the lowercased version of the
token to be the title of the corresponding node.
NONE This action corresponds to ignoring this
token, in the case that the node should not align to
any corresponding AMR fragment.
VERB This action captures the verb-sense dis-
ambiguation feature of AMR. To execute on a to-
ken, we find the most similar verb in PropBank
based on Jaro-Winkler distance, and adopt its most
frequent sense. This serves as a reasonable base-
line for word sense disambiguation, although of
course accuracy would be expected to improve if
a sophisticated system were incorporated.
VALUE This action interprets a token by its in-
teger value. The AMR representation is sensitive
to the difference between a node with a title of 5
(the integer value) and “5” or “five” – the string
value. This is a rare action, but is nonetheless dis-
tinct from any of the other classes. We execute this
action by extracting an integer value with a regex
based number normalizer, and using the result as
the title of the generated node.
LEMMA AMR often performs stemming and
part-of-speech transformations on the source to-
ken in generating a node. For example, we get
glee from gleefully. We capture this by a LEMMA
action, which is executed by using the lemma of
the source token as the generated node title. Note
that this does not capture all lemmatizations, as
</bodyText>
<page confidence="0.995435">
985
</page>
<bodyText confidence="0.999901538461538">
there are often discrepancies between the lemma
generated by the lemmatizer and the correct AMR
lemma.
NAME AMR often references names with a
special structured data type: the name construc-
tion. For example, Rover in Figure 1. We can
capture this phenomenon on unseen names by at-
taching a created name node to the top of a span.
PERSON A variant of the NAME action, this
action produces a subgraph identical to the NAME
action, but adds a node person as a parent. This
is, in effect, a name node with an implicit entity
type of person. Due to discrepancies between the
output of our named entity tagger and the richer
AMR named entity ontology, we only apply this
tag to the person named entity tag.
DATE The most frequent of the structured data
type in the data, after name, is the date-entity con-
struction (for an example see Figure 4). We de-
terministically take the output of SUTime (Chang
and Manning, 2012) and convert it into the date-
entity AMR representation.
DICT This class serves as a back-off for the
other classes, implementing an approach similar
to Flanigan et al. (2014). In particular, we mem-
orize a simple mapping from spans of text (such
as sailor) to their corresponding most frequently
aligned AMR subgraphs in the training data (i.e.,
the graph in Figure 3). See Section 5 for details
on the alignment process. At test time we can do a
lookup in this dictionary for any element that gets
labeled with a DICT action. If an entry is not
found in the mapping, we back off to the second
most probable class proposed by the classifier.
It is worth observing at this point that our ac-
tions derive much of their power from the similar-
ity between English words and their AMR coun-
terparts; creating an analogue of these actions for
other languages remains an open problem.
</bodyText>
<subsectionHeader confidence="0.99618">
4.2 Action Reliability
</subsectionHeader>
<bodyText confidence="0.987188765957447">
In many cases, multiple actions could yield the
same subgraph when applied to a node. In this
section we introduce a method for resolving this
ambiguity based on comparing the reliability with
which actions generate the correct subgraph, and
discuss implications.
Even given a perfect action classification for
a token, certain action executions can introduce
Figure 5: Reliability of each action. The top row
are actions which are deterministic; the second
row occasionally produce errors. DICT is the least
preferred action, with a relatively high error rate.
errors. Some of our actions are entirely deter-
ministic in their conversion from the word to the
AMR subgraph (e.g., IDENTITY), but others are
prone to making mistakes in this conversion (e.g.,
VERB, DICT). We define the notion of action re-
liability as the probability of deriving the correct
node from a span of tokens, conditioned on hav-
ing chosen the correct action.
To provide a concrete example, our dictionary
lookup classifier predicts the correct AMR sub-
graph 67% of the time on the dev set. We therefore
define the reliability of the DICT action as 0.67.
In contrast to DICT, correctly labeling a node as
IDENTITY, NAME, PERSON, and NONE have
action reliability of 1.0, since there is no ambigu-
ity in the node generation once one of those ac-
tions have been selected, and we are guaranteed to
generate the correct node given the correct action.
We can therefore construct a hierarchy of reli-
ability (Figure 5) – all else being equal, we pre-
fer to generate actions from higher in the hierar-
chy, as they are more likely to produce the cor-
rect subgraph. This hierarchy is useful in resolv-
ing ambiguity throughout our system. During the
creation of training data for our classifier (Sec-
tion 4.3) from our aligner, when two actions could
both generate the aligned AMR node we prefer the
more reliable one. In turn, in our aligner we bias
alignments towards those which generating more
reliable action sequences as training data (see Sec-
tion 5).
The primary benefit of this action-based
NER++ approach is that we can reduce the us-
age of low reliability actions, like DICT. The
approach taken in Flanigan et al. (2014) can be
</bodyText>
<page confidence="0.98722">
986
</page>
<table confidence="0.9999357">
Action # Tokens % Total
NONE 41538 36.2
DICT 30027 26.1
IDENTITY 19034 16.6
VERB 11739 10.2
LEMMA 5029 4.5
NAME 4537 3.9
DATE 1418 1.1
PERSON 1336 1.1
VALUE 122 0.1
</table>
<tableCaption confidence="0.8046375">
Table 1: Distribution of action types in the
proxy section of the newswire section of the
LDC2014T12 dataset, generated from automati-
cally aligned data.
</tableCaption>
<table confidence="0.832121111111111">
Input token; word embedding
Left+right token / bigram
Token length indicator
Token starts with “non”
POS; Left+right POS / bigram
Dependency parent token / POS
Incoming dependency arc
Bag of outgoing dependency arcs
Number of outgoing dependency arcs
Max Jaro-Winkler to any lemma in PropBank
Output tag of the VERB action if applied
Output tag of the DICT action if applied
NER; Left+right NER / bigram
Capitalization
Incoming prep * or appos + parent has NER
Token is pronoun
Token is part of a coref chain
Token pronoun and part of a coref chain
</table>
<tableCaption confidence="0.665382">
Table 2: The features for the NER++ maxent clas-
sifier.
</tableCaption>
<bodyText confidence="0.995438444444445">
thought of as equivalent to classifying every token
as the DICT action.
We analyze the empirical distribution of actions
in our automatically aligned corpus in Table 1.
The cumulative frequency of the non-DICT ac-
tions is striking: we can generate 74% of the to-
kens with high reliability (p ≥ 0.9) actions. In this
light, it is unsurprising that our results demonstrate
a large gain in recall on the test set.
</bodyText>
<subsectionHeader confidence="0.999104">
4.3 Training the Action Classifier
</subsectionHeader>
<bodyText confidence="0.999981971428571">
Given a set of AMR training data, in the form of
(graph, sentence) pairs, we first induce alignments
from the graph nodes to the sentence (see Sec-
tion 5). Formally, for every node ni in the AMR
graph, alignment gives us some token sj (at the
jth index in the sentence) that we believe gener-
ated the node ni.
Then, for each action type, we can ask whether
or not that action type is able to take token sj and
correctly generate ni. For concreteness, imagine
the token sj is running, and the node ni has the
title run-01. The two action types we find that are
able to correctly generate this node are DICT and
VERB. We choose the most reliable action type
of those available (see Figure 5) to generate the
observed node – in this case, VERB.
In cases where an AMR subgraph is generated
from multiple tokens, we assign the action label to
each token which generates the subgraph. Each of
these tokens are added to the training set; at test
time, we collapse sequences of adjacent identical
action labels, and apply the action once to the re-
sulting token span.
Inducing the most reliable action (according to
the alignments) for every token in the training cor-
pus provides a supervised training set for our ac-
tion classifier, with some noise introduced by the
automatically generated alignments. We then train
a simple maxent classifier1 to make action deci-
sions at each node. At test time, the classifier takes
as input a pair (i, 5), where i is the index of the to-
ken in the input sentence, and 5 is a sequence to-
kens representing the source sentence. It then uses
the features in Table 2 to predict the actions to take
at that node.
</bodyText>
<sectionHeader confidence="0.979799" genericHeader="method">
5 Automatic Alignment of Training Data
</sectionHeader>
<bodyText confidence="0.999951052631579">
AMR training data is in the form of bi-text, where
we are given a set of (sentence, graph) pairs, with
no explicit alignments between them. We would
like to induce a mapping from each node in the
AMR graph to the token it represents. It is per-
fectly possible for multiple nodes to align to the
same token – this is the case with sailors, for in-
stance.
It is not possible, within our framework, to rep-
resent a single node being sourced from multi-
ple tokens. Note that a subgraph can consist of
many individual nodes; in cases where a subgraph
should align to multiple tokens, we generate an
alignment from the subgraph’s nodes to the associ-
ated tokens in the sentence. It is empirically very
rare for a subgraph to have more nodes than the
token span it should align to.
There have been two previous attempts at pro-
ducing automatic AMR alignments. The first was
</bodyText>
<footnote confidence="0.9664945">
1A sequence model was tried and showed no improve-
ment over a simple maxent classifier.
</footnote>
<page confidence="0.995956">
987
</page>
<bodyText confidence="0.999983076923077">
published as a component of JAMR, and used a
rule-based approach to perform alignments. This
was shown to work well on the sample of 100
hand-labeled sentences used to develop the sys-
tem. Pourdamghani et al. (2014) approached the
alignment problem in the framework of the IBM
alignment models. They rendered AMR graphs as
text, and then used traditional machine translation
alignment techniques to generate an alignment.
We propose a novel alignment method, since
our decomposition of the AMR node generation
process into a set of actions provides an additional
objective for the aligner to optimize, in addition to
the accuracy of the alignment itself. We would like
to produce the most reliable sequence of actions
for the NER++ model to train from, where reliable
is taken in the sense defined in Section 4.2. To give
an example, a sequence of all DICT actions could
generate any AMR graph, but is very low reliabil-
ity. A sequence of all IDENTITY actions could
only generate one set of nodes, but does it with
absolute certainty.
We formulate this objective as a Boolean LP
problem. Let Q be a matrix in 10,1}|N|x|S |of
Boolean constrained variables, where N are the
nodes in an AMR graph, and S are the tokens in
the sentence. The meaning of Qi,j = ✶ can be
interpreted as node ni having being aligned to to-
ken sj. Furthermore, let V be a matrix T |N|x|S|,
where T is the set of NER++ actions from Sec-
tion 4. Each matrix element Vi,j is assigned the
most reliable action which would generate node
ni from token sj. We would like to maximize the
probability of the actions collectively generating a
perfect set of nodes. This can be formulated lin-
early by maximizing the log-likelihood of the ac-
tions. Let the function REL(l) be the reliability of
action l (probability of generating intended node).
Our objective can then be formulated as follows:
</bodyText>
<equation confidence="0.991692857142857">
�
max Qi,j [log(REL(Vi,j)) + α£i,j] (1)
Q
i,j
s.t.
j
Qk,j + Ql,j &lt;_ 1 bk, l, j; nk � nl (3)
</equation>
<bodyText confidence="0.999967193548387">
where £ is the Jaro-Winkler similarity between the
title of the node i and the token j, α is a hyper-
parameter (set to 0.8 in our experiments), and the
operator +/-&gt; denotes that two nodes in the AMR
graph are both not adjacent and do not have the
same title.
The constraint (2), combined with the binary
constraint on Q, ensures that every node in the
graph is aligned to exactly one token in the source
sentence. The constraint (3) ensures that only ad-
jacent nodes or nodes that share a title can refer to
the same token.
The objective value penalizes alignments which
map to the unreliable DICT tag, while rewarding
alignments with high overlap between the title of
the node and the token. Note that most incorrect
alignments fall into the DICT class by default, as
no other action could generate the correct AMR
subgraph. Therefore, if there exists an alignment
that would consume the token using another ac-
tion, the optimization prefers that alignment. The
Jaro-Winkler similarity term, in turn, serves as
a tie-breaker between equally (un)reliable align-
ments.
There are many packages which can solve
this Boolean LP efficiently. We used Gurobi
(Gurobi Optimization, 2015). Given a matrix Q
that maximizes our objective, we can decode our
solved alignment as follows: for each i, align ni
to the j s.t. Qi,j = 1. By our constraints, exactly
one such j must exist.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999273833333333">
Prior work in AMR and related formalisms in-
clude Jones et al. (2012), and Flanigan et al.
(2014). Jones et al. (2012), motivated by appli-
cations in Machine Translation, proposed a graph-
ical semantic meaning representation that predates
AMR, but is intimately related. They propose
a hyper-edge replacement grammar (HRG) ap-
proach to parsing into and out of this graphical
semantic form. Flanigan et al. (2014) forms the
basis of the approach of this paper. Their system
introduces the two-stage approach we use: they
implement a rule-based alignment to learn a map-
ping from tokens to subgraphs, and train a vari-
ant of a maximum spanning tree parser adapted to
graphs and with additional constraints for their re-
lation identifications (SRL++) component. Wang
et al. (2015) uses a transition based algorithm
to transform dependency trees into AMR parses.
They achieve 64/62/63 P/R/F1 with contributions
roughly orthogonal to our own. Their transforma-
tion action set could be easily augmented by the
robust subgraph generation we propose here, al-
though we leave this to future work.
Beyond the connection of our work with Flani-
</bodyText>
<equation confidence="0.964301">
Qi,j = 1 bi (2)
</equation>
<page confidence="0.99231">
988
</page>
<bodyText confidence="0.999942666666667">
gan et al. (2014), we note that the NER++ com-
ponent of AMR encapsulates a number of lex-
ical NLP tasks. These include named entity
recognition (Nadeau and Sekine, 2007; Finkel et
al., 2005), word sense disambiguation (Yarowsky,
1995; Banerjee and Pedersen, 2002), lemmatiza-
tion, and a number of more domain specific tasks.
For example, a full understanding of AMR re-
quires normalizing temporal expressions (Verha-
gen et al., 2010; Str¨otgen and Gertz, 2010; Chang
and Manning, 2012).
In turn, the SRL++ facet of AMR takes many
insights from semantic role labeling (Gildea and
Jurafsky, 2002; Punyakanok et al., 2004; Sriku-
mar, 2013; Das et al., 2014) to capture the rela-
tions between verbs and their arguments. In addi-
tion, many of the arcs in AMR have nearly syntac-
tic interpretations (e.g., mod for adjective/adverb
modification, op for compound noun expressions).
These are similar to representations used in syn-
tactic dependency parsing (de Marneffe and Man-
ning, 2008; McDonald et al., 2005; Buchholz and
Marsi, 2006).
More generally, parsing to a semantic represen-
tation is has been explored in depth for when the
representation is a logical form (Kate et al., 2005;
Zettlemoyer and Collins, 2005; Liang et al., 2011).
Recent work has applied semantic parsing tech-
niques to representations beyond lambda calculus
expressions. For example, work by Berant et al.
(2014) parses text into a formal representation of
a biological process. Hosseini et al. (2014) solves
algebraic word problems by parsing them into a
structured meaning representation. In contrast to
these approaches, AMR attempts to capture open
domain semantics over arbitrary text.
Interlingua (Mitamura et al., 1991; Carbonell et
al., 1999; Levin et al., 1998) are an important in-
spiration for decoupling the semantics of the AMR
language from the surface form of the text being
parsed; although, AMR has a self-admitted En-
glish bias.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.9991764">
We present improvements in end-to-end AMR
parsing on two datasets using our NER++ compo-
nent. Action type classifier accuracy on an auto-
matically aligned corpus and alignment accuracy
on a small hand-labeled corpus are also reported.
</bodyText>
<tableCaption confidence="0.6612548">
Table 3: Results on two AMR datasets for JAMR
and our NER++ embedded in the JAMR SRL++
component. Note that recall is consistently higher
across both datasets, with only a small loss in pre-
cision.
</tableCaption>
<subsectionHeader confidence="0.990751">
7.1 End-to-end AMR Parsing
</subsectionHeader>
<bodyText confidence="0.999981727272727">
We evaluate our NER++ component in the context
of end-to-end AMR parsing on two corpora: the
newswire section of LDC2014T12 and the split
given in Flanigan et al. (2014) of LDC2013E117,
both consisting primarily of newswire. We com-
pare two systems: the JAMR parser (Flanigan
et al., 2014),2 and the JAMR SRL++ component
with our NER++ approach.
AMR parsing accuracy is measured with a met-
ric called smatch (Cai and Knight, 2013), which
stands for “s(emantic) match.” The metric is the F1
of a best-match between triples implied by the tar-
get graph, and triples in the parsed graph – that is,
the set of (parent, edge, child) triples in the graph.
Our results are given in Table 3. We report
much higher recall numbers on both datasets, with
only small (≤ 1 point) loss in precision. This
is natural considering our approach. A better
NER++ system allows for more correct AMR sub-
graphs to be generated – improving recall – but
does not in itself necessarily improve the accuracy
of the SRL++ system it is integrated in.
</bodyText>
<subsectionHeader confidence="0.998686">
7.2 Component Accuracy
</subsectionHeader>
<bodyText confidence="0.9999377">
We evaluate our aligner on a small set of 100 hand-
labeled alignments, and evaluate our NER++ clas-
sifier on automatically generated alignments over
the whole corpus,
On a hand-annotated dataset of 100 AMR
parses from the LDC2014T12 corpus,3 our aligner
achieves an accuracy of 83.2. This is a measure-
ment of the percentage of AMR nodes that are
aligned to the correct token in their source sen-
tence. Note that this is a different metric than the
</bodyText>
<footnote confidence="0.949835">
2Available at https://github.com/jflanigan/
jamr.
3Our dataset is publicly available at http://nlp.
stanford.edu/projects/amr
</footnote>
<table confidence="0.9995428">
Dataset System P R F1
2014T12 JAMR 67.1 53.2 59.3
Our System 66.6 58.3 62.2
2013E117 JAMR 66.9 52.9 59.1
Our System 65.9 59.0 62.3
</table>
<page confidence="0.998018">
989
</page>
<bodyText confidence="0.999959607142857">
precision/recall of prior work on alignments, and
is based on both a different alignment dataset and
subtly different alignment annotation scheme. In
particular, we require that every AMR node aligns
to some token in the sentence, which forces the
system to always align nodes, even when unsure.
A standard semantics and annotation guideline for
AMR alignment is left for future work; our accu-
racy should be considered only an informal metric.
We find our informativeness-based alignment
objective slightly improves end-to-end perfor-
mance when compared to the rule-based approach
of (Flanigan et al., 2014), improving F1 by roughly
1 point (64/59/61 P/R/F1 to 65/59/62 P/R/F1).
On the automatic alignments over the
LDC2014T12 corpus, our action classifier
achieved a test accuracy of 0.841. The classifier’s
most common class of mistakes are incorrect
DICT classifications. It is reassuring that some of
these errors can be recovered from by the naive
dictionary lookup finding the correct mapping.
The DICT action lookup table achieved an ac-
curacy of 0.67. This is particularly impressive
given that our model moves many of the difficult
semantic tasks onto the DICT tag, and that this
lookup does not make use of any learning beyond
a simple count of observed span to subgraph map-
pings.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999993142857143">
We address a key challenge in AMR parsing: the
task of generating subgraphs from lexical items
in the sentence. We show that a simple classi-
fier over actions which generate these subgraphs
improves end-to-end recall for AMR parsing with
only a small drop in precision, leading to an over-
all gain in F1. A clear direction of future work is
improving the coverage of the defined actions. For
example, a richer lemmatizer could shift the bur-
den of lemmatizing unknown words into the AMR
lemma semantics and away from the dictionary
lookup component. We hope our decomposition
provides a useful framework to guide future work
in NER++ and AMR in general.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999939181818182">
We thank the anonymous reviewers for their
thoughtful feedback. Stanford University grate-
fully acknowledges the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999380697674419">
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. Proc. Linguistic Annotation Work-
shop.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using wordnet. In Computational linguistics
and intelligent text processing.
Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,
Brad Huang, Christopher D Manning, Abby Van-
der Linden, Brittany Harding, and Peter Clark.
2014. Modeling biological processes for reading
comprehension. In Proc. EMNLP.
Sabine Buchholz and Erwin Marsi. 2006. CONLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.
Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In ACL
(2), pages 748–752.
Jaime G Carbonell, Teruko Mitamura, and Eric H Ny-
berg. 1999. The KANT perspective: A critique
of pure transfer (and pure interlingua, pure statis-
tics,...).
Angel Chang and Chris Manning. 2012. SUTIME: a
library for recognizing and normalizing time expres-
sions. In Language Resources and Evaluation.
Dipanjan Das, Desai Chen, Andr´e FT Martins, Nathan
Schneider, and Noah A Smith. 2014. Frame-
semantic parsing. Computational Linguistics,
40(1):9–56.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic
of Decision and Action, pages 81–120. University of
Pittsburgh Press, Pittsburgh, PA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
</reference>
<page confidence="0.968998">
990
</page>
<reference confidence="0.99980005">
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.
Inc. Gurobi Optimization. 2015. Gurobi optimizer ref-
erence manual.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In EMNLP.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING, pages
1359–1376.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In AAAI, Pittsburgh, PA.
Lori S Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An interlingua based on domain
actions for machine translation of task-oriented di-
alogues. In ICSLP, volume 98, pages 1155–1158.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
ACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL, Morristown, NJ, USA.
Teruko Mitamura, Eric H Nyberg, and Jaime G Car-
bonell. 1991. An efficient interlingua translation
system for multi-lingual document production. Pro-
ceedings of Machine Translation Summit III.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.
Terence Parsons. 1990. Events in the Semantics of En-
glish: A study in subatomic semantics. MIT Press,
Cambridge, MA.
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In EMNLP.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1346. Association for Computa-
tional Linguistics.
Vivek Srikumar. 2013. The semantics of role label-
ing. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
Jannik Str¨otgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
Sem-Eval.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, Uppsala,
Sweden.
Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015. A transition-based algorithm for amr parsing.
In NAACL-HLT.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI. AUAI Press.
</reference>
<page confidence="0.998086">
991
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259282">
<title confidence="0.982678666666667">Robust Subgraph Generation Improves Abstract Meaning Representation Parsing Keenon</title>
<author confidence="0.910579">Stanford</author>
<email confidence="0.999186">keenon@stanford.edu</email>
<author confidence="0.745288">Gabor</author>
<affiliation confidence="0.519883">Stanford</affiliation>
<email confidence="0.998898">angeli@stanford.edu</email>
<author confidence="0.972004">D Christopher</author>
<affiliation confidence="0.734723">Stanford</affiliation>
<email confidence="0.999543">manning@stanford.edu</email>
<abstract confidence="0.9946917">The Abstract Meaning Representation (AMR) is a representation for opendomain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by both the LDC2013E117 and LDC2014T12 datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>Proc. Linguistic Annotation Workshop.</booktitle>
<contexts>
<context position="1061" citStr="Banarescu et al., 2013" startWordPosition="146" endWordPosition="149">ion, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F1 on both the LDC2013E117 and LDC2014T12 datasets. 1 Introduction The Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rich, graph-based language for expressing semantics over a broad domain. The formalism is backed by a large datalabeling effort, and it holds promise for enabling a new breed of natural language applications ranging from semantically aware MT to rich broaddomain QA over text-based knowledge bases. Figure 1 shows an example AMR for “he gleefully ran to his dog Rover,” and we give a brief introduction to AMR in Section 2. This paper focuses on AMR parsing, the task of mapping a natural language sentence into an AMR graph. We follow previous work (Flanigan et al., 2014) in dividing AMR pars</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. Proc. Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using wordnet. In Computational linguistics and intelligent text processing.</title>
<date>2002</date>
<contexts>
<context position="27056" citStr="Banerjee and Pedersen, 2002" startWordPosition="4627" endWordPosition="4630">a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expression</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using wordnet. In Computational linguistics and intelligent text processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Vivek Srikumar</author>
<author>Pei-Chun Chen</author>
<author>Brad Huang</author>
<author>Christopher D Manning</author>
<author>Abby Vander Linden</author>
<author>Brittany Harding</author>
<author>Peter Clark</author>
</authors>
<title>Modeling biological processes for reading comprehension. In</title>
<date>2014</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="28165" citStr="Berant et al. (2014)" startWordPosition="4806" endWordPosition="4809"> nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 Results We present improvements in end-to-end AMR parsing on two </context>
</contexts>
<marker>Berant, Srikumar, Chen, Huang, Manning, Linden, Harding, Clark, 2014</marker>
<rawString>Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Brad Huang, Christopher D Manning, Abby Vander Linden, Brittany Harding, and Peter Clark. 2014. Modeling biological processes for reading comprehension. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CONLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27814" citStr="Buchholz and Marsi, 2006" startWordPosition="4750" endWordPosition="4753">xpressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain s</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CONLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: an evaluation metric for semantic feature structures.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>748--752</pages>
<contexts>
<context position="5296" citStr="Cai and Knight (2013)" startWordPosition="873" endWordPosition="876">ctions (Section 4). This classifier is trained from automatically generated alignments between the gold AMR trees and their associated sentences (Section 5), using an objective which favors alignment mistakes which are least harmful to the NER++ component. Finally, the concept subgraphs are combined into a coherent AMR parse using the maximum spanning connected subgraph algorithm of Flanigan et al. (2014). We show that our approach provides a large boost to recall over previous approaches, and that end to end performance is improved from 59 to 62 smatch (an F1 measure of correct AMR arcs; see Cai and Knight (2013)) when incorporated into the SRL++ parser of Flanigan et al. (2014). When evaluating the performance of our action classifier in isolation, we obtain an action classification accuracy of 84.1%. 2 The AMR Formalism AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts. AMR is based on neo-Davidsonian semantics, (Davidson, 1967; Parsons, 1990). The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment is often ge</context>
<context position="29593" citStr="Cai and Knight, 2013" startWordPosition="5039" endWordPosition="5042">asets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consisting primarily of newswire. We compare two systems: the JAMR parser (Flanigan et al., 2014),2 and the JAMR SRL++ component with our NER++ approach. AMR parsing accuracy is measured with a metric called smatch (Cai and Knight, 2013), which stands for “s(emantic) match.” The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph – that is, the set of (parent, edge, child) triples in the graph. Our results are given in Table 3. We report much higher recall numbers on both datasets, with only small (≤ 1 point) loss in precision. This is natural considering our approach. A better NER++ system allows for more correct AMR subgraphs to be generated – improving recall – but does not in itself necessarily improve the accuracy of the SRL++ system it is integrated in. 7.2 Compo</context>
</contexts>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In ACL (2), pages 748–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Teruko Mitamura</author>
<author>Eric H Nyberg</author>
</authors>
<title>The KANT perspective: A critique of pure transfer (and pure interlingua, pure statistics,...).</title>
<date>1999</date>
<contexts>
<context position="28502" citStr="Carbonell et al., 1999" startWordPosition="4856" endWordPosition="4859"> explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 Results We present improvements in end-to-end AMR parsing on two datasets using our NER++ component. Action type classifier accuracy on an automatically aligned corpus and alignment accuracy on a small hand-labeled corpus are also reported. Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, w</context>
</contexts>
<marker>Carbonell, Mitamura, Nyberg, 1999</marker>
<rawString>Jaime G Carbonell, Teruko Mitamura, and Eric H Nyberg. 1999. The KANT perspective: A critique of pure transfer (and pure interlingua, pure statistics,...).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel Chang</author>
<author>Chris Manning</author>
</authors>
<title>SUTIME: a library for recognizing and normalizing time expressions. In Language Resources and Evaluation.</title>
<date>2012</date>
<contexts>
<context position="15247" citStr="Chang and Manning, 2012" startWordPosition="2566" endWordPosition="2569">a created name node to the top of a span. PERSON A variant of the NAME action, this action produces a subgraph identical to the NAME action, but adds a node person as a parent. This is, in effect, a name node with an implicit entity type of person. Due to discrepancies between the output of our named entity tagger and the richer AMR named entity ontology, we only apply this tag to the person named entity tag. DATE The most frequent of the structured data type in the data, after name, is the date-entity construction (for an example see Figure 4). We deterministically take the output of SUTime (Chang and Manning, 2012) and convert it into the dateentity AMR representation. DICT This class serves as a back-off for the other classes, implementing an approach similar to Flanigan et al. (2014). In particular, we memorize a simple mapping from spans of text (such as sailor) to their corresponding most frequently aligned AMR subgraphs in the training data (i.e., the graph in Figure 3). See Section 5 for details on the alignment process. At test time we can do a lookup in this dictionary for any element that gets labeled with a DICT action. If an entry is not found in the mapping, we back off to the second most pr</context>
<context position="27275" citStr="Chang and Manning, 2012" startWordPosition="4662" endWordPosition="4665">ust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has</context>
</contexts>
<marker>Chang, Manning, 2012</marker>
<rawString>Angel Chang and Chris Manning. 2012. SUTIME: a library for recognizing and normalizing time expressions. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andr´e FT Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Framesemantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="27443" citStr="Das et al., 2014" startWordPosition="4691" endWordPosition="4694"> the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied se</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, Andr´e FT Martins, Nathan Schneider, and Noah A Smith. 2014. Framesemantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>The Logic of Decision and Action,</booktitle>
<pages>81--120</pages>
<editor>In Nicholas Rescher, editor,</editor>
<publisher>University of Pittsburgh Press,</publisher>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="5740" citStr="Davidson, 1967" startWordPosition="943" endWordPosition="944">e boost to recall over previous approaches, and that end to end performance is improved from 59 to 62 smatch (an F1 measure of correct AMR arcs; see Cai and Knight (2013)) when incorporated into the SRL++ parser of Flanigan et al. (2014). When evaluating the performance of our action classifier in isolation, we obtain an action classification accuracy of 84.1%. 2 The AMR Formalism AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts. AMR is based on neo-Davidsonian semantics, (Davidson, 1967; Parsons, 1990). The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment is often generated to train AMR parsers, it is not provided in the training corpora. The semantics of nodes can represent lexical items (e.g., dog), sense tagged lexical items (e.g., run-01), type markers (e.g., date-entity), and a host of other phenomena. The edges (relationships) in AMR describe one of a number of semantic relationships between concepts. The most salient of these is semantic role labels, such as the ARG0 and destination arcs in Figu</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Donald Davidson. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action, pages 81–120. University of Pittsburgh Press, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26983" citStr="Finkel et al., 2005" startWordPosition="4618" endWordPosition="4621">ation identifications (SRL++) component. Wang et al. (2015) uses a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1640" citStr="Flanigan et al., 2014" startWordPosition="250" endWordPosition="253">presentation (AMR) (Banarescu et al., 2013) is a rich, graph-based language for expressing semantics over a broad domain. The formalism is backed by a large datalabeling effort, and it holds promise for enabling a new breed of natural language applications ranging from semantically aware MT to rich broaddomain QA over text-based knowledge bases. Figure 1 shows an example AMR for “he gleefully ran to his dog Rover,” and we give a brief introduction to AMR in Section 2. This paper focuses on AMR parsing, the task of mapping a natural language sentence into an AMR graph. We follow previous work (Flanigan et al., 2014) in dividing AMR parsing into two steps. The first step is concept identification, which generates AMR nodes from text, and which we’ll refer to as NER++ (Section 3.1). The second step is relation Figure 1: The AMR graph for He gleefully ran to his dog Rover. We show that improving the generation of low level subgraphs (e.g., Rover generating name :op1 −−→ “Rover”) significantly improves end-to-end performance. identification, which adds arcs to link these nodes into a fully connected AMR graph, which we’ll call SRL++ (Section 3.2). We observe that SRL++ is not the hard part of AMR parsing; ra</context>
<context position="5083" citStr="Flanigan et al. (2014)" startWordPosition="834" endWordPosition="837">he sentence. Our approach to the end-to-end AMR parsing task is therefore as follows: we define an action space for generating AMR concepts, and create a classifier for classifying lexical items into one of these actions (Section 4). This classifier is trained from automatically generated alignments between the gold AMR trees and their associated sentences (Section 5), using an objective which favors alignment mistakes which are least harmful to the NER++ component. Finally, the concept subgraphs are combined into a coherent AMR parse using the maximum spanning connected subgraph algorithm of Flanigan et al. (2014). We show that our approach provides a large boost to recall over previous approaches, and that end to end performance is improved from 59 to 62 smatch (an F1 measure of correct AMR arcs; see Cai and Knight (2013)) when incorporated into the SRL++ parser of Flanigan et al. (2014). When evaluating the performance of our action classifier in isolation, we obtain an action classification accuracy of 84.1%. 2 The AMR Formalism AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts. A</context>
<context position="15421" citStr="Flanigan et al. (2014)" startWordPosition="2595" endWordPosition="2598"> This is, in effect, a name node with an implicit entity type of person. Due to discrepancies between the output of our named entity tagger and the richer AMR named entity ontology, we only apply this tag to the person named entity tag. DATE The most frequent of the structured data type in the data, after name, is the date-entity construction (for an example see Figure 4). We deterministically take the output of SUTime (Chang and Manning, 2012) and convert it into the dateentity AMR representation. DICT This class serves as a back-off for the other classes, implementing an approach similar to Flanigan et al. (2014). In particular, we memorize a simple mapping from spans of text (such as sailor) to their corresponding most frequently aligned AMR subgraphs in the training data (i.e., the graph in Figure 3). See Section 5 for details on the alignment process. At test time we can do a lookup in this dictionary for any element that gets labeled with a DICT action. If an entry is not found in the mapping, we back off to the second most probable class proposed by the classifier. It is worth observing at this point that our actions derive much of their power from the similarity between English words and their A</context>
<context position="18324" citStr="Flanigan et al. (2014)" startWordPosition="3093" endWordPosition="3096">chy, as they are more likely to produce the correct subgraph. This hierarchy is useful in resolving ambiguity throughout our system. During the creation of training data for our classifier (Section 4.3) from our aligner, when two actions could both generate the aligned AMR node we prefer the more reliable one. In turn, in our aligner we bias alignments towards those which generating more reliable action sequences as training data (see Section 5). The primary benefit of this action-based NER++ approach is that we can reduce the usage of low reliability actions, like DICT. The approach taken in Flanigan et al. (2014) can be 986 Action # Tokens % Total NONE 41538 36.2 DICT 30027 26.1 IDENTITY 19034 16.6 VERB 11739 10.2 LEMMA 5029 4.5 NAME 4537 3.9 DATE 1418 1.1 PERSON 1336 1.1 VALUE 122 0.1 Table 1: Distribution of action types in the proxy section of the newswire section of the LDC2014T12 dataset, generated from automatically aligned data. Input token; word embedding Left+right token / bigram Token length indicator Token starts with “non” POS; Left+right POS / bigram Dependency parent token / POS Incoming dependency arc Bag of outgoing dependency arcs Number of outgoing dependency arcs Max Jaro-Winkler to</context>
<context position="25748" citStr="Flanigan et al. (2014)" startWordPosition="4414" endWordPosition="4417">ignment that would consume the token using another action, the optimization prefers that alignment. The Jaro-Winkler similarity term, in turn, serves as a tie-breaker between equally (un)reliable alignments. There are many packages which can solve this Boolean LP efficiently. We used Gurobi (Gurobi Optimization, 2015). Given a matrix Q that maximizes our objective, we can decode our solved alignment as follows: for each i, align ni to the j s.t. Qi,j = 1. By our constraints, exactly one such j must exist. 6 Related Work Prior work in AMR and related formalisms include Jones et al. (2012), and Flanigan et al. (2014). Jones et al. (2012), motivated by applications in Machine Translation, proposed a graphical semantic meaning representation that predates AMR, but is intimately related. They propose a hyper-edge replacement grammar (HRG) approach to parsing into and out of this graphical semantic form. Flanigan et al. (2014) forms the basis of the approach of this paper. Their system introduces the two-stage approach we use: they implement a rule-based alignment to learn a mapping from tokens to subgraphs, and train a variant of a maximum spanning tree parser adapted to graphs and with additional constraint</context>
<context position="29334" citStr="Flanigan et al. (2014)" startWordPosition="4996" endWordPosition="4999">resent improvements in end-to-end AMR parsing on two datasets using our NER++ component. Action type classifier accuracy on an automatically aligned corpus and alignment accuracy on a small hand-labeled corpus are also reported. Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consisting primarily of newswire. We compare two systems: the JAMR parser (Flanigan et al., 2014),2 and the JAMR SRL++ component with our NER++ approach. AMR parsing accuracy is measured with a metric called smatch (Cai and Knight, 2013), which stands for “s(emantic) match.” The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph – that is, the set of (parent, edge, child) triples in the graph. Our results are given in Table 3. We report much higher recall numbers on both datasets, with only small (≤ 1 point) loss </context>
<context position="31519" citStr="Flanigan et al., 2014" startWordPosition="5357" endWordPosition="5360">9 59.0 62.3 989 precision/recall of prior work on alignments, and is based on both a different alignment dataset and subtly different alignment annotation scheme. In particular, we require that every AMR node aligns to some token in the sentence, which forces the system to always align nodes, even when unsure. A standard semantics and annotation guideline for AMR alignment is left for future work; our accuracy should be considered only an informal metric. We find our informativeness-based alignment objective slightly improves end-to-end performance when compared to the rule-based approach of (Flanigan et al., 2014), improving F1 by roughly 1 point (64/59/61 P/R/F1 to 65/59/62 P/R/F1). On the automatic alignments over the LDC2014T12 corpus, our action classifier achieved a test accuracy of 0.841. The classifier’s most common class of mistakes are incorrect DICT classifications. It is reassuring that some of these errors can be recovered from by the naive dictionary lookup finding the correct mapping. The DICT action lookup table achieved an accuracy of 0.67. This is particularly impressive given that our model moves many of the difficult semantic tasks onto the DICT tag, and that this lookup does not mak</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="27383" citStr="Gildea and Jurafsky, 2002" startWordPosition="4680" endWordPosition="4683">r work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Co</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<date>2015</date>
<note>Gurobi optimizer reference manual.</note>
<contexts>
<context position="25445" citStr="Optimization, 2015" startWordPosition="4358" endWordPosition="4359">nments which map to the unreliable DICT tag, while rewarding alignments with high overlap between the title of the node and the token. Note that most incorrect alignments fall into the DICT class by default, as no other action could generate the correct AMR subgraph. Therefore, if there exists an alignment that would consume the token using another action, the optimization prefers that alignment. The Jaro-Winkler similarity term, in turn, serves as a tie-breaker between equally (un)reliable alignments. There are many packages which can solve this Boolean LP efficiently. We used Gurobi (Gurobi Optimization, 2015). Given a matrix Q that maximizes our objective, we can decode our solved alignment as follows: for each i, align ni to the j s.t. Qi,j = 1. By our constraints, exactly one such j must exist. 6 Related Work Prior work in AMR and related formalisms include Jones et al. (2012), and Flanigan et al. (2014). Jones et al. (2012), motivated by applications in Machine Translation, proposed a graphical semantic meaning representation that predates AMR, but is intimately related. They propose a hyper-edge replacement grammar (HRG) approach to parsing into and out of this graphical semantic form. Flaniga</context>
</contexts>
<marker>Optimization, 2015</marker>
<rawString>Inc. Gurobi Optimization. 2015. Gurobi optimizer reference manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Javad Hosseini</author>
<author>Hannaneh Hajishirzi</author>
<author>Oren Etzioni</author>
<author>Nate Kushman</author>
</authors>
<title>Learning to solve arithmetic word problems with verb categorization.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28254" citStr="Hosseini et al. (2014)" startWordPosition="4820" endWordPosition="4823">compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 Results We present improvements in end-to-end AMR parsing on two datasets using our NER++ component. Action type classifier accuracy on an automatically a</context>
</contexts>
<marker>Hosseini, Hajishirzi, Etzioni, Kushman, 2014</marker>
<rawString>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<title>Semantics-based machine translation with hyperedge replacement grammars.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1359--1376</pages>
<contexts>
<context position="25720" citStr="Jones et al. (2012)" startWordPosition="4409" endWordPosition="4412">re, if there exists an alignment that would consume the token using another action, the optimization prefers that alignment. The Jaro-Winkler similarity term, in turn, serves as a tie-breaker between equally (un)reliable alignments. There are many packages which can solve this Boolean LP efficiently. We used Gurobi (Gurobi Optimization, 2015). Given a matrix Q that maximizes our objective, we can decode our solved alignment as follows: for each i, align ni to the j s.t. Qi,j = 1. By our constraints, exactly one such j must exist. 6 Related Work Prior work in AMR and related formalisms include Jones et al. (2012), and Flanigan et al. (2014). Jones et al. (2012), motivated by applications in Machine Translation, proposed a graphical semantic meaning representation that predates AMR, but is intimately related. They propose a hyper-edge replacement grammar (HRG) approach to parsing into and out of this graphical semantic form. Flanigan et al. (2014) forms the basis of the approach of this paper. Their system introduces the two-stage approach we use: they implement a rule-based alignment to learn a mapping from tokens to subgraphs, and train a variant of a maximum spanning tree parser adapted to graphs an</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In COLING, pages 1359–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages. In AAAI,</title>
<date>2005</date>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="27963" citStr="Kate et al., 2005" startWordPosition="4776" endWordPosition="4779">e labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoup</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In AAAI, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori S Levin</author>
<author>Donna Gates</author>
<author>Alon Lavie</author>
<author>Alex Waibel</author>
</authors>
<title>An interlingua based on domain actions for machine translation of task-oriented dialogues.</title>
<date>1998</date>
<booktitle>In ICSLP,</booktitle>
<volume>98</volume>
<pages>1155--1158</pages>
<contexts>
<context position="28523" citStr="Levin et al., 1998" startWordPosition="4860" endWordPosition="4863">hen the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 Results We present improvements in end-to-end AMR parsing on two datasets using our NER++ component. Action type classifier accuracy on an automatically aligned corpus and alignment accuracy on a small hand-labeled corpus are also reported. Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss</context>
</contexts>
<marker>Levin, Gates, Lavie, Waibel, 1998</marker>
<rawString>Lori S Levin, Donna Gates, Alon Lavie, and Alex Waibel. 1998. An interlingua based on domain actions for machine translation of task-oriented dialogues. In ICSLP, volume 98, pages 1155–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28015" citStr="Liang et al., 2011" startWordPosition="4784" endWordPosition="4787">et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surf</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27787" citStr="McDonald et al., 2005" startWordPosition="4746" endWordPosition="4749"> normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attemp</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
<author>Eric H Nyberg</author>
<author>Jaime G Carbonell</author>
</authors>
<title>An efficient interlingua translation system for multi-lingual document production.</title>
<date>1991</date>
<booktitle>Proceedings of Machine Translation Summit III.</booktitle>
<contexts>
<context position="28478" citStr="Mitamura et al., 1991" startWordPosition="4852" endWordPosition="4855">resentation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 Results We present improvements in end-to-end AMR parsing on two datasets using our NER++ component. Action type classifier accuracy on an automatically aligned corpus and alignment accuracy on a small hand-labeled corpus are also reported. Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher</context>
</contexts>
<marker>Mitamura, Nyberg, Carbonell, 1991</marker>
<rawString>Teruko Mitamura, Eric H Nyberg, and Jaime G Carbonell. 1991. An efficient interlingua translation system for multi-lingual document production. Proceedings of Machine Translation Summit III.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="26961" citStr="Nadeau and Sekine, 2007" startWordPosition="4614" endWordPosition="4617">constraints for their relation identifications (SRL++) component. Wang et al. (2015) uses a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntacti</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="7274" citStr="Palmer et al., 2005" startWordPosition="1201" endWordPosition="1204">nt structured data by creating additional nodes such as date-entity to signify the presence of special structure. a semantics more akin to syntactic dependencies (e.g., mod standing in for adjective and adverbial modification), or take on domain-specific meaning (e.g., the month, day, and year arcs of a dateentity). To introduce AMR and its notation in more detail, we’ll unpack the translation of the sentence “he gleefully ran to his dog Rover.” We show in Figure 1 the interpretation of this sentence as an AMR graph. The root node of the graph is labeled run-01, corresponding to the PropBank (Palmer et al., 2005) definition of the verb ran. run-01 has an outgoing ARG0 arc to a node he, with the usual PropBank semantics. The outgoing mod edge from run-01 to glee takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent. We note that run-01 has a destination arc to dog. The label for destination is taken from a finite set of special arc sense tags similar to the preposition senses found in (Srikumar, 2013). The last portion of the figure parses dog to a node which serves as a type marker similar to named entity types, and Rover into t</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Parsons</author>
</authors>
<title>Events in the Semantics of English: A study in subatomic semantics.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5756" citStr="Parsons, 1990" startWordPosition="945" endWordPosition="946">l over previous approaches, and that end to end performance is improved from 59 to 62 smatch (an F1 measure of correct AMR arcs; see Cai and Knight (2013)) when incorporated into the SRL++ parser of Flanigan et al. (2014). When evaluating the performance of our action classifier in isolation, we obtain an action classification accuracy of 84.1%. 2 The AMR Formalism AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts. AMR is based on neo-Davidsonian semantics, (Davidson, 1967; Parsons, 1990). The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment is often generated to train AMR parsers, it is not provided in the training corpora. The semantics of nodes can represent lexical items (e.g., dog), sense tagged lexical items (e.g., run-01), type markers (e.g., date-entity), and a host of other phenomena. The edges (relationships) in AMR describe one of a number of semantic relationships between concepts. The most salient of these is semantic role labels, such as the ARG0 and destination arcs in Figure 2. However, o</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Terence Parsons. 1990. Events in the Semantics of English: A study in subatomic semantics. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nima Pourdamghani</author>
<author>Yang Gao</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
</authors>
<title>Aligning english strings with abstract meaning representation graphs.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="22539" citStr="Pourdamghani et al. (2014)" startWordPosition="3848" endWordPosition="3851">h should align to multiple tokens, we generate an alignment from the subgraph’s nodes to the associated tokens in the sentence. It is empirically very rare for a subgraph to have more nodes than the token span it should align to. There have been two previous attempts at producing automatic AMR alignments. The first was 1A sequence model was tried and showed no improvement over a simple maxent classifier. 987 published as a component of JAMR, and used a rule-based approach to perform alignments. This was shown to work well on the sample of 100 hand-labeled sentences used to develop the system. Pourdamghani et al. (2014) approached the alignment problem in the framework of the IBM alignment models. They rendered AMR graphs as text, and then used traditional machine translation alignment techniques to generate an alignment. We propose a novel alignment method, since our decomposition of the AMR node generation process into a set of actions provides an additional objective for the aligner to optimize, in addition to the accuracy of the alignment itself. We would like to produce the most reliable sequence of actions for the NER++ model to train from, where reliable is taken in the sense defined in Section 4.2. T</context>
</contexts>
<marker>Pourdamghani, Gao, Hermjakob, Knight, 2014</marker>
<rawString>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning english strings with abstract meaning representation graphs. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1346</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="27408" citStr="Punyakanok et al., 2004" startWordPosition="4684" endWordPosition="4687">i (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al.</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
</authors>
<title>The semantics of role labeling.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at UrbanaChampaign.</institution>
<contexts>
<context position="7743" citStr="Srikumar, 2013" startWordPosition="1286" endWordPosition="1287">interpretation of this sentence as an AMR graph. The root node of the graph is labeled run-01, corresponding to the PropBank (Palmer et al., 2005) definition of the verb ran. run-01 has an outgoing ARG0 arc to a node he, with the usual PropBank semantics. The outgoing mod edge from run-01 to glee takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent. We note that run-01 has a destination arc to dog. The label for destination is taken from a finite set of special arc sense tags similar to the preposition senses found in (Srikumar, 2013). The last portion of the figure parses dog to a node which serves as a type marker similar to named entity types, and Rover into the larger subgraph indicating a concept with name “Rover.” 2.1 AMR Subgraphs The mapping from tokens of a sentence to AMR nodes is not one-to-one. A single token or span of tokens can generate a subgraph of AMR consisting of multiple nodes. These subgraphs can logically be considered the expression of a single concept, and are useful to treat as such (e.g., see Section 3.1). Many of these multi-node subgraphs capture structured data such as time expressions, as in </context>
<context position="27424" citStr="Srikumar, 2013" startWordPosition="4688" endWordPosition="4690">4), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent </context>
</contexts>
<marker>Srikumar, 2013</marker>
<rawString>Vivek Srikumar. 2013. The semantics of role labeling. Ph.D. thesis, University of Illinois at UrbanaChampaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannik Str¨otgen</author>
<author>Michael Gertz</author>
</authors>
<title>Heideltime: High quality rule-based extraction and normalization of temporal expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval.</booktitle>
<marker>Str¨otgen, Gertz, 2010</marker>
<rawString>Jannik Str¨otgen and Michael Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="27222" citStr="Verhagen et al., 2010" startWordPosition="4653" endWordPosition="4657">on action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More g</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan Wang</author>
<author>Nianwen Xue</author>
<author>Sameer Pradhan</author>
</authors>
<title>A transition-based algorithm for amr parsing.</title>
<date>2015</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="26422" citStr="Wang et al. (2015)" startWordPosition="4522" endWordPosition="4525">ine Translation, proposed a graphical semantic meaning representation that predates AMR, but is intimately related. They propose a hyper-edge replacement grammar (HRG) approach to parsing into and out of this graphical semantic form. Flanigan et al. (2014) forms the basis of the approach of this paper. Their system introduces the two-stage approach we use: they implement a rule-based alignment to learn a mapping from tokens to subgraphs, and train a variant of a maximum spanning tree parser adapted to graphs and with additional constraints for their relation identifications (SRL++) component. Wang et al. (2015) uses a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, </context>
</contexts>
<marker>Wang, Xue, Pradhan, 2015</marker>
<rawString>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for amr parsing. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="27026" citStr="Yarowsky, 1995" startWordPosition="4625" endWordPosition="4626">al. (2015) uses a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with FlaniQi,j = 1 bi (2) 988 gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, o</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.</title>
<date>2005</date>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="27994" citStr="Zettlemoyer and Collins, 2005" startWordPosition="4780" endWordPosition="4783">and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR l</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>