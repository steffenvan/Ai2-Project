<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098280">
<title confidence="0.991897">
Detecting Pitch Accents at the Word, Syllable and Vowel Level
</title>
<author confidence="0.999079">
Andrew Rosenberg Julia Hirschberg
</author>
<affiliation confidence="0.999814">
Columbia University Columbia University
Department of Computer Science Department of Computer Science
</affiliation>
<email confidence="0.999435">
amaxwell@cs.columbia.edu julia@cs.columbia.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806133333333">
The automatic identification of prosodic
events such as pitch accent in English has long
been a topic of interest to speech researchers,
with applications to a variety of spoken lan-
guage processing tasks. However, much re-
mains to be understood about the best meth-
ods for obtaining high accuracy detection. We
describe experiments examining the optimal
domain for accent analysis. Specifically, we
compare pitch accent identification at the syl-
lable, vowel or word level as domains for anal-
ysis of acoustic indicators of accent. Our re-
sults indicate that a word-based approach is
superior to syllable- or vowel-based detection,
achieving an accuracy of 84.2%.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995870625">
Prosody in a language like Standard American En-
glish can be used by speakers to convey semantic,
pragmatic and paralinguistic information. Words are
made intonationall prominent, or accented to convey
information such as contrast, focus, topic, and in-
formation status. The communicative implications
of accenting influence the interpretation of a word
or phrase. However, the acoustic excursions associ-
ated with accent are typically aligned with the lex-
ically stressed syllable of the accented word. This
disparity between the domains of acoustic proper-
ties and communicative impact has led to different
approaches to pitch accent detection, and to the use
of different domains of analysis.
In this paper, we compare automatic pitch accent
detection at the vowel, syllable, and word level to
</bodyText>
<page confidence="0.978753">
81
</page>
<bodyText confidence="0.9999931">
determine which approach is optimal. While lex-
ical and syntactic information has been shown to
contribute to the detection of pitch accent, we only
explore acoustic features. This decision allows us
to closely examine the indicators of accent that are
present in the speech signal in isolation from lin-
guistic effects that may indicate that a word or syl-
lable may be accented. The choice of domain for
automatic pitch accent prediction it also related to
how that prediction is to be used and impacts how
it can be evaluated in comparison with other re-
search efforts. While some downstream spoken lan-
guage processing tasks benefit by knowing which
syllable in a word is accented, such as clarifica-
tion of communication misunderstandings, such as
“I said unlock the door – not lock it!”, most appli-
cations care only about which word is intonation-
ally prominent. For the identification of contrast,
given/new status, or focus, only word-level informa-
tion is required. While the performance of nucleus-
or syllable-based predictions can be translated to
word predictions, such a translation is rarely per-
formed, making it difficult to compare performance
and thus determine which approach is best.
In this paper, we describe experiments in pitch ac-
cent detection comparing the use of vowel nuclei,
syllables and words as units of analysis. In Section
2, we discuss related work. We describe the ma-
terials in Section 3, the experiments themselves in
Section 4 and conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9963">
Acoustic-based approaches to pitch accent detection
have explored prediction at the word, syllable, and
</bodyText>
<subsubsectionHeader confidence="0.833111">
Proceedings of NAACL HLT 2009: Short Papers, pages 81–84,
</subsubsectionHeader>
<bodyText confidence="0.991058395348837">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
vowel level, but have rarely compared prediction
accuracies across these different domains. An ex-
ception is the work of Ross and Ostendorf (1996),
who detect accent on the Boston University Radio
News Corpus (BURNC) at both the syllable and
word level. Using CART predictions as input to an
HMM, they detect pitch accents on syllables spoken
by a single speaker from BURNC with 87.7% accu-
racy, corresponding to 82.5% word-based accuracy,
using both lexical and acoustic features. In compar-
ing the discriminative usefulness of syllables vs. syl-
lable nuclei for accent detection, Tamburini (2003)
finds syllable nuclei (vowel) duration to be as useful
to full syllables. Rosenberg and Hirschberg (2007)
used an energy-based ensemble technique to detect
pitch accents with 84.1% accuracy on the read por-
tion of the Boston Directions Corpus, without us-
ing lexical information. Sridhar et al. (2008) ob-
tain 86.0% word-based accuracy using maximum
entropy models from acoustic and syntactic infor-
mation on the BURNC. Syllable-based detection
by Ananthakrishnan and Narayanan (2008) com-
bines acoustic, lexical and syntactic FSM models
to achieve a detection rate of 86.75%. Similar
suprasegmental features have also been explored in
work at SRI/ICSI which employs a hidden event
model to model intonational information for a va-
riety of tasks including punctuation and disfluency
detection (Baron et al., 2002). However, while
progress has been made in accent detection perfor-
mance in the past 15 years, with both word and syl-
lable accuracy at about 86%, these accuracies have
been achieved with different methods and some have
included lexico-syntactic as well as acoustic fea-
tures. It is still not clear which domain of acoustic
analysis provides the most accurate cues for accent
prediction. To address this issue, our work compares
accent detection at the syllable nucleus, full syllable,
and word levels, using a common modeling tech-
nique and a common corpus, to focus on the ques-
tion of which domain of acoustic analysis is most
useful for pitch accent prediction.
</bodyText>
<sectionHeader confidence="0.958856" genericHeader="method">
3 Boston University Radio News Corpus
</sectionHeader>
<bodyText confidence="0.999951838709678">
Our experiments use 157.9 minutes (29,578 words)
from six speakers in the BURNC (Ostendorf et al.,
1995) recordings of professionally read radio news.
This corpus has been prosodically annotated with
full ToBI labeling (Silverman et al., 1992), includ-
ing the presence and type of accents; these are an-
notated at the syllable level and 54.7% (16,178) of
words are accented. Time-aligned phone boundaries
generated by forced alignment are used to identify
vowel regions for analysis. There are 48,359 vow-
els in the corpus and 34.8 of these are accented. To
generate time-aligned syllable boundaries, we align
the forced-aligned phones with a syllabified lexicon
included with the corpus.
The use of BURNC for comparative accent pre-
diction in our three domains is not straightforward,
due to anomalies in the corpus. First, the lexicon
and forced-alignment output in BURNC use distinct
phonetic inventories; to align these, we have em-
ployed a minimum edit distance procedure where
aligning any two vowels incurs zero cost. This guar-
antees that, at a minimum the vowels will be aligned
correctly. Also, the number of syllables per word
in the lexicon does not always match the number
of vowels in the forced alignment. This leads to
114 syllables containing two forced-aligned vowels,
and 8 containing none. Instead of performing post
hoc correction of the syllabification results, we in-
clude all of the automatically identified syllables in
the data set. This syllabification approach generates
48,253 syllables, 16,781 (34.8%) bearing accent.
</bodyText>
<sectionHeader confidence="0.992678" genericHeader="method">
4 Pitch Accent Detection Experiments
</sectionHeader>
<bodyText confidence="0.999921933333333">
We train logistic regression models to detect the
presence of pitch accent using acoustic features
drawn from each word, syllable and vowel, using
Weka (Witten et al., 1999). The features we use in-
cluded pitch (f0), energy and duration, which have
been shown to correlate with pitch accent in En-
glish. To model these, we calculate pitch and en-
ergy contours for each token using Praat (Boersma,
2001). Duration information is derived using the
vowel, syllable or word segmentation described in
Section 3. The feature vectors we construct include
features derived from both raw and speaker z-score
normalized1 pitch and energy contours. The feature
vector used in all three analysis scenarios is com-
prised of minimum, maximum, mean, standard de-
</bodyText>
<footnote confidence="0.523677">
1Z-score normalization:xnorm _ x−µ
σ , where x is a value
to normalize, µ and σ are mean and standard deviation. These
are estimated from all pitch or intensity values for a speaker.
</footnote>
<page confidence="0.996285">
82
</page>
<bodyText confidence="0.999780642857143">
viation and the z-score of the maximum of these raw
and normalized acoustic contours. The duration of
the region in seconds is also included.
The results of ten-fold cross validation classifica-
tion experiments are shown in Table 1. Note that,
when running ten-fold cross validation on syllables
and vowels, we divide the folds by words, so that
each syllable within a word is a member of the
same fold. To allow for direct comparison of the
three approaches, we generate word-based results
from vowel- and syllable-based experiments. If any
syllable or vowel in a word is hypothesized as ac-
cented, the containing word is predicted to be ac-
cented. Vowel/syllable accuracies should be higher
</bodyText>
<table confidence="0.9992885">
Region Accuracy (%) F-Measure
Vowel 68.5 f 0.319 0.651 f 0.00329
Syllable 75.6 f 0.125 0.756 f 0.00188
Word 82.9 f 0.168 0.845 f 0.00162
</table>
<tableCaption confidence="0.999687">
Table 1: Word-level accuracy and F-Measure
</tableCaption>
<bodyText confidence="0.99997725">
than word-based accuracies since the baseline is sig-
nificantly higher. However, we find that the F-
measure for detecting accent is consistently higher
for word-based results. A prediction of accented
on any component syllable is sufficient to generate
a correct word prediction.
Our results suggest, first of all, that there is dis-
criminative information beyond the syllable nucleus.
Syllable-based classification is significantly better
than vowel-based classification, whether we com-
pare accuracy or F-measure. It is possible that the
narrow region of analysis offered by syllable and
vowel-based analysis makes the aggregated features
more susceptible to the effects of noise. Moreover,
errors in the forced-alignment phone boundaries
and syllabification may negatively impact the per-
formance of vowel- and syllable-based approaches.
Until automatic phone alignment improves, word-
based prediction appears to be more reliable. An
automatic, acoustic syllable-nucleus detection ap-
proach may be able generate more discriminative re-
gions of analysis for pitch accent detection than the
forced-alignment and lexicon alignment technique
used here. This remains an area for future study.
However, if we accept that the feature represen-
tations accurately model the acoustic information
contained in the regions of analysis and that the
BURNC annotation is accurate, the most likely ex-
planation for the superiority of word-based predic-
tion over syllable- or vowel-based strategiesis is that
the acoustic excursions correlated with accent occur
outside a word’s lexically stressed syllable. In par-
ticular, complex pitch accents in English are gener-
ally realized on multiple syllables. To examine this
possibility, we looked at the distribution of misses
from the three classification scenarios. The distribu-
tion of pitch accent types of missed detections using
evaluation of the three scenarios is shown in Table
2. In the ToBI framework, the complex pitch ac-
cents include L+H*, L*+H, H+!H* and their down-
stepped variants. As we suspected, larger units of
analysis lead to improved performance on complex
tones; x2 analysis of the difference between the er-
ror distributions yields a x2 of 42.108, p&lt; 0.0001.
Since accenting is the perception of a word as
more prominent than surrounding words, features
that incorporate local contextual acoustic informa-
tion should improve detection accuracy at all lev-
els. To represent surrounding acoustic context in
feature vectors, we calculate the z-score of the max-
imum and mean pitch and energy over six regions.
Three of these are “short range” regions: one pre-
vious region, one following region, and both the
previous and following region. The other three are
“long range” regions. For words, these regions
are defined as two previous words, two following
words, and both two previous and two following
words. To give syllable- and vowel-based classifi-
cation scenarios access to a comparable amount of
acoustic context, the “long range” regions covered
ranges of three syllables or vowels. There are ap-
proximately 1.63 syllables/vowels per word in the
BURNC corpus; thus, on balance, a window of two
words is equivalent to one of three syllables. Du-
ration is also normalized relative to the duration of
regions within the contextual regions. Accuracy and
f-measure results from ten-fold cross validation ex-
periments are shown in Table 3. We find dramatic
</bodyText>
<table confidence="0.998224">
Analysis Region Accuracy (%) F-Measure
Vowel 77.4 f 0.264 0.774 f 0.00370
Syllable 81.9 f 0.197 0.829 f 0.00195
Word 84.2 f 0.247 0.858 f 0.00276
</table>
<tableCaption confidence="0.982534">
Table 3: Word-level accuracy and F-Measure with Con-
textual Features
</tableCaption>
<bodyText confidence="0.527442">
increases in the performance of vowel- and syllable-
</bodyText>
<page confidence="0.985785">
83
</page>
<table confidence="0.99898725">
Region H* L* Complex Total Misses
Vowel .6825 (3732) .0686 (375) .2489 (1361) 1.0 (5468)
Syllable .7033 (2422) .0851 (293) .2117 (729) 1.0 (3444)
Word .7422 (2002) .0610 (165) .1986 (537) 1.0 (2704)
</table>
<tableCaption confidence="0.994983">
Table 2: Distribution of missed detections organized by H*, L* and complex pitch accents.
</tableCaption>
<bodyText confidence="0.999678">
based performance when we include contextual fea-
tures. Vowel-based classification shows nearly 10%
absolute increase accuracy when translated to the
word level. The improvements in word-based clas-
sification, however, are less dramatic. It may be
that word-based analysis already incorporates much
the contextual information that is helpful for detect-
ing pitch accents. The feature representations in
each of these three experiments include a compara-
ble amount of acoustic context. This suggests that
the superiority of word-based detection is not sim-
ply due to the access to more contextual informa-
tion, but rather that there is discriminative informa-
tion outside the accent-bearing syllable.
</bodyText>
<sectionHeader confidence="0.99104" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989235294117">
In this paper, we describe experiments comparing
the detection of pitch accents on three acoustic do-
mains – words, syllables and vowels – using acous-
tic features alone. To permit direct comparison be-
tween accent prediction in these three domains of
analysis, we generate word-, syllable-, and vowel-
based results directly, and then transfer syllable- and
nucleus-based predictions to word predictions.
Our experiments show that word-based accent
detection significantly outperforms syllable- and
vowel-based approaches. Extracting features that
incorporate acoustic information from surrounding
context improves performance in all three domains.
We find that there is, in fact, acoustic information
discriminative to pitch accent that is found within
accented words, outside the accent-bearing sylla-
ble. We achieve 84.2% word-based accuracy —
significantly below the 86.0% reported by Sridhar
et al. (2008) using syntactic and acoustic compo-
nents. However, our experiments use only acoustic
features, since we are concerned with comparing do-
mains of acoustic analysis within the larger task of
accent identification. Our 84.2% accuracy is signifi-
cantly higher than the 80.09% accuracy obtained by
the 10ms frame-based acoustic modeling described
in (Sridhar et al., 2008). Our aggregations of pitch
and energy contours over a region of analysis appear
to be more helpful than short frame modeling.
In future work, we will explore a number of tech-
niques to transfer word based predictions to sylla-
bles. This will allow us to compare word-based de-
tection to published syllable-based results. Prelimi-
nary results suggest that word-based detection is su-
perior regardless of the domain of evaluation.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936405405405">
S. Ananthakrishnan and S. Narayanan. 2008. Auto-
matic prosodic event detection using acoustic, lexical
and syntactic evidence. IEEE Transactions on Audio,
Speech &amp; Language Processing, 16(1):216–228.
D. Baron, E. Shriberg, and A. Stolcke. 2002. Auto-
matic punctuation and disfluency detection in multi-
party meetings using prosodic and lexical cues. In IC-
SLP.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9-10):341–345.
M. Ostendorf, P. Price, and S. Shattuck-Hufnagel. 1995.
The boston university radio news corpus. Technical
Report ECS-95-001, Boston University, March.
A. Rosenberg and J. Hirschberg. 2007. Detecting pitch
accent using pitch-corrected energy-based predictors.
In Interspeech.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Computer
Speech &amp; Language, 10(3):155–185.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. Tobi: A standard for labeling en-
glish prosody. In Proc. of the 1992 International Con-
ference on Spoken Language Processing, volume 2,
pages 12–16.
V. R. Sridhar, S. Bangalore, and S. Narayanan. 2008.
Exploiting acoustic and syntactic features for prosody
labeling in a maximum entropy framework. IEEE
Transactions on Audio, Speech &amp; Language Process-
ing, 16(4):797–811.
F. Tamburini. 2003. Prosodic prominence detection in
speech. In ISSPA2003, pages 385–388.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In ICONIP/ANZIIS/ANNES International Work-
shop, pages 192–196.
</reference>
<page confidence="0.999245">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909556">
<title confidence="0.999544">Detecting Pitch Accents at the Word, Syllable and Vowel Level</title>
<author confidence="0.999361">Andrew Rosenberg Julia Hirschberg</author>
<affiliation confidence="0.999521">Columbia University Columbia University Department of Computer Science Department of Computer Science</affiliation>
<email confidence="0.998861">amaxwell@cs.columbia.edujulia@cs.columbia.edu</email>
<abstract confidence="0.994035875">The automatic identification of prosodic such as accent English has long been a topic of interest to speech researchers, with applications to a variety of spoken language processing tasks. However, much remains to be understood about the best methods for obtaining high accuracy detection. We describe experiments examining the optimal accent analysis. Specifically, we compare pitch accent identification at the syllable, vowel or word level as domains for analysis of acoustic indicators of accent. Our results indicate that a word-based approach is superior to syllableor vowel-based detection, achieving an accuracy of 84.2%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ananthakrishnan</author>
<author>S Narayanan</author>
</authors>
<title>Automatic prosodic event detection using acoustic, lexical and syntactic evidence.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="4535" citStr="Ananthakrishnan and Narayanan (2008)" startWordPosition="698" endWordPosition="701">sing both lexical and acoustic features. In comparing the discriminative usefulness of syllables vs. syllable nuclei for accent detection, Tamburini (2003) finds syllable nuclei (vowel) duration to be as useful to full syllables. Rosenberg and Hirschberg (2007) used an energy-based ensemble technique to detect pitch accents with 84.1% accuracy on the read portion of the Boston Directions Corpus, without using lexical information. Sridhar et al. (2008) obtain 86.0% word-based accuracy using maximum entropy models from acoustic and syntactic information on the BURNC. Syllable-based detection by Ananthakrishnan and Narayanan (2008) combines acoustic, lexical and syntactic FSM models to achieve a detection rate of 86.75%. Similar suprasegmental features have also been explored in work at SRI/ICSI which employs a hidden event model to model intonational information for a variety of tasks including punctuation and disfluency detection (Baron et al., 2002). However, while progress has been made in accent detection performance in the past 15 years, with both word and syllable accuracy at about 86%, these accuracies have been achieved with different methods and some have included lexico-syntactic as well as acoustic features.</context>
</contexts>
<marker>Ananthakrishnan, Narayanan, 2008</marker>
<rawString>S. Ananthakrishnan and S. Narayanan. 2008. Automatic prosodic event detection using acoustic, lexical and syntactic evidence. IEEE Transactions on Audio, Speech &amp; Language Processing, 16(1):216–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Baron</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Automatic punctuation and disfluency detection in multiparty meetings using prosodic and lexical cues.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="4862" citStr="Baron et al., 2002" startWordPosition="749" endWordPosition="752">y on the read portion of the Boston Directions Corpus, without using lexical information. Sridhar et al. (2008) obtain 86.0% word-based accuracy using maximum entropy models from acoustic and syntactic information on the BURNC. Syllable-based detection by Ananthakrishnan and Narayanan (2008) combines acoustic, lexical and syntactic FSM models to achieve a detection rate of 86.75%. Similar suprasegmental features have also been explored in work at SRI/ICSI which employs a hidden event model to model intonational information for a variety of tasks including punctuation and disfluency detection (Baron et al., 2002). However, while progress has been made in accent detection performance in the past 15 years, with both word and syllable accuracy at about 86%, these accuracies have been achieved with different methods and some have included lexico-syntactic as well as acoustic features. It is still not clear which domain of acoustic analysis provides the most accurate cues for accent prediction. To address this issue, our work compares accent detection at the syllable nucleus, full syllable, and word levels, using a common modeling technique and a common corpus, to focus on the question of which domain of a</context>
</contexts>
<marker>Baron, Shriberg, Stolcke, 2002</marker>
<rawString>D. Baron, E. Shriberg, and A. Stolcke. 2002. Automatic punctuation and disfluency detection in multiparty meetings using prosodic and lexical cues. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
</authors>
<title>Praat, a system for doing phonetics by computer.</title>
<date>2001</date>
<journal>Glot International,</journal>
<pages>5--9</pages>
<contexts>
<context position="7538" citStr="Boersma, 2001" startWordPosition="1182" endWordPosition="1183">ation results, we include all of the automatically identified syllables in the data set. This syllabification approach generates 48,253 syllables, 16,781 (34.8%) bearing accent. 4 Pitch Accent Detection Experiments We train logistic regression models to detect the presence of pitch accent using acoustic features drawn from each word, syllable and vowel, using Weka (Witten et al., 1999). The features we use included pitch (f0), energy and duration, which have been shown to correlate with pitch accent in English. To model these, we calculate pitch and energy contours for each token using Praat (Boersma, 2001). Duration information is derived using the vowel, syllable or word segmentation described in Section 3. The feature vectors we construct include features derived from both raw and speaker z-score normalized1 pitch and energy contours. The feature vector used in all three analysis scenarios is comprised of minimum, maximum, mean, standard de1Z-score normalization:xnorm _ x−µ σ , where x is a value to normalize, µ and σ are mean and standard deviation. These are estimated from all pitch or intensity values for a speaker. 82 viation and the z-score of the maximum of these raw and normalized acou</context>
</contexts>
<marker>Boersma, 2001</marker>
<rawString>P. Boersma. 2001. Praat, a system for doing phonetics by computer. Glot International, 5(9-10):341–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>P Price</author>
<author>S Shattuck-Hufnagel</author>
</authors>
<title>The boston university radio news corpus.</title>
<date>1995</date>
<tech>Technical Report ECS-95-001,</tech>
<institution>Boston University,</institution>
<contexts>
<context position="5665" citStr="Ostendorf et al., 1995" startWordPosition="882" endWordPosition="885"> with different methods and some have included lexico-syntactic as well as acoustic features. It is still not clear which domain of acoustic analysis provides the most accurate cues for accent prediction. To address this issue, our work compares accent detection at the syllable nucleus, full syllable, and word levels, using a common modeling technique and a common corpus, to focus on the question of which domain of acoustic analysis is most useful for pitch accent prediction. 3 Boston University Radio News Corpus Our experiments use 157.9 minutes (29,578 words) from six speakers in the BURNC (Ostendorf et al., 1995) recordings of professionally read radio news. This corpus has been prosodically annotated with full ToBI labeling (Silverman et al., 1992), including the presence and type of accents; these are annotated at the syllable level and 54.7% (16,178) of words are accented. Time-aligned phone boundaries generated by forced alignment are used to identify vowel regions for analysis. There are 48,359 vowels in the corpus and 34.8 of these are accented. To generate time-aligned syllable boundaries, we align the forced-aligned phones with a syllabified lexicon included with the corpus. The use of BURNC f</context>
</contexts>
<marker>Ostendorf, Price, Shattuck-Hufnagel, 1995</marker>
<rawString>M. Ostendorf, P. Price, and S. Shattuck-Hufnagel. 1995. The boston university radio news corpus. Technical Report ECS-95-001, Boston University, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>Detecting pitch accent using pitch-corrected energy-based predictors.</title>
<date>2007</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="4160" citStr="Rosenberg and Hirschberg (2007)" startWordPosition="642" endWordPosition="645">ese different domains. An exception is the work of Ross and Ostendorf (1996), who detect accent on the Boston University Radio News Corpus (BURNC) at both the syllable and word level. Using CART predictions as input to an HMM, they detect pitch accents on syllables spoken by a single speaker from BURNC with 87.7% accuracy, corresponding to 82.5% word-based accuracy, using both lexical and acoustic features. In comparing the discriminative usefulness of syllables vs. syllable nuclei for accent detection, Tamburini (2003) finds syllable nuclei (vowel) duration to be as useful to full syllables. Rosenberg and Hirschberg (2007) used an energy-based ensemble technique to detect pitch accents with 84.1% accuracy on the read portion of the Boston Directions Corpus, without using lexical information. Sridhar et al. (2008) obtain 86.0% word-based accuracy using maximum entropy models from acoustic and syntactic information on the BURNC. Syllable-based detection by Ananthakrishnan and Narayanan (2008) combines acoustic, lexical and syntactic FSM models to achieve a detection rate of 86.75%. Similar suprasegmental features have also been explored in work at SRI/ICSI which employs a hidden event model to model intonational </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>A. Rosenberg and J. Hirschberg. 2007. Detecting pitch accent using pitch-corrected energy-based predictors. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ross</author>
<author>M Ostendorf</author>
</authors>
<title>Prediction of abstract prosodic labels for speech synthesis.</title>
<date>1996</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="3605" citStr="Ross and Ostendorf (1996)" startWordPosition="555" endWordPosition="558"> comparing the use of vowel nuclei, syllables and words as units of analysis. In Section 2, we discuss related work. We describe the materials in Section 3, the experiments themselves in Section 4 and conclude in Section 5. 2 Related Work Acoustic-based approaches to pitch accent detection have explored prediction at the word, syllable, and Proceedings of NAACL HLT 2009: Short Papers, pages 81–84, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics vowel level, but have rarely compared prediction accuracies across these different domains. An exception is the work of Ross and Ostendorf (1996), who detect accent on the Boston University Radio News Corpus (BURNC) at both the syllable and word level. Using CART predictions as input to an HMM, they detect pitch accents on syllables spoken by a single speaker from BURNC with 87.7% accuracy, corresponding to 82.5% word-based accuracy, using both lexical and acoustic features. In comparing the discriminative usefulness of syllables vs. syllable nuclei for accent detection, Tamburini (2003) finds syllable nuclei (vowel) duration to be as useful to full syllables. Rosenberg and Hirschberg (2007) used an energy-based ensemble technique to d</context>
</contexts>
<marker>Ross, Ostendorf, 1996</marker>
<rawString>K. Ross and M. Ostendorf. 1996. Prediction of abstract prosodic labels for speech synthesis. Computer Speech &amp; Language, 10(3):155–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Silverman</author>
<author>M Beckman</author>
<author>J Pitrelli</author>
<author>M Ostendorf</author>
<author>C Wightman</author>
<author>P Price</author>
<author>J Pierrehumbert</author>
<author>J Hirschberg</author>
</authors>
<title>Tobi: A standard for labeling english prosody.</title>
<date>1992</date>
<booktitle>In Proc. of the 1992 International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>12--16</pages>
<contexts>
<context position="5804" citStr="Silverman et al., 1992" startWordPosition="902" endWordPosition="905">c analysis provides the most accurate cues for accent prediction. To address this issue, our work compares accent detection at the syllable nucleus, full syllable, and word levels, using a common modeling technique and a common corpus, to focus on the question of which domain of acoustic analysis is most useful for pitch accent prediction. 3 Boston University Radio News Corpus Our experiments use 157.9 minutes (29,578 words) from six speakers in the BURNC (Ostendorf et al., 1995) recordings of professionally read radio news. This corpus has been prosodically annotated with full ToBI labeling (Silverman et al., 1992), including the presence and type of accents; these are annotated at the syllable level and 54.7% (16,178) of words are accented. Time-aligned phone boundaries generated by forced alignment are used to identify vowel regions for analysis. There are 48,359 vowels in the corpus and 34.8 of these are accented. To generate time-aligned syllable boundaries, we align the forced-aligned phones with a syllabified lexicon included with the corpus. The use of BURNC for comparative accent prediction in our three domains is not straightforward, due to anomalies in the corpus. First, the lexicon and forced</context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrehumbert, Hirschberg, 1992</marker>
<rawString>K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, C. Wightman, P. Price, J. Pierrehumbert, and J. Hirschberg. 1992. Tobi: A standard for labeling english prosody. In Proc. of the 1992 International Conference on Spoken Language Processing, volume 2, pages 12–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Sridhar</author>
<author>S Bangalore</author>
<author>S Narayanan</author>
</authors>
<title>Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="4354" citStr="Sridhar et al. (2008)" startWordPosition="673" endWordPosition="676">ns as input to an HMM, they detect pitch accents on syllables spoken by a single speaker from BURNC with 87.7% accuracy, corresponding to 82.5% word-based accuracy, using both lexical and acoustic features. In comparing the discriminative usefulness of syllables vs. syllable nuclei for accent detection, Tamburini (2003) finds syllable nuclei (vowel) duration to be as useful to full syllables. Rosenberg and Hirschberg (2007) used an energy-based ensemble technique to detect pitch accents with 84.1% accuracy on the read portion of the Boston Directions Corpus, without using lexical information. Sridhar et al. (2008) obtain 86.0% word-based accuracy using maximum entropy models from acoustic and syntactic information on the BURNC. Syllable-based detection by Ananthakrishnan and Narayanan (2008) combines acoustic, lexical and syntactic FSM models to achieve a detection rate of 86.75%. Similar suprasegmental features have also been explored in work at SRI/ICSI which employs a hidden event model to model intonational information for a variety of tasks including punctuation and disfluency detection (Baron et al., 2002). However, while progress has been made in accent detection performance in the past 15 years</context>
<context position="14510" citStr="Sridhar et al. (2008)" startWordPosition="2258" endWordPosition="2261">le-, and vowelbased results directly, and then transfer syllable- and nucleus-based predictions to word predictions. Our experiments show that word-based accent detection significantly outperforms syllable- and vowel-based approaches. Extracting features that incorporate acoustic information from surrounding context improves performance in all three domains. We find that there is, in fact, acoustic information discriminative to pitch accent that is found within accented words, outside the accent-bearing syllable. We achieve 84.2% word-based accuracy — significantly below the 86.0% reported by Sridhar et al. (2008) using syntactic and acoustic components. However, our experiments use only acoustic features, since we are concerned with comparing domains of acoustic analysis within the larger task of accent identification. Our 84.2% accuracy is significantly higher than the 80.09% accuracy obtained by the 10ms frame-based acoustic modeling described in (Sridhar et al., 2008). Our aggregations of pitch and energy contours over a region of analysis appear to be more helpful than short frame modeling. In future work, we will explore a number of techniques to transfer word based predictions to syllables. This</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2008</marker>
<rawString>V. R. Sridhar, S. Bangalore, and S. Narayanan. 2008. Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework. IEEE Transactions on Audio, Speech &amp; Language Processing, 16(4):797–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tamburini</author>
</authors>
<title>Prosodic prominence detection in speech.</title>
<date>2003</date>
<booktitle>In ISSPA2003,</booktitle>
<pages>385--388</pages>
<contexts>
<context position="4054" citStr="Tamburini (2003)" startWordPosition="628" endWordPosition="629">utational Linguistics vowel level, but have rarely compared prediction accuracies across these different domains. An exception is the work of Ross and Ostendorf (1996), who detect accent on the Boston University Radio News Corpus (BURNC) at both the syllable and word level. Using CART predictions as input to an HMM, they detect pitch accents on syllables spoken by a single speaker from BURNC with 87.7% accuracy, corresponding to 82.5% word-based accuracy, using both lexical and acoustic features. In comparing the discriminative usefulness of syllables vs. syllable nuclei for accent detection, Tamburini (2003) finds syllable nuclei (vowel) duration to be as useful to full syllables. Rosenberg and Hirschberg (2007) used an energy-based ensemble technique to detect pitch accents with 84.1% accuracy on the read portion of the Boston Directions Corpus, without using lexical information. Sridhar et al. (2008) obtain 86.0% word-based accuracy using maximum entropy models from acoustic and syntactic information on the BURNC. Syllable-based detection by Ananthakrishnan and Narayanan (2008) combines acoustic, lexical and syntactic FSM models to achieve a detection rate of 86.75%. Similar suprasegmental feat</context>
</contexts>
<marker>Tamburini, 2003</marker>
<rawString>F. Tamburini. 2003. Prosodic prominence detection in speech. In ISSPA2003, pages 385–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
<author>L Trigg</author>
<author>M Hall</author>
<author>G Holmes</author>
<author>S Cunningham</author>
</authors>
<title>Weka: Practical machine learning tools and techniques with java implementation.</title>
<date>1999</date>
<booktitle>In ICONIP/ANZIIS/ANNES International Workshop,</booktitle>
<pages>192--196</pages>
<contexts>
<context position="7312" citStr="Witten et al., 1999" startWordPosition="1140" endWordPosition="1143"> in the lexicon does not always match the number of vowels in the forced alignment. This leads to 114 syllables containing two forced-aligned vowels, and 8 containing none. Instead of performing post hoc correction of the syllabification results, we include all of the automatically identified syllables in the data set. This syllabification approach generates 48,253 syllables, 16,781 (34.8%) bearing accent. 4 Pitch Accent Detection Experiments We train logistic regression models to detect the presence of pitch accent using acoustic features drawn from each word, syllable and vowel, using Weka (Witten et al., 1999). The features we use included pitch (f0), energy and duration, which have been shown to correlate with pitch accent in English. To model these, we calculate pitch and energy contours for each token using Praat (Boersma, 2001). Duration information is derived using the vowel, syllable or word segmentation described in Section 3. The feature vectors we construct include features derived from both raw and speaker z-score normalized1 pitch and energy contours. The feature vector used in all three analysis scenarios is comprised of minimum, maximum, mean, standard de1Z-score normalization:xnorm _ </context>
</contexts>
<marker>Witten, Frank, Trigg, Hall, Holmes, Cunningham, 1999</marker>
<rawString>I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and S. Cunningham. 1999. Weka: Practical machine learning tools and techniques with java implementation. In ICONIP/ANZIIS/ANNES International Workshop, pages 192–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>