<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018924">
<title confidence="0.9980935">
Lightly-Supervised Word Sense Translation Error Detection for an
Interactive Conversational Spoken Language Translation System
</title>
<author confidence="0.869925">
Dennis N. Mehay, Sankaranarayanan Ananthakrishnan and Sanjika Hewavitharana
</author>
<affiliation confidence="0.673866">
Speech, Language and Multimedia Processing Unit
Raytheon BBN Technologies
</affiliation>
<address confidence="0.934092">
Cambridge, MA, 02138, USA
</address>
<email confidence="0.999647">
{dmehay,sanantha,shewavit}@bbn.com
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996245">
Lexical ambiguity can lead to concept
transfer failure in conversational spo-
ken language translation (CSLT) systems.
This paper presents a novel, classification-
based approach to accurately detecting
word sense translation errors (WSTEs) of
ambiguous source words. The approach
requires minimal human annotation effort,
and can be easily scaled to new language
pairs and domains, with only a word-
aligned parallel corpus and a small set of
manual translation judgments. We show
that this approach is highly precise in de-
tecting WSTEs, even in highly skewed
data, making it practical for use in an in-
teractive CSLT system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.903714636363636">
Lexical ambiguity arises when a single word form
can refer to different concepts. Selecting a con-
textually incorrect translation of such a word —
here referred to as a word sense translation error
(WSTE) — can lead to a critical failure in a con-
versational spoken language translation (CSLT)
system, where accuracy of concept transfer is
paramount. Interactive CSLT systems are espe-
cially prone to mis-translating less frequent word
senses, when they use phrase-based statistical ma-
chine translation (SMT), due to its limited use of
source context (source phrases) when constructing
translation hypotheses. Figure 1 illustrates a typi-
cal WSTE in a phrase-based English-to-Iraqi Ara-
bic CSLT system, where the English word board
Disclaimer: This paper is based upon work supported by
the DARPA BOLT program. The views expressed here are
those of the authors and do not reflect the official policy or
position of the Department of Defense or the U.S. Govern-
ment.
Distribution Statement A (Approved for Public Release,
Distribution Unlimited)
</bodyText>
<figureCaption confidence="0.99915">
Figure 1: Example WSTE in English-to-Iraqi SMT.
</figureCaption>
<bodyText confidence="0.999293617647059">
is mis-translated as mjls (“council”), completely
distorting the intended message.
Interactive CSLT systems can mitigate this
problem by automatically detecting WSTEs in
SMT hypotheses, and engaging the operator in a
clarification dialogue (e.g. requesting an unam-
biguous rephrasing). We propose a novel, two-
level classification approach to accurately detect
WSTEs. In the first level, a bank of word-specific
classifiers predicts, given a rich set of contextual
and syntactic features, a distribution over possi-
ble target translations for each ambiguous source
word in our inventory. A single, second-level clas-
sifier then compares the predicted target words to
those chosen by the decoder and determines the
likelihood that an error was made.
A significant novelty of our approach is that the
first-level classifiers are fully unsupervised with
respect to manual annotation and can easily be
expanded to accommodate new ambiguous words
and additional parallel data. The other innovative
aspect of our solution is the use of a small set of
manual translation judgments to train the second-
level classifier. This classifier uses high-level fea-
tures derived from the output of the first-level clas-
sifiers to produce a binary WSTE prediction, and
can be re-used unchanged even when the first level
of classifiers is expanded.
Our goal departs from the large body of work
devoted to lightly-supervised word sense disam-
biguation (WSD) using monolingual and bilingual
corpora (Yarowsky, 1995; Schutze, 1998; Diab
and Resnik, 2002; Ng et al., 2003; Li and Li, 2002;
Purandare and Pedersen, 2004), which seeks to la-
</bodyText>
<page confidence="0.985775">
54
</page>
<note confidence="0.801785">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999326714285714">
bel and group unlabeled sense instances. Instead,
our approach detects mis-translations of a known
set of ambiguous words.
The proposed method also deviates from ex-
isting work on global lexical selection models
(Mauser et al., 2009) and on integration of WSD
features within SMT systems with the goal of im-
proving offline translation performance (Chan et
al., 2007). Rather, we detect translation errors due
to ambiguous source words with the goal of pro-
viding feedback to and soliciting clarification from
the system operator in real time. Our approach
is partly inspired by Carpuat and Wu’s (2007b;
2007a) unsupervised sense disambiguation mod-
els for offline SMT. More recently, Carpuat et al.
(2013) identify unseen target senses in new do-
mains, but their approach requires the full test cor-
pus upfront, which is unavailable in spontaneous
CSLT. Our approach can, in principle, identify
novel senses when unfamiliar source contexts are
encountered, but this is not our current focus.
</bodyText>
<sectionHeader confidence="0.975791" genericHeader="method">
2 Baseline SMT System
</sectionHeader>
<bodyText confidence="0.999967291666667">
In this paper, we focus on WSTE detection in
the context of phrase-based English-to-Iraqi Ara-
bic SMT, an integral component of our interac-
tive, two-way CSLT system that mediates con-
versation between monolingual speakers of En-
glish and Iraqi Arabic. The parallel training cor-
pus of approximately 773K sentence pairs (7.3M
English words) was derived from the DARPA
TransTac English-Iraqi two-way spoken dialogue
collection and spans a variety of domains includ-
ing force protection, medical diagnosis and aid,
etc. Phrase pairs were extracted from bidirectional
IBM Model 4 word alignment after applying a
merging heuristic similar to that of Koehn et al.
(2003). A 4-gram target LM was trained on Iraqi
Arabic transcriptions. Our phrase-based decoder,
similar to Moses (Koehn et al., 2007), performs
beam search stack decoding based on a standard
log-linear model, whose parameters were tuned
with MERT (Och, 2003) on a held-out develop-
ment set (3,534 sentence pairs, 45K words). The
BLEU and METEOR scores of this system on a
separate test set (3,138 sentence pairs, 38K words)
were 16.1 and 42.5, respectively.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="method">
3 WSTE Detection
</sectionHeader>
<bodyText confidence="0.999971">
The core of the WSTE detector is a novel, two-
level classification pipeline. Our approach avoids
</bodyText>
<figureCaption confidence="0.98584">
Figure 2: An English–Iraqi training pair.
</figureCaption>
<bodyText confidence="0.99999525">
the need for expensive, sense-labeled training data
based on the observation that knowing the sense of
an ambiguous source word is distinct from know-
ing whether a sense translation error has occurred.
Instead, the target (Iraqi Arabic) words typically
associated with a given sense of an ambiguous
source (English) word serve as implicit sense la-
bels, as the following describes.
</bodyText>
<subsectionHeader confidence="0.99705">
3.1 A First Level of Unsupervised Classifiers
</subsectionHeader>
<bodyText confidence="0.999719516129033">
The main intuition behind our approach is that
strong disagreement between the expanded con-
text of an ambiguous source word and the corre-
sponding SMT hypothesis indicates an increased
likelihood that a WSTE has occurred. To identify
such disagreement, we train a bank of maximum-
entropy classifiers (Berger et al., 1996), one for
each ambiguous word. The classifiers are trained
on the same word-aligned parallel data used for
training the baseline SMT system, as follows.
For each instance of an ambiguous source word
in the training set, and for each target word it is
aligned to, we emit a training instance associating
that target word and the wider source context of
the ambiguous word. Figure 2 illustrates a typical
training instance for the ambiguous English word
board, which emits a tuple of contextual features
and the aligned Iraqi Arabic word lwHp (“plac-
ard”) as a target label. We use the following con-
textual features similar to those of Carpuat and
Wu (2005), which are in turn based on the clas-
sic WSD features of Yarowsky (1995).
Neighboring Words/Lemmas/POSs. The to-
kens, t, to the left and right of the current ambigu-
ous token, as well as all trigrams of tokens that
span the current token. Separate features for word,
lemma and parts of speech tokens, t.
Lemma/POS Dependencies. The lemma-
lemma and POS-POS labeled and unlabeled
directed syntactic dependencies of the current
ambiguous token.
</bodyText>
<page confidence="0.996789">
55
</page>
<figureCaption confidence="0.998335">
Figure 3: An unsupervised first-level classifier.
</figureCaption>
<bodyText confidence="0.999745833333333">
Bag-of-words/lemmas. Distance decayed bag-
of-words-style features for each word and lemma
in a seven-word window around the current token.
Figure 3 schematically illustrates how this classi-
fier operates on a sample test sentence. The ex-
ample assumes that the ambiguous English word
board is only ever associated with the Iraqi Arabic
words lwHp (“placard”) and mjls (“council”) in
the training word alignment. We emphasize that
even though the first-level maximum entropy clas-
sifiers are intrinsically supervised, their training
data is derived via unsupervised word alignment.
</bodyText>
<subsectionHeader confidence="0.998019">
3.2 A Second-Level Meta-Classifier
</subsectionHeader>
<bodyText confidence="0.999779148148148">
The first-level classifiers do not directly predict
the presence of a WSTE, but induce a distribu-
tion over possible target words that could be gen-
erated by the ambiguous source word in that con-
text. In order to make a binary decision, this distri-
bution must be contrasted with the corresponding
target phrase hypothesized by the SMT decoder.
One straightforward approach, which we use as
a baseline, is to threshold the posterior probabil-
ity of the word in the SMT target phrase which is
ranked highest in the classifier-predicted distribu-
tion. However, this approach is not ideal because
each classifier has a different target label set and is
trained on a different number of instances.
To address this issue, we introduce a second
meta-classifier, which is trained on a small number
of hand-annotated translation judgments of SMT
hypotheses of source sentences containing am-
biguous words. The bilingual annotator was sim-
ply asked to label the phrasal translation of source
phrases containing ambiguous words as correct or
incorrect. We obtained translation judgments for
511 instances from the baseline SMT development
and test sets, encompassing 147 pre-defined am-
biguous words obtained heuristically from Word-
Net, public domain homograph lists, etc.
The second-level classifier is trained on a small
</bodyText>
<figureCaption confidence="0.999093">
Figure 4: The two-level WSTE architecture.
</figureCaption>
<bodyText confidence="0.999502571428571">
set of meta-features drawn from the predictions of
the first-level classifiers and from simple statistics
of the training corpus. For an ambiguous word
wa in source sentence S, with contextual features
f1(S), and aligned to target words t E T (the set
of words in the target phrase) in the SMT hypoth-
esis, we extract the following features:
</bodyText>
<listItem confidence="0.9575085">
1. The first-level classifier’s maximum
likelihood of any decoded target word:
</listItem>
<equation confidence="0.9262986">
maTxpwa (t|f1(S))
t∈
2. The entropy of the predicted distribution:
E pwa(t|f1(S)) · ln(pwa(t|f1(S)))
t
</equation>
<listItem confidence="0.96561175">
3. The number of training instances for wa
4. The inverse of the number of distinct target
labels for wa.
5. The product of meta-features (1) and (4)
</listItem>
<bodyText confidence="0.999989555555556">
A high value for feature 1 indicates that the first-
level model and the SMT decoder agree. By con-
trast, a high value for feature 2 indicates uncer-
tainty in the classifier’s prediction, due either to a
novel source context, or inadequate training data.
Feature 3 indicates whether the second scenario of
meta-feature 2 might be at play, and feature 4 can
be thought of as a simple, uniform prior for each
classifier. Finally, feature 5 attenuates feature 1
by this simple, uniform prior. We feed these fea-
tures to a random forest (Breiman, 2001), which
is a committee of decision trees, trained using ran-
domly selected features and data points, using the
implementation in Weka (Hall et al., 2009). The
target labels for training the second-level classifier
are obtained from the binary translation judgments
on the small annotated corpus. Figure 4 illustrates
the interaction of the two levels of classification.
</bodyText>
<page confidence="0.991943">
56
</page>
<subsectionHeader confidence="0.988206">
3.3 Scalability and Portability
</subsectionHeader>
<bodyText confidence="0.999989045454545">
Scalability was an important consideration in de-
signing the proposed WSTE approach. For in-
stance, we may wish to augment the inventory
with new ambiguous words if the vocabulary
grows due to addition of new parallel data or due
to a change in the domain. The primary advan-
tage of the two-level approach is that new ambigu-
ous words can be accommodated by augmenting
the unsupervised first-level classifier set with addi-
tional word-specific classifiers, which can be done
by simply extending the pre-defined list of am-
biguous words. Further, the current classification
stack requires only ≈1.5GB of RAM and performs
per-word WSTE inference in only a few millisec-
onds on a commodity, quad-core laptop, which is
critical for real-time, interactive CSLT.
The minimal annotation requirements also al-
low a high level of portability to new language
pairs. Moreover, as our results indicate (below), a
good quality WSTE detector can be bootstrapped
for a new language pair without any annotation ef-
fort by simply leveraging the first-level classifiers.
</bodyText>
<sectionHeader confidence="0.992716" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999589846153846">
The 511 WSTE-annotated instances used for train-
ing the second-level classifier doubled as an eval-
uation set using the leave-one-out cross-validation
method. Of these, 115 were labeled as errors by
the bilingual judge, while the remaining 396 were
translated correctly by the baseline SMT system.
The error prediction score from the second-level
classifier was thresholded to obtain the receiver
operating characteristic (ROC) curve shown in the
top (black) curve of Figure 5. We obtain a 43%
error detection rate with only 10% false alarms
and 71% detection with 20% false alarms, in spite
of the highly skewed label distribution. In abso-
lute terms, true positives outnumber false alarms
at both the 10% (49 to 39) and 20% (81 to 79) false
alarm rates. This is important for deployment, as
we do not want to disrupt the flow of conversation
with more false alarms than true positives.
For comparison, the bottom (red) ROC curve
shows the performance of a baseline WSTE pre-
dictor comprised of just meta-feature (1), obtain-
able directly from the first-level classifiers. This
performs slightly worse than the two-level model
at 10% false alarms (40% detection, 46 true pos-
itives, 39 false alarms), and considerably worse
at 20% false alarms (57% detection, 66 true pos-
</bodyText>
<figureCaption confidence="0.992903">
Figure 5: WST error detection ROC curve.
</figureCaption>
<bodyText confidence="0.54666925">
itives, 78 false alarms). Nevertheless, this result
indicates the possibility of bootstrapping a good
quality baseline WSTE detector in a new language
or domain without any annotation effort.
</bodyText>
<sectionHeader confidence="0.996874" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999987642857143">
We proposed a novel, lightly-supervised, two-
level classification architecture that identifies pos-
sible mis-translations of pre-defined ambiguous
source words. The WSTE detector pre-empts
communication failure in an interactive CSLT sys-
tem by serving as a trigger for initiating feed-
back and clarification. The first level of our de-
tector comprises of a bank of word-specific classi-
fiers trained on automatic word alignment over the
SMT parallel training corpus. Their predicted dis-
tributions over target words feed into the second-
level meta-classifier, which is trained on a small
set of manual translation judgments. On a 511-
instance test set, the two-level approach exhibits
WSTE detection rates of 43% and 71% at 10%
and 20% false alarm rates, respectively, in spite of
a nearly 1:4 skew against actual WSTE instances.
Because adding new ambiguous words to the in-
ventory only requires augmenting the set of first-
level unsupervised classifiers, our WSTE detec-
tion approach is scalable to new domains and
training data. It is also easily portable to new lan-
guage pairs due to the minimal annotation effort
required for training the second-level classifier. Fi-
nally, we show that it is possible to bootstrap a
good quality WSTE detector in a new language
pair without any annotation effort using only un-
supervised classifiers and a parallel corpus.
</bodyText>
<page confidence="0.99806">
57
</page>
<sectionHeader confidence="0.996345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821425742574">
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39–71.
Leo Breiman. 2001. Random Forests. Technical re-
port, Statistics Department, University of California,
Berkeley, Berkeley, CA, USA, January.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 387–394.
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In
Proceedings of the 11th International Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI 2007), Skovde, Sweden, Septem-
ber.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72, Prague,
Czech Republic, June.
Marine Carpuat, Hal Daum´e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. Sensespotting: Never let your par-
allel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1435–1445, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting ofthe Association of Computational
Linguistics, pages 33–40, Prague, Czech Republic,
June.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255–262, July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDDExplorations, 11(1).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hang Li and Cong Li. 2002. Word translation dis-
ambiguation using bilingual bootstrapping. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 343–351, July.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
210–218, Singapore, August. Association for Com-
putational Linguistics.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In Proceedings of 41st
Annual Meeting on Association for Computational
Linguistics, pages 455–462, July.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In Proceedings of the Confer-
ence on ComputationalNatural Language Learning,
pages 41–48.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Journal of Computational Linguistics,
24:97–123.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, ACL ’95, pages 189–
196, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.999261">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388067">
<title confidence="0.9990155">Lightly-Supervised Word Sense Translation Error Detection for Interactive Conversational Spoken Language Translation System</title>
<author confidence="0.834782">Dennis N Mehay</author>
<author confidence="0.834782">Sankaranarayanan Ananthakrishnan</author>
<author confidence="0.834782">Sanjika</author>
<affiliation confidence="0.627968">Speech, Language and Multimedia Processing Raytheon BBN Technologies</affiliation>
<address confidence="0.999856">Cambridge, MA, 02138, USA</address>
<abstract confidence="0.985970529411765">Lexical ambiguity can lead to concept transfer failure in conversational spoken language translation (CSLT) systems. This paper presents a novel, classificationbased approach to accurately detecting word sense translation errors (WSTEs) of ambiguous source words. The approach requires minimal human annotation effort, and can be easily scaled to new language pairs and domains, with only a wordaligned parallel corpus and a small set of manual translation judgments. We show that this approach is highly precise in detecting WSTEs, even in highly skewed data, making it practical for use in an interactive CSLT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6908" citStr="Berger et al., 1996" startWordPosition="1057" endWordPosition="1060">rd is distinct from knowing whether a sense translation error has occurred. Instead, the target (Iraqi Arabic) words typically associated with a given sense of an ambiguous source (English) word serve as implicit sense labels, as the following describes. 3.1 A First Level of Unsupervised Classifiers The main intuition behind our approach is that strong disagreement between the expanded context of an ambiguous source word and the corresponding SMT hypothesis indicates an increased likelihood that a WSTE has occurred. To identify such disagreement, we train a bank of maximumentropy classifiers (Berger et al., 1996), one for each ambiguous word. The classifiers are trained on the same word-aligned parallel data used for training the baseline SMT system, as follows. For each instance of an ambiguous source word in the training set, and for each target word it is aligned to, we emit a training instance associating that target word and the wider source context of the ambiguous word. Figure 2 illustrates a typical training instance for the ambiguous English word board, which emits a tuple of contextual features and the aligned Iraqi Arabic word lwHp (“placard”) as a target label. We use the following context</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Statistics Department, University of California, Berkeley,</institution>
<location>Berkeley, CA, USA,</location>
<contexts>
<context position="11230" citStr="Breiman, 2001" startWordPosition="1757" endWordPosition="1758">inct target labels for wa. 5. The product of meta-features (1) and (4) A high value for feature 1 indicates that the firstlevel model and the SMT decoder agree. By contrast, a high value for feature 2 indicates uncertainty in the classifier’s prediction, due either to a novel source context, or inadequate training data. Feature 3 indicates whether the second scenario of meta-feature 2 might be at play, and feature 4 can be thought of as a simple, uniform prior for each classifier. Finally, feature 5 attenuates feature 1 by this simple, uniform prior. We feed these features to a random forest (Breiman, 2001), which is a committee of decision trees, trained using randomly selected features and data points, using the implementation in Weka (Hall et al., 2009). The target labels for training the second-level classifier are obtained from the binary translation judgments on the small annotated corpus. Figure 4 illustrates the interaction of the two levels of classification. 56 3.3 Scalability and Portability Scalability was an important consideration in designing the proposed WSTE approach. For instance, we may wish to augment the inventory with new ambiguous words if the vocabulary grows due to addit</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Technical report, Statistics Department, University of California, Berkeley, Berkeley, CA, USA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="7562" citStr="Carpuat and Wu (2005)" startWordPosition="1168" endWordPosition="1171">e classifiers are trained on the same word-aligned parallel data used for training the baseline SMT system, as follows. For each instance of an ambiguous source word in the training set, and for each target word it is aligned to, we emit a training instance associating that target word and the wider source context of the ambiguous word. Figure 2 illustrates a typical training instance for the ambiguous English word board, which emits a tuple of contextual features and the aligned Iraqi Arabic word lwHp (“placard”) as a target label. We use the following contextual features similar to those of Carpuat and Wu (2005), which are in turn based on the classic WSD features of Yarowsky (1995). Neighboring Words/Lemmas/POSs. The tokens, t, to the left and right of the current ambiguous token, as well as all trigrams of tokens that span the current token. Separate features for word, lemma and parts of speech tokens, t. Lemma/POS Dependencies. The lemmalemma and POS-POS labeled and unlabeled directed syntactic dependencies of the current ambiguous token. 55 Figure 3: An unsupervised first-level classifier. Bag-of-words/lemmas. Distance decayed bagof-words-style features for each word and lemma in a seven-word win</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI</booktitle>
<location>Skovde, Sweden,</location>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007), Skovde, Sweden, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<location>Prague, Czech Republic,</location>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daum´e Katharine Henry</author>
<author>Ann Irvine</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Rachel Rudinger</author>
</authors>
<title>Sensespotting: Never let your parallel data tie you to an old domain.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1435--1445</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4590" citStr="Carpuat et al. (2013)" startWordPosition="689" endWordPosition="692">ations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline translation performance (Chan et al., 2007). Rather, we detect translation errors due to ambiguous source words with the goal of providing feedback to and soliciting clarification from the system operator in real time. Our approach is partly inspired by Carpuat and Wu’s (2007b; 2007a) unsupervised sense disambiguation models for offline SMT. More recently, Carpuat et al. (2013) identify unseen target senses in new domains, but their approach requires the full test corpus upfront, which is unavailable in spontaneous CSLT. Our approach can, in principle, identify novel senses when unfamiliar source contexts are encountered, but this is not our current focus. 2 Baseline SMT System In this paper, we focus on WSTE detection in the context of phrase-based English-to-Iraqi Arabic SMT, an integral component of our interactive, two-way CSLT system that mediates conversation between monolingual speakers of English and Iraqi Arabic. The parallel training corpus of approximatel</context>
</contexts>
<marker>Carpuat, Henry, Irvine, Jagarlamudi, Rudinger, 2013</marker>
<rawString>Marine Carpuat, Hal Daum´e III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger. 2013. Sensespotting: Never let your parallel data tie you to an old domain. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1435–1445, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4253" citStr="Chan et al., 2007" startWordPosition="636" endWordPosition="639">re and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline translation performance (Chan et al., 2007). Rather, we detect translation errors due to ambiguous source words with the goal of providing feedback to and soliciting clarification from the system operator in real time. Our approach is partly inspired by Carpuat and Wu’s (2007b; 2007a) unsupervised sense disambiguation models for offline SMT. More recently, Carpuat et al. (2013) identify unseen target senses in new domains, but their approach requires the full test corpus upfront, which is unavailable in spontaneous CSLT. Our approach can, in principle, identify novel senses when unfamiliar source contexts are encountered, but this is n</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages 33–40, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="3592" citStr="Diab and Resnik, 2002" startWordPosition="534" endWordPosition="537"> to accommodate new ambiguous words and additional parallel data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of </context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 255–262, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDDExplorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11382" citStr="Hall et al., 2009" startWordPosition="1780" endWordPosition="1783">coder agree. By contrast, a high value for feature 2 indicates uncertainty in the classifier’s prediction, due either to a novel source context, or inadequate training data. Feature 3 indicates whether the second scenario of meta-feature 2 might be at play, and feature 4 can be thought of as a simple, uniform prior for each classifier. Finally, feature 5 attenuates feature 1 by this simple, uniform prior. We feed these features to a random forest (Breiman, 2001), which is a committee of decision trees, trained using randomly selected features and data points, using the implementation in Weka (Hall et al., 2009). The target labels for training the second-level classifier are obtained from the binary translation judgments on the small annotated corpus. Figure 4 illustrates the interaction of the two levels of classification. 56 3.3 Scalability and Portability Scalability was an important consideration in designing the proposed WSTE approach. For instance, we may wish to augment the inventory with new ambiguous words if the vocabulary grows due to addition of new parallel data or due to a change in the domain. The primary advantage of the two-level approach is that new ambiguous words can be accommodat</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDDExplorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5556" citStr="Koehn et al. (2003)" startWordPosition="841" endWordPosition="844">n in the context of phrase-based English-to-Iraqi Arabic SMT, an integral component of our interactive, two-way CSLT system that mediates conversation between monolingual speakers of English and Iraqi Arabic. The parallel training corpus of approximately 773K sentence pairs (7.3M English words) was derived from the DARPA TransTac English-Iraqi two-way spoken dialogue collection and spans a variety of domains including force protection, medical diagnosis and aid, etc. Phrase pairs were extracted from bidirectional IBM Model 4 word alignment after applying a merging heuristic similar to that of Koehn et al. (2003). A 4-gram target LM was trained on Iraqi Arabic transcriptions. Our phrase-based decoder, similar to Moses (Koehn et al., 2007), performs beam search stack decoding based on a standard log-linear model, whose parameters were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words). The BLEU and METEOR scores of this system on a separate test set (3,138 sentence pairs, 38K words) were 16.1 and 42.5, respectively. 3 WSTE Detection The core of the WSTE detector is a novel, twolevel classification pipeline. Our approach avoids Figure 2: An English–Iraqi training</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5684" citStr="Koehn et al., 2007" startWordPosition="861" endWordPosition="864"> mediates conversation between monolingual speakers of English and Iraqi Arabic. The parallel training corpus of approximately 773K sentence pairs (7.3M English words) was derived from the DARPA TransTac English-Iraqi two-way spoken dialogue collection and spans a variety of domains including force protection, medical diagnosis and aid, etc. Phrase pairs were extracted from bidirectional IBM Model 4 word alignment after applying a merging heuristic similar to that of Koehn et al. (2003). A 4-gram target LM was trained on Iraqi Arabic transcriptions. Our phrase-based decoder, similar to Moses (Koehn et al., 2007), performs beam search stack decoding based on a standard log-linear model, whose parameters were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words). The BLEU and METEOR scores of this system on a separate test set (3,138 sentence pairs, 38K words) were 16.1 and 42.5, respectively. 3 WSTE Detection The core of the WSTE detector is a novel, twolevel classification pipeline. Our approach avoids Figure 2: An English–Iraqi training pair. the need for expensive, sense-labeled training data based on the observation that knowing the sense of an ambiguous sourc</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Cong Li</author>
</authors>
<title>Word translation disambiguation using bilingual bootstrapping.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="3626" citStr="Li and Li, 2002" startWordPosition="542" endWordPosition="545">additional parallel data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline translation perf</context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>Hang Li and Cong Li. 2002. Word translation disambiguation using bilingual bootstrapping. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 343–351, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4120" citStr="Mauser et al., 2009" startWordPosition="614" endWordPosition="617">sing monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline translation performance (Chan et al., 2007). Rather, we detect translation errors due to ambiguous source words with the goal of providing feedback to and soliciting clarification from the system operator in real time. Our approach is partly inspired by Carpuat and Wu’s (2007b; 2007a) unsupervised sense disambiguation models for offline SMT. More recently, Carpuat et al. (2013) identify unseen target senses in new domains, but their approach requires the full test corpus upfront, which is unavailable in s</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210–218, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="3609" citStr="Ng et al., 2003" startWordPosition="538" endWordPosition="541">iguous words and additional parallel data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of 41st Annual Meeting on Association for Computational Linguistics, pages 455–462, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5809" citStr="Och, 2003" startWordPosition="882" endWordPosition="883">ntence pairs (7.3M English words) was derived from the DARPA TransTac English-Iraqi two-way spoken dialogue collection and spans a variety of domains including force protection, medical diagnosis and aid, etc. Phrase pairs were extracted from bidirectional IBM Model 4 word alignment after applying a merging heuristic similar to that of Koehn et al. (2003). A 4-gram target LM was trained on Iraqi Arabic transcriptions. Our phrase-based decoder, similar to Moses (Koehn et al., 2007), performs beam search stack decoding based on a standard log-linear model, whose parameters were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words). The BLEU and METEOR scores of this system on a separate test set (3,138 sentence pairs, 38K words) were 16.1 and 42.5, respectively. 3 WSTE Detection The core of the WSTE detector is a novel, twolevel classification pipeline. Our approach avoids Figure 2: An English–Iraqi training pair. the need for expensive, sense-labeled training data based on the observation that knowing the sense of an ambiguous source word is distinct from knowing whether a sense translation error has occurred. Instead, the target (Iraqi Arabic) words typi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on ComputationalNatural Language Learning,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="3657" citStr="Purandare and Pedersen, 2004" startWordPosition="546" endWordPosition="549">el data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT systems with the goal of improving offline translation performance (Chan et al., 2007). Ra</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on ComputationalNatural Language Learning, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics,</journal>
<pages>24--97</pages>
<contexts>
<context position="3569" citStr="Schutze, 1998" startWordPosition="532" endWordPosition="533">ily be expanded to accommodate new ambiguous words and additional parallel data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD features within SMT sy</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Journal of Computational Linguistics, 24:97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3554" citStr="Yarowsky, 1995" startWordPosition="530" endWordPosition="531">tion and can easily be expanded to accommodate new ambiguous words and additional parallel data. The other innovative aspect of our solution is the use of a small set of manual translation judgments to train the secondlevel classifier. This classifier uses high-level features derived from the output of the first-level classifiers to produce a binary WSTE prediction, and can be re-used unchanged even when the first level of classifiers is expanded. Our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation (WSD) using monolingual and bilingual corpora (Yarowsky, 1995; Schutze, 1998; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2002; Purandare and Pedersen, 2004), which seeks to la54 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 54–58, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics bel and group unlabeled sense instances. Instead, our approach detects mis-translations of a known set of ambiguous words. The proposed method also deviates from existing work on global lexical selection models (Mauser et al., 2009) and on integration of WSD feature</context>
<context position="7634" citStr="Yarowsky (1995)" startWordPosition="1184" endWordPosition="1185">ning the baseline SMT system, as follows. For each instance of an ambiguous source word in the training set, and for each target word it is aligned to, we emit a training instance associating that target word and the wider source context of the ambiguous word. Figure 2 illustrates a typical training instance for the ambiguous English word board, which emits a tuple of contextual features and the aligned Iraqi Arabic word lwHp (“placard”) as a target label. We use the following contextual features similar to those of Carpuat and Wu (2005), which are in turn based on the classic WSD features of Yarowsky (1995). Neighboring Words/Lemmas/POSs. The tokens, t, to the left and right of the current ambiguous token, as well as all trigrams of tokens that span the current token. Separate features for word, lemma and parts of speech tokens, t. Lemma/POS Dependencies. The lemmalemma and POS-POS labeled and unlabeled directed syntactic dependencies of the current ambiguous token. 55 Figure 3: An unsupervised first-level classifier. Bag-of-words/lemmas. Distance decayed bagof-words-style features for each word and lemma in a seven-word window around the current token. Figure 3 schematically illustrates how thi</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95, pages 189– 196, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>