<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013449">
<title confidence="0.96895">
A Corpus-Centered Approach to Spoken Language Translation
</title>
<note confidence="0.7969354">
Eiichiro SUMITA, Yasuhiro AKIBA, Takao DO!, Andrew FINCH, Kenji IMAMURA,
Michael PAUL, Mitsuo SHIMOHATA, and Taro WATANABE
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN
e iichiro sumitagatr. co jp
</note>
<sectionHeader confidence="0.980116" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999439666666667">
This paper reports the latest performance of com-
ponents and features of a project named Corpus-
Centered Computation (C&apos;3), which targets a trans-
lation technology suitable for spoken language
translation. C3 places corpora at the center of the
technology. Translation knowledge is extracted
from corpora by both EBMT and SMT methods,
translation quality is gauged by referring to cor-
pora, the best translation among multiple-engine
outputs is selected based on corpora and the cor-
pora themselves are paraphrased or filtered by
automated processes.
</bodyText>
<sectionHeader confidence="0.997892" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999001">
Our project, named Corpus-Centered Computation
((3), proposes solutions for efficiently constructing
a high-quality translation subsystem for a speech-
to-speech translation system.
This paper introduces recent progress in C3. Sec-
tions 2 and 3 demonstrate a competition between
multiple machine translation systems developed in
our project, and Sections 4 and 5 explain the fea-
tures that differentiate our project from other cor-
pus-based projects.
</bodyText>
<sectionHeader confidence="0.968312" genericHeader="method">
2 Three Corpus-based MT Systems
</sectionHeader>
<bodyText confidence="0.963837181818182">
There are two main strategies in corpus-based ma-
chine translation: (i) Example-Based Machine
Translation (EBMT; Nagao, 1984; Somers, 1999)
and (ii) Statistical Machine Translation (SMT;
Brown et al., 1993; Knight, 1997; Ney, 2001; Al-
shawi et al., 2000). C3 is developing both tech-
nologies in parallel and blending them. In this
paper, we introduce three different machine trans-
lation systems: Di, HPAT, and SAT.
The three MT systems are characterized by dif-
ferent translation units. D3, HPAT, and SAT use
sentences, phrases, and words, respectively.
D3 (Sentence-based EBMT): It retrieves the most
similar example by DP-matching of the input and
example sentences and adjusts the gap between the
input and the retrieved example by using dictionar-
ies. (Sumita 2001)
HPAT (Phrase-based EBMT): Based on phrase-
aligned bilingual trees, transfer patterns are gener-
ated. According to the patterns, the source phrase
structure is obtained and converted to generate tar-
get sentences (Imamura 2002)
</bodyText>
<listItem confidence="0.7709598">
SAT (Word-based SMT): Watanabe et al.
(2002b) implemented SAT dealing with Japanese
and English on top of a word-based SMT frame-
work (Brown et al. 1993).
3 Competition on the Same Corpus
</listItem>
<subsectionHeader confidence="0.724881">
3.1 Resources
</subsectionHeader>
<bodyText confidence="0.999890583333333">
In our competitive evaluation of the MT systems,
we used the BTEC corpus &apos;, which is a collection
of Japanese sentences and their English transla-
tions typically found in phrasebooks for tourists.
The size is about 150 thousand sentence pairs. A
quality evaluation was done using a test set
consisting of 345 sentences selected randomly
from the above corpus, and the remaining
sentences were used for learning and verification.
For each source sentence in the test set, 16
reference translations were prepared by 5 bilingual
translators.
</bodyText>
<footnote confidence="0.9397535">
1 BTEC was called BE in the paper (Takezawa et al.,
2002).
</footnote>
<page confidence="0.997483">
171
</page>
<bodyText confidence="0.9858665">
We used bilingual dictionaries and thesauri of
about fifty thousand words for the travel domain.
</bodyText>
<subsectionHeader confidence="0.996843">
3.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999296666666667">
We used the measures below. The BLEU score
and the RED rank are measured by referring to the
test corpus, i.e., a set of input sentences and their
multiple reference translations; the HUMAN rank
and the estimated TOEIC score are judged by bi-
lingual translators.
</bodyText>
<listItem confidence="0.960107047619048">
(1) Average of Ranks2:
HUMAN rank: In our evaluation, 9 translators
who are native speakers of the target language
ranked the MT translations into 4 ranks: A, B, C,
and D, from good to bad (Sumita et al., 1999). 3
RED rank: An automatic ranker is learned as a
decision tree from HUMAN-ranked examples. It
exploits edit-distances between MT and multiple
reference translations (Akiba et al., 2001).
(2) BLEU score: The MT translations are scored
based on the precision of N-grams in an entire set
of multiple reference translations (Papineni et al.,
2002). It ranges from 1.0 (best) down to 0.0
(worst).
(3) Estimated TOEIC score: It is important to
interpret MT performance from the viewpoint of a
language proficiency test such as TOEIC4. A trans-
lator compared MT translations with human ones,
then, MT&apos;s proficiency is estimated by regression
analysis (Sugaya et al., 2000). It ranges from 10
(lowest) to 990 points (perfect).
</listItem>
<subsectionHeader confidence="0.808102">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.98483396969697">
Table 1 wraps up the results. So far, SMT has been
applied mainly to language pairs of similar Euro-
pean languages. Skeptical opinions dominate about
2 Average is calculated: A, B, C, and D are assigned
values of 4, 3, 2, and 1, respectively, and their sum is
divided by the sentence count (345 in the experiment).
3 The final rank for each translation is the median of the
nine ranks given by independent evaluators.
4 TOEIC is an acronym for &apos;Test of English for
International Communication&apos;, which is an English
language proficiency test for people whose native
language is not English (http://www.chauncey.com/).
the effectiveness or applicability of SMT to dis-
similar language pairs. However, we implemented
SMT for translation between Japanese and English.
They are dissimilar in many points, such as word
order and lexical systems. We found that SAT,
which is an SMT, worked in both J-to-E and E-
to-J directions.
The EBMT systems, HPAT and D3, surpassed
SA7&apos; in the HUMAN rank. This is the reverse re-
sult obtained in a Verbmobil experiment (Ney,
2001) where an SMT system scored highest. We
are studying these interesting contradictory
observations.
Let&apos;s consider the relationships among the
HUMAN rank, the RED rank, and the BLEU score.
While RED accords with HUMAN, BLEU fails
to agree with HUMAN in the EJ evaluation.
One reason for this is that the BLEU score favors
SAT translations in that they are more similar to the
reference translation from the viewpoint of N-
grams.
</bodyText>
<tableCaption confidence="0.991272">
Table 1 Quality Evaluation of Three MTs5
</tableCaption>
<table confidence="0.9911786">
pair MT Average of Average of BLEU
JE HUMAN RED
SAT 3.21 3.44
HPAT 2.66 2.61
EJ 3.17 3.13
SAT 2.89 2.91
0.49
0.43
0.48
0.56
</table>
<bodyText confidence="0.999219666666667">
Let&apos;s move on to the estimated TOEIC score of
the most accurate JE system in the experiment. D3
achieved a high score of 870. This is more than
one hundred points higher than the average score
of a Japanese businessperson in an overseas de-
partment of a company.
</bodyText>
<sectionHeader confidence="0.996736" genericHeader="method">
4 Corpus-based Selector
</sectionHeader>
<bodyText confidence="0.999967142857143">
This section introduces a feature of C3: selection of
the best from outputs produced by multiple transla-
tion engines.
No single system can achieve complete transla-
tion of every input. The quality rank of a given
input sentence changes system by system. We
show a sample of different English translations
</bodyText>
<footnote confidence="0.4835095">
5 HPAT for JE and D3 for EJ also work well, but we
omitted them from the table because we could not
afford the time and cost of the human evaluation for
them.
</footnote>
<page confidence="0.99304">
172
</page>
<bodyText confidence="0.9883575">
obtained by the three systems for the Japanese sen-
tence, &apos;o-shiharai wa genkin desu ka kurejitto
kaado desu ka&apos; (Table 2). The brackets show the
HUMAN rank, as described above.
</bodyText>
<tableCaption confidence="0.986426">
Table 2. Sample of Translation Variety
</tableCaption>
<bodyText confidence="0.991748857142857">
[B] Is the payment cash? Or is it the credit card?
[A] Would you like to pay in cash or with a credit card?
[C] Could you cash or credit card?
In our experiment, while D3, HPAT, and SAT for
the E-to-J direction have A-ratios of 0.62, 0.55,
and 0.53, respectively, the ideal selection would
have an interestingly high A-ratio of 0.79. Thus,
we could obtain a large increase in accuracy if it
were possible to select the best one of the three
different translations for each input sentence.
Unlike other approaches such as (Brown and
Frederking, 1995), we do not merge multiple re-
sults into a single one but we select the best one
because the large difference between multiple
translations for distant language pairs such as Japa-
nese and English makes merging infeasible.
Methods using N-gram statistics of a target lan-
guage corpus have been proposed before (Brown
and Frederking, 1995; Callison-Burch et al., 2001).
They are based on the assumptions that (1) the
naturalness of the translations is effective for se-
lecting good translations because they are sensitive
to the broken target sentences due to errors in
translation processes, and (2) the source and target
correspondences from the semantic point of view
are maintained in a state-of-the-art translation sys-
tem. However, the second assumption does not
necessarily hold. To solve this problem, Akiba et al.
(2002) used not only a language model but also a
translation model of SMT derived from a corpus,
and Sumita et al. (2002) exploited a corpus whose
sentences are converted into semantic class se-
quences. These two selectors outperformed con-
ventional selectors using the target N-gram in our
experiments.
</bodyText>
<sectionHeader confidence="0.861236" genericHeader="method">
5 Paraphrasing and Filtering
</sectionHeader>
<bodyText confidence="0.999694028571428">
This section introduces another feature of C3:
paraphrasing and filtering corpora.
The large variety of possible translations in a
corpus causes difficulty in building machine trans-
lation on the corpus. For example, the variety
makes it harder to estimate the parameters for SAT,
to find appropriate translation examples for D3, to
extract good transfer patterns for HPAT. We pro-
pose ways to overcome these problems by para-
phrasing corpora through automated processes or
filtering corpora by abandoning inappropriate ex-
pressions.
Two methods have been investigated for auto-
matic paraphrasing. (1) Shimohata et al. (2002a)
group sentences by the equivalence of the transla-
tion and extract rules of paraphrasing by DP-
matching. (2) Finch et al. (2002) cluster sentences
in a handcrafted paraphrase corpus (Sugaya et al.,
2002) to obtain pairs that are similar to each other
for training SMT models, then by using the models
the decoder generates a paraphrase. The experi-
mental results indicate that (i) the EBMT based on
normalization had increased coverage (Shimohata
et al., 2002b) and (ii) the SMT created on the nor-
malized sentences had a reduced word-error-rate
(Watanabe et al., 2002a).
Imamura et al. (2003) proposed a calculation
that measures the literalness of a translation pair
and called it TCR. After the word alignment of a
translation pair, TCR is calculated as the rate of the
aligned word count over the count of words in the
translation pair. After abandoning the non-literal
parts of the corpus, the acquisition of HPAT trans-
fer patterns is done. The effect has been confirmed
by an improvement in translation quality.
</bodyText>
<sectionHeader confidence="0.991871" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999941">
Our project, called C3, places corpora at the center
of speech-to-speech technology. Good perform-
ance in translation components is demonstrated in
the experiment. In addition, the corpus-based proc-
esses of translation, evaluation, and paraphrasing
have synergistic effects. Therefore, we are optimis-
tic about the further progress of components and
their integration.
</bodyText>
<sectionHeader confidence="0.970078" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999372">
The research reported here was supported in part
by a contract with the Telecommunications
Advancement Organization of Japan entitled, &amp;quot;A
study of speech dialogue translation technology
based on a large corpus.&amp;quot;
</bodyText>
<sectionHeader confidence="0.93136" genericHeader="references">
References
</sectionHeader>
<page confidence="0.998931">
173
</page>
<reference confidence="0.998319282608696">
Alshawi, H., Bangalore, S. and Douglas, S. 2000.
Learning Dependency Translation Models as
Collections of Finite-State Head Transducers,
Computational Linguistics, 26 (1), pp. 45--60.
Akiba, Y, Imamura, K., and Sumita, E., 2001. Us-
ing multiple edit distances to automatically rank
machine translation output. In Proc. of MTSum-
mit VIII, pages 15-20.
Akiba, Y., Watanabe, T., and Sumita, E. 2002.
&amp;quot;Using Language and Translation Models to Se-
lect the Best among Outputs from Multiple MT
Systems, Proc. of Coling, pp. 8-14.
Brown, P., Cocke, J., Della Pietra, S. A., Della
Pietra, V. J., Jelinek, F., Laffetry, J. D., Mercer,
R. L., and Roossin, P. S., 1993. A Statistical
Approach to Machine Translation, Computa-
tional Linguistics 16, pp. 79-85.
Brown, R. and Frederking, R., 1995. Applying Sta-
tistical English Language Modeling to Symbolic
Machine Translation. In Proc. of the 6th TMI, pp.
221-239.
Callison-Burch, C. and Flournoy, S., 2001. A Pro-
gram for Automatically Selecting the Best Out-
put from Multiple Machine Translation Engines,
Proc. of MT-SUMMIT.
Finch, A, Watanabe, T., and Sumita, E., Paraphras-
ing by Statistical Machine Translation, 2002,
Proc. of FIT, E-53, pp.187 188.
Imamura, K., 2002. Application of Translation
Knowledge Acquired by Hierarchical Phrase
Alignment, Proc. of TMI.
Imamura, K., Sumita, E. and Matsumoto, Y., 2003.
Automatic Construction of Machine Translation
Knowledge Using Translation Literality, Proc.
of EACL.
Knight, K., 1997. Automating Knowledge Acquisi-
tion for Machine Translation, Al Magazine, 18
(4), pp. 81--96
Nagao, M., 1984. A Framework of a Mechanical
Translation between Japanese and English by
Analogy Principle, in A. Elithorn and R. Banerji
(eds), Artificial and Human Intelligence, Am-
sterdam: North-Holland, pp. 173--180.
Ney, H., 2001. Stochastic Modeling: From pattern
classification to language translation, in Proc. of
the ACL 2001 Workshop on DDMT, pp. 33--37.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.
J., 2002. Bleu: A Method for Automatic Evalua-
tion of Machine Translation, Proc. of the 406
ACL, pp. 311 318.
Shimohata, M. and Sumita, E., 2002a. Automatic
paraphrasing based on parallel corpus for nor-
malization, Proc. of LREC.
Shimohata, M. and Sumita, E., 2002b. &amp;quot;Identifying
Synonymous Expressions from a Bilingual Cor-
pus for Example-Based Machine Translation,&amp;quot;
Proc. of the Workshop on Machine Translation
in Asia, Coling, pp. 20--25.
Somers, H., 1999. Review Article: Example-based
Machine Translation, Journal of Machine
Translation, pp. 113-157.
Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y.,
and Yamamoto, S., 2000. Evaluation of the
ATR-MATRIX Speech Translation System with
a Pair Comparison Method Between the System
and Humans, Proc. of ICSLP, pp. 1105-1108.
Sugaya, F., Takezawa, T., Kikui, G. and Yama-
moto, S., 2002. Proposal of a very-large-corpus
acquisition method by cell-formed registration,
Proceedings of the LREC.
Sumita, E., 2001. Example-based machine transla-
tion using DP-matching between word se-
quences, Proc. of ACL 2001 Workshop on
DDMT, pp. 1 8.
Sumita, E., Akiba, Y., and Imamura, K., 2002.
&amp;quot;Reliability Measures For Translation Quality,&amp;quot;
ICSLP, pp. 1893-1896.
Sumita, E., Yamada, S., Yamamoto, K., Paul, M.,
Kashioka, H., Ishikawa, K., and Shirai, S., 1999.
Solutions to Problems Inherent in Spoken-
language Translation: The ATR-MATRIX Ap-
proach, Proc. of MT Summit, pp. 229-235.
Takezawa, T. et al., 2002. Toward a Broad-
coverage Bilingual Corpus for Speech Transla-
tion of Travel Conversations in the Real World,
Proc. of LREC.
Watanabe, T. et al., 2002a. Statistical Machine
Translation Based on Paraphrased Corpora, Proc.
of LREC.
Watanabe, T. et al., 2002b. &amp;quot;Bidirectional Decod-
ing for Statistical Machine Translation,&amp;quot; Proc. of
Coling, pp. 1079-1085.
</reference>
<page confidence="0.998422">
174
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658054">
<title confidence="0.999979">A Corpus-Centered Approach to Spoken Language Translation</title>
<author confidence="0.963834">Eiichiro SUMITA</author>
<author confidence="0.963834">Yasuhiro AKIBA</author>
<author confidence="0.963834">Takao DO</author>
<author confidence="0.963834">Andrew FINCH</author>
<author confidence="0.963834">Kenji IMAMURA</author>
<author confidence="0.963834">Michael PAUL</author>
<author confidence="0.963834">Mitsuo SHIMOHATA</author>
<author confidence="0.963834">Taro WATANABE</author>
<affiliation confidence="0.998999">ATR Spoken Language Translation Research Laboratories</affiliation>
<address confidence="0.98365">2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, JAPAN</address>
<email confidence="0.721384">eiichirosumitagatr.cojp</email>
<abstract confidence="0.999082538461538">reports the latest performance of comand features of a project named Corpus- Computation targets a translation technology suitable for spoken language places the center of the Translation knowledge is corpora both EBMT and SMT methods, quality is by referring to corbest translation among multiple-engine is based on corpora the corthemselves are paraphrased or filtered automated processes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning Dependency Translation Models as</title>
<date>2000</date>
<journal>Collections of Finite-State Head Transducers, Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>45--60</pages>
<contexts>
<context position="1625" citStr="Alshawi et al., 2000" startWordPosition="230" endWordPosition="234">uality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Alshawi, H., Bangalore, S. and Douglas, S. 2000. Learning Dependency Translation Models as Collections of Finite-State Head Transducers, Computational Linguistics, 26 (1), pp. 45--60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Akiba</author>
<author>K Imamura</author>
<author>E Sumita</author>
</authors>
<title>Using multiple edit distances to automatically rank machine translation output.</title>
<date>2001</date>
<booktitle>In Proc. of MTSummit VIII,</booktitle>
<pages>15--20</pages>
<contexts>
<context position="3943" citStr="Akiba et al., 2001" startWordPosition="608" endWordPosition="611">ore and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks2: HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC4. A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been appl</context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Akiba, Y, Imamura, K., and Sumita, E., 2001. Using multiple edit distances to automatically rank machine translation output. In Proc. of MTSummit VIII, pages 15-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Akiba</author>
<author>T Watanabe</author>
<author>E Sumita</author>
</authors>
<title>Using Language and Translation Models to Select the Best among Outputs from Multiple MT Systems,</title>
<date>2002</date>
<booktitle>Proc. of Coling,</booktitle>
<pages>8--14</pages>
<contexts>
<context position="8477" citStr="Akiba et al. (2002)" startWordPosition="1379" endWordPosition="1382">ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3: paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for </context>
</contexts>
<marker>Akiba, Watanabe, Sumita, 2002</marker>
<rawString>Akiba, Y., Watanabe, T., and Sumita, E. 2002. &amp;quot;Using Language and Translation Models to Select the Best among Outputs from Multiple MT Systems, Proc. of Coling, pp. 8-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
<author>F Jelinek</author>
<author>J D Laffetry</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<pages>79--85</pages>
<contexts>
<context position="1577" citStr="Brown et al., 1993" startWordPosition="222" endWordPosition="225">lutions for efficiently constructing a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phra</context>
</contexts>
<marker>Brown, Cocke, Pietra, A, Pietra, J, Jelinek, Laffetry, Mercer, Roossin, 1993</marker>
<rawString>Brown, P., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Laffetry, J. D., Mercer, R. L., and Roossin, P. S., 1993. A Statistical Approach to Machine Translation, Computational Linguistics 16, pp. 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
<author>R Frederking</author>
</authors>
<title>Applying Statistical English Language Modeling to Symbolic Machine Translation.</title>
<date>1995</date>
<booktitle>In Proc. of the 6th TMI,</booktitle>
<pages>221--239</pages>
<contexts>
<context position="7654" citStr="Brown and Frederking, 1995" startWordPosition="1248" endWordPosition="1251">ow the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3, HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the </context>
</contexts>
<marker>Brown, Frederking, 1995</marker>
<rawString>Brown, R. and Frederking, R., 1995. Applying Statistical English Language Modeling to Symbolic Machine Translation. In Proc. of the 6th TMI, pp. 221-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>S Flournoy</author>
</authors>
<title>A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines,</title>
<date>2001</date>
<booktitle>Proc. of MT-SUMMIT.</booktitle>
<marker>Callison-Burch, Flournoy, 2001</marker>
<rawString>Callison-Burch, C. and Flournoy, S., 2001. A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines, Proc. of MT-SUMMIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>T Watanabe</author>
<author>E Sumita</author>
</authors>
<title>Paraphrasing by Statistical Machine Translation,</title>
<date>2002</date>
<booktitle>Proc. of FIT, E-53,</booktitle>
<pages>187--188</pages>
<contexts>
<context position="9550" citStr="Finch et al. (2002)" startWordPosition="1547" endWordPosition="1550">a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3, to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation </context>
</contexts>
<marker>Finch, Watanabe, Sumita, 2002</marker>
<rawString>Finch, A, Watanabe, T., and Sumita, E., Paraphrasing by Statistical Machine Translation, 2002, Proc. of FIT, E-53, pp.187 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Imamura</author>
</authors>
<title>Application of Translation Knowledge Acquired by Hierarchical Phrase Alignment,</title>
<date>2002</date>
<booktitle>Proc. of TMI.</booktitle>
<contexts>
<context position="2361" citStr="Imamura 2002" startWordPosition="347" endWordPosition="348">nslation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f</context>
</contexts>
<marker>Imamura, 2002</marker>
<rawString>Imamura, K., 2002. Application of Translation Knowledge Acquired by Hierarchical Phrase Alignment, Proc. of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Imamura</author>
<author>E Sumita</author>
<author>Y Matsumoto</author>
</authors>
<title>Automatic Construction of Machine Translation Knowledge Using Translation Literality,</title>
<date>2003</date>
<booktitle>Proc. of EACL.</booktitle>
<contexts>
<context position="10013" citStr="Imamura et al. (2003)" startWordPosition="1622" endWordPosition="1625">g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3, places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment.</context>
</contexts>
<marker>Imamura, Sumita, Matsumoto, 2003</marker>
<rawString>Imamura, K., Sumita, E. and Matsumoto, Y., 2003. Automatic Construction of Machine Translation Knowledge Using Translation Literality, Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Automating Knowledge Acquisition for Machine Translation,</title>
<date>1997</date>
<journal>Al Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>81--96</pages>
<contexts>
<context position="1591" citStr="Knight, 1997" startWordPosition="226" endWordPosition="227">tly constructing a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bili</context>
</contexts>
<marker>Knight, 1997</marker>
<rawString>Knight, K., 1997. Automating Knowledge Acquisition for Machine Translation, Al Magazine, 18 (4), pp. 81--96</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A Framework of a Mechanical Translation between Japanese and English by Analogy Principle,</title>
<date>1984</date>
<booktitle>in A. Elithorn and R. Banerji (eds), Artificial and Human Intelligence,</booktitle>
<pages>173--180</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="1496" citStr="Nagao, 1984" startWordPosition="212" endWordPosition="213">roduction Our project, named Corpus-Centered Computation ((3), proposes solutions for efficiently constructing a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved exa</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Nagao, M., 1984. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle, in A. Elithorn and R. Banerji (eds), Artificial and Human Intelligence, Amsterdam: North-Holland, pp. 173--180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>Stochastic Modeling: From pattern classification to language translation,</title>
<date>2001</date>
<booktitle>in Proc. of the ACL 2001 Workshop on DDMT,</booktitle>
<pages>33--37</pages>
<contexts>
<context position="1602" citStr="Ney, 2001" startWordPosition="228" endWordPosition="229">ng a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees</context>
<context position="5544" citStr="Ney, 2001" startWordPosition="877" endWordPosition="878">st of English for International Communication&apos;, which is an English language proficiency test for people whose native language is not English (http://www.chauncey.com/). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D3, surpassed SA7&apos; in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs5 pair MT Average of Average of BLEU JE HUMAN RED SAT 3.21 3.44 HPAT 2.66 2.61 EJ 3.17 3.13 SAT 2.89 2.91 0.49 0.43 0.48 0.56 Let&apos;s mo</context>
</contexts>
<marker>Ney, 2001</marker>
<rawString>Ney, H., 2001. Stochastic Modeling: From pattern classification to language translation, in Proc. of the ACL 2001 Workshop on DDMT, pp. 33--37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: A Method for Automatic Evaluation of Machine Translation,</title>
<date>2002</date>
<booktitle>Proc. of the 406 ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4101" citStr="Papineni et al., 2002" startWordPosition="634" endWordPosition="637"> and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks2: HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC4. A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about 2 Average is calculated: A, B, C, and D are assigned values of 4</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W. J., 2002. Bleu: A Method for Automatic Evaluation of Machine Translation, Proc. of the 406 ACL, pp. 311 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimohata</author>
<author>E Sumita</author>
</authors>
<title>Automatic paraphrasing based on parallel corpus for normalization,</title>
<date>2002</date>
<booktitle>Proc. of LREC.</booktitle>
<marker>Shimohata, Sumita, 2002</marker>
<rawString>Shimohata, M. and Sumita, E., 2002a. Automatic paraphrasing based on parallel corpus for normalization, Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimohata</author>
<author>E Sumita</author>
</authors>
<title>Identifying Synonymous Expressions from a Bilingual Corpus for Example-Based Machine Translation,&amp;quot;</title>
<date>2002</date>
<booktitle>Proc. of the Workshop on Machine Translation in Asia, Coling,</booktitle>
<pages>20--25</pages>
<marker>Shimohata, Sumita, 2002</marker>
<rawString>Shimohata, M. and Sumita, E., 2002b. &amp;quot;Identifying Synonymous Expressions from a Bilingual Corpus for Example-Based Machine Translation,&amp;quot; Proc. of the Workshop on Machine Translation in Asia, Coling, pp. 20--25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
</authors>
<title>Review Article: Example-based Machine Translation,</title>
<date>1999</date>
<journal>Journal of Machine Translation,</journal>
<pages>113--157</pages>
<contexts>
<context position="1511" citStr="Somers, 1999" startWordPosition="214" endWordPosition="215"> project, named Corpus-Centered Computation ((3), proposes solutions for efficiently constructing a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3. Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using d</context>
</contexts>
<marker>Somers, 1999</marker>
<rawString>Somers, H., 1999. Review Article: Example-based Machine Translation, Journal of Machine Translation, pp. 113-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>A Yokoo</author>
<author>Y Sagisaka</author>
<author>S Yamamoto</author>
</authors>
<date>2000</date>
<booktitle>Evaluation of the ATR-MATRIX Speech Translation System with a Pair Comparison Method Between the System and Humans, Proc. of ICSLP,</booktitle>
<pages>1105--1108</pages>
<contexts>
<context position="4422" citStr="Sugaya et al., 2000" startWordPosition="686" endWordPosition="689">as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC4. A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about 2 Average is calculated: A, B, C, and D are assigned values of 4, 3, 2, and 1, respectively, and their sum is divided by the sentence count (345 in the experiment). 3 The final rank for each translation is the median of the nine ranks given by independent evaluators. 4 TOEIC is an acronym for &apos;Test of English for International Communication&apos;, which is an English language proficiency</context>
</contexts>
<marker>Sugaya, Takezawa, Yokoo, Sagisaka, Yamamoto, 2000</marker>
<rawString>Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y., and Yamamoto, S., 2000. Evaluation of the ATR-MATRIX Speech Translation System with a Pair Comparison Method Between the System and Humans, Proc. of ICSLP, pp. 1105-1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>G Kikui</author>
<author>S Yamamoto</author>
</authors>
<title>Proposal of a very-large-corpus acquisition method by cell-formed registration,</title>
<date>2002</date>
<booktitle>Proceedings of the LREC.</booktitle>
<contexts>
<context position="9625" citStr="Sugaya et al., 2002" startWordPosition="1558" endWordPosition="1561">For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3, to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun</context>
</contexts>
<marker>Sugaya, Takezawa, Kikui, Yamamoto, 2002</marker>
<rawString>Sugaya, F., Takezawa, T., Kikui, G. and Yamamoto, S., 2002. Proposal of a very-large-corpus acquisition method by cell-formed registration, Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
</authors>
<title>Example-based machine translation using DP-matching between word sequences,</title>
<date>2001</date>
<booktitle>Proc. of ACL 2001 Workshop on DDMT,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2137" citStr="Sumita 2001" startWordPosition="314" endWordPosition="315">istical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3, HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra</context>
</contexts>
<marker>Sumita, 2001</marker>
<rawString>Sumita, E., 2001. Example-based machine translation using DP-matching between word sequences, Proc. of ACL 2001 Workshop on DDMT, pp. 1 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>Y Akiba</author>
<author>K Imamura</author>
</authors>
<title>Reliability Measures For Translation Quality,&amp;quot; ICSLP,</title>
<date>2002</date>
<pages>1893--1896</pages>
<contexts>
<context position="8592" citStr="Sumita et al. (2002)" startWordPosition="1401" endWordPosition="1404">Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3: paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3, to extract good transfer patterns for HPAT. We propose ways t</context>
</contexts>
<marker>Sumita, Akiba, Imamura, 2002</marker>
<rawString>Sumita, E., Akiba, Y., and Imamura, K., 2002. &amp;quot;Reliability Measures For Translation Quality,&amp;quot; ICSLP, pp. 1893-1896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>S Yamada</author>
<author>K Yamamoto</author>
<author>M Paul</author>
<author>H Kashioka</author>
<author>K Ishikawa</author>
<author>S Shirai</author>
</authors>
<title>Solutions to Problems Inherent in Spokenlanguage Translation: The ATR-MATRIX Approach,</title>
<date>1999</date>
<booktitle>Proc. of MT Summit,</booktitle>
<pages>229--235</pages>
<contexts>
<context position="3757" citStr="Sumita et al., 1999" startWordPosition="580" endWordPosition="583">akezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks2: HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC4. A translator compared MT translations with human ones, then, MT&apos;s prof</context>
</contexts>
<marker>Sumita, Yamada, Yamamoto, Paul, Kashioka, Ishikawa, Shirai, 1999</marker>
<rawString>Sumita, E., Yamada, S., Yamamoto, K., Paul, M., Kashioka, H., Ishikawa, K., and Shirai, S., 1999. Solutions to Problems Inherent in Spokenlanguage Translation: The ATR-MATRIX Approach, Proc. of MT Summit, pp. 229-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
</authors>
<title>Toward a Broadcoverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World,</title>
<date>2002</date>
<booktitle>Proc. of LREC.</booktitle>
<marker>Takezawa, 2002</marker>
<rawString>Takezawa, T. et al., 2002. Toward a Broadcoverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World, Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
</authors>
<date>2002</date>
<booktitle>Statistical Machine Translation Based on Paraphrased Corpora, Proc. of LREC.</booktitle>
<marker>Watanabe, 2002</marker>
<rawString>Watanabe, T. et al., 2002a. Statistical Machine Translation Based on Paraphrased Corpora, Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
</authors>
<title>Bidirectional Decoding for Statistical Machine Translation,&amp;quot;</title>
<date>2002</date>
<booktitle>Proc. of Coling,</booktitle>
<pages>1079--1085</pages>
<marker>Watanabe, 2002</marker>
<rawString>Watanabe, T. et al., 2002b. &amp;quot;Bidirectional Decoding for Statistical Machine Translation,&amp;quot; Proc. of Coling, pp. 1079-1085.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>