<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000768">
<title confidence="0.988411">
Lexicalized Grammar Acquisition
</title>
<author confidence="0.988302">
Yusuke Miyaot Takashi Ninorniyat Jun&apos;ichi Tsujiit
</author>
<affiliation confidence="0.994878">
f Department of Computer Science, University of Tokyo
</affiliation>
<address confidence="0.721621">
Bongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
$CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
</address>
<email confidence="0.39301">
{yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp
</email>
<sectionHeader confidence="0.99354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969857142857">
This paper presents a formalization
of automatic grammar acquisition that
is based on lexicalized grammar for-
malisms (e.g. LTAG and HPSG). We
state the conditions for the consistent ac-
quisition of a unique lexicalized gram-
mar from an annotated corpus.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966267857143">
Linguistically motivated and computationally ori-
ented grammar theories take the form of lexical-
ized grammar formalisms; examples include Lex-
icalized Tree Adjoining Grammar (LTAG) (Sch-
abes et al., 1988), Combinatory Categorial Gram-
mar (Steedman, 2000), and Head-driven Phrase
Structure Grammar (HPSG) (Sag and Wasow,
1999). They have been a great success in terms
of linguistic analysis and efficiency in the parsing
of real-world texts. However, such grammars have
not generally been considered suitable for the syn-
tactic analysis within practical NLP systems be-
cause considerable effort is required to develop
and maintain lexicalized grammars that are both
robust and provide broad coverage.
One novel approach to grammar development is
based on the automatic acquisition of lexicalized
grammars from annotated corpora. Since lexical-
ized grammars represent grammatical constraints
with a few grammar rules and a large number of
lexical entries, the rules are quite easy to write but
the construction of a lexicon is unrealistic. The
idea in this study is to automatically obtain the lex-
ical entries from an annotated corpus, which will
greatly reduce the cost of building the grammar.
The approach has the following advantages.
First, the grammars obtained will be robust be-
cause appropriate lexical entries are consistently
acquired even for constructions beyond grammar
developers&apos; intuition. Secondly, the grammar de-
velopers are simply required to annotate the closed
set of the training corpus, where various heuris-
tic and statistical methods are applicable. Con-
sistency between the grammar rules and the ob-
tained lexical entries is assured independently of
the methods of annotation. Lastly, the validity of
the grammar theories is evaluated on real-world
texts. A degree of low coverage by a linguistically
motivated grammar does not necessarily reflect in-
adequacy of the grammar theories; a lack of appro-
priate lexical entries may also be responsible. The
analysis of obtained grammars gives us grounds
for discussing the pros and cons of the theories.
The studies on the extraction of LTAG (Xia,
1999; Chen and Vijay-Shanker, 2000; Chiang,
2000) and CCG (Hockenmaier and Steedman,
2002) represent the first attempts at the acquisition
of linguistically motivated grammars from anno-
tated corpora. Those studies are limited to specific
formalisms, and can be interpreted as instances of
our approach as described in Section 3. This pa-
per does not describe any concrete algorithms for
grammar acquisition that depend on specific gram-
mar formalisms. The contribution of our work is
to formally state the conditions required for the ac-
quisition of lexicalized grammars and to demon-
</bodyText>
<page confidence="0.995936">
127
</page>
<bodyText confidence="0.99816">
strate that it can be applied to lexicalized gram-
mars other than LTAG and CCG, such as HPSG.
</bodyText>
<sectionHeader confidence="0.950286" genericHeader="method">
2 Lexicalized Grammars
</sectionHeader>
<bodyText confidence="0.999848916666667">
In this section, we define the general form of lexi-
calized grammars including LTAG (Schabes et al.,
1988) and HPSG (Sag and Wasow, 1999). The
concepts behind a lexicalized grammar are that
i) grammar rules represent general grammatical
constructions in the language while ii) lexical en-
tries describe word-specific lexicallsyntactic con-
straints. Let W be a set of all words and C a set of
linguistic representations (e.g. the tree structures
of LTAG or typed feature structures of HPSG). We
can formally define a lexicalized grammar in the
following way.
</bodyText>
<construct confidence="0.9821022">
Definition 1 (Lexicalized grammar) A lexical-
ized grammar is a tuple G = (L, R), where L
is a lexicon, i.e. Lc Wx C, and R is a set of
grammar rules, i.e. r E R is a partial function:
C x C C.
</construct>
<bodyText confidence="0.962286692307692">
In what follows, we assume that all grammar rules
are binary for simplicity.
Parsing with a lexicalized grammar is the pro-
cess of applying grammar rules to lexical entries.
Since we assume that the rules are binary, the his-
tory of the process constitutes a binary-branching
tree; we define such structures in this paper as con-
stituent structures&apos; (Miller, 2000).
Definition 2 (Constituent structure) Given a
sentence w, a constituent structure of w is the
least set F c IV satisfying i) w e F and ii)
V7 G F.(171 &gt; 1 71, 72 G F.(7 = 7172))
The first condition represents inclusion of the top
node in the structure, and the second constrains the
terminals (words) to a linear order.
The process of parsing is then depicted by a
constituent structure labeled with grammar rules,
which we call a derivation history.
Definition 3 (Derivation history) Given a sen-
tence w, a derivation history is a tuple T = Kr, p),
where F is a constituent structure of w and p is a
function: F R. Derivation history T = F, p)
is a well-formed derivation history iff there exists
a function satisfying the following conditions.
&apos;Note that the constituent structures we define here repre-
sent the process of parsing rather than the result of parsing.
</bodyText>
<listItem confidence="0.975610666666667">
• ew E w. ](w, e L A =c
• V7 E F. 71.72] E F.7 = 7172 A -(7) =
P(7)((71), (72))
</listItem>
<bodyText confidence="0.986732266666667">
Let ai be &amp;quot;(7i) or 7i, we denote (7) al ...
when 7 =
The results of parsing (e.g. derived trees of
LTAG and phrasal signs of HPSG) by a lexicalized
grammar are outputs of the application of rules ac-
cording to derivation histories.
Definition 4 (Parse result) Given a sentence w,
c, E C is a parse result of w iff c, w for some
well-formed derivation history.
Given the above definitions, our task is to ob-
tain lexical entries (w, c) E L from a parsed cor-
pus, i.e., parse results C, E C. The idea is to
make derivation histories from a parsed corpus,
and reduce the parse results into lexical entries by
traversing the derivation histories.
</bodyText>
<sectionHeader confidence="0.998105" genericHeader="method">
3 Grammar Acquisition
</sectionHeader>
<bodyText confidence="0.976425421052632">
Given Definitions 1, 3 and 4, we can deter-
mine unique lexical entries from a parse re-
sult if a derivation history is given and the
grammar rules r E R are injective functions,
i.e., Ve. Eel, c2, , c/2. (c = r(ci, c2) A c
r (ci ci2)) (Cl = c1 A c2 = c&apos;2)
Theorem 1 (Grammar acquisition) Given cs
C as the parse result of sentence w, and T as
its derivation history, lexical entries for w are
uniquely determined if all grammar rules r e R
are infective functions. That is, ]!ci, ,c„ E
C. (Vi &lt; n. (wi, CL A cs c1
Proof. We construct lexical entries by inversely
applying the grammar rules to the parse result.
Since the grammar rules are infective, we are able
to uniquely determine the inputs for any rule ap-
plication. That is, V7 E F. ]!c C CA01)2 E
C5. c5 Oic,32 A c 7. We prove this lemma
by the mathematical induction of the length of 7.
</bodyText>
<figure confidence="0.781998761904762">
I. When 1-y1 = = w according to Defini-
tion 2. This case is trivial.
2. When 71 = k, we assume that the lemma
is true for Vi &gt; k. From Definition 2,
C F. y 77 V = 7&amp;quot;7.
128
[-HEADno,m1„--head-filler
LSLASH
NP [HEAD verb
LSLASH &lt;Np&gt;
head-subject
,head-complement
the girl rip HAD verbp,
LENH
he
talked /PP
&amp;quot;about
-HEAD verb -HEADVerb
talked: 88%s3r; talked: 88V,t6sW:
_SLASH &lt;&gt; L _SLASH NP&lt;
about: Etirkai 117p,:j about: pgissi P.7 ]
</figure>
<figureCaption confidence="0.999967">
Figure 1: A non-injective grammar rule in HPSG
</figureCaption>
<bodyText confidence="0.900195633333333">
Without loss of generality, we assume ^,1 =
77&amp;quot;. By the assumption of the induction,
]!ci cs 13102 A c&apos; . Since p(71)
is injective, E!c, . c&apos; = p(7&apos;)(c, c&amp;quot;). Hence,
]!c, c&amp;quot;. c, 131cc&amp;quot; 132. By Definition 3, c
We thus find that unique c exists for
The lemma is proved by 1 and 2. According to the
lemma, Vw C w. ]!c. c, =- 13102 A c
Hence, the theorem has been proved. 111
The theorem shows that we are able to obtain a
unique sequence of lexical entries that are consis-
tent with the grammar rules, given the constituent
structures and corresponding rule applications.
However, the grammar rules of a lexicalized
grammar are not necessarily injective. For exam-
ple, Figure 1 shows a situation where the SLASH
feature of HPSG causes the same parse result for
distinct inputs. Phrase &amp;quot;talk about&amp;quot; has an NP
in the SLASH feature, but we cannot determine
the origination of the NP. A similar argument can
be made for LTAG on its adjunction rule. When
an auxiliary tree is adjoined into another tree, its
spine is melted into the other tree, which produces
the same result for distinct inputs.
To enable the acquisition of unique lexical en-
tries even when the grammar rules are not injec-
tive, we introduce a further definition that provides
a mark which disambiguates the inputs to gram-
mar rules. This allows us to map non-injective
rules into injective rules.
</bodyText>
<equation confidence="0.5995345">
Definition 5 (Pseudo-injective) Grammar rule
r C R is pseudo-injective when there exists a func-
</equation>
<bodyText confidence="0.99791193877551">
tion p such that r„ = c2.(r(ci, c2), P(ci , c2))
is an injective function. We call it a marking
function, and use RA to denote a set of r)„.
If all grammar rules are at least pseudo-injective,
unique lexical entries are determined Theorem 1
is now revised into a more general form.
Theorem 2 (Grammar acquisition (revised))
Given a parse result c, C C of a sentence w, a
marking function 11, and a derivation history T of
R„, lexical entries for w are uniquely determined
if all grammar rules r C R are pseudo-injective
functions with respect to it.
Proof. We omit the proof since it is much the
same as the proof of Theorem].
Non-injective grammar rules in lexicalized
grammars can often be redefined as pseudo-
injective functions by the addition of some infor-
mation, which depends on grammar theories. We
now discuss grammar-theory-dependent issues.
CG The injective condition is apparently pre-
served in a simple CG, which is the class of cat-
egorial grammars composed of the two reduction
rules: Forward Application (FA) and Backward
Application (BA). While these rules take two cat-
egories as inputs and output another category, we
can regard the rules as taking two derived trees as
inputs and outputting a combined tree. With this
insight, we can regard reduction rules as grammar
rules, derived trees as parse results, derivations as
derivation histories, then Theorem 1 can be ap-
plied. The annotation cost will not be problematic
because we only need to annotate FA or BA to the
derived trees by using simple heuristic rules.
This argument is also valid for the extraction
of CCG. In order to annotate the other rules de-
fined by CCG (type-raising, composition, and co-
ordination rules), existing work (Hockenmaier and
Steedman, 2002) exploits the annotations of traces
and coordinations in the Penn Treebank.
LTAG As discussed above, adjoining leads to
the situations where the injective condition is vi-
olated. We can make the grammar rules pseudo-
injective by determining the lengths of the spines,
i.e., defining it as returning the spine length. Exist-
ing studies assume the length as one (Xia, 1999),
or determine the length by using heuristic rules
(Chen and Vijay-Shanker, 2000; Chiang, 2000).
The above discussion was empirically eval-
uated by acquiring LTAG grammars from the
</bodyText>
<page confidence="0.994579">
129
</page>
<table confidence="0.9998846">
Grammar (a) Grammar (b)
# words 4,514 # words 4,539
# initial trees 1,015 # initial trees 1,068
# aux. trees 1,094 # aux. trees 1,107
coverage 93.4 % coverage 94.0 %
</table>
<tableCaption confidence="0.758276">
Table 1: Specifications of the LTAG grammars ac-
quired from the Penn Treebank
</tableCaption>
<bodyText confidence="0.99945103030303">
Penn Treebank (Marcus et al., 1994). We as-
sumed that the length of all spines was one, i.e.,
Vci, c2. c2) = 1. 47 heuristic rules were
used for the annotation, which shows the annota-
tion was not so costly. A grammar was success-
fully acquired from Sections 02-21 in 910 sec-
onds on a Xeon 2.0 GHz CPU. As shown in Gram-
mar (a) in Table 1, the grammar had the sufficient
coverage (measured against Section 22, 1700 sen-
tences), which shows the robustness of the gram-
mar. We next acquired another grammar (Gram-
mar (b) in Table 1) by including two rules for an-
notating insertions and coordinations. Since the
grammar provides more accurate analyses for in-
sertions and coordinations, the coverage of the
grammar has been slightly improved.
HPSG In HPSG, two points lead to violate the
injective condition. One is ambiguity in terms
of the origin of subcategorization frames as de-
scribed in Section 3. In Figure 1, SLASH fea-
tures in HPSG are represented with a set, and
the mother&apos;s SLASH is the union of the daugh-
ters&apos; ones. The union operation is apparently
non-injective, but the grammar rules can be made
pseudo-injective by defining itt as telling the ori-
gin of SLASH features. The other is generaliza-
tion through the use of a type hierarchy. In HPSG,
linguistic generalizations are described by a type
hierarchy, where the entities are placed in gen-
eral/specific relations. Merging of types, unifica-
tion, is inherently not injective. The generaliza-
tion problem is beyond the scope of our study, and
must be left for future research.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999980352941177">
This study has demonstrated the conditions for ac-
quiring lexicalized grammars from annotated cor-
pora. We proved that a unique lexicalized gram-
mar consistent with the given grammar rules is ac-
quired when derivation histories are given. Our
approach enables the development of lexicalized
grammars that are robust and broad-coverage, and
also lets us compare various grammar theories
with real-world texts. This study provides a start-
ing point for the application of linguistic grammar
theories to real-world NLP systems.
Future work includes an application of our ap-
proach to other grammar theories, such as HPSG.
Although annotation of grammar rules (schemata)
might be more difficult than LTAG since they have
more rules, this will be solved by careful imple-
mentation of heuristic annotation rules.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998886">
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proc. 6th IWPT.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL 2000, pages 456-463.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with combinatory
categorial grammar. In Proc. 40th ACL, pages 335-
342.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In ARPA Human
Language Technology Workshop.
Philip H. Miller. 2000. Strong Generative Capacity:
The Semantics of Linguistic Formalism. CSLI Pub-
lications.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory —A Formal Introduction. CSLI Lcture Notes no.
92. CLSI Publications.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with `lexicalized gram-
mars&apos;: Application to tree adjoining grammars. In
Proc. 12th COLING, pages 578-583.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Fei Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proc. 5th NLPRS.
</reference>
<page confidence="0.997614">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635926">
<title confidence="0.998993">Lexicalized Grammar Acquisition</title>
<author confidence="0.998951">Miyaot Takashi Ninorniyat Jun&apos;ichi</author>
<affiliation confidence="0.998965">of Computer Science, University of Tokyo</affiliation>
<address confidence="0.927682">Bongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN</address>
<affiliation confidence="0.950441">CREST, JST (Japan Science and Technology Corporation)</affiliation>
<address confidence="0.940679">Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN</address>
<email confidence="0.790725">ninomi,tsujii}@issacjp</email>
<abstract confidence="0.989885625">This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of TAGs from the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proc. 6th IWPT.</booktitle>
<contexts>
<context position="2751" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="412" endWordPosition="415">us heuristic and statistical methods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized gramm</context>
<context position="11087" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="1898" endWordPosition="1901">d for the extraction of CCG. In order to annotate the other rules defined by CCG (type-raising, composition, and coordination rules), existing work (Hockenmaier and Steedman, 2002) exploits the annotations of traces and coordinations in the Penn Treebank. LTAG As discussed above, adjoining leads to the situations where the injective condition is violated. We can make the grammar rules pseudoinjective by determining the lengths of the spines, i.e., defining it as returning the spine length. Existing studies assume the length as one (Xia, 1999), or determine the length by using heuristic rules (Chen and Vijay-Shanker, 2000; Chiang, 2000). The above discussion was empirically evaluated by acquiring LTAG grammars from the 129 Grammar (a) Grammar (b) # words 4,514 # words 4,539 # initial trees 1,015 # initial trees 1,068 # aux. trees 1,094 # aux. trees 1,107 coverage 93.4 % coverage 94.0 % Table 1: Specifications of the LTAG grammars acquired from the Penn Treebank Penn Treebank (Marcus et al., 1994). We assumed that the length of all spines was one, i.e., Vci, c2. c2) = 1. 47 heuristic rules were used for the annotation, which shows the annotation was not so costly. A grammar was successfully acquired from Sectio</context>
</contexts>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>John Chen and K. Vijay-Shanker. 2000. Automated extraction of TAGs from the Penn Treebank. In Proc. 6th IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proc. ACL</booktitle>
<pages>456--463</pages>
<contexts>
<context position="2766" citStr="Chiang, 2000" startWordPosition="416" endWordPosition="417">ethods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than </context>
<context position="11102" citStr="Chiang, 2000" startWordPosition="1902" endWordPosition="1903">n order to annotate the other rules defined by CCG (type-raising, composition, and coordination rules), existing work (Hockenmaier and Steedman, 2002) exploits the annotations of traces and coordinations in the Penn Treebank. LTAG As discussed above, adjoining leads to the situations where the injective condition is violated. We can make the grammar rules pseudoinjective by determining the lengths of the spines, i.e., defining it as returning the spine length. Existing studies assume the length as one (Xia, 1999), or determine the length by using heuristic rules (Chen and Vijay-Shanker, 2000; Chiang, 2000). The above discussion was empirically evaluated by acquiring LTAG grammars from the 129 Grammar (a) Grammar (b) # words 4,514 # words 4,539 # initial trees 1,015 # initial trees 1,068 # aux. trees 1,094 # aux. trees 1,107 coverage 93.4 % coverage 94.0 % Table 1: Specifications of the LTAG grammars acquired from the Penn Treebank Penn Treebank (Marcus et al., 1994). We assumed that the length of all spines was one, i.e., Vci, c2. c2) = 1. 47 heuristic rules were used for the annotation, which shows the annotation was not so costly. A grammar was successfully acquired from Sections 02-21 in 910</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proc. ACL 2000, pages 456-463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<pages>335--342</pages>
<contexts>
<context position="2807" citStr="Hockenmaier and Steedman, 2002" startWordPosition="420" endWordPosition="423">Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than LTAG and CCG, such as HPSG. 2 Lexicalized</context>
<context position="10639" citStr="Hockenmaier and Steedman, 2002" startWordPosition="1826" endWordPosition="1829">utput another category, we can regard the rules as taking two derived trees as inputs and outputting a combined tree. With this insight, we can regard reduction rules as grammar rules, derived trees as parse results, derivations as derivation histories, then Theorem 1 can be applied. The annotation cost will not be problematic because we only need to annotate FA or BA to the derived trees by using simple heuristic rules. This argument is also valid for the extraction of CCG. In order to annotate the other rules defined by CCG (type-raising, composition, and coordination rules), existing work (Hockenmaier and Steedman, 2002) exploits the annotations of traces and coordinations in the Penn Treebank. LTAG As discussed above, adjoining leads to the situations where the injective condition is violated. We can make the grammar rules pseudoinjective by determining the lengths of the spines, i.e., defining it as returning the spine length. Existing studies assume the length as one (Xia, 1999), or determine the length by using heuristic rules (Chen and Vijay-Shanker, 2000; Chiang, 2000). The above discussion was empirically evaluated by acquiring LTAG grammars from the 129 Grammar (a) Grammar (b) # words 4,514 # words 4,</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with combinatory categorial grammar. In Proc. 40th ACL, pages 335-342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="11469" citStr="Marcus et al., 1994" startWordPosition="1965" endWordPosition="1968">eudoinjective by determining the lengths of the spines, i.e., defining it as returning the spine length. Existing studies assume the length as one (Xia, 1999), or determine the length by using heuristic rules (Chen and Vijay-Shanker, 2000; Chiang, 2000). The above discussion was empirically evaluated by acquiring LTAG grammars from the 129 Grammar (a) Grammar (b) # words 4,514 # words 4,539 # initial trees 1,015 # initial trees 1,068 # aux. trees 1,094 # aux. trees 1,107 coverage 93.4 % coverage 94.0 % Table 1: Specifications of the LTAG grammars acquired from the Penn Treebank Penn Treebank (Marcus et al., 1994). We assumed that the length of all spines was one, i.e., Vci, c2. c2) = 1. 47 heuristic rules were used for the annotation, which shows the annotation was not so costly. A grammar was successfully acquired from Sections 02-21 in 910 seconds on a Xeon 2.0 GHz CPU. As shown in Grammar (a) in Table 1, the grammar had the sufficient coverage (measured against Section 22, 1700 sentences), which shows the robustness of the grammar. We next acquired another grammar (Grammar (b) in Table 1) by including two rules for annotating insertions and coordinations. Since the grammar provides more accurate an</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H Miller</author>
</authors>
<title>Strong Generative Capacity: The Semantics of Linguistic Formalism.</title>
<date>2000</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="4525" citStr="Miller, 2000" startWordPosition="715" endWordPosition="716">formally define a lexicalized grammar in the following way. Definition 1 (Lexicalized grammar) A lexicalized grammar is a tuple G = (L, R), where L is a lexicon, i.e. Lc Wx C, and R is a set of grammar rules, i.e. r E R is a partial function: C x C C. In what follows, we assume that all grammar rules are binary for simplicity. Parsing with a lexicalized grammar is the process of applying grammar rules to lexical entries. Since we assume that the rules are binary, the history of the process constitutes a binary-branching tree; we define such structures in this paper as constituent structures&apos; (Miller, 2000). Definition 2 (Constituent structure) Given a sentence w, a constituent structure of w is the least set F c IV satisfying i) w e F and ii) V7 G F.(171 &gt; 1 71, 72 G F.(7 = 7172)) The first condition represents inclusion of the top node in the structure, and the second constrains the terminals (words) to a linear order. The process of parsing is then depicted by a constituent structure labeled with grammar rules, which we call a derivation history. Definition 3 (Derivation history) Given a sentence w, a derivation history is a tuple T = Kr, p), where F is a constituent structure of w and p is a</context>
</contexts>
<marker>Miller, 2000</marker>
<rawString>Philip H. Miller. 2000. Strong Generative Capacity: The Semantics of Linguistic Formalism. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
</authors>
<title>Syntactic Theory —A Formal Introduction.</title>
<date>1999</date>
<booktitle>CSLI Lcture Notes no. 92.</booktitle>
<publisher>CLSI Publications.</publisher>
<contexts>
<context position="932" citStr="Sag and Wasow, 1999" startWordPosition="129" endWordPosition="132">i}@is s .u-tokyo ac jp Abstract This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 1 Introduction Linguistically motivated and computationally oriented grammar theories take the form of lexicalized grammar formalisms; examples include Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Combinatory Categorial Grammar (Steedman, 2000), and Head-driven Phrase Structure Grammar (HPSG) (Sag and Wasow, 1999). They have been a great success in terms of linguistic analysis and efficiency in the parsing of real-world texts. However, such grammars have not generally been considered suitable for the syntactic analysis within practical NLP systems because considerable effort is required to develop and maintain lexicalized grammars that are both robust and provide broad coverage. One novel approach to grammar development is based on the automatic acquisition of lexicalized grammars from annotated corpora. Since lexicalized grammars represent grammatical constraints with a few grammar rules and a large n</context>
<context position="3553" citStr="Sag and Wasow, 1999" startWordPosition="543" endWordPosition="546">e limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than LTAG and CCG, such as HPSG. 2 Lexicalized Grammars In this section, we define the general form of lexicalized grammars including LTAG (Schabes et al., 1988) and HPSG (Sag and Wasow, 1999). The concepts behind a lexicalized grammar are that i) grammar rules represent general grammatical constructions in the language while ii) lexical entries describe word-specific lexicallsyntactic constraints. Let W be a set of all words and C a set of linguistic representations (e.g. the tree structures of LTAG or typed feature structures of HPSG). We can formally define a lexicalized grammar in the following way. Definition 1 (Lexicalized grammar) A lexicalized grammar is a tuple G = (L, R), where L is a lexicon, i.e. Lc Wx C, and R is a set of grammar rules, i.e. r E R is a partial function</context>
</contexts>
<marker>Sag, Wasow, 1999</marker>
<rawString>Ivan A. Sag and Thomas Wasow. 1999. Syntactic Theory —A Formal Introduction. CSLI Lcture Notes no. 92. CLSI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with `lexicalized grammars&apos;: Application to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proc. 12th COLING,</booktitle>
<pages>578--583</pages>
<contexts>
<context position="812" citStr="Schabes et al., 1988" startWordPosition="112" endWordPosition="116">T (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN {yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp Abstract This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 1 Introduction Linguistically motivated and computationally oriented grammar theories take the form of lexicalized grammar formalisms; examples include Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Combinatory Categorial Grammar (Steedman, 2000), and Head-driven Phrase Structure Grammar (HPSG) (Sag and Wasow, 1999). They have been a great success in terms of linguistic analysis and efficiency in the parsing of real-world texts. However, such grammars have not generally been considered suitable for the syntactic analysis within practical NLP systems because considerable effort is required to develop and maintain lexicalized grammars that are both robust and provide broad coverage. One novel approach to grammar development is based on the automatic acquisition of lexicalized grammars fro</context>
<context position="3522" citStr="Schabes et al., 1988" startWordPosition="537" endWordPosition="540">otated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than LTAG and CCG, such as HPSG. 2 Lexicalized Grammars In this section, we define the general form of lexicalized grammars including LTAG (Schabes et al., 1988) and HPSG (Sag and Wasow, 1999). The concepts behind a lexicalized grammar are that i) grammar rules represent general grammatical constructions in the language while ii) lexical entries describe word-specific lexicallsyntactic constraints. Let W be a set of all words and C a set of linguistic representations (e.g. the tree structures of LTAG or typed feature structures of HPSG). We can formally define a lexicalized grammar in the following way. Definition 1 (Lexicalized grammar) A lexicalized grammar is a tuple G = (L, R), where L is a lexicon, i.e. Lc Wx C, and R is a set of grammar rules, i</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeille, and Aravind K. Joshi. 1988. Parsing strategies with `lexicalized grammars&apos;: Application to tree adjoining grammars. In Proc. 12th COLING, pages 578-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="861" citStr="Steedman, 2000" startWordPosition="121" endWordPosition="122">-8, Kawaguchi-shi, Saitama 332-0012 JAPAN {yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp Abstract This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 1 Introduction Linguistically motivated and computationally oriented grammar theories take the form of lexicalized grammar formalisms; examples include Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Combinatory Categorial Grammar (Steedman, 2000), and Head-driven Phrase Structure Grammar (HPSG) (Sag and Wasow, 1999). They have been a great success in terms of linguistic analysis and efficiency in the parsing of real-world texts. However, such grammars have not generally been considered suitable for the syntactic analysis within practical NLP systems because considerable effort is required to develop and maintain lexicalized grammars that are both robust and provide broad coverage. One novel approach to grammar development is based on the automatic acquisition of lexicalized grammars from annotated corpora. Since lexicalized grammars r</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proc. 5th NLPRS.</booktitle>
<contexts>
<context position="2721" citStr="Xia, 1999" startWordPosition="410" endWordPosition="411">where various heuristic and statistical methods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can b</context>
<context position="11007" citStr="Xia, 1999" startWordPosition="1888" endWordPosition="1889">es by using simple heuristic rules. This argument is also valid for the extraction of CCG. In order to annotate the other rules defined by CCG (type-raising, composition, and coordination rules), existing work (Hockenmaier and Steedman, 2002) exploits the annotations of traces and coordinations in the Penn Treebank. LTAG As discussed above, adjoining leads to the situations where the injective condition is violated. We can make the grammar rules pseudoinjective by determining the lengths of the spines, i.e., defining it as returning the spine length. Existing studies assume the length as one (Xia, 1999), or determine the length by using heuristic rules (Chen and Vijay-Shanker, 2000; Chiang, 2000). The above discussion was empirically evaluated by acquiring LTAG grammars from the 129 Grammar (a) Grammar (b) # words 4,514 # words 4,539 # initial trees 1,015 # initial trees 1,068 # aux. trees 1,094 # aux. trees 1,107 coverage 93.4 % coverage 94.0 % Table 1: Specifications of the LTAG grammars acquired from the Penn Treebank Penn Treebank (Marcus et al., 1994). We assumed that the length of all spines was one, i.e., Vci, c2. c2) = 1. 47 heuristic rules were used for the annotation, which shows t</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proc. 5th NLPRS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>