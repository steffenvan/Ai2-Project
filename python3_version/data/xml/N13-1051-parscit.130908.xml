<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.994467">
Parameter Estimation for LDA-Frames
</title>
<author confidence="0.996901">
Jiˇr´ı Materna
</author>
<affiliation confidence="0.9958425">
Centre for Natural Language Processing
Faculty of Informatics, Masaryk University
</affiliation>
<address confidence="0.943798">
Botanick´a 68a, 602 00, Brno, Czech Republic
</address>
<email confidence="0.996113">
xmaterna@fi.muni.cz
</email>
<sectionHeader confidence="0.995568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999004666666667">
LDA-frames is an unsupervised approach for
identifying semantic frames from semanti-
cally unlabeled text corpora, and seems to
be a useful competitor for manually created
databases of selectional preferences. The most
limiting property of the algorithm is such that
the number of frames and roles must be pre-
defined. In this paper we present a modifi-
cation of the LDA-frames algorithm allowing
the number of frames and roles to be deter-
mined automatically, based on the character
and size of training data.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894052631579">
Semantic frames and valency lexicons are useful
lexical sources capturing semantic roles valid for
a set of lexical units. The structures of linked seman-
tic roles are called semantic frames. Linguists are
using them for their ability to describe an interface
between syntax and semantics. In practical natural
language processing applications, they can be used,
for instance, for the word sense disambiguation task
or in order to resolve ambiguities in syntactic analy-
sis of natural languages.
The lexicons of semantic frames or verb valencies
are mainly created manually or semi-automatically
by highly trained linguists. Manually created lex-
icons involve, for instance, a well-known lexi-
con of semantic frames FrameNet (Ruppenhofer
et al., 2006) or a lexicon of verb valencies VerbNet
(Schuler, 2006). These and other similar lexical re-
sources have many promising applications, but suf-
fer from several disadvantages:
</bodyText>
<listItem confidence="0.993888733333333">
• Creation of them requires manual work of
trained linguists which is very time-consuming
and expensive.
• Coverage of the resources is usually small or
limited to some specific domain.
• Most of the resources do not provide any
information about relative frequency of us-
age in corpora. For instance, both patterns
[Person] acquire [Physical object]
and [Person] acquire [Disease] reflect
correct usage of verb acquire, but the former
is much more frequent in English.
• Notion of semantic classes and frames is sub-
jectively biased when the frames are created
manually without corpus evidence.
</listItem>
<bodyText confidence="0.9999644375">
In order to avoid those problems we proposed
a method for creating probabilistic semantic frames
called LDA-frames (Materna, 2012). The main idea
of LDA-frames is to generate the set of semantic
frames and roles automatically by maximizing pos-
terior probability of a probabilistic model on a syn-
tactically annotated training corpus. A semantic role
is represented as probability distribution over all its
realizations in the corpus, a semantic frame as a tu-
ple of semantic roles, each of them connected with
some grammatical relation. For every lexical unit
(a verb in case of computing verb valencies), a prob-
ability distribution over all semantic frames is gen-
erated, where the probability of a frame corresponds
to the relative frequency of usage in the corpus for
a given lexical unit. An example of LDA-frames
</bodyText>
<page confidence="0.981104">
482
</page>
<subsectionHeader confidence="0.294238">
Proceedings of NAACL-HLT 2013, pages 482–486,
</subsectionHeader>
<bodyText confidence="0.978784823529412">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
computed on the British National Corpus is avail-
able at the LDA-frames website1.
The original LDA-frames algorithm has two pa-
rameters that must be predefined – number of frames
and number of roles – which is the most limiting
property of the algorithm. A simple cross-validation
approach can be used in case of very small data.
However, real data is much bigger and it is not re-
commended to use such techniques. For example,
the inference on the British National Corpus using a
single core 2.4 GHz CPU takes several days to com-
pute one reasonable combination of parameters.
In this paper we present a non-parametric modifi-
cation of the LDA-frames algorithm allowing to de-
termine the parameters automatically, based on the
character and size of training data.
</bodyText>
<sectionHeader confidence="0.730108" genericHeader="introduction">
2 LDA-Frames
</sectionHeader>
<bodyText confidence="0.999755035714286">
LDA-frames (Materna, 2012) is an unsupervised ap-
proach for identifying semantic frames from seman-
tically unlabeled text corpora. In the LDA-frames,
a frame is represented as a tuple of semantic roles,
each of them connected with a grammatical rela-
tion i.e. subject, object, modifier, etc. These frames
are related to a lexical unit via probability distribu-
tion. Every semantic role is represented as probabil-
ity distribution over its realizations.
The method of automatic identification of se-
mantic frames is based on probabilistic generative
process. Training data for the algorithm consists
of tuples of grammatical relation realizations ac-
quired using a dependency parser from the train-
ing corpus for every lexical unit. For example, sup-
pose that the goal is to generate semantic frames of
verbs from a corpus for grammatical relations sub-
ject and object. The training data for lexical unit
eat may look like 1(peter, cake), (man,
breakfast), (dog, meat), ...}, where
the first component of the tuples corresponds to sub-
ject and the second to object.
In the generative process, each grammatical rela-
tion realization is treated as being generated from
a given semantic frame according to the realiza-
tion distribution of the corresponding semantic role.
Supposing the number of frames is given by param-
eter F, the number of semantic roles by R, the num-
</bodyText>
<footnote confidence="0.905061">
1http://nlp.fi.muni.cz/projekty/lda-frames/
</footnote>
<bodyText confidence="0.98931575">
ber of slots (grammatical relations) by 5 and the size
of vocabulary is V . The realizations are generated as
follows.
For each lexical unit u E 11, 2, ... , U}:
</bodyText>
<listItem confidence="0.948325">
1. Choose a frame distribution cou from Dir(α).
2. For each lexical unit realization
t E 11, 2, ... , Tu} choose a frame fu,t from
Multinomial(cou), where fu,t E 11, 2, ... , F}.
3. For each slot s E 11, 2, ... , 5} of frame
fu,t, generate a grammatical relation realiza-
</listItem>
<bodyText confidence="0.7818369">
tion wu,t,s from Multinomial(Brfu,t,s), where
rf,s is a projection (f, s) H r, which assigns
a semantic role for each slot s in frame f. The
multinomial distribution of realizations, sym-
bolized by Br, for semantic role r is generated
from Dir(Q).
The graphical model for LDA-Frames is shown
in figure 1. It is parametrized by hyperparameters of
prior distributions α and Q, usually set by hand to
a value between 0.01 – 0.1.
</bodyText>
<figureCaption confidence="0.98603">
Figure 1: Graphical model for LDA-frames.
</figureCaption>
<bodyText confidence="0.9999604">
The inference is performed using the Collapsed
Gibbs sampling (Neal, 2000), where the B and co dis-
tributions are marginalized out of the equations. In
each iteration, latent variables fu,t and rf,s are sam-
pled as follows
</bodyText>
<equation confidence="0.9852682">
P(fu,t|f−(u,t), r, w, α, Q) a
−(u,t,s) Q (1)
Wcwu,t,s,rfu,t,3 +
wc −(u,t,s) /�
*,rfu t,s + V N
</equation>
<figure confidence="0.808281933333333">
U
φ f
u u,t
α
T
F S
S
rf,s
wu,t,s
er
β
R
(f−(u,t)
cfu,t,u + α)
S
</figure>
<page confidence="0.715583">
H
s=1
483
</page>
<equation confidence="0.980357166666667">
P(rf,s|f, r−(f,s), w, α, β) a
−(f,s) wc
3 f,s,v
wcv,rf,s + β 1 (2)
−(f,s),
+ V β
</equation>
<bodyText confidence="0.990586916666667">
where fc−(u,t)
f,u is the number of times frame f is
assigned to lexical unit u excluding (u, t), wc−(u,t,s)
v,r
is the number of times word v is assigned to role
r excluding (u, t, s), and wcf,s,v is the number of
times word v is assigned to slot s in frame f. The
asterisk sign * stands for any value in its position.
After having all latent variables f and r inferred,
one can proceed to compute the lexical unit–frame
distribution and the semantic role–word distribution
using the following formulas:
</bodyText>
<equation confidence="0.9967">
__ f cf,u + α (3)
ϕu E f fcf,u + Fα
θr
wcv,r + β (4)
=
Ev wcv,r + V β .
</equation>
<sectionHeader confidence="0.991583" genericHeader="method">
3 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999881875">
As one can see from the LDA-frames model, the
requirement is to define the number of frames and
roles in advance. It is not clear, however, how to se-
lect the best values that depend on several factors.
First of all, the number of frames and roles usually
increase with the growing size of training corpus. If
the training data is small and covers just a small pro-
portion of lexical unit usage patterns, the number of
semantic frames should be small as well. The pa-
rameters are also affected by the granularity of roles
and frames. One way to estimate the parameters au-
tomatically is to select those that maximize posterior
probability of the model given training data.
LDA-frames algorithm generates frames from the
Dirichlet distribution (DD) which requires a fixed
number of components. Similarly, the latent vari-
ables rf,s are chosen from a fixed set of semantic
roles. In order to be able to update the number of
frames and roles during the inference process, we
propose to add the Chinese restaurant process (CRP)
(Aldous, 1985) prior for the rf,s variables, and to re-
place the Dirichlet distribution the semantic frames
are generated from with the Dirichlet process (Fer-
guson, 1973).
</bodyText>
<subsectionHeader confidence="0.993817">
3.1 Number of Semantic Roles
</subsectionHeader>
<bodyText confidence="0.999936923076923">
In the original version of the LDA-frames model,
the latent variables rf,s, representing semantic role
assignment for slot s in frame f, are chosen from
a fixed set of semantic roles without any prior distri-
bution. We propose to generate rf,s from the CRP,
which is a single parameter distribution over parti-
tions of integers. The generative process can be de-
scribed by using an analogy with a Chinese restau-
rant. Consider a restaurant with an infinite number
of tables, each of them associated with some dish,
and N customers choosing a table. The first cus-
tomer sits at the first table. The nth customer sits at
table t drawn from the following distribution
</bodyText>
<equation confidence="0.921049666666667">
ni
P(t = occupied table i) =
(5)
</equation>
<bodyText confidence="0.998998272727273">
where ni is the number of customers sitting at the
table i and γ &gt; 0 is a concentration parameter which
controls how often a customer chooses a new table.
The seating plan makes a partition of the customers
(Aldous, 1985).
In the proposed modification of the LDA-frames
model, the dishes are replaced with the semantic role
numbers and customers with slots of frames. In the
model we use prior distribution ω corresponding to
the CRP with concentration parameter γ. The latent
variables rf,s are then sampled as follows
</bodyText>
<equation confidence="0.984982125">
P(rf,s|f, r−(f,s), w, α, β,γ) a
−(f,s) wcf,s,v
rf,s + β (6)
wcv
,
wc∗
−(f,s) + V β
,rf,s
</equation>
<bodyText confidence="0.984041333333333">
where rc−(f,s)
r is the number of times role r is used
in any frame and slot excluding slot s in frame f.
Notice that the sampling space has R+1 dimensions
with the probability of the last unseen component
proportional to
</bodyText>
<equation confidence="0.85139775">
H 1
γ (7)
v=1 Vwcf,s,v .
V
</equation>
<subsectionHeader confidence="0.99849">
3.2 Number of Semantic Frames
</subsectionHeader>
<bodyText confidence="0.95256175">
Estimating the number of frames is a little bit more
complicated than the case of semantic roles. The
idea is to replace DD
with the Diri
</bodyText>
<page confidence="0.394075">
ϕu
</page>
<bodyText confidence="0.696486">
chlet process.
</bodyText>
<equation confidence="0.997646230769231">
V
H
v=1
wc
γ + n − 1
γ
P(t = next unoccupied table) =
γ + n − 1,
(rc−(f,s)
rf,s + γ)
V
H
v=1
</equation>
<page confidence="0.993505">
484
</page>
<bodyText confidence="0.932766571428571">
The Dirichlet process DP(α0, G0) is a stochastic
process that generates discrete probability distribu-
tions. It has two parameters, a base distribution G0
and a concentration parameter α0 &gt; 0. A sample
from the Dirichlet process (DP) is then
each u and f (Heinrich, 2011):
ατf
</bodyText>
<equation confidence="0.998471666666667">
P(uf,u,r = 1) = br E [1, f cf,u]
ατf + r — 1
�
τ — Dir({uf}f, δ) with uf =
u
�
r
uf,u,r.
(9)
00
G = βkδφk, (8) Finally, the latent variables fu,t are sampled as fol-
k=1 lows
</equation>
<bodyText confidence="0.9997179375">
where φk are independent random variables dis-
tributed according to G0, δφk is an atom at φk, and
weights βk are also random and dependent on the
parameter α0 (Teh et al., 2006). Simply, DP is a dis-
tribution over some infinite and discrete distribu-
tions. It is the reason why DP is often used instead of
DD in order to avoid using a fixed number of com-
ponents.
The question, however, is how to make the sam-
pled frames shared between different lexical units.
We propose to generate base distributions of the
DPs from GEM distribution (Pitman, 2002) τ with
concentration parameter δ. The idea is inspired by
the Hierarchical Dirichlet Process (Teh et al., 2006)
used for topic modeling. The graphical model of the
non-parametric LDA-frames is shown in figure 2.
</bodyText>
<figureCaption confidence="0.8706905">
Figure 2: Graphical model for non-parametric LDA-
frames.
</figureCaption>
<bodyText confidence="0.999901857142857">
Since it is hard to integrate out the DP with base
distribution generated from GEM in this model, we
proceeded to sample τ separately (Porteous, 2010).
The base distribution proportions can be sampled
by simulating how new components are created for
fcf,u draws from DP with the concentration param-
eter ατf, which is a sequence of Bernoulli trials for
</bodyText>
<equation confidence="0.993145166666667">
P(fu,t|f−(u,t), r, w, α, β, τ) a
−(u,t,s)β
wcwu,t,s,rfu,t,s +
.
−(u,t,s)
*,rfu,t,s + V β
</equation>
<sectionHeader confidence="0.995391" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.993401">
The non-parametric algorithm was evaluated by
an experiment on a synthetic data set consisting
of 155 subject-object tuples. The training data
was generated randomly from a predefined set of 7
frames and 4 roles for 16 verbs using the following
algorithm. For every lexical unit u:
</bodyText>
<listItem confidence="0.966971375">
1. Choose a number of corpus realizations Nu E
{5, ... ,15} from the uniform distribution.
2. For each realization nu E {1, ... , Nu}, among
all permitted frames for lexical unit u, choose
a semantic frame fnu from the uniform distri-
bution.
3. For each frame fnu, generate a realization of all
its roles from the uniform distribution.
</listItem>
<bodyText confidence="0.999186875">
Each semantic role had 6 possible realizations on
average, some of them assigned to more than one se-
mantic role to reflect the character of real languages.
Since the data was generated artificially, we knew
the number of frames and roles, how the frames were
defined, and which frame and which role was re-
sponsible for generating each realization in the data.
We ran the non-parametric algorithm with hyper-
parameters α = 5, β = γ = 0.1, δ = 1.5. It has
been shown that the selection of hyperparameters
has little impact on the resulting frames when they
are in some reasonable range, thus, the hyperparam-
eters were chosen empirically by hand. The experi-
ment led to correct assignments of fu,t and rf,s after
56 iterations on average (based on 10 independent
runs of the algorithm).
</bodyText>
<figure confidence="0.999873826086956">
b
T � f
u u,t
U
α
T
W
Y
00
S
S
rf,s
Wu,t,s
0r
R
00
(fc−(u,t)
fu,t,u + ατf)
S
H
s=1
wc
(10)
</figure>
<page confidence="0.998144">
485
</page>
<bodyText confidence="0.9999758">
In order to compare the non-parametric algorithm
with the original, we ran the original algorithm with
the same data that had the number of frames and
roles set to R E {1...10}, F E {1... 20}, and
measured the perplexity of the data given to the
model after convergence. The perplexities for all
settings are shown in figure 3. The lowest perplexity
was reached with F = 7, R = 4 and had the same
value as the case of the non-parametric algorithm.
The f,,,t and rf,s assignments were correct as well.
</bodyText>
<figureCaption confidence="0.989517">
Figure 3: Perplexities for different values of F and R.
</figureCaption>
<bodyText confidence="0.99999">
We also ran the non-parametric algorithm with the
same hyperparameters on real data (1.4 millions of
subject-object tuples) acquired from the British Na-
tional Corpus2 using the Stanford Parser (de Marn-
effe et al., 2006). The algorithm reached the opti-
mal perplexity with 427 frames and 144 roles. This
experiment has been performed only for illustrating
the algorithm on real data. Because of long running
time of the algorithm on such huge data set, we did
not perform the same experiments as with the case
of the small synthetic data.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999089125">
In this paper we presented a method for estimat-
ing the number of frames and roles for the LDA-
frames model. The idea is based on using the Chi-
nese Restaurant Process and the Dirichlet Process
instead of the Dirichlet Distributions and selecting
such parameters that maximize the posterior proba-
bility of the model for given training data. An ex-
periment showed that the non-parametric algorithm
</bodyText>
<footnote confidence="0.740326">
2http://www.natcorp.ox.ac.uk
</footnote>
<bodyText confidence="0.9978805">
infers correct values of both the number of frames
and roles on a synthetic data set.
</bodyText>
<sectionHeader confidence="0.996592" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987606">
This work has been partly supported by the Min-
istry of Education of the Czech Republic under the
project LINDAT-Clarin LM2010013.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702368421053">
Aldous, D. J. (1985). Exchangeability and Related Top-
ics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII –
1983, 1117:1 – 198.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating Typed Dependency Parses from
Phrase Structure Parses. In The International Confer-
ence on Language Resources and Evaluation (LREC)
2006.
Ferguson, T. S. (1973). A Bayesian Analysis of Some
Nonparametric Problems. The Annals of Statistics,
1:209 – 230.
Heinrich, G. (2011). ”Infinite LDA” – Implementing the
HDP with Minimum Code complexity. Technical re-
port.
Materna, J. (2012). LDA-Frames: An Unsupervised Ap-
proach to Generating Semantic Frames. In Gelbukh,
A., editor, Proceedings of the 13th International Con-
ference CICLing 2012, Part I, pages 376–387, New
Delhi, India. Springer Berlin / Heidelberg.
Neal, R. M. (2000). Markov Chain Sampling Methods
for Dirichlet Process Mixture Models. Journal of com-
putational and graphical statistics, 9(2):249–265.
Pitman, J. (2002). Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School.
Porteous, I. (2010). Networks of mixture blocks for non
parametric bayesian models with applications. PhD
thesis, University of California.
Ruppenhofer, J., Ellsworth, M., Petruck, M. R. L.,
Johnson, C. R., and Scheffczyk, J. (2006).
FrameNet II: Extended Theory and Practice.
http://www.icsi.berkeley.edu/framenet.
Schuler, K. K. (2006). VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. PhD thesis, University
of Pennsylvania.
Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M.
(2006). Hierarchical Dirichlet processes . Journal
of the American Statistical Association, 101:1566 –
1581.
</reference>
<page confidence="0.999038">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762773">
<title confidence="0.999307">Parameter Estimation for LDA-Frames</title>
<author confidence="0.828125">Jiˇr´ı</author>
<affiliation confidence="0.949057">Centre for Natural Language Faculty of Informatics, Masaryk</affiliation>
<address confidence="0.935678">Botanick´a 68a, 602 00, Brno, Czech Republic</address>
<email confidence="0.984865">xmaterna@fi.muni.cz</email>
<abstract confidence="0.999681153846154">LDA-frames is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora, and seems to be a useful competitor for manually created databases of selectional preferences. The most limiting property of the algorithm is such that the number of frames and roles must be predefined. In this paper we present a modification of the LDA-frames algorithm allowing the number of frames and roles to be determined automatically, based on the character and size of training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D J Aldous</author>
</authors>
<title>Exchangeability and Related Topics. ´Ecole d’</title>
<date>1985</date>
<booktitle>Et´e de Probabilit´es de Saint-Flour XIII – 1983, 1117:1 – 198.</booktitle>
<contexts>
<context position="8362" citStr="Aldous, 1985" startWordPosition="1400" endWordPosition="1401">antic frames should be small as well. The parameters are also affected by the granularity of roles and frames. One way to estimate the parameters automatically is to select those that maximize posterior probability of the model given training data. LDA-frames algorithm generates frames from the Dirichlet distribution (DD) which requires a fixed number of components. Similarly, the latent variables rf,s are chosen from a fixed set of semantic roles. In order to be able to update the number of frames and roles during the inference process, we propose to add the Chinese restaurant process (CRP) (Aldous, 1985) prior for the rf,s variables, and to replace the Dirichlet distribution the semantic frames are generated from with the Dirichlet process (Ferguson, 1973). 3.1 Number of Semantic Roles In the original version of the LDA-frames model, the latent variables rf,s, representing semantic role assignment for slot s in frame f, are chosen from a fixed set of semantic roles without any prior distribution. We propose to generate rf,s from the CRP, which is a single parameter distribution over partitions of integers. The generative process can be described by using an analogy with a Chinese restaurant. </context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>Aldous, D. J. (1985). Exchangeability and Related Topics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII – 1983, 1117:1 – 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In The International Conference on Language Resources and Evaluation (LREC)</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., MacCartney, B., and Manning, C. D. (2006). Generating Typed Dependency Parses from Phrase Structure Parses. In The International Conference on Language Resources and Evaluation (LREC) 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Ferguson</author>
</authors>
<title>A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics,</title>
<date>1973</date>
<pages>1--209</pages>
<contexts>
<context position="8517" citStr="Ferguson, 1973" startWordPosition="1424" endWordPosition="1426">ically is to select those that maximize posterior probability of the model given training data. LDA-frames algorithm generates frames from the Dirichlet distribution (DD) which requires a fixed number of components. Similarly, the latent variables rf,s are chosen from a fixed set of semantic roles. In order to be able to update the number of frames and roles during the inference process, we propose to add the Chinese restaurant process (CRP) (Aldous, 1985) prior for the rf,s variables, and to replace the Dirichlet distribution the semantic frames are generated from with the Dirichlet process (Ferguson, 1973). 3.1 Number of Semantic Roles In the original version of the LDA-frames model, the latent variables rf,s, representing semantic role assignment for slot s in frame f, are chosen from a fixed set of semantic roles without any prior distribution. We propose to generate rf,s from the CRP, which is a single parameter distribution over partitions of integers. The generative process can be described by using an analogy with a Chinese restaurant. Consider a restaurant with an infinite number of tables, each of them associated with some dish, and N customers choosing a table. The first customer sits </context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Ferguson, T. S. (1973). A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics, 1:209 – 230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heinrich</author>
</authors>
<date>2011</date>
<booktitle>Infinite LDA” – Implementing the HDP with Minimum Code complexity. Technical report.</booktitle>
<contexts>
<context position="10652" citStr="Heinrich, 2011" startWordPosition="1818" endWordPosition="1819">t unseen component proportional to H 1 γ (7) v=1 Vwcf,s,v . V 3.2 Number of Semantic Frames Estimating the number of frames is a little bit more complicated than the case of semantic roles. The idea is to replace DD with the Diri ϕu chlet process. V H v=1 wc γ + n − 1 γ P(t = next unoccupied table) = γ + n − 1, (rc−(f,s) rf,s + γ) V H v=1 484 The Dirichlet process DP(α0, G0) is a stochastic process that generates discrete probability distributions. It has two parameters, a base distribution G0 and a concentration parameter α0 &gt; 0. A sample from the Dirichlet process (DP) is then each u and f (Heinrich, 2011): ατf P(uf,u,r = 1) = br E [1, f cf,u] ατf + r — 1 � τ — Dir({uf}f, δ) with uf = u � r uf,u,r. (9) 00 G = βkδφk, (8) Finally, the latent variables fu,t are sampled as folk=1 lows where φk are independent random variables distributed according to G0, δφk is an atom at φk, and weights βk are also random and dependent on the parameter α0 (Teh et al., 2006). Simply, DP is a distribution over some infinite and discrete distributions. It is the reason why DP is often used instead of DD in order to avoid using a fixed number of components. The question, however, is how to make the sampled frames shar</context>
</contexts>
<marker>Heinrich, 2011</marker>
<rawString>Heinrich, G. (2011). ”Infinite LDA” – Implementing the HDP with Minimum Code complexity. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Materna</author>
</authors>
<title>LDA-Frames: An Unsupervised Approach to Generating Semantic Frames.</title>
<date>2012</date>
<booktitle>Proceedings of the 13th International Conference CICLing 2012, Part I,</booktitle>
<pages>376--387</pages>
<editor>In Gelbukh, A., editor,</editor>
<publisher>Springer</publisher>
<location>New Delhi, India.</location>
<contexts>
<context position="2371" citStr="Materna, 2012" startWordPosition="360" endWordPosition="361">erage of the resources is usually small or limited to some specific domain. • Most of the resources do not provide any information about relative frequency of usage in corpora. For instance, both patterns [Person] acquire [Physical object] and [Person] acquire [Disease] reflect correct usage of verb acquire, but the former is much more frequent in English. • Notion of semantic classes and frames is subjectively biased when the frames are created manually without corpus evidence. In order to avoid those problems we proposed a method for creating probabilistic semantic frames called LDA-frames (Materna, 2012). The main idea of LDA-frames is to generate the set of semantic frames and roles automatically by maximizing posterior probability of a probabilistic model on a syntactically annotated training corpus. A semantic role is represented as probability distribution over all its realizations in the corpus, a semantic frame as a tuple of semantic roles, each of them connected with some grammatical relation. For every lexical unit (a verb in case of computing verb valencies), a probability distribution over all semantic frames is generated, where the probability of a frame corresponds to the relative</context>
<context position="3987" citStr="Materna, 2012" startWordPosition="623" endWordPosition="624">ber of roles – which is the most limiting property of the algorithm. A simple cross-validation approach can be used in case of very small data. However, real data is much bigger and it is not recommended to use such techniques. For example, the inference on the British National Corpus using a single core 2.4 GHz CPU takes several days to compute one reasonable combination of parameters. In this paper we present a non-parametric modification of the LDA-frames algorithm allowing to determine the parameters automatically, based on the character and size of training data. 2 LDA-Frames LDA-frames (Materna, 2012) is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora. In the LDA-frames, a frame is represented as a tuple of semantic roles, each of them connected with a grammatical relation i.e. subject, object, modifier, etc. These frames are related to a lexical unit via probability distribution. Every semantic role is represented as probability distribution over its realizations. The method of automatic identification of semantic frames is based on probabilistic generative process. Training data for the algorithm consists of tuples of grammatical relation</context>
</contexts>
<marker>Materna, 2012</marker>
<rawString>Materna, J. (2012). LDA-Frames: An Unsupervised Approach to Generating Semantic Frames. In Gelbukh, A., editor, Proceedings of the 13th International Conference CICLing 2012, Part I, pages 376–387, New Delhi, India. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Journal of computational and graphical statistics,</title>
<date>2000</date>
<pages>9--2</pages>
<contexts>
<context position="6331" citStr="Neal, 2000" startWordPosition="1016" endWordPosition="1017">11, 2, ... , 5} of frame fu,t, generate a grammatical relation realization wu,t,s from Multinomial(Brfu,t,s), where rf,s is a projection (f, s) H r, which assigns a semantic role for each slot s in frame f. The multinomial distribution of realizations, symbolized by Br, for semantic role r is generated from Dir(Q). The graphical model for LDA-Frames is shown in figure 1. It is parametrized by hyperparameters of prior distributions α and Q, usually set by hand to a value between 0.01 – 0.1. Figure 1: Graphical model for LDA-frames. The inference is performed using the Collapsed Gibbs sampling (Neal, 2000), where the B and co distributions are marginalized out of the equations. In each iteration, latent variables fu,t and rf,s are sampled as follows P(fu,t|f−(u,t), r, w, α, Q) a −(u,t,s) Q (1) Wcwu,t,s,rfu,t,3 + wc −(u,t,s) /� *,rfu t,s + V N U φ f u u,t α T F S S rf,s wu,t,s er β R (f−(u,t) cfu,t,u + α) S H s=1 483 P(rf,s|f, r−(f,s), w, α, β) a −(f,s) wc 3 f,s,v wcv,rf,s + β 1 (2) −(f,s), + V β where fc−(u,t) f,u is the number of times frame f is assigned to lexical unit u excluding (u, t), wc−(u,t,s) v,r is the number of times word v is assigned to role r excluding (u, t, s), and wcf,s,v is t</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Neal, R. M. (2000). Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Journal of computational and graphical statistics, 9(2):249–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
</authors>
<title>Combinatorial Stochastic Processes. Lecture Notes for St. Flour Summer School.</title>
<date>2002</date>
<contexts>
<context position="11377" citStr="Pitman, 2002" startWordPosition="1961" endWordPosition="1962"> (8) Finally, the latent variables fu,t are sampled as folk=1 lows where φk are independent random variables distributed according to G0, δφk is an atom at φk, and weights βk are also random and dependent on the parameter α0 (Teh et al., 2006). Simply, DP is a distribution over some infinite and discrete distributions. It is the reason why DP is often used instead of DD in order to avoid using a fixed number of components. The question, however, is how to make the sampled frames shared between different lexical units. We propose to generate base distributions of the DPs from GEM distribution (Pitman, 2002) τ with concentration parameter δ. The idea is inspired by the Hierarchical Dirichlet Process (Teh et al., 2006) used for topic modeling. The graphical model of the non-parametric LDA-frames is shown in figure 2. Figure 2: Graphical model for non-parametric LDAframes. Since it is hard to integrate out the DP with base distribution generated from GEM in this model, we proceeded to sample τ separately (Porteous, 2010). The base distribution proportions can be sampled by simulating how new components are created for fcf,u draws from DP with the concentration parameter ατf, which is a sequence of </context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>Pitman, J. (2002). Combinatorial Stochastic Processes. Lecture Notes for St. Flour Summer School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Porteous</author>
</authors>
<title>Networks of mixture blocks for non parametric bayesian models with applications.</title>
<date>2010</date>
<tech>PhD thesis,</tech>
<institution>University of California.</institution>
<contexts>
<context position="11796" citStr="Porteous, 2010" startWordPosition="2029" endWordPosition="2030">mponents. The question, however, is how to make the sampled frames shared between different lexical units. We propose to generate base distributions of the DPs from GEM distribution (Pitman, 2002) τ with concentration parameter δ. The idea is inspired by the Hierarchical Dirichlet Process (Teh et al., 2006) used for topic modeling. The graphical model of the non-parametric LDA-frames is shown in figure 2. Figure 2: Graphical model for non-parametric LDAframes. Since it is hard to integrate out the DP with base distribution generated from GEM in this model, we proceeded to sample τ separately (Porteous, 2010). The base distribution proportions can be sampled by simulating how new components are created for fcf,u draws from DP with the concentration parameter ατf, which is a sequence of Bernoulli trials for P(fu,t|f−(u,t), r, w, α, β, τ) a −(u,t,s)β wcwu,t,s,rfu,t,s + . −(u,t,s) *,rfu,t,s + V β 4 Evaluation The non-parametric algorithm was evaluated by an experiment on a synthetic data set consisting of 155 subject-object tuples. The training data was generated randomly from a predefined set of 7 frames and 4 roles for 16 verbs using the following algorithm. For every lexical unit u: 1. Choose a nu</context>
</contexts>
<marker>Porteous, 2010</marker>
<rawString>Porteous, I. (2010). Networks of mixture blocks for non parametric bayesian models with applications. PhD thesis, University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M R L Petruck</author>
<author>C R Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice.</title>
<date>2006</date>
<note>http://www.icsi.berkeley.edu/framenet.</note>
<contexts>
<context position="1475" citStr="Ruppenhofer et al., 2006" startWordPosition="218" endWordPosition="221"> The structures of linked semantic roles are called semantic frames. Linguists are using them for their ability to describe an interface between syntax and semantics. In practical natural language processing applications, they can be used, for instance, for the word sense disambiguation task or in order to resolve ambiguities in syntactic analysis of natural languages. The lexicons of semantic frames or verb valencies are mainly created manually or semi-automatically by highly trained linguists. Manually created lexicons involve, for instance, a well-known lexicon of semantic frames FrameNet (Ruppenhofer et al., 2006) or a lexicon of verb valencies VerbNet (Schuler, 2006). These and other similar lexical resources have many promising applications, but suffer from several disadvantages: • Creation of them requires manual work of trained linguists which is very time-consuming and expensive. • Coverage of the resources is usually small or limited to some specific domain. • Most of the resources do not provide any information about relative frequency of usage in corpora. For instance, both patterns [Person] acquire [Physical object] and [Person] acquire [Disease] reflect correct usage of verb acquire, but the </context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Ruppenhofer, J., Ellsworth, M., Petruck, M. R. L., Johnson, C. R., and Scheffczyk, J. (2006). FrameNet II: Extended Theory and Practice. http://www.icsi.berkeley.edu/framenet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Schuler</author>
</authors>
<title>VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1530" citStr="Schuler, 2006" startWordPosition="229" endWordPosition="230">s. Linguists are using them for their ability to describe an interface between syntax and semantics. In practical natural language processing applications, they can be used, for instance, for the word sense disambiguation task or in order to resolve ambiguities in syntactic analysis of natural languages. The lexicons of semantic frames or verb valencies are mainly created manually or semi-automatically by highly trained linguists. Manually created lexicons involve, for instance, a well-known lexicon of semantic frames FrameNet (Ruppenhofer et al., 2006) or a lexicon of verb valencies VerbNet (Schuler, 2006). These and other similar lexical resources have many promising applications, but suffer from several disadvantages: • Creation of them requires manual work of trained linguists which is very time-consuming and expensive. • Coverage of the resources is usually small or limited to some specific domain. • Most of the resources do not provide any information about relative frequency of usage in corpora. For instance, both patterns [Person] acquire [Physical object] and [Person] acquire [Disease] reflect correct usage of verb acquire, but the former is much more frequent in English. • Notion of se</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>Schuler, K. K. (2006). VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes .</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="11007" citStr="Teh et al., 2006" startWordPosition="1893" endWordPosition="1896">chlet process DP(α0, G0) is a stochastic process that generates discrete probability distributions. It has two parameters, a base distribution G0 and a concentration parameter α0 &gt; 0. A sample from the Dirichlet process (DP) is then each u and f (Heinrich, 2011): ατf P(uf,u,r = 1) = br E [1, f cf,u] ατf + r — 1 � τ — Dir({uf}f, δ) with uf = u � r uf,u,r. (9) 00 G = βkδφk, (8) Finally, the latent variables fu,t are sampled as folk=1 lows where φk are independent random variables distributed according to G0, δφk is an atom at φk, and weights βk are also random and dependent on the parameter α0 (Teh et al., 2006). Simply, DP is a distribution over some infinite and discrete distributions. It is the reason why DP is often used instead of DD in order to avoid using a fixed number of components. The question, however, is how to make the sampled frames shared between different lexical units. We propose to generate base distributions of the DPs from GEM distribution (Pitman, 2002) τ with concentration parameter δ. The idea is inspired by the Hierarchical Dirichlet Process (Teh et al., 2006) used for topic modeling. The graphical model of the non-parametric LDA-frames is shown in figure 2. Figure 2: Graphic</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. (2006). Hierarchical Dirichlet processes . Journal of the American Statistical Association, 101:1566 – 1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>