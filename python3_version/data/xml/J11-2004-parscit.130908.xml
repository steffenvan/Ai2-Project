<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.6847975">
Squibs
Stable Classification of Text Genres
</title>
<author confidence="0.995217">
Philipp Petrenz*
</author>
<affiliation confidence="0.994635">
University of Edinburgh
</affiliation>
<author confidence="0.990879">
Bonnie Webber**
</author>
<affiliation confidence="0.991698">
University of Edinburgh
</affiliation>
<bodyText confidence="0.9873626">
Every text has at least one topic and at least one genre. Evidence for a text’s topic and genre
comes, in part, from its lexical and syntactic features—features used in both Automatic Topic
Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should
be stable in the face of changes in topic distribution, we assess five previously published AGC
methods with respect to both performance on the same topic–genre distribution on which they
were trained and stability of that performance across changes in topic–genre distribution. Our
experiments lead us to conclude that (1) stability in the face of changing topical distributions
should be added to the evaluation critera for new approaches to AGC, and (2) part-of-speech
features should be considered individually when developing a high-performing, stable AGC
system for a particular, possibly changing corpus.
</bodyText>
<sectionHeader confidence="0.996108" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999471466666667">
This article concerns Automated Genre Classification (AGC). Genre has a range of
definitions, but for Language Technology, a good one is a class of documents that share
a communicative purpose (e.g., Kessler, Nunberg, and Sch¨utze 1997). Although com-
municative purpose may be difficult to recognize without document understanding,
researchers have found low-level features of texts to correlate with genre, making it a
useful proxy.
AGC can directly benefit Information Retrieval (Freund, Clarke, and Toms 2006),
where users may want documents that serve a particular communicative purpose
(instructions, reviews, user guides, etc.). AGC can also benefit Language Technology
indirectly, where differences in the low-level properties that correlate with genre may
impact system performance. For example, if a part–of–speech (PoS) tagger or Statistical
Machine Translation system trained on a corpus of editorials was then used for PoS
tagging or translating a corpus of letters to the editor, it would benefit from the knowledge
that inter alia the likelihood of the word “states” being a verb is considerably higher in
letters (-20%) than in editorials (-2%).1
</bodyText>
<affiliation confidence="0.801077">
* University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK.
</affiliation>
<email confidence="0.992677">
E-mail: p.petrenz@sms.ed.ac.uk.
</email>
<affiliation confidence="0.843724">
** University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK.
</affiliation>
<email confidence="0.976238">
E-mail: bonnie.webber@ed.ac.uk.
</email>
<footnote confidence="0.718095">
1 This holds in the NYT Annotated Corpus, whether the topics are as different as Health and Defense.
Submission received: 23 July 2010; revised submission received: 15 October 2010; accepted for publication:
8 December 2010.
</footnote>
<note confidence="0.823992">
© 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.999947954545454">
Genre differs from topic, which is what a text is about. Theoretically, a text from any
given genre can be about any given topic (Finn and Kushmerick 2006), yet it is clear that
co-variances exist between genre and topic, with some genre–topic combinations more
likely than others (cf. fiction vs. news reports about dragons).
Because both genre classification and topic classification exploit low-level features
of text as a basis for their predictions, a feature indicative of topic might benefit a genre
classifier through correlations in the training corpus. However, if the topics addressed in
different genres can change unpredictably over time, such correlated features can then
harm performance. Although domain adaptation techniques might remedy this, they
typically require extensive data in the target domain, and the remedy may fail as soon
as the distribution changes again.
To date, the correlation between topic and genre has not been quantified, nor has
the extent to which it may change in an actively growing corpus. In order to motivate
research on stability in AGC,2 we analyzed a large, publicly available newspaper corpus
and found that (1) genres and topics do correlate substantially and (2) these correlations
vary substantially over time.3
This squib makes two points: (1) Low-level features that correlate with topic can
degrade the performance of AGC systems and are best removed unless the genre–topic
distribution is guaranteed to be fixed, and (2) PoS features should not be lumped to-
gether in AGC because they have different correlations with genre and topic. Although
the experiments used to make these points reflect an extreme situation—a complete
change in genre–topic distribution—they do allow us to make these points convincingly.
</bodyText>
<sectionHeader confidence="0.958247" genericHeader="keywords">
2. Method
</sectionHeader>
<bodyText confidence="0.997926944444444">
The data for our experiments come from the New York Times Annotated Corpus (NYTAC)
(Sandhaus 2008), covering 21 years of publication (1987–2007) and more than 1.8 million
articles.4 Articles are richly annotated with meta-data, including fields whose values can
be used to infer their genre and topic.
Genre: Two meta-data fields are related to the notion of genre as communicative pur-
pose: Types of Material and Taxonomic Classifier. The former, appearing with 41.5% of
articles, specifies the editorial category of an article. Usually the field has a single value,
sometimes more than one. Although these values are not drawn from a fixed set, they
can be used to infer genre after spelling errors are corrected (e.g., from Reivew to Review)
and similar values are merged (e.g., Editorial, editorial, Op-Ed, and Editors’ Note).
The values in the second genre-related field (Taxonomic Classifier) are drawn from
a hierarchy, some of whose divisions indicate the section of the newspaper in which
a document appears. In total, 99.5% of documents in the corpus contain a Taxonomic
Classifier field, with an average of 4.5 values per article. Although the hierarchy varies
in depth, its second level comprises a set of four fairly high level genres—that is, Top/
Classifieds, Top/Features, Top/News, and Top/Opinion.
Because the Types of Material field did not include news reports, we used the Taxo-
nomic Classifier field to recognize documents from this genre. Specifically, we considered
</bodyText>
<footnote confidence="0.9913074">
2 The term stability is used in machine learning to describe the repeatability of a learner’s results (cf.
Turney 1995). Here, we use it to describe the robustness of a method to (topical) domain changes or
changes in the topic–genre distribution.
3 The results of our analysis can be found at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.
4 www.ldc.upenn.edu, Catalog Id=LDC2008T19.
</footnote>
<page confidence="0.997383">
386
</page>
<note confidence="0.903267">
Petrenz and Webber Stable Classification of Text Genres
</note>
<tableCaption confidence="0.990231">
Table 1
</tableCaption>
<figure confidence="0.581985769230769">
Genre (columns) and topic (rows) distribution in the data sets used in the experiments.
Training set Test set 1 Test set 2
(3 x 4,309 = 12,927 texts) (3 x 2,155 = 6,465 texts) (6 x 2,285 = 13,710 texts)
News Edit. LttE News Edit. LttE News Edit. LttE
Edu x
Def x
Med x
Edu x
Def x
Med x
Edu x x
Def x x
Med x x
</figure>
<figureCaption confidence="0.143772">
any document with no Types of Material tag as a news report, if at least one of its Taxonomic
Classifier values started with Top/News.
</figureCaption>
<bodyText confidence="0.999923787878788">
Topic: Topic descriptors were drawn from the General Online Descriptors meta-data field.
The field appears with 79.7% of documents, with 3.3 descriptors per document on
average.5 Whereas General Online Descriptors are structured in a hierarchy, a document
tagged with the more specific United States Politics and Government will also typically be
tagged with the less-specific (i.e., closer to the root) value Politics and Government, but
not vice versa.
Framework: Our experiments use news reports, editorials, and letters as target variables
because similar classes have been used elsewhere in AGC research (e.g., Finn and
Kushmerick 2006; Karlgren and Cutting 1994). For topics, we chose three that occur
frequently and that were distinct from each other, in order to maximize differences in
the formal cues used by classifiers. We based distinctiveness on the percentage overlap
of topic tags in the corpus: Topics were taken to be distinct if (for our three chosen
genres) fewer than 5% of texts that had one of the tags had another of them. The
degree of overlap in the three topic tags we chose, (Education and Schools), (Armament,
Defense and Military Forces), and (Medicine and Health), ranges from 0.5% to 2.9%.
For comparison, the degree of overlap of the pair (Politics and Government) and
(International Relations) ranges from 28.4% to 38.1% for our three genres. For all ex-
periments, we only used texts which were unambiguously about Education, Medicine,
or Defense. The small proportion of documents with more than one of these tags was
ignored.
In order to examine the impact on system performance of a complete shift in
topics, we varied the topical distribution of the test sets with respect to the training
set. The training set consisted of 12,927 texts: News reports (News) about Education (Edu),
Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med).
(This pairing yields more articles in the corpus than any of the five other possible
combinations.) Table 1 shows the genre–topic distribution of both the training set and
the two test sets used in these experiments. The first test set (6,465 articles) had the
same distribution as the training set. In the second test set (13,710 articles), genre–topic
pairings were inverted (see Table 1). All sets were balanced with an equal number of
texts for each genre–topic combination (where not zero). The difference in the size of the
test sets reflects the number of articles available with the desired topic–genre pairing.
The training set and test set 1 were created by a random 2:1 split within each genre class.
Because test set 2 comes from a different distribution than the training set, we report
</bodyText>
<footnote confidence="0.9220555">
5 Genre-related differences in the number of General Online Descriptors are described further at
http://homepages.inf.ed.ac.uk/s0895822/SCTG/.
</footnote>
<page confidence="0.985621">
387
</page>
<note confidence="0.273044">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.961271428571428">
results on these large holdout sets rather than performing cross-validation. We inferred
confidence intervals by assuming that the number of misclassifications is approximately
,/
normally distributed with mean µ = e x n and standard deviation 6 = µ x (1 − e),
where e is the percentage of misclassified instances and n is the size of the test set.
We took two classification results to differ significantly only if their 95% confidence
intervals (i.e., µ ± 1.96 x 6) did not overlap.
</bodyText>
<subsectionHeader confidence="0.461198">
3. Assessing Performance on Static and Altered Genre–Topic Distributions
</subsectionHeader>
<bodyText confidence="0.999557666666667">
To make our first point—that low-level features that correlate with topic can degrade
the performance of AGC systems and are best removed unless the genre–topic dis-
tribution is fixed—we show how five published approaches to AGC perform in our
experimental framework (Section 3). The choice of methods was partly motivated by
the study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequen-
cies, and text statistics as document representations in genre classification tasks across
topical domains. The methods we assessed were chosen so that all these features were
represented to different degrees. All were implemented on the same platform (Petrenz
2009).
KC: Karlgren and Cutting (1994) use a small set of textual features and discriminant
analysis to predict genres. Most of these features involve either PoS frequencies or text
statistics. Counts based on the fixed length of texts used in their experiments were
adjusted to represent frequencies rather than absolute counts.
KNS/KNSPOS: Kessler, Nunberg, and Sch¨utze (1997) predict genre based on surface
cues. Because the paper gives few details about the specific features they use, we
communicated with the authors directly. The list they gave us included features that
require PoS tagging. As their published experiments do not make use of such features,
we included two versions of their method, one version with PoS-based features and one
without. 6
FCT: Freund, Clarke, and Toms (2006) predict genre using a support vector machine on
a simple bag-of-words representation of a text. This feature set is not filtered using stop
words or other techniques.
FMOG: Feldman et al. (2009) use part-of-speech histograms and principal component
analysis to construct features. The authors classify genres using the QDA and Naive
Bayes algorithms. We followed their decision to compute histograms on a sliding win-
dow of five PoS tags.
SWM: Sharoff, Wu, and Markert (2010) found a character n-gram based feature set
to perform better than PoS n-grams and word n-grams in extensive AGC experi-
ments using nine data collections and different choices of n. Although variable length
n-grams as features for genre classification had previously been proposed by Kanaris
and Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length
4-grams, which they found to yield higher accuracies.
Although a variety of Machine Learning (ML) methods were used in these ap-
proaches, here we just used the SVM implementation by Joachims (1999) because other
ML methods produced similar, albeit poorer, results (Petrenz 2009). For PoS tagging, we
used the Stanford maximum entropy tagger described in Toutanova et al. (2003).
</bodyText>
<footnote confidence="0.902054">
6 The features we used are listed at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.
</footnote>
<page confidence="0.989407">
388
</page>
<note confidence="0.924107">
Petrenz and Webber Stable Classification of Text Genres
</note>
<figureCaption confidence="0.7956295">
Figure 1
Classification accuracy of six different genre classification methods (cf. Section 3). Chart shows
the percentage of correctly classified instances in both test sets (TS 1/2, cf. Table 1), with the
confidence interval boundaries given in parentheses.
</figureCaption>
<bodyText confidence="0.99131575">
Figure 1 shows the results of training and testing on the same genre–topic distribu-
tion (test set 1). These results confirm the findings of SWM that binary character n-grams
are good features in AGC. Both their approach and the bag-of-words approach used by
FCT significantly outperform all other methods when the genre–topic distribution is the
same for training and testing.
A different picture emerges from the second test set, however, whose genre–topic
distributions differ from the training set. It shows that some feature sets owe their good
results on test set 1 to the strong correlation between topics and genres. The performance
of both SWM and FCT is significantly worse, with the latter even worse than the 33.3%
that a random guess classifier would achieve in this balanced 3-class classification task.
The performance drop for both KC and KNS is slight but still significant. Whereas
KNSPOS had significantly outperformed KNS on test set 1, its performance is signifi-
cantly worse than that of KNS on test set 2. (More on this shortly.)
Some of these results are not surprising: As bag-of-words and character n-grams re-
flect lexical differences of texts, systems that rely on them (SWM and FCT) will be misled
by a major change in genre–topic distribution. Similar findings were reported in Finn
and Kushmerick (2006) for bag-of-words and in (SWM) for character n-grams, although
no explicit tests with topical distributions were carried out in the latter. More surprising
are the results involving PoS tags: Two of the methods that used PoS tags—FMOG
and KNSPOS—suffered when the genre–topic distribution changed, even though PoS
frequencies had previously been reported to perform well as a feature set when used in
new topical domains (Finn and Kushmerick 2006). However, the PoS frequencies in the
KC feature set did not seem to harm stability much. This brings us to the second point
of this squib.
</bodyText>
<sectionHeader confidence="0.714934" genericHeader="introduction">
4. Impact of PoS Features on Performance and Stability
</sectionHeader>
<bodyText confidence="0.999781666666667">
We justify our second point—that PoS features should not be lumped together in
AGC because they have different correlations with genre and topic—through a set of
experiments that assess the effect of adding PoS features to a set of basic non-PoS
</bodyText>
<page confidence="0.997565">
389
</page>
<table confidence="0.419397">
Computational Linguistics Volume 37, Number 2
</table>
<tableCaption confidence="0.98664">
Table 2
</tableCaption>
<table confidence="0.91283575">
Prediction accuracies for the baseline feature set and the same set with all 36 PoS tags added.
Test set 1 Test set 2
13 surface features (Baseline) 72.9% (f 1.1%) 70.8% (f 0.8%)
13 surface + 36 PoS features 87.1% (f 0.8%) 72.5% (f 0.7%)
</table>
<bodyText confidence="0.999112351351352">
features similar to those used earlier by Karlgren and Cutting (1994), here normalized by
document length (in words). These basic non-PoS features include character count per
document, sentence count per document, average character count per sentence, average
word count per sentence, average character count per word, type/token ratio, frequency
of long words (ones with more than 6 letters), and the frequencies of the words therefore,
I, me, it, that, and which.
The texts were PoS-tagged (Toutanova et al. 2003), using the same tag set as in the
Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). All 36 non-punctuation
tags were used, and counts of PoS-tags were normalized by document length.
Each experiment involved the 13 non-PoS features and a single PoS frequency
feature (36 sets). Comparing performance with that on the non-PoS features alone (as
a baseline) demonstrated the effect of adding each PoS frequency feature on classifier
accuracy and stability.7 All 36 feature sets as well as the baseline set were trained on
the same training set. They were then tested on both test sets described in Section 2. As
before, we use the same Support Vector Machines (SVM) classifier in all experiments,
with the set of features as the experimental variable.
Table 2 shows both the accuracy of the baseline system on the two test sets as well as
the accuracy of a system with the baseline features plus all 36 PoS features. Recall from
Table 1 that the genre–topic distribution for test set 1 is the same as in the training set,
whereas in test set 2 it is different. The first thing to note in Table 2 is that the classifier
performs significantly better on the larger set of 49 features than on the smaller set of
13 basic features when the genre–topic distribution is not altered (column 2). When it is
altered (column 3), the losses are less severe on our baseline than on the set that includes
PoS features. This can be explained by looking at the contributions of each feature.
Figure 2 shows how accuracy changes when each PoS feature is added individually
to the basic set of non-PoS features. To highlight the most interesting results, we only
show features which cause a deviation of more than 1% from the baseline for at least
one of the two test sets.8 When the topic–genre distribution remains the same (i.e., test
set 1), PoS frequencies appear to have a positive impact on prediction accuracy: When
added to the basic feature set, accuracy increases. This is especially true for the tags VBD
(past tense verb), JJ (adjective), RB (adverb), NN (singular noun), VB (base form verb),
and NNP (plural proper noun).
The same is not true when the topic–genre distribution is changed (test set 2).
Figure 2 shows that the PoS tags NN (singular noun), NNS (plural noun), and NNPS
(plural proper noun) all have a toxic effect when added to the baseline set. This means
the NN, NNS, and NNPS frequency features improve accuracy in stable conditions,
whereas they severely harm it as topics change. This is not good for stability. A similar,
</bodyText>
<footnote confidence="0.817106333333333">
7 The goal was not to select the best subset of PoS features—for that one would use a different method—but
rather to show precisely how PoS features differ from each other with respect to AGC stability.
8 The full results can be found at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.
</footnote>
<page confidence="0.993287">
390
</page>
<note confidence="0.955106">
Petrenz and Webber Stable Classification of Text Genres
</note>
<figureCaption confidence="0.968912">
Figure 2
</figureCaption>
<bodyText confidence="0.9656935">
Deviation in percentage of correctly classified instances from the baseline feature set (cf.
Table 2) for each added PoS tag frequency and test set (TS 1/2, cf. Table 1). CC = coordinating
conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP =
proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB =
base form verb; VBD = past tense verb; VBZ = third-person singular present verb.
if somewhat weaker effect, can be observed for the CD (cardinal number) frequency.
Other PoS tags like CC (coordinating conjunction), RB (adverb), and VB (base form
verb) increase predictive power while not impairing stability. Even more interesting are
the results for the tags VBD (past tense verb) and VBZ (third-person singular present
verb). Adding these features eliminates the significant difference between accuracies on
test set 1 and 2, which we observed on the baseline feature set.
This makes sense if we consider how different genres and topics vary in their use
of different parts of speech. In our data sets, for example, the frequency of plural noun
(NNS) varies more by topic (on average, 8.0% of words are NNS in texts on Education
and Schools, 7.6% in texts on Medicine and Health, and 6.3% in texts on Defense and Military
Forces) than it does by genre (on average, 7.3% of words are NNS in news reports, 7.1%
in letters, and 7.5% in editorials). The opposite holds for past tense verbs (VBD): Their
average frequency varies less by topic (2.9%, 2.9%, and 3.3%) than by genre (4.8%, 1.8%,
and 2.5%).
The odd result here is that singular proper noun (NNP) frequencies do not impair
classifier stability: Unlike NN, NNS, and NNPS frequencies, they are topic-independent.
This is because the most commonly tagged singular proper nouns in news reports are
titles such as “Mr.”, “Ms.”, “Dr.”, etc., regardless of the topic. The frequency of titles
among proper nouns is much lower in editorials and even lower in letters, across all
three topical domains. NNP frequency in news reports is increased by the fact that titles
are usually followed by one or more names, which are also singular proper nouns. We
assume that this is the reason that the fluctuation between NNP frequencies is relatively
low across topics (for the three topics and genres we examined) and hence a stable
contributor to genre prediction.
Note that we are not making any claim about whether these specific results (e.g.,
that NN frequencies are bad for stability) hold for settings with different genres and
topics. Rather, our point is that PoS tags should not be included or excluded wholesale
</bodyText>
<page confidence="0.993917">
391
</page>
<note confidence="0.588685">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.989793666666667">
for AGC. If one is going to the expense of PoS-tagging texts, only a subset of PoS tags
should be used as features in AGC in order to maintain performance across changes in
the topical distribution.
</bodyText>
<sectionHeader confidence="0.997557" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999931555555556">
Our results suggest that prediction accuracy on a static topic distribution should not
be the sole basis for assessing the quality of Automatic Genre Classification sys-
tems: In particular, approaches that perform well on a static topic distribution can be
severely impacted by changes in topical distributions. The notion of topic indepen-
dence for features has rarely been explored in the literature on genre classification.
Nevertheless, this is an important issue, especially in dynamic environments like the
World Wide Web, where new topics emerge rapidly and unpredictably. In topical
domains with little or no labeled data to train on, instability can impede any useful
application of classifiers. Because of this, we believe that stability should join accu-
racy as a criterion for assessing any new developments in genre classification. To this
end, we introduced a cross-product methodology in Section 2 as a way of assessing
stability.
Our results also suggest that, where the cost of PoS-tagging is acceptable, selective
use of PoS-based features can yield high performance that is stable even when topical
distribution differs from training to test sets. Although we have not identified a set of
PoS-based features that supports classification among an arbitrary set of genres, we
can say that it is crucial to evaluate and select PoS-based features carefully in order to
achieve genre classification which is as topic-independent as possible.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995108">
We would like to thank Katja Markert,
Mark Steedman, Maria Wolters, and four
anonymous reviewers, each of whom gave
us many valuable comments on earlier
drafts of this article.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834744680851">
Feldman, S., M. A. Marin, M. Ostendorf,
and M. R. Gupta. 2009. Part-of-speech
histograms for genre classification of
text. In Proceedings of the 2009 IEEE
International Conference on Acoustics,
Speech and Signal Processing,
pages 4781–4784, Washington, DC.
Finn, Aidan and Nicholas Kushmerick.
2006. Learning to classify documents
according to genre. Journal of the
American Society for Information Science
and Technology, 57(11):1506–1518.
Freund, Luanne, Charles L. A. Clarke,
and Elaine G. Toms. 2006. Towards
genre classification for IR in the
workplace. In Proceedings of the 1st
International Conference on Information
Interaction in Context, pages 30–36,
New York, NY.
Joachims, Thorsten.1999. Making
large-scale support vector machine
learning practical. In B. Sch¨olkopf,
C. J. C. Burges, and A. J. Smola, editors,
Advances in Kernel Methods: Support Vector
Learning. MIT Press, Cambridge, MA,
pages 169–184.
Kanaris, Ioannis and Efstathios Stamatatos.
2007. Webpage genre identification using
variable-length character n-grams. In
Proceedings of the 19th IEEE International
Conference on Tools with AI, pages 3–10,
Washington, DC.
Karlgren, Jussi and Douglass Cutting.
1994. Recognizing text genres with
simple metrics using discriminant
analysis. In Proceedings of the 15th
Conference on Computational Linguistics,
pages 1071–1075, Morristown, NJ.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Sch¨utze. 1997. Automatic
detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics, pages 32–38,
Morristown, NJ.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
</reference>
<page confidence="0.992116">
392
</page>
<note confidence="0.867244">
Petrenz and Webber Stable Classification of Text Genres
</note>
<reference confidence="0.999236423076923">
English: The Penn treebank. Computational
Linguistics, 19(2):313–330.
Petrenz, Philipp. 2009. Assessing approaches
to genre classification. M.Sc. thesis, School
of Informatics, University of Edinburgh.
Sandhaus, Evan. 2008. New York Times
corpus: Corpus overview. LDC catalogue
entry LDC2008T19.
Sharoff, Serge, Zhili Wu, and Katja
Markert. 2010. The Web Library of
Babel: Evaluating genre collections.
In Proceedings of the Seventh Conference
on International Language Resources
and Evaluation, pages 3063–3070,
Valletta.
Toutanova, Kristina, Dan Klein,
Christopher D. Manning, and Yoram
Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency
network. In Proceedings of the 2003
Conference of the North American Chapter of
the ACL and Human Language Technology,
pages 173–180, Morristown, NJ.
Turney, Peter. 1995. Technical note: Bias
and the quantification of stability.
Machine Learning, 20(1–2):23–33.
</reference>
<page confidence="0.99937">
393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548121">
<title confidence="0.988059">Squibs</title>
<author confidence="0.616278">Stable Classification of Text Genres</author>
<affiliation confidence="0.998891">University of Edinburgh University of Edinburgh</affiliation>
<abstract confidence="0.9849473">Every text has at least one topic and at least one genre. Evidence for a text’s topic and genre comes, in part, from its lexical and syntactic features—features used in both Automatic Topic Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should be stable in the face of changes in topic distribution, we assess five previously published AGC methods with respect to both performance on the same topic–genre distribution on which they were trained and stability of that performance across changes in topic–genre distribution. Our experiments lead us to conclude that (1) stability in the face of changing topical distributions should be added to the evaluation critera for new approaches to AGC, and (2) part-of-speech features should be considered individually when developing a high-performing, stable AGC system for a particular, possibly changing corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Feldman</author>
<author>M A Marin</author>
<author>M Ostendorf</author>
<author>M R Gupta</author>
</authors>
<title>Part-of-speech histograms for genre classification of text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>4781--4784</pages>
<location>Washington, DC.</location>
<contexts>
<context position="12011" citStr="Feldman et al. (2009)" startWordPosition="1879" endWordPosition="1882"> (1997) predict genre based on surface cues. Because the paper gives few details about the specific features they use, we communicated with the authors directly. The list they gave us included features that require PoS tagging. As their published experiments do not make use of such features, we included two versions of their method, one version with PoS-based features and one without. 6 FCT: Freund, Clarke, and Toms (2006) predict genre using a support vector machine on a simple bag-of-words representation of a text. This feature set is not filtered using stop words or other techniques. FMOG: Feldman et al. (2009) use part-of-speech histograms and principal component analysis to construct features. The authors classify genres using the QDA and Naive Bayes algorithms. We followed their decision to compute histograms on a sliding window of five PoS tags. SWM: Sharoff, Wu, and Markert (2010) found a character n-gram based feature set to perform better than PoS n-grams and word n-grams in extensive AGC experiments using nine data collections and different choices of n. Although variable length n-grams as features for genre classification had previously been proposed by Kanaris and Stamatatos (2007), we fol</context>
</contexts>
<marker>Feldman, Marin, Ostendorf, Gupta, 2009</marker>
<rawString>Feldman, S., M. A. Marin, M. Ostendorf, and M. R. Gupta. 2009. Part-of-speech histograms for genre classification of text. In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 4781–4784, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Learning to classify documents according to genre.</title>
<date>2006</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>57</volume>
<issue>11</issue>
<contexts>
<context position="2876" citStr="Finn and Kushmerick 2006" startWordPosition="424" endWordPosition="427">h EH8 9AB, UK. E-mail: p.petrenz@sms.ed.ac.uk. ** University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK. E-mail: bonnie.webber@ed.ac.uk. 1 This holds in the NYT Annotated Corpus, whether the topics are as different as Health and Defense. Submission received: 23 July 2010; revised submission received: 15 October 2010; accepted for publication: 8 December 2010. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 2 Genre differs from topic, which is what a text is about. Theoretically, a text from any given genre can be about any given topic (Finn and Kushmerick 2006), yet it is clear that co-variances exist between genre and topic, with some genre–topic combinations more likely than others (cf. fiction vs. news reports about dragons). Because both genre classification and topic classification exploit low-level features of text as a basis for their predictions, a feature indicative of topic might benefit a genre classifier through correlations in the training corpus. However, if the topics addressed in different genres can change unpredictably over time, such correlated features can then harm performance. Although domain adaptation techniques might remedy </context>
<context position="7527" citStr="Finn and Kushmerick 2006" startWordPosition="1163" endWordPosition="1166">criptors were drawn from the General Online Descriptors meta-data field. The field appears with 79.7% of documents, with 3.3 descriptors per document on average.5 Whereas General Online Descriptors are structured in a hierarchy, a document tagged with the more specific United States Politics and Government will also typically be tagged with the less-specific (i.e., closer to the root) value Politics and Government, but not vice versa. Framework: Our experiments use news reports, editorials, and letters as target variables because similar classes have been used elsewhere in AGC research (e.g., Finn and Kushmerick 2006; Karlgren and Cutting 1994). For topics, we chose three that occur frequently and that were distinct from each other, in order to maximize differences in the formal cues used by classifiers. We based distinctiveness on the percentage overlap of topic tags in the corpus: Topics were taken to be distinct if (for our three chosen genres) fewer than 5% of texts that had one of the tags had another of them. The degree of overlap in the three topic tags we chose, (Education and Schools), (Armament, Defense and Military Forces), and (Medicine and Health), ranges from 0.5% to 2.9%. For comparison, th</context>
<context position="10715" citStr="Finn and Kushmerick (2006)" startWordPosition="1679" endWordPosition="1682">ssified instances and n is the size of the test set. We took two classification results to differ significantly only if their 95% confidence intervals (i.e., µ ± 1.96 x 6) did not overlap. 3. Assessing Performance on Static and Altered Genre–Topic Distributions To make our first point—that low-level features that correlate with topic can degrade the performance of AGC systems and are best removed unless the genre–topic distribution is fixed—we show how five published approaches to AGC perform in our experimental framework (Section 3). The choice of methods was partly motivated by the study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequencies, and text statistics as document representations in genre classification tasks across topical domains. The methods we assessed were chosen so that all these features were represented to different degrees. All were implemented on the same platform (Petrenz 2009). KC: Karlgren and Cutting (1994) use a small set of textual features and discriminant analysis to predict genres. Most of these features involve either PoS frequencies or text statistics. Counts based on the fixed length of texts used in their experiments were adjusted to represent frequenc</context>
<context position="14776" citStr="Finn and Kushmerick (2006)" startWordPosition="2312" endWordPosition="2315">worse than the 33.3% that a random guess classifier would achieve in this balanced 3-class classification task. The performance drop for both KC and KNS is slight but still significant. Whereas KNSPOS had significantly outperformed KNS on test set 1, its performance is significantly worse than that of KNS on test set 2. (More on this shortly.) Some of these results are not surprising: As bag-of-words and character n-grams reflect lexical differences of texts, systems that rely on them (SWM and FCT) will be misled by a major change in genre–topic distribution. Similar findings were reported in Finn and Kushmerick (2006) for bag-of-words and in (SWM) for character n-grams, although no explicit tests with topical distributions were carried out in the latter. More surprising are the results involving PoS tags: Two of the methods that used PoS tags—FMOG and KNSPOS—suffered when the genre–topic distribution changed, even though PoS frequencies had previously been reported to perform well as a feature set when used in new topical domains (Finn and Kushmerick 2006). However, the PoS frequencies in the KC feature set did not seem to harm stability much. This brings us to the second point of this squib. 4. Impact of </context>
</contexts>
<marker>Finn, Kushmerick, 2006</marker>
<rawString>Finn, Aidan and Nicholas Kushmerick. 2006. Learning to classify documents according to genre. Journal of the American Society for Information Science and Technology, 57(11):1506–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luanne Freund</author>
<author>Charles L A Clarke</author>
<author>Elaine G Toms</author>
</authors>
<title>Towards genre classification for IR in the workplace.</title>
<date>2006</date>
<booktitle>In Proceedings of the 1st International Conference on Information Interaction in Context,</booktitle>
<pages>30--36</pages>
<location>New York, NY.</location>
<marker>Freund, Clarke, Toms, 2006</marker>
<rawString>Freund, Luanne, Charles L. A. Clarke, and Elaine G. Toms. 2006. Towards genre classification for IR in the workplace. In Proceedings of the 1st International Conference on Information Interaction in Context, pages 30–36, New York, NY.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thorsten 1999 Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<booktitle>Advances in Kernel Methods: Support Vector Learning.</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Joachims, </marker>
<rawString>Joachims, Thorsten.1999. Making large-scale support vector machine learning practical. In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector Learning. MIT Press, Cambridge, MA, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Kanaris</author>
<author>Efstathios Stamatatos</author>
</authors>
<title>Webpage genre identification using variable-length character n-grams.</title>
<date>2007</date>
<booktitle>In Proceedings of the 19th IEEE International Conference on Tools with AI,</booktitle>
<pages>3--10</pages>
<location>Washington, DC.</location>
<contexts>
<context position="12603" citStr="Kanaris and Stamatatos (2007)" startWordPosition="1970" endWordPosition="1973">chniques. FMOG: Feldman et al. (2009) use part-of-speech histograms and principal component analysis to construct features. The authors classify genres using the QDA and Naive Bayes algorithms. We followed their decision to compute histograms on a sliding window of five PoS tags. SWM: Sharoff, Wu, and Markert (2010) found a character n-gram based feature set to perform better than PoS n-grams and word n-grams in extensive AGC experiments using nine data collections and different choices of n. Although variable length n-grams as features for genre classification had previously been proposed by Kanaris and Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length 4-grams, which they found to yield higher accuracies. Although a variety of Machine Learning (ML) methods were used in these approaches, here we just used the SVM implementation by Joachims (1999) because other ML methods produced similar, albeit poorer, results (Petrenz 2009). For PoS tagging, we used the Stanford maximum entropy tagger described in Toutanova et al. (2003). 6 The features we used are listed at http://homepages.inf.ed.ac.uk/s0895822/SCTG/. 388 Petrenz and Webber Stable Classification of Text Genres Figure 1 Classific</context>
</contexts>
<marker>Kanaris, Stamatatos, 2007</marker>
<rawString>Kanaris, Ioannis and Efstathios Stamatatos. 2007. Webpage genre identification using variable-length character n-grams. In Proceedings of the 19th IEEE International Conference on Tools with AI, pages 3–10, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Douglass Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th Conference on Computational Linguistics,</booktitle>
<pages>1071--1075</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="7555" citStr="Karlgren and Cutting 1994" startWordPosition="1167" endWordPosition="1170">he General Online Descriptors meta-data field. The field appears with 79.7% of documents, with 3.3 descriptors per document on average.5 Whereas General Online Descriptors are structured in a hierarchy, a document tagged with the more specific United States Politics and Government will also typically be tagged with the less-specific (i.e., closer to the root) value Politics and Government, but not vice versa. Framework: Our experiments use news reports, editorials, and letters as target variables because similar classes have been used elsewhere in AGC research (e.g., Finn and Kushmerick 2006; Karlgren and Cutting 1994). For topics, we chose three that occur frequently and that were distinct from each other, in order to maximize differences in the formal cues used by classifiers. We based distinctiveness on the percentage overlap of topic tags in the corpus: Topics were taken to be distinct if (for our three chosen genres) fewer than 5% of texts that had one of the tags had another of them. The degree of overlap in the three topic tags we chose, (Education and Schools), (Armament, Defense and Military Forces), and (Medicine and Health), ranges from 0.5% to 2.9%. For comparison, the degree of overlap of the p</context>
<context position="11056" citStr="Karlgren and Cutting (1994)" startWordPosition="1728" endWordPosition="1731">degrade the performance of AGC systems and are best removed unless the genre–topic distribution is fixed—we show how five published approaches to AGC perform in our experimental framework (Section 3). The choice of methods was partly motivated by the study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequencies, and text statistics as document representations in genre classification tasks across topical domains. The methods we assessed were chosen so that all these features were represented to different degrees. All were implemented on the same platform (Petrenz 2009). KC: Karlgren and Cutting (1994) use a small set of textual features and discriminant analysis to predict genres. Most of these features involve either PoS frequencies or text statistics. Counts based on the fixed length of texts used in their experiments were adjusted to represent frequencies rather than absolute counts. KNS/KNSPOS: Kessler, Nunberg, and Sch¨utze (1997) predict genre based on surface cues. Because the paper gives few details about the specific features they use, we communicated with the authors directly. The list they gave us included features that require PoS tagging. As their published experiments do not </context>
<context position="16029" citStr="Karlgren and Cutting (1994)" startWordPosition="2522" endWordPosition="2525">e and Stability We justify our second point—that PoS features should not be lumped together in AGC because they have different correlations with genre and topic—through a set of experiments that assess the effect of adding PoS features to a set of basic non-PoS 389 Computational Linguistics Volume 37, Number 2 Table 2 Prediction accuracies for the baseline feature set and the same set with all 36 PoS tags added. Test set 1 Test set 2 13 surface features (Baseline) 72.9% (f 1.1%) 70.8% (f 0.8%) 13 surface + 36 PoS features 87.1% (f 0.8%) 72.5% (f 0.7%) features similar to those used earlier by Karlgren and Cutting (1994), here normalized by document length (in words). These basic non-PoS features include character count per document, sentence count per document, average character count per sentence, average word count per sentence, average character count per word, type/token ratio, frequency of long words (ones with more than 6 letters), and the frequencies of the words therefore, I, me, it, that, and which. The texts were PoS-tagged (Toutanova et al. 2003), using the same tag set as in the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). All 36 non-punctuation tags were used, and counts of PoS-tag</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>Karlgren, Jussi and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proceedings of the 15th Conference on Computational Linguistics, pages 1071–1075, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
<author>Geoffrey Nunberg</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>32--38</pages>
<location>Morristown, NJ.</location>
<marker>Kessler, Nunberg, Sch¨utze, 1997</marker>
<rawString>Kessler, Brett, Geoffrey Nunberg, and Hinrich Sch¨utze. 1997. Automatic detection of text genre. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 32–38, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Petrenz</author>
</authors>
<title>Assessing approaches to genre classification.</title>
<date>2009</date>
<tech>M.Sc. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="11023" citStr="Petrenz 2009" startWordPosition="1725" endWordPosition="1726">ate with topic can degrade the performance of AGC systems and are best removed unless the genre–topic distribution is fixed—we show how five published approaches to AGC perform in our experimental framework (Section 3). The choice of methods was partly motivated by the study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequencies, and text statistics as document representations in genre classification tasks across topical domains. The methods we assessed were chosen so that all these features were represented to different degrees. All were implemented on the same platform (Petrenz 2009). KC: Karlgren and Cutting (1994) use a small set of textual features and discriminant analysis to predict genres. Most of these features involve either PoS frequencies or text statistics. Counts based on the fixed length of texts used in their experiments were adjusted to represent frequencies rather than absolute counts. KNS/KNSPOS: Kessler, Nunberg, and Sch¨utze (1997) predict genre based on surface cues. Because the paper gives few details about the specific features they use, we communicated with the authors directly. The list they gave us included features that require PoS tagging. As th</context>
<context position="12941" citStr="Petrenz 2009" startWordPosition="2025" endWordPosition="2026">re set to perform better than PoS n-grams and word n-grams in extensive AGC experiments using nine data collections and different choices of n. Although variable length n-grams as features for genre classification had previously been proposed by Kanaris and Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length 4-grams, which they found to yield higher accuracies. Although a variety of Machine Learning (ML) methods were used in these approaches, here we just used the SVM implementation by Joachims (1999) because other ML methods produced similar, albeit poorer, results (Petrenz 2009). For PoS tagging, we used the Stanford maximum entropy tagger described in Toutanova et al. (2003). 6 The features we used are listed at http://homepages.inf.ed.ac.uk/s0895822/SCTG/. 388 Petrenz and Webber Stable Classification of Text Genres Figure 1 Classification accuracy of six different genre classification methods (cf. Section 3). Chart shows the percentage of correctly classified instances in both test sets (TS 1/2, cf. Table 1), with the confidence interval boundaries given in parentheses. Figure 1 shows the results of training and testing on the same genre–topic distribution (test se</context>
</contexts>
<marker>Petrenz, 2009</marker>
<rawString>Petrenz, Philipp. 2009. Assessing approaches to genre classification. M.Sc. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>New York Times corpus: Corpus overview. LDC catalogue entry LDC2008T19.</title>
<date>2008</date>
<contexts>
<context position="4599" citStr="Sandhaus 2008" startWordPosition="691" endWordPosition="692">3 This squib makes two points: (1) Low-level features that correlate with topic can degrade the performance of AGC systems and are best removed unless the genre–topic distribution is guaranteed to be fixed, and (2) PoS features should not be lumped together in AGC because they have different correlations with genre and topic. Although the experiments used to make these points reflect an extreme situation—a complete change in genre–topic distribution—they do allow us to make these points convincingly. 2. Method The data for our experiments come from the New York Times Annotated Corpus (NYTAC) (Sandhaus 2008), covering 21 years of publication (1987–2007) and more than 1.8 million articles.4 Articles are richly annotated with meta-data, including fields whose values can be used to infer their genre and topic. Genre: Two meta-data fields are related to the notion of genre as communicative purpose: Types of Material and Taxonomic Classifier. The former, appearing with 41.5% of articles, specifies the editorial category of an article. Usually the field has a single value, sometimes more than one. Although these values are not drawn from a fixed set, they can be used to infer genre after spelling error</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Sandhaus, Evan. 2008. New York Times corpus: Corpus overview. LDC catalogue entry LDC2008T19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
<author>Zhili Wu</author>
<author>Katja Markert</author>
</authors>
<title>The Web Library of Babel: Evaluating genre collections.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh Conference on International Language Resources and Evaluation,</booktitle>
<pages>3063--3070</pages>
<location>Valletta.</location>
<marker>Sharoff, Wu, Markert, 2010</marker>
<rawString>Sharoff, Serge, Zhili Wu, and Katja Markert. 2010. The Web Library of Babel: Evaluating genre collections. In Proceedings of the Seventh Conference on International Language Resources and Evaluation, pages 3063–3070, Valletta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the ACL and Human Language Technology,</booktitle>
<pages>173--180</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="13040" citStr="Toutanova et al. (2003)" startWordPosition="2039" endWordPosition="2042">sing nine data collections and different choices of n. Although variable length n-grams as features for genre classification had previously been proposed by Kanaris and Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length 4-grams, which they found to yield higher accuracies. Although a variety of Machine Learning (ML) methods were used in these approaches, here we just used the SVM implementation by Joachims (1999) because other ML methods produced similar, albeit poorer, results (Petrenz 2009). For PoS tagging, we used the Stanford maximum entropy tagger described in Toutanova et al. (2003). 6 The features we used are listed at http://homepages.inf.ed.ac.uk/s0895822/SCTG/. 388 Petrenz and Webber Stable Classification of Text Genres Figure 1 Classification accuracy of six different genre classification methods (cf. Section 3). Chart shows the percentage of correctly classified instances in both test sets (TS 1/2, cf. Table 1), with the confidence interval boundaries given in parentheses. Figure 1 shows the results of training and testing on the same genre–topic distribution (test set 1). These results confirm the findings of SWM that binary character n-grams are good features in </context>
<context position="16475" citStr="Toutanova et al. 2003" startWordPosition="2590" endWordPosition="2593">atures (Baseline) 72.9% (f 1.1%) 70.8% (f 0.8%) 13 surface + 36 PoS features 87.1% (f 0.8%) 72.5% (f 0.7%) features similar to those used earlier by Karlgren and Cutting (1994), here normalized by document length (in words). These basic non-PoS features include character count per document, sentence count per document, average character count per sentence, average word count per sentence, average character count per word, type/token ratio, frequency of long words (ones with more than 6 letters), and the frequencies of the words therefore, I, me, it, that, and which. The texts were PoS-tagged (Toutanova et al. 2003), using the same tag set as in the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). All 36 non-punctuation tags were used, and counts of PoS-tags were normalized by document length. Each experiment involved the 13 non-PoS features and a single PoS frequency feature (36 sets). Comparing performance with that on the non-PoS features alone (as a baseline) demonstrated the effect of adding each PoS frequency feature on classifier accuracy and stability.7 All 36 feature sets as well as the baseline set were trained on the same training set. They were then tested on both test sets describe</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, Kristina, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the ACL and Human Language Technology, pages 173–180, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Technical note: Bias and the quantification of stability.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="6108" citStr="Turney 1995" startWordPosition="929" endWordPosition="930">h a document appears. In total, 99.5% of documents in the corpus contain a Taxonomic Classifier field, with an average of 4.5 values per article. Although the hierarchy varies in depth, its second level comprises a set of four fairly high level genres—that is, Top/ Classifieds, Top/Features, Top/News, and Top/Opinion. Because the Types of Material field did not include news reports, we used the Taxonomic Classifier field to recognize documents from this genre. Specifically, we considered 2 The term stability is used in machine learning to describe the repeatability of a learner’s results (cf. Turney 1995). Here, we use it to describe the robustness of a method to (topical) domain changes or changes in the topic–genre distribution. 3 The results of our analysis can be found at http://homepages.inf.ed.ac.uk/s0895822/SCTG/. 4 www.ldc.upenn.edu, Catalog Id=LDC2008T19. 386 Petrenz and Webber Stable Classification of Text Genres Table 1 Genre (columns) and topic (rows) distribution in the data sets used in the experiments. Training set Test set 1 Test set 2 (3 x 4,309 = 12,927 texts) (3 x 2,155 = 6,465 texts) (6 x 2,285 = 13,710 texts) News Edit. LttE News Edit. LttE News Edit. LttE Edu x Def x Med </context>
</contexts>
<marker>Turney, 1995</marker>
<rawString>Turney, Peter. 1995. Technical note: Bias and the quantification of stability. Machine Learning, 20(1–2):23–33.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>