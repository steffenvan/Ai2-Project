<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997008">
Dynamic Programming for Linear-Time Incremental Parsing
</title>
<author confidence="0.993546">
Liang Huang
</author>
<affiliation confidence="0.665554333333333">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
</affiliation>
<email confidence="0.998035">
lhuang@isi.edu
</email>
<note confidence="0.6028095">
Kenji Sagae
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
</note>
<email confidence="0.998891">
sagae@ict.usc.edu
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999315">
Incremental parsing techniques such as
shift-reduce have gained popularity thanks
to their efficiency, but there remains a
major problem: the search is greedy and
only explores a tiny fraction of the whole
space (even with beam search) as op-
posed to dynamic programming. We show
that, surprisingly, dynamic programming
is in fact possible for many shift-reduce
parsers, by merging “equivalent” stacks
based on feature values. Empirically, our
algorithm yields up to a five-fold speedup
over a state-of-the-art shift-reduce depen-
dency parser with no loss in accuracy. Bet-
ter search also leads to better learning, and
our final parser outperforms all previously
reported dependency parsers for English
and Chinese, yet is much faster.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996084375">
In terms of search strategy, most parsing al-
gorithms in current use for data-driven parsing
can be divided into two broad categories: dy-
namic programming which includes the domi-
nant CKY algorithm, and greedy search which in-
cludes most incremental parsing methods such as
shift-reduce.1 Both have pros and cons: the for-
mer performs an exact search (in cubic time) over
an exponentially large space, while the latter is
much faster (in linear-time) and is psycholinguis-
tically motivated (Frazier and Rayner, 1982), but
its greedy nature may suffer from severe search er-
rors, as it only explores a tiny fraction of the whole
space even with a beam.
Can we combine the advantages of both ap-
proaches, that is, construct an incremental parser
</bodyText>
<footnote confidence="0.9701055">
1McDonald et al. (2005b) is a notable exception: the MST
algorithm is exact search but not dynamic programming.
</footnote>
<bodyText confidence="0.9954534375">
that runs in (almost) linear-time, yet searches over
a huge space with dynamic programming?
Theoretically, the answer is negative, as Lee
(2002) shows that context-free parsing can be used
to compute matrix multiplication, where sub-cubic
algorithms are largely impractical.
We instead propose a dynamic programming al-
ogorithm for shift-reduce parsing which runs in
polynomial time in theory, but linear-time (with
beam search) in practice. The key idea is to merge
equivalent stacks according to feature functions,
inspired by Earley parsing (Earley, 1970; Stolcke,
1995) and generalized LR parsing (Tomita, 1991).
However, our formalism is more flexible and our
algorithm more practical. Specifically, we make
the following contributions:
</bodyText>
<listItem confidence="0.948307772727273">
• theoretically, we show that for a large class
of modern shift-reduce parsers, dynamic pro-
gramming is in fact possible and runs in poly-
nomial time as long as the feature functions
are bounded and monotonic (which almost al-
ways holds in practice);
• practically, dynamic programming is up to
five times faster (with the same accuracy) as
conventional beam-search on top of a state-
of-the-art shift-reduce dependency parser;
• as a by-product, dynamic programming can
output a forest encoding exponentially many
trees, out of which we can draw better and
longer k-best lists than beam search can;
• finally, better and faster search also leads to
better and faster learning. Our final parser
achieves the best (unlabeled) accuracies that
we are aware of in both English and Chi-
nese among dependency parsers trained on
the Penn Treebanks. Being linear-time, it is
also much faster than most other parsers,
even with a pure Python implementation.
</listItem>
<page confidence="0.967577">
1077
</page>
<note confidence="0.9504285">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.481409">
input: w0 ... w,,,−1
axiom 0 : (0, ǫ): 0
</bodyText>
<equation confidence="0.9967105">
ℓ : (j, S) : c
j &lt; n
ℓ + 1 : (j + 1, S|wj) : c + ξ
ℓ : (j, S|s1|s0) : c
ℓ + 1 : (j, S|s1xs0) : c + λ
ℓ : (j, S|s1|s0) : c
ℓ + 1 : (j, S|s1ys0) : c + ρ
goal 2n − 1 : (n, s0): c
</equation>
<bodyText confidence="0.9997785">
where ℓ is the step, c is the cost, and the shift cost ξ
and reduce costs λ and ρ are:
</bodyText>
<equation confidence="0.999035666666667">
ξ = w · fsh(j, S) (1)
λ = w · fr, x (j, S|s1|s0) (2)
ρ = w · fr, y (j, S|s1|s0) (3)
</equation>
<figureCaption confidence="0.999118">
Figure 1: Deductive system of vanilla shift-reduce.
</figureCaption>
<bodyText confidence="0.999832">
For convenience of presentation and experimen-
tation, we will focus on shift-reduce parsing for
dependency structures in the remainder of this pa-
per, though our formalism and algorithm can also
be applied to phrase-structure parsing.
</bodyText>
<sectionHeader confidence="0.993544" genericHeader="method">
2 Shift-Reduce Parsing
</sectionHeader>
<subsectionHeader confidence="0.776099">
2.1 Vanilla Shift-Reduce
</subsectionHeader>
<bodyText confidence="0.999366125">
Shift-reduce parsing performs a left-to-right scan
of the input sentence, and at each step, choose one
of the two actions: either shift the current word
onto the stack, or reduce the top two (or more)
items at the end of the stack (Aho and Ullman,
1972). To adapt it to dependency parsing, we split
the reduce action into two cases, rex and rey, de-
pending on which one of the two items becomes
the head after reduction. This procedure is known
as “arc-standard” (Nivre, 2004), and has been en-
gineered to achieve state-of-the-art parsing accu-
racy in Huang et al. (2009), which is also the ref-
erence parser in our experiments.2
More formally, we describe a parser configura-
tion by a state (j, S) where S is a stack of trees
s0, s1, ... where s0 is the top tree, and j is the
</bodyText>
<footnote confidence="0.999871">
2There is another popular variant, “arc-eager” (Nivre,
2004; Zhang and Clark, 2008), which is more complicated
and less similar to the classical shift-reduce algorithm.
</footnote>
<note confidence="0.851471">
input: “I saw Al with Joe”
</note>
<figure confidence="0.610758125">
step action stack queue
0 - I ...
1 sh I saw ...
2 sh I saw Al ...
3 rex Ixsaw Al ...
4 sh Ixsaw Al with ...
5a rey IxsawyAl with...
5b sh Ixsaw Al with Joe
</figure>
<figureCaption confidence="0.980785">
Figure 2: A trace of vanilla shift-reduce. After
step (4), the parser branches off into (5a) or (5b).
</figureCaption>
<bodyText confidence="0.9972475">
queue head position (current word q0 is wj). At
each step, we choose one of the three actions:
</bodyText>
<listItem confidence="0.9990435">
1. sh: move the head of queue, wj, onto stack S
as a singleton tree;
2. rex: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1xs0.
3. rey: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1ys0.
</listItem>
<bodyText confidence="0.999945578947368">
Note that the shorthand notation txt′ denotes a
new tree by “attaching tree t′ as the leftmost child
of the root of tree t”. This procedure can be sum-
marized as a deductive system in Figure 1. States
are organized according to step ℓ, which denotes
the number of actions accumulated. The parser
runs in linear-time as there are exactly 2n−1 steps
for a sentence of n words.
As an example, consider the sentence “I saw AZ
with Joe” in Figure 2. At step (4), we face a shift-
reduce conflict: either combine “saw” and “Al” in
a rey action (5a), or shift “with” (5b). To resolve
this conflict, there is a cost c associated with each
state so that we can pick the best one (or few, with
a beam) at each step. Costs are accumulated in
each step: as shown in Figure 1, actions sh, rex,
and rey have their respective costs ξ, λ, and ρ,
which are dot-products of the weights w and fea-
tures extracted from the state and the action.
</bodyText>
<subsectionHeader confidence="0.960885">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999979666666667">
We view features as “abstractions” or (partial) ob-
servations of the current state, which is an im-
portant intuition for the development of dynamic
programming in Section 3. Feature templates
are functions that draw information from the fea-
ture window (see Tab. 1(b)), consisting of the
top few trees on the stack and the first few
words on the queue. For example, one such fea-
ture templatef100 = s0.w o q0.t is a conjunction
</bodyText>
<equation confidence="0.550711666666667">
sh
rex
rey
</equation>
<page confidence="0.976848">
1078
</page>
<bodyText confidence="0.9954725">
of two atomic features s0.w and q0.t, capturing
the root word of the top tree s0 on the stack, and
the part-of-speech tag of the current head word q0
on the queue. See Tab. 1(a) for the list of feature
templates used in the full model. Feature templates
are instantiated for a specific state. For example, at
step (4) in Fig. 2, the above template f100 will gen-
erate a feature instance
</bodyText>
<equation confidence="0.950564">
(s0.w = Al) o (q0.t = IN).
</equation>
<bodyText confidence="0.999700142857143">
More formally, we denote f to be the feature func-
tion, such that f(j, S) returns a vector of feature
instances for state (j, S). To decide which action
is the best for the current state, we perform a three-
way classification based on f(j, S), and to do so,
we further conjoin these feature instances with the
action, producing action-conjoined instances like
</bodyText>
<equation confidence="0.943994">
(s0.w = Al) o (q0.t = IN) o (action = sh).
</equation>
<bodyText confidence="0.99971925">
We denote fsh(j, S), frer (j, S), and fres, (j, S) to
be the conjoined feature instances, whose dot-
products with the weight vector decide the best ac-
tion (see Eqs. (1-3) in Fig. 1).
</bodyText>
<subsectionHeader confidence="0.999799">
2.3 Beam Search and Early Update
</subsectionHeader>
<bodyText confidence="0.999969894736842">
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search
(Zhang and Clark, 2008), where b states develop
in parallel. At each step we extend the states in
the current beam by applying one of the three ac-
tions, and then choose the best b resulting states
for the next step. Our dynamic programming algo-
rithm also runs on top of beam search in practice.
To train the model, we use the averaged percep-
tron algorithm (Collins, 2002). Following Collins
and Roark (2004) we also use the “early-update”
strategy, where an update happens whenever the
gold-standard action-sequence falls off the beam,
with the rest of the sequence neglected.3 The intu-
ition behind this strategy is that later mistakes are
often caused by previous ones, and are irrelevant
when the parser is on the wrong track. Dynamic
programming turns out to be a great fit for early
updating (see Section 4.3 for details).
</bodyText>
<sectionHeader confidence="0.996928" genericHeader="method">
3 Dynamic Programming (DP)
</sectionHeader>
<subsectionHeader confidence="0.999622">
3.1 Merging Equivalent States
</subsectionHeader>
<bodyText confidence="0.933886">
The key observation for dynamic programming
is to merge “equivalent states” in the same beam
</bodyText>
<footnote confidence="0.8882395">
3As a special case, for the deterministic mode (b=1), up-
dates always co-occur with the first mistake made.
</footnote>
<table confidence="0.998169642857143">
(a) Features Templates f(j, S) qi = wj+i
s0.w s0.t s0.w o s0.t
s1.w s1.t s1.w o s1.t
q0.w q0.t q0.w o q0.t
s0.w o s1.w s0.t o s1.t
s0.t o q0.t s0.w o s0.t o s1.t
s0.t o s1.w o s1.t s0.w o s1.w o s1.t
s0.w o s0.t o s1.w s0.w o s0.t o s1 o s1.t
s0.t o q0.t o q1.t s1.t o s0.t o q0.t
s0.w o q0.t o q1.t s1.t o s0.w o q0.t
s1.t o s1.lc.t o s0.t s1.t o s1.rc.t o s0.t
s1.t o s0.t o s0.rc.t s1.t o s1.lc.t o s0
s1.t o s1.rc.t o s0.w s1.t o s0.w o s0.lc.t
s2.t o s1.t o s0.t
</table>
<tableCaption confidence="0.998436">
Table 1: (a) feature templates used in this work,
</tableCaption>
<bodyText confidence="0.945594470588235">
adapted from Huang et al. (2009). x.w and x.t de-
notes the root word and POS tag of tree (or word)
x. and x.lc and x.rc denote x’s left- and rightmost
child. (b) feature window. (c) kernel features.
(i.e., same step) if they have the same feature
values, because they will have the same costs as
shown in the deductive system in Figure 1. Thus
we can define two states (j, S) and (j′, S′) to be
equivalent, notated (j, S) — (j′, S′), iff.
j = j′ and f(j, S) = f(j′, S′). (4)
Note that j = j′ is also needed because the
queue head position j determines which word to
shift next. In practice, however, a small subset of
atomic features will be enough to determine the
whole feature vector, which we call kernel fea-
tures �f(j, S), defined as the smallest set of atomic
templates such that
</bodyText>
<equation confidence="0.6526">
�f(j, S) = �f(j′, S′) =&gt;. (j, S) — (j′, S′).
</equation>
<bodyText confidence="0.999987">
For example, the full list of 28 feature templates
in Table 1(a) can be determined by just 12 atomic
features in Table 1(c), which just look at the root
words and tags of the top two trees on stack, as
well as the tags of their left- and rightmost chil-
dren, plus the root tag of the third tree s2, and fi-
nally the word and tag of the queue head q0 and the
</bodyText>
<figure confidence="0.998576935483871">
f2(s2)
f1(s1)
f0(s0)
j
s2.t
s1.w s1.t
s0.w s0.t
q0.w q0.t
s1.lc.t s1.rc.t
s0.lc.t s0.rc.t
q1.t
...
...
...
...
(b) +— stack
... s2
...
s1.lc
s1
...
s1.rc
s0.lc
s0
...
s0.rc
queue --+
q0 q1 ...
Kernel features for DP// /
f (j, S) = (j, f2 (s2), f1 (81), f0 ls0 ))
(c)
</figure>
<page confidence="0.801852">
1079
</page>
<bodyText confidence="0.568733">
state form ℓ : hi, j, sd...s0i: (c, v, π) ℓ: step; c, v: prefix and inside costs; π: predictor states
</bodyText>
<equation confidence="0.9495722">
equivalence ℓ : hi, j, sd...s0i ∼ ℓ : hi, j, s′d...s′0i iff. f(j, sd...s0) = f(j, s′d...s′0)
ordering ℓ : : (c, v, ) ≺ ℓ : : (c′, v′, ) iff. c &lt; c′ or (c = c′ and v &lt; v′).
axiom (p0) 0 : h0, 0, ǫi: (0, 0, ∅)
ℓ + 1 : 1 s _ ...s wj) : (c+ 0 { &lt;n
(j� j+ E d i o� � p})
state q:
ℓ :hi, j, sd...s0i: ( , v, π)
xs0i : (c′ + v + δ, v′ + v + δ, π′) p ∈ π
ℓ + 1 : hk, j, s′ d...s′ 1, s′ 0
goal 2n − 1 : h0, n, sd...s0i: (c, c, {p0})
</equation>
<bodyText confidence="0.983384">
where ξ = w · fsh(j, sd...s0), and δ = ξ′ + λ, with ξ′ = w · fsh(i, s′d...s′0) and λ = w · ffex (j, sd...s0).
</bodyText>
<figureCaption confidence="0.881483">
Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set π
is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995).
</figureCaption>
<figure confidence="0.669565555555556">
The rey case is similar, replacing s′ xs0 with s′ ys0, and λ with ρ = w · ffey (j, sd...s0). Irrelevant
0 0
information in a deduction step is marked as an underscore ( ) which means “can match anything”.
sh
rex
state p:
: hk, i, s′d...s′ 0i: (c′, v′, π′)
state p:
ℓ : h ,j, sd...s0i: (c, , )
</figure>
<bodyText confidence="0.99966125">
tag of the next word q1. Since the queue is static
information to the parser (unlike the stack, which
changes dynamically), we can use j to replace fea-
tures from the queue. So in general we write
</bodyText>
<equation confidence="0.808065">
�f(j, S) = (j, fd(sd), ... , f0(s0))
</equation>
<bodyText confidence="0.99996575">
if the feature window looks at top d + 1 trees
on stack, and where fz(sz) extracts kernel features
from tree sz (0 ≤ i ≤ d). For example, for the full
model in Table 1(a) we have
</bodyText>
<equation confidence="0.981944">
f(j, S) = (j,f2(s2),f1(s1),f0(s0)), (5)
</equation>
<bodyText confidence="0.9964985">
where d = 2, f2(x) = x.t, and f1(x) = f0(x) =
(x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
</bodyText>
<subsectionHeader confidence="0.999934">
3.2 Graph-Structured Stack and Deduction
</subsectionHeader>
<bodyText confidence="0.999941451612903">
Now that we have the kernel feature functions, it
is intuitive that we might only need to remember
the relevant bits of information from only the last
(d + 1) trees on stack instead of the whole stack,
because they provide all the relevant information
for the features, and thus determine the costs. For
shift, this suffices as the stack grows on the right;
but for reduce actions the stack shrinks, and in or-
der still to maintain d + 1 trees, we have to know
something about the history. This is exactly why
we needed the full stack for vanilla shift-reduce
parsing in the first place, and why dynamic pro-
gramming seems hard here.
To solve this problem we borrow the idea
of “graph-structured stack” (GSS) from Tomita
(1991). Basically, each state p carries with it a set
π(p) of predictor states, each of which can be
combined with p in a reduction step. In a shift step,
if state p generates state q (we say “p predicts q”
in Earley (1970) terms), then p is added onto π(q).
When two equivalent shifted states get merged,
their predictor states get combined. In a reduction
step, state q tries to combine with every predictor
state p ∈ π(q), and the resulting state r inherits
the predictor states set from p, i.e., π(r) = π(p).
Interestingly, when two equivalent reduced states
get merged, we can prove (by induction) that their
predictor states are identical (proof omitted).
Figure 3 shows the new deductive system with
dynamic programming and GSS. A new state has
the form
</bodyText>
<equation confidence="0.593964">
ℓ : hi, j, sd...s0i
</equation>
<bodyText confidence="0.885842714285714">
where [i..j] is the span of the top tree s0, and
sd..s1 are merely “left-contexts”. It can be com-
bined with some predictor state p spanning [k..i]
′ ′
ℓ′ h k, i, sd...s0i
to form a larger state spanning [k..j], with the
resulting top tree being either s1xs0 or s1ys0.
</bodyText>
<page confidence="0.977187">
1080
</page>
<bodyText confidence="0.99995075">
This style resembles CKY and Earley parsers. In
fact, the chart in Earley and other agenda-based
parsers is indeed a GSS when viewed left-to-right.
In these parsers, when a state is popped up from
the agenda, it looks for possible sibling states
that can combine with it; GSS, however, explicitly
maintains these predictor states so that the newly-
popped state does not need to look them up.4
</bodyText>
<subsectionHeader confidence="0.99905">
3.3 Correctness and Polynomial Complexity
</subsectionHeader>
<bodyText confidence="0.8913306">
We state the main theoretical result with the proof
omitted due to space constraints:
Theorem 1. The deductive system is optimal and
runs in worst-case polynomial time as long as the
kernel feature function satisfies two properties:
</bodyText>
<listItem confidence="0.9969708">
• bounded: f(j, S) = (j, fd(sd), ... , f0(s0))
for some constant d, and each |ft(x) |also
bounded by a constant for all possible tree x.
• monotonic: ft(x) = ft(y) ==&gt;. ft+1(x) =
ft+1(y), for all t and all possible trees x, y.
</listItem>
<bodyText confidence="0.999675111111112">
Intuitively, boundedness means features can
only look at a local window and can only extract
bounded information on each tree, which is always
the case in practice since we can not have infinite
models. Monotonicity, on the other hand, says that
features drawn from trees farther away from the
top should not be more refined than from those
closer to the top. This is also natural, since the in-
formation most relevant to the current decision is
always around the stack top. For example, the ker-
nel feature function in Eq. 5 is bounded and mono-
tonic, since f2 is less refined than f1 and f0.
These two requirements are related to grammar
refinement by annotation (Johnson, 1998), where
annotations must be bounded and monotonic: for
example, one cannot refine a grammar by only
remembering the grandparent but not the parent
symbol. The difference here is that the annotations
are not vertical ((grand-)parent), but rather hori-
zontal (left context). For instance, a context-free
rule A —* B C would become DA —* DB BC
for some D if there exists a rule E —* αDAQ.
This resembles the reduce step in Fig. 3.
The very high-level idea of the proof is that
boundedness is crucial for polynomial-time, while
monotonicity is used for the optimal substructure
property required by the correctness of DP.
</bodyText>
<footnote confidence="0.934895333333333">
4In this sense, GSS (Tomita, 1988) is really not a new in-
vention: an efficient implementation of Earley (1970) should
already have it implicitly, similar to what we have in Fig. 3.
</footnote>
<subsectionHeader confidence="0.764172">
3.4 Beam Search based on Prefix Cost
</subsectionHeader>
<bodyText confidence="0.972105090909091">
Though the DP algorithm runs in polynomial-
time, in practice the complexity is still too high,
esp. with a rich feature set like the one in Ta-
ble 1. So we apply the same beam search idea
from Sec. 2.3, where each step can accommodate
only the best b states. To decide the ordering of
states in each beam we borrow the concept of pre-
fix cost from Stolcke (1995), originally developed
for weighted Earley parsing. As shown in Fig. 3,
the prefix cost c is the total cost of the best action
sequence from the initial state to the end of state p,
i.e., it includes both the inside cost v (for Viterbi
inside derivation), and the cost of the (best) path
leading towards the beginning of state p. We say
that a state p with prefix cost c is better than a state
p′ with prefix cost c′, notated p � p′ in Fig. 3, if
c &lt; c′. We can also prove (by contradiction) that
optimizing for prefix cost implies optimal inside
cost (Nederhof, 2003, Sec. 4). 5
As shown in Fig. 3, when a state q with costs
(c, v) is combined with a predictor state p with
costs (c′, v′), the resulting state r will have costs
</bodyText>
<equation confidence="0.984338">
(c′ + v + δ, v′ + v + δ),
</equation>
<bodyText confidence="0.99998">
where the inside cost is intuitively the combined
inside costs plus an additional combo cost δ from
the combination, while the resulting prefix cost
c′ + v + δ is the sum of the prefix cost of the pre-
dictor state q, the inside cost of the current state p,
and the combo cost. Note the prefix cost of q is ir-
relevant. The combo cost δ = ξ′ + λ consists of
shift cost ξ′ of p and reduction cost λ of q.
The cost in the non-DP shift-reduce algorithm
(Fig. 1) is indeed a prefix cost, and the DP algo-
rithm subsumes the non-DP one as a special case
where no two states are equivalent.
</bodyText>
<subsectionHeader confidence="0.756284">
3.5 Example: Edge-Factored Model
</subsectionHeader>
<bodyText confidence="0.999888666666667">
As a concrete example, Figure 4 simulates an
edge-factored model (Eisner, 1996; McDonald et
al., 2005a) using shift-reduce with dynamic pro-
gramming, which is similar to bilexical PCFG
parsing using CKY (Eisner and Satta, 1999). Here
the kernel feature function is
</bodyText>
<equation confidence="0.80084">
�f(j, S) = (j, h(s1), h(s0))
</equation>
<bodyText confidence="0.695773833333333">
5Note that using inside cost v for ordering would be a
bad idea, as it will always prefer shorter derivations like in
best-first parsing. As in A* search, we need some estimate
of “outside cost” to predict which states are more promising,
and the prefix cost includes an exact cost for the left outside
context, but no right outside context.
</bodyText>
<page confidence="0.821035">
1081
</page>
<equation confidence="0.64468">
ℓ : ( , h
</equation>
<bodyText confidence="0.865381">
where reg, cost λ = w · &amp;_(h′, h)
</bodyText>
<figureCaption confidence="0.885015">
Figure 4: Example of shift-reduce with dynamic
programming: simulating an edge-factored model.
GSS is implicit here, and re, case omitted.
</figureCaption>
<bodyText confidence="0.999937722222222">
where h(x) returns the head word index of tree x,
because all features in this model are based on the
head and modifier indices in a dependency link.
This function is obviously bounded and mono-
tonic in our definitions. The theoretical complexity
of this algorithm is O(n7) because in a reduction
step we have three span indices and three head in-
dices, plus a step index E. By contrast, the naive
CKY algorithm for this model is O(n5) which can
be improved to O(n3) (Eisner, 1996).6 The higher
complexity of our algorithm is due to two factors:
first, we have to maintain both h and h′ in one
state, because the current shift-reduce model can
not draw features across different states (unlike
CKY); and more importantly, we group states by
step E in order to achieve incrementality and lin-
ear runtime with beam search that is not (easily)
possible with CKY or MST.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9950255">
We first reimplemented the reference shift-reduce
parser of Huang et al. (2009) in Python (hence-
forth “non-DP”), and then extended it to do dy-
namic programing (henceforth “DP”). We evalu-
ate their performances on the standard Penn Tree-
bank (PTB) English dependency parsing task7 us-
ing the standard split: secs 02-21 for training, 22
for development, and 23 for testing. Both DP and
non-DP parsers use the same feature templates in
Table 1. For Secs. 4.1-4.2, we use abaseline model
trained with non-DP for both DP and non-DP, so
that we can do a side-by-side comparison of search
</bodyText>
<footnote confidence="0.9986365">
6Or O(n2) with MST, but including non-projective trees.
7Using the head rules of Yamada and Matsumoto (2003).
</footnote>
<bodyText confidence="0.898287">
quality; in Sec. 4.3 we will retrain the model with
DP and compare it against training with non-DP.
</bodyText>
<subsectionHeader confidence="0.993515">
4.1 Speed Comparisons
</subsectionHeader>
<bodyText confidence="0.9999871">
To compare parsing speed between DP and non-
DP, we run each parser on the development set,
varying the beam width b from 2 to 16 (DP) or 64
(non-DP). Fig. 5a shows the relationship between
search quality (as measured by the average model
score per sentence, higher the better) and speed
(average parsing time per sentence), where DP
with a beam width of b=16 achieves the same
search quality with non-DP at b=64, while being 5
times faster. Fig. 5b shows a similar comparison
for dependency accuracy. We also test with an
edge-factored model (Sec. 3.5) using feature tem-
plates (1)-(3) in Tab. 1, which is a subset of those
in McDonald et al. (2005b). As expected, this dif-
ference becomes more pronounced (8 times faster
in Fig. 5c), since the less expressive feature set
makes more states “equivalent” and mergeable in
DP. Fig. 5d shows the (almost linear) correlation
between dependency accuracy and search quality,
confirming that better search yields better parsing.
</bodyText>
<subsectionHeader confidence="0.99357">
4.2 Search Space, Forest, and Oracles
</subsectionHeader>
<bodyText confidence="0.999975454545455">
DP achieves better search quality because it ex-
pores an exponentially large search space rather
than only b trees allowed by the beam (see Fig. 6a).
As a by-product, DP can output a forest encoding
these exponentially many trees, out of which we
can draw longer and better (in terms of oracle) k-
best lists than those in the beam (see Fig. 6b). The
forest itself has an oracle of 98.15 (as if k → ∞),
computed a` la Huang (2008, Sec. 4.1). These can-
didate sets may be used for reranking (Charniak
and Johnson, 2005; Huang, 2008).8
</bodyText>
<subsectionHeader confidence="0.999628">
4.3 Perceptron Training and Early Updates
</subsectionHeader>
<bodyText confidence="0.989271307692308">
Another interesting advantage of DP over non-DP
is the faster training with perceptron, even when
both parsers use the same beam width. This is due
to the use of early updates (see Sec. 2.3), which
happen much more often with DP, because a gold-
standard state p is often merged with an equivalent
(but incorrect) state that has a higher model score,
which triggers update immediately. By contrast, in
non-DP beam search, states such as p might still
8DP’s k-best lists are extracted from the forest using the
algorithm of Huang and Chiang (2005), rather than those in
the final beam as in the non-DP case, because many deriva-
tions have been merged during dynamic programming.
</bodyText>
<equation confidence="0.9139085">
sh
re.
...j
ℓ + 1 : (h, j) : (c, 0) j&lt; n
: (h′′, h′ ) : (c′, v′) ℓ : (h′, h ) : ( , v)
ℓ + 1 : (h′′, h
) : (c′ + v + λ, v′ + v + λ)
h′
i...j
k...i
k...i i...j
) : (c, )
</equation>
<page confidence="0.976711">
1082
</page>
<figure confidence="0.997558603174603">
avg. model score
dependency accuracy
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
2394
2391
2388
2385
2382
2379
2376
2373
2370
b=16 b=64
DP
non-DP
93.1
92.9
92.8
92.7
92.6
92.5
92.4
92.3
92.2
93
b=16 b=64
DP
non-DP
(a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model)
dependency accuracy
93.5
92.5
91.5
90.5
89.5
88.5
93
92
91
90
89
avg. model score
2335
2330
2325
2320
2315
2310
2305
2300
2295
2290
b=16
DP
non-DP
b=64
full, DP
full, non-DP
edge-factor, DP
edge-factor, non-DP
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 2280 2300 2320 2340 2360 2380 2400
(c) search quality vs. time (edge-factored model) (d) correlation b/w parsing (y) and search (x)
</figure>
<figureCaption confidence="0.9941942">
Figure 5: Speed comparisons between DP and non-DP, with beam size b ranging 2-16 for DP and 2-64
for non-DP. Speed is measured by avg. parsing time (secs) per sentence on x axis. With the same level
of search quality or parsing accuracy, DP (at b=16) is -4.8 times faster than non-DP (at b=64) with the
full model in plots (a)-(b), or -8 times faster with the simplified edge-factored model in plot (c). Plot (d)
shows the (roughly linear) correlation between parsing accuracy and search quality (avg. model score).
</figureCaption>
<figure confidence="0.964228346153846">
number of trees explored
1012
1010
108
106
104
102
100
DP forest
non-DP (16)
0 10 20 30 40 50 60 70
sentence length
1 4 8 16 32 64
k
oracle precision
99
98
97
96
95
94
93
DP forest (98.15)
DP k-best in forest
non-DP k-best in beam
(a) sizes of search spaces (b) oracle precision on dev
</figure>
<figureCaption confidence="0.9729245">
Figure 6: DP searches over a forest of exponentially many trees, which also produces better and longer
k-best lists with higher oracles, while non-DP only explores b trees allowed in the beam (b = 16 here).
</figureCaption>
<page confidence="0.856294">
1083
</page>
<figure confidence="0.9848085">
0 4 8 12 16 20 24
hours
</figure>
<figureCaption confidence="0.9294672">
Figure 7: Learning curves (showing precision on
dev) of perceptron training for 25 iterations (b=8).
DP takes 18 hours, peaking at the 17th iteration
(93.27%) with 12 hours, while non-DP takes 23
hours, peaking at the 18th (93.04%) with 16 hours.
</figureCaption>
<bodyText confidence="0.999657913043478">
survive in the beam throughout, even though it is
no longer possible to rank the best in the beam.
The higher frequency of early updates results
in faster iterations of perceptron training. Table 2
shows the percentage of early updates and the time
per iteration during training. While the number of
updates is roughly comparable between DP and
non-DP, the rate of early updates is much higher
with DP, and the time per iteration is consequently
shorter. Figure 7 shows that training with DP is
about 1.2 times faster than non-DP, and achieves
+0.2% higher accuracy on the dev set (93.27%).
Besides training with gold POS tags, we also
trained on noisy tags, since they are closer to the
test setting (automatic tags on sec 23). In that
case, we tag the dev and test sets using an auto-
matic POS tagger (at 97.2% accuracy), and tag
the training set using four-way jackknifing sim-
ilar to Collins (2000), which contributes another
+0.1% improvement in accuracy on the test set.
Faster training also enables us to incorporate more
features, where we found more lookahead features
(q2) results in another +0.3% improvement.
</bodyText>
<subsectionHeader confidence="0.999082">
4.4 Final Results on English and Chinese
</subsectionHeader>
<bodyText confidence="0.999817">
Table 3 presents the final test results of our DP
parser on the Penn English Treebank, compared
with other state-of-the-art parsers. Our parser
achieves the highest (unlabeled) dependency ac-
curacy among dependency parsers trained on the
Treebank, and is also much faster than most other
parsers even with a pure Python implementation
</bodyText>
<table confidence="0.9963286">
it update early% time update early% time
1 31943 98.9 22 31189 87.7 29
5 20236 98.3 38 19027 70.3 47
17 8683 97.1 48 7434 49.5 60
25 5715 97.2 51 4676 41.2 65
</table>
<tableCaption confidence="0.9870735">
Table 2: Perceptron iterations with DP (left) and
non-DP (right). Early updates happen much more
often with DP due to equivalent state merging,
which leads to faster training (time in minutes).
</tableCaption>
<table confidence="0.9994797">
word L time comp.
McDonald 05b 90.2 Ja 0.12
McDonald 05a 90.9 Ja 0.15
Koo 08 base 92.0 − −
Zhang 08 single 91.4 C 0.11 O(n)‡
this work 92.1 Py 0.04 O(n)
†Charniak 00 92.5 C 0.49
†Petrov 07 92.4 Ja 0.21
Zhang 08 combo 92.1 C − O(n2)‡
Koo 08 semisup 93.2 − −
</table>
<tableCaption confidence="0.999108">
Table 3: Final test results on English (PTB). Our
</tableCaption>
<bodyText confidence="0.996337142857143">
parser (in pure Python) has the highest accuracy
among dependency parsers trained on the Tree-
bank, and is also much faster than major parsers.
†converted from constituency trees. C=C/C++,
Py=Python, Ja=Java. Time is in seconds per sen-
tence. Search spaces: ‡linear; others exponential.
(on a 3.2GHz Xeon CPU). Best-performing con-
stituency parsers like Charniak (2000) and Berke-
ley (Petrov and Klein, 2007) do outperform our
parser, since they consider more information dur-
ing parsing, but they are at least 5 times slower.
Figure 8 shows the parse time in seconds for each
test sentence. The observed time complexity of our
DP parser is in fact linear compared to the super-
linear complexity of Charniak, MST (McDonald
et al., 2005b), and Berkeley parsers. Additional
techniques such as semi-supervised learning (Koo
et al., 2008) and parser combination (Zhang and
Clark, 2008) do achieve accuracies equal to or
higher than ours, but their results are not directly
comparable to ours since they have access to ex-
tra information like unlabeled data. Our technique
is orthogonal to theirs, and combining these tech-
niques could potentially lead to even better results.
We also test our final parser on the Penn Chi-
nese Treebank (CTB5). Following the set-up of
Duan et al. (2007) and Zhang and Clark (2008), we
split CTB5 into training (secs 001-815 and 1001-
</bodyText>
<figure confidence="0.983544">
17th
DP
non-DP
18th
93.5
93
92.5
92
91.5
91
accuracy on dev (each round)
90.5
1084
1.4
Cha
Berk
MST
DP
0.8
0.6
0.4
0.2
0
0 10 20 30 40 50 60 70
sentence length
</figure>
<figureCaption confidence="0.995221666666667">
Figure 8: Scatter plot of parsing time against sen-
tence length, comparing with Charniak, Berkeley,
and the O(n2) MST parsers.
</figureCaption>
<table confidence="0.78007525">
word non-root root compl.
Duan 07 83.88 84.36 73.70 32.70
Zhang 08† 84.33 84.69 76.73 32.79
this work 85.20 85.52 78.32 33.72
</table>
<tableCaption confidence="0.977148">
Table 4: Final test results on Chinese (CTB5).
†The transition parser in Zhang and Clark (2008).
</tableCaption>
<bodyText confidence="0.998789">
1136), development (secs 886-931 and 1148-
1151), and test (secs 816-885 and 1137-1147) sets,
assume gold-standard POS-tags for the input, and
use the head rules of Zhang and Clark (2008). Ta-
ble 4 summarizes the final test results, where our
work performs the best in all four types of (unla-
beled) accuracies: word, non-root, root, and com-
plete match (all excluding punctuations). 9,10
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.935965486486487">
This work was inspired in part by Generalized LR
parsing (Tomita, 1991) and the graph-structured
stack (GSS). Tomita uses GSS for exhaustive LR
parsing, where the GSS is equivalent to a dy-
namic programming chart in chart parsing (see
Footnote 4). In fact, Tomita’s GLR is an in-
stance of techniques for tabular simulation of non-
deterministic pushdown automata based on deduc-
tive systems (Lang, 1974), which allow for cubic-
time exhaustive shift-reduce parsing with context-
free grammars (Billot and Lang, 1989).
Our work advances this line of research in two
aspects. First, ours is more general than GLR in
9Duan et al. (2007) and Zhang and Clark (2008) did not
report word accuracies, but those can be recovered given non-
root and root ones, and the number of non-punctuation words.
10Parser combination in Zhang and Clark (2008) achieves
a higher word accuracy of 85.77%, but again, it is not directly
comparable to our work.
that it is not restricted to LR (a special case of
shift-reduce), and thus does not require building an
LR table, which is impractical for modern gram-
mars with a large number of rules or features. In
contrast, we employ the ideas behind GSS more
flexibly to merge states based on features values,
which can be viewed as constructing an implicit
LR table on-the-fly. Second, unlike previous the-
oretical results about cubic-time complexity, we
achieved linear-time performance by smart beam
search with prefix cost inspired by Stolcke (1995),
allowing for state-of-the-art data-driven parsing.
To the best of our knowledge, our work is the
first linear-time incremental parser that performs
dynamic programming. The parser of Roark and
Hollingshead (2009) is also almost linear time, but
they achieved this by discarding parts of the CKY
chart, and thus do achieve incrementality.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999984">
We have presented a dynamic programming al-
gorithm for shift-reduce parsing, which runs in
linear-time in practice with beam search. This
framework is general and applicable to a large-
class of shift-reduce parsers, as long as the feature
functions satisfy boundedness and monotonicity.
Empirical results on a state-the-art dependency
parser confirm the advantage of DP in many as-
pects: faster speed, larger search space, higher ora-
cles, and better and faster learning. Our final parser
outperforms all previously reported dependency
parsers trained on the Penn Treebanks for both
English and Chinese, and is much faster in speed
(even with a Python implementation). For future
work we plan to extend it to constituency parsing.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999750357142857">
We thank David Chiang, Yoav Goldberg, Jonathan
Graehl, Kevin Knight, and Roger Levy for help-
ful discussions and the three anonymous review-
ers for comments. Mark-Jan Nederhof inspired the
use of prefix cost. Yue Zhang helped with Chinese
datasets, and Wenbin Jiang with feature sets. This
work is supported in part by DARPA GALE Con-
tract No. HR0011-06-C-0022 under subcontract to
BBN Technologies, and by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). Statements and opinions expressed
do not necessarily reflect the position or the policy
of the United States Government, and no official
endorsement should be inferred.
</bodyText>
<figure confidence="0.947237666666667">
parsing time (secs)
1.2
1
</figure>
<page confidence="0.986835">
1085
</page>
<sectionHeader confidence="0.994502" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867455445544">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory ofParsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In Proceedings of the
27th ACL, pages 143–151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACL, Ann Ar-
bor, MI.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings ofNAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings ofACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of ICML,
pages 175–182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/PKDD.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the ACM, 13(2):94–
102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings ofACL.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of COLING.
Lyn Frazier and Keith Rayner. 1982. Making and cor-
recting errors during sentence comprehension: Eye
movements in the analysis of structurally ambigu-
ous sentences. Cognitive Psychology, 14(2):178 –
210.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings ofEMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24:613–632.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL.
B. Lang. 1974. Deterministic techniques for efficient
non-deterministic parsers. In Automata, Languages
and Programming, 2nd Colloquium, volume 14 of
Lecture Notes in Computer Science, pages 255–269,
Saarbr¨ucken. Springer-Verlag.
Lillian Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1–15.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. ofHLT-
EMNLP.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computational Linguis-
tics, pages 135–143.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings ofHLT-NAACL.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.
Masaru Tomita. 1988. Graph-structured stack and nat-
ural language parsing. In Proceedings of the 26th
annual meeting on Association for Computational
Linguistics, pages 249–257, Morristown, NJ, USA.
Association for Computational Linguistics.
Masaru Tomita, editor. 1991. Generalized LR Parsing.
Kluwer Academic Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings ofIWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings ofEMNLP.
</reference>
<page confidence="0.995583">
1086
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765334">
<title confidence="0.999934">Dynamic Programming for Linear-Time Incremental Parsing</title>
<author confidence="0.998099">Liang Huang</author>
<affiliation confidence="0.99997">USC Information Sciences Institute</affiliation>
<address confidence="0.9976345">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.999618">lhuang@isi.edu</email>
<author confidence="0.835527">Kenji Sagae</author>
<affiliation confidence="0.999166">USC Institute for Creative Technologies</affiliation>
<address confidence="0.9974385">13274 Fiji Way Marina del Rey, CA 90292</address>
<email confidence="0.999592">sagae@ict.usc.edu</email>
<abstract confidence="0.996048736842105">Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory ofParsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="4742" citStr="Aho and Ullman, 1972" startWordPosition="789" endWordPosition="792">(2) ρ = w · fr, y (j, S|s1|s0) (3) Figure 1: Deductive system of vanilla shift-reduce. For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing. 2 Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the 2There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 20</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory ofParsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation. Prentice Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Billot</author>
<author>B Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th ACL,</booktitle>
<pages>143--151</pages>
<contexts>
<context position="31103" citStr="Billot and Lang, 1989" startWordPosition="5659" endWordPosition="5662">(unlabeled) accuracies: word, non-root, root, and complete match (all excluding punctuations). 9,10 5 Related Work This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS). Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4). In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989). Our work advances this line of research in two aspects. First, ours is more general than GLR in 9Duan et al. (2007) and Zhang and Clark (2008) did not report word accuracies, but those can be recovered given nonroot and root ones, and the number of non-punctuation words. 10Parser combination in Zhang and Clark (2008) achieves a higher word accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rule</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>S. Billot and B. Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th ACL, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="23287" citStr="Charniak and Johnson, 2005" startWordPosition="4266" endWordPosition="4269">y and search quality, confirming that better search yields better parsing. 4.2 Search Space, Forest, and Oracles DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a). As a by-product, DP can output a forest encoding these exponentially many trees, out of which we can draw longer and better (in terms of oracle) kbest lists than those in the beam (see Fig. 6b). The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec. 4.1). These candidate sets may be used for reranking (Charniak and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8DP’s k-best lists are extracted from the forest using the algorithm of Huang and Chiang (20</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine-grained n-best parsing and discriminative reranking. In Proceedings of the 43rd ACL, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="28698" citStr="Charniak (2000)" startWordPosition="5252" endWordPosition="5253">0.9 Ja 0.15 Koo 08 base 92.0 − − Zhang 08 single 91.4 C 0.11 O(n)‡ this work 92.1 Py 0.04 O(n) †Charniak 00 92.5 C 0.49 †Petrov 07 92.4 Ja 0.21 Zhang 08 combo 92.1 C − O(n2)‡ Koo 08 semisup 93.2 − − Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. †converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly com</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8971" citStr="Collins and Roark (2004)" startWordPosition="1582" endWordPosition="1585">ose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same beam 3As a special case, for the de</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="27078" citStr="Collins (2000)" startWordPosition="4975" endWordPosition="4976">the number of updates is roughly comparable between DP and non-DP, the rate of early updates is much higher with DP, and the time per iteration is consequently shorter. Figure 7 shows that training with DP is about 1.2 times faster than non-DP, and achieves +0.2% higher accuracy on the dev set (93.27%). Besides training with gold POS tags, we also trained on noisy tags, since they are closer to the test setting (automatic tags on sec 23). In that case, we tag the dev and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to Collins (2000), which contributes another +0.1% improvement in accuracy on the test set. Faster training also enables us to incorporate more features, where we found more lookahead features (q2) results in another +0.3% improvement. 4.4 Final Results on English and Chinese Table 3 presents the final test results of our DP parser on the Penn English Treebank, compared with other state-of-the-art parsers. Our parser achieves the highest (unlabeled) dependency accuracy among dependency parsers trained on the Treebank, and is also much faster than most other parsers even with a pure Python implementation it upd</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8935" citStr="Collins, 2002" startWordPosition="1579" endWordPosition="1580">ined feature instances, whose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic models for action-based chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ECML/PKDD.</booktitle>
<contexts>
<context position="29605" citStr="Duan et al. (2007)" startWordPosition="5402" endWordPosition="5405">ared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 1001- 17th DP non-DP 18th 93.5 93 92.5 92 91.5 91 accuracy on dev (each round) 90.5 1084 1.4 Cha Berk MST DP 0.8 0.6 0.4 0.2 0 0 10 20 30 40 50 60 70 sentence length Figure 8: Scatter plot of parsing time against sentence length, comparing with Charniak, Berkeley, and the O(n2) MST parsers. word non-root root compl. Duan 07 83.88 84.36 73.70 32.70 Zhang 08† 84.33 84.69 76.73 32.79 this work 85.20 85.52 78.32 33.72 Table 4: Final test results on Chinese (CTB5). †The transition parser in Zhang and Clark (2008). 1136), dev</context>
<context position="31220" citStr="Duan et al. (2007)" startWordPosition="5681" endWordPosition="5684">ork was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS). Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4). In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989). Our work advances this line of research in two aspects. First, ours is more general than GLR in 9Duan et al. (2007) and Zhang and Clark (2008) did not report word accuracies, but those can be recovered given nonroot and root ones, and the number of non-punctuation words. 10Parser combination in Zhang and Clark (2008) achieves a higher word accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rules or features. In contrast, we employ the ideas behind GSS more flexibly to merge states based on features values, wh</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based chinese dependency parsing. In Proceedings of ECML/PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>102</pages>
<contexts>
<context position="2446" citStr="Earley, 1970" startWordPosition="375" endWordPosition="376"> algorithm is exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: • theoretically, we show that for a large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice); • practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a stateof-the-art shift-reduce dependency </context>
<context position="14291" citStr="Earley (1970)" startWordPosition="2664" endWordPosition="2665">he stack grows on the right; but for reduce actions the stack shrinks, and in order still to maintain d + 1 trees, we have to know something about the history. This is exactly why we needed the full stack for vanilla shift-reduce parsing in the first place, and why dynamic programming seems hard here. To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991). Basically, each state p carries with it a set π(p) of predictor states, each of which can be combined with p in a reduction step. In a shift step, if state p generates state q (we say “p predicts q” in Earley (1970) terms), then p is added onto π(q). When two equivalent shifted states get merged, their predictor states get combined. In a reduction step, state q tries to combine with every predictor state p ∈ π(q), and the resulting state r inherits the predictor states set from p, i.e., π(r) = π(p). Interestingly, when two equivalent reduced states get merged, we can prove (by induction) that their predictor states are identical (proof omitted). Figure 3 shows the new deductive system with dynamic programming and GSS. A new state has the form ℓ : hi, j, sd...s0i where [i..j] is the span of the top tree s</context>
<context position="17420" citStr="Earley (1970)" startWordPosition="3206" endWordPosition="3207">e grandparent but not the parent symbol. The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context). For instance, a context-free rule A —* B C would become DA —* DB BC for some D if there exists a rule E —* αDAQ. This resembles the reduce step in Fig. 3. The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the correctness of DP. 4In this sense, GSS (Tomita, 1988) is really not a new invention: an efficient implementation of Earley (1970) should already have it implicitly, similar to what we have in Fig. 3. 3.4 Beam Search based on Prefix Cost Though the DP algorithm runs in polynomialtime, in practice the complexity is still too high, esp. with a rich feature set like the one in Table 1. So we apply the same beam search idea from Sec. 2.3, where each step can accommodate only the best b states. To decide the ordering of states in each beam we borrow the concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As shown in Fig. 3, the prefix cost c is the total cost of the best action sequen</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and headautomaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="19481" citStr="Eisner and Satta, 1999" startWordPosition="3602" endWordPosition="3605">the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ′ + λ consists of shift cost ξ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is �f(j, S) = (j, h(s1), h(s0)) 5Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : ( , h where reg, cost λ = w · &amp;_(h′, h) Figure 4: Example of shift-reduce with dynamic programming: simulating an edge-factored model. GSS is implicit here, and re, case omitted. whe</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and headautomaton grammars. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="19333" citStr="Eisner, 1996" startWordPosition="3581" endWordPosition="3582">al combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ′ + λ consists of shift cost ξ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is �f(j, S) = (j, h(s1), h(s0)) 5Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : ( , h where reg, cost λ = w · &amp;_(h</context>
<context position="20557" citStr="Eisner, 1996" startWordPosition="3797" endWordPosition="3798">e 4: Example of shift-reduce with dynamic programming: simulating an edge-factored model. GSS is implicit here, and re, case omitted. where h(x) returns the head word index of tree x, because all features in this model are based on the head and modifier indices in a dependency link. This function is obviously bounded and monotonic in our definitions. The theoretical complexity of this algorithm is O(n7) because in a reduction step we have three span indices and three head indices, plus a step index E. By contrast, the naive CKY algorithm for this model is O(n5) which can be improved to O(n3) (Eisner, 1996).6 The higher complexity of our algorithm is due to two factors: first, we have to maintain both h and h′ in one state, because the current shift-reduce model can not draw features across different states (unlike CKY); and more importantly, we group states by step E in order to achieve incrementality and linear runtime with beam search that is not (easily) possible with CKY or MST. 4 Experiments We first reimplemented the reference shift-reduce parser of Huang et al. (2009) in Python (henceforth “non-DP”), and then extended it to do dynamic programing (henceforth “DP”). We evaluate their perfo</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Keith Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.</title>
<date>1982</date>
<journal>Cognitive Psychology,</journal>
<volume>14</volume>
<issue>2</issue>
<pages>210</pages>
<contexts>
<context position="1552" citStr="Frazier and Rayner, 1982" startWordPosition="232" endWordPosition="235">utperforms all previously reported dependency parsers for English and Chinese, yet is much faster. 1 Introduction In terms of search strategy, most parsing algorithms in current use for data-driven parsing can be divided into two broad categories: dynamic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.1 Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam. Can we combine the advantages of both approaches, that is, construct an incremental parser 1McDonald et al. (2005b) is a notable exception: the MST algorithm is exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely </context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Lyn Frazier and Keith Rayner. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14(2):178 – 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT-2005).</booktitle>
<contexts>
<context position="23890" citStr="Huang and Chiang (2005)" startWordPosition="4368" endWordPosition="4371">ak and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8DP’s k-best lists are extracted from the forest using the algorithm of Huang and Chiang (2005), rather than those in the final beam as in the non-DP case, because many derivations have been merged during dynamic programming. sh re. ...j ℓ + 1 : (h, j) : (c, 0) j&lt; n : (h′′, h′ ) : (c′, v′) ℓ : (h′, h ) : ( , v) ℓ + 1 : (h′′, h ) : (c′ + v + λ, v′ + v + λ) h′ i...j k...i k...i i...j ) : (c, ) 1082 avg. model score dependency accuracy 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 2394 2391 2388 2385 2382 2379 2376 2373 2370 b=16 b=64 DP non-DP 93.1 92.9 92.8 92.7 92.6 92.5 92.4 92.3 92.2 93 b=16 b=64 DP non-DP (a) search quality vs. time (full model) (b) parsing accu</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="5057" citStr="Huang et al. (2009)" startWordPosition="844" endWordPosition="847">g. 2 Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the 2There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical shift-reduce algorithm. input: “I saw Al with Joe” step action stack queue 0 - I ... 1 sh I saw ... 2 sh I saw Al ... 3 rex Ixsaw Al ... 4 sh Ixsaw Al with ... 5a rey IxsawyAl with... 5b sh Ixsaw Al with Joe Figure 2: A trace of vanilla shift-reduce.</context>
<context position="10198" citStr="Huang et al. (2009)" startWordPosition="1821" endWordPosition="1824">tic mode (b=1), updates always co-occur with the first mistake made. (a) Features Templates f(j, S) qi = wj+i s0.w s0.t s0.w o s0.t s1.w s1.t s1.w o s1.t q0.w q0.t q0.w o q0.t s0.w o s1.w s0.t o s1.t s0.t o q0.t s0.w o s0.t o s1.t s0.t o s1.w o s1.t s0.w o s1.w o s1.t s0.w o s0.t o s1.w s0.w o s0.t o s1 o s1.t s0.t o q0.t o q1.t s1.t o s0.t o q0.t s0.w o q0.t o q1.t s1.t o s0.w o q0.t s1.t o s1.lc.t o s0.t s1.t o s1.rc.t o s0.t s1.t o s0.t o s0.rc.t s1.t o s1.lc.t o s0 s1.t o s1.rc.t o s0.w s1.t o s0.w o s0.lc.t s2.t o s1.t o s0.t Table 1: (a) feature templates used in this work, adapted from Huang et al. (2009). x.w and x.t denotes the root word and POS tag of tree (or word) x. and x.lc and x.rc denote x’s left- and rightmost child. (b) feature window. (c) kernel features. (i.e., same step) if they have the same feature values, because they will have the same costs as shown in the deductive system in Figure 1. Thus we can define two states (j, S) and (j′, S′) to be equivalent, notated (j, S) — (j′, S′), iff. j = j′ and f(j, S) = f(j′, S′). (4) Note that j = j′ is also needed because the queue head position j determines which word to shift next. In practice, however, a small subset of atomic features</context>
<context position="21035" citStr="Huang et al. (2009)" startWordPosition="3876" endWordPosition="3879">head indices, plus a step index E. By contrast, the naive CKY algorithm for this model is O(n5) which can be improved to O(n3) (Eisner, 1996).6 The higher complexity of our algorithm is due to two factors: first, we have to maintain both h and h′ in one state, because the current shift-reduce model can not draw features across different states (unlike CKY); and more importantly, we group states by step E in order to achieve incrementality and linear runtime with beam search that is not (easily) possible with CKY or MST. 4 Experiments We first reimplemented the reference shift-reduce parser of Huang et al. (2009) in Python (henceforth “non-DP”), and then extended it to do dynamic programing (henceforth “DP”). We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing. Both DP and non-DP parsers use the same feature templates in Table 1. For Secs. 4.1-4.2, we use abaseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search 6Or O(n2) with MST, but including non-projective trees. 7Using the head rules of Yamada and Matsumot</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL: HLT,</booktitle>
<location>Columbus, OH,</location>
<contexts>
<context position="23200" citStr="Huang (2008" startWordPosition="4253" endWordPosition="4254">Fig. 5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing. 4.2 Search Space, Forest, and Oracles DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a). As a by-product, DP can output a forest encoding these exponentially many trees, out of which we can draw longer and better (in terms of oracle) kbest lists than those in the beam (see Fig. 6b). The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec. 4.1). These candidate sets may be used for reranking (Charniak and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8DP’s</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the ACL: HLT, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--613</pages>
<contexts>
<context position="16693" citStr="Johnson, 1998" startWordPosition="3083" endWordPosition="3084">l window and can only extract bounded information on each tree, which is always the case in practice since we can not have infinite models. Monotonicity, on the other hand, says that features drawn from trees farther away from the top should not be more refined than from those closer to the top. This is also natural, since the information most relevant to the current decision is always around the stack top. For example, the kernel feature function in Eq. 5 is bounded and monotonic, since f2 is less refined than f1 and f0. These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol. The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context). For instance, a context-free rule A —* B C would become DA —* DB BC for some D if there exists a rule E —* αDAQ. This resembles the reduce step in Fig. 3. The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the co</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24:613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="29160" citStr="Koo et al., 2008" startWordPosition="5327" endWordPosition="5330">e is in seconds per sentence. Search spaces: ‡linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 1001- 17th DP non-DP 18th 93.5 93 92.5 92 91.5 91 accuracy on dev (each round) 9</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>In Automata, Languages and Programming, 2nd Colloquium,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="30994" citStr="Lang, 1974" startWordPosition="5645" endWordPosition="5646"> Table 4 summarizes the final test results, where our work performs the best in all four types of (unlabeled) accuracies: word, non-root, root, and complete match (all excluding punctuations). 9,10 5 Related Work This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS). Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4). In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989). Our work advances this line of research in two aspects. First, ours is more general than GLR in 9Duan et al. (2007) and Zhang and Clark (2008) did not report word accuracies, but those can be recovered given nonroot and root ones, and the number of non-punctuation words. 10Parser combination in Zhang and Clark (2008) achieves a higher word accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thu</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>B. Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Automata, Languages and Programming, 2nd Colloquium, volume 14 of Lecture Notes in Computer Science, pages 255–269, Saarbr¨ucken. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Fast context-free grammar parsing requires fast Boolean matrix multiplication.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="2034" citStr="Lee (2002)" startWordPosition="315" endWordPosition="316">y large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam. Can we combine the advantages of both approaches, that is, construct an incremental parser 1McDonald et al. (2005b) is a notable exception: the MST algorithm is exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: • t</context>
</contexts>
<marker>Lee, 2002</marker>
<rawString>Lillian Lee. 2002. Fast context-free grammar parsing requires fast Boolean matrix multiplication. Journal of the ACM, 49(1):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="1800" citStr="McDonald et al. (2005" startWordPosition="277" endWordPosition="280">mic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.1 Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam. Can we combine the advantages of both approaches, that is, construct an incremental parser 1McDonald et al. (2005b) is a notable exception: the MST algorithm is exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature funct</context>
<context position="19356" citStr="McDonald et al., 2005" startWordPosition="3583" endWordPosition="3586">δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ′ + λ consists of shift cost ξ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is �f(j, S) = (j, h(s1), h(s0)) 5Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : ( , h where reg, cost λ = w · &amp;_(h′, h) Figure 4: Example</context>
<context position="22413" citStr="McDonald et al. (2005" startWordPosition="4116" endWordPosition="4119">between DP and nonDP, we run each parser on the development set, varying the beam width b from 2 to 16 (DP) or 64 (non-DP). Fig. 5a shows the relationship between search quality (as measured by the average model score per sentence, higher the better) and speed (average parsing time per sentence), where DP with a beam width of b=16 achieves the same search quality with non-DP at b=64, while being 5 times faster. Fig. 5b shows a similar comparison for dependency accuracy. We also test with an edge-factored model (Sec. 3.5) using feature templates (1)-(3) in Tab. 1, which is a subset of those in McDonald et al. (2005b). As expected, this difference becomes more pronounced (8 times faster in Fig. 5c), since the less expressive feature set makes more states “equivalent” and mergeable in DP. Fig. 5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing. 4.2 Search Space, Forest, and Oracles DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a). As a by-product, DP can output a forest encoding these exponentially many trees, out of wh</context>
<context position="29061" citStr="McDonald et al., 2005" startWordPosition="5314" endWordPosition="5317">so much faster than major parsers. †converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. ofHLTEMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. ofHLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>135--143</pages>
<contexts>
<context position="18454" citStr="Nederhof, 2003" startWordPosition="3404" endWordPosition="3405"> the concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As shown in Fig. 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p′ with prefix cost c′, notated p � p′ in Fig. 3, if c &lt; c′. We can also prove (by contradiction) that optimizing for prefix cost implies optimal inside cost (Nederhof, 2003, Sec. 4). 5 As shown in Fig. 3, when a state q with costs (c, v) is combined with a predictor state p with costs (c′, v′), the resulting state r will have costs (c′ + v + δ, v′ + v + δ), where the inside cost is intuitively the combined inside costs plus an additional combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ′ + λ consists of shift cost ξ′ of p and reduction cost λ of q. The cost in</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, pages 135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="4964" citStr="Nivre, 2004" startWordPosition="830" endWordPosition="831">per, though our formalism and algorithm can also be applied to phrase-structure parsing. 2 Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the 2There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical shift-reduce algorithm. input: “I saw Al with Joe” step action stack queue 0 - I ... 1 sh I saw ... 2 sh I saw Al ... 3 rex Ixsaw Al ... 4 sh Ixsaw Al with .</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="28736" citStr="Petrov and Klein, 2007" startWordPosition="5257" endWordPosition="5260"> − Zhang 08 single 91.4 C 0.11 O(n)‡ this work 92.1 Py 0.04 O(n) †Charniak 00 92.5 C 0.49 †Petrov 07 92.4 Ja 0.21 Zhang 08 combo 92.1 C − O(n2)‡ Koo 08 semisup 93.2 − − Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. †converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="32274" citStr="Roark and Hollingshead (2009)" startWordPosition="5850" endWordPosition="5853">mpractical for modern grammars with a large number of rules or features. In contrast, we employ the ideas behind GSS more flexibly to merge states based on features values, which can be viewed as constructing an implicit LR table on-the-fly. Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing. To the best of our knowledge, our work is the first linear-time incremental parser that performs dynamic programming. The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality. 6 Conclusion We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search. This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity. Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster lea</context>
</contexts>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2462" citStr="Stolcke, 1995" startWordPosition="377" endWordPosition="378">exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: • theoretically, we show that for a large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice); • practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a stateof-the-art shift-reduce dependency parser; • as a b</context>
<context position="12470" citStr="Stolcke (1995)" startWordPosition="2316" endWordPosition="2317">v′, ) iff. c &lt; c′ or (c = c′ and v &lt; v′). axiom (p0) 0 : h0, 0, ǫi: (0, 0, ∅) ℓ + 1 : 1 s _ ...s wj) : (c+ 0 { &lt;n (j� j+ E d i o� � p}) state q: ℓ :hi, j, sd...s0i: ( , v, π) xs0i : (c′ + v + δ, v′ + v + δ, π′) p ∈ π ℓ + 1 : hk, j, s′ d...s′ 1, s′ 0 goal 2n − 1 : h0, n, sd...s0i: (c, c, {p0}) where ξ = w · fsh(j, sd...s0), and δ = ξ′ + λ, with ξ′ = w · fsh(i, s′d...s′0) and λ = w · ffex (j, sd...s0). Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set π is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995). The rey case is similar, replacing s′ xs0 with s′ ys0, and λ with ρ = w · ffey (j, sd...s0). Irrelevant 0 0 information in a deduction step is marked as an underscore ( ) which means “can match anything”. sh rex state p: : hk, i, s′d...s′ 0i: (c′, v′, π′) state p: ℓ : h ,j, sd...s0i: (c, , ) tag of the next word q1. Since the queue is static information to the parser (unlike the stack, which changes dynamically), we can use j to replace features from the queue. So in general we write �f(j, S) = (j, fd(sd), ... , f0(s0)) if the feature window looks at top d + 1 trees on stack, and where fz(sz</context>
<context position="17887" citStr="Stolcke (1995)" startWordPosition="3296" endWordPosition="3297">y required by the correctness of DP. 4In this sense, GSS (Tomita, 1988) is really not a new invention: an efficient implementation of Earley (1970) should already have it implicitly, similar to what we have in Fig. 3. 3.4 Beam Search based on Prefix Cost Though the DP algorithm runs in polynomialtime, in practice the complexity is still too high, esp. with a rich feature set like the one in Table 1. So we apply the same beam search idea from Sec. 2.3, where each step can accommodate only the best b states. To decide the ordering of states in each beam we borrow the concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As shown in Fig. 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p′ with prefix cost c′, notated p � p′ in Fig. 3, if c &lt; c′. We can also prove (by contradiction) that optimizing for prefix cost implies optimal inside cost (Nederhof, 2003, Sec. 4). 5 As shown in Fig. 3, </context>
<context position="32060" citStr="Stolcke (1995)" startWordPosition="5822" endWordPosition="5823"> accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rules or features. In contrast, we employ the ideas behind GSS more flexibly to merge states based on features values, which can be viewed as constructing an implicit LR table on-the-fly. Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing. To the best of our knowledge, our work is the first linear-time incremental parser that performs dynamic programming. The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality. 6 Conclusion We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search. This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions </context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Graph-structured stack and natural language parsing.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>249--257</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12416" citStr="Tomita, 1988" startWordPosition="2306" endWordPosition="2307">f(j, s′d...s′0) ordering ℓ : : (c, v, ) ≺ ℓ : : (c′, v′, ) iff. c &lt; c′ or (c = c′ and v &lt; v′). axiom (p0) 0 : h0, 0, ǫi: (0, 0, ∅) ℓ + 1 : 1 s _ ...s wj) : (c+ 0 { &lt;n (j� j+ E d i o� � p}) state q: ℓ :hi, j, sd...s0i: ( , v, π) xs0i : (c′ + v + δ, v′ + v + δ, π′) p ∈ π ℓ + 1 : hk, j, s′ d...s′ 1, s′ 0 goal 2n − 1 : h0, n, sd...s0i: (c, c, {p0}) where ξ = w · fsh(j, sd...s0), and δ = ξ′ + λ, with ξ′ = w · fsh(i, s′d...s′0) and λ = w · ffex (j, sd...s0). Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set π is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995). The rey case is similar, replacing s′ xs0 with s′ ys0, and λ with ρ = w · ffey (j, sd...s0). Irrelevant 0 0 information in a deduction step is marked as an underscore ( ) which means “can match anything”. sh rex state p: : hk, i, s′d...s′ 0i: (c′, v′, π′) state p: ℓ : h ,j, sd...s0i: (c, , ) tag of the next word q1. Since the queue is static information to the parser (unlike the stack, which changes dynamically), we can use j to replace features from the queue. So in general we write �f(j, S) = (j, fd(sd), ... , f0(s0)) if the feature win</context>
<context position="17344" citStr="Tomita, 1988" startWordPosition="3193" endWordPosition="3194">d monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol. The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context). For instance, a context-free rule A —* B C would become DA —* DB BC for some D if there exists a rule E —* αDAQ. This resembles the reduce step in Fig. 3. The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the correctness of DP. 4In this sense, GSS (Tomita, 1988) is really not a new invention: an efficient implementation of Earley (1970) should already have it implicitly, similar to what we have in Fig. 3. 3.4 Beam Search based on Prefix Cost Though the DP algorithm runs in polynomialtime, in practice the complexity is still too high, esp. with a rich feature set like the one in Table 1. So we apply the same beam search idea from Sec. 2.3, where each step can accommodate only the best b states. To decide the ordering of states in each beam we borrow the concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As sh</context>
</contexts>
<marker>Tomita, 1988</marker>
<rawString>Masaru Tomita. 1988. Graph-structured stack and natural language parsing. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 249–257, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>1991</date>
<booktitle>Generalized LR Parsing.</booktitle>
<editor>Masaru Tomita, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="14074" citStr="(1991)" startWordPosition="2622" endWordPosition="2622"> information from only the last (d + 1) trees on stack instead of the whole stack, because they provide all the relevant information for the features, and thus determine the costs. For shift, this suffices as the stack grows on the right; but for reduce actions the stack shrinks, and in order still to maintain d + 1 trees, we have to know something about the history. This is exactly why we needed the full stack for vanilla shift-reduce parsing in the first place, and why dynamic programming seems hard here. To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991). Basically, each state p carries with it a set π(p) of predictor states, each of which can be combined with p in a reduction step. In a shift step, if state p generates state q (we say “p predicts q” in Earley (1970) terms), then p is added onto π(q). When two equivalent shifted states get merged, their predictor states get combined. In a reduction step, state q tries to combine with every predictor state p ∈ π(q), and the resulting state r inherits the predictor states set from p, i.e., π(r) = π(p). Interestingly, when two equivalent reduced states get merged, we can prove (by induction) tha</context>
</contexts>
<marker>1991</marker>
<rawString>Masaru Tomita, editor. 1991. Generalized LR Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<contexts>
<context position="21643" citStr="Yamada and Matsumoto (2003)" startWordPosition="3979" endWordPosition="3982">Huang et al. (2009) in Python (henceforth “non-DP”), and then extended it to do dynamic programing (henceforth “DP”). We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing. Both DP and non-DP parsers use the same feature templates in Table 1. For Secs. 4.1-4.2, we use abaseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search 6Or O(n2) with MST, but including non-projective trees. 7Using the head rules of Yamada and Matsumoto (2003). quality; in Sec. 4.3 we will retrain the model with DP and compare it against training with non-DP. 4.1 Speed Comparisons To compare parsing speed between DP and nonDP, we run each parser on the development set, varying the beam width b from 2 to 16 (DP) or 64 (non-DP). Fig. 5a shows the relationship between search quality (as measured by the average model score per sentence, higher the better) and speed (average parsing time per sentence), where DP with a beam width of b=16 achieves the same search quality with non-DP at b=64, while being 5 times faster. Fig. 5b shows a similar comparison f</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="5345" citStr="Zhang and Clark, 2008" startWordPosition="899" endWordPosition="902">o and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the 2There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical shift-reduce algorithm. input: “I saw Al with Joe” step action stack queue 0 - I ... 1 sh I saw ... 2 sh I saw Al ... 3 rex Ixsaw Al ... 4 sh Ixsaw Al with ... 5a rey IxsawyAl with... 5b sh Ixsaw Al with Joe Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). queue head position (current word q0 is wj). At each step, we choose one of the three actions: 1. sh: move the head of queue, wj, onto stack S as a singleton tree; 2. rex: combine the top two trees on the stack, s0 and s1, and r</context>
<context position="8587" citStr="Zhang and Clark, 2008" startWordPosition="1513" endWordPosition="1516">for state (j, S). To decide which action is the best for the current state, we perform a threeway classification based on f(j, S), and to do so, we further conjoin these feature instances with the action, producing action-conjoined instances like (s0.w = Al) o (q0.t = IN) o (action = sh). We denote fsh(j, S), frer (j, S), and fres, (j, S) to be the conjoined feature instances, whose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that la</context>
<context position="29207" citStr="Zhang and Clark, 2008" startWordPosition="5334" endWordPosition="5337">s: ‡linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 1001- 17th DP non-DP 18th 93.5 93 92.5 92 91.5 91 accuracy on dev (each round) 90.5 1084 1.4 Cha Berk MST DP 0.8 0.6 0.4 0.2 0 </context>
<context position="31247" citStr="Zhang and Clark (2008)" startWordPosition="5686" endWordPosition="5689">t by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS). Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4). In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989). Our work advances this line of research in two aspects. First, ours is more general than GLR in 9Duan et al. (2007) and Zhang and Clark (2008) did not report word accuracies, but those can be recovered given nonroot and root ones, and the number of non-punctuation words. 10Parser combination in Zhang and Clark (2008) achieves a higher word accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rules or features. In contrast, we employ the ideas behind GSS more flexibly to merge states based on features values, which can be viewed as constr</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings ofEMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>