<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.891704">
Mining Parenthetical Translations from the Web by Word Alignment
</title>
<note confidence="0.89880625">
Dekang Lin Shaojun Zhao† Benjamin Van Durme† Marius Pa�ca
Google, Inc. University of Rochester University of Rochester Google, Inc.
Mountain View Rochester Rochester Mountain View
CA, 94043 NY, 14627 NY, 14627 CA, 94043
</note>
<email confidence="0.840504">
lindek@google.com zhao@cs.rochester.edu vandurme@cs.rochester.edu mars@google.com
</email>
<sectionHeader confidence="0.995723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99907575">
Documents in languages such as Chinese,
Japanese and Korean sometimes annotate
terms with their translations in English inside
a pair of parentheses. We present a method to
extract such translations from a large collec-
tion of web documents by building a partially
parallel corpus and use a word alignment al-
gorithm to identify the terms being translated.
The method is able to generalize across the
translations for different terms and can relia-
bly extract translations that occurred only
once in the entire web. Our experiment on
Chinese web pages produced more than 26
million pairs of translations, which is over two
orders of magnitude more than previous re-
sults. We show that the addition of the ex-
tracted translation pairs as training data
provides significant increase in the BLEU
score for a statistical machine translation sys-
tem.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999524">
In natural language documents, a term (word or
phrase) is sometimes followed by its translation in
another language in a pair of parentheses. We call
these parenthetical translations. The following
examples are from Chinese web pages (we added
underlines to indicate what is being translated):
</bodyText>
<listItem confidence="0.997463285714286">
(1) 美国智库布鲁金斯学会(Brookings Institution)专研
跨大西洋恐怖主义的美欧中心研究部主任杰若米á夏皮
罗(Jeremy Shapiro)却认为,...
(2) 消化性溃疡的症状往往与消化不良(indigestion),胃
炎(gastritis)等其他胃部疾病症状相似.
(3) 殊不知美国是不会接受(not going to fly)这一想法的
(4) É当是一次式时,叫线性规划(linear programming).
</listItem>
<bodyText confidence="0.985299731707317">
†Contributions made during an internship at Google
The parenthetically translated terms are typically
new words, technical terminologies, idioms, prod-
ucts, titles of movies, books, songs, and names of
persons, organizations locations, etc. Commonly,
an author might use such a parenthetical when a
given term has no standard translation (or translit-
eration), and does not appear in conventional dic-
tionaries. That is, an author might expect a term to
be an out-of-vocabulary item for the target reader,
and thus helpfully provides a reference translation
in situ.
For example, in (1), the name Shapiro was
transliterated as 夏皮罗. The name has many other
transliterations in web documents, such as 夏皮洛,
夏比洛, 夏布洛, 夏皮羅, 沙皮罗, 夏皮若, 夏庇罗, 夏皮諾,
夏畢洛, 夏比羅, 夏比罗, 夏普羅, 夏批羅, 夏批罗, 夏彼羅,
夏彼罗, 夏培洛, 夏卜尔, 夏匹若 ..., where the three
Chinese characters corresponds to the three sylla-
bles in Sha-pi-ro respectively. Each syllable may
be mapped into different characters: &apos;Sha&apos; into 夏 or
沙, &apos;pi&apos; into 皮, 比, 批, and &apos;ro&apos; into 罗, 洛, 若, ....
Variation is not limited to the effects of phonetic
similarity. Story titles, for instance, are commonly
translated semantically, often leading to a number
of translations that have similar meaning, yet differ
greatly in lexicographic form. For example, while
the movie title Syriana is sometimes phonetically
transliterated as 辛瑞那, 辛瑞纳, it may also be trans-
lated semantically according to the plot of the
movie, e.g., 迷中迷 (mystery in mystery), 实录 (real
log), 谍 对 谍 (spy against spy), 油 激 暗 战 (oil-
triggered secret war), 叙利亚 (Syria), 迷经 (mystery
journey), ...
The parenthetical translations are extremely
valuable both as a stand-alone on-line dictionary
and as training data for statistical machine transla-
tion systems. They provide fresh data (new words)
and cover a much wider range of topics than typi-
cal parallel training data for statistical machine
translation systems.
</bodyText>
<page confidence="0.973898">
994
</page>
<note confidence="0.715291">
Proceedings of ACL-08: HLT, pages 994–1002,
</note>
<page confidence="0.537606">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9964294">
The main contribution of this paper is a method
for mining parenthetical translations by treating
text snippets containing candidate pairs as a par-
tially parallel corpus and using a word alignment
algorithm to establish the correspondences be-
tween in-parenthesis and pre-parenthesis words.
This technique allows us to identify translation
pairs even if they only appeared once on the entire
web. As a result, we were able to obtain 26.7 mil-
lion Chinese-English translation pairs from web
documents in Chinese. This is over two orders of
magnitude more than the number of extracted
translation pairs in the previously reported results
(Cao, et al. 2007).
The next section presents an overview of our al-
gorithm, which is then detailed in Sections 3 and 4.
We evaluate our results in Section 5 by comparison
with bilingually linked Wikipedia titles and by us-
ing the extracted pairs as additional training data in
a statistical machine translation system.
</bodyText>
<sectionHeader confidence="0.945699" genericHeader="method">
2 Mining Parenthetical Translations
</sectionHeader>
<bodyText confidence="0.802753">
A parenthetical translation matches the pattern:
</bodyText>
<equation confidence="0.598127">
(4) f1f2...fm (e1e2...en)
</equation>
<bodyText confidence="0.983386833333334">
which is a sequence of m non-English words fol-
lowed by a sequence of n English words in paren-
theses. In the remainder of the paper, we assume
the non-English text is Chinese, but our technique
works for other languages as well.
There have been two approaches to finding such
parenthetical translations. One is to assume that the
English term e1e2...en is given and use a search en-
gine to retrieve text snippets containing e1e2...en
from predominately non-English web pages (Na-
gata et al, 2001, Kwok et al, 2005). Another
method (Cao et al, 2007) is to go through a non-
English corpus and collect all instances that match
the parenthetical pattern in (4). We followed the
second approach since it does not require a prede-
fined list of English terms and is amendable for
extraction at large scale.
In both cases, one can obtain a list of candidate
pairs, where the translation of the in-parenthesis
terms is a suffix of the pre-parenthesis text. The
lengths and frequency counts of the suffixes have
been used to determine what is the translation of
the in-parenthesis term (Kwok et al, 2005). For
example, Table 1 lists a set of Chinese segments
(with word-to-word translation underneath) that
precede the English term Lower Egypt. Owing to
the frequency with which 下埃及 appears as a can-
didate, and in varying contexts, one has a good
reason to believe下埃及is the correct translation of
Lower Egypt.
</bodyText>
<table confidence="0.957329375">
... T ikX )(j T AR
downstream region is down Egypt
... 中心 位于 下 埃及
center located-at down Egypt
... 以及 所� 的 下 埃及
and so-called of down Egypt
... 叫做 下 埃及
called down Egypt
</table>
<tableCaption confidence="0.999923">
Table 1: Chinese text preceding Lower Egypt
</tableCaption>
<bodyText confidence="0.618106217391304">
Unfortunately, this heuristic does not hold as of-
ten as one might imagine. Consider the candidates
for Channel Spacing in Table 2. The suffixf7隔
(gap) has the highest frequency count. It is none-
theless an incomplete translation of Channel Spac-
ing. The correct translations in rows c to h
occurred with Channel Spacing only once.
a ... ❑ )(j &apos;%MI f7R
X is channel distance
b ... 其 &apos;%道 f7距
its channel distance
c ... 除了 降低 波道 f7距
in-addition-to reducing wave-passage distance
d ... 亦 展示 具 波道 f7隔
also showed have wave-passage gap
e ... 也 就 是 &apos;%道 f7隔
also therefore is channel gap
f ... 且 &apos;%道 的 f7隔
and channel ’s gap
g ... —^ Arc * L A 14MI f7�
an important property is signal-passage gap
h ... 已4-9-; 能J 达到 通道 f7隔
already able reach passage gap
</bodyText>
<tableCaption confidence="0.979379">
Table 2: Text preceding Channel Spacing
</tableCaption>
<bodyText confidence="0.923097666666667">
The crucial observation we make here is that al-
though the words like 信道 (in row g) co-occurred
with Channel Spacing only once, there are many
co-occurrences of 信道and Channel in other candi-
date pairs, such as:
... f�q T A isa 14MI (Speech Channel)
... 1 �iR -A MI (Block Flat Fading Channel)
... 14MI B (Channel B)
... tff 14MI Vhf&apos; (Fiber Channel Probes)
</bodyText>
<page confidence="0.994786">
995
</page>
<bodyText confidence="0.964582170731707">
... RA IM (Reverse Channel)
... AV 9R RA IM (Reverse Channel)
Unlike previous approaches that rely solely on
the preceding text of a single English term to de-
termine its translation, we treat the entire collection
of candidate pairs as a partially parallel corpus and
establish the correspondences between the words
using a word alignment algorithm.
At first glance, word alignment appears to be a
more difficult problem than the extraction of par-
enthetical translations. Extraction of parenthetical
translations need only determine the first pre-
parenthesis word aligned with an in-parenthesis
word, whereas word alignment requires the respec-
tive linking of all such (pre,in)-parenthesis word
pairs. However, by casting the problem as word
alignment, we are able to generalize across in-
stances involving different in-parenthesis terms,
giving us a larger number of, and more varied, ex-
ample contexts per word.
For the examples in Table 2, the words,%IM
(channel), R IM(wave passage), W IM (signal pas-
sage), and AIM (passage) are aligned with Channel,
and the wordsNW_(distance) and NN (gap) are
aligned with Spacing. Given these alignments, the
left boundary of the translated Chinese term is
simply the leftmost word that is linked to one of
the English words.
Our algorithm consists of two steps:
Step 1 constructs a partially parallel corpus. This
step takes as input a large collection of Chinese
web pages and converts the sentences with pa-
rentheses containing English text into pairs of
candidates.
Step 2 uses an unsupervised algorithm to align
English and Chinese and identify the term being
translated according to the left-most aligned
Chinese word. If no word alignments can be es-
tablished, the pair is not considered a translation.
The next two sections present the details of each of
the two steps.
</bodyText>
<sectionHeader confidence="0.7204" genericHeader="method">
3 Constructing a Partially Parallel Corpus
</sectionHeader>
<subsectionHeader confidence="0.99976">
3.1 Filtering out non-translations
</subsectionHeader>
<bodyText confidence="0.9924672">
The first step of our algorithm is to extract paren-
theticals and then filter out those that are not trans-
lations. This filtering is required as parenthetical
translations represent only a small fraction of the
usages for parentheses (see Sec. 5.1). Table 3
shows some example of parentheses that are not
translations.
The input to Step 1 is a collection of arbitrary
web documents. We used the following criteria to
identify candidate pairs:
</bodyText>
<listItem confidence="0.9961175">
• The pre-parenthesis text (Tp) is predominantly in
Chinese and the in-parenthesis text (Ti) is pre-
dominantly in English.
• The concatenation of the digits in Tp must be
identical to the concatenation of the digits in Ti.
For example, rows a, b and c in Table 3 can be
ruled out this way.
• If Tp contains some text in English, the same text
must also appear in Ti. This filters out row d.
• Remove the pairs where Ti is part of anchor text.
</listItem>
<bodyText confidence="0.914662">
This rule is often applied to instances like row e
where the file type tends to be inside a clickable
link to a media file.
</bodyText>
<listItem confidence="0.82929575">
• The punctuation characters in Tp must also ap-
pear in Ti, unless they are quotation marks. The
example in row f is ruled out because ‘/’ is not
found in the pre-parenthesis text.
</listItem>
<bodyText confidence="0.747376">
Examples with translations in Function of the in-
italic parenthesis text
a ARTi3l9t1.4~3.0ZN to provide citation
(MacArthur, 1967)
The range of its values is within
</bodyText>
<table confidence="0.992480666666667">
1.4~3.0 (MacArthur, 1967)
b cJIL��S/��� (VN901 flight information
15:20-22:30)
Vietnam Airlines Beijing/Ho Chi
Minh (VN901 15:20-22:30)
c �&apos; Af*#(255-8FT) product Id.
sale of pool table (255-8FT)
d // _tV1T_- // void main ( void ) function declaration
// main program // void
main (void )
e *,*,?;#;: -T�� (DVD) DVD is the file type
movie title: Thousand Year Lake
(DVD)
f 71C off {� J ,j ( g/L) measurement unit
mass consumed by water sample
(g/L)
g 3P*Q%rF_ffiA (Sensitive) to indicate the type
gentle protective facial cream of the cream
(Sensitive)
h X[A � ifMgMVqt Chapter 4 is about
(Ask Jeeves) Ask Jeeves
Evaluation of Nine Main Search
Engines in the US: Chapter 4
(Ask Jeeves)
</table>
<tableCaption confidence="0.999743">
Table 3: Other uses of parentheses
</tableCaption>
<page confidence="0.996919">
996
</page>
<bodyText confidence="0.999680333333333">
The instances in rows g and h cannot be eliminated
by these simple rules, and are filtered only later, as
we fail to discover a convincing word alignment.
</bodyText>
<subsectionHeader confidence="0.999903">
3.2 Constraining term boundaries
</subsectionHeader>
<bodyText confidence="0.999954954545454">
Similar to (Cao et al. 2007), we segmented the pre-
parenthesis Chinese text and restrict the term
boundary to be one of the segmentation bounda-
ries. Since parenthetical translations are mostly
translation of terms, it makes sense to further con-
strain the left boundary of the Chinese side to be a
term boundary. Determining what should be
counted as a term is a difficult task and there are
not yet well-accepted solutions (Sag et al, 2003).
We compiled an approximate term vocabulary
by taking the top 5 million most frequent Chinese
queries as according to a fully anonymized collec-
tion of search engine query logs.
Given a Chinese sentence, we first identify all
(possibly overlapping) sequences of words in the
sentence that match one of the top-5M queries. A
matching sequence is called a maximal match if it
is not properly contained in another matching se-
quence. We then define the potential boundary
positions to be the boundaries of maximal matches
or words that are not covered by any of the top-5M
queries.
</bodyText>
<subsectionHeader confidence="0.996015">
3.3 Length-based trimming
</subsectionHeader>
<bodyText confidence="0.999994">
If there are numerous Chinese words preceding a
pair of parentheses containing two English words,
it is very unlikely for all but the right-most few
Chinese words to be part of the translation of the
English words. Including extremely long se-
quences as potential candidates introduces signifi-
cantly more noise and makes word alignment
harder than necessary. We therefore trimmed the
pre-parenthesis text with a length-based constraint.
The cut-off point is the first (counting from right to
left) potential boundary position (see Sec. 3.2)
such that C &gt; 2 E + K, where C is the length of the
Chinese text, E is the length of the English text in
the parentheses and K is a constant (we used K=6
in our experiments). The lengths C and E are
measured in bytes, except when the English text is
an abbreviation (in that case, E is multiplied by 5).
</bodyText>
<sectionHeader confidence="0.997234" genericHeader="method">
4 Word Alignment
</sectionHeader>
<bodyText confidence="0.9997936">
Word alignment is a well-studied topic in Machine
Translation with many algorithms having been
proposed (Brown et al, 1993; Och and Ney 2003).
We used a modified version of one of the simplest
word alignment algorithms called Competitive
Linking (Melamed, 2000). The algorithm assumes
that there is a score associated with each pair of
words in a bi-text. It sorts the word pairs in de-
scending order of their scores, selecting pairs based
on the resultant order. A pair of words is linked if
none of the two words were previously linked to
any other words. The algorithm terminates when
there are no more links to make.
Tiedemann (2004) compared a variety of align-
ment algorithms and found Competitive Linking to
have one of the highest precision scores. A disad-
vantage of Competitive Linking, however, is that
the alignments are restricted word-to-word align-
ments, which implies that multi-word expressions
can only be partially linked at best.
</bodyText>
<subsectionHeader confidence="0.997628">
4.1 Dealing with multi-word alignment
</subsectionHeader>
<bodyText confidence="0.999961777777778">
We made a small change to Competitive Linking
to allow consecutive sequence of words on one
side to be linked to the same word on the other
side. Specifically, instead of requiring both e; and fj
to have no previous linkages, we only require that
at least one of them be unlinked and that (suppose
e; is unlinked and fj is linked to ek) none of the
words between e; and ek be linked to any word
other than fj.
</bodyText>
<subsectionHeader confidence="0.998331">
4.2 Link scoring
</subsectionHeader>
<bodyText confidence="0.999761125">
We used T2 (Gale and Church, 1991) as the link
score in the modified competitive linking algo-
rithm, although there are many other possible
choices for the link scores, such as X2 (Zhang, S.
Vogel. 2005), log-likelihood ratio (Dunning, 1993)
and discriminatively trained weights (Taskar et al,
2005). The T2 statistics for a pair of words e; and fj
is computed as
</bodyText>
<equation confidence="0.990835666666667">
(ad ! bc )2
(a b)( a c)( b d)( c d)
+ + + +
</equation>
<bodyText confidence="0.828127428571429">
where
a is the number of sentence pairs containing both e;
and fj;
a+b is the number of sentence pairs containing e;;
a+c is the number of sentence pairs containing fj;
d is the number of sentence pairs containing nei-
ther e; nor fj.
</bodyText>
<equation confidence="0.902421">
∀
z =
</equation>
<page confidence="0.976989">
997
</page>
<bodyText confidence="0.999850666666667">
The φ2 score ranges from 0 to 1. We set a
threshold at 0.001, below which the φ2 scores are
treated as 0.
</bodyText>
<subsectionHeader confidence="0.939719">
4.3 Bias in the partially parallel corpus
</subsectionHeader>
<bodyText confidence="0.999989375">
Since only the last few Chinese words in a candi-
date pair are expected to be translated, there should
be a preference for linking the words towards the
end of the Chinese text. One advantage of Com-
petitive Linking is that it is quite easy to introduce
such preferences into the algorithm, by using the
word positions to break ties of the φ2 scores when
sorting the word pairs.
</bodyText>
<subsectionHeader confidence="0.999686">
4.4 Capturing syllable-level regularities
</subsectionHeader>
<bodyText confidence="0.999257975609756">
Many of the parenthetical translations involve
proper names, which are often transliterated ac-
cording to the sound. Word alignment algorithms
have generally ignored syllable-level regularities in
transliterated terms. Consider again the Shapiro
example in the introduction section. There are nu-
merous correct transliterations for the same Eng-
lish word, some of which are not very frequent.
For example, the word 夏布洛happens to have a
similar φ2 score with Shapiro as the word 流利
(fluency), which is totally unrelated to Shapiro but
happened to have the same co-occurrence statistics
in the (partially) parallel corpus.
Previous approaches to parenthetical translations
relied on specialized algorithms to deal with trans-
literations (Cao et al, 2007; Jiang et al, 2007; Wu
and Chang, 2007). They convert Chinese words
into their phonetic representations (Pinyin) and use
the known transliterations in a bilingual dictionary
to train a transliteration model.
We adopted a simpler approach that does not re-
quire any additional resources such as pronuncia-
tion dictionaries and bilingual dictionaries. In
addition to computing the φ2 scores between
words, we also compute the φ2 scores of prefixes
and suffixes of Chinese and English words. For
both languages, the prefix of a word is defined as
the first three bytes of the word and the suffix is
defined as the last three bytes. Since we used UTF-
8 encoding, the first and last three bytes of a Chi-
nese word, except in very rare cases, correspond to
the first and last Chinese character of the word.
Table 4 lists the English prefixes and suffixes that
have the highest φ2 scores with the Chinese prefix
夏and suffix洛.
Type Chinese English
prefix 夏 sha, amo, cha, sum, haw, lav, lun,
xia, xal, hnl, shy, eve, she, cfh, ...
suffix 洛 rlo, llo, ouh, low, ilo, owe, lol, lor,
zlo, klo, gue, ude, vir, row, oro, olo,
aro, ulo, ero, iro, rro, loh, lok, ...
</bodyText>
<tableCaption confidence="0.978725">
Table 4: Example prefixes and suffixes with top φ2
</tableCaption>
<bodyText confidence="0.9941165625">
In our modified version of the competitive link-
ing algorithm, the link score of a pair of words is
the sum of the φ2 scores of the words themselves,
their prefixes and their suffixes.
In addition to syllable-level correspondences in
transliterations, the φ2 scores of prefixes and suf-
fixes can also capture correlations in morphologi-
cally composed words. For example, the Chinese
prefix 三 (three) has a relatively high φ2 score with
the English prefix tri. Such scores enable word
alignments to be made that may otherwise be
missed. Consider the following text snippet:
...... 三 嗪 氟草胺 (triaziflam)
The correct translation for triaziflam is三嗪氟草胺
. However, the Chinese term is segmented as 三 +
嗪 + 氟草胺. The association between三 (three)
and triaziflam is very weak because 三is a very
frequent word, whereas triaziflam is an extremely
rare word. With the addition of the φ2 score be-
tween 三and tri, we were able to correctly estab-
lish the connection between triaziflam and 三.
It turns out to be quite effective to assume pre-
fixes and suffixes of words consist of three bytes,
despite its apparent simplicity. The benefit of φ2
scores for prefixes and suffixes is not limited to
morphemes that happen to be three bytes long. For
example, the English morpheme “du-” corresponds
to the Chinese character 二 (two). Although the φ2
between du and二 won’t be computed, we do find
high φ2 scores between二 and due and between二
and dua. The three letter prefixes account for many
of the words with the du- prefix.
</bodyText>
<sectionHeader confidence="0.997856" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999933">
We extracted from Chinese web pages about 1.58
billion unique sentences with parentheses that con-
tain ASCII text. We removed duplicate sentences
so that duplications of web documents will not
skew the statistics. By applying the filtering algo-
rithm in Sec. 3.1, we constructed a partially paral-
</bodyText>
<page confidence="0.996389">
998
</page>
<bodyText confidence="0.999677833333333">
lel corpus with 126,612,447 candidate pairs
(46,791,841 unique), which is about 8% of the
number of sentences. Using the word alignment
algorithm in Sec. 4, we extracted 26,753,972 trans-
lation pairs between 13,471,221 unique English
terms and 11,577,206 unique Chinese terms.
Parenthetical translations mined from the Web
have mostly been evaluated by manual examina-
tion of a small sample of results (usually a few
hundred entries) or in a Cross Lingual Information
Retrieval setup. There does not yet exist a common
evaluation data set.
</bodyText>
<subsectionHeader confidence="0.997633">
5.1 Evaluation with Wikipedia
</subsectionHeader>
<bodyText confidence="0.970446333333333">
Our first evaluation is based on translations in
Wikipedia, which contains far more terminology
and proper names than bilingual dictionaries. We
extracted the titles of Chinese and English Wikipe-
dia articles that are linked to each other and treated
them as gold standard translations. There are
79,714 such pairs. We removed the following
types of pairs because they are not translations or
are not terms:
</bodyText>
<listItem confidence="0.999005666666667">
• Pairs with identical strings. For example, both
English and Chinese versions have an entry ti-
tled “.ch”;
• Pairs where the English term begins with a
digit, e.g., “245”, “300 BC”, “1991 in film”;
• Pairs where the English term matches the regu-
lar expression ‘List of .*’, e.g., “List of birds”,
“List of cinemas in Hong Kong”;
• Pairs where the Chinese title does not have any
</listItem>
<bodyText confidence="0.930813352941177">
non-ASCII code. For example, the English en-
try “Syncfusion” is linked to “.NET Frame-
work” in the Chinese Wikipedia.
The resulting data set contains 68,131 transla-
tion pairs between 62,581 Chinese terms and
67,613 English terms. Only a small percentage of
terms have more than one translation. Whenever
there is more than one translation, we randomly
pick one as the answer key.
For each Chinese and English word in the
Wikipedia data, we first find whether there is a
translation for the word in the extracted translation
pairs. The Coverage of the Wikipedia data is
measured by the percentage of words for which
one or more translations are found. We then see
whether our most frequent translation is an Exact
Match of the answer key in the Wikipedia data.
</bodyText>
<table confidence="0.999283">
Coverage Exact Match
Full 70.8% 36.4%
-term 67.1% 34.8%
-pre-suffix 67.6% 34.4%
IBM 67.6% 31.2%
LDC 10.8% 4.8%
</table>
<tableCaption confidence="0.992201">
Table 5: Chinese to English Results
</tableCaption>
<table confidence="0.999876">
Coverage Exact Match
Full 59.6% 27.9%
-term 59.6% 27.5%
-pre-suffix 58.9% 27.4%
IBM 52.4% 13.4%
LDC 3.0% 1.4%
</table>
<tableCaption confidence="0.99966">
Table 6: English to Chinese Results
</tableCaption>
<bodyText confidence="0.99042721875">
Table 5 and 6 show the Chinese-to-English and
English-to-Chinese results for the following sys-
tems:
Full refers to our system described in Sec. 3
and 4;
-term is the system without the use of query
logs to restrict potential term boundary posi-
tions (Sec. 3.2);
-pre-suffix is the system without using the T2
score of the prefixes and suffixes;
IBM refers to a system where we substitute
our word alignment algorithm with IBM
Model 1 and Model 2 followed by the HMM
alignment (Och and Ney 2003), which is a
common configuration for the word align-
ment components in machine translations
systems;
LDC refers to the LDC2.0 English to Chinese
bilingual dictionary with 161,117 translation
pairs.
It can be seen that the use of queries to constrain
boundary positions and the addition of T2 scores of
prefixes/suffixes improve the percentage of Exact
Match. The IBM Model tends to make many more
alignments than Completive Linking. While this is
often beneficial for machine translation systems, it
is not very suitable for creating bilingual dictionar-
ies, where precision is of paramount importance.
The LDC dictionary was manually compiled from
diverse resources within LDC and (mostly) from
the Internet. Its coverage of Wikipedia data is ex-
tremely low, compared to our method.
</bodyText>
<page confidence="0.995317">
999
</page>
<table confidence="0.999949355555556">
English Wikipedia Parenthetical
Translation Translation
Pumping lemma 泵引理 引理1
Topic-prominent 话题优先语言 突出性语言1
language
Yoido Full Gos- 汝矣岛纯福音教 全备福音教会1
pel Church 会
First Bulgarian 第一保加利亚帝 强大的保加利
Empire 国 亚帝国2
Vespid 黄蜂 针对境内胡蜂2
Ibrahim Rugova 易卜拉欣·鲁戈瓦 鲁戈瓦3
Jerry West 杰里·韦斯特 威斯特3
Nicky Butt 尼基·巴特 巴特3
Benito Mussolini 贝尼托·墨索里尼 墨索里尼3
Ecology of Hong 香港生态 本文介绍的*
Kong
Paracetamol 对乙酰氨基酚 扑热息痛*
Thermidor 热月 必杀*
Udo 独活 乌多
Public opinion 舆论 公众舆论
Michael Bay 麦可·贝 迈克尔·贝
Dagestan 达吉斯坦共和国 达吉斯坦
Battle of Leyte 莱特湾海战 莱伊特海湾战
Gulf 役
Glock 格洛克手枪 格洛克
Ergonomics 人因工程学 工效学
Frank Sinatra 法兰·仙纳杜拉 法兰克辛纳屈
Zaragoza 萨拉戈萨省 萨拉戈萨
Komodo 科莫多岛 科摩多岛
Eli Vance 伊莱·万斯 伊莱‧凡斯博士
Manitoba 缅尼托巴 曼尼托巴省
Giant Bottlenose 阿氏贝喙鲸 巨瓶鼻鲸
Whale
Exclusionary rule 证据排除法则 证据排除规则
Computer worm 蠕虫病毒 计算机蠕虫
Social network 社会性网络 社会网络
Glasgow School 格拉斯哥艺术学 格拉斯哥艺术
of Art 校 学院
Dee Hock 狄伊·哈克 迪伊·霍克
Bondage 绑缚 束缚
The China Post 英文中国邮报 中国邮报
Rachel 拉结 瑞秋
John Nash 约翰·纳西 约翰·纳什
Hattusa 哈图沙 哈图萨
Bangladesh 孟加拉国 孟加拉
</table>
<tableCaption confidence="0.998768">
Table 7: A random sample of non-exact-matches
</tableCaption>
<footnote confidence="0.99005875">
1the extracted translation is too short
2the extracted translation is too long
3the extracted translation contains only the last name
*the extracted term is completely wrong.
</footnote>
<bodyText confidence="0.999360058823529">
Note that Exact Match is a rather stringent crite-
rion. Table 7 shows a random sample of extracted
parenthetical translations that failed the Exact
Match test. Only a small percentage of them are
genuine errors. We nonetheless adopted this meas-
ure because it has the advantage of automated
evaluation and our goal is mainly to compare the
relative performances.
To determine the upper bound of the coverage
of our web data, for each Wikipedia English term
we searched within the total set of available paren-
thesized text fragments (our English candidate set
before filtering as by Step 1). We discovered 81%
of the Wikipedia titles, which is approximately
10% above the coverage of our final output. This
indicates a minor loss of recall because of mistakes
made in filtering (Sec. 3.1) and/or word alignment.
</bodyText>
<subsectionHeader confidence="0.998427">
5.2 Evaluation with term translation requests
</subsectionHeader>
<bodyText confidence="0.981880517241379">
To evaluate the coverage of output produced by
their method, Cao et al (2007) extracted English
queries from the query log of a Chinese search en-
gine. They assume that the reason why users typed
the English queries in a Chinese search box is
mostly to find out their Chinese translations. Ex-
amining our own Chinese query logs, however, the
most-frequent English queries appear to be naviga-
tional queries instead of translation requests. We
therefore used the following regular expression to
identify queries that are unambiguously translation
requests:
/^[a-zA-Z ]* 的中文$/
where的中文means “’s Chinese”. This regular ex-
pression matched 1579 unique queries in the logs.
We manually judged the translation for 200 of
them. A small random sample of the 200 is shown
in Table 8. The empty cells indicate that the Eng-
lish term is missing from our translation pairs. We
use * to mark incorrect translations. When com-
pared with the sample queries in (Cao et al., 2007),
the queries in our sample seem to contain more
phrasal words and technical terminology. It is in-
teresting to see that even though parenthetical
translations tend to be out-of-vocabulary words, as
we have remarked in the introduction, the sheer
size of the web means that occasionally transla-
tions of common words such as ‘use’ are some-
times included as well.
</bodyText>
<page confidence="0.849442">
1000
</page>
<table confidence="0.999822285714286">
buckingham palace 0��Vlb
chinadaily r&apos;FAQA
coo �fm�V�
diammonium sulfate
emilio pucci ��� a�
finishing school TN Te��
gloria 4MMA
horny KA&amp;!9q:4*
jam ��
lean six sigma TN MAN-A4A
meiosis AR3tS w
near miss A3ff011
pachycephalosaurus ���
pops #l�&apos;IILii1
recreation vehicle �KNXVAF
shanghai ethylene
cracker complex
stenonychosaurus NIRA
theanine ,1 AM
use *M
with you all the time HE*Qfl&apos;,V_ j1KMQTA
</table>
<tableCaption confidence="0.980715">
Table 8: A small sample of manually judged query
translations
</tableCaption>
<bodyText confidence="0.9577026875">
We compared our results with translations ob-
tained from Google and Yahoo’s translation serv-
ices. The numbers of correct translations for the
random sample of 200 queries are as follows:
Systems Google Yahoo! Mined Mined+G
Correct 115 84 116 135
Our system’s outputs (Mined) have the same
accuracy as the Google Translate. Our outputs
have results for 154 out of the 200 queries. The 46
missing results are considered incorrect. If we
combine our results with Google Translate by
looking up Google results for missing entries, the
accuracy increases from 56% to 68% (Mined+G).
If we treat the LDC Chinese-English Dictionary
2.0 as a translator, it only covers 20.5% of the 200
queries.
</bodyText>
<subsectionHeader confidence="0.997362">
5.3 Evaluation with SMT
</subsectionHeader>
<bodyText confidence="0.9999676">
The extracted translations may serve as training
data for statistical machine translation systems. To
evaluate their effectiveness for this purpose, we
trained a baseline phrase-based SMT system
(Koehn et al, 2003; Brants et al, 2007) with the
FBIS Chinese-English parallel text (NIST, 2003).
We then added the extracted translation pairs as
additional parallel training corpus. This resulted in
a 0.57 increase of BLEU score based on the test
data in the 2006 NIST MT Evaluation Workshop.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999972157894737">
Nagata et al. (2001) made the first proposal to
mine translations from the web. Their work was
concentrated on terminologies, and assumed the
English terms were given as input. Wu and Chang
(2007), Kwok et al. (2005) also employed search
engines and assumed the English term given as
input, but their focus was on name transliteration.
It is difficult to build a truly large-scale translation
lexicon this way because the English terms them-
selves may be hard to come by.
Cao et al. (2007), like us, used a 300GB collec-
tion of web documents as input. They used super-
vised learning to build models that deal with
phonetic transliterations and semantic translations
separately. Our work relies on unsupervised learn-
ing and does not make a distinction between trans-
lations and transliterations. Furthermore, we are
able to extract two orders of magnitude more trans-
lations from than (Cao et al., 2007).
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984666666667">
We presented a method to apply a word alignment
algorithm on a partially parallel corpus to extract
translation pairs from the web. Treating the transla-
tion extraction problem as a word alignment prob-
lem allowed us to generalize across instances
involving different in-parenthesis terms. Our algo-
rithm extends Competitive Linking to deal with
multi-word alignments and takes advantage of
word-internal correspondences between transliter-
ated words or morphologically composed words.
Finally, through our discussion of parallel Wikipe-
dia topic titles as a gold standard, we presented the
first evaluation of such an extraction system that
went beyond manual judgments on small sized
samples.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998655">
We would like to thank the anonymous reviewers
for their valuable comments.
</bodyText>
<page confidence="0.987992">
1001
</page>
<sectionHeader confidence="0.998332" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999840072727273">
T. Brants, A. Popat, P. Xu, F. Och and J. Dean, Large
Language Models for Machine Translation, EMNLP-
CoNLL-2007.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311.
G. Cao, J. Gao and J.Y. Nie. 2007. A system to mine
large-scale bilingual dictionaries from monolingual
Web pages, MT Summit, pp. 57-64.
T. Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics 19, 1.
W. Gale and K. Church. 1991. Identifying word corre-
spondence in parallel text. In Proceedings of the
DARPA NLP Workshop.
L. Jiang, M. Zhou, L.F. Chien, C. Niu. 2007. Named
Entity Translation with Web Mining and Translitera-
tion. In Proc. of IJCAI-2007. pp. 1629-1634.
P. Koehn, F. Och and D. Marcu, Statistical Phrase-
based Translation, In Proc. of HLT-NAACL 2003.
K.L. Kwok, P. Deng, N. Dinstl, H.L. Sun, W. Xu, P.
Peng, and J. Doyon. 2005. CHINET: a Chinese name
finder system for document triage. In Proceedings of
2005 International Conference on Intelligence
Analysis.
I.D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics,
26(2):221–249.
M. Nagata, T. Saito, and K. Suzuki. 2001. Using the
Web as a bilingual dictionary. In Proc. of ACL 2001
DD-MT Workshop, pp.95-102.
NIST. 2003. The NIST machine translation evaluations.
http://www.nist.gov/speech/tests/mt/.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
I.A. Sag, T. Baldwin, F. Bond, A. Copestake, and D.
Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of CICLing-2002, pp 1–
15, Mexico City, Mexico.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. of HLT/EMNLP-05. Vancouver, BC.
J. Tiedemann. 2004. Word to word alignment strategies.
In Proceedings of the 20th international Conference
on Computational Linguistics. Geneva, Switzerland.
J.C. Wu and J.S. Chang. 2007. Learning to Find English
to Chinese Transliterations on the Web. In Proc. of
EMNLP-CoNLL-2007. pp.996-1004. Prague, Czech
Republic.
Y. Zhang, S. Vogel. 2005 Competitive Grouping in In-
tegrated Phrase Segmentation and Alignment Model.
in Proceedings of ACL-05 Workshop on Building and
Parallel Text. Ann Arbor, MI.
</reference>
<page confidence="0.995277">
1002
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737333">
<title confidence="0.999516">Mining Parenthetical Translations from the Web by Word Alignment</title>
<author confidence="0.999867">Dekang Lin Van</author>
<affiliation confidence="0.985799">Google, Inc. University of Rochester University of Rochester Google, Inc.</affiliation>
<address confidence="0.974757">Mountain View Rochester Rochester Mountain View CA, 94043 NY, 14627 NY, 14627 CA, 94043</address>
<email confidence="0.999629">lindek@google.comzhao@cs.rochester.eduvandurme@cs.rochester.edumars@google.com</email>
<abstract confidence="0.989780428571428">Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collection of web documents by building a partially parallel corpus and use a word alignment algorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can reliably extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous results. We show that the addition of the extracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Popat</author>
<author>P Xu</author>
<author>F Och</author>
<author>J Dean</author>
</authors>
<title>Large Language Models for Machine Translation,</title>
<date>2007</date>
<contexts>
<context position="28482" citStr="Brants et al, 2007" startWordPosition="4739" endWordPosition="4742">r outputs have results for 154 out of the 200 queries. The 46 missing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries. 5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliter</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>T. Brants, A. Popat, P. Xu, F. Och and J. Dean, Large Language Models for Machine Translation, EMNLPCoNLL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13766" citStr="Brown et al, 1993" startWordPosition="2277" endWordPosition="2280">fore trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C &gt; 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cao</author>
<author>J Gao</author>
<author>J Y Nie</author>
</authors>
<title>A system to mine large-scale bilingual dictionaries from monolingual Web pages,</title>
<date>2007</date>
<journal>MT Summit,</journal>
<pages>57--64</pages>
<contexts>
<context position="4422" citStr="Cao, et al. 2007" startWordPosition="668" endWordPosition="671"> is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a partially parallel corpus and using a word alignment algorithm to establish the correspondences between in-parenthesis and pre-parenthesis words. This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 million Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al. 2007). The next section presents an overview of our algorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by using the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern: (4) f1f2...fm (e1e2...en) which is a sequence of m non-English words followed by a sequence of n English words in parentheses. In the remainder of the paper, we assume the non-English text is Chinese, but our techni</context>
<context position="11768" citStr="Cao et al. 2007" startWordPosition="1937" endWordPosition="1940">le type movie title: Thousand Year Lake (DVD) f 71C off {� J ,j ( g/L) measurement unit mass consumed by water sample (g/L) g 3P*Q%rF_ffiA (Sensitive) to indicate the type gentle protective facial cream of the cream (Sensitive) h X[A � ifMgMVqt Chapter 4 is about (Ask Jeeves) Ask Jeeves Evaluation of Nine Main Search Engines in the US: Chapter 4 (Ask Jeeves) Table 3: Other uses of parentheses 996 The instances in rows g and h cannot be eliminated by these simple rules, and are filtered only later, as we fail to discover a convincing word alignment. 3.2 Constraining term boundaries Similar to (Cao et al. 2007), we segmented the preparenthesis Chinese text and restrict the term boundary to be one of the segmentation boundaries. Since parenthetical translations are mostly translation of terms, it makes sense to further constrain the left boundary of the Chinese side to be a term boundary. Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al, 2003). We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collection of search engine query logs. Given a Chi</context>
<context position="17019" citStr="Cao et al, 2007" startWordPosition="2842" endWordPosition="2845">lgorithms have generally ignored syllable-level regularities in transliterated terms. Consider again the Shapiro example in the introduction section. There are numerous correct transliterations for the same English word, some of which are not very frequent. For example, the word 夏布洛happens to have a similar φ2 score with Shapiro as the word 流利 (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus. Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not require any additional resources such as pronunciation dictionaries and bilingual dictionaries. In addition to computing the φ2 scores between words, we also compute the φ2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defin</context>
<context position="25833" citStr="Cao et al (2007)" startWordPosition="4311" endWordPosition="4314">mainly to compare the relative performances. To determine the upper bound of the coverage of our web data, for each Wikipedia English term we searched within the total set of available parenthesized text fragments (our English candidate set before filtering as by Step 1). We discovered 81% of the Wikipedia titles, which is approximately 10% above the coverage of our final output. This indicates a minor loss of recall because of mistakes made in filtering (Sec. 3.1) and/or word alignment. 5.2 Evaluation with term translation requests To evaluate the coverage of output produced by their method, Cao et al (2007) extracted English queries from the query log of a Chinese search engine. They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations. Examining our own Chinese query logs, however, the most-frequent English queries appear to be navigational queries instead of translation requests. We therefore used the following regular expression to identify queries that are unambiguously translation requests: /^[a-zA-Z ]* 的中文$/ where的中文means “’s Chinese”. This regular expression matched 1579 unique queries in the logs. We manually </context>
<context position="29241" citStr="Cao et al. (2007)" startWordPosition="4867" endWordPosition="4870">s. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007), like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learning and does not make a distinction between translations and transliterations. Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007). 7 Conclusion We presented a method to apply a word alignment algorithm on a partially parallel corpus to extract translation pairs from the web. Treating the translation extraction problem </context>
</contexts>
<marker>Cao, Gao, Nie, 2007</marker>
<rawString>G. Cao, J. Gao and J.Y. Nie. 2007. A system to mine large-scale bilingual dictionaries from monolingual Web pages, MT Summit, pp. 57-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>1</pages>
<contexts>
<context position="15295" citStr="Dunning, 1993" startWordPosition="2544" endWordPosition="2545">w consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both e; and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose e; is unlinked and fj is linked to ek) none of the words between e; and ek be linked to any word other than fj. 4.2 Link scoring We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The T2 statistics for a pair of words e; and fj is computed as (ad ! bc )2 (a b)( a c)( b d)( c d) + + + + where a is the number of sentence pairs containing both e; and fj; a+b is the number of sentence pairs containing e;; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither e; nor fj. ∀ z = 997 The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics 19, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
</authors>
<title>Identifying word correspondence in parallel text.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA NLP Workshop.</booktitle>
<contexts>
<context position="15089" citStr="Gale and Church, 1991" startWordPosition="2509" endWordPosition="2512">ts are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best. 4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both e; and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose e; is unlinked and fj is linked to ek) none of the words between e; and ek be linked to any word other than fj. 4.2 Link scoring We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The T2 statistics for a pair of words e; and fj is computed as (ad ! bc )2 (a b)( a c)( b d)( c d) + + + + where a is the number of sentence pairs containing both e; and fj; a+b is the number of sentence pairs containing e;; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither e; no</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. Gale and K. Church. 1991. Identifying word correspondence in parallel text. In Proceedings of the DARPA NLP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Jiang</author>
<author>M Zhou</author>
<author>L F Chien</author>
<author>C Niu</author>
</authors>
<title>Named Entity Translation with Web Mining and Transliteration.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI-2007.</booktitle>
<pages>1629--1634</pages>
<contexts>
<context position="17038" citStr="Jiang et al, 2007" startWordPosition="2846" endWordPosition="2849">nerally ignored syllable-level regularities in transliterated terms. Consider again the Shapiro example in the introduction section. There are numerous correct transliterations for the same English word, some of which are not very frequent. For example, the word 夏布洛happens to have a similar φ2 score with Shapiro as the word 流利 (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus. Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not require any additional resources such as pronunciation dictionaries and bilingual dictionaries. In addition to computing the φ2 scores between words, we also compute the φ2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last thre</context>
</contexts>
<marker>Jiang, Zhou, Chien, Niu, 2007</marker>
<rawString>L. Jiang, M. Zhou, L.F. Chien, C. Niu. 2007. Named Entity Translation with Web Mining and Transliteration. In Proc. of IJCAI-2007. pp. 1629-1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrasebased Translation,</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<contexts>
<context position="28461" citStr="Koehn et al, 2003" startWordPosition="4735" endWordPosition="4738">oogle Translate. Our outputs have results for 154 out of the 200 queries. The 46 missing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries. 5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus w</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och and D. Marcu, Statistical Phrasebased Translation, In Proc. of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
<author>P Deng</author>
<author>N Dinstl</author>
<author>H L Sun</author>
<author>W Xu</author>
<author>P Peng</author>
<author>J Doyon</author>
</authors>
<title>CHINET: a Chinese name finder system for document triage.</title>
<date>2005</date>
<booktitle>In Proceedings of 2005 International Conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="5344" citStr="Kwok et al, 2005" startWordPosition="820" endWordPosition="823">ing Parenthetical Translations A parenthetical translation matches the pattern: (4) f1f2...fm (e1e2...en) which is a sequence of m non-English words followed by a sequence of n English words in parentheses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well. There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005). Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005). For example, Table 1</context>
<context position="28969" citStr="Kwok et al. (2005)" startWordPosition="4820" endWordPosition="4823">valuate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007), like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learning and does not make a distinction between translations and transliterations. Furthermore, we are able </context>
</contexts>
<marker>Kwok, Deng, Dinstl, Sun, Xu, Peng, Doyon, 2005</marker>
<rawString>K.L. Kwok, P. Deng, N. Dinstl, H.L. Sun, W. Xu, P. Peng, and J. Doyon. 2005. CHINET: a Chinese name finder system for document triage. In Proceedings of 2005 International Conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="13905" citStr="Melamed, 2000" startWordPosition="2301" endWordPosition="2302">oundary position (see Sec. 3.2) such that C &gt; 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignme</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I.D. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagata</author>
<author>T Saito</author>
<author>K Suzuki</author>
</authors>
<title>Using the Web as a bilingual dictionary.</title>
<date>2001</date>
<booktitle>In Proc. of ACL 2001 DD-MT Workshop,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="5325" citStr="Nagata et al, 2001" startWordPosition="815" endWordPosition="819">lation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern: (4) f1f2...fm (e1e2...en) which is a sequence of m non-English words followed by a sequence of n English words in parentheses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well. There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005). Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005). F</context>
<context position="28773" citStr="Nagata et al. (2001)" startWordPosition="4787" endWordPosition="4790">ctionary 2.0 as a translator, it only covers 20.5% of the 200 queries. 5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007), like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic </context>
</contexts>
<marker>Nagata, Saito, Suzuki, 2001</marker>
<rawString>M. Nagata, T. Saito, and K. Suzuki. 2001. Using the Web as a bilingual dictionary. In Proc. of ACL 2001 DD-MT Workshop, pp.95-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The NIST machine translation evaluations.</title>
<date>2003</date>
<note>http://www.nist.gov/speech/tests/mt/.</note>
<contexts>
<context position="28539" citStr="NIST, 2003" startWordPosition="4749" endWordPosition="4750">sing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries. 5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale trans</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The NIST machine translation evaluations. http://www.nist.gov/speech/tests/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13785" citStr="Och and Ney 2003" startWordPosition="2281" endWordPosition="2284">e-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C &gt; 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest prec</context>
<context position="22936" citStr="Och and Ney 2003" startWordPosition="3848" endWordPosition="3851">ull 59.6% 27.9% -term 59.6% 27.5% -pre-suffix 58.9% 27.4% IBM 52.4% 13.4% LDC 3.0% 1.4% Table 6: English to Chinese Results Table 5 and 6 show the Chinese-to-English and English-to-Chinese results for the following systems: Full refers to our system described in Sec. 3 and 4; -term is the system without the use of query logs to restrict potential term boundary positions (Sec. 3.2); -pre-suffix is the system without using the T2 score of the prefixes and suffixes; IBM refers to a system where we substitute our word alignment algorithm with IBM Model 1 and Model 2 followed by the HMM alignment (Och and Ney 2003), which is a common configuration for the word alignment components in machine translations systems; LDC refers to the LDC2.0 English to Chinese bilingual dictionary with 161,117 translation pairs. It can be seen that the use of queries to constrain boundary positions and the addition of T2 scores of prefixes/suffixes improve the percentage of Exact Match. The IBM Model tends to make many more alignments than Completive Linking. While this is often beneficial for machine translation systems, it is not very suitable for creating bilingual dictionaries, where precision is of paramount importance</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proc. of CICLing-2002, pp 1– 15,</booktitle>
<location>Mexico City, Mexico.</location>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>I.A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of CICLing-2002, pp 1– 15, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP-05.</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="15353" citStr="Taskar et al, 2005" startWordPosition="2550" endWordPosition="2553">ked to the same word on the other side. Specifically, instead of requiring both e; and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose e; is unlinked and fj is linked to ek) none of the words between e; and ek be linked to any word other than fj. 4.2 Link scoring We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The T2 statistics for a pair of words e; and fj is computed as (ad ! bc )2 (a b)( a c)( b d)( c d) + + + + where a is the number of sentence pairs containing both e; and fj; a+b is the number of sentence pairs containing e;; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither e; nor fj. ∀ z = 997 The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candidate pair are expected to be translated, there shoul</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In Proc. of HLT/EMNLP-05. Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Word to word alignment strategies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international Conference on Computational Linguistics.</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="14280" citStr="Tiedemann (2004)" startWordPosition="2369" endWordPosition="2370">well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best. 4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both e; and fj to have no previous linkages, we only require that at le</context>
</contexts>
<marker>Tiedemann, 2004</marker>
<rawString>J. Tiedemann. 2004. Word to word alignment strategies. In Proceedings of the 20th international Conference on Computational Linguistics. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Wu</author>
<author>J S Chang</author>
</authors>
<title>Learning to Find English to Chinese Transliterations on the Web. In</title>
<date>2007</date>
<booktitle>Proc. of EMNLP-CoNLL-2007.</booktitle>
<pages>996--1004</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17059" citStr="Wu and Chang, 2007" startWordPosition="2850" endWordPosition="2853">lable-level regularities in transliterated terms. Consider again the Shapiro example in the introduction section. There are numerous correct transliterations for the same English word, some of which are not very frequent. For example, the word 夏布洛happens to have a similar φ2 score with Shapiro as the word 流利 (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus. Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not require any additional resources such as pronunciation dictionaries and bilingual dictionaries. In addition to computing the φ2 scores between words, we also compute the φ2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we use</context>
<context position="28949" citStr="Wu and Chang (2007)" startWordPosition="4816" endWordPosition="4819">slation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007), like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learning and does not make a distinction between translations and transliterations. Furth</context>
</contexts>
<marker>Wu, Chang, 2007</marker>
<rawString>J.C. Wu and J.S. Chang. 2007. Learning to Find English to Chinese Transliterations on the Web. In Proc. of EMNLP-CoNLL-2007. pp.996-1004. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Competitive Grouping in Integrated Phrase Segmentation and Alignment Model.</title>
<date>2005</date>
<booktitle>in Proceedings of ACL-05 Workshop on Building and Parallel Text.</booktitle>
<location>Ann Arbor, MI.</location>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Y. Zhang, S. Vogel. 2005 Competitive Grouping in Integrated Phrase Segmentation and Alignment Model. in Proceedings of ACL-05 Workshop on Building and Parallel Text. Ann Arbor, MI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>