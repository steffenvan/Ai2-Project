<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.999108">
A Probabilistic Framework for Answer Selection in Question Answering
</title>
<author confidence="0.999518">
Jeongwoo Ko&apos;, Luo Si2, Eric Nyberg&apos;
</author>
<affiliation confidence="0.9991355">
&apos;Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA 15213
2Department of Computer Science, Purdue University, West Lafayette, IN 47907
</affiliation>
<email confidence="0.970552">
jko@cs.cmu.edu, lsi@cs.purdue.edu, ehn@cs.cmu.edu
</email>
<sectionHeader confidence="0.982584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990764705882">
This paper describes a probabilistic an-
swer selection framework for question an-
swering. In contrast with previous work
using individual resources such as ontolo-
gies and the Web to validate answer can-
didates, our work focuses on developing
a unified framework that not only uses
multiple resources for validating answer
candidates, but also considers evidence of
similarity among answer candidates in or-
der to boost the ranking of the correct an-
swer. This framework has been used to se-
lect answers from candidates generated by
four different answer extraction methods.
An extensive set of empirical results based
on TREC factoid questions demonstrates
the effectiveness of the unified framework.
</bodyText>
<sectionHeader confidence="0.995168" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999229833333333">
Question answering aims at finding exact answers
to a user’s natural language question from a large
collection of documents. Most QA systems com-
bine information retrieval with extraction techniques
to identify a set of likely candidates and then uti-
lize some selection strategy to generate the final
answers (Prager et al., 2000; Clarke et al., 2001;
Harabagiu et al., 2001). Since answer extractors
may be based on imprecise empirical methods, the
selection process can be very challenging, as it often
entails identifying correct answer(s) amongst many
incorrect ones.
</bodyText>
<table confidence="0.931159">
Answer Score Docum
candidates extract
Beijing 0.7 AP880
Hong Kong 0.65 WSJ92
Shanghai 0.64
Taiwan
Shanghai 0.4
</table>
<tableCaption confidence="0.27827">
0. 5
</tableCaption>
<bodyText confidence="0.999543416666667">
Figure 1 shows a traditional QA architecture with
an example question. Given the question “Which
city in China has the largest number offoreign fi-
nancial companies?”, the answer extraction com-
ponent produces a ranked list of five answer can-
didates. Due to imprecision in answer extraction,
an incorrect answer (“Beijing”) was ranked at the
top position. The correct answer (“Shanghai”) was
extracted from two documents with different confi-
dence scores and ranked at the third and the fifth po-
sitions. In order to select “Shanghai” as the final
answer, we need to address two issues:
</bodyText>
<listItem confidence="0.998104">
• Answer Validation. How do we identify correct
answer(s) amongst incorrect ones? Validating
an answer may involve searching for facts in
a knowledge base, e.g. IS-A(Shanghai,
city), IS-IN(Shanghai, China).
• Answer Similarity. How do we exploit evi-
dence of similarity among answer candidates?
</listItem>
<figure confidence="0.9929434375">
Question
Query
Docs
Answer
candidates
Question
Analysis
Document
Retrieval
Answer
Extraction
FBIS3-
FT942-
FBIS3-
Answ
Selecti
</figure>
<figureCaption confidence="0.847918">
Figure 1: A traditional QA pipeline architecture
</figureCaption>
<figure confidence="0.88552">
Corpus
Which city in China has the
largest number of foreign
financial companies?
524
Proceedings of NAACL HLT 2007, pages 524–531,
</figure>
<note confidence="0.517108">
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959864864865">
For example, when there are redundant an-
swers (“Shanghai”, as above) or several an-
swers which represent a single instance (e.g.
“Clinton, Bill” and “William Jefferson Clin-
ton”) in the candidate list, how much should we
boost the answer candidate scores?
To address the first issue, several answer selec-
tion approaches have used semantic resources. One
of the most common approaches relies on Word-
Net, CYC and gazetteers for answer validation or
answer reranking; answer candidates are pruned
or discounted if they are not found within a re-
source’s hierarchy corresponding to the expected an-
swer type (Xu et al., 2003; Moldovan et al., 2003;
Prager et al., 2004). In addition, the Web has been
used for answer reranking by exploiting search en-
gine results produced by queries containing the an-
swer candidate and question keywords (Magnini et
al., 2002), and Wikipedia’s structured information
has been used for answer type checking (Buscaldi
and Rosso, 2006).
To use more than one resource for answer
type checking of location questions, Schlobach
et al. (2004) combined WordNet with geographi-
cal databases. However, in their experiments the
combination actually hurt performance because of
the increased semantic ambiguity that accompanies
broader coverage of location names. This demon-
strates that the method used to combine potential
answers may matter as much as the choice of re-
sources.
To address the second issue we must determine
how to detect and exploit answer similarity. As an-
swer candidates are extracted from different docu-
ments, they may contain identical, similar or com-
plementary text snippets. For example, the United
States may be represented by the strings “U.S.”,
“United States” or “USA” indifferent documents. It
is important to detect this type of similarity and ex-
ploit it to boost answer confidence, especially for list
questions that require a set of unique answers. One
approach is to incorporate answer clustering (Kwok
et al., 2001; Nyberg et al., 2003; Jijkoun et al.,
2006). For example, we might merge “April 1912”
and “14 Apr 1912” into a cluster and then choose
one answer as the cluster head. However, clustering
raises new issues: how to choose the cluster head
and how to calculate the scores of the clustered an-
swers.
Although many QA systems individually address
these issues in answer selection, there has been lit-
tle research on generating a generalized probabilistic
framework that allows any validation and similarity
features to be easily incorporated.
In this paper we describe a probabilistic answer
selection framework to address the two issues. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer validation features and answer sim-
ilarity features. Experimental results on TREC
factoid questions (Voorhees, 2004) show that our
framework significantly improved answer selection
performance for four different extraction techniques,
when compared to default selection using the indi-
vidual candidate scores produced by each extractor.
This paper is organized as follows: Section 2 de-
scribes our answer selection framework and Section
3 lists the features that generate similarity and va-
lidity scores for factoid questions. In Section 4, we
describe the experimental methodology and the re-
sults. Section 5 describes how we intend to extend
our framework to handle complex questions. Finally
Section 6 concludes with suggestions for future re-
search.
</bodyText>
<sectionHeader confidence="0.920298" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.981274388888889">
Answer validation is based on an estimate of the
probability P(correct(AZ)JAZ,Q), where Q is a
question and AZ is an answer candidate to the ques-
tion. Answer similarity is is based on an estimate
of the probability P(correct(AZ)|AZ, Aj), where Aj
is similar to AZ. Since both probabilities influ-
ence answer selection performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P(correct(AZ)IQ, Al,..., An).
In this paper, we propose a proba-
bilistic framework that directly estimates
P(correct(AZ)IQ, Ai, ..., An) using multiple
answer validation features and answer similarity
features. The framework was implemented with
logistic regression, which is a statistical machine
learning technique used to predict the probability
of a binary variable from input variables. Logistic
</bodyText>
<equation confidence="0.980986125">
525
P(correct(Ai)|Q, A1,..., An) (1)
� P(correct(Ai)|val1(Ai),...,valK1(Ai),sim1(Ai),...,simK2(Ai))
exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai))
k=1 k=1
=
1 + exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai))
k=1 k=1
</equation>
<bodyText confidence="0.976981363636364">
where, simk(Ai) = N simk(Ai, Aj).
7
j=1(j 0
α, A A = argmax R Nj logP(correct(Ai)|val1(Ai),..., valK1(Ai), sim1(Ai),..., simK2(Ai)) (2)
~α,~β,~λ j=1 i=1
regression has been successfully employed in many
applications including multilingual document merg-
ing (Si and Callan, 2005). In our previous work (Ko
et al., 2006), we showed that logistic regression
performed well in merging three resources to vali-
date answers to location and proper name questions.
We extended this approach to combine multiple
similarity features with multiple answer validation
features. The extended framework estimates the
probability that an answer candidate is correct given
the degree of answer correctness and the amount
of supporting evidence provided in a set of answer
candidates (Equation 1).
In Equation 1, each valk(Ai) is a feature function
used to produce an answer validity score for an an-
swer candidate Ai. Each simk(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj. K1 and K2 are the number of
answer validation and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric. For example, if Levenshtein distance is used
as one similarity metric, simk(Ai) is calculated by
summing N-1 Levenshtein distances between one
answer candidate and all other candidates. As some
string similarity metrics (e.g. Levenshtein distance)
produce a number between 0 and 1 (where 1 means
two strings are identical and 0 means they are differ-
ent), similarity scores less than some threshold value
are ignored.
The parameters α, Q, A were estimated from train-
ing data by maximizing the log likelihood as shown
in Equation 2, where R is the number of training
questions and Nj is the number of answer candidates
for each question Qj. For parameter estimation, we
used the Quasi-Newton algorithm (Minka, 2003).
To select correct answers, the initial answer candi-
date set is reranked according to the estimated prob-
ability of each candidate. For factoid questions, the
top answer is selected as the final answer to the ques-
tion. As logistic regression can be used for binary
classification with a default threshold of 0.5, we can
also use the framework to classify incorrect answers:
if the probability of an answer candidate is lower
than 0.5, it is considered to be a wrong answer and
is filtered out of the answer list. This is useful in
deciding whether or not a valid answer exists in the
corpus, an important aspect of the TREC QA evalu-
ation (Voorhees, 2004).
</bodyText>
<sectionHeader confidence="0.984299" genericHeader="method">
3 Feature Representation
</sectionHeader>
<bodyText confidence="0.999841">
This section details the features used to generate an-
swer validity scores and answer similarity scores for
our answer selection framework.
</bodyText>
<page confidence="0.602885">
526
</page>
<subsectionHeader confidence="0.996351">
3.1 Answer Validation Features
</subsectionHeader>
<bodyText confidence="0.999972125">
Each answer validation feature produces a validity
score which predicts whether or not an answer can-
didate is a correct answer for the question. This task
can be done by exploiting external QA resources
such as the Web, databases, and ontologies. For fac-
toid questions, we used gazetteers and WordNet in a
knowledge-based approach; we also used Wikipedia
and Google in a data-driven approach.
</bodyText>
<subsubsectionHeader confidence="0.405359">
3.1.1 Knowledge-based Features
</subsubsectionHeader>
<bodyText confidence="0.937293684210526">
In order to generate answer validity scores using
gazetteers and WordNet, we reused the algorithms
described in our previous work (Ko et al., 2006).
Gazetteers: Gazetteers provide geographic
information, which allows us to identify
strings as instances of countries, their cities,
continents, capitals, etc. For answer selec-
tion, we used three gazetteer resources: the
Tipster Gazetteer, the CIA World Factbook
(https://www.cia.gov/cia/publications/factbook/inde
x.html) and information about the US states pro-
vided by 50states.com (http://www.50states.com).
These resources were used to assign an answer
validity score between -1 and 1 to each candidate
(Figure 2). A score of 0 means the gazetteers did
not contribute to the answer selection process for
that candidate. For some numeric questions, range
checking was added to validate numeric questions
similarly to Prager et al. (2004). For example, given
the question “How many people live in Chile?”,
if an answer candidate is within f 10% of the
population stated in the CIA World Factbook, it
receives a score of 1.0. If it is in the range of 20%,
its score is 0.5. If it significantly differs by more
than 20%, it receives a score of -1.0. The threshold
may vary based on when the document was written
and when the census was taken1.
WordNet: The WordNet lexical database includes
English words organized in synonym sets, called
synsets (Fellbaum, 1998). We used WordNet in or-
der to produce an answer validity score between -1
and 1, following the algorithm in Figure 3. A score
1The ranges used here were found to work effectively, but
were not explicitly validated or tuned.
1) If the answer candidate directly matches the gazetteer
answer for the question, its gazetteer score is 1.0. (e.g.
Given the question “What continent is Togo on?”, the
candidate “Africa” receives a score of 1.0.)
</bodyText>
<listItem confidence="0.890522482758621">
2) If the answer candidate occurs in the gazetteer within
the subcategory of the expected answer type, its score
is 0.5. (e.g., Given the question “Which city in China
has the largest number of foreign financial
companies?”, the candidates “Shanghai” and “Boston”
receive a score of 0.5 because they are both cities.)
3) If the answer candidate is not the correct semantic
type, its score is -1. (e.g., Given the question “Which
city in China has the largest number of foreign
financial companies?”, the candidate “Taiwan”
receives a score of -1 because it is not a city.)
4) Otherwise, the score is 0.0.
Figure 2: Validity scoring with gazetteers.
1) If the answer candidate directly matches WordNet, its
WordNet score is 1.0. (e.g. Given the question “What is
the capital of Uruguay?”, the candidate “Montevideo”
receives a score of 1.0.)
2) If the answer candidate’s hypernyms include a
subcategory of the expected answer type, its score is
0.5. (e.g., Given the question “Who wrote the book
‘Song of Solomon’?&amp;quot;, the candidate “Mark Twain”
receives a score of 0.5 because its hypernyms include
“writer”.)
3) If the answer candidate is not the correct semantic
type, this candidate receives a score of -1. (e.g., Given
the question “What state is Niagara Falls located in?”,
the candidate “Toronto” gets a score of -1 because it is
not a state.)
4) Otherwise, the score is 0.0.
</listItem>
<figureCaption confidence="0.991413">
Figure 3: Validity scoring with WordNet.
</figureCaption>
<bodyText confidence="0.9963605">
of 0 means that WordNet does not contribute to the
answer selection process for a candidate.
</bodyText>
<subsubsectionHeader confidence="0.576706">
3.1.2 Data-driven Features
</subsubsectionHeader>
<bodyText confidence="0.999964777777778">
Wikipedia and Google were used in a data-driven
approach to generate answer validity scores.
Wikipedia: Wikipedia (http://www.wikipedia.org)
is a multilingual free on-line encyclopedia. Fig-
ure 4 shows the algorithm used to generate an
answer validity score from Wikipedia. If there
is a Wikipedia document whose title matches an
answer candidate, the document is analyzed to
obtain the term frequency (tf) and the inverse term
</bodyText>
<page confidence="0.601322">
527
</page>
<figureCaption confidence="0.999972">
Figure 4: Validity scoring with Wikipedia
Figure 5: Validity scoring with Google
</figureCaption>
<bodyText confidence="0.999485866666667">
frequency (idf) of the candidate, from which a
tf.idf score is calculated. When there is no matched
document, each question keyword is also processed
as a back-off strategy, and the answer validity score
is calculated by summing the tf.idf scores. To
calculate word frequency, the TREC Web Corpus
(http://ir.dcs.gla.ac.uk/test collections/wt10g.html)
was used as a large background corpus.
Google: Following Magnini et al. (2002), we used
Google to generate a numeric score. A query con-
sisting of an answer candidate and question key-
words was sent to the Google search engine. To
calculate a score, the top 10 text snippets returned
by Google were then analyzed using the algorithm
in Figure 5.
</bodyText>
<subsectionHeader confidence="0.999235">
3.2 Answer Similarity Features
</subsectionHeader>
<bodyText confidence="0.999380666666667">
We calculate the similarity between two answer can-
didates using multiple string distance metrics and a
list of synonyms.
</bodyText>
<subsectionHeader confidence="0.94032">
3.2.1 String Distance Metrics
</subsectionHeader>
<bodyText confidence="0.99980125">
There are several different string distance metrics
to calculate the similarity of short strings. We used
five popular string distance metrics: Levenshtein,
Jaccard, Jaro, Jaro-Winkler, and Cosine similarity.
</bodyText>
<subsectionHeader confidence="0.72046">
3.2.2 Synonyms
</subsectionHeader>
<bodyText confidence="0.995271">
Synonyms can be used as another metric to calcu-
late answer similarity. We defined a binary similar-
ity score for synonyms.
</bodyText>
<equation confidence="0.975305666666667">
� 1, if AZ is a synonym of Aj
sim(AZ,Aj) �
0, otherwise
</equation>
<bodyText confidence="0.9989678">
To get a list of synonyms, we used three knowl-
edge bases: WordNet, Wikipedia and the CIA World
Factbook. WordNet includes synonyms for English
words. Wikipedia redirection is used to obtain an-
other set of synonyms. For example, “Calif.” is redi-
rected to “California” in Wikipedia, and “William
Jefferson Clinton” is redirected to “Bill Clinton”.
The CIA World Factbook includes five different
names for a country: conventional long form, con-
ventional short form, local long form, local short
form and former name. For example, the conven-
tional long form of Egypt is “Arab Republic of
Egypt”, the conventional short form is “Egypt”, the
local short form is “Misr”, the local long form is
“Jumhuriyat Misr al-Arabiyah” and the former name
is “United Arab Republic (with Syria)”. All are con-
sidered to be synonyms of “Egypt”.
In addition, manually generated rules are used to
obtain synonyms for different types of answer can-
didates (Nyberg et al., 2003):
</bodyText>
<listItem confidence="0.99221925">
• Dates are converted into the ISO 8601 date for-
mat (YYYY-MM-DD) (e.g., “April 12 1914”
and “12th Apr. 1914” are converted into “1914-
04-12” and considered as synonyms).
• Temporal expressions are converted into the
HH:MM:SS format (e.g., “six thirty five p.m.”
and “6:35 pm” are converted into “18:35:xx”
and considered as synonyms).
• Numeric expression are converted into sci-
entific notation (e.g, “one million” and
“1,000,000” are converted into “1e+06” and
considered as synonyms).
</listItem>
<figure confidence="0.983919533333333">
For each answer candidate Ai,
1. Initialize the Google score: gs(Ai) = 0
2. For each snippet s:
2.1. Initialize the snippet co-occurrence score
2.2. For each question keyword k in s:
2.2.1 Compute distance d, the minimum nu
words between k and the answer candidate
2.2.2 Update the snippet
cs(s) = cs(s) × co-occurrence scot
d)
2.3. gs(Ai) = gs(Ai) + cs(s)
3. Normalize the Google score (dividing it by a
2
(1+
528
</figure>
<listItem confidence="0.9680596">
• Representative entities are converted into the
represented entity when the expected answer
type is COUNTRY (e.g., “the Egyptian govern-
ment” is changed to “Egypt” and “Clinton ad-
ministration” is changed to “U.S.”).
</listItem>
<sectionHeader confidence="0.981915" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999976">
This section describes the experiments we used
to evaluate our answer selection framework. The
JAVELIN QA system (Nyberg et al., 2006) was used
as a testbed for the evaluation.
</bodyText>
<subsectionHeader confidence="0.993944">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999907827586207">
A total of 1760 factoid questions from the TREC8-
12 QA evaluations served as a dataset, with 5-fold
cross validation.
To better understand how the performance of our
framework varies for different extraction techniques,
we tested it with four JAVELIN answer extraction
modules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-
berg et al., 2006). FST is an answer extractor based
on finite state transducers that incorporate a set of
extraction patterns (both manually-created and gen-
eralized patterns). LIGHTv1 is an extractor that se-
lects answer candidates using a non-linear distance
heuristic between the keywords and an answer can-
didate. LIGHTv2 is another extractor based on a
different distance heuristic, originally developed as
part of a multilingual QA system. SVM is an extrac-
tor that uses Support Vector Machines to discrimi-
nate between correct and incorrect answers.
Answer selection performance was measured by
average accuracy: the number of correct top answers
divided by the number of questions where at least
one correct answer exists in the candidate list pro-
vided by an extractor. The baseline was calculated
with the answer candidate scores provided by each
individual extractor; the answer with the best extrac-
tor score was chosen, and no validation or similarity
processing was performed. For Wikipedia, we used
a version downloaded in Nov. 2005, which con-
tained 1,811,554 articles.
</bodyText>
<subsectionHeader confidence="0.933739">
4.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.971885421052632">
We first analyzed the average accuracy when us-
ing individual validation features. Figure 6 shows
the effect of the individual answer validation fea-
tures on different extraction outputs. The combina-
Figure 6: Average accuracy of individual answer
validation features (GZ: gazetteers, WN: WordNet,
WIKI: Wikipedia, GL: Google, ALL: combination
of all features).
tion of all features significantly improved the per-
formance when compared to answer selection using
a single feature. Comparing the data-driven features
with the knowledge-based features, the data-driven
features (such as Wikipedia and Google) increased
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet); our intuition
is that the knowledge-based features covered fewer
questions. The biggest improvement was found with
candidates produced by the SVM extractor: a 242%
improvement over the baseline. It was mostly be-
cause SVM tended to produce several answer can-
didates with the same or very similar confidence
scores, but our framework could select the correct
answer among many incorrect ones by exploiting
answer validation features.
Table 1 shows the effect of individual similarity
features on different extractors when using 0.3 and
0.5 as a similarity threshold, respectively. When
comparing five different string similarity features
(Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-
sine similarity), Levenshtein and Jaccard tended to
perform better than the others. When comparing
synonym features with string similarity features,
synonyms performed slightly better.
We also analyzed answer selection performance
when combining all six similarity features (“All” in
Table 1). Combining all similarity features did not
improve the performance except for the FST extrac-
tor, because including five string similarity features
</bodyText>
<page confidence="0.580242">
529
</page>
<table confidence="0.999930916666667">
Similarity FST LIGHTv1 LIGHTv2 SVM
feature
0.3 0.5 0.3 0.5 0.3 0.5 0.3 0.5
Levenshtein 0.728 0.728 0.471 0.455 0.399 0.400 0.381 0.383
Jaro 0.708 0.705 0.422 0.440 0.373 0.378 0.274 0.282
Jaro-Winkler 0.701 0.705 0.426 0.442 0.374 0.379 0.277 0.275
Jaccard 0.738 0.738 0.438 0.448 0.452 0.448 0.382 0.390
Cosine 0.738 0.738 0.436 0.435 0.418 0.422 0.380 0.378
Synonyms 0.745 0.745 0.458 0.458 0.442 0.442 0.412 0.412
Lev+Syn 0.748 0.751 0.460 0.466 0.445 0.448 0.420 0.412
Jac+Syn 0.742 0.742 0.456 0.465 0.440 0.445 0.396 0.396
All 0.755 0.755 0.405 0.425 0.435 0.431 0.303 0.302
</table>
<tableCaption confidence="0.986007333333333">
Table 1: Average accuracy using individual similarity features under different thresholds: 0.3 and 0.5
(“Lev+Syn”: the combination of Levenshtein with synonyms, “Jac+Syn”: the combination of Jaccard and
synonyms, “All”: the combination of all similarity metrics)
</tableCaption>
<table confidence="0.9984922">
Baseline Sim Val All
FST 0.658 0.751 0.855 0.877
LIGHTv1 0.394 0.466 0.612 0.628
LIGHTv2 0.343 0.448 0.578 0.582
SVM 0.169 0.420 0.578 0.586
</table>
<tableCaption confidence="0.988125">
Table 2: Average accuracy of individual features
</tableCaption>
<bodyText confidence="0.99477575">
(Sim: merging similarity features, Val: merging val-
idation features, ALL: combination of all features).
provided too much redundancy to the logistic regres-
sion. We also compared the combination of Leven-
shtein with synonyms and the combination of Jac-
card with synonyms, and then chose Levenshtein
and synonyms as the two best similarity features in
our framework.
We also analyzed the degree to which the average
accuracy was affected by answer similarity and val-
idation features. Table 2 compares the average ac-
curacy using the baseline, the answer similarity fea-
tures, the answer validation features and all feature
combinations. As can be seen, the similarity fea-
tures significantly improved performance, so we can
conclude that exploiting answer similarity improves
answer selection performance. The validation fea-
tures also significantly improved the performance.
When combining both sets of features together,
the answer selection performance increased for all
four extractors: an average of 102% over the base-
line, 30% over the similarity features and 1.82%
over the validation features. Adding the similarity
features to the validation features generated small
but consistent improvement in all configurations.
We expect more performance gain from similar-
ity features when merging similar answers returned
from all four extractors.
</bodyText>
<sectionHeader confidence="0.999062" genericHeader="method">
5 Extensions for Complex Questions
</sectionHeader>
<bodyText confidence="0.999936263157895">
Although we conducted our experiments on fac-
toid questions, our framework can be easily ex-
tended to handle complex questions, which require
longer answers representing facts or relations (e.g.,
“What is the relationship between Alan Greenspan
and Robert Rubin?”). As answer candidates are
long text snippets, different features should be used
for answer selection. Possible validation features
include question keyword inclusion and predicate
structure match (Nyberg et al., 2005). For exam-
ple, given the question “Did Egypt sell Scud mis-
siles to Syria?”, the key predicate from the ques-
tion is Sell(Egypt, Syria, Scud missile). If there is
a sentence which contains the predicate structure
Buy(Syria, Scud missile, Egypt), we can calculate
the predicate structure distance and use it as a val-
idation feature. For answer similarity, we intend to
explore novelty detection approaches evaluated in
Allan et al. (2003).
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999480333333333">
In this paper, we described our answer selection
framework for estimating the probability that an an-
swer candidate is correct given multiple answer vali-
</bodyText>
<page confidence="0.484161">
530
</page>
<bodyText confidence="0.999975444444444">
dation and similarity features. We conducted a series
of experiments to evaluate the performance of the
framework and analyzed the effect of individual val-
idation and similarity features. Empirical results on
TREC questions show that our framework improved
answer selection performance in the JAVELIN QA
system by an average of 102% over the baseline,
30% over the similarity features alone and 1.82%
over the validation features alone.
We plan to improve our framework by adding reg-
ularization and selecting the final answers among
candidates returned from all extractors. As our
current framework is based on the assumption that
each answer is independent, we are building another
probabilistic framework which does not require any
independence assumption, and uses an undirected
graphical model to estimate the joint probability of
all answer candidates.
</bodyText>
<sectionHeader confidence="0.998595" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99704825">
This work was supported in part by ARDA/DTO
Advanced Question Answering for Intelli-
gence (AQUAINT) program award number
NBCHC040164.
</bodyText>
<sectionHeader confidence="0.995756" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999746971014493">
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceedings
of SIGIR.
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Grju, V. Rus, and
P. Morarescu. 2001. FALCON: Boosting knowledge
for answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Hiyakumoto, and E. Nyberg. 2006. Exploit-
ing semantic resources for answer selection. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proceedings of
WWW10 Conference.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings ofHLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2003. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of the Text REtrieval Conference.
E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro,
and A. Schlaikjer. 2006. JAVELIN I and II Systems at
TREC 2005. In Proceedings of TREC.
E. Nyberg, T. Mitamura, R. Frederking, V. Pedro,
M. Bilotti, A. Schlaikjer, and K. Hannan. 2005. Ex-
tending the javelin qa system with domain semantics.
In Proceedings ofAAAI-05 Workshop on Question An-
swering in Restricted Domains.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2004 IBM’s Piquant in
Trec2003. In Proceedings of TREC.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type checking in open-domain question answering. In
Proceedings of European Conference on Artificial In-
telligence.
L. Si and J. Callan. 2005 CLEF2005: Multilingual
retrieval by combining multiple multilingual ranked
lists. In Proceedings of Cross-Language Evaluation
Forum.
E. Voorhees. 2004. Overview of the TREC 2003 ques-
tion answering track. In Proceedings of TREC.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2003. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of TREC.
</reference>
<page confidence="0.917296">
531
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488211">
<title confidence="0.999823">A Probabilistic Framework for Answer Selection in Question Answering</title>
<author confidence="0.975867">Luo Eric</author>
<affiliation confidence="0.69713">Technologies Institute, Carnegie Mellon, Pittsburgh, PA</affiliation>
<address confidence="0.685895">of Computer Science, Purdue University, West Lafayette, IN</address>
<email confidence="0.999594">jko@cs.cmu.edu,lsi@cs.purdue.edu,ehn@cs.cmu.edu</email>
<abstract confidence="0.993781444444444">This paper describes a probabilistic answer selection framework for question answering. In contrast with previous work using individual resources such as ontologies and the Web to validate answer candidates, our work focuses on developing a unified framework that not only uses multiple resources for validating answer candidates, but also considers evidence of similarity among answer candidates in order to boost the ranking of the correct answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>C Wade</author>
<author>A Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="24755" citStr="Allan et al. (2003)" startWordPosition="3857" endWordPosition="3860">xt snippets, different features should be used for answer selection. Possible validation features include question keyword inclusion and predicate structure match (Nyberg et al., 2005). For example, given the question “Did Egypt sell Scud missiles to Syria?”, the key predicate from the question is Sell(Egypt, Syria, Scud missile). If there is a sentence which contains the predicate structure Buy(Syria, Scud missile, Egypt), we can calculate the predicate structure distance and use it as a validation feature. For answer similarity, we intend to explore novelty detection approaches evaluated in Allan et al. (2003). 6 Conclusion In this paper, we described our answer selection framework for estimating the probability that an answer candidate is correct given multiple answer vali530 dation and similarity features. We conducted a series of experiments to evaluate the performance of the framework and analyzed the effect of individual validation and similarity features. Empirical results on TREC questions show that our framework improved answer selection performance in the JAVELIN QA system by an average of 102% over the baseline, 30% over the similarity features alone and 1.82% over the validation features</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and novelty detection at the sentence level. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Buscaldi</author>
<author>P Rosso</author>
</authors>
<title>Mining Knowledge from Wikipedia for the Question Answering task.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="3928" citStr="Buscaldi and Rosso, 2006" startWordPosition="596" endWordPosition="599">ntic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of location names. This demonstrates that the method used to combine potential answers may matter as much as the choice of resources. To address the second issue we must determine how to detect and exploit answer similarity. As answer candidates are extracted from different documents, they may conta</context>
</contexts>
<marker>Buscaldi, Rosso, 2006</marker>
<rawString>D. Buscaldi and P. Rosso. 2006. Mining Knowledge from Wikipedia for the Question Answering task. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>T Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1372" citStr="Clarke et al., 2001" startWordPosition="199" endWordPosition="202">t answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework. 1 Introduction Question answering aims at finding exact answers to a user’s natural language question from a large collection of documents. Most QA systems combine information retrieval with extraction techniques to identify a set of likely candidates and then utilize some selection strategy to generate the final answers (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2001). Since answer extractors may be based on imprecise empirical methods, the selection process can be very challenging, as it often entails identifying correct answer(s) amongst many incorrect ones. Answer Score Docum candidates extract Beijing 0.7 AP880 Hong Kong 0.65 WSJ92 Shanghai 0.64 Taiwan Shanghai 0.4 0. 5 Figure 1 shows a traditional QA architecture with an example question. Given the question “Which city in China has the largest number offoreign financial companies?”, the answer extraction component produces a ranked list of five answer candidates. Due to imprec</context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12134" citStr="Fellbaum, 1998" startWordPosition="1885" endWordPosition="1886">ns, range checking was added to validate numeric questions similarly to Prager et al. (2004). For example, given the question “How many people live in Chile?”, if an answer candidate is within f 10% of the population stated in the CIA World Factbook, it receives a score of 1.0. If it is in the range of 20%, its score is 0.5. If it significantly differs by more than 20%, it receives a score of -1.0. The threshold may vary based on when the document was written and when the census was taken1. WordNet: The WordNet lexical database includes English words organized in synonym sets, called synsets (Fellbaum, 1998). We used WordNet in order to produce an answer validity score between -1 and 1, following the algorithm in Figure 3. A score 1The ranges used here were found to work effectively, but were not explicitly validated or tuned. 1) If the answer candidate directly matches the gazetteer answer for the question, its gazetteer score is 1.0. (e.g. Given the question “What continent is Togo on?”, the candidate “Africa” receives a score of 1.0.) 2) If the answer candidate occurs in the gazetteer within the subcategory of the expected answer type, its score is 0.5. (e.g., Given the question “Which city in</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunescu</author>
<author>R Grju</author>
<author>V Rus</author>
<author>P Morarescu</author>
</authors>
<title>FALCON: Boosting knowledge for answer engines.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="1397" citStr="Harabagiu et al., 2001" startWordPosition="203" endWordPosition="206">ork has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework. 1 Introduction Question answering aims at finding exact answers to a user’s natural language question from a large collection of documents. Most QA systems combine information retrieval with extraction techniques to identify a set of likely candidates and then utilize some selection strategy to generate the final answers (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2001). Since answer extractors may be based on imprecise empirical methods, the selection process can be very challenging, as it often entails identifying correct answer(s) amongst many incorrect ones. Answer Score Docum candidates extract Beijing 0.7 AP880 Hong Kong 0.65 WSJ92 Shanghai 0.64 Taiwan Shanghai 0.4 0. 5 Figure 1 shows a traditional QA architecture with an example question. Given the question “Which city in China has the largest number offoreign financial companies?”, the answer extraction component produces a ranked list of five answer candidates. Due to imprecision in answer extractio</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Grju, Rus, Morarescu, 2001</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Grju, V. Rus, and P. Morarescu. 2001. FALCON: Boosting knowledge for answer engines. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>J van Rantwijk</author>
<author>D Ahn</author>
<author>E Tjong Kim Sang</author>
<author>M de Rijke</author>
</authors>
<date>2006</date>
<journal>The University of Amsterdam at CLEF@QA</journal>
<booktitle>In Working Notes CLEF.</booktitle>
<marker>Jijkoun, van Rantwijk, Ahn, Sang, de Rijke, 2006</marker>
<rawString>V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang, and M. de Rijke. 2006. The University of Amsterdam at CLEF@QA 2006. In Working Notes CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ko</author>
<author>L Hiyakumoto</author>
<author>E Nyberg</author>
</authors>
<title>Exploiting semantic resources for answer selection.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="7810" citStr="Ko et al., 2006" startWordPosition="1191" endWordPosition="1194">echnique used to predict the probability of a binary variable from input variables. Logistic 525 P(correct(Ai)|Q, A1,..., An) (1) � P(correct(Ai)|val1(Ai),...,valK1(Ai),sim1(Ai),...,simK2(Ai)) exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai)) k=1 k=1 = 1 + exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai)) k=1 k=1 where, simk(Ai) = N simk(Ai, Aj). 7 j=1(j 0 α, A A = argmax R Nj logP(correct(Ai)|val1(Ai),..., valK1(Ai), sim1(Ai),..., simK2(Ai)) (2) ~α,~β,~λ j=1 i=1 regression has been successfully employed in many applications including multilingual document merging (Si and Callan, 2005). In our previous work (Ko et al., 2006), we showed that logistic regression performed well in merging three resources to validate answers to location and proper name questions. We extended this approach to combine multiple similarity features with multiple answer validation features. The extended framework estimates the probability that an answer candidate is correct given the degree of answer correctness and the amount of supporting evidence provided in a set of answer candidates (Equation 1). In Equation 1, each valk(Ai) is a feature function used to produce an answer validity score for an answer candidate Ai. Each simk(Ai, Aj) i</context>
<context position="10870" citStr="Ko et al., 2006" startWordPosition="1686" endWordPosition="1689">ramework. 526 3.1 Answer Validation Features Each answer validation feature produces a validity score which predicts whether or not an answer candidate is a correct answer for the question. This task can be done by exploiting external QA resources such as the Web, databases, and ontologies. For factoid questions, we used gazetteers and WordNet in a knowledge-based approach; we also used Wikipedia and Google in a data-driven approach. 3.1.1 Knowledge-based Features In order to generate answer validity scores using gazetteers and WordNet, we reused the algorithms described in our previous work (Ko et al., 2006). Gazetteers: Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc. For answer selection, we used three gazetteer resources: the Tipster Gazetteer, the CIA World Factbook (https://www.cia.gov/cia/publications/factbook/inde x.html) and information about the US states provided by 50states.com (http://www.50states.com). These resources were used to assign an answer validity score between -1 and 1 to each candidate (Figure 2). A score of 0 means the gazetteers did not contribute to the answer selection pro</context>
</contexts>
<marker>Ko, Hiyakumoto, Nyberg, 2006</marker>
<rawString>J. Ko, L. Hiyakumoto, and E. Nyberg. 2006. Exploiting semantic resources for answer selection. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the web.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW10 Conference.</booktitle>
<contexts>
<context position="4931" citStr="Kwok et al., 2001" startWordPosition="754" endWordPosition="757"> may matter as much as the choice of resources. To address the second issue we must determine how to detect and exploit answer similarity. As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets. For example, the United States may be represented by the strings “U.S.”, “United States” or “USA” indifferent documents. It is important to detect this type of similarity and exploit it to boost answer confidence, especially for list questions that require a set of unique answers. One approach is to incorporate answer clustering (Kwok et al., 2001; Nyberg et al., 2003; Jijkoun et al., 2006). For example, we might merge “April 1912” and “14 Apr 1912” into a cluster and then choose one answer as the cluster head. However, clustering raises new issues: how to choose the cluster head and how to calculate the scores of the clustered answers. Although many QA systems individually address these issues in answer selection, there has been little research on generating a generalized probabilistic framework that allows any validation and similarity features to be easily incorporated. In this paper we describe a probabilistic answer selection fram</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling question answering to the web. In Proceedings of WWW10 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Pervete</author>
<author>H Tanev</author>
</authors>
<title>Comparing statistical and content-based techniques for answer validation on the web.</title>
<date>2002</date>
<booktitle>In Proceedings of the VIII Convegno AI*IA.</booktitle>
<contexts>
<context position="3822" citStr="Magnini et al., 2002" startWordPosition="581" endWordPosition="584">nswer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of location names. This demonstrates that the method used to combine potential answers may matter as much as the choice of resources. To address the second issue we must determine how to detect </context>
<context position="15055" citStr="Magnini et al. (2002)" startWordPosition="2355" endWordPosition="2358">tches an answer candidate, the document is analyzed to obtain the term frequency (tf) and the inverse term 527 Figure 4: Validity scoring with Wikipedia Figure 5: Validity scoring with Google frequency (idf) of the candidate, from which a tf.idf score is calculated. When there is no matched document, each question keyword is also processed as a back-off strategy, and the answer validity score is calculated by summing the tf.idf scores. To calculate word frequency, the TREC Web Corpus (http://ir.dcs.gla.ac.uk/test collections/wt10g.html) was used as a large background corpus. Google: Following Magnini et al. (2002), we used Google to generate a numeric score. A query consisting of an answer candidate and question keywords was sent to the Google search engine. To calculate a score, the top 10 text snippets returned by Google were then analyzed using the algorithm in Figure 5. 3.2 Answer Similarity Features We calculate the similarity between two answer candidates using multiple string distance metrics and a list of synonyms. 3.2.1 String Distance Metrics There are several different string distance metrics to calculate the similarity of short strings. We used five popular string distance metrics: Levensht</context>
</contexts>
<marker>Magnini, Negri, Pervete, Tanev, 2002</marker>
<rawString>B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002. Comparing statistical and content-based techniques for answer validation on the web. In Proceedings of the VIII Convegno AI*IA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Minka</author>
</authors>
<title>A Comparison of Numerical Optimizers for Logistic Regression. Unpublished draft.</title>
<date>2003</date>
<contexts>
<context position="9444" citStr="Minka, 2003" startWordPosition="1453" endWordPosition="1454">ing N-1 Levenshtein distances between one answer candidate and all other candidates. As some string similarity metrics (e.g. Levenshtein distance) produce a number between 0 and 1 (where 1 means two strings are identical and 0 means they are different), similarity scores less than some threshold value are ignored. The parameters α, Q, A were estimated from training data by maximizing the log likelihood as shown in Equation 2, where R is the number of training questions and Nj is the number of answer candidates for each question Qj. For parameter estimation, we used the Quasi-Newton algorithm (Minka, 2003). To select correct answers, the initial answer candidate set is reranked according to the estimated probability of each candidate. For factoid questions, the top answer is selected as the final answer to the question. As logistic regression can be used for binary classification with a default threshold of 0.5, we can also use the framework to classify incorrect answers: if the probability of an answer candidate is lower than 0.5, it is considered to be a wrong answer and is filtered out of the answer list. This is useful in deciding whether or not a valid answer exists in the corpus, an impor</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>T. Minka. 2003. A Comparison of Numerical Optimizers for Logistic Regression. Unpublished draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>D Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="3610" citStr="Moldovan et al., 2003" startWordPosition="546" endWordPosition="549">there are redundant answers (“Shanghai”, as above) or several answers which represent a single instance (e.g. “Clinton, Bill” and “William Jefferson Clinton”) in the candidate list, how much should we boost the answer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>J Carbonell</author>
<author>J Callan</author>
<author>K Collins-Thompson</author>
<author>K Czuba</author>
<author>M Duggan</author>
<author>L Hiyakumoto</author>
<author>N Hu</author>
<author>Y Huang</author>
<author>J Ko</author>
<author>L Lita</author>
<author>S Murtagh</author>
<author>V Pedro</author>
<author>D Svoboda</author>
</authors>
<date>2003</date>
<booktitle>The JAVELIN Question-Answering System at TREC</booktitle>
<contexts>
<context position="4952" citStr="Nyberg et al., 2003" startWordPosition="758" endWordPosition="761"> as the choice of resources. To address the second issue we must determine how to detect and exploit answer similarity. As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets. For example, the United States may be represented by the strings “U.S.”, “United States” or “USA” indifferent documents. It is important to detect this type of similarity and exploit it to boost answer confidence, especially for list questions that require a set of unique answers. One approach is to incorporate answer clustering (Kwok et al., 2001; Nyberg et al., 2003; Jijkoun et al., 2006). For example, we might merge “April 1912” and “14 Apr 1912” into a cluster and then choose one answer as the cluster head. However, clustering raises new issues: how to choose the cluster head and how to calculate the scores of the clustered answers. Although many QA systems individually address these issues in answer selection, there has been little research on generating a generalized probabilistic framework that allows any validation and similarity features to be easily incorporated. In this paper we describe a probabilistic answer selection framework to address the </context>
<context position="16856" citStr="Nyberg et al., 2003" startWordPosition="2650" endWordPosition="2653">Bill Clinton”. The CIA World Factbook includes five different names for a country: conventional long form, conventional short form, local long form, local short form and former name. For example, the conventional long form of Egypt is “Arab Republic of Egypt”, the conventional short form is “Egypt”, the local short form is “Misr”, the local long form is “Jumhuriyat Misr al-Arabiyah” and the former name is “United Arab Republic (with Syria)”. All are considered to be synonyms of “Egypt”. In addition, manually generated rules are used to obtain synonyms for different types of answer candidates (Nyberg et al., 2003): • Dates are converted into the ISO 8601 date format (YYYY-MM-DD) (e.g., “April 12 1914” and “12th Apr. 1914” are converted into “1914- 04-12” and considered as synonyms). • Temporal expressions are converted into the HH:MM:SS format (e.g., “six thirty five p.m.” and “6:35 pm” are converted into “18:35:xx” and considered as synonyms). • Numeric expression are converted into scientific notation (e.g, “one million” and “1,000,000” are converted into “1e+06” and considered as synonyms). For each answer candidate Ai, 1. Initialize the Google score: gs(Ai) = 0 2. For each snippet s: 2.1. Initializ</context>
</contexts>
<marker>Nyberg, Mitamura, Carbonell, Callan, Collins-Thompson, Czuba, Duggan, Hiyakumoto, Hu, Huang, Ko, Lita, Murtagh, Pedro, Svoboda, 2003</marker>
<rawString>E. Nyberg, T. Mitamura, J. Carbonell, J. Callan, K. Collins-Thompson, K. Czuba, M. Duggan, L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita, S. Murtagh, V. Pedro, and D. Svoboda. 2003. The JAVELIN Question-Answering System at TREC 2002. In Proceedings of the Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>R Frederking</author>
<author>M Bilotti</author>
<author>K Hannan</author>
<author>L Hiyakumoto</author>
<author>J Ko</author>
<author>F Lin</author>
<author>V Pedro</author>
<author>A Schlaikjer</author>
</authors>
<title>JAVELIN I and II Systems at TREC</title>
<date>2006</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="18124" citStr="Nyberg et al., 2006" startWordPosition="2857" endWordPosition="2860">question keyword k in s: 2.2.1 Compute distance d, the minimum nu words between k and the answer candidate 2.2.2 Update the snippet cs(s) = cs(s) × co-occurrence scot d) 2.3. gs(Ai) = gs(Ai) + cs(s) 3. Normalize the Google score (dividing it by a 2 (1+ 528 • Representative entities are converted into the represented entity when the expected answer type is COUNTRY (e.g., “the Egyptian government” is changed to “Egypt” and “Clinton administration” is changed to “U.S.”). 4 Experiment This section describes the experiments we used to evaluate our answer selection framework. The JAVELIN QA system (Nyberg et al., 2006) was used as a testbed for the evaluation. 4.1 Experimental Setup A total of 1760 factoid questions from the TREC8- 12 QA evaluations served as a dataset, with 5-fold cross validation. To better understand how the performance of our framework varies for different extraction techniques, we tested it with four JAVELIN answer extraction modules: FST, LIGHTv1, LIGHTv2 and SVM (Nyberg et al., 2006). FST is an answer extractor based on finite state transducers that incorporate a set of extraction patterns (both manually-created and generalized patterns). LIGHTv1 is an extractor that selects answer c</context>
</contexts>
<marker>Nyberg, Mitamura, Frederking, Bilotti, Hannan, Hiyakumoto, Ko, Lin, Pedro, Schlaikjer, 2006</marker>
<rawString>E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti, K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro, and A. Schlaikjer. 2006. JAVELIN I and II Systems at TREC 2005. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>R Frederking</author>
<author>V Pedro</author>
<author>M Bilotti</author>
<author>A Schlaikjer</author>
<author>K Hannan</author>
</authors>
<title>Extending the javelin qa system with domain semantics.</title>
<date>2005</date>
<booktitle>In Proceedings ofAAAI-05 Workshop on Question Answering in Restricted Domains.</booktitle>
<contexts>
<context position="24320" citStr="Nyberg et al., 2005" startWordPosition="3786" endWordPosition="3789">erformance gain from similarity features when merging similar answers returned from all four extractors. 5 Extensions for Complex Questions Although we conducted our experiments on factoid questions, our framework can be easily extended to handle complex questions, which require longer answers representing facts or relations (e.g., “What is the relationship between Alan Greenspan and Robert Rubin?”). As answer candidates are long text snippets, different features should be used for answer selection. Possible validation features include question keyword inclusion and predicate structure match (Nyberg et al., 2005). For example, given the question “Did Egypt sell Scud missiles to Syria?”, the key predicate from the question is Sell(Egypt, Syria, Scud missile). If there is a sentence which contains the predicate structure Buy(Syria, Scud missile, Egypt), we can calculate the predicate structure distance and use it as a validation feature. For answer similarity, we intend to explore novelty detection approaches evaluated in Allan et al. (2003). 6 Conclusion In this paper, we described our answer selection framework for estimating the probability that an answer candidate is correct given multiple answer va</context>
</contexts>
<marker>Nyberg, Mitamura, Frederking, Pedro, Bilotti, Schlaikjer, Hannan, 2005</marker>
<rawString>E. Nyberg, T. Mitamura, R. Frederking, V. Pedro, M. Bilotti, A. Schlaikjer, and K. Hannan. 2005. Extending the javelin qa system with domain semantics. In Proceedings ofAAAI-05 Workshop on Question Answering in Restricted Domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>A Coden</author>
<author>D Radev</author>
</authors>
<title>Question answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1351" citStr="Prager et al., 2000" startWordPosition="195" endWordPosition="198">ranking of the correct answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework. 1 Introduction Question answering aims at finding exact answers to a user’s natural language question from a large collection of documents. Most QA systems combine information retrieval with extraction techniques to identify a set of likely candidates and then utilize some selection strategy to generate the final answers (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2001). Since answer extractors may be based on imprecise empirical methods, the selection process can be very challenging, as it often entails identifying correct answer(s) amongst many incorrect ones. Answer Score Docum candidates extract Beijing 0.7 AP880 Hong Kong 0.65 WSJ92 Shanghai 0.64 Taiwan Shanghai 0.4 0. 5 Figure 1 shows a traditional QA architecture with an example question. Given the question “Which city in China has the largest number offoreign financial companies?”, the answer extraction component produces a ranked list of five answer cand</context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>J. Prager, E. Brown, A. Coden, and D. Radev. 2000. Question answering by predictive annotation. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>C Welty</author>
<author>A Ittycheriah</author>
<author>R Mahindru</author>
</authors>
<title>IBM’s Piquant in Trec2003.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="3632" citStr="Prager et al., 2004" startWordPosition="550" endWordPosition="553">wers (“Shanghai”, as above) or several answers which represent a single instance (e.g. “Clinton, Bill” and “William Jefferson Clinton”) in the candidate list, how much should we boost the answer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of l</context>
<context position="11611" citStr="Prager et al. (2004)" startWordPosition="1790" endWordPosition="1793">heir cities, continents, capitals, etc. For answer selection, we used three gazetteer resources: the Tipster Gazetteer, the CIA World Factbook (https://www.cia.gov/cia/publications/factbook/inde x.html) and information about the US states provided by 50states.com (http://www.50states.com). These resources were used to assign an answer validity score between -1 and 1 to each candidate (Figure 2). A score of 0 means the gazetteers did not contribute to the answer selection process for that candidate. For some numeric questions, range checking was added to validate numeric questions similarly to Prager et al. (2004). For example, given the question “How many people live in Chile?”, if an answer candidate is within f 10% of the population stated in the CIA World Factbook, it receives a score of 1.0. If it is in the range of 20%, its score is 0.5. If it significantly differs by more than 20%, it receives a score of -1.0. The threshold may vary based on when the document was written and when the census was taken1. WordNet: The WordNet lexical database includes English words organized in synonym sets, called synsets (Fellbaum, 1998). We used WordNet in order to produce an answer validity score between -1 and</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, Welty, Ittycheriah, Mahindru, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Ittycheriah, and R. Mahindru. 2004 IBM’s Piquant in Trec2003. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schlobach</author>
<author>M Olsthoorn</author>
<author>M de Rijke</author>
</authors>
<title>Type checking in open-domain question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of European Conference on Artificial Intelligence.</booktitle>
<marker>Schlobach, Olsthoorn, de Rijke, 2004</marker>
<rawString>S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004. Type checking in open-domain question answering. In Proceedings of European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Si</author>
<author>J Callan</author>
</authors>
<title>CLEF2005: Multilingual retrieval by combining multiple multilingual ranked lists.</title>
<date>2005</date>
<booktitle>In Proceedings of Cross-Language Evaluation Forum.</booktitle>
<contexts>
<context position="7770" citStr="Si and Callan, 2005" startWordPosition="1183" endWordPosition="1186">n, which is a statistical machine learning technique used to predict the probability of a binary variable from input variables. Logistic 525 P(correct(Ai)|Q, A1,..., An) (1) � P(correct(Ai)|val1(Ai),...,valK1(Ai),sim1(Ai),...,simK2(Ai)) exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai)) k=1 k=1 = 1 + exp(α0 + K1E Qkvalk(Ai) + K2E Aksimk(Ai)) k=1 k=1 where, simk(Ai) = N simk(Ai, Aj). 7 j=1(j 0 α, A A = argmax R Nj logP(correct(Ai)|val1(Ai),..., valK1(Ai), sim1(Ai),..., simK2(Ai)) (2) ~α,~β,~λ j=1 i=1 regression has been successfully employed in many applications including multilingual document merging (Si and Callan, 2005). In our previous work (Ko et al., 2006), we showed that logistic regression performed well in merging three resources to validate answers to location and proper name questions. We extended this approach to combine multiple similarity features with multiple answer validation features. The extended framework estimates the probability that an answer candidate is correct given the degree of answer correctness and the amount of supporting evidence provided in a set of answer candidates (Equation 1). In Equation 1, each valk(Ai) is a feature function used to produce an answer validity score for an </context>
</contexts>
<marker>Si, Callan, 2005</marker>
<rawString>L. Si and J. Callan. 2005 CLEF2005: Multilingual retrieval by combining multiple multilingual ranked lists. In Proceedings of Cross-Language Evaluation Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="5804" citStr="Voorhees, 2004" startWordPosition="891" endWordPosition="892"> the scores of the clustered answers. Although many QA systems individually address these issues in answer selection, there has been little research on generating a generalized probabilistic framework that allows any validation and similarity features to be easily incorporated. In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor. This paper is organized as follows: Section 2 describes our answer selection framework and Section 3 lists the features that generate similarity and validity scores for factoid questions. In Section 4, we describe the experimental methodology and the results. Section 5 describes how we intend to extend our framework to handle complex questions. Finally Section 6 concludes with </context>
<context position="10098" citStr="Voorhees, 2004" startWordPosition="1568" endWordPosition="1569">l answer candidate set is reranked according to the estimated probability of each candidate. For factoid questions, the top answer is selected as the final answer to the question. As logistic regression can be used for binary classification with a default threshold of 0.5, we can also use the framework to classify incorrect answers: if the probability of an answer candidate is lower than 0.5, it is considered to be a wrong answer and is filtered out of the answer list. This is useful in deciding whether or not a valid answer exists in the corpus, an important aspect of the TREC QA evaluation (Voorhees, 2004). 3 Feature Representation This section details the features used to generate answer validity scores and answer similarity scores for our answer selection framework. 526 3.1 Answer Validation Features Each answer validation feature produces a validity score which predicts whether or not an answer candidate is a correct answer for the question. This task can be done by exploiting external QA resources such as the Web, databases, and ontologies. For factoid questions, we used gazetteers and WordNet in a knowledge-based approach; we also used Wikipedia and Google in a data-driven approach. 3.1.1 </context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>E. Voorhees. 2004. Overview of the TREC 2003 question answering track. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Licuanan</author>
<author>J May</author>
<author>S Miller</author>
<author>R Weischedel</author>
</authors>
<title>TREC</title>
<date>2003</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="3587" citStr="Xu et al., 2003" startWordPosition="542" endWordPosition="545">or example, when there are redundant answers (“Shanghai”, as above) or several answers which represent a single instance (e.g. “Clinton, Bill” and “William Jefferson Clinton”) in the candidate list, how much should we boost the answer candidate scores? To address the first issue, several answer selection approaches have used semantic resources. One of the most common approaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a resource’s hierarchy corresponding to the expected answer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002), and Wikipedia’s structured information has been used for answer type checking (Buscaldi and Rosso, 2006). To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographical databases. However, in their experiments the combination actually hurt performance because of the increased semantic amb</context>
</contexts>
<marker>Xu, Licuanan, May, Miller, Weischedel, 2003</marker>
<rawString>J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel. 2003. TREC 2002 QA at BBN: Answer Selection and Confidence Estimation. In Proceedings of TREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>