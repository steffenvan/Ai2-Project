<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023425">
<title confidence="0.985225">
Towards Emotion Prediction in Spoken Tutoring Dialogues
</title>
<author confidence="0.997647">
Diane Litman
</author>
<affiliation confidence="0.928557">
Dept. of Computer Science
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
</affiliation>
<email confidence="0.996738">
litman@cs.pitt.edu
</email>
<author confidence="0.858969">
Kate Forbes
</author>
<affiliation confidence="0.7859">
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
</affiliation>
<email confidence="0.995919">
forbesk@pitt.edu
</email>
<author confidence="0.983532">
Scott Silliman
</author>
<affiliation confidence="0.882081">
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
</affiliation>
<email confidence="0.998496">
scotts@pitt.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899">
Human tutors detect and respond to student
emotional states, but current machine tutors do
not. Our preliminary machine learning experi-
ments involving transcription, emotion annota-
tion and automatic feature extraction from our
human-human spoken tutoring corpus indicate
that the spoken tutoring system we are devel-
oping can be enhanced to automatically predict
and adapt to student emotional states.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99574721875">
Connections between learning and emotion are well-
documented (Coles, 1999), and studies have shown con-
siderable benefits of spoken tutoring (Hausmann and Chi,
2002). Human tutors can respond to both the content of
student speech and the manner with which it is spoken
(e.g. ‘confidently’ or ‘uncertainly’), but most intelligent
tutoring dialogue systems are text-based and thus limited
in their ability to recognize such learning states (Rose and
Freedman, 2000; Rose and Aleven, 2002). Building spo-
ken dialogue tutoring systems has great potential benefit,
for speech is the most natural and easy to use form of
natural language interaction, and it supplies a rich source
of prosodic and acoustic information about the speaker’s
current mental state, which can be used to monitor the
pedagogical effectiveness of student-computer interac-
tions. The success of computer-based tutoring systems
could increase if they predicted and adapted to student
emotional states, e.g. reinforcing positive states, while
rectifying negative states (Evens, 2002).
Although (Ang et al., 2002; Litman et al., 2001; Bat-
liner et al., 2000) have hand-labeled naturally-occurring
utterances in a variety of corpora for various emotions,
then extracted acoustic, prosodic and lexical features and
used machine-learning techniques to develop predictive
models, little work to date has addressed emotion detec-
tion in computer-based educational settings. In this paper
we describe preliminary annotation of positive, negative,
and neutral emotions in a human-human tutoring corpus
and discuss the results of pilot machine learning experi-
ments whose goal is to develop computational models of
specific emotional states (Section 3) for use in a spoken
dialogue system (Section 2).
</bodyText>
<sectionHeader confidence="0.942775" genericHeader="method">
2 The ITSPOKE System and Corpus
</sectionHeader>
<bodyText confidence="0.983617921568627">
We are developing a spoken dialogue system, called IT-
SPOKE (Intelligent Tutoring SPOKEn dialogue system),
which uses as its “back-end” the text-based Why2-Atlas
dialogue tutoring system (VanLehn et al., 2002). In
Why2-Atlas, a student types an essay answering a quali-
tative physics problem and a computer tutor then engages
him/her in dialogue to provide feedback, correct mis-
conceptions, and elicit more complete explanations, after
which the student revises his/her essay, thereby ending
the tutoring or causing another round of tutoring/essay
revision. To date we have interfaced the Sphinx2 speech
recognizer with stochastic language models trained from
example user utterances, and the Festival speech synthe-
sizer for text-to-speech, to the Why2-Atlas back-end, and
are adapting the knowledge sources needed by the spoken
language components; e.g. we have developed a set of
dialogue-dependent language models using 4551 student
utterances from the Why2-Atlas 2002 human-computer
typed corpus and will enhance them using student utter-
ances from our human-human spoken corpus.
Our human-human spoken corpus contains spoken dia-
logues collected via a web interface supplemented with a
high quality audio link, where a human tutor performs the
same task as ITSPOKE and Why2-Atlas. Our subjects
are U. Pittsburgh students who have taken no college level
physics and are native speakers of (Amer.) English. Our
experimental procedure, taking roughly 7 hours/student
over 1-2 sessions, is as follows: students 1) take a pretest
measuring their physics knowledge, 2) read a small doc-
ument of background material, 3) use the web and voice
interface to work through up to 10 training problems with
the human tutor, and 4) take a post-test similar to the
pretest. We have to date collected 63 dialogues (1290
minutes of speech from 4 females and 4 males) and tran-
scribed 20 of them. A corpus example is shown in Fig-
ure 1, containing the problem, the student’s essay, and an
annotated excerpt from the subsequent dialogue.
PROBLEM: If a car is able to accelerate at 2 m/s2, what accel-
eration can it attain if it is towing another car of equal mass?
ORIGINAL ESSAY: If the car is towing another car of equal
mass, the maximum acceleration would be the same because
the car would be towed behind and the friction caused would
only be by the front of the first car.
... dialogue excerpt at 6.5 minutes into session ...
TUTOR: Now this law that force is equal to mass times accel-
eration, what’s this law called? This is uh since this it is a very
important basic uh fact uh it is it is a law of physics. Um you
have you have read it in the background material. Can you re-
call it?
STUDENT: Um no it was one of Newton’s laws but I don’t
remember which one. (laugh) (EMOTION=NEGATIVE)
</bodyText>
<figure confidence="0.475289">
TUTOR: Right, right, that is Newton’s second law of motion.
STUDENT: Ok, because I remember one, two, and three,
but I didn’t know if there was a different name (EMO-
TION=POSITIVE)
</figure>
<figureCaption confidence="0.7077565">
TUTOR: Yeah that’s right. You know Newton was a genius and
uh he looked at a large number of experiments and experimen-
tal data that was available and from that he could come to this
general law...
STUDENT: mm-hm (EMOTION=NEUTRAL)
Figure 1: Human-Human Spoken Corpus Example
</figureCaption>
<sectionHeader confidence="0.942877" genericHeader="method">
3 Predicting Emotional Speech
</sectionHeader>
<bodyText confidence="0.998122222222222">
For this pilot study, we annotated 14 transcribed dia-
logues from 7 students, 2 dialogues per student. First,
turn boundaries were manually annotated (based on con-
sensus labelings from two coders). Each turn was
then manually annotated for speaker affect (by a sin-
gle coder) using three general categorizations: negative
(e.g.‘uncertain’, ‘frustration’), positive (e.g. ‘confident’,
‘certain’), or neutral/indeterminate, as shown in Figure 1.
Table 1 shows the distribution of our labeled turns.
</bodyText>
<table confidence="0.729745">
neutral positive negative total
248 167 141 553
</table>
<tableCaption confidence="0.998823">
Table 1: Labeled Turn Counts: ITSPOKE Pilot Corpus
</tableCaption>
<bodyText confidence="0.97295446875">
We next conducted experiments using the RIPPER (Co-
hen, 1996) rule induction machine learning program,
which takes as input the classes to be learned (e.g. our
emotion annotations), the names and possible values in
a feature set (discussed below), and training examples,
each specifying its class and feature values (e.g. the la-
beled student turns in our pilot corpus), then outputs a
classification model for classifying future examples, ex-
pressed as an ordered set of if-then rules. RIPPER’s “set-
valued” features allow us to represent the speech recog-
nizer’s best hypothesis and/or the turn transcription as a
set of words, and its rule output is an intuitive way to gain
insight into our data.
For our first pilot machine learning experiment, our
feature set consisted of SUBJECT ID and PROBLEM ID,
both representing system state, TURN START-TIME (rel-
ative to start of dialogue) and TURN DURATION, both
representing timing information, TEXT IN TURN (tran-
scription), and NUMBER OF WORDS IN TURN. Figure 2
presents the ruleset that was learned for this classifica-
tion task. For example, the first learned rule states that
if the duration of the turn is greater than 0.65 seconds
and the transcribed text of the turn contains the lexical
item “I”, then the turn is predicted to be labeled EMO-
TION=NEGATIVE. The estimated mean error and stan-
dard error of this ruleset is 33.03% +/- 2.45%, based on
25-fold cross-validation.
if (duration 0.65) (text has “I”) then neg
else if (duration 2.98) then neg
else if (duration 0.93) (startTime 297.62) then pos
else if (text has “right”) then pos
else neutral
</bodyText>
<figureCaption confidence="0.99466">
Figure 2: All-Features Ruleset for Emotion Prediction
</figureCaption>
<bodyText confidence="0.993870230769231">
For comparison, our feature set in our second pilot
machine learning experiment consisted of just TEXT IN
TURN. The ruleset learned for this classification task con-
tained 21 rules; Figure 3 presents an (ordered) excerpt&apos;.
Estimated mean error and standard error of this ruleset is
39.03% +/- 2.40%, based on 25-fold cross-validation.
if (text has “I”) (text has “don’t”) then neg
else if (text has “um”) (text has “ hn ”) then neg
else if (text has “the”) (text has “ fs ”) then neg
else if (text has “right”) then pos
else if (text has “so”) then pos
else if (text has “(laugh)”) (text has “that’s”) then pos
else neutral
</bodyText>
<figureCaption confidence="0.996625">
Figure 3: Text-Feature Ruleset for Emotion Prediction
</figureCaption>
<bodyText confidence="0.994986333333333">
Although both these error rates are still fairly high,
they are a significant improvement over a majority class
&apos; hn = human noise (e.g. sighs and coughs), and fs =
false start (e.g. “I th- think”)
baseline that always predicts the majority class in our cor-
pus (neutral/indeterminate) - which has an error rate of
55.69%. Moreover, many of the learned rules contain
features that are intuitively associated with the predicted
emotion; for example, disfluencies such as false starts are
often associated with negative emotions such as ‘uncer-
tainty’, as are lexical items such as “um” used in combi-
nation with human noises such as sighs.
</bodyText>
<sectionHeader confidence="0.992398" genericHeader="method">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.999964484848485">
Even using a small corpus classified by one coder and
predicted using only a handful of features, our results
suggest that there are indeed features that can automati-
cally distinguish emotions in tutoring dialogues. We will
next explore the utility of a wider variety of features rep-
resenting many knowledge sources (including acoustic,
prosodic, lexical, syntactic, semantic, discourse, and lo-
cal and global contextual dialogue features), using abla-
tion studies. We will perform our learning using and com-
paring large corpora of both human-human and human-
computer data for training and testing, and will evaluate
our results using a variety of metrics (e.g. recall, pre-
cision, and F-measure). We will also investigate a va-
riety of emotion annotations with the goal of producing
a reliable annotation scheme for the emotions associated
with our tutoring domain. Previous studies have shown
low inter-annotator reliability (around 70%, Kappa val-
ues around 0.47 (Narayanan, 2002)), which originates
partly in vague descriptions of the emotions to be labeled.
Finally, we hope to use this work to demonstrate that
enhancing a spoken dialogue tutoring system to automat-
ically predict and then dynamically respond to student
emotional states will measurably improve system perfor-
mance. Our enhancements will be motivated by tutor-
ing literature (Evens, 2002; Aist et al., 2002) that ad-
dresses how a tutor might make use of such information
if it could be inferred, as well as by looking at how the
human tutor actually responded to emotionally labeled
turns. Our methodology will build on previous adaptive
(non-tutoring) dialogue systems (see (Litman and Pan,
2002)); however, our system will predict and adapt to
both problematic and positive dialogue situations in tu-
toring.
</bodyText>
<sectionHeader confidence="0.998525" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.94293975">
This research is supported by the NSF under Grant No.
9720359 to the Center for Interdisciplinary Research on
Constructive Learning Environments (CIRCLE) at the
University of Pittsburgh and Carnegie-Mellon University.
</bodyText>
<sectionHeader confidence="0.958493" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.9667032">
G. Aist, B. Kort, Rob R.lly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intelli-
gent tutoring system with human-supplied capabilities:
Adding human-provided emotional scaffolding to an
automated reading tutor that listens. In Proc. ofITS.
</bodyText>
<reference confidence="0.998947833333333">
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP.
A. Batliner, R. Huber, H. R. Niemann, E. N¨oth, J. Spilker,
and K. Fischer. 2000. The recognition of emotion. In
Proc. of the ISCA Workshop on Speech and Emotion.
William Cohen. 1996. Learning trees and rules with set-
valued features. In Proc. ofAAAI.
G. Coles. 1999. Literacy, emotions, and the brain. Read-
ing Online, March 1999.
M. Evens. 2002. New questions for Circsim-Tutor. Pre-
sentation at the 2002 Symposium on Natural Language
Tutoring, University of Pittsburgh.
Robert Hausmann and Michelene Chi. 2002. Can a com-
puter interface support self-explaining? The Interna-
tional Journal of Cognitive Technology, 7(1).
Diane J. Litman and Shimei Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system. User
Modeling and User-Adapted Interaction, 12.
D. Litman, J. Hirschberg, and M. Swerts. 2001. Predict-
ing user reactions to system error. In Proc.ofACL.
S. Narayanan. 2002. Towards modeling user behavior in
human-machine interaction: Effect of errors and emo-
tions. In Proc. ofISLE.
C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002
workshop on empirical methods for tutorial dialogue
systems. Technical report, San Sebastian, Spain, June.
C. P. Rose and R. Freedman. 2000. Building dialogue
systems for tutorial applications. Technical Report FS-
00-01 (Working Notes of the Fall Symposium), AAAI.
K. VanLehn, P. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. ofITS.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.667816">
<title confidence="0.999961">Towards Emotion Prediction in Spoken Tutoring Dialogues</title>
<author confidence="0.999634">Diane Litman</author>
<affiliation confidence="0.999852">Dept. of Computer Science LRDC, Univ. of Pittsburgh</affiliation>
<address confidence="0.998749">Pittsburgh PA, 15260, USA</address>
<email confidence="0.999402">litman@cs.pitt.edu</email>
<author confidence="0.891552">Kate</author>
<affiliation confidence="0.986012">LRDC, Univ. of</affiliation>
<address confidence="0.982151">Pittsburgh PA, 15260,</address>
<email confidence="0.99885">forbesk@pitt.edu</email>
<author confidence="0.790702">Scott</author>
<affiliation confidence="0.987883">LRDC, Univ. of</affiliation>
<address confidence="0.986417">Pittsburgh PA, 15260,</address>
<email confidence="0.996938">scotts@pitt.edu</email>
<abstract confidence="0.9994212">Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="1828" citStr="Ang et al., 2002" startWordPosition="262" endWordPosition="265">000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings. In this paper we describe preliminary annotation of positive, negative, and neutral emotions in a human-human tutoring corpus and discuss the results of pilot machine learning experiments whose goal is to develop computational models of specific</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>R Huber</author>
<author>H R Niemann</author>
<author>E N¨oth</author>
<author>J Spilker</author>
<author>K Fischer</author>
</authors>
<title>The recognition of emotion.</title>
<date>2000</date>
<booktitle>In Proc. of the ISCA Workshop on Speech and Emotion.</booktitle>
<marker>Batliner, Huber, Niemann, N¨oth, Spilker, Fischer, 2000</marker>
<rawString>A. Batliner, R. Huber, H. R. Niemann, E. N¨oth, J. Spilker, and K. Fischer. 2000. The recognition of emotion. In Proc. of the ISCA Workshop on Speech and Emotion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Learning trees and rules with setvalued features.</title>
<date>1996</date>
<booktitle>In Proc. ofAAAI.</booktitle>
<contexts>
<context position="6448" citStr="Cohen, 1996" startWordPosition="1008" endWordPosition="1010">scribed dialogues from 7 students, 2 dialogues per student. First, turn boundaries were manually annotated (based on consensus labelings from two coders). Each turn was then manually annotated for speaker affect (by a single coder) using three general categorizations: negative (e.g.‘uncertain’, ‘frustration’), positive (e.g. ‘confident’, ‘certain’), or neutral/indeterminate, as shown in Figure 1. Table 1 shows the distribution of our labeled turns. neutral positive negative total 248 167 141 553 Table 1: Labeled Turn Counts: ITSPOKE Pilot Corpus We next conducted experiments using the RIPPER (Cohen, 1996) rule induction machine learning program, which takes as input the classes to be learned (e.g. our emotion annotations), the names and possible values in a feature set (discussed below), and training examples, each specifying its class and feature values (e.g. the labeled student turns in our pilot corpus), then outputs a classification model for classifying future examples, expressed as an ordered set of if-then rules. RIPPER’s “setvalued” features allow us to represent the speech recognizer’s best hypothesis and/or the turn transcription as a set of words, and its rule output is an intuitive</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>William Cohen. 1996. Learning trees and rules with setvalued features. In Proc. ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Coles</author>
</authors>
<title>Literacy, emotions, and the brain. Reading Online,</title>
<date>1999</date>
<contexts>
<context position="828" citStr="Coles, 1999" startWordPosition="113" endWordPosition="114">A, 15260, USA forbesk@pitt.edu Scott Silliman LRDC, Univ. of Pittsburgh Pittsburgh PA, 15260, USA scotts@pitt.edu Abstract Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states. 1 Introduction Connections between learning and emotion are welldocumented (Coles, 1999), and studies have shown considerable benefits of spoken tutoring (Hausmann and Chi, 2002). Human tutors can respond to both the content of student speech and the manner with which it is spoken (e.g. ‘confidently’ or ‘uncertainly’), but most intelligent tutoring dialogue systems are text-based and thus limited in their ability to recognize such learning states (Rose and Freedman, 2000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source o</context>
</contexts>
<marker>Coles, 1999</marker>
<rawString>G. Coles. 1999. Literacy, emotions, and the brain. Reading Online, March 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evens</author>
</authors>
<title>New questions for Circsim-Tutor. Presentation at the 2002 Symposium on Natural Language Tutoring,</title>
<date>2002</date>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1800" citStr="Evens, 2002" startWordPosition="259" endWordPosition="260">es (Rose and Freedman, 2000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings. In this paper we describe preliminary annotation of positive, negative, and neutral emotions in a human-human tutoring corpus and discuss the results of pilot machine learning experiments whose goal is to develop comp</context>
<context position="10725" citStr="Evens, 2002" startWordPosition="1706" endWordPosition="1707">annotations with the goal of producing a reliable annotation scheme for the emotions associated with our tutoring domain. Previous studies have shown low inter-annotator reliability (around 70%, Kappa values around 0.47 (Narayanan, 2002)), which originates partly in vague descriptions of the emotions to be labeled. Finally, we hope to use this work to demonstrate that enhancing a spoken dialogue tutoring system to automatically predict and then dynamically respond to student emotional states will measurably improve system performance. Our enhancements will be motivated by tutoring literature (Evens, 2002; Aist et al., 2002) that addresses how a tutor might make use of such information if it could be inferred, as well as by looking at how the human tutor actually responded to emotionally labeled turns. Our methodology will build on previous adaptive (non-tutoring) dialogue systems (see (Litman and Pan, 2002)); however, our system will predict and adapt to both problematic and positive dialogue situations in tutoring. Acknowledgments This research is supported by the NSF under Grant No. 9720359 to the Center for Interdisciplinary Research on Constructive Learning Environments (CIRCLE) at the Un</context>
</contexts>
<marker>Evens, 2002</marker>
<rawString>M. Evens. 2002. New questions for Circsim-Tutor. Presentation at the 2002 Symposium on Natural Language Tutoring, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Hausmann</author>
<author>Michelene Chi</author>
</authors>
<title>Can a computer interface support self-explaining?</title>
<date>2002</date>
<journal>The International Journal of Cognitive Technology,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="918" citStr="Hausmann and Chi, 2002" startWordPosition="125" endWordPosition="128">rgh PA, 15260, USA scotts@pitt.edu Abstract Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states. 1 Introduction Connections between learning and emotion are welldocumented (Coles, 1999), and studies have shown considerable benefits of spoken tutoring (Hausmann and Chi, 2002). Human tutors can respond to both the content of student speech and the manner with which it is spoken (e.g. ‘confidently’ or ‘uncertainly’), but most intelligent tutoring dialogue systems are text-based and thus limited in their ability to recognize such learning states (Rose and Freedman, 2000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be</context>
</contexts>
<marker>Hausmann, Chi, 2002</marker>
<rawString>Robert Hausmann and Michelene Chi. 2002. Can a computer interface support self-explaining? The International Journal of Cognitive Technology, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
</authors>
<title>Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction,</title>
<date>2002</date>
<pages>12</pages>
<contexts>
<context position="11034" citStr="Litman and Pan, 2002" startWordPosition="1756" endWordPosition="1759">ns to be labeled. Finally, we hope to use this work to demonstrate that enhancing a spoken dialogue tutoring system to automatically predict and then dynamically respond to student emotional states will measurably improve system performance. Our enhancements will be motivated by tutoring literature (Evens, 2002; Aist et al., 2002) that addresses how a tutor might make use of such information if it could be inferred, as well as by looking at how the human tutor actually responded to emotionally labeled turns. Our methodology will build on previous adaptive (non-tutoring) dialogue systems (see (Litman and Pan, 2002)); however, our system will predict and adapt to both problematic and positive dialogue situations in tutoring. Acknowledgments This research is supported by the NSF under Grant No. 9720359 to the Center for Interdisciplinary Research on Constructive Learning Environments (CIRCLE) at the University of Pittsburgh and Carnegie-Mellon University. References G. Aist, B. Kort, Rob R.lly, J. Mostow, and R. Picard. 2002. Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding human-provided emotional scaffolding to an automated reading tutor that listens. In </context>
</contexts>
<marker>Litman, Pan, 2002</marker>
<rawString>Diane J. Litman and Shimei Pan. 2002. Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Hirschberg</author>
<author>M Swerts</author>
</authors>
<title>Predicting user reactions to system error.</title>
<date>2001</date>
<booktitle>In Proc.ofACL.</booktitle>
<contexts>
<context position="1849" citStr="Litman et al., 2001" startWordPosition="266" endWordPosition="269">en, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings. In this paper we describe preliminary annotation of positive, negative, and neutral emotions in a human-human tutoring corpus and discuss the results of pilot machine learning experiments whose goal is to develop computational models of specific emotional states (Se</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2001</marker>
<rawString>D. Litman, J. Hirschberg, and M. Swerts. 2001. Predicting user reactions to system error. In Proc.ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
</authors>
<title>Towards modeling user behavior in human-machine interaction: Effect of errors and emotions.</title>
<date>2002</date>
<booktitle>In Proc. ofISLE.</booktitle>
<contexts>
<context position="10351" citStr="Narayanan, 2002" startWordPosition="1649" endWordPosition="1650">ctic, semantic, discourse, and local and global contextual dialogue features), using ablation studies. We will perform our learning using and comparing large corpora of both human-human and humancomputer data for training and testing, and will evaluate our results using a variety of metrics (e.g. recall, precision, and F-measure). We will also investigate a variety of emotion annotations with the goal of producing a reliable annotation scheme for the emotions associated with our tutoring domain. Previous studies have shown low inter-annotator reliability (around 70%, Kappa values around 0.47 (Narayanan, 2002)), which originates partly in vague descriptions of the emotions to be labeled. Finally, we hope to use this work to demonstrate that enhancing a spoken dialogue tutoring system to automatically predict and then dynamically respond to student emotional states will measurably improve system performance. Our enhancements will be motivated by tutoring literature (Evens, 2002; Aist et al., 2002) that addresses how a tutor might make use of such information if it could be inferred, as well as by looking at how the human tutor actually responded to emotionally labeled turns. Our methodology will bui</context>
</contexts>
<marker>Narayanan, 2002</marker>
<rawString>S. Narayanan. 2002. Towards modeling user behavior in human-machine interaction: Effect of errors and emotions. In Proc. ofISLE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>V Aleven</author>
</authors>
<title>workshop on empirical methods for tutorial dialogue systems.</title>
<date>2002</date>
<booktitle>Proc. of the ITS</booktitle>
<tech>Technical report,</tech>
<location>San Sebastian, Spain,</location>
<contexts>
<context position="1239" citStr="Rose and Aleven, 2002" startWordPosition="175" endWordPosition="178">e spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states. 1 Introduction Connections between learning and emotion are welldocumented (Coles, 1999), and studies have shown considerable benefits of spoken tutoring (Hausmann and Chi, 2002). Human tutors can respond to both the content of student speech and the manner with which it is spoken (e.g. ‘confidently’ or ‘uncertainly’), but most intelligent tutoring dialogue systems are text-based and thus limited in their ability to recognize such learning states (Rose and Freedman, 2000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et</context>
</contexts>
<marker>Rose, Aleven, 2002</marker>
<rawString>C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002 workshop on empirical methods for tutorial dialogue systems. Technical report, San Sebastian, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>R Freedman</author>
</authors>
<title>Building dialogue systems for tutorial applications.</title>
<date>2000</date>
<booktitle>(Working Notes of the Fall Symposium), AAAI.</booktitle>
<tech>Technical Report FS00-01</tech>
<contexts>
<context position="1215" citStr="Rose and Freedman, 2000" startWordPosition="171" endWordPosition="174">g corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states. 1 Introduction Connections between learning and emotion are welldocumented (Coles, 1999), and studies have shown considerable benefits of spoken tutoring (Hausmann and Chi, 2002). Human tutors can respond to both the content of student speech and the manner with which it is spoken (e.g. ‘confidently’ or ‘uncertainly’), but most intelligent tutoring dialogue systems are text-based and thus limited in their ability to recognize such learning states (Rose and Freedman, 2000; Rose and Aleven, 2002). Building spoken dialogue tutoring systems has great potential benefit, for speech is the most natural and easy to use form of natural language interaction, and it supplies a rich source of prosodic and acoustic information about the speaker’s current mental state, which can be used to monitor the pedagogical effectiveness of student-computer interactions. The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang</context>
</contexts>
<marker>Rose, Freedman, 2000</marker>
<rawString>C. P. Rose and R. Freedman. 2000. Building dialogue systems for tutorial applications. Technical Report FS00-01 (Working Notes of the Fall Symposium), AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>P Jordan</author>
<author>C Ros´e</author>
<author>D Bhembe</author>
<author>M B¨ottner</author>
<author>A Gaydos</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>M Ringenberg</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>R Wilson</author>
</authors>
<title>The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In</title>
<date>2002</date>
<booktitle>Proc. ofITS.</booktitle>
<marker>VanLehn, Jordan, Ros´e, Bhembe, B¨ottner, Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque, Siler, Srivastava, Wilson, 2002</marker>
<rawString>K. VanLehn, P. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. ofITS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>