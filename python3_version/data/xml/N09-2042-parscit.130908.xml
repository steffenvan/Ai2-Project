<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.065123">
<title confidence="0.9968925">
Search Result Re-ranking by Feedback Control Adjustment for
Time-sensitive Query
</title>
<author confidence="0.965504">
Ruiqiang Zhang† and Yi Chang† and Zhaohui Zheng†
Donald Metzler† and Jian-yun Nie‡†Yahoo! Labs, 701 First Avenue, Sunnyvale, CA94089
</author>
<affiliation confidence="0.996324">
‡University of Montreal, Montreal, Quebec,H3C 3J7, Canada
</affiliation>
<email confidence="0.708284">
†{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com
‡nie@iro.umontreal.ca
</email>
<sectionHeader confidence="0.99374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999095">
We propose a new method to rank a special
category of time-sensitive queries that are year
qualified. The method adjusts the retrieval
scores of a base ranking function according
to time-stamps of web documents so that the
freshest documents are ranked higher. Our
method, which is based on feedback control
theory, uses ranking errors to adjust the search
engine behavior. For this purpose, we use
a simple but effective method to extract year
qualified queries by mining query logs and a
time-stamp recognition method that considers
titles and urls of web documents. Our method
was tested on a commercial search engine. The
experiments show that our approach can sig-
nificantly improve relevance ranking for year
qualified queries even if all the existing meth-
ods for comparison failed.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997765722222222">
Relevance ranking plays a crucial role in search
engines. There are many proposed machine learn-
ing based ranking algorithms such as language
modeling-based methods (Zhai and Lafferty, 2004),
RankSVM (Joachims, 2002), RankBoost (Freund et al.,
1998) and GBrank (Zheng et al., 2007). The input to
these algorithms is a set of feature vectors extracted from
queries and documents. The goal is to find the parameter
setting that optimizes some relevance metric given
training data. While these machine learning algorithms
can improve average relevance, they may be ineffctive
for certain special cases. Time-sensitive queries are one
such special case that machine-learned ranking functions
may have a hard time learning, due to the small number
of such queries.
Consider the query “sigir” (the name of a conference),
which is time sensitive. Table 1 shows two example
search result pages for the query, SERP1 and SERP2. The
</bodyText>
<footnote confidence="0.990525909090909">
query: sigir
SERP1 url1: http://www.sigir.org
url2: http://www.sigir2008.org
url3: http://www.sigir2004.org
url4: http://www.sigir2009.org
url5: http://www.sigir2009.org/schedule
SERP2 url1: http://www.sigir.org
url2: http://www.sigir2009.org
url3: http://www.sigir2009.org/schedule
url4: http://www.sigir2008.org
url5: http://www.sigir2004.org
</footnote>
<tableCaption confidence="0.998508">
Table 1: Two contrived search engine result pages
</tableCaption>
<bodyText confidence="0.99977616">
ranking of SERP2 is clearly better than that of SERP1 be-
cause the most recent event, “sigir2009”, is ranked higher
than other years.
Time is an important dimension of relevance in web
search, since users tend to prefer recent documents to old
documents. At the time of this writing (February 2009),
none of the major commercial search engines ranked the
homepage for SIGIR 2009 higher than previous SIGIR
homepages for the query “sigir”. One possible reason for
this is that ranking algorithms are typically based on an-
chor text features, hyperlink induced features, and click-
through rate features. However, these features tend to fa-
vor old pages more than recent ones. For example, “si-
gir2008” has more links and clicks than “sigir2009” be-
cause “sigir2008” has existed longer time and therefore
has been visited more. It is less likely that newer web
pages from “sigir2009” can be ranked higher using fea-
tures that implicitly favor old pages.
However, the fundamental problem is that current ap-
proaches have focused on improving general ranking al-
gorithms. Methods for improving ranking of specific
types of query like temporal queries are often overlooked.
Aiming to improve ranking results, some methods of
re-ranking search results are proposed, such as the work
by (Agichtein et al., 2006) and (Teevan et al., 2005).
</bodyText>
<page confidence="0.995852">
165
</page>
<note confidence="0.4466235">
Proceedings of NAACL HLT 2009: Short Papers, pages 165–168,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.98905475">
Controller
Search Engine
error
R(q, yo)
+
_
Detector
R(q, yn)
</figure>
<figureCaption confidence="0.99988">
Figure 1: Feedback control for search engine
</figureCaption>
<bodyText confidence="0.999982444444444">
These work uses user search behavior information or per-
sonalization information as features that are integrated
into an enhanced ranking model. We propose a novel
method of re-ranking search results. This new method
is based on feedback control theory, as illustrated in 1.
We make a Detector to monitor search engine (SE) out-
put and compare it with the input, which is the desired
search engine ranking. If an error is found, we design
the controller that uses the error to adjust the search en-
gine output, such that the search engine output tracks the
input. We will detail the algorithm in Section 4.1.
Our method was applied to a special class of time-
sensitive query, year qualified queries (YQQs). For this
category, we found users either attached a year with the
query explicitly, like “sigir 2009”, or used the query only
without a year attached,like “sigir”. We call the former
explicit YQQs, and the latter implicit YQQs. Using query
log analysis, we found these types of queries made up
about 10% of the total query volume. We focus exclu-
sively on implicit YQQs by translating the user’s im-
plicit intention as the most recent year. Explicit YQQs
are less interesting, because the user’s temporal inten-
tion is clearly specified in the query. Therefore, rank-
ing for these types of queries is relatively straightfor-
ward. Throughout the remainder of this paper, we use
the “YQQ” to refer to implicit YQQs, unless otherwise
stated.
</bodyText>
<sectionHeader confidence="0.91506" genericHeader="method">
2 Adaptive score adjustment
</sectionHeader>
<bodyText confidence="0.992262">
Our proposed re-ranking model is shown in Eq. 1, as be-
low.
</bodyText>
<equation confidence="0.989092285714286">
� R(q, d) if q � Y44
F(q, d) =
� R(q, d) + Q(q, d) otherwise
_ (e(do, dn) + k)eAα(q) i f y(d) = yn
Q(q, d) 0 otherwise
e(do, dn) = R(q, do) − R(q, dn)
(1)
</equation>
<bodyText confidence="0.99990405882353">
This work assumes that a base ranking function is used
to rank documents with respect to an incoming query. We
denote this base ranking function as R(q, d). This ranking
function is conditioned on a query q and a document d. It
is assumed to model the relevance between q and d.
Our proposed method is flexible for all YQQ queries.
Suppose the current base ranking function gives the re-
sults as SERP1 of Table 1. To correct the ranking, we
propose making an adjustment to R(q, d).
In Eq. 1, F(q, d) is the final ranking function. If the
query is not an YQQ, the base ranking function is used.
Otherwise, we propose an adjustment function, Q(q, d) ,
to adjust the base ranking function. Q(q, d) is controlled
by the ranking error, e(do, dn), signifying the base func-
tion ranking error if the newest web page dn is ranked
lower than the oldest web page do. y(d) is the year that
the event described by d has occurred or will occur. If
yo and yn indicate the oldest year and the newest year,
then y(do) = yo, y(dn) = yn. R(q, do) and R(q, dn) are the
base ranking function scores for the oldest and the newest
documents.
k is a small shift value for direction control. When
k &lt; 0, the newest document is adjusted slightly under the
old one. Otherwise, it is adjusted slightly over the old
one. Experiments show k &gt; 0 gave better results. The
value of k is determined in training.
α(q) is the confidence score of a YQQ query, mean-
ing the likelihood of a query to be YQQ. The confidence
score is bigger if a query is more likely to be YQQ. More
details are given in next section. A is a weighting param-
eter for adjusting α(q).
The exp function eAα(q) is a weighting to control boost-
ing value. A higher value, confidence α, a larger boosting
value, Q(q, d).
Our method can be understood by feedback control
theory, as illustrated in Fig. 1. The ideal input is R(q, yo)
representing the desired ranking score for the newest
Web page, R(q, yn). But the search engine real output
is R(q, yn). Because search engine is a dynamic system,
its ranking is changing over time. This results in ranking
errors, e(do, dn) = R(q, do) − R(q, dn). The function of
“Controller” is to design a function to adjust the search
engine ranking so that the error approximates to zero,
e(do, dn) = 0. For this work, “Controller” is Q(q, d).
“Detector” is a document year-stamp recognizer, which
will be described more in the next section. “Detector”
is used to detect the newest Web pages and their ranking
scores. Fig. 1 is an ideal implementation of our methods.
We cannot carry out real-time experiments in this work.
Therefore, the calculation of ranking errors was made in
offline training.
</bodyText>
<sectionHeader confidence="0.9919925" genericHeader="method">
3 YQQ detection and year-stamp
recognition
</sectionHeader>
<bodyText confidence="0.9997602">
To implement Eq. 1, we need to find YQQ queries and to
identify the year-stamp of web documents.
Our YQQ detection method is simple, efficient, and
relies only on having access to a query log with frequency
information. First, we extracted all explicit YQQs from
</bodyText>
<page confidence="0.994636">
166
</page>
<bodyText confidence="0.999604176470588">
query log. Then, we removed all the years from explicit
YQQs. Thus, implicit YQQs are obtained from explicit
YQQs. The implicit YQQs are saved in a dictionary. In
online test, we match input queries with each of implicit
YQQs in the dictionary. If an exact match is found, we
regard the input query as YQQ, and apply Eq. 1 to re-rank
search results.
After analyzing samples of the extracted YQQs, we
group them into three classes. One is recurring-event
query, like “sigir”, “us open tennis”; the second is news-
worthy query, like “steve ballmer”, “china foreign re-
serves”; And the class not belong to any of the above
two, like “christmas”, “youtube”. We found our proposed
methods were the most effective for the first category. In
Eq. 1, we can use confidence α(q) to distinguish the three
categories and their change of ranking as shown in Eq.1,
that is defined as below.
</bodyText>
<equation confidence="0.997585">
�y w(q, y)
α(q) = (2)
#(q) + Zy w(q, y)
</equation>
<bodyText confidence="0.999963363636364">
where w(q, y) = #(q.y)+#(y.q). #(q.y) denotes the num-
ber of times that the base query q is post-qualified with
the year y in the query log. Similarly, #(y.q) is the num-
ber of times that q is pre-qualified with the year y. This
weight measures how likely q is to be qualified with y,
which forms the basis of our mining and analysis. #(q) is
the counts of independent query, without associating with
any other terms.
We also need to know the year-stamp y(d) for each
web document so that the ranking score of a document
is updated if y(d) = yn is satisfied. We can do this
from a few sources such as title, url, anchar text, and
extract date from documents that is possible for many
news pages. For example, from url of the web page,
“www.sigir2009.org”, we detect its year-stamp is 2009.
We have also tried to use some machine generated
dates. However, in the end we found such dates are in-
accurate and cannot be trusted. For example, discovery
time is the time when the document was found by the
crawler. But a web document may exist several years be-
fore a crawler found it. We show the worse effect of using
discovery time in the experiments.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9884971875">
We will describe the implementation methods and experi-
mental results in this section. Our methods include offline
dictionary building and online test. In offline training, our
first step is to mine YQQs. A commercial search engine
company provided us with six months of query logs. We
extracted a list of YQQs using Section 3’s method. For
each of the YQQs, we run the search engine and output
the top N results. For each document, we used the method
described in Section 3 to recognize the year-stamp and
find the oldest and the newest page. If there are multiple
urls with the same yearstamp, we choose the first oldest
and the first most recent. Next,we calculated the boost-
ing value according to Eq. 1. Each query has a boosting
value. For online test, a user’s query is matched with each
of the YQQs in the dictionary. If an exact match is found,
the boosting value will be added to the base ranking score
iff the document has the newest yearstamp.
For evaluating our methods, we randomly extracted
600 YQQs from the dictionary. We extracted the top-5
search results for each of queries using the base ranking
function and the proposed ranking function. We asked
human editors to judge all the scraped results. We used
five judgment grades: Perfect, Excellent, Good, Fair,
and Bad. Editors were instructed to consider temporal
issues when judging. For example, sigir2004 is given
a worse grade than sigir2009. To avoid bias, we ad-
vised editors to retain relevance as their primary judg-
ment criteria. Our evaluation metric is relative change
in DCG, %Δ dg DCGproposed−DCGbaseline where DCG is
dcg = DCGbaseline ,
the traditional Discounted Cumulative Gain (Jarvelin and
Kekalainen, 2002).
</bodyText>
<subsectionHeader confidence="0.998614">
4.1 Effect of the proposed boosting method
</subsectionHeader>
<bodyText confidence="0.9992517">
Our experimental results are shown in Table 2, where we
compared our work with the existing methods. While we
cannot apply (Li and Croft, 2003)’s approach directly be-
cause first, our search engine is not based on language
modeling; second, it is impossible to obtain exact times-
tamp for web pages as (Li and Croft, 2003) did in the
track evaluation. However, we tried to simulate (Li and
Croft, 2003)’s approach in web search by using the linear
integration method exactly as the same as(Li and Croft,
2003) by adding a time-based function with our base
ranking function. For the timestamp, we used discovery
time in the time-based function. The parameters (λ, α)
have the exact same meaning as in (Li and Croft, 2003)
but were tuned according to our base ranking function.
With regards to the approach by (Diaz and Jones, 2004),
we ranked the web pages in decreasing order of discov-
ery time. Our own approaches were tested under options
with and without using adaptation. For no adaption, we
let the e of Eq.1 equal to 0, meaning no score difference
between the oldest document and the newest document
was captured, but a constant value was used. It is equiv-
alent to an open loop in Fig.1. For adaption, we used the
ranking errors to adjust the base ranking. In the Table we
used multiple ks to show the effect of changing k. Using
different k can have a big impact on the performance. The
best value we found was k = 0.3. In this experiment, we
let α(q) = 0 so that the result responds to k only.
Our approach is significantly better than the existing
methods. Both of the two existing methods produced
worse results than the baseline, which shows the ap-
</bodyText>
<page confidence="0.9896">
167
</page>
<table confidence="0.998440875">
Li &amp; Croft (A, α)=(0.2,2.0) -0.5
(A, α)=(0.2,4.0) -1.2
Diaz &amp; Jones -4.5∗
No adaptation (e = 0, k=0.3 1.2
open loop)
k=0.4 0.8
Adaptation (closed loop) k=0.3 6.6∗
k=0.4 6.2∗
</table>
<tableCaption confidence="0.975119666666667">
Table 2: %Adcg of proposed method comparing with
existing methods.A sign “∗” indicates statistical signifi-
cance (p-value&lt;0.05)
</tableCaption>
<table confidence="0.991014">
A 0 0.2 0.4 0.6 0.8 1.0
%Adcg 6.6∗ 7.8∗ 8.4∗ 4.5 2.1 -0.2∗
</table>
<tableCaption confidence="0.999837">
Table 3: Effect of confidence as changing A.
</tableCaption>
<bodyText confidence="0.998173142857143">
proaches may be inappropriate for Web search. Not sur-
prisingly, using adaption achieved much better results
than without using adaption. Thus, these experiments
prove the effectiveness of our proposed methods.
Another important parameter in the Eq.1 is the confi-
dence score α(q), which indicates the confidence of query
to be YQQ. In Eq. 1, A is used to adjusting α(q). We
observed dcg gain for each different A. The results are
shown in Table 3. The value of A needs to be tuned for
different base ranking functions. A higher A can hurt per-
formance. In our experiments, the best value of 0.4 gave a
8.4% statistically significant gain in DCG. The A = 0 set-
ting means we turn off confidence, which results in lower
performance. Thus, using YQQ confidence is effective.
</bodyText>
<sectionHeader confidence="0.9805" genericHeader="conclusions">
5 Discussions and conclusions
</sectionHeader>
<bodyText confidence="0.999769">
In this paper, we proposed a novel approach to solve
YQQ ranking problem, which is a problem that seems
to plague most major commercial search engines. Our
approach for handling YQQs does not involve any query
expansion that adds a year to the query. Instead, keeping
the user’s query intact, we re-rank search results by ad-
justing the base ranking function. Our work assumes the
intent of YQQs is to find documents about the most recent
year. For this reason, we use YQQ confidence to measure
the probability of this intent. As our results showed, our
proposed method is highly effective. A real example is
given in Fig. 2 to show the significant improvement by
our method.
Our adaptive methods are not limited to YQQs only.
We believe this framework can be applied to any category
of queries once a query classification and a score detector
have been implemented.
</bodyText>
<figureCaption confidence="0.9914615">
Figure 2: Ranking improvement for query ICML by our
method: before re-rank(left) and after(right)
</figureCaption>
<sectionHeader confidence="0.997509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998654138888889">
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ’06, pages 19–26.
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Proc.
27th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, pages 18–24, New
York, NY, USA. ACM.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In ICML ’98: Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 170–178.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ’02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133–
142.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based
language models. In Proc. 12th Intl. Conf. on Infor-
mation and Knowledge Management, pages 469–475,
New York, NY, USA. ACM.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005.
Personalizing search via automated analysis of inter-
ests and activities. In SIGIR ’05, pages 449–456.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179–
214.
Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan
Zha. 2007. A regression framework for learning rank-
ing functions using relative relevance judgments. In
SIGIR ’07, pages 287–294.
</reference>
<page confidence="0.997283">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407654">
<title confidence="0.732527">Search Result Re-ranking by Feedback Control Adjustment Time-sensitive Query</title>
<address confidence="0.9379825">Labs, 701 First Avenue, Sunnyvale, of Montreal, Montreal, Quebec,H3C 3J7,</address>
<abstract confidence="0.998129736842105">We propose a new method to rank a special category of time-sensitive queries that are year qualified. The method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher. Our method, which is based on feedback control theory, uses ranking errors to adjust the search engine behavior. For this purpose, we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Eric Brill</author>
<author>Susan Dumais</author>
</authors>
<title>Improving web search ranking by incorporating user behavior information.</title>
<date>2006</date>
<booktitle>In SIGIR ’06,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3766" citStr="Agichtein et al., 2006" startWordPosition="551" endWordPosition="554">or example, “sigir2008” has more links and clicks than “sigir2009” because “sigir2008” has existed longer time and therefore has been visited more. It is less likely that newer web pages from “sigir2009” can be ranked higher using features that implicitly favor old pages. However, the fundamental problem is that current approaches have focused on improving general ranking algorithms. Methods for improving ranking of specific types of query like temporal queries are often overlooked. Aiming to improve ranking results, some methods of re-ranking search results are proposed, such as the work by (Agichtein et al., 2006) and (Teevan et al., 2005). 165 Proceedings of NAACL HLT 2009: Short Papers, pages 165–168, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Controller Search Engine error R(q, yo) + _ Detector R(q, yn) Figure 1: Feedback control for search engine These work uses user search behavior information or personalization information as features that are integrated into an enhanced ranking model. We propose a novel method of re-ranking search results. This new method is based on feedback control theory, as illustrated in 1. We make a Detector to monitor search engine (SE)</context>
</contexts>
<marker>Agichtein, Brill, Dumais, 2006</marker>
<rawString>Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving web search ranking by incorporating user behavior information. In SIGIR ’06, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Diaz</author>
<author>Rosie Jones</author>
</authors>
<title>Using temporal profiles of queries for precision prediction.</title>
<date>2004</date>
<booktitle>In Proc. 27th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval,</booktitle>
<pages>18--24</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13337" citStr="Diaz and Jones, 2004" startWordPosition="2242" endWordPosition="2245">ed on language modeling; second, it is impossible to obtain exact timestamp for web pages as (Li and Croft, 2003) did in the track evaluation. However, we tried to simulate (Li and Croft, 2003)’s approach in web search by using the linear integration method exactly as the same as(Li and Croft, 2003) by adding a time-based function with our base ranking function. For the timestamp, we used discovery time in the time-based function. The parameters (λ, α) have the exact same meaning as in (Li and Croft, 2003) but were tuned according to our base ranking function. With regards to the approach by (Diaz and Jones, 2004), we ranked the web pages in decreasing order of discovery time. Our own approaches were tested under options with and without using adaptation. For no adaption, we let the e of Eq.1 equal to 0, meaning no score difference between the oldest document and the newest document was captured, but a constant value was used. It is equivalent to an open loop in Fig.1. For adaption, we used the ranking errors to adjust the base ranking. In the Table we used multiple ks to show the effect of changing k. Using different k can have a big impact on the performance. The best value we found was k = 0.3. In t</context>
</contexts>
<marker>Diaz, Jones, 2004</marker>
<rawString>Fernando Diaz and Rosie Jones. 2004. Using temporal profiles of queries for precision prediction. In Proc. 27th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 18–24, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj D Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>170--178</pages>
<contexts>
<context position="1404" citStr="Freund et al., 1998" startWordPosition="200" endWordPosition="203"> extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed. 1 Introduction Relevance ranking plays a crucial role in search engines. There are many proposed machine learning based ranking algorithms such as language modeling-based methods (Zhai and Lafferty, 2004), RankSVM (Joachims, 2002), RankBoost (Freund et al., 1998) and GBrank (Zheng et al., 2007). The input to these algorithms is a set of feature vectors extracted from queries and documents. The goal is to find the parameter setting that optimizes some relevance metric given training data. While these machine learning algorithms can improve average relevance, they may be ineffctive for certain special cases. Time-sensitive queries are one such special case that machine-learned ranking functions may have a hard time learning, due to the small number of such queries. Consider the query “sigir” (the name of a conference), which is time sensitive. Table 1 s</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. 1998. An efficient boosting algorithm for combining preferences. In ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning, pages 170–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>20--2002</pages>
<contexts>
<context position="12464" citStr="Jarvelin and Kekalainen, 2002" startWordPosition="2092" endWordPosition="2095">ch results for each of queries using the base ranking function and the proposed ranking function. We asked human editors to judge all the scraped results. We used five judgment grades: Perfect, Excellent, Good, Fair, and Bad. Editors were instructed to consider temporal issues when judging. For example, sigir2004 is given a worse grade than sigir2009. To avoid bias, we advised editors to retain relevance as their primary judgment criteria. Our evaluation metric is relative change in DCG, %Δ dg DCGproposed−DCGbaseline where DCG is dcg = DCGbaseline , the traditional Discounted Cumulative Gain (Jarvelin and Kekalainen, 2002). 4.1 Effect of the proposed boosting method Our experimental results are shown in Table 2, where we compared our work with the existing methods. While we cannot apply (Li and Croft, 2003)’s approach directly because first, our search engine is not based on language modeling; second, it is impossible to obtain exact timestamp for web pages as (Li and Croft, 2003) did in the track evaluation. However, we tried to simulate (Li and Croft, 2003)’s approach in web search by using the linear integration method exactly as the same as(Li and Croft, 2003) by adding a time-based function with our base r</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20:2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1371" citStr="Joachims, 2002" startWordPosition="197" endWordPosition="198">mple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed. 1 Introduction Relevance ranking plays a crucial role in search engines. There are many proposed machine learning based ranking algorithms such as language modeling-based methods (Zhai and Lafferty, 2004), RankSVM (Joachims, 2002), RankBoost (Freund et al., 1998) and GBrank (Zheng et al., 2007). The input to these algorithms is a set of feature vectors extracted from queries and documents. The goal is to find the parameter setting that optimizes some relevance metric given training data. While these machine learning algorithms can improve average relevance, they may be ineffctive for certain special cases. Time-sensitive queries are one such special case that machine-learned ranking functions may have a hard time learning, due to the small number of such queries. Consider the query “sigir” (the name of a conference), w</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyan Li</author>
<author>W Bruce Croft</author>
</authors>
<title>Time-based language models.</title>
<date>2003</date>
<booktitle>In Proc. 12th Intl. Conf. on Information and Knowledge Management,</booktitle>
<pages>469--475</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12652" citStr="Li and Croft, 2003" startWordPosition="2124" endWordPosition="2127">lent, Good, Fair, and Bad. Editors were instructed to consider temporal issues when judging. For example, sigir2004 is given a worse grade than sigir2009. To avoid bias, we advised editors to retain relevance as their primary judgment criteria. Our evaluation metric is relative change in DCG, %Δ dg DCGproposed−DCGbaseline where DCG is dcg = DCGbaseline , the traditional Discounted Cumulative Gain (Jarvelin and Kekalainen, 2002). 4.1 Effect of the proposed boosting method Our experimental results are shown in Table 2, where we compared our work with the existing methods. While we cannot apply (Li and Croft, 2003)’s approach directly because first, our search engine is not based on language modeling; second, it is impossible to obtain exact timestamp for web pages as (Li and Croft, 2003) did in the track evaluation. However, we tried to simulate (Li and Croft, 2003)’s approach in web search by using the linear integration method exactly as the same as(Li and Croft, 2003) by adding a time-based function with our base ranking function. For the timestamp, we used discovery time in the time-based function. The parameters (λ, α) have the exact same meaning as in (Li and Croft, 2003) but were tuned according</context>
</contexts>
<marker>Li, Croft, 2003</marker>
<rawString>Xiaoyan Li and W. Bruce Croft. 2003. Time-based language models. In Proc. 12th Intl. Conf. on Information and Knowledge Management, pages 469–475, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Teevan</author>
<author>Susan T Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>Personalizing search via automated analysis of interests and activities.</title>
<date>2005</date>
<booktitle>In SIGIR ’05,</booktitle>
<pages>449--456</pages>
<contexts>
<context position="3792" citStr="Teevan et al., 2005" startWordPosition="556" endWordPosition="559">ore links and clicks than “sigir2009” because “sigir2008” has existed longer time and therefore has been visited more. It is less likely that newer web pages from “sigir2009” can be ranked higher using features that implicitly favor old pages. However, the fundamental problem is that current approaches have focused on improving general ranking algorithms. Methods for improving ranking of specific types of query like temporal queries are often overlooked. Aiming to improve ranking results, some methods of re-ranking search results are proposed, such as the work by (Agichtein et al., 2006) and (Teevan et al., 2005). 165 Proceedings of NAACL HLT 2009: Short Papers, pages 165–168, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Controller Search Engine error R(q, yo) + _ Detector R(q, yn) Figure 1: Feedback control for search engine These work uses user search behavior information or personalization information as features that are integrated into an enhanced ranking model. We propose a novel method of re-ranking search results. This new method is based on feedback control theory, as illustrated in 1. We make a Detector to monitor search engine (SE) output and compare it wit</context>
</contexts>
<marker>Teevan, Dumais, Horvitz, 2005</marker>
<rawString>Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing search via automated analysis of interests and activities. In SIGIR ’05, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>214</pages>
<contexts>
<context position="1345" citStr="Zhai and Lafferty, 2004" startWordPosition="192" endWordPosition="195">vior. For this purpose, we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed. 1 Introduction Relevance ranking plays a crucial role in search engines. There are many proposed machine learning based ranking algorithms such as language modeling-based methods (Zhai and Lafferty, 2004), RankSVM (Joachims, 2002), RankBoost (Freund et al., 1998) and GBrank (Zheng et al., 2007). The input to these algorithms is a set of feature vectors extracted from queries and documents. The goal is to find the parameter setting that optimizes some relevance metric given training data. While these machine learning algorithms can improve average relevance, they may be ineffctive for certain special cases. Time-sensitive queries are one such special case that machine-learned ranking functions may have a hard time learning, due to the small number of such queries. Consider the query “sigir” (th</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Zheng</author>
<author>Keke Chen</author>
<author>Gordon Sun</author>
<author>Hongyuan Zha</author>
</authors>
<title>A regression framework for learning ranking functions using relative relevance judgments.</title>
<date>2007</date>
<booktitle>In SIGIR ’07,</booktitle>
<pages>287--294</pages>
<contexts>
<context position="1436" citStr="Zheng et al., 2007" startWordPosition="206" endWordPosition="209">y mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed. 1 Introduction Relevance ranking plays a crucial role in search engines. There are many proposed machine learning based ranking algorithms such as language modeling-based methods (Zhai and Lafferty, 2004), RankSVM (Joachims, 2002), RankBoost (Freund et al., 1998) and GBrank (Zheng et al., 2007). The input to these algorithms is a set of feature vectors extracted from queries and documents. The goal is to find the parameter setting that optimizes some relevance metric given training data. While these machine learning algorithms can improve average relevance, they may be ineffctive for certain special cases. Time-sensitive queries are one such special case that machine-learned ranking functions may have a hard time learning, due to the small number of such queries. Consider the query “sigir” (the name of a conference), which is time sensitive. Table 1 shows two example search result p</context>
</contexts>
<marker>Zheng, Chen, Sun, Zha, 2007</marker>
<rawString>Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan Zha. 2007. A regression framework for learning ranking functions using relative relevance judgments. In SIGIR ’07, pages 287–294.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>