<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001147">
<title confidence="0.998434">
Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models
</title>
<author confidence="0.997691">
Stefan Thater and Hagen Fürstenau and Manfred Pinkal
</author>
<affiliation confidence="0.997453">
Department of Computational Linguistics
Saarland University
</affiliation>
<email confidence="0.996196">
{stth, hagenf, pinkal}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831923076923">
We present a syntactically enriched vec-
tor model that supports the computation
of contextualized semantic representations
in a quasi compositional fashion. It em-
ploys a systematic combination of first- and
second-order context vectors. We apply
our model to two different tasks and show
that (i) it substantially outperforms previ-
ous work on a paraphrase ranking task, and
(ii) achieves promising results on a word-
sense similarity task; to our knowledge, it is
the first time that an unsupervised method
has been applied to this task.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841307692308">
In the logical paradigm of natural-language seman-
tics originating from Montague (1973), semantic
structure, composition and entailment have been
modelled to an impressive degree of detail and
formal consistency. These approaches, however,
lack coverage and robustness, and their impact
on realistic natural-language applications is lim-
ited: The logical framework suffers from over-
specificity, and is inappropriate to model the per-
vasive vagueness, ambivalence, and uncertainty
of natural-language semantics. Also, the hand-
crafting of resources covering the huge amounts
of content which are required for deep semantic
processing is highly inefficient and expensive.
Co-occurrence-based semantic vector models of-
fer an attractive alternative. In the standard ap-
proach, word meaning is represented by feature
vectors, with large sets of context words as dimen-
sions, and their co-occurrence frequencies as val-
ues. Semantic similarity information can be ac-
quired using unsupervised methods at virtually no
cost, and the information gained is soft and gradual.
Many NLP tasks have been modelled successfully
using vector-based models. Examples include in-
formation retrieval (Manning et al., 2008), word-
sense discrimination (Schütze, 1998) and disam-
biguation (McCarthy and Carroll, 2003), to name
but a few.
Standard vector-space models have serious lim-
itations, however: While semantic information is
typically encoded in phrases and sentences, distri-
butional semantics, in sharp contrast to logic-based
semantics, does not offer any natural concept of
compositionality that would allow the semantics
of a complex expression to be computed from the
meaning of its parts. A different, but related prob-
lem is caused by word-sense ambiguity and con-
textual variation of usage. Frequency counts of
context words for a given target word provide in-
variant representations averaging over all different
usages of the target word. There is no obvious way
to distinguish the different senses of e.g. acquire
in different contexts, such as acquire knowledge or
acquire shares.
Several approaches for word-sense disambigua-
tion in the framework of distributional semantics
have been proposed in the literature (Schütze, 1998;
McCarthy and Carroll, 2003). In contrast to these
approaches, we present a method to model the mu-
tual contextualization of words in a phrase in a com-
positional way, guided by syntactic structure. To
some extent, our method resembles the approaches
proposed by Mitchell and Lapata (2008) and Erk
and Padó (2008). We go one step further, however,
in that we employ syntactically enriched vector
models as the basic meaning representations, as-
suming a vector space spanned by combinations
of dependency relations and words (Lin, 1998).
This allows us to model the semantic interaction
between the meaning of a head word and its de-
pendent at the micro-level of relation-specific co-
occurrence frequencies. It turns out that the benefit
to precision is considerable.
Using syntactically enriched vector models
raises problems of different kinds: First, the use
</bodyText>
<page confidence="0.962157">
948
</page>
<note confidence="0.943897">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959634146341">
of syntax increases dimensionality and thus may
cause data sparseness (Padó and Lapata, 2007).
Second, the vectors of two syntactically related
words, e.g., a target verb acquire and its direct ob-
ject knowledge, typically have different syntactic
environments, which implies that their vector repre-
sentations encode complementary information and
there is no direct way of combining the information
encoded in the respective vectors.
To solve these problems, we build upon pre-
vious work (Thater et al., 2009) and propose to
use syntactic second-order vector representations.
Second-order vector representations in a bag-of-
words setting were first used by SchUtze (1998);
in a syntactic setting, they also feature in Dligach
and Palmer (2008). For the problem at hand, the
use of second-order vectors alleviates the sparse-
ness problem, and enables the definition of vector
space transformations that make the distributional
information attached to words in different syntactic
positions compatible. Thus, it allows vectors for
a predicate and its arguments to be combined in a
compositional way.
We conduct two experiments to assess the suit-
ability of our method. Our first experiment is car-
ried out on the SemEval 2007 lexical substitution
task dataset (McCarthy and Navigli, 2007). It will
show that our method significantly outperforms
other unsupervised methods that have been pro-
posed in the literature to rank words with respect
to their semantic similarity in a given linguistic
context. In a second experiment, we apply our
model to the “word sense similarity task” recently
proposed by Erk and McCarthy (2009), which is
a refined variant of a word-sense disambiguation
task. The results show a substantial positive effect.
Plan of the paper. We will first review related
work in Section 2, before presenting our model in
Section 3. In Sections 4 and 5 we evaluate our
model on the two different tasks. Section 6 con-
cludes.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962406779661">
Several approaches to contextualize vector repre-
sentations of word meaning have been proposed.
One common approach is to represent the mean-
ing of a word a in context b simply as the sum, or
centroid of a and b (Landauer and Dumais, 1997).
Kintsch (2001) considers a variant of this simple
model. By using vector representations of a predi-
cate p and an argument a, Kintsch identifies words
that are similar to p and a, and takes the centroid
of these words’ vectors to be the representation of
the complex expression p(a).
Mitchell and Lapata (2008), henceforth M&amp;L,
propose a general framework in which meaning rep-
resentations for complex expressions are computed
compositionally by combining the vector represen-
tations of the individual words of the complex ex-
pression. They focus on the assessment of different
operations combining the vectors of the subexpres-
sions. An important finding is that component-wise
multiplication outperforms the more common addi-
tion method. Although their composition method
is guided by syntactic structure, the actual instanti-
ations of M&amp;L’s framework are insensitive to syn-
tactic relations and word-order, assigning identical
representation to dog bites man and man bites dog
(see Erk and Padó (2008) for a discussion). Also,
they use syntax-free bag-of-words-based vectors as
basic representations of word meaning.
Erk and Padó (2008), henceforth E&amp;P, represent
the meaning of a word w through a collection of
vectors instead of a single vector: They assume
selectional preferences and inverse selectional pref-
erences to be constitutive parts of the meaning in
addition to the meaning proper. The interpretation
of a word p in context a is a combination of p’s
meaning with the (inverse) selectional preference
of a. Thus, a verb meaning does not combine di-
rectly with the meaning of its object noun, as on
the M&amp;L account, but with the centroid of the vec-
tors of the verbs to which the noun can stand in an
object relation. Clearly, their approach is sensitive
to syntactic structure. Their evaluation shows that
their model outperforms the one proposed by M&amp;L
on a lexical substitution task (see Section 4). The
basic vectors, however, are constructed in a word
space similar to the one of the M&amp;L approach.
In Thater et al. (2009), henceforth TDP, we took
up the basic idea from E&amp;P of exploiting selec-
tional preference information for contextualization.
Instead of using collections of different vectors,
we incorporated syntactic information by assuming
a richer internal structure of the vector represen-
tations. In a small case study, moderate improve-
ments over E&amp;P on a lexical substitution task could
be shown. In the present paper, we formulate a
general model of syntactically informed contextu-
alization and show how to apply it to a number a
of representative lexical substitution tasks. Eval-
uation shows significant improvements over TDP
</bodyText>
<page confidence="0.999024">
949
</page>
<figureCaption confidence="0.988273">
Figure 1: Co-occurrence graph of a small sample
corpus of dependency trees.
</figureCaption>
<bodyText confidence="0.869568">
and E&amp;P.
</bodyText>
<sectionHeader confidence="0.995945" genericHeader="method">
3 The model
</sectionHeader>
<bodyText confidence="0.999732833333333">
In this section, we present our method of contex-
tualizing semantic vector representations. We first
give an overview of the main ideas, which is fol-
lowed by a technical description of first-order and
second-order vectors (Section 3.2) and the contex-
tualization operation (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.987915">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999985411764706">
Our model employs vector representations for
words and expressions containing syntax-specific
first and second order co-occurrences information.
The basis for the construction of both kinds of
vector representations are co-occurrence graphs.
Figure 1 shows the co-occurrence graph of a small
sample corpus of dependency trees: Words are
represented as nodes in the graph, possible depen-
dency relations between them are drawn as labeled
edges, with weights corresponding to the observed
frequencies. From this graph, we can directly read
off the first-order vector for every word w: the vec-
tor’s dimensions correspond to pairs (r,w&apos;) of a
grammatical relation and a neighboring word, and
are assigned the frequency count of (w,r,w&apos;).
The noun knowledge, for instance, would be rep-
resented by the following vector:
</bodyText>
<equation confidence="0.615357">
(5(OBJ−1,gain),2(CONJ−1,skill),3(OBJ−1,acquire),...)
</equation>
<bodyText confidence="0.99977">
This vector talks about the possible dependency
heads of knowledge and thus can be seen as the
(inverse) selectional preference of knowledge (see
Erk and Padó (2008)).
As soon as we want to compute a meaning rep-
resentation for a phrase like acquire knowledge
from the verb acquire together with its direct ob-
ject knowledge, we are facing the problem that
verbs have different syntactic neighbors than nouns,
hence their first-order vectors are not easily com-
parable. To solve this problem we additionally
introduce another kind of vectors capturing infor-
mations about all words that can be reached with
two steps in the co-occurrence graph. Such a path
is characterized by two dependency relations and
two words, i.e., a quadruple (r,w&apos;,r&apos;,w&apos;&apos;), whose
weight is the product of the weights of the two
edges used in the path. To avoid overly sparse vec-
tors we generalize over the “middle word” w&apos; and
build our second-order vectors on the dimensions
corresponding to triples (r,r&apos;,w&apos;&apos;) of two depen-
dency relations and one word at the end of the two-
step path. For instance, the second-order vector for
acquire is
</bodyText>
<equation confidence="0.78832675">
(15(OBJ,OBJ−1,gain),
6(OBJ,CONJ−1,skill),
6(OBJ,OBJ−1,buy-back),
42(OBJ,OBJ−1,purchase),...)
</equation>
<bodyText confidence="0.99692415625">
In this simple example, the values are the prod-
ucts of the edge weights on each of the paths. The
method of computation is detailed in Section 3.2.
Note that second order vectors in particular con-
tain paths of the form (r,r−1,w&apos;), relating a verb
w to other verbs w&apos; which are possible substitution
candidates.
With first- and second-order vectors we can
now model the interaction of semantic informa-
tion within complex expressions. Given a pair
of words in a particular grammatical relation like
acquire knowledge, we contextualize the second-
order vector of acquire with the first-order vec-
tor of knowledge. We let the first-order vector
with its selectional preference information act as a
kind of weighting filter on the second-order vector,
and thus refine the meaning representation of the
verb. The actual operation we will use is point-
wise multiplication, which turned out to be the
best-performing one for our purpose. Interestingly,
Mitchell and Lapata (2008) came to the same result
in a different setting.
In our example, we obtain a new second-order
vector for acquire in the context of knowledge:
(75(OBJ,OBJ−1,gain),
12(OBJ,CONJ−1,skill),
0(OBJ,OBJ−1,buy-back),
0(OBJ,OBJ−1,purchase),...)
Note that all dimensions that are not “licensed” by
the argument knowledge are filtered out as they are
multiplied with 0. Also, contextualisation of ac-
quire with the argument share instead of knowledge
</bodyText>
<equation confidence="0.903787">
gainVB skillNN acquireVB buy-backNN purchaseVB
knowlegeNN shareNN
conj, 2 nn, 1
obj, 5 obj, 3 obj, 6 obj, 7
</equation>
<page confidence="0.936712">
950
</page>
<bodyText confidence="0.999921">
would have led to a very different vector, which
reflects the fact that the two argument nouns induce
different readings of the inherently ambiguous ac-
quire.
</bodyText>
<subsectionHeader confidence="0.999749">
3.2 First and second-order vectors
</subsectionHeader>
<bodyText confidence="0.9995218">
Assuming a set W of words and a set R of depen-
dency relation labels, we consider a Euclidean vec-
tor space V1 spanned by the set of orthonormal
basis vectors {~er,w0  |r ∈ R,w0 ∈ W}, i.e., a vector
space whose dimensions correspond to pairs of a re-
lation and a word. Recall that any vector of V1 can
be represented as a finite sum of the form ∑ai~er,w0
with appropriate scalar factors ai. In this vector
space we define the first-order vector [w] of a word
w as follows:
</bodyText>
<equation confidence="0.998622333333333">
[w] = ∑ ω(w,r,w0) ·~er,w0
r∈R
w0∈W
</equation>
<bodyText confidence="0.999859875">
where ω is a function that assigns the dependency
triple (w,r,w0) a corresponding weight. In the sim-
plest case, ω would denote the frequency in a cor-
pus of dependency trees of w occurring together
with w0 in relation r. In the experiments reported be-
low, we use pointwise mutual information (Church
and Hanks, 1990) instead as it proved superior to
raw frequency counts:
</bodyText>
<equation confidence="0.9879055">
pmi(w,r,w0) = log p(w,w0  |r)
p(w  |r)p(w0  |r)
</equation>
<bodyText confidence="0.999946545454545">
We further consider a similarly defined vec-
tor space V2, spanned by an orthonormal basis
{~er,r0,w0  |r,r0 ∈ R,w0 ∈ W}. Its dimensions there-
fore correspond to triples of two relations and a
word. Evidently this is a higher dimensional space
than V1, which therefore can be embedded into
V2 by the “lifting maps” Lr : V1 ,→ V2 defined by
Lr(~er0,w0) :=~er,r0,w0 (and by linear extension there-
fore on all vectors of V1). Using these lifting maps
we define the second-order vector [[w]] of a word w
as
</bodyText>
<equation confidence="0.993716333333333">
[[w]] = ∑ ω(w,r,w0)·Lr([w0]�
r∈R
w0∈W
</equation>
<bodyText confidence="0.9478025">
Substituting the definitions of Lr and [w0], this
yields
</bodyText>
<equation confidence="0.992187">
[[w]] = ∑ ∑ Iω(w,r,w0)ω(w0,r0,w00) ~er,r0,w00
r,r0∈R w0∈W
w00∈W
</equation>
<bodyText confidence="0.999710857142857">
which shows the generalization over w0 in form of
the inner sum.
For example, if w is a verb, r = OBJ and r0 =
OBJ−1 (i.e., the inverse object relation), then the
coefficients of ~er,r0,w00 in [[w]] would characterize
the distribution of verbs w00 which share objects
with w.
</bodyText>
<subsectionHeader confidence="0.996431">
3.3 Composition
</subsectionHeader>
<bodyText confidence="0.999955">
Both first and second-order vectors are defined for
lexical expressions only. In order to represent the
meaning of complex expressions we need to com-
bine the vectors for grammatically related words
in a given sentence. Given two words w and w0 in
relation r we contextualize the second-order vector
of w with the r-lifted first-order vector of w0:
</bodyText>
<equation confidence="0.919434">
[[wr:w0]] = [[w]] × Lr([w0])
</equation>
<bodyText confidence="0.999970764705882">
Here × may denote any operator on V2. The ob-
jective is to incorporate (inverse) selectional pref-
erence information from the context (r,w0) in such
a way as to identify the correct word sense of w.
This suggests that the dimensions of [[w]] should
be filtered so that only those compatible with the
context remain. A more flexible approach than
simple filtering, however, is to re-weight those di-
mensions with context information. This can be
expressed by pointwise vector multiplication (in
terms of the given basis of V2). We therefore take
× to be pointwise multiplication.
To contextualize (the vector of) a word w with
multiple words w1,...,wn and corresponding rela-
tions r1,...,rn, we compute the sum of the results
of the pairwise contextualizations of the target vec-
tor with the vectors of the respective dependents:
</bodyText>
<equation confidence="0.910794">
[[wr1:w1,...,rn:wn]] =
</equation>
<sectionHeader confidence="0.991676" genericHeader="method">
4 Experiments: Ranking Paraphrases
</sectionHeader>
<bodyText confidence="0.99997">
In this section, we evaluate our model on a para-
phrase ranking task. We consider sentences with
an occurrence of some target word w and a list of
paraphrase candidates w1,...,wk such that each of
the wi is a paraphrase of w for some sense of w.
The task is to decide for each of the paraphrase
candidates wi how appropriate it is as a paraphrase
of w in the given context. For instance, buy, pur-
chase and obtain are all paraphrases of acquire, in
the sense that they can be substituted for acquire in
some contexts, but purchase and buy are not para-
phrases of acquire in the first sentence of Table 1.
</bodyText>
<equation confidence="0.87210075">
n
∑
k=1
[[wrk:wk]]
</equation>
<page confidence="0.952035">
951
</page>
<subsectionHeader confidence="0.275753">
Sentence Paraphrases
</subsectionHeader>
<bodyText confidence="0.691523666666667">
Teacher education students will acquire the knowl-
edge and skills required to [... ]
Ontario Inc. will [... ] acquire the remaining IXOS
shares [... ]
gain 4; amass 1; receive 1; obtain 1
buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1
</bodyText>
<tableCaption confidence="0.99579">
Table 1: Two examples from the lexical substitution task data set
</tableCaption>
<subsectionHeader confidence="0.937762">
4.1 Resources
</subsectionHeader>
<bodyText confidence="0.999993636363636">
We use a vector model based on dependency trees
obtained from parsing the English Gigaword corpus
(LDC2003T05). The corpus consists of news from
several newswire services, and contains over four
million documents. We parse the corpus using the
Stanford parser1 (de Marneffe et al., 2006) and a
non-lexicalized parser model, and extract over 1.4
billion dependency triples for about 3.9 million
words (lemmas) from the parsed corpus.
To evaluate the performance of our model, we
use various subsets of the SemEval 2007 lexical
substitution task (McCarthy and Navigli, 2007)
dataset. The complete dataset contains 10 instances
for each of 200 target words—nouns, verbs, adjec-
tives and adverbs—in different sentential contexts.
Systems that participated in the task had to generate
paraphrases for every instance, and were evaluated
against a gold standard containing up to 10 possible
paraphrases for each of the individual instances.
There are two natural subtasks in generating
paraphrases: identifying paraphrase candidates and
ranking them according to the context. We follow
E&amp;P and evaluate it only on the second subtask:
we extract paraphrase candidates from the gold
standard by pooling all annotated gold-standard
paraphrases for all instances of a verb in all con-
texts, and use our model to rank these paraphrase
candidates in specific contexts. Table 1 shows two
instances of the target verb acquire together with
its paraphrases in the gold standard as an example.
The paraphrases are attached with weights, which
correspond to the number of times they have been
given by different annotators.
</bodyText>
<subsectionHeader confidence="0.954162">
4.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.927944076923077">
To evaluate the performance of our method we use
generalized average precision (Kishida, 2005), a
1We use version 1.6 of the parser. We modify the depen-
dency trees by “folding” prepositions into the edge labels to
make the relation between a head word and the head noun of
a prepositional phrase explicit.
variant of average precision.
Average precision (Buckley and Voorhees, 2000)
is a measure commonly used to evaluate systems
that return ranked lists of results. Generalized aver-
age precision (GAP) additionally rewards the cor-
rect order of positive cases w.r.t. their gold standard
weight. We define average precision first:
</bodyText>
<equation confidence="0.9921275">
R pi = Σik=1xk
i
</equation>
<bodyText confidence="0.99990175">
where xi is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n is the number of paraphrase candidates to
be ranked. If we take xi to be the gold standard
weight of the ith item or zero if it is not in the
gold standard, we can define generalized average
precision as follows:
</bodyText>
<equation confidence="0.9863225">
∑n i=1 I(xi) pi
GAP = ∑R i=1 I(yi)yi
</equation>
<bodyText confidence="0.9999892">
where I(xi) = 1 if xi is larger than zero, zero oth-
erwise, and yi is the average weight of the ideal
ranked list y1,...,yi of gold standard paraphrases.
As a second scoring method, we use precision
out of ten (P10). The measure is less discriminative
than GAP. We use it because we want to compare
our model with E&amp;P. P10 measures the percentage
of gold-standard paraphrases in the top-ten list of
paraphrases as ranked by the system, and can be
defined as follows (McCarthy and Navigli, 2007):
</bodyText>
<equation confidence="0.816094">
ΣscMnG f(s)
Σsc-G f (s) ,
</equation>
<bodyText confidence="0.99985925">
where M is the list of 10 paraphrase candidates top-
ranked by the model, G is the corresponding anno-
tated gold-standard data, and f (s) is the weight of
the individual paraphrases.
</bodyText>
<subsectionHeader confidence="0.991641">
4.3 Experiment 1: Verb paraphrases
</subsectionHeader>
<bodyText confidence="0.999983">
In our first experiment, we consider verb para-
phrases using the same controlled subset of the
</bodyText>
<equation confidence="0.9961635">
AP =
Σn
i=1xi pi
P10 =
</equation>
<page confidence="0.981073">
952
</page>
<bodyText confidence="0.998258795918368">
lexical substitution task data that had been used by
TDP in an earlier study. We compare our model
to various baselines and the models of TDP and
E&amp;P, and show that our new model substantially
outperforms previous work.
Dataset. The dataset is identical to the one used
by TDP and has been constructed in the same way
as the dataset used by E&amp;P: it contains those gold-
standard instances of verbs that have—according
to the analyses produced by the MiniPar parser
(Lin, 1993)—an overtly realized subject and object.
Gold-standard paraphrases that do not occur in the
parsed British National Corpus are removed.2 In
total, the dataset contains 162 instances for 34 dif-
ferent verbs. On average, target verbs have 20.5
substitution candidates; for individual instances of
a target verb, an average of 3.9 of the substitution
candidates are annotated as correct paraphrases.
Below, we will refer to this dataset as “LST/SO.”
Experimental procedure. To compute the vec-
tor space, we consider only a subset of the complete
set of dependency triples extracted from the parsed
Gigaword corpus. We experimented with various
strategies, and found that models which consider
all dependency triples exceeding certain pmi- and
frequency thresholds perform best.
Since the dataset is rather small, we use a four-
fold cross-validation method for parameter tuning:
We divide the dataset into four subsets, test vari-
ous parameter settings on one subset and use the
parameters that perform best (in terms of GAP) to
evaluate the model on the three other subsets. We
consider the following parameters: pmi-thresholds
for the dependency triples used in the computa-
tion of the first- and second-order vectors, and
frequency thresholds. The parameters differ only
slightly between the four subsets, and the general
tendency is that good results are obtained if a low
pmi-threshold (G 2) is applied to filter dependency
triples used in the computation of the second-order
vectors, and a relatively high pmi-threshold (&gt; 4)
to filter dependency triples in the computation of
the first-order vectors. Good performing frequency
thresholds are 10 or 15. The threshold values for
context vectors are slightly different: a medium
pmi-threshold between 2 and 4 and a low frequency
threshold of 3.
To rank paraphrases in context, we compute con-
textualized vectors for the verb in the input sen-
</bodyText>
<footnote confidence="0.500261">
2Both TDP and E&amp;P use the British National Corpus.
</footnote>
<bodyText confidence="0.986988942307692">
tence, i.e., a second order vector for the verb that
is contextually constrained by the first order vec-
tors of all its arguments, and compare them to the
unconstrained (second-order) vectors of each para-
phrase candidate, using cosine similarity.3 For the
first sentence in Table 1, for example, we compute
[[acquireSUB.►:student,OB.►:knowledge]] and compare it to
[[gain]],[[amass]],[[buy]],[[purchase]] and so on.
Baselines. We evaluate our model against a ran-
dom baseline and two variants of our model: One
variant (“2nd order uncontexualized”) simply uses
contextually unconstrained second-order vectors
to rank paraphrase candidates. Comparing the full
model to this variant will show how effective our
method of contextualizing vectors is. The sec-
ond variant (“1st order contextualized”) represents
verbs in context by their first order vectors that
specify how often the verb co-occurs with its argu-
ments in the parsed Gigaword corpus. We compare
our model to this baseline to demonstrate the bene-
fit of (contextualized) second-order vectors. As for
the full model, we use pmi values rather than raw
frequency counts as co-occurrence statistics.
Results. For the LST/SO dataset, the generalized
average precision, averaged over all instances in the
dataset, is 45.94%, and the average P10 is 73.11%.
Table 2 compares our model to the random base-
line, the two variants of our model, and previous
work. As can be seen, our model improves about
8% in terms of GAP and almost 7% in terms of
P10 upon the two variants of our model, which in
turn perform 10% above the random baseline. We
conclude that both the use of second-order vectors,
as well as the method used to contextualize them,
are very effective for the task under consideration.
The table also compares our model to the model
of TDP and two different instantiations of E&amp;P’s
model. The results for these three models are cited
from Thater et al. (2009). We can observe that
our model improves about 9% in terms of GAP
and about 7% in terms of P10 upon previous work.
Note that the results for the E&amp;P models are based
3Note that the context information is the same for both
words. With our choice of pointwise multiplication for the
composition operator x we have (v1 xw) ·v2 =v1 · (v2 xw).
Therefore the choice of which word is contextualized does not
strongly influence their cosine similarity, and contextualizing
both should not add any useful information. On the contrary
we found that it even lowers performance. Although this
could be repaired by appropriately modifying the operator x,
for this experiment we stick with the easier solution of only
contextualizing one of the words.
</bodyText>
<page confidence="0.996526">
953
</page>
<table confidence="0.999582875">
Model GAP P10
Random baseline 26.03 54.25
E&amp;P (add, object) 29.93 66.20
E&amp;P (min, subject &amp; object) 32.22 64.86
TDP 36.54 63.32
1st order contextualized 36.09 59.35
2nd order uncontextualized 37.65 66.32
Full model 45.94 73.11
</table>
<tableCaption confidence="0.999426">
Table 2: Results of Experiment 1
</tableCaption>
<bodyText confidence="0.99913672">
on a reimplementation of E&amp;P’s original model—
the P10-scores reported by Erk and Padó (2009)
range between 60.2 and 62.3, over a slightly lower
random baseline.
According to a paired t-test the differences are
statistically significant at p &lt; 0.01.
Performance on the complete dataset. To find
out how our model performs on less controlled
datasets, we extracted all instances from the lexical
substitution task dataset with a verb target, exclud-
ing only instances which could not be parsed by
the Stanford parser, or in which the target was mis-
tagged as a non-verb by the parser. The resulting
dataset contains 496 instances. As for the LST/SO
dataset, we ignore all gold-standard paraphrases
that do not occur in the parsed (Gigaword) corpus.
If we use the best-performing parameters from
the first experiment, we obtain a GAP score of
45.17% and a P10-score of 75.43%, compared to
random baselines of 27.42% (GAP) and 58.83%
(P10). The performance on this larger dataset is
thus almost the same compared to our results for
the more controlled dataset. We take this as evi-
dence that our model is quite robust w.r.t. different
realizations of a verb’s subcategorization frame.
</bodyText>
<subsectionHeader confidence="0.999552">
4.4 Experiment 2: Non-verb paraphrases
</subsectionHeader>
<bodyText confidence="0.999388363636364">
We now apply our model to parts of speech (POS)
other than verbs. The main difference between
verbs on the one hand, and nouns, adjectives, and
adverbs on the other hand, is that verbs typically
come with a rich context—subject, object, and so
on—while non-verbs often have either no depen-
dents at all or only closed class dependents such as
determiners which provide only limited contextual
informations, if any at all. While we can apply the
same method as before also to non-verbs, we might
expect it to work less well due to limited contextual
</bodyText>
<table confidence="0.9990255">
POS Instances M1 M2 Baseline
Noun 535 46.38 42.54 30.01
Adj 508 39.41 43.21 28.32
Adv 284 48.19 51.43 37.25
</table>
<tableCaption confidence="0.928394">
Table 3: GAP-scores for non-verb paraphrases us-
ing two different methods.
</tableCaption>
<bodyText confidence="0.980946771428571">
information.
We therefore propose an alternative method to
rank non-verb paraphrases: We take the second-
order vector of the target’s head and contextually
constrain it by the first order vector of the target.
For instance, if we want to rank the paraphrase
candidates hint and star for the noun lead in the
sentence
(1) Meet for coffee early, swap leads and get per-
mission to contact if possible.
we compute [[swapOBJ:lead]] and compare it to the
lifted first-order vectors of all paraphrase candi-
dates, LOBJ([hint]) and LOBJ([star]), using cosine
similarity.
To evaluate the performance of the two methods,
we extract all instances from the lexical substitution
task dataset with a nominal, adjectival, or adverbial
target, excluding instances with incorrect parse or
no parse at all. As before, we ignore gold-standard
paraphrases that do not occur in the parsed Giga-
word corpus.
The results are shown in Table 3, where “M1”
refers to the method we used before on verbs, and
“M2” refers to the alternative method described
above. As one can see, M1 achieves better results
than M2 if applied to nouns, while M2 is better
than M1 if applied to adjectives and adverbs. The
second result is unsurprising, as adjectives and ad-
verbs often have no dependents at all.
We can observe that the performance of our
model is similarly strong on non-verbs. GAP scores
on nouns (using M1) and adverbs are even higher
than those on verbs. We take these results to show
that our model can be successfully applied to all
open word classes.
</bodyText>
<sectionHeader confidence="0.997302" genericHeader="method">
5 Experiment: Ranking Word Senses
</sectionHeader>
<bodyText confidence="0.934075333333333">
In this section, we apply our model to a different
word sense ranking task: Given a word w in context,
the task is to decide to what extent the different
</bodyText>
<page confidence="0.9959">
954
</page>
<bodyText confidence="0.9997607">
WordNet (Fellbaum, 1998) senses of w apply to
this occurrence of w.
Dataset. We use the dataset provided by Erk and
McCarthy (2009). The dataset contains ordinal
judgments of the applicability of WordNet senses
on a 5 point scale, ranging from completely differ-
ent to identical for eight different lemmas in 50
different sentential contexts. In this experiment,
we concentrate on the three verbs in the dataset:
ask, add and win.
Experimental procedure. Similar to Pennac-
chiotti et al. (2008), we represent different word
senses by the words in the corresponding synsets.
For each word sense, we compute the centroid of
the second-order vectors of its synset members.
Since synsets tend to be small (they even may con-
tain only the target word itself), we additionally
add the centroid of the sense’s hypernyms, scaled
down by the factor 10 (chosen as a rough heuristic
without any attempt at optimization).
We apply the same method as in Section 4.3:
For each instance in the dataset, we compute the
second-order vector of the target verb, contextually
constrain it by the first-order vectors of the verb’s
arguments, and compare the resulting vector to
the vectors that represent the different WordNet
senses of the verb. The WordNet senses are then
ranked according to the cosine similarity between
their sense vector and the contextually constrained
target verb vector.
To compare the predicted ranking to the gold-
standard ranking, we use Spearman’s p, a standard
method to compare ranked lists to each other. We
compute p between the similarity scores averaged
over all three annotators and our model’s predic-
tions. Based on agreement between human judges,
Erk and McCarthy (2009) estimate an upper bound
p of 0.544 for the dataset.
Results. Table 4 shows the results of our exper-
iment. The first column shows the correlation of
our model’s predictions with the human judgments
from the gold-standard, averaged over all instances.
All correlations are significant (p &lt; 0.001) as tested
by approximate randomization (Noreen, 1989).
The second column shows the results of a
frequency-informed baseline, which predicts the
ranking based on the order of the senses in Word-
Net. This (weakly supervised) baseline outper-
forms our unsupervised model for two of the three
verbs. As a final step, we explored the effect of
</bodyText>
<table confidence="0.9992346">
Word Present paper WN-Freq Combined
ask 0.344 0.369 0.431
add 0.256 0.164 0.270
win 0.236 0.343 0.381
average 0.279 0.291 0.361
</table>
<tableCaption confidence="0.9626365">
Table 4: Correlation of model predictions and hu-
man judgments
</tableCaption>
<bodyText confidence="0.99797075">
combining our rankings with those of the frequency
baseline, by simply computing the average ranks
of those two models. The results are shown in the
third column. Performance is significantly higher
than for both the original model and the frequency-
informed baseline. This shows that our model cap-
tures an additional kind of information, and thus
can be used to improve the frequency-based model.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999891133333333">
We have presented a novel method for adapting
the vector representations of words according to
their context. In contrast to earlier approaches, our
model incorporates detailed syntactic information.
We solved the problems of data sparseness and
incompatibility of dimensions which are inherent in
this approach by modeling contextualization as an
interplay between first- and second-order vectors.
Evaluating on the SemEval 2007 lexical substitu-
tion task dataset, our model performs substantially
better than all earlier approaches, exceeding the
state of the art by around 9% in terms of general-
ized average precision and around 7% in terms of
precision out of ten. Also, our system is the first un-
supervised method that has been applied to Erk and
McCarthy’s (2009) graded word sense assignment
task, showing a substantial positive correlation with
the gold standard. We further showed that a weakly
supervised heuristic, making use of WordNet sense
ranks, can be significantly improved by incorporat-
ing information from our system.
We studied the effect that context has on target
words in a series of experiments, which vary the
target word and keep the context constant. A natu-
ral objective for further research is the influence of
varying contexts on the meaning of target expres-
sions. This extension might also shed light on the
status of the modelled semantic process, which we
have been referring to in this paper as “contextu-
alization”. This process can be considered one of
</bodyText>
<page confidence="0.995468">
955
</page>
<bodyText confidence="0.999941526315789">
mutual disambiguation, which is basically the view
of E&amp;P. Alternatively, one can conceptualize it as
semantic composition: in particular, the head of a
phrase incorporates semantic information from its
dependents, and the final result may to some extent
reflect the meaning of the whole phrase.
Another direction for further study will be the
generalization of our model to larger syntactic con-
texts, including more than only the direct neighbors
in the dependency graph, ultimately incorporating
context information from the whole sentence in a
recursive fashion.
Acknowledgments. We would like to thank Ed-
uard Hovy and Georgiana Dinu for inspiring discus-
sions and helpful comments. This work was sup-
ported by the Cluster of Excellence “Multimodal
Computing and Interaction”, funded by the Ger-
man Excellence Initiative, and the project SALSA,
funded by DFG (German Science Foundation).
</bodyText>
<sectionHeader confidence="0.999103" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933458823529">
Chris Buckley and Ellen M. Voorhees. 2000. Evaluat-
ing evaluation measure stability. In Proceedings of
the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 33–40, Athens, Greece.
Kenneth W. Church and Patrick Hanks. 1990. Word
association, mutual information and lexicography.
Computational Linguistics, 16(1):22–29.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the fifth international conference on
Language Resources and Evaluation (LREC 2006),
pages 449–454, Genoa, Italy.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
Proceedings of ACL-08: HLT, Short Papers, pages
29–32, Columbus, OH, USA.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440–449, Singapore.
Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Honolulu,
HI, USA.
Katrin Erk and Sebastian Padó. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proc. of the Workshop
on Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. Bradford Book.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173–202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211–240.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of the 31st Annual
Meeting of the Association for Computational Lin-
guistics, pages 112–120, Columbus, OH, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768–774.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proc. of SemEval, Prague, Czech Republic.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, OH,
USA.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Jaakko Hin-
tikka, Julius Moravcsik, and Patrick Suppes, editors,
Approaches to Natural Language, pages 221–242.
Dordrecht.
Eric W. Noreen. 1989. Computer-intensive Methods
for Testing Hypotheses: An Introduction. John Wi-
ley and Sons Inc.
Sebastian Padó and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 457–465, Hon-
olulu, HI, USA.
</reference>
<page confidence="0.985595">
956
</page>
<reference confidence="0.99873">
Hinrich Schütze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97–124.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44–47, Singapore.
</reference>
<page confidence="0.997573">
957
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802933">
<title confidence="0.9979915">Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</title>
<author confidence="0.869483">Thater Fürstenau Pinkal</author>
<affiliation confidence="0.99908">Department of Computational Linguistics Saarland University</affiliation>
<email confidence="0.934934">stth@coli.uni-saarland.de</email>
<email confidence="0.934934">hagenf@coli.uni-saarland.de</email>
<email confidence="0.934934">pinkal@coli.uni-saarland.de</email>
<abstract confidence="0.99921">We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of firstand second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Evaluating evaluation measure stability.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>33--40</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="19168" citStr="Buckley and Voorhees, 2000" startWordPosition="3047" endWordPosition="3050">of the target verb acquire together with its paraphrases in the gold standard as an example. The paraphrases are attached with weights, which correspond to the number of times they have been given by different annotators. 4.2 Evaluation metrics To evaluate the performance of our method we use generalized average precision (Kishida, 2005), a 1We use version 1.6 of the parser. We modify the dependency trees by “folding” prepositions into the edge labels to make the relation between a head word and the head noun of a prepositional phrase explicit. variant of average precision. Average precision (Buckley and Voorhees, 2000) is a measure commonly used to evaluate systems that return ranked lists of results. Generalized average precision (GAP) additionally rewards the correct order of positive cases w.r.t. their gold standard weight. We define average precision first: R pi = Σik=1xk i where xi is a binary variable indicating whether the ith item as ranked by the model is in the gold standard or not, R is the size of the gold standard, and n is the number of paraphrase candidates to be ranked. If we take xi to be the gold standard weight of the ith item or zero if it is not in the gold standard, we can define gener</context>
</contexts>
<marker>Buckley, Voorhees, 2000</marker>
<rawString>Chris Buckley and Ellen M. Voorhees. 2000. Evaluating evaluation measure stability. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33–40, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="13883" citStr="Church and Hanks, 1990" startWordPosition="2167" endWordPosition="2170">space whose dimensions correspond to pairs of a relation and a word. Recall that any vector of V1 can be represented as a finite sum of the form ∑ai~er,w0 with appropriate scalar factors ai. In this vector space we define the first-order vector [w] of a word w as follows: [w] = ∑ ω(w,r,w0) ·~er,w0 r∈R w0∈W where ω is a function that assigns the dependency triple (w,r,w0) a corresponding weight. In the simplest case, ω would denote the frequency in a corpus of dependency trees of w occurring together with w0 in relation r. In the experiments reported below, we use pointwise mutual information (Church and Hanks, 1990) instead as it proved superior to raw frequency counts: pmi(w,r,w0) = log p(w,w0 |r) p(w |r)p(w0 |r) We further consider a similarly defined vector space V2, spanned by an orthonormal basis {~er,r0,w0 |r,r0 ∈ R,w0 ∈ W}. Its dimensions therefore correspond to triples of two relations and a word. Evidently this is a higher dimensional space than V1, which therefore can be embedded into V2 by the “lifting maps” Lr : V1 ,→ V2 defined by Lr(~er0,w0) :=~er,r0,w0 (and by linear extension therefore on all vectors of V1). Using these lifting maps we define the second-order vector [[w]] of a word w as [</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association, mutual information and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), pages 449–454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Novel semantic features for verb sense disambiguation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>29--32</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="4793" citStr="Dligach and Palmer (2008)" startWordPosition="704" endWordPosition="707">of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by SchUtze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms oth</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Novel semantic features for verb sense disambiguation. In Proceedings of ACL-08: HLT, Short Papers, pages 29–32, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded word sense assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>440--449</pages>
<contexts>
<context position="5669" citStr="Erk and McCarthy (2009)" startWordPosition="842" endWordPosition="845">le. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms other unsupervised methods that have been proposed in the literature to rank words with respect to their semantic similarity in a given linguistic context. In a second experiment, we apply our model to the “word sense similarity task” recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a varia</context>
<context position="29824" citStr="Erk and McCarthy (2009)" startWordPosition="4833" endWordPosition="4836">erbs often have no dependents at all. We can observe that the performance of our model is similarly strong on non-verbs. GAP scores on nouns (using M1) and adverbs are even higher than those on verbs. We take these results to show that our model can be successfully applied to all open word classes. 5 Experiment: Ranking Word Senses In this section, we apply our model to a different word sense ranking task: Given a word w in context, the task is to decide to what extent the different 954 WordNet (Fellbaum, 1998) senses of w apply to this occurrence of w. Dataset. We use the dataset provided by Erk and McCarthy (2009). The dataset contains ordinal judgments of the applicability of WordNet senses on a 5 point scale, ranging from completely different to identical for eight different lemmas in 50 different sentential contexts. In this experiment, we concentrate on the three verbs in the dataset: ask, add and win. Experimental procedure. Similar to Pennacchiotti et al. (2008), we represent different word senses by the words in the corresponding synsets. For each word sense, we compute the centroid of the second-order vectors of its synset members. Since synsets tend to be small (they even may contain only the </context>
<context position="31377" citStr="Erk and McCarthy (2009)" startWordPosition="5084" endWordPosition="5087">n it by the first-order vectors of the verb’s arguments, and compare the resulting vector to the vectors that represent the different WordNet senses of the verb. The WordNet senses are then ranked according to the cosine similarity between their sense vector and the contextually constrained target verb vector. To compare the predicted ranking to the goldstandard ranking, we use Spearman’s p, a standard method to compare ranked lists to each other. We compute p between the similarity scores averaged over all three annotators and our model’s predictions. Based on agreement between human judges, Erk and McCarthy (2009) estimate an upper bound p of 0.544 for the dataset. Results. Table 4 shows the results of our experiment. The first column shows the correlation of our model’s predictions with the human judgments from the gold-standard, averaged over all instances. All correlations are significant (p &lt; 0.001) as tested by approximate randomization (Noreen, 1989). The second column shows the results of a frequency-informed baseline, which predicts the ranking based on the order of the senses in WordNet. This (weakly supervised) baseline outperforms our unsupervised model for two of the three verbs. As a final</context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded word sense assignment. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 440–449, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="3317" citStr="Erk and Padó (2008)" startWordPosition="484" endWordPosition="487"> the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Assoc</context>
<context position="7233" citStr="Erk and Padó (2008)" startWordPosition="1097" endWordPosition="1100">sentations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. They focus on the assessment of different operations combining the vectors of the subexpressions. An important finding is that component-wise multiplication outperforms the more common addition method. Although their composition method is guided by syntactic structure, the actual instantiations of M&amp;L’s framework are insensitive to syntactic relations and word-order, assigning identical representation to dog bites man and man bites dog (see Erk and Padó (2008) for a discussion). Also, they use syntax-free bag-of-words-based vectors as basic representations of word meaning. Erk and Padó (2008), henceforth E&amp;P, represent the meaning of a word w through a collection of vectors instead of a single vector: They assume selectional preferences and inverse selectional preferences to be constitutive parts of the meaning in addition to the meaning proper. The interpretation of a word p in context a is a combination of p’s meaning with the (inverse) selectional preference of a. Thus, a verb meaning does not combine directly with the meaning of its object noun</context>
<context position="10315" citStr="Erk and Padó (2008)" startWordPosition="1581" endWordPosition="1584">em are drawn as labeled edges, with weights corresponding to the observed frequencies. From this graph, we can directly read off the first-order vector for every word w: the vector’s dimensions correspond to pairs (r,w&apos;) of a grammatical relation and a neighboring word, and are assigned the frequency count of (w,r,w&apos;). The noun knowledge, for instance, would be represented by the following vector: (5(OBJ−1,gain),2(CONJ−1,skill),3(OBJ−1,acquire),...) This vector talks about the possible dependency heads of knowledge and thus can be seen as the (inverse) selectional preference of knowledge (see Erk and Padó (2008)). As soon as we want to compute a meaning representation for a phrase like acquire knowledge from the verb acquire together with its direct object knowledge, we are facing the problem that verbs have different syntactic neighbors than nouns, hence their first-order vectors are not easily comparable. To solve this problem we additionally introduce another kind of vectors capturing informations about all words that can be reached with two steps in the co-occurrence graph. Such a path is characterized by two dependency relations and two words, i.e., a quadruple (r,w&apos;,r&apos;,w&apos;&apos;), whose weight is the</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>Katrin Erk and Sebastian Padó. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="26121" citStr="Erk and Padó (2009)" startWordPosition="4207" endWordPosition="4210"> not add any useful information. On the contrary we found that it even lowers performance. Although this could be repaired by appropriately modifying the operator x, for this experiment we stick with the easier solution of only contextualizing one of the words. 953 Model GAP P10 Random baseline 26.03 54.25 E&amp;P (add, object) 29.93 66.20 E&amp;P (min, subject &amp; object) 32.22 64.86 TDP 36.54 63.32 1st order contextualized 36.09 59.35 2nd order uncontextualized 37.65 66.32 Full model 45.94 73.11 Table 2: Results of Experiment 1 on a reimplementation of E&amp;P’s original model— the P10-scores reported by Erk and Padó (2009) range between 60.2 and 62.3, over a slightly lower random baseline. According to a paired t-test the differences are statistically significant at p &lt; 0.01. Performance on the complete dataset. To find out how our model performs on less controlled datasets, we extracted all instances from the lexical substitution task dataset with a verb target, excluding only instances which could not be parsed by the Stanford parser, or in which the target was mistagged as a non-verb by the parser. The resulting dataset contains 496 instances. As for the LST/SO dataset, we ignore all gold-standard paraphrase</context>
</contexts>
<marker>Erk, Padó, 2009</marker>
<rawString>Katrin Erk and Sebastian Padó. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proc. of the Workshop on Geometrical Models of Natural Language Semantics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>Bradford Book.</publisher>
<contexts>
<context position="4721" citStr="(1998)" startWordPosition="695" endWordPosition="695">rseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by SchUtze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Na</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet: An Electronic Lexical Database. Bradford Book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="6251" citStr="Kintsch (2001)" startWordPosition="947" endWordPosition="948">posed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a variant of this simple model. By using vector representations of a predicate p and an argument a, Kintsch identifies words that are similar to p and a, and takes the centroid of these words’ vectors to be the representation of the complex expression p(a). Mitchell and Lapata (2008), henceforth M&amp;L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. They focus on the assessment of different operations combining the vectors of the </context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments.</title>
<date>2005</date>
<tech>NII Technical Report.</tech>
<contexts>
<context position="18880" citStr="Kishida, 2005" startWordPosition="3001" endWordPosition="3002">he second subtask: we extract paraphrase candidates from the gold standard by pooling all annotated gold-standard paraphrases for all instances of a verb in all contexts, and use our model to rank these paraphrase candidates in specific contexts. Table 1 shows two instances of the target verb acquire together with its paraphrases in the gold standard as an example. The paraphrases are attached with weights, which correspond to the number of times they have been given by different annotators. 4.2 Evaluation metrics To evaluate the performance of our method we use generalized average precision (Kishida, 2005), a 1We use version 1.6 of the parser. We modify the dependency trees by “folding” prepositions into the edge labels to make the relation between a head word and the head noun of a prepositional phrase explicit. variant of average precision. Average precision (Buckley and Voorhees, 2000) is a measure commonly used to evaluate systems that return ranked lists of results. Generalized average precision (GAP) additionally rewards the correct order of positive cases w.r.t. their gold standard weight. We define average precision first: R pi = Σik=1xk i where xi is a binary variable indicating whethe</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments. NII Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="6235" citStr="Landauer and Dumais, 1997" startWordPosition="943" endWordPosition="946">imilarity task” recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a variant of this simple model. By using vector representations of a predicate p and an argument a, Kintsch identifies words that are similar to p and a, and takes the centroid of these words’ vectors to be the representation of the complex expression p(a). Mitchell and Lapata (2008), henceforth M&amp;L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. They focus on the assessment of different operations combining the</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>112--120</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="21173" citStr="Lin, 1993" startWordPosition="3413" endWordPosition="3414">rb paraphrases In our first experiment, we consider verb paraphrases using the same controlled subset of the AP = Σn i=1xi pi P10 = 952 lexical substitution task data that had been used by TDP in an earlier study. We compare our model to various baselines and the models of TDP and E&amp;P, and show that our new model substantially outperforms previous work. Dataset. The dataset is identical to the one used by TDP and has been constructed in the same way as the dataset used by E&amp;P: it contains those goldstandard instances of verbs that have—according to the analyses produced by the MiniPar parser (Lin, 1993)—an overtly realized subject and object. Gold-standard paraphrases that do not occur in the parsed British National Corpus are removed.2 In total, the dataset contains 162 instances for 34 different verbs. On average, target verbs have 20.5 substitution candidates; for individual instances of a target verb, an average of 3.9 of the substitution candidates are annotated as correct paraphrases. Below, we will refer to this dataset as “LST/SO.” Experimental procedure. To compute the vector space, we consider only a subset of the complete set of dependency triples extracted from the parsed Gigawor</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>Dekang Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 112–120, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<contexts>
<context position="3538" citStr="Lin, 1998" startWordPosition="520" endWordPosition="521"> distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapat</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1991" citStr="Manning et al., 2008" startWordPosition="279" endWordPosition="282">of content which are required for deep semantic processing is highly inefficient and expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of con</context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2081" citStr="McCarthy and Carroll, 2003" startWordPosition="291" endWordPosition="294"> expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide invariant representations averaging over all di</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<date>2007</date>
<booktitle>SemEval2007 Task 10: English Lexical Substitution Task. In Proc. of SemEval,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5333" citStr="McCarthy and Navigli, 2007" startWordPosition="788" endWordPosition="791"> SchUtze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms other unsupervised methods that have been proposed in the literature to rank words with respect to their semantic similarity in a given linguistic context. In a second experiment, we apply our model to the “word sense similarity task” recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model</context>
<context position="17731" citStr="McCarthy and Navigli, 2007" startWordPosition="2823" endWordPosition="2826">m the lexical substitution task data set 4.1 Resources We use a vector model based on dependency trees obtained from parsing the English Gigaword corpus (LDC2003T05). The corpus consists of news from several newswire services, and contains over four million documents. We parse the corpus using the Stanford parser1 (de Marneffe et al., 2006) and a non-lexicalized parser model, and extract over 1.4 billion dependency triples for about 3.9 million words (lemmas) from the parsed corpus. To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. The complete dataset contains 10 instances for each of 200 target words—nouns, verbs, adjectives and adverbs—in different sentential contexts. Systems that participated in the task had to generate paraphrases for every instance, and were evaluated against a gold standard containing up to 10 possible paraphrases for each of the individual instances. There are two natural subtasks in generating paraphrases: identifying paraphrase candidates and ranking them according to the context. We follow E&amp;P and evaluate it only on the second subtask: we extract paraphrase candidates from the gold</context>
<context position="20335" citStr="McCarthy and Navigli, 2007" startWordPosition="3261" endWordPosition="3264"> zero if it is not in the gold standard, we can define generalized average precision as follows: ∑n i=1 I(xi) pi GAP = ∑R i=1 I(yi)yi where I(xi) = 1 if xi is larger than zero, zero otherwise, and yi is the average weight of the ideal ranked list y1,...,yi of gold standard paraphrases. As a second scoring method, we use precision out of ten (P10). The measure is less discriminative than GAP. We use it because we want to compare our model with E&amp;P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007): ΣscMnG f(s) Σsc-G f (s) , where M is the list of 10 paraphrase candidates topranked by the model, G is the corresponding annotated gold-standard data, and f (s) is the weight of the individual paraphrases. 4.3 Experiment 1: Verb paraphrases In our first experiment, we consider verb paraphrases using the same controlled subset of the AP = Σn i=1xi pi P10 = 952 lexical substitution task data that had been used by TDP in an earlier study. We compare our model to various baselines and the models of TDP and E&amp;P, and show that our new model substantially outperforms previous work. Dataset. The dat</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval2007 Task 10: English Lexical Substitution Task. In Proc. of SemEval, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="3293" citStr="Mitchell and Lapata (2008)" startWordPosition="479" endWordPosition="482">ng over all different usages of the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Ann</context>
<context position="6546" citStr="Mitchell and Lapata (2008)" startWordPosition="996" endWordPosition="999">ate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a variant of this simple model. By using vector representations of a predicate p and an argument a, Kintsch identifies words that are similar to p and a, and takes the centroid of these words’ vectors to be the representation of the complex expression p(a). Mitchell and Lapata (2008), henceforth M&amp;L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. They focus on the assessment of different operations combining the vectors of the subexpressions. An important finding is that component-wise multiplication outperforms the more common addition method. Although their composition method is guided by syntactic structure, the actual instantiations of M&amp;L’s framework are insensitive to syntactic relations and word-order, assigni</context>
<context position="12321" citStr="Mitchell and Lapata (2008)" startWordPosition="1901" endWordPosition="1904">ctors we can now model the interaction of semantic information within complex expressions. Given a pair of words in a particular grammatical relation like acquire knowledge, we contextualize the secondorder vector of acquire with the first-order vector of knowledge. We let the first-order vector with its selectional preference information act as a kind of weighting filter on the second-order vector, and thus refine the meaning representation of the verb. The actual operation we will use is pointwise multiplication, which turned out to be the best-performing one for our purpose. Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting. In our example, we obtain a new second-order vector for acquire in the context of knowledge: (75(OBJ,OBJ−1,gain), 12(OBJ,CONJ−1,skill), 0(OBJ,OBJ−1,buy-back), 0(OBJ,OBJ−1,purchase),...) Note that all dimensions that are not “licensed” by the argument knowledge are filtered out as they are multiplied with 0. Also, contextualisation of acquire with the argument share instead of knowledge gainVB skillNN acquireVB buy-backNN purchaseVB knowlegeNN shareNN conj, 2 nn, 1 obj, 5 obj, 3 obj, 6 obj, 7 950 would have led to a very different vector, which r</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>Approaches to Natural Language,</booktitle>
<pages>221--242</pages>
<editor>In Jaakko Hintikka, Julius Moravcsik, and Patrick Suppes, editors,</editor>
<location>Dordrecht.</location>
<contexts>
<context position="886" citStr="Montague (1973)" startWordPosition="123" endWordPosition="124">cally enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task. 1 Introduction In the logical paradigm of natural-language semantics originating from Montague (1973), semantic structure, composition and entailment have been modelled to an impressive degree of detail and formal consistency. These approaches, however, lack coverage and robustness, and their impact on realistic natural-language applications is limited: The logical framework suffers from overspecificity, and is inappropriate to model the pervasive vagueness, ambivalence, and uncertainty of natural-language semantics. Also, the handcrafting of resources covering the huge amounts of content which are required for deep semantic processing is highly inefficient and expensive. Co-occurrence-based </context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary English. In Jaakko Hintikka, Julius Moravcsik, and Patrick Suppes, editors, Approaches to Natural Language, pages 221–242. Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley and Sons Inc.</publisher>
<contexts>
<context position="31726" citStr="Noreen, 1989" startWordPosition="5141" endWordPosition="5142">ndard ranking, we use Spearman’s p, a standard method to compare ranked lists to each other. We compute p between the similarity scores averaged over all three annotators and our model’s predictions. Based on agreement between human judges, Erk and McCarthy (2009) estimate an upper bound p of 0.544 for the dataset. Results. Table 4 shows the results of our experiment. The first column shows the correlation of our model’s predictions with the human judgments from the gold-standard, averaged over all instances. All correlations are significant (p &lt; 0.001) as tested by approximate randomization (Noreen, 1989). The second column shows the results of a frequency-informed baseline, which predicts the ranking based on the order of the senses in WordNet. This (weakly supervised) baseline outperforms our unsupervised model for two of the three verbs. As a final step, we explored the effect of Word Present paper WN-Freq Combined ask 0.344 0.369 0.431 add 0.256 0.164 0.270 win 0.236 0.343 0.381 average 0.279 0.291 0.361 Table 4: Correlation of model predictions and human judgments combining our rankings with those of the frequency baseline, by simply computing the average ranks of those two models. The re</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive Methods for Testing Hypotheses: An Introduction. John Wiley and Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4146" citStr="Padó and Lapata, 2007" startWordPosition="607" endWordPosition="610">ds (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by SchUtze (1998); in a syntactic setting,</context>
</contexts>
<marker>Padó, Lapata, 2007</marker>
<rawString>Sebastian Padó and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Diego De Cao</author>
<author>Roberto Basili</author>
<author>Danilo Croce</author>
<author>Michael Roth</author>
</authors>
<title>Automatic induction of framenet lexical units.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>457--465</pages>
<location>Honolulu, HI, USA.</location>
<marker>Pennacchiotti, De Cao, Basili, Croce, Roth, 2008</marker>
<rawString>Marco Pennacchiotti, Diego De Cao, Roberto Basili, Danilo Croce, and Michael Roth. 2008. Automatic induction of framenet lexical units. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457–465, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2033" citStr="Schütze, 1998" startWordPosition="286" endWordPosition="287">rocessing is highly inefficient and expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Hinrich Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Georgiana Dinu</author>
<author>Manfred Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<contexts>
<context position="4560" citStr="Thater et al., 2009" startWordPosition="670" endWordPosition="673">l Linguistics, pages 948–957, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by SchUtze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct tw</context>
<context position="8267" citStr="Thater et al. (2009)" startWordPosition="1272" endWordPosition="1275">f a word p in context a is a combination of p’s meaning with the (inverse) selectional preference of a. Thus, a verb meaning does not combine directly with the meaning of its object noun, as on the M&amp;L account, but with the centroid of the vectors of the verbs to which the noun can stand in an object relation. Clearly, their approach is sensitive to syntactic structure. Their evaluation shows that their model outperforms the one proposed by M&amp;L on a lexical substitution task (see Section 4). The basic vectors, however, are constructed in a word space similar to the one of the M&amp;L approach. In Thater et al. (2009), henceforth TDP, we took up the basic idea from E&amp;P of exploiting selectional preference information for contextualization. Instead of using collections of different vectors, we incorporated syntactic information by assuming a richer internal structure of the vector representations. In a small case study, moderate improvements over E&amp;P on a lexical substitution task could be shown. In the present paper, we formulate a general model of syntactically informed contextualization and show how to apply it to a number a of representative lexical substitution tasks. Evaluation shows significant impro</context>
<context position="25026" citStr="Thater et al. (2009)" startWordPosition="4024" endWordPosition="4027"> 73.11%. Table 2 compares our model to the random baseline, the two variants of our model, and previous work. As can be seen, our model improves about 8% in terms of GAP and almost 7% in terms of P10 upon the two variants of our model, which in turn perform 10% above the random baseline. We conclude that both the use of second-order vectors, as well as the method used to contextualize them, are very effective for the task under consideration. The table also compares our model to the model of TDP and two different instantiations of E&amp;P’s model. The results for these three models are cited from Thater et al. (2009). We can observe that our model improves about 9% in terms of GAP and about 7% in terms of P10 upon previous work. Note that the results for the E&amp;P models are based 3Note that the context information is the same for both words. With our choice of pointwise multiplication for the composition operator x we have (v1 xw) ·v2 =v1 · (v2 xw). Therefore the choice of which word is contextualized does not strongly influence their cosine similarity, and contextualizing both should not add any useful information. On the contrary we found that it even lowers performance. Although this could be repaired b</context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44–47, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>