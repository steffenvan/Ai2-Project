<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.061877">
<title confidence="0.99559">
Gestural Cohesion for Topic Segmentation
</title>
<author confidence="0.998722">
Jacob Eisenstein, Regina Barzilay and Randall Davis
</author>
<affiliation confidence="0.844777666666667">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
</affiliation>
<email confidence="0.994952">
{jacobe, regina, davis}@csail.mit.edu
</email>
<sectionHeader confidence="0.995582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997664384615384">
This paper explores the relationship between
discourse segmentation and coverbal gesture.
Introducing the idea of gestural cohesion, we
show that coherent topic segments are char-
acterized by homogeneous gestural forms and
that changes in the distribution of gestural
features predict segment boundaries. Gestu-
ral features are extracted automatically from
video, and are combined with lexical features
in a Bayesian generative model. The resulting
multimodal system outperforms text-only seg-
mentation on both manual and automatically-
recognized speech transcripts.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951288461539">
When people communicate face-to-face, discourse
cues are expressed simultaneously through multiple
channels. Previous research has extensively studied
how discourse cues correlate with lexico-syntactic
and prosodic features (Hearst, 1994; Hirschberg and
Nakatani, 1998; Passonneau and Litman, 1997); this
work informs various text and speech processing
applications, such as automatic summarization and
segmentation. Gesture is another communicative
modality that frequently accompanies speech, yet it
has not been exploited for computational discourse
analysis.
This paper empirically demonstrates that gesture
correlates with discourse structure. In particular,
we show that automatically-extracted visual fea-
tures can be combined with lexical cues in a sta-
tistical model to predict topic segmentation, a fre-
quently studied form of discourse structure. Our
method builds on the idea that coherent discourse
segments are characterized by gestural cohesion; in
other words, that such segments exhibit homoge-
neous gestural patterns. Lexical cohesion (Halliday
and Hasan, 1976) forms the backbone of many ver-
bal segmentation algorithms, on the theory that seg-
mentation boundaries should be placed where the
distribution of words changes (Hearst, 1994). With
gestural cohesion, we explore whether the same idea
holds for gesture features.
The motivation for this approach comes from a
series of psycholinguistic studies suggesting that
gesture supplements speech with meaningful and
unique semantic content (McNeill, 1992; Kendon,
2004). We assume that repeated patterns in gesture
are indicative of the semantic coherence that charac-
terizes well-defined discourse segments. An advan-
tage of this view is that gestures can be brought to
bear on discourse analysis without undertaking the
daunting task of recognizing and interpreting indi-
vidual gestures. This is crucial because coverbal
gesture – unlike formal sign language – rarely fol-
lows any predefined form or grammar, and may vary
dramatically by speaker.
A key implementational challenge is automati-
cally extracting gestural information from raw video
and representing it in a way that can applied to dis-
course analysis. We employ a representation of vi-
sual codewords, which capture clusters of low-level
motion patterns. For example, one codeword may
correspond to strong left-right motion in the up-
per part of the frame. These codewords are then
treated similarly to lexical items; our model iden-
tifies changes in their distribution, and predicts topic
</bodyText>
<page confidence="0.972495">
852
</page>
<note confidence="0.71542">
Proceedings of ACL-08: HLT, pages 852–860,
</note>
<page confidence="0.537719">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999695933333333">
boundaries appropriately. The overall framework is
implemented as a hierarchical Bayesian model, sup-
porting flexible integration of multiple knowledge
sources.
Experimental results support the hypothesis that
gestural cohesion is indicative of discourse struc-
ture. Applying our algorithm to a dataset of face-
to-face dialogues, we find that gesture commu-
nicates unique information, improving segmenta-
tion performance over lexical features alone. The
positive impact of gesture is most pronounced
when automatically-recognized speech transcripts
are used, but gestures improve performance by a
significant margin even in combination with manual
transcripts.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999911883116883">
Gesture and discourse Much of the work on ges-
ture in natural language processing has focused
on multimodal dialogue systems in which the ges-
tures and speech may be constrained, e.g. (Johnston,
1998). In contrast, we focus on improving discourse
processing on unconstrained natural language be-
tween humans. This effort follows basic psycho-
logical and linguistic research on the communicative
role of gesture (McNeill, 1992; Kendon, 2004), in-
cluding some efforts that made use of automatically
acquired visual features (Quek, 2003). We extend
these empirical studies with a statistical model of the
relationship between gesture and discourse segmen-
tation.
Hand-coded descriptions of body posture shifts
and eye gaze behavior have been shown to correlate
with topic and turn boundaries in task-oriented dia-
logue (Cassell et al., 2001). These findings are ex-
ploited to generate realistic conversational “ground-
ing” behavior in an animated agent. The seman-
tic content of gesture was leveraged – again, for
gesture generation – in (Kopp et al., 2007), which
presents an animated agent that is capable of aug-
menting navigation directions with gestures that de-
scribe the physical properties of landmarks along
the route. Both systems generate plausible and
human-like gestural behavior; we address the con-
verse problem of interpreting such gestures.
In this vein, hand-coded gesture features have
been used to improve sentence segmentation, show-
ing that sentence boundaries are unlikely to over-
lap gestures that are in progress (Chen et al., 2006).
Features that capture the start and end of gestures
are shown to improve sentence segmentation beyond
lexical and prosodic features alone. This idea of ges-
tural features as a sort of visual punctuation has par-
allels in the literature on prosody, which we discuss
in the next subsection.
Finally, ambiguous noun phrases can be resolved
by examining the similarity of co-articulated ges-
tures (Eisenstein and Davis, 2007). While noun
phrase coreference can be viewed as a discourse pro-
cessing task, we address the higher-level discourse
phenomenon of topic segmentation. In addition, this
prior work focused primarily on pointing gestures
directed at pre-printed visual aids. The current pa-
per presents a new domain, in which speakers do not
have access to visual aids. Thus pointing gestures
are less frequent than “iconic” gestures, in which the
form of motion is the principle communicative fea-
ture (McNeill, 1992).
Non-textual features for topic segmentation Re-
search on non-textual features for topic segmenta-
tion has primarily focused on prosody, under the as-
sumption that a key prosodic function is to mark
structure at the discourse level (Steedman, 1990;
Grosz and Hirshberg, 1992; Swerts, 1997). The ul-
timate goal of this research is to find correlates of
hierarchical discourse structure in phonetic features.
Today, research on prosody has converged on
prosodic cues which correlate with discourse struc-
ture. Such markers include pause duration, fun-
damental frequency, and pitch range manipula-
tions (Grosz and Hirshberg, 1992; Hirschberg and
Nakatani, 1998). These studies informed the devel-
opment of applications such as segmentation tools
for meeting analysis, e.g. (Tur et al., 2001; Galley et
al., 2003).
In comparison, the connection between gesture
and discourse structure is a relatively unexplored
area, at least with respect to computational ap-
proaches. One conclusion that emerges from our
analysis is that gesture may signal discourse struc-
ture in a different way than prosody does: while spe-
cific prosodic markers characterize segment bound-
aries, gesture predicts segmentation through intra-
segmental cohesion. The combination of these two
</bodyText>
<page confidence="0.998403">
853
</page>
<bodyText confidence="0.994014">
modalities is an exciting direction for future re-
search.
</bodyText>
<sectionHeader confidence="0.966836" genericHeader="method">
3 Visual Features for Discourse Analysis
</sectionHeader>
<bodyText confidence="0.997954">
This section describes the process of building a rep-
resentation that permits the assessment of gestural
cohesion. The core signal-level features are based
on spatiotemporal interest points, which provide a
sparse representation of the motion in the video. At
each interest point, visual, spatial, and kinematic
characteristics are extracted and then concatenated
into vectors. Principal component analysis (PCA)
reduces the dimensionality to a feature vector of
manageable size (Bishop, 2006). These feature vec-
tors are then clustered, yielding a codebook of visual
forms. This video processing pipeline is shown in
Figure 1; the remainder of the section describes the
individual steps in greater detail.
</bodyText>
<subsectionHeader confidence="0.997666">
3.1 Spatiotemporal Interest Points
</subsectionHeader>
<bodyText confidence="0.999982473684211">
Spatiotemporal interest points (Laptev, 2005) pro-
vide a sparse representation of motion in video. The
idea is to select a few local regions that contain high
information content in both the spatial and tempo-
ral dimensions. The image features at these regions
should be relatively robust to lighting and perspec-
tive changes, and they should capture the relevant
movement in the video. The set of spatiotemporal
interest points thereby provides a highly compressed
representation of the key visual features. Purely spa-
tial interest points have been successful in a variety
of image processing tasks (Lowe, 1999), and spa-
tiotemporal interest points are beginning to show
similar advantages for video processing (Laptev,
2005).
The use of spatiotemporal interest points is specif-
ically motivated by techniques from the computer
vision domain of activity recognition (Efros et al.,
2003; Niebles et al., 2006). The goal of activity
recognition is to classify video sequences into se-
mantic categories: e.g., walking, running, jumping.
As a simple example, consider the task of distin-
guishing videos of walking from videos of jump-
ing. In the walking videos, the motion at most of
the interest points will be horizontal, while in the
jumping videos it will be vertical. Spurious vertical
motion in a walking video is unlikely to confuse the
classifier, as long as the majority of interest points
move horizontally. The hypothesis of this paper is
that just as such low-level movement features can be
applied in a supervised fashion to distinguish activi-
ties, they can be applied in an unsupervised fashion
to group co-speech gestures into perceptually mean-
ingful clusters.
The Activity Recognition Toolbox (Doll´ar et al.,
2005)1 is used to detect spatiotemporal interest
points for our dataset. This toolbox ranks interest
points using a difference-of-Gaussians filter in the
spatial dimension, and a set of Gabor filters in the
temporal dimension. The total number of interest
points extracted per video is set to equal the number
of frames in the video. This bounds the complexity
of the representation to be linear in the length of the
video; however, the system may extract many inter-
est points in some frames and none in other frames.
Figure 2 shows the interest points extracted from
a representative video frame from our corpus. Note
that the system has identified high contrast regions
of the gesturing hand. From manual inspection,
the large majority of interest points extracted in our
dataset capture motion created by hand gestures.
Thus, for this dataset it is reasonable to assume that
an interest point-based representation expresses the
visual properties of the speakers’ hand gestures. In
videos containing other sources of motion, prepro-
cessing may be required to filter out interest points
that are extraneous to gestural communication.
</bodyText>
<subsectionHeader confidence="0.999339">
3.2 Visual Descriptors
</subsectionHeader>
<bodyText confidence="0.9999625">
At each interest point, the temporal and spatial
brightness gradients are constructed across a small
space-time volume of nearby pixels. Brightness gra-
dients have been used for a variety of problems in
computer vision (Forsyth and Ponce, 2003), and pro-
vide a fairly general way to describe the visual ap-
pearance of small image patches. However, even for
a small space-time volume, the resulting dimension-
ality is still quite large: a 10-by-10 pixel box across 5
video frames yields a 500-dimensional feature vec-
tor for each of the three gradients. For this reason,
principal component analysis (Bishop, 2006) is used
to reduce the dimensionality. The spatial location of
the interest point is added to the final feature vector.
</bodyText>
<footnote confidence="0.996869">
1http://vision.ucsd.edu/—pdollar/research/cuboids doc/index.html
</footnote>
<page confidence="0.996983">
854
</page>
<figureCaption confidence="0.998837666666667">
Figure 1: The visual processing pipeline for the extraction of gestural codewords from video.
Figure 2: Circles indicate the interest points extracted
from this frame of the corpus.
</figureCaption>
<bodyText confidence="0.999465620689655">
This visual feature representation is substantially
lower-level than the descriptions of gesture form
found in both the psychology and computer science
literatures. For example, when manually annotat-
ing gesture, it is common to employ a taxonomy
of hand shapes and trajectories, and to describe the
location with respect to the body and head (Mc-
Neill, 1992; Martell, 2005). Working with automatic
hand tracking, Quek (2003) automatically computes
perceptually-salient gesture features, such as sym-
metric motion and oscillatory repetitions.
In contrast, our feature representation takes the
form of a vector of continuous values and is not eas-
ily interpretable in terms of how the gesture actu-
ally appears. However, this low-level approach of-
fers several important advantages. Most critically,
it requires no initialization and comparatively little
tuning: it can be applied directly to any video with a
fixed camera position and static background. Sec-
ond, it is robust: while image noise may cause a
few spurious interest points, the majority of inter-
est points should still guide the system to an appro-
priate characterization of the gesture. In contrast,
hand tracking can become irrevocably lost, requiring
manual resets (Gavrila, 1999). Finally, the success
of similar low-level interest point representations at
the activity-recognition task provides reason for op-
timism that they may also be applicable to unsuper-
vised gesture analysis.
</bodyText>
<subsectionHeader confidence="0.998305">
3.3 A Lexicon of Visual Forms
</subsectionHeader>
<bodyText confidence="0.99999128">
After extracting a set of low-dimensional feature
vectors to characterize the visual appearance at each
spatiotemporal interest point, it remains only to
convert this into a representation amenable to a
cohesion-based analysis. Using k-means cluster-
ing (Bishop, 2006), the feature vectors are grouped
into codewords: a compact, lexicon-like representa-
tion of salient visual features in video. The number
of clusters is a tunable parameter, though a system-
atic investigation of the role of this parameter is left
for future work.
Codewords capture frequently-occurring patterns
of motion and appearance at a local scale – interest
points that are clustered together have a similar vi-
sual appearance. Because most of the motion in our
videos is gestural, the codewords that appear during
a given sentence provide a succinct representation of
the ongoing gestural activity. Distributions of code-
words over time can be analyzed in similar terms
to the distribution of lexical features. A change in
the distribution of codewords indicates new visual
kinematic elements entering the discourse. Thus, the
codeword representation allows gestural cohesion to
be assessed in much the same way as lexical cohe-
sion.
</bodyText>
<sectionHeader confidence="0.98439" genericHeader="method">
4 Bayesian Topic Segmentation
</sectionHeader>
<bodyText confidence="0.996918166666667">
Topic segmentation is performed in a Bayesian
framework, with each sentence’s segment index en-
coded in a hidden variable, written zt. The hidden
variables are assumed to be generated by a linear
segmentation, such that zt E {zt−1, zt−1 + 11. Ob-
servations – the words and gesture codewords – are
</bodyText>
<page confidence="0.994579">
855
</page>
<bodyText confidence="0.999791842105263">
generated by multinomial language models that are
indexed according to the segment. In this frame-
work, a high-likelihood segmentation will include
language models that are tightly focused on a com-
pact vocabulary. Such a segmentation maximizes
the lexical cohesion of each segment. This model
thus provides a principled, probabilistic framework
for cohesion-based segmentation, and we will see
that the Bayesian approach is particularly well-
suited to the combination of multiple modalities.
Formally, our goal is to identify the best possible
segmentation S, where S is a tuple: S = hz, θ, φi.
The segment indices for each sentence are written
zt; for segment i, θi and φi are multinomial lan-
guage models over words and gesture codewords re-
spectively. For each sentence, xt and yt indicate
the words and gestures that appear. We will seek to
identify the segmentation S = argmaxSp(S, x, y),
conditioned on priors that will be defined below.
</bodyText>
<equation confidence="0.9718495">
p(S,x,y) = p(x,y|S)p(S)
i
</equation>
<bodyText confidence="0.999731392857143">
The language models θi and φi are multinomial
distributions, so the log-likelihood of the obser-
vations xt is log p(xt|θi) = Ew n(t,j)log θi��,
where n(t, j) is the count of word j in sentence t,
and W is the size of the vocabulary. An analogous
equation is used for the gesture codewords. Each
language model is given a symmetric Dirichlet prior
α. As we will see shortly, the use of different pri-
ors for the verbal and gestural language models al-
lows us to weight these modalities in a Bayesian
framework. Finally, we model the probability of
the segmentation z by considering the durations of
each segment: p(z) = Hi p(dur(i)|ψ). A negative-
binomial distribution with parameter ψ is applied to
discourage extremely short or long segments.
Inference Crucially, both the likelihood (equa-
tion 1) and the prior (equation 2) factor into a prod-
uct across the segments. This factorization en-
ables the optimal segmentation to be found using
a dynamic program, similar to those demonstrated
by Utiyama and Isahara (2001) and Malioutov and
Barzilay (2006). For each set of segmentation points
z, the associated language models are set to their
posterior expectations, e.g., θi = E[θ|{xt : zt =
i}, α].
The Dirichlet prior is conjugate to the multino-
mial, so this expectation can be computed in closed
form:
</bodyText>
<equation confidence="0.996491">
n(i, j) + α
θi�� = (3)
N(i) + Wα
</equation>
<bodyText confidence="0.995620594594595">
where n(i, j) is the count of word j in segment
i and N(i) is the total number of words in seg-
ment i (Bernardo and Smith, 2000). The symmetric
Dirichlet prior α acts as a smoothing pseudo-count.
In the multimodal context, the priors act to control
the weight of each modality. If the prior for the ver-
bal language model θ is high relative to the prior for
the gestural language model φ then the verbal multi-
nomial will be smoother, and will have a weaker im-
pact on the final segmentation. The impact of the
priors on the weights of each modality is explored
in Section 6.
Estimation of priors The distribution over seg-
ment durations is negative-binomial, with parame-
ters ψ. In general, the maximum likelihood estimate
of the parameters of a negative-binomial distribu-
tion cannot be found in closed form (Balakrishnan
and Nevzorov, 2003). For any given segmentation,
the maximum-likelihood setting for ψ is found via
a gradient-based search. This setting is then used
to generate another segmentation, and the process
is iterated until convergence, as in hard expectation-
maximization. The Dirichlet priors on the language
models are symmetric, and are chosen via cross-
validation. Sampling or gradient-based techniques
may be used to estimate these parameters, but this is
left for future work.
Relation to other segmentation models Other
cohesion-based techniques have typically focused
on hand-crafted similarity metrics between sen-
tences, such as cosine similarity (Galley et al., 2003;
Malioutov and Barzilay, 2006). In contrast, the
model described here is probabilistically motivated,
maximizing the joint probability of the segmentation
with the observed words and gestures. Our objec-
tive criterion is similar in form to that of Utiyama
and Isahara (2001); however, in contrast to this prior
</bodyText>
<equation confidence="0.973038125">
11
p(x, y|S) =
i
p({xt : zt = i}|θi)p({yt : zt = i}|φi)
(1)
p(θi)p(φi) (2)
11
p(S) = p(z)
</equation>
<page confidence="0.992872">
856
</page>
<bodyText confidence="0.999948142857143">
work, our criterion is justified by a Bayesian ap-
proach. Also, while the smoothing in our approach
arises naturally from the symmetric Dirichlet prior,
Utiyama and Isahara apply Laplace’s rule and add
pseudo-counts of one in all cases. Such an approach
would be incapable of flexibly balancing the contri-
butions of each modality.
</bodyText>
<sectionHeader confidence="0.995177" genericHeader="method">
5 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.999971588235294">
Dataset Our dataset is composed of fifteen audio-
video recordings of dialogues limited to three min-
utes in duration. The dataset includes nine differ-
ent pairs of participants. In each video one of five
subjects is discussed. The potential subjects include
a “Tom and Jerry” cartoon, a “Star Wars” toy, and
three mechanical devices: a latchbox, a piston, and
a candy dispenser. One participant – “participant A”
– was familiarized with the topic, and is tasked with
explaining it to participant B, who is permitted to
ask questions. Audio from both participants is used,
but only video of participant A is used; we do not ex-
amine whether B’s gestures are relevant to discourse
segmentation.
Video was recorded using standard camcorders,
with a resolution of 720 by 480 at 30 frames per
second. The video was reduced to 360 by 240 gray-
scale images before visual analysis is applied. Audio
was recorded using headset microphones. No man-
ual postprocessing is applied to the video.
Annotations and data processing All speech was
transcribed by hand, and time stamps were obtained
using the SPHINX-II speech recognition system for
forced alignment (Huang et al., 1993). Sentence
boundaries are annotated according to (NIST, 2003),
and additional sentence boundaries are automati-
cally inserted at all turn boundaries. Commonly-
occurring terms unlikely to impact segmentation are
automatically removed by using a stoplist.
For automatic speech recognition, the default Mi-
crosoft speech recognizer was applied to each sen-
tence, and the top-ranked recognition result was re-
ported. As is sometimes the case in real-world ap-
plications, no speaker-specific training data is avail-
able. The resulting recognition quality is very poor,
yielding a word error rate of 77%.
Annotators were instructed to select segment
boundaries that divide the dialogue into coherent
topics. Segmentation points are required to coincide
with sentence or turn boundaries. A second annota-
tor – who is not an author on any paper connected
with this research – provided an additional set of
segment annotations on six documents. On this sub-
set of documents, the Pk between annotators was
.306, and the WindowDiff was .325 (these metrics
are explained in the next subsection). This is simi-
lar to the interrater agreement reported by Malioutov
and Barzilay (2006).
Over the fifteen dialogues, a total of 7458 words
were transcribed (497 per dialogue), spread over
1440 sentences or interrupted turns (96 per dia-
logue). There were a total of 102 segments (6.8
per dialogue), from a minimum of four to a maxi-
mum of ten. This rate of fourteen sentences or in-
terrupted turns per segment indicates relatively fine-
grained segmentation. In the physics lecture corpus
used by Malioutov and Barzilay (2006), there are
roughly 100 sentences per segment. On the ICSI
corpus of meeting transcripts, Galley et al. (2003)
report 7.5 segments per meeting, with 770 “poten-
tial boundaries,” suggesting a similar rate of roughly
100 sentences or interrupted turns per segment.
The size of this multimodal dataset is orders of
magnitude smaller than many other segmentation
corpora. For example, the Broadcast News corpus
used by Beeferman et al. (1999) and others con-
tains two million words. The entire ICSI meeting
corpus contains roughly 600,000 words, although
only one third of this dataset was annotated for seg-
mentation (Galley et al., 2003). The physics lecture
corpus that was mentioned above contains 232,000
words (Malioutov and Barzilay, 2006). The task
considered in this section is thus more difficult than
much of the previous discourse segmentation work
on two dimensions: there is less training data, and a
finer-grained segmentation is required.
Metrics All experiments are evaluated in terms
of the commonly-used Pk (Beeferman et al., 1999)
and WindowDiff (WD) (Pevzner and Hearst, 2002)
scores. These metrics are penalties, so lower val-
ues indicate better segmentations. The Pk metric
expresses the probability that any randomly chosen
pair of sentences is incorrectly segmented, if they
are k sentences apart (Beeferman et al., 1999). Fol-
lowing tradition, k is set to half of the mean seg-
</bodyText>
<page confidence="0.993893">
857
</page>
<table confidence="0.998783875">
Method Pk WD
1. gesture only .486 .502
2. ASR only .462 .476
3. ASR + gesture .388 .401
4. transcript only .382 .397
5. transcript + gesture .332 .349
6. random .473 .526
7. equal-width .508 .515
</table>
<tableCaption confidence="0.994910666666667">
Table 1: For each method, the score of the best perform-
ing configuration is shown. Pk and WD are penalties, so
lower values indicate better performance.
</tableCaption>
<bodyText confidence="0.999643666666667">
ment length. The WindowDiff metric is a varia-
tion of Pk (Pevzner and Hearst, 2002), applying a
penalty whenever the number of segments within the
k-sentence window differs for the reference and hy-
pothesized segmentations.
Baselines Two naive baselines are evaluated.
Given that the annotator has divided the dialogue
into K segments, the random baseline arbitrary
chooses K random segmentation points. The re-
sults of this baseline are averaged over 1000 itera-
tions. The equal-width baseline places boundaries
such that all segments contain an equal number of
sentences. Both the experimental systems and these
naive baselines were given the correct number of
segments, and also were provided with manually an-
notated sentence boundaries – their task is to select
the k sentence boundaries that most accurately seg-
ment the text.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999049426229509">
Table 1 shows the segmentation performance for a
range of feature sets, as well as the two baselines.
Given only gesture features the segmentation results
are poor (line 1), barely outperforming the baselines
(lines 6 and 7). However, gesture proves highly ef-
fective as a supplementary modality. The combina-
tion of gesture with ASR transcripts (line 3) yields
an absolute 7.4% improvement over ASR transcripts
alone (line 4). Paired t-tests show that this result
is statistically significant (t(14) = 2.71,p &lt; .01
for both Pk and WindowDiff). Even when man-
ual speech transcripts are available, gesture features
yield a substantial improvement, reducing Pk and
WD by roughly 5%. This result is statistically sig-
nificant for both Pk (t(14) = 2.00,p &lt; .05) and
WD (t(14) = 1.94,p &lt; .05).
Interactions of verbal and gesture features We
now consider the relative contribution of the verbal
and gesture features. In a discriminative setting, the
contribution of each modality would be explicitly
weighted. In a Bayesian generative model, the same
effect is achieved through the Dirichlet priors, which
act to smooth the verbal and gestural multinomials –
see equation 3. For example, when the gesture prior
is high and verbal prior is low, the gesture counts are
smoothed, and the verbal counts play a greater role
in segmentation. When both priors are very high,
the model will simply try to find equally-sized seg-
ments, satisfying the distribution over durations.
The effects of these parameters can be seen in Fig-
ure 3. The gesture model prior is held constant at
its ideal value, and the segmentation performance
is plotted against the logarithm of the verbal prior.
Low values of the verbal prior cause it to domi-
nate the segmentation; this can be seen at the left
of both graphs, where the performance of the multi-
modal and verbal-only systems are nearly identical.
High values of the verbal prior cause it to be over-
smoothed, and performance thus approaches that of
the gesture-only segmenter.
Comparison to other models While much of
the research on topic segmentation focuses on writ-
ten text, there are some comparable systems that
also aim at unsupervised segmentation of sponta-
neous spoken language. For example, Malioutov
and Barzilay (2006) segment a corpus of classroom
lectures, using similar lexical cohesion-based fea-
tures. With manual transcriptions, they report a .383
Pk and .417 WD on artificial intelligence (AI) lec-
tures, and .298 Pk and .311 WD on physics lectures.
Our results are in the range bracketed by these two
extremes; the wide range of results suggests that seg-
mentation scores are difficult to compare across do-
mains. The segmentation of physics lectures was at
a very course level of granularity, while the segmen-
tation of AI lectures was more similar to our anno-
tations.
We applied the publicly-available executable for
this algorithm to our data, but performance was
poor, yielding a .417 Pk and .465 WD even when
both verbal and gestural features were available.
</bodyText>
<page confidence="0.981241">
858
</page>
<figure confidence="0.998466090909091">
Pk
WD
log verbal prior
log verbal prior
0.42
0.4
0.38
0.36
verbal−only
multimodal
−3 −2.5 −2 −1.5 −1 −0.5
0.34
0.32
0.42
0.4
0.38
0.36
verbal−only
multimodal
−3 −2.5 −2 −1.5 −1 −0.5
0.34
0.32
</figure>
<figureCaption confidence="0.9964925">
Figure 3: The multimodal and verbal-only performance using the reference transcript. The x-axis shows the logarithm
of the verbal prior; the gestural prior is held fixed at the optimal value.
</figureCaption>
<bodyText confidence="0.99989">
This may be because the technique is not de-
signed for the relatively fine-grained segmentation
demanded by our dataset (Malioutov, 2006).
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999994234042553">
This research shows a novel relationship between
gestural cohesion and discourse structure. Automat-
ically extracted gesture features are predictive of dis-
course segmentation when used in isolation; when
lexical information is present, segmentation perfor-
mance is further improved. This suggests that ges-
tures provide unique information not present in the
lexical features alone, even when perfect transcripts
are available.
There are at least two possibilities for how ges-
ture might impact topic segmentation: “visual punc-
tuation,” and cohesion. The visual punctuation view
would attempt to identify specific gestural patterns
that are characteristic of segment boundaries. This
is analogous to research that identifies prosodic sig-
natures of topic boundaries, such as (Hirschberg and
Nakatani, 1998). By design, our model is incapable
of exploiting such phenomena, as our goal is to in-
vestigate the notion of gestural cohesion. Thus, the
performance gains demonstrated in this paper can-
not be explained by such punctuation-like phenom-
ena; we believe that they are due to the consistent
gestural themes that characterize coherent topics.
However, we are interested in pursuing the idea of
visual punctuation in the future, so as to compare the
power of visual punctuation and gestural cohesion
to predict segment boundaries. In addition, the in-
teraction of gesture and prosody suggests additional
possibilities for future research.
The videos in the dataset for this paper are fo-
cused on the description of physical devices and
events, leading to a fairly concrete set of gestures.
In other registers of conversation, gestural form may
be driven more by spatial metaphors, or may con-
sist mainly of temporal “beats.” In such cases, the
importance of gestural cohesion for discourse seg-
mentation may depend on the visual expressivity of
the speaker. We plan to examine the extensibility of
gesture cohesion to more naturalistic settings, such
as classroom lectures.
Finally, topic segmentation provides only an out-
line of the discourse structure. Richer models of dis-
course include hierarchical structure (Grosz and Sid-
ner, 1986) and Rhetorical Structure Theory (Mann
and Thompson, 1988). The application of gestural
analysis to such models may lead to fruitful areas of
future research.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.72114025">
We thank Aaron Adler, C. Mario Christoudias,
Michael Collins, Lisa Guttentag, Igor Malioutov,
Brian Milch, Matthew Rasmussen, Candace Sidner,
Luke Zettlemoyer, and the anonymous reviewers.
This research was supported by Quanta Computer,
the National Science Foundation (CAREER grant
IIS-0448168 and grant IIS-0415865) and the Mi-
crosoft Research Faculty Fellowship.
</bodyText>
<page confidence="0.998309">
859
</page>
<sectionHeader confidence="0.989376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999715081081081">
Narayanaswamy Balakrishnan and Valery B. Nevzorov.
2003. A primer on statistical distributions. John Wi-
ley &amp; Sons.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
Jos´e M. Bernardo and Adrian F. M. Smith. 2000.
Bayesian Theory. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner, and Charles Rich. 2001.
Non-verbal cues for discourse structure. In Proceed-
ings of ACL, pages 106–115.
Lei Chen, Mary Harper, and Zhongqiang Huang. 2006.
Using maximum entropy (ME) model to incorporate
gesture cues for sentence segmentation. In Proceed-
ings of ICMI, pages 185–192.
Piotr Doll´ar, Vincent Rabaud, Garrison Cottrell, and
Serge Belongie. 2005. Behavior recognition via
sparse spatio-temporal features. In ICCV VS-PETS.
Alexei A. Efros, Alexander C. Berg, Greg Mori, and Ji-
tendra Malik. 2003. Recognizing action at a distance.
In Proceedings of ICCV, pages 726–733.
Jacob Eisenstein and Randall Davis. 2007. Conditional
modality fusion for coreference resolution. In Pro-
ceedings of ACL, pages 352–359.
David A. Forsyth and Jean Ponce. 2003. Computer Vi-
sion: A Modern Approach. Prentice Hall.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. Proceedings of
ACL, pages 562–569.
Dariu M. Gavrila. 1999. Visual analysis of human move-
ment: A survey. Computer Vision and Image Under-
standing, 73(1):82–98.
Barbara Grosz and Julia Hirshberg. 1992. Some into-
national characteristics of discourse structure. In Pro-
ceedings of ICSLP, pages 429–432.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175–204.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings of
ICSLP.
Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and
Ronald Rosenfeld. 1993. An overview of the Sphinx-
II speech recognition system. In Proceedings ofARPA
Human Language Technology Workshop, pages 81–
86.
Michael Johnston. 1998. Unification-based multimodal
parsing. In Proceedings of COLING, pages 624–630.
Adam Kendon. 2004. Gesture: Visible Action as Utter-
ance. Cambridge University Press.
Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine
Cassell. 2007. Trading spaces: How humans and hu-
manoids use speech and gesture to give directions. In
Toyoaki Nishida, editor, Conversational Informatics:
An Engineering Approach. Wiley.
Ivan Laptev. 2005. On space-time interest points. In-
ternational Journal of Computer Vision, 64(2-3):107–
123.
David G. Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of ICCV, vol-
ume 2, pages 1150–1157.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25–32.
Igor Malioutov. 2006. Minimum cut model for spoken
lecture segmentation. Master’s thesis, Massachusetts
Institute of Technology.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8:243–281.
Craig Martell. 2005. FORM: An experiment in the anno-
tation of the kinematics of gesture. Ph.D. thesis, Uni-
versity of Pennsylvania.
David McNeill. 1992. Hand and Mind. The University
of Chicago Press.
Juan Carlos Niebles, Hongcheng Wang, and Li Fei-Fei.
2006. Unsupervised Learning of Human Action Cate-
gories Using Spatial-Temporal Words. In Proceedings
of the British Machine Vision Conference.
NIST. 2003. The Rich Transcription Fall 2003 (RT-03F)
Evaluation plan.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103–139.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36.
Francis Quek. 2003. The catchment feature model
for multimodal language analysis. In Proceedings of
ICCV.
Mark Steedman. 1990. Structure and intonation in spo-
ken language understanding. In Proceedings of ACL,
pages 9–16.
Marc Swerts. 1997. Prosodic features at discourse
boundaries of different strength. The Journal of the
Acoustical Society of America, 101:514.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, and
Elizabeth Shriberg. 2001. Integrating prosodic and
lexical cues for automatic topic segmentation. Com-
putational Linguistics, 27(1):31–57.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491–498.
</reference>
<page confidence="0.997699">
860
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588180">
<title confidence="0.971352">Gestural Cohesion for Topic Segmentation</title>
<author confidence="0.711851">Regina Barzilay Davis Eisenstein</author>
<affiliation confidence="0.9999825">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<address confidence="0.999934">77 Massachusetts Ave., Cambridge MA 02139</address>
<email confidence="0.843789">regina,</email>
<abstract confidence="0.998512571428572">This paper explores the relationship between discourse segmentation and coverbal gesture. the idea of we show that coherent topic segments are characterized by homogeneous gestural forms and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Narayanaswamy Balakrishnan</author>
<author>Valery B Nevzorov</author>
</authors>
<title>A primer on statistical distributions.</title>
<date>2003</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="18694" citStr="Balakrishnan and Nevzorov, 2003" startWordPosition="2875" endWordPosition="2878">ltimodal context, the priors act to control the weight of each modality. If the prior for the verbal language model θ is high relative to the prior for the gestural language model φ then the verbal multinomial will be smoother, and will have a weaker impact on the final segmentation. The impact of the priors on the weights of each modality is explored in Section 6. Estimation of priors The distribution over segment durations is negative-binomial, with parameters ψ. In general, the maximum likelihood estimate of the parameters of a negative-binomial distribution cannot be found in closed form (Balakrishnan and Nevzorov, 2003). For any given segmentation, the maximum-likelihood setting for ψ is found via a gradient-based search. This setting is then used to generate another segmentation, and the process is iterated until convergence, as in hard expectationmaximization. The Dirichlet priors on the language models are symmetric, and are chosen via crossvalidation. Sampling or gradient-based techniques may be used to estimate these parameters, but this is left for future work. Relation to other segmentation models Other cohesion-based techniques have typically focused on hand-crafted similarity metrics between sentenc</context>
</contexts>
<marker>Balakrishnan, Nevzorov, 2003</marker>
<rawString>Narayanaswamy Balakrishnan and Valery B. Nevzorov. 2003. A primer on statistical distributions. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="23290" citStr="Beeferman et al. (1999)" startWordPosition="3609" endWordPosition="3612">f ten. This rate of fourteen sentences or interrupted turns per segment indicates relatively finegrained segmentation. In the physics lecture corpus used by Malioutov and Barzilay (2006), there are roughly 100 sentences per segment. On the ICSI corpus of meeting transcripts, Galley et al. (2003) report 7.5 segments per meeting, with 770 “potential boundaries,” suggesting a similar rate of roughly 100 sentences or interrupted turns per segment. The size of this multimodal dataset is orders of magnitude smaller than many other segmentation corpora. For example, the Broadcast News corpus used by Beeferman et al. (1999) and others contains two million words. The entire ICSI meeting corpus contains roughly 600,000 words, although only one third of this dataset was annotated for segmentation (Galley et al., 2003). The physics lecture corpus that was mentioned above contains 232,000 words (Malioutov and Barzilay, 2006). The task considered in this section is thus more difficult than much of the previous discourse segmentation work on two dimensions: there is less training data, and a finer-grained segmentation is required. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al.,</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John D. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e M Bernardo</author>
<author>Adrian F M Smith</author>
</authors>
<title>Bayesian Theory.</title>
<date>2000</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="17985" citStr="Bernardo and Smith, 2000" startWordPosition="2755" endWordPosition="2758">a product across the segments. This factorization enables the optimal segmentation to be found using a dynamic program, similar to those demonstrated by Utiyama and Isahara (2001) and Malioutov and Barzilay (2006). For each set of segmentation points z, the associated language models are set to their posterior expectations, e.g., θi = E[θ|{xt : zt = i}, α]. The Dirichlet prior is conjugate to the multinomial, so this expectation can be computed in closed form: n(i, j) + α θi�� = (3) N(i) + Wα where n(i, j) is the count of word j in segment i and N(i) is the total number of words in segment i (Bernardo and Smith, 2000). The symmetric Dirichlet prior α acts as a smoothing pseudo-count. In the multimodal context, the priors act to control the weight of each modality. If the prior for the verbal language model θ is high relative to the prior for the gestural language model φ then the verbal multinomial will be smoother, and will have a weaker impact on the final segmentation. The impact of the priors on the weights of each modality is explored in Section 6. Estimation of priors The distribution over segment durations is negative-binomial, with parameters ψ. In general, the maximum likelihood estimate of the pa</context>
</contexts>
<marker>Bernardo, Smith, 2000</marker>
<rawString>Jos´e M. Bernardo and Adrian F. M. Smith. 2000. Bayesian Theory. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="8457" citStr="Bishop, 2006" startWordPosition="1237" endWordPosition="1238">ation of these two 853 modalities is an exciting direction for future research. 3 Visual Features for Discourse Analysis This section describes the process of building a representation that permits the assessment of gestural cohesion. The core signal-level features are based on spatiotemporal interest points, which provide a sparse representation of the motion in the video. At each interest point, visual, spatial, and kinematic characteristics are extracted and then concatenated into vectors. Principal component analysis (PCA) reduces the dimensionality to a feature vector of manageable size (Bishop, 2006). These feature vectors are then clustered, yielding a codebook of visual forms. This video processing pipeline is shown in Figure 1; the remainder of the section describes the individual steps in greater detail. 3.1 Spatiotemporal Interest Points Spatiotemporal interest points (Laptev, 2005) provide a sparse representation of motion in video. The idea is to select a few local regions that contain high information content in both the spatial and temporal dimensions. The image features at these regions should be relatively robust to lighting and perspective changes, and they should capture the </context>
<context position="12185" citStr="Bishop, 2006" startWordPosition="1827" endWordPosition="1828">tors At each interest point, the temporal and spatial brightness gradients are constructed across a small space-time volume of nearby pixels. Brightness gradients have been used for a variety of problems in computer vision (Forsyth and Ponce, 2003), and provide a fairly general way to describe the visual appearance of small image patches. However, even for a small space-time volume, the resulting dimensionality is still quite large: a 10-by-10 pixel box across 5 video frames yields a 500-dimensional feature vector for each of the three gradients. For this reason, principal component analysis (Bishop, 2006) is used to reduce the dimensionality. The spatial location of the interest point is added to the final feature vector. 1http://vision.ucsd.edu/—pdollar/research/cuboids doc/index.html 854 Figure 1: The visual processing pipeline for the extraction of gestural codewords from video. Figure 2: Circles indicate the interest points extracted from this frame of the corpus. This visual feature representation is substantially lower-level than the descriptions of gesture form found in both the psychology and computer science literatures. For example, when manually annotating gesture, it is common to e</context>
<context position="14295" citStr="Bishop, 2006" startWordPosition="2140" endWordPosition="2141"> of the gesture. In contrast, hand tracking can become irrevocably lost, requiring manual resets (Gavrila, 1999). Finally, the success of similar low-level interest point representations at the activity-recognition task provides reason for optimism that they may also be applicable to unsupervised gesture analysis. 3.3 A Lexicon of Visual Forms After extracting a set of low-dimensional feature vectors to characterize the visual appearance at each spatiotemporal interest point, it remains only to convert this into a representation amenable to a cohesion-based analysis. Using k-means clustering (Bishop, 2006), the feature vectors are grouped into codewords: a compact, lexicon-like representation of salient visual features in video. The number of clusters is a tunable parameter, though a systematic investigation of the role of this parameter is left for future work. Codewords capture frequently-occurring patterns of motion and appearance at a local scale – interest points that are clustered together have a similar visual appearance. Because most of the motion in our videos is gestural, the codewords that appear during a given sentence provide a succinct representation of the ongoing gestural activi</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
<author>Yukiko I Nakano</author>
<author>Timothy W Bickmore</author>
<author>Candace L Sidner</author>
<author>Charles Rich</author>
</authors>
<title>Non-verbal cues for discourse structure.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>106--115</pages>
<contexts>
<context position="4993" citStr="Cassell et al., 2001" startWordPosition="706" endWordPosition="709">t, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al., 2007), which presents an animated agent that is capable of augmenting navigation directions with gestures that describe the physical properties of landmarks along the route. Both systems generate plausible and human-like gestural behavior; we address the converse problem of interpreting such gestures. In this vein, hand-coded gesture features have been used to improve sentence segmentation</context>
</contexts>
<marker>Cassell, Nakano, Bickmore, Sidner, Rich, 2001</marker>
<rawString>Justine Cassell, Yukiko I. Nakano, Timothy W. Bickmore, Candace L. Sidner, and Charles Rich. 2001. Non-verbal cues for discourse structure. In Proceedings of ACL, pages 106–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Mary Harper</author>
<author>Zhongqiang Huang</author>
</authors>
<title>Using maximum entropy (ME) model to incorporate gesture cues for sentence segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ICMI,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="5701" citStr="Chen et al., 2006" startWordPosition="817" endWordPosition="820">n an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al., 2007), which presents an animated agent that is capable of augmenting navigation directions with gestures that describe the physical properties of landmarks along the route. Both systems generate plausible and human-like gestural behavior; we address the converse problem of interpreting such gestures. In this vein, hand-coded gesture features have been used to improve sentence segmentation, showing that sentence boundaries are unlikely to overlap gestures that are in progress (Chen et al., 2006). Features that capture the start and end of gestures are shown to improve sentence segmentation beyond lexical and prosodic features alone. This idea of gestural features as a sort of visual punctuation has parallels in the literature on prosody, which we discuss in the next subsection. Finally, ambiguous noun phrases can be resolved by examining the similarity of co-articulated gestures (Eisenstein and Davis, 2007). While noun phrase coreference can be viewed as a discourse processing task, we address the higher-level discourse phenomenon of topic segmentation. In addition, this prior work f</context>
</contexts>
<marker>Chen, Harper, Huang, 2006</marker>
<rawString>Lei Chen, Mary Harper, and Zhongqiang Huang. 2006. Using maximum entropy (ME) model to incorporate gesture cues for sentence segmentation. In Proceedings of ICMI, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Doll´ar</author>
<author>Vincent Rabaud</author>
<author>Garrison Cottrell</author>
<author>Serge Belongie</author>
</authors>
<title>Behavior recognition via sparse spatio-temporal features.</title>
<date>2005</date>
<booktitle>In ICCV VS-PETS.</booktitle>
<marker>Doll´ar, Rabaud, Cottrell, Belongie, 2005</marker>
<rawString>Piotr Doll´ar, Vincent Rabaud, Garrison Cottrell, and Serge Belongie. 2005. Behavior recognition via sparse spatio-temporal features. In ICCV VS-PETS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexei A Efros</author>
<author>Alexander C Berg</author>
<author>Greg Mori</author>
<author>Jitendra Malik</author>
</authors>
<title>Recognizing action at a distance.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>726--733</pages>
<contexts>
<context position="9586" citStr="Efros et al., 2003" startWordPosition="1409" endWordPosition="1412">ld be relatively robust to lighting and perspective changes, and they should capture the relevant movement in the video. The set of spatiotemporal interest points thereby provides a highly compressed representation of the key visual features. Purely spatial interest points have been successful in a variety of image processing tasks (Lowe, 1999), and spatiotemporal interest points are beginning to show similar advantages for video processing (Laptev, 2005). The use of spatiotemporal interest points is specifically motivated by techniques from the computer vision domain of activity recognition (Efros et al., 2003; Niebles et al., 2006). The goal of activity recognition is to classify video sequences into semantic categories: e.g., walking, running, jumping. As a simple example, consider the task of distinguishing videos of walking from videos of jumping. In the walking videos, the motion at most of the interest points will be horizontal, while in the jumping videos it will be vertical. Spurious vertical motion in a walking video is unlikely to confuse the classifier, as long as the majority of interest points move horizontally. The hypothesis of this paper is that just as such low-level movement featu</context>
</contexts>
<marker>Efros, Berg, Mori, Malik, 2003</marker>
<rawString>Alexei A. Efros, Alexander C. Berg, Greg Mori, and Jitendra Malik. 2003. Recognizing action at a distance. In Proceedings of ICCV, pages 726–733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Conditional modality fusion for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>352--359</pages>
<contexts>
<context position="6121" citStr="Eisenstein and Davis, 2007" startWordPosition="884" endWordPosition="887">gestures. In this vein, hand-coded gesture features have been used to improve sentence segmentation, showing that sentence boundaries are unlikely to overlap gestures that are in progress (Chen et al., 2006). Features that capture the start and end of gestures are shown to improve sentence segmentation beyond lexical and prosodic features alone. This idea of gestural features as a sort of visual punctuation has parallels in the literature on prosody, which we discuss in the next subsection. Finally, ambiguous noun phrases can be resolved by examining the similarity of co-articulated gestures (Eisenstein and Davis, 2007). While noun phrase coreference can be viewed as a discourse processing task, we address the higher-level discourse phenomenon of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation ha</context>
</contexts>
<marker>Eisenstein, Davis, 2007</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2007. Conditional modality fusion for coreference resolution. In Proceedings of ACL, pages 352–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Forsyth</author>
<author>Jean Ponce</author>
</authors>
<title>Computer Vision: A Modern Approach.</title>
<date>2003</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="11820" citStr="Forsyth and Ponce, 2003" startWordPosition="1765" endWordPosition="1768">taset capture motion created by hand gestures. Thus, for this dataset it is reasonable to assume that an interest point-based representation expresses the visual properties of the speakers’ hand gestures. In videos containing other sources of motion, preprocessing may be required to filter out interest points that are extraneous to gestural communication. 3.2 Visual Descriptors At each interest point, the temporal and spatial brightness gradients are constructed across a small space-time volume of nearby pixels. Brightness gradients have been used for a variety of problems in computer vision (Forsyth and Ponce, 2003), and provide a fairly general way to describe the visual appearance of small image patches. However, even for a small space-time volume, the resulting dimensionality is still quite large: a 10-by-10 pixel box across 5 video frames yields a 500-dimensional feature vector for each of the three gradients. For this reason, principal component analysis (Bishop, 2006) is used to reduce the dimensionality. The spatial location of the interest point is added to the final feature vector. 1http://vision.ucsd.edu/—pdollar/research/cuboids doc/index.html 854 Figure 1: The visual processing pipeline for t</context>
</contexts>
<marker>Forsyth, Ponce, 2003</marker>
<rawString>David A. Forsyth and Jean Ponce. 2003. Computer Vision: A Modern Approach. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen R. McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. Proceedings of ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dariu M Gavrila</author>
</authors>
<title>Visual analysis of human movement: A survey.</title>
<date>1999</date>
<journal>Computer Vision and Image Understanding,</journal>
<volume>73</volume>
<issue>1</issue>
<contexts>
<context position="13794" citStr="Gavrila, 1999" startWordPosition="2067" endWordPosition="2068">ues and is not easily interpretable in terms of how the gesture actually appears. However, this low-level approach offers several important advantages. Most critically, it requires no initialization and comparatively little tuning: it can be applied directly to any video with a fixed camera position and static background. Second, it is robust: while image noise may cause a few spurious interest points, the majority of interest points should still guide the system to an appropriate characterization of the gesture. In contrast, hand tracking can become irrevocably lost, requiring manual resets (Gavrila, 1999). Finally, the success of similar low-level interest point representations at the activity-recognition task provides reason for optimism that they may also be applicable to unsupervised gesture analysis. 3.3 A Lexicon of Visual Forms After extracting a set of low-dimensional feature vectors to characterize the visual appearance at each spatiotemporal interest point, it remains only to convert this into a representation amenable to a cohesion-based analysis. Using k-means clustering (Bishop, 2006), the feature vectors are grouped into codewords: a compact, lexicon-like representation of salient</context>
</contexts>
<marker>Gavrila, 1999</marker>
<rawString>Dariu M. Gavrila. 1999. Visual analysis of human movement: A survey. Computer Vision and Image Understanding, 73(1):82–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Julia Hirshberg</author>
</authors>
<title>Some intonational characteristics of discourse structure.</title>
<date>1992</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>429--432</pages>
<contexts>
<context position="6889" citStr="Grosz and Hirshberg, 1992" startWordPosition="1004" endWordPosition="1007">ation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structur</context>
</contexts>
<marker>Grosz, Hirshberg, 1992</marker>
<rawString>Barbara Grosz and Julia Hirshberg. 1992. Some intonational characteristics of discourse structure. In Proceedings of ICSLP, pages 429–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="1929" citStr="Halliday and Hasan, 1976" startWordPosition="251" endWordPosition="254">modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure. Our method builds on the idea that coherent discourse segments are characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns. Lexical cohesion (Halliday and Hasan, 1976) forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes (Hearst, 1994). With gestural cohesion, we explore whether the same idea holds for gesture features. The motivation for this approach comes from a series of psycholinguistic studies suggesting that gesture supplements speech with meaningful and unique semantic content (McNeill, 1992; Kendon, 2004). We assume that repeated patterns in gesture are indicative of the semantic coherence that characterizes well-defined discourse segments. An </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1091" citStr="Hearst, 1994" startWordPosition="137" endWordPosition="138">gestural forms and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 1 Introduction When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels. Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features (Hearst, 1994; Hirschberg and Nakatani, 1998; Passonneau and Litman, 1997); this work informs various text and speech processing applications, such as automatic summarization and segmentation. Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of dis</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine Nakatani</author>
</authors>
<title>Acoustic indicators of topic segmentation.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="1122" citStr="Hirschberg and Nakatani, 1998" startWordPosition="139" endWordPosition="142"> and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 1 Introduction When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels. Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features (Hearst, 1994; Hirschberg and Nakatani, 1998; Passonneau and Litman, 1997); this work informs various text and speech processing applications, such as automatic summarization and segmentation. Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure. Our method bu</context>
<context position="7269" citStr="Hirschberg and Nakatani, 1998" startWordPosition="1060" endWordPosition="1063"> for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structure is a relatively unexplored area, at least with respect to computational approaches. One conclusion that emerges from our analysis is that gesture may signal discourse structure in a different way than prosody does: while specific prosodic markers characterize segment boundaries, gesture predicts segmentation through intrasegmental cohesion. The combination of these two 853 mo</context>
<context position="29772" citStr="Hirschberg and Nakatani, 1998" startWordPosition="4653" endWordPosition="4656">ion when used in isolation; when lexical information is present, segmentation performance is further improved. This suggests that gestures provide unique information not present in the lexical features alone, even when perfect transcripts are available. There are at least two possibilities for how gesture might impact topic segmentation: “visual punctuation,” and cohesion. The visual punctuation view would attempt to identify specific gestural patterns that are characteristic of segment boundaries. This is analogous to research that identifies prosodic signatures of topic boundaries, such as (Hirschberg and Nakatani, 1998). By design, our model is incapable of exploiting such phenomena, as our goal is to investigate the notion of gestural cohesion. Thus, the performance gains demonstrated in this paper cannot be explained by such punctuation-like phenomena; we believe that they are due to the consistent gestural themes that characterize coherent topics. However, we are interested in pursuing the idea of visual punctuation in the future, so as to compare the power of visual punctuation and gestural cohesion to predict segment boundaries. In addition, the interaction of gesture and prosody suggests additional pos</context>
</contexts>
<marker>Hirschberg, Nakatani, 1998</marker>
<rawString>Julia Hirschberg and Christine Nakatani. 1998. Acoustic indicators of topic segmentation. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Fileno Alleva</author>
<author>Mei-Yuh Hwang</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>An overview of the SphinxII speech recognition system.</title>
<date>1993</date>
<booktitle>In Proceedings ofARPA Human Language Technology Workshop,</booktitle>
<pages>81--86</pages>
<contexts>
<context position="21259" citStr="Huang et al., 1993" startWordPosition="3287" endWordPosition="3290">both participants is used, but only video of participant A is used; we do not examine whether B’s gestures are relevant to discourse segmentation. Video was recorded using standard camcorders, with a resolution of 720 by 480 at 30 frames per second. The video was reduced to 360 by 240 grayscale images before visual analysis is applied. Audio was recorded using headset microphones. No manual postprocessing is applied to the video. Annotations and data processing All speech was transcribed by hand, and time stamps were obtained using the SPHINX-II speech recognition system for forced alignment (Huang et al., 1993). Sentence boundaries are annotated according to (NIST, 2003), and additional sentence boundaries are automatically inserted at all turn boundaries. Commonlyoccurring terms unlikely to impact segmentation are automatically removed by using a stoplist. For automatic speech recognition, the default Microsoft speech recognizer was applied to each sentence, and the top-ranked recognition result was reported. As is sometimes the case in real-world applications, no speaker-specific training data is available. The resulting recognition quality is very poor, yielding a word error rate of 77%. Annotato</context>
</contexts>
<marker>Huang, Alleva, Hwang, Rosenfeld, 1993</marker>
<rawString>Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and Ronald Rosenfeld. 1993. An overview of the SphinxII speech recognition system. In Proceedings ofARPA Human Language Technology Workshop, pages 81– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
</authors>
<title>Unification-based multimodal parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>624--630</pages>
<contexts>
<context position="4360" citStr="Johnston, 1998" startWordPosition="614" endWordPosition="615">structure. Applying our algorithm to a dataset of faceto-face dialogues, we find that gesture communicates unique information, improving segmentation performance over lexical features alone. The positive impact of gesture is most pronounced when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriente</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Michael Johnston. 1998. Unification-based multimodal parsing. In Proceedings of COLING, pages 624–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kendon</author>
</authors>
<title>Gesture: Visible Action as Utterance.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2387" citStr="Kendon, 2004" startWordPosition="320" endWordPosition="321">e characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns. Lexical cohesion (Halliday and Hasan, 1976) forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes (Hearst, 1994). With gestural cohesion, we explore whether the same idea holds for gesture features. The motivation for this approach comes from a series of psycholinguistic studies suggesting that gesture supplements speech with meaningful and unique semantic content (McNeill, 1992; Kendon, 2004). We assume that repeated patterns in gesture are indicative of the semantic coherence that characterizes well-defined discourse segments. An advantage of this view is that gestures can be brought to bear on discourse analysis without undertaking the daunting task of recognizing and interpreting individual gestures. This is crucial because coverbal gesture – unlike formal sign language – rarely follows any predefined form or grammar, and may vary dramatically by speaker. A key implementational challenge is automatically extracting gestural information from raw video and representing it in a wa</context>
<context position="4598" citStr="Kendon, 2004" startWordPosition="648" endWordPosition="649"> when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al</context>
</contexts>
<marker>Kendon, 2004</marker>
<rawString>Adam Kendon. 2004. Gesture: Visible Action as Utterance. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefan Kopp</author>
<author>Paul Tepper</author>
<author>Kim Ferriman</author>
</authors>
<note>and Justine</note>
<marker>Kopp, Tepper, Ferriman, </marker>
<rawString>Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cassell</author>
</authors>
<title>Trading spaces: How humans and humanoids use speech and gesture to give directions.</title>
<date>2007</date>
<editor>In Toyoaki Nishida, editor, Conversational</editor>
<publisher>Wiley.</publisher>
<marker>Cassell, 2007</marker>
<rawString>Cassell. 2007. Trading spaces: How humans and humanoids use speech and gesture to give directions. In Toyoaki Nishida, editor, Conversational Informatics: An Engineering Approach. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Laptev</author>
</authors>
<title>On space-time interest points.</title>
<date>2005</date>
<journal>International Journal of Computer Vision,</journal>
<pages>64--2</pages>
<contexts>
<context position="8750" citStr="Laptev, 2005" startWordPosition="1280" endWordPosition="1281"> interest points, which provide a sparse representation of the motion in the video. At each interest point, visual, spatial, and kinematic characteristics are extracted and then concatenated into vectors. Principal component analysis (PCA) reduces the dimensionality to a feature vector of manageable size (Bishop, 2006). These feature vectors are then clustered, yielding a codebook of visual forms. This video processing pipeline is shown in Figure 1; the remainder of the section describes the individual steps in greater detail. 3.1 Spatiotemporal Interest Points Spatiotemporal interest points (Laptev, 2005) provide a sparse representation of motion in video. The idea is to select a few local regions that contain high information content in both the spatial and temporal dimensions. The image features at these regions should be relatively robust to lighting and perspective changes, and they should capture the relevant movement in the video. The set of spatiotemporal interest points thereby provides a highly compressed representation of the key visual features. Purely spatial interest points have been successful in a variety of image processing tasks (Lowe, 1999), and spatiotemporal interest points</context>
</contexts>
<marker>Laptev, 2005</marker>
<rawString>Ivan Laptev. 2005. On space-time interest points. International Journal of Computer Vision, 64(2-3):107– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object recognition from local scale-invariant features.</title>
<date>1999</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<volume>2</volume>
<pages>1150--1157</pages>
<contexts>
<context position="9314" citStr="Lowe, 1999" startWordPosition="1371" endWordPosition="1372">s Spatiotemporal interest points (Laptev, 2005) provide a sparse representation of motion in video. The idea is to select a few local regions that contain high information content in both the spatial and temporal dimensions. The image features at these regions should be relatively robust to lighting and perspective changes, and they should capture the relevant movement in the video. The set of spatiotemporal interest points thereby provides a highly compressed representation of the key visual features. Purely spatial interest points have been successful in a variety of image processing tasks (Lowe, 1999), and spatiotemporal interest points are beginning to show similar advantages for video processing (Laptev, 2005). The use of spatiotemporal interest points is specifically motivated by techniques from the computer vision domain of activity recognition (Efros et al., 2003; Niebles et al., 2006). The goal of activity recognition is to classify video sequences into semantic categories: e.g., walking, running, jumping. As a simple example, consider the task of distinguishing videos of walking from videos of jumping. In the walking videos, the motion at most of the interest points will be horizont</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>David G. Lowe. 1999. Object recognition from local scale-invariant features. In Proceedings of ICCV, volume 2, pages 1150–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="17573" citStr="Malioutov and Barzilay (2006)" startWordPosition="2673" endWordPosition="2676">nd gestural language models allows us to weight these modalities in a Bayesian framework. Finally, we model the probability of the segmentation z by considering the durations of each segment: p(z) = Hi p(dur(i)|ψ). A negativebinomial distribution with parameter ψ is applied to discourage extremely short or long segments. Inference Crucially, both the likelihood (equation 1) and the prior (equation 2) factor into a product across the segments. This factorization enables the optimal segmentation to be found using a dynamic program, similar to those demonstrated by Utiyama and Isahara (2001) and Malioutov and Barzilay (2006). For each set of segmentation points z, the associated language models are set to their posterior expectations, e.g., θi = E[θ|{xt : zt = i}, α]. The Dirichlet prior is conjugate to the multinomial, so this expectation can be computed in closed form: n(i, j) + α θi�� = (3) N(i) + Wα where n(i, j) is the count of word j in segment i and N(i) is the total number of words in segment i (Bernardo and Smith, 2000). The symmetric Dirichlet prior α acts as a smoothing pseudo-count. In the multimodal context, the priors act to control the weight of each modality. If the prior for the verbal language m</context>
<context position="19375" citStr="Malioutov and Barzilay, 2006" startWordPosition="2974" endWordPosition="2977"> setting for ψ is found via a gradient-based search. This setting is then used to generate another segmentation, and the process is iterated until convergence, as in hard expectationmaximization. The Dirichlet priors on the language models are symmetric, and are chosen via crossvalidation. Sampling or gradient-based techniques may be used to estimate these parameters, but this is left for future work. Relation to other segmentation models Other cohesion-based techniques have typically focused on hand-crafted similarity metrics between sentences, such as cosine similarity (Galley et al., 2003; Malioutov and Barzilay, 2006). In contrast, the model described here is probabilistically motivated, maximizing the joint probability of the segmentation with the observed words and gestures. Our objective criterion is similar in form to that of Utiyama and Isahara (2001); however, in contrast to this prior 11 p(x, y|S) = i p({xt : zt = i}|θi)p({yt : zt = i}|φi) (1) p(θi)p(φi) (2) 11 p(S) = p(z) 856 work, our criterion is justified by a Bayesian approach. Also, while the smoothing in our approach arises naturally from the symmetric Dirichlet prior, Utiyama and Isahara apply Laplace’s rule and add pseudo-counts of one in a</context>
<context position="22419" citStr="Malioutov and Barzilay (2006)" startWordPosition="3467" endWordPosition="3470">ion quality is very poor, yielding a word error rate of 77%. Annotators were instructed to select segment boundaries that divide the dialogue into coherent topics. Segmentation points are required to coincide with sentence or turn boundaries. A second annotator – who is not an author on any paper connected with this research – provided an additional set of segment annotations on six documents. On this subset of documents, the Pk between annotators was .306, and the WindowDiff was .325 (these metrics are explained in the next subsection). This is similar to the interrater agreement reported by Malioutov and Barzilay (2006). Over the fifteen dialogues, a total of 7458 words were transcribed (497 per dialogue), spread over 1440 sentences or interrupted turns (96 per dialogue). There were a total of 102 segments (6.8 per dialogue), from a minimum of four to a maximum of ten. This rate of fourteen sentences or interrupted turns per segment indicates relatively finegrained segmentation. In the physics lecture corpus used by Malioutov and Barzilay (2006), there are roughly 100 sentences per segment. On the ICSI corpus of meeting transcripts, Galley et al. (2003) report 7.5 segments per meeting, with 770 “potential bo</context>
<context position="27683" citStr="Malioutov and Barzilay (2006)" startWordPosition="4325" endWordPosition="4328">ted against the logarithm of the verbal prior. Low values of the verbal prior cause it to dominate the segmentation; this can be seen at the left of both graphs, where the performance of the multimodal and verbal-only systems are nearly identical. High values of the verbal prior cause it to be oversmoothed, and performance thus approaches that of the gesture-only segmenter. Comparison to other models While much of the research on topic segmentation focuses on written text, there are some comparable systems that also aim at unsupervised segmentation of spontaneous spoken language. For example, Malioutov and Barzilay (2006) segment a corpus of classroom lectures, using similar lexical cohesion-based features. With manual transcriptions, they report a .383 Pk and .417 WD on artificial intelligence (AI) lectures, and .298 Pk and .311 WD on physics lectures. Our results are in the range bracketed by these two extremes; the wide range of results suggests that segmentation scores are difficult to compare across domains. The segmentation of physics lectures was at a very course level of granularity, while the segmentation of AI lectures was more similar to our annotations. We applied the publicly-available executable </context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation. Master’s thesis,</title>
<date>2006</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="28956" citStr="Malioutov, 2006" startWordPosition="4539" endWordPosition="4540"> yielding a .417 Pk and .465 WD even when both verbal and gestural features were available. 858 Pk WD log verbal prior log verbal prior 0.42 0.4 0.38 0.36 verbal−only multimodal −3 −2.5 −2 −1.5 −1 −0.5 0.34 0.32 0.42 0.4 0.38 0.36 verbal−only multimodal −3 −2.5 −2 −1.5 −1 −0.5 0.34 0.32 Figure 3: The multimodal and verbal-only performance using the reference transcript. The x-axis shows the logarithm of the verbal prior; the gestural prior is held fixed at the optimal value. This may be because the technique is not designed for the relatively fine-grained segmentation demanded by our dataset (Malioutov, 2006). 7 Conclusions This research shows a novel relationship between gestural cohesion and discourse structure. Automatically extracted gesture features are predictive of discourse segmentation when used in isolation; when lexical information is present, segmentation performance is further improved. This suggests that gestures provide unique information not present in the lexical features alone, even when perfect transcripts are available. There are at least two possibilities for how gesture might impact topic segmentation: “visual punctuation,” and cohesion. The visual punctuation view would atte</context>
</contexts>
<marker>Malioutov, 2006</marker>
<rawString>Igor Malioutov. 2006. Minimum cut model for spoken lecture segmentation. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8:243–281.</tech>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8:243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Martell</author>
</authors>
<title>FORM: An experiment in the annotation of the kinematics of gesture.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12928" citStr="Martell, 2005" startWordPosition="1936" endWordPosition="1937">ision.ucsd.edu/—pdollar/research/cuboids doc/index.html 854 Figure 1: The visual processing pipeline for the extraction of gestural codewords from video. Figure 2: Circles indicate the interest points extracted from this frame of the corpus. This visual feature representation is substantially lower-level than the descriptions of gesture form found in both the psychology and computer science literatures. For example, when manually annotating gesture, it is common to employ a taxonomy of hand shapes and trajectories, and to describe the location with respect to the body and head (McNeill, 1992; Martell, 2005). Working with automatic hand tracking, Quek (2003) automatically computes perceptually-salient gesture features, such as symmetric motion and oscillatory repetitions. In contrast, our feature representation takes the form of a vector of continuous values and is not easily interpretable in terms of how the gesture actually appears. However, this low-level approach offers several important advantages. Most critically, it requires no initialization and comparatively little tuning: it can be applied directly to any video with a fixed camera position and static background. Second, it is robust: wh</context>
</contexts>
<marker>Martell, 2005</marker>
<rawString>Craig Martell. 2005. FORM: An experiment in the annotation of the kinematics of gesture. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and Mind.</title>
<date>1992</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="2372" citStr="McNeill, 1992" startWordPosition="318" endWordPosition="319">rse segments are characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns. Lexical cohesion (Halliday and Hasan, 1976) forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes (Hearst, 1994). With gestural cohesion, we explore whether the same idea holds for gesture features. The motivation for this approach comes from a series of psycholinguistic studies suggesting that gesture supplements speech with meaningful and unique semantic content (McNeill, 1992; Kendon, 2004). We assume that repeated patterns in gesture are indicative of the semantic coherence that characterizes well-defined discourse segments. An advantage of this view is that gestures can be brought to bear on discourse analysis without undertaking the daunting task of recognizing and interpreting individual gestures. This is crucial because coverbal gesture – unlike formal sign language – rarely follows any predefined form or grammar, and may vary dramatically by speaker. A key implementational challenge is automatically extracting gestural information from raw video and represen</context>
<context position="4583" citStr="McNeill, 1992" startWordPosition="646" endWordPosition="647">most pronounced when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation –</context>
<context position="6617" citStr="McNeill, 1992" startWordPosition="964" endWordPosition="965">guous noun phrases can be resolved by examining the similarity of co-articulated gestures (Eisenstein and Davis, 2007). While noun phrase coreference can be viewed as a discourse processing task, we address the higher-level discourse phenomenon of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz</context>
<context position="12912" citStr="McNeill, 1992" startWordPosition="1933" endWordPosition="1935">ctor. 1http://vision.ucsd.edu/—pdollar/research/cuboids doc/index.html 854 Figure 1: The visual processing pipeline for the extraction of gestural codewords from video. Figure 2: Circles indicate the interest points extracted from this frame of the corpus. This visual feature representation is substantially lower-level than the descriptions of gesture form found in both the psychology and computer science literatures. For example, when manually annotating gesture, it is common to employ a taxonomy of hand shapes and trajectories, and to describe the location with respect to the body and head (McNeill, 1992; Martell, 2005). Working with automatic hand tracking, Quek (2003) automatically computes perceptually-salient gesture features, such as symmetric motion and oscillatory repetitions. In contrast, our feature representation takes the form of a vector of continuous values and is not easily interpretable in terms of how the gesture actually appears. However, this low-level approach offers several important advantages. Most critically, it requires no initialization and comparatively little tuning: it can be applied directly to any video with a fixed camera position and static background. Second, </context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and Mind. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Carlos Niebles</author>
<author>Hongcheng Wang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words.</title>
<date>2006</date>
<booktitle>In Proceedings of the British Machine Vision Conference.</booktitle>
<contexts>
<context position="9609" citStr="Niebles et al., 2006" startWordPosition="1413" endWordPosition="1416">ust to lighting and perspective changes, and they should capture the relevant movement in the video. The set of spatiotemporal interest points thereby provides a highly compressed representation of the key visual features. Purely spatial interest points have been successful in a variety of image processing tasks (Lowe, 1999), and spatiotemporal interest points are beginning to show similar advantages for video processing (Laptev, 2005). The use of spatiotemporal interest points is specifically motivated by techniques from the computer vision domain of activity recognition (Efros et al., 2003; Niebles et al., 2006). The goal of activity recognition is to classify video sequences into semantic categories: e.g., walking, running, jumping. As a simple example, consider the task of distinguishing videos of walking from videos of jumping. In the walking videos, the motion at most of the interest points will be horizontal, while in the jumping videos it will be vertical. Spurious vertical motion in a walking video is unlikely to confuse the classifier, as long as the majority of interest points move horizontally. The hypothesis of this paper is that just as such low-level movement features can be applied in a</context>
</contexts>
<marker>Niebles, Wang, Fei-Fei, 2006</marker>
<rawString>Juan Carlos Niebles, Hongcheng Wang, and Li Fei-Fei. 2006. Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words. In Proceedings of the British Machine Vision Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The Rich Transcription Fall</title>
<date>2003</date>
<note>(RT-03F) Evaluation plan.</note>
<contexts>
<context position="21320" citStr="NIST, 2003" startWordPosition="3297" endWordPosition="3298">e do not examine whether B’s gestures are relevant to discourse segmentation. Video was recorded using standard camcorders, with a resolution of 720 by 480 at 30 frames per second. The video was reduced to 360 by 240 grayscale images before visual analysis is applied. Audio was recorded using headset microphones. No manual postprocessing is applied to the video. Annotations and data processing All speech was transcribed by hand, and time stamps were obtained using the SPHINX-II speech recognition system for forced alignment (Huang et al., 1993). Sentence boundaries are annotated according to (NIST, 2003), and additional sentence boundaries are automatically inserted at all turn boundaries. Commonlyoccurring terms unlikely to impact segmentation are automatically removed by using a stoplist. For automatic speech recognition, the default Microsoft speech recognizer was applied to each sentence, and the top-ranked recognition result was reported. As is sometimes the case in real-world applications, no speaker-specific training data is available. The resulting recognition quality is very poor, yielding a word error rate of 77%. Annotators were instructed to select segment boundaries that divide t</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The Rich Transcription Fall 2003 (RT-03F) Evaluation plan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1152" citStr="Passonneau and Litman, 1997" startWordPosition="143" endWordPosition="146">bution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 1 Introduction When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels. Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features (Hearst, 1994; Hirschberg and Nakatani, 1998; Passonneau and Litman, 1997); this work informs various text and speech processing applications, such as automatic summarization and segmentation. Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure. Our method builds on the idea that coherent</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="23943" citStr="Pevzner and Hearst, 2002" startWordPosition="3710" endWordPosition="3713">lion words. The entire ICSI meeting corpus contains roughly 600,000 words, although only one third of this dataset was annotated for segmentation (Galley et al., 2003). The physics lecture corpus that was mentioned above contains 232,000 words (Malioutov and Barzilay, 2006). The task considered in this section is thus more difficult than much of the previous discourse segmentation work on two dimensions: there is less training data, and a finer-grained segmentation is required. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. These metrics are penalties, so lower values indicate better segmentations. The Pk metric expresses the probability that any randomly chosen pair of sentences is incorrectly segmented, if they are k sentences apart (Beeferman et al., 1999). Following tradition, k is set to half of the mean seg857 Method Pk WD 1. gesture only .486 .502 2. ASR only .462 .476 3. ASR + gesture .388 .401 4. transcript only .382 .397 5. transcript + gesture .332 .349 6. random .473 .526 7. equal-width .508 .515 Table 1: For each method, the score of the best performing configuration is shown. Pk and WD are </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Quek</author>
</authors>
<title>The catchment feature model for multimodal language analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV.</booktitle>
<contexts>
<context position="4691" citStr="Quek, 2003" startWordPosition="662" endWordPosition="663"> a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al., 2007), which presents an animated agent that is capable of augmenting navigation direction</context>
<context position="12979" citStr="Quek (2003)" startWordPosition="1943" endWordPosition="1944">l 854 Figure 1: The visual processing pipeline for the extraction of gestural codewords from video. Figure 2: Circles indicate the interest points extracted from this frame of the corpus. This visual feature representation is substantially lower-level than the descriptions of gesture form found in both the psychology and computer science literatures. For example, when manually annotating gesture, it is common to employ a taxonomy of hand shapes and trajectories, and to describe the location with respect to the body and head (McNeill, 1992; Martell, 2005). Working with automatic hand tracking, Quek (2003) automatically computes perceptually-salient gesture features, such as symmetric motion and oscillatory repetitions. In contrast, our feature representation takes the form of a vector of continuous values and is not easily interpretable in terms of how the gesture actually appears. However, this low-level approach offers several important advantages. Most critically, it requires no initialization and comparatively little tuning: it can be applied directly to any video with a fixed camera position and static background. Second, it is robust: while image noise may cause a few spurious interest p</context>
</contexts>
<marker>Quek, 2003</marker>
<rawString>Francis Quek. 2003. The catchment feature model for multimodal language analysis. In Proceedings of ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Structure and intonation in spoken language understanding.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="6862" citStr="Steedman, 1990" startWordPosition="1002" endWordPosition="1003">of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between ges</context>
</contexts>
<marker>Steedman, 1990</marker>
<rawString>Mark Steedman. 1990. Structure and intonation in spoken language understanding. In Proceedings of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Swerts</author>
</authors>
<title>Prosodic features at discourse boundaries of different strength.</title>
<date>1997</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<pages>101--514</pages>
<contexts>
<context position="6904" citStr="Swerts, 1997" startWordPosition="1008" endWordPosition="1009">ior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structure is a relative</context>
</contexts>
<marker>Swerts, 1997</marker>
<rawString>Marc Swerts. 1997. Prosodic features at discourse boundaries of different strength. The Journal of the Acoustical Society of America, 101:514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-Tur</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Integrating prosodic and lexical cues for automatic topic segmentation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="7397" citStr="Tur et al., 2001" startWordPosition="1080" endWordPosition="1083">key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structure is a relatively unexplored area, at least with respect to computational approaches. One conclusion that emerges from our analysis is that gesture may signal discourse structure in a different way than prosody does: while specific prosodic markers characterize segment boundaries, gesture predicts segmentation through intrasegmental cohesion. The combination of these two 853 modalities is an exciting direction for future research. 3 Visual Features for Discourse Analysis This section describes the proce</context>
</contexts>
<marker>Tur, Hakkani-Tur, Stolcke, Shriberg, 2001</marker>
<rawString>Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, and Elizabeth Shriberg. 2001. Integrating prosodic and lexical cues for automatic topic segmentation. Computational Linguistics, 27(1):31–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="17539" citStr="Utiyama and Isahara (2001)" startWordPosition="2668" endWordPosition="2671">fferent priors for the verbal and gestural language models allows us to weight these modalities in a Bayesian framework. Finally, we model the probability of the segmentation z by considering the durations of each segment: p(z) = Hi p(dur(i)|ψ). A negativebinomial distribution with parameter ψ is applied to discourage extremely short or long segments. Inference Crucially, both the likelihood (equation 1) and the prior (equation 2) factor into a product across the segments. This factorization enables the optimal segmentation to be found using a dynamic program, similar to those demonstrated by Utiyama and Isahara (2001) and Malioutov and Barzilay (2006). For each set of segmentation points z, the associated language models are set to their posterior expectations, e.g., θi = E[θ|{xt : zt = i}, α]. The Dirichlet prior is conjugate to the multinomial, so this expectation can be computed in closed form: n(i, j) + α θi�� = (3) N(i) + Wα where n(i, j) is the count of word j in segment i and N(i) is the total number of words in segment i (Bernardo and Smith, 2000). The symmetric Dirichlet prior α acts as a smoothing pseudo-count. In the multimodal context, the priors act to control the weight of each modality. If t</context>
<context position="19618" citStr="Utiyama and Isahara (2001)" startWordPosition="3011" endWordPosition="3014">mmetric, and are chosen via crossvalidation. Sampling or gradient-based techniques may be used to estimate these parameters, but this is left for future work. Relation to other segmentation models Other cohesion-based techniques have typically focused on hand-crafted similarity metrics between sentences, such as cosine similarity (Galley et al., 2003; Malioutov and Barzilay, 2006). In contrast, the model described here is probabilistically motivated, maximizing the joint probability of the segmentation with the observed words and gestures. Our objective criterion is similar in form to that of Utiyama and Isahara (2001); however, in contrast to this prior 11 p(x, y|S) = i p({xt : zt = i}|θi)p({yt : zt = i}|φi) (1) p(θi)p(φi) (2) 11 p(S) = p(z) 856 work, our criterion is justified by a Bayesian approach. Also, while the smoothing in our approach arises naturally from the symmetric Dirichlet prior, Utiyama and Isahara apply Laplace’s rule and add pseudo-counts of one in all cases. Such an approach would be incapable of flexibly balancing the contributions of each modality. 5 Evaluation Setup Dataset Our dataset is composed of fifteen audiovideo recordings of dialogues limited to three minutes in duration. The </context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL, pages 491–498.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>