<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995507">
Timeline Generation through Evolutionary Trans-Temporal Summarization
</title>
<author confidence="0.998315">
Rui Yan†, Liang Kong† , Congrui Huang†, Xiaojun Wan$, Xiaoming Lib, Yan Zhang†*
</author>
<affiliation confidence="0.989494">
†School of Electronics Engineering and Computer Science, Peking University, China
$Institute of Computer Science and Technology, Peking University, China
bState Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China
</affiliation>
<email confidence="0.9544715">
{r.yan,kongliang,hcr,lxm}@pku.edu.cn,
wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn
</email>
<sectionHeader confidence="0.981811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999828208333333">
We investigate an important and challeng-
ing problem in summary generation, i.e.,
Evolutionary Trans-Temporal Summarization
(ETTS), which generates news timelines from
massive data on the Internet. ETTS greatly
facilitates fast news browsing and knowl-
edge comprehension, and hence is a neces-
sity. Given the collection of time-stamped web
documents related to the evolving news, ETTS
aims to return news evolution along the time-
line, consisting of individual but correlated
summaries on each date. Existing summariza-
tion algorithms fail to utilize trans-temporal
characteristics among these component sum-
maries. We propose to model trans-temporal
correlations among component summaries for
timelines, using inter-date and intra-date sen-
tence dependencies, and present a novel com-
bination. We develop experimental systems to
compare 5 rival algorithms on 6 instinctively
different datasets which amount to 10251 doc-
uments. Evaluation results in ROUGE metrics
indicate the effectiveness of the proposed ap-
proach based on trans-temporal information.
</bodyText>
<sectionHeader confidence="0.992522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988242975">
Along with the rapid growth of the World Wide
Web, document floods spread throughout the Inter-
net. Given a large document collection related to
a news subject (for example, BP Oil Spill), readers
get lost in the sea of articles, feeling confused and
powerless. General search engines can rank these
∗Corresponding author.
news webpages by relevance to a user specified as-
pect, i.e., a query such as “first relief effort for BP
Oil Spill”, but search engines are not quite capable
of ranking documents given the whole news subject
without particular aspects. Faced with thousands of
news documents, people usually have a myriad of in-
terest aspects about the beginning, the development
or the latest situation. However, traditional infor-
mation retrieval techniques can only rank webpages
according to their understanding of relevance, which
is obviously insufficient (Jin et al., 2010).
Even if the ranked documents could be in a satis-
fying order to help users understand news evolution,
readers prefer to monitor the evolutionary trajecto-
ries by simply browsing rather than navigate every
document in the overwhelming collection. Summa-
rization is an ideal solution to provide an abbrevi-
ated, informative reorganization for faster and bet-
ter representation of news documents. Particularly,
a timeline (see Table 1) can summarize evolutionary
news as a series of individual but correlated com-
ponent summaries (items in Table 1) and offer an
option to understand the big picture of evolution.
With unique characteristics, summarizing time-
lines is significantly different from traditional sum-
marization methods which are awkward in such sce-
narios. We first study a manual timeline of BP Oil
Spill in Mexico Gulf in Table 1 from Reuters News1
to understand why timelines generation is observ-
ably different from traditional summarization. No
traditional method has considered to partition corpus
into subsets by timestamps for trans-temporal cor-
relations. However, we discover two unique trans-
</bodyText>
<footnote confidence="0.963801">
1http://www.reuters.com
</footnote>
<page confidence="0.55653">
433
</page>
<note confidence="0.996811">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 433–443,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<tableCaption confidence="0.9775635">
Table 1: Part of human generated timeline about BP Oil
Spill in 2010 from Reuters News website.
</tableCaption>
<note confidence="0.959125285714286">
April 22, 2010
The Deepwater Horizon rig, valued at more than $560 million,
sinks and a five mile long (8 km) oil slick is seen.
April 25, 2010
The Coast Guard approves a plan to have remote underwater vehi-
cles activate a blowout preventer and stop leak. Efforts to activate
the blowout preventer fail.
April 28, 2010
The Coast Guard says the flow of oil is 5,000 barrels per day (bpd)
(210,000 gallons/795,000 litres) – five times greater than first esti-
mated. A controlled burn is held on the giant oil slick.
April 29, 2010
U.S. President Barack Obama pledges “every single available re-
source,” including the U.S. military, to contain the spreading spill.
Obama also says BP is responsible for the cleanup. Louisiana de-
clares state of emergency due to the threat to the state’s natural
resources.
April 30, 2010
An Obama aide says no drilling will be allowed in new areas, as the
president had recently proposed, until the cause of the Deepwater
Horizon accident is known.
</note>
<bodyText confidence="0.998900181818182">
temporal characteristics of component summaries
from the handcrafted timeline. Individuality. The
component summaries are summarized locally: the
component item on date t is constituted by sentences
with timestamp t. Correlativeness. The compo-
nent summaries are correlative across dates, based
on the global collection. To the best of our knowl-
edge, no traditional method has examined the rela-
tionships among these timeline items.
Although it is profitable, summarizing timeline
faces with new challenges:
</bodyText>
<listItem confidence="0.957296916666667">
• The first challenge for timeline generation is
to deliver important contents and avoid information
overlaps among component summaries under the
trans-temporal scenario based on global/local source
collection. Component items are individual but not
completely isolated due to the dynamic evolution.
• As we have individuality and correlativeness
to evaluate the qualities of component summaries,
both locally and globally, the second challenge is to
formulate the combination task into a balanced op-
timization problem to generate the timelines which
satisfy both standards with maximum utilities.
</listItem>
<bodyText confidence="0.999042833333333">
We introduce a novel approach for the web min-
ing problem Evolutionary Trans-Temporal Summa-
rization (ETTS). Taking a collection relevant to a
news subject as input, the system automatically out-
puts a timeline with items of component summaries
which represent evolutionary trajectories on specific
dates. We classify sentence relationships as inter-
date and intra-date dependencies. Particularly, the
inter-date dependency calculation includes temporal
decays to project sentences from all dates onto the
same time horizon (Figure 1 (a)). Based on intra-
/inter-date sentence dependencies, we then model
affinity and diversity to compute the saliency score
of each sentence and merge local and global rank-
ings into one unified ranking framework. Finally we
select top ranked sentences. We build an experimen-
tal system on 6 real datasets to verify the effective-
ness of our methods compared with other 4 rivals.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.993416258064516">
Multi-document summarization (MDS) aims to pro-
duce a summary delivering the majority of informa-
tion content from a set of documents and has drawn
much attention in recent years. Conferences such as
ACL, SIGIR, EMNLP, etc., have advanced the tech-
nology and produced several experimental systems.
Generally speaking, MDS methods can be either
extractive or abstractive summarization. Abstractive
summarization (e.g. NewsBlaster2) usually needs
information fusion, sentence compression and refor-
mulation. We focus on extraction-based methods,
which usually involve assigning saliency scores to
some units (e.g. sentences, paragraphs) of the docu-
ments and extracting the units with highest scores.
To date, various extraction-based methods have
been proposed for generic multi-document summa-
rization. The centroid-based method MEAD (Radev
et al., 2004) is an implementation of the centroid-
based method that scores sentences based on fea-
tures such as cluster centroids, position, and TF.IDF,
etc. NeATS (Lin and Hovy, 2002) adds new features
such as topic signature and term clustering to select
important content, and use MMR (Goldstein et al.,
1999) to remove redundancy.
Graph-based ranking methods have been pro-
posed to rank sentences/passages based on “votes”
or “recommendations” between each other. Tex-
tRank (Mihalcea and Tarau, 2005) and LexPageR-
ank (Erkan and Radev, 2004) use algorithms similar
to PageRank and HITS to compute sentence impor-
tance. Wan et al. have improved the graph-ranking
</bodyText>
<footnote confidence="0.805522">
2http://www1.cs.columbia.edu/nlp/newsblaster/
</footnote>
<page confidence="0.890076">
434
</page>
<bodyText confidence="0.999536142857143">
algorithm by differentiating intra-document and
inter-document links between sentences (2007b),
and have proposed a manifold-ranking method to
utilize sentence-to-sentence and sentence-to-topic
relationships (Wan et al., 2007a).
ETTS seems to be related to a very recent task of
“update summarization” started in DUC 2007 and
continuing with TAC. However, update summariza-
tion only dealt with a single update and we make a
novel contribution with multi-step evolutionary up-
dates. Further related work includes similar timeline
systems proposed by (Swan and Allan, 2000) us-
ing named entities, by (Allan et al., 2001) measured
in usefulness and novelty, and by (Chieu and Lee,
2004) measured in interest and burstiness. We have
proposed a timeline algorithm named “Evolution-
ary Timeline Summarization (ETS)” in (Yan et al.,
2011b) but the refining process based on generated
component summaries is time consuming. We aim
to seek for more efficient summarizing approach.
To the best of our knowledge, neither update sum-
marization nor traditional systems have considered
the relationship among “component summaries”, or
have utilized trans-temporal properties. ETTS ap-
proach can also naturally and simultaneously take
into account global/local summarization with biased
information richness and information novelty, and
combine both summarization in optimization.
</bodyText>
<sectionHeader confidence="0.998591" genericHeader="method">
3 Trans-temporal Summarization
</sectionHeader>
<bodyText confidence="0.9993485">
We conduct trans-temporal summarization based on
the global biased graph using inter-date dependency
and local biased graph using intra-date dependency.
Each graph is the complementary graph to the other.
</bodyText>
<subsectionHeader confidence="0.999393">
3.1 Global Biased Summarization
</subsectionHeader>
<bodyText confidence="0.999940285714286">
The intuition for global biased summarization is that
the selected summary should be correlative with sen-
tences from neighboring dates, especially with those
informative ones. To generate the component sum-
mary on date t, we project all sentences in the collec-
tion onto the time horizon of t to construct a global
affinity graph, using temporal decaying kernels.
</bodyText>
<subsectionHeader confidence="0.835708">
3.1.1 Temporal Proximity Based Projection
</subsectionHeader>
<bodyText confidence="0.999516666666667">
Clearly, a major technical challenge in ETTS is
how to define the temporal biased projection func-
tion Γ(Δt), where Δt is the distance between the
</bodyText>
<figureCaption confidence="0.999856">
Figure 1: Construct global/local biased graphs. Solid cir-
cles denote intra-date sentences on the pending date t and
dash ones represent inter-date sentences from other dates.
Figure 2: Proximity-based kernel functions, where σ=10.
</figureCaption>
<bodyText confidence="0.998841">
pending date t and neighboring date t&apos;, i.e., Δt =
|t&apos; − t|. As in (Lv and Zhai, 2009), we present 5
representative kernel functions: Gaussian, Triangle,
Cosine, Circle, and Window, shown in Figure 2. Dif-
ferent kernels lead to different projections.
</bodyText>
<listItem confidence="0.521706">
1. Gaussian kernel
</listItem>
<equation confidence="0.618324">
−Δt2
2σ2 ]
</equation>
<listItem confidence="0.6199465">
2. Triangle kernel
�
</listItem>
<equation confidence="0.817256333333333">
1 − Δt σ if Δt ≤ σ
Γ(Δt) =
0 otherwise
</equation>
<listItem confidence="0.6686956">
3. Cosine (Hamming) kernel
Γ(Δt) = 12 [1 + cos(` or
%`)] if Δt ≤ σ
0 otherwise
4. Circle kernel
</listItem>
<equation confidence="0.689570875">
(Δt) = V1 − (Δtσ )2 if Δt ≤ σ
0 otherwise
Γ(Δt) = exp[
435
5. Window kernel any date from T. |C |is the sentences set size and
( Nw is the number of sentences containing term w.
1 if Δt ≤ σ We let p(si → si|t)=0 to avoid self transition.
Γ(Δt) = Note that although f(.) is a symmetric function,
</equation>
<bodyText confidence="0.986708142857143">
0 otherwise p(si → sj|t) is usually not equal to p(sj → si|t),
All kernels have one parameter σ to tune, which depending on the degrees of nodes si and sj.
controls the spread of kernel curves, i.e., it restricts Now we establish the affinity matrix Mti,jand by
the projection scope of each sentence. In general, using the general form of PageRank, we obtain:
the optimal setting of σ may vary according to the λ~ = µM−1~λ + 1 − µ
news set because sentences presumably would have |C |e~ (4)
wider semantic scope in certain news subjects, thus ~
requiring a higher value of σ and vice versa. where λ is the selective probability of all sentence
3.1.2 Modeling Global Affinity nodes and e~ is a column vector with all elements
Given the sentence collection C partitioned by the equaling to 1. µ is the damping factor set as 0.85.
timestamp set T, C = {C1, C2,..., C|T |}, we ob- Usually the convergence of the iteration algorithm is
tain Ct = {sti|1 ≤ i ≤ |Ct|} where si is a sentence achieved when difference between the scores com-
with the timestamp t = tsi. When we generate com- puted at two successive iterations for any sentences
ponent summary on t, we project all sentences onto falls below a given threshold (0.0001 in this study).
time horizon t. After projection, all sentences are 3.1.3 Modeling Diversity
weighted by their influence on t. We use an affinity Diversity is to reflect both biased information
matrix Mt with the entry of the inter-date transition richness and sentence novelty, which aims to reduce
probability on date t. The sum of each row equals to information redundancy. However, using standard
1. Note that for the global biased matrix, we mea- PageRank of Equation (4) will not result in diver-
sure the affinity between local sentences from t and sity. The aggregational effect of PageRank assigns
global sentences from other dates. Therefore, intra- high salient scores to closely connected node com-
date transition probability between sentences with munities (Figure 3 (b)). A greedy vertex selection
the timestamp t is set to 0 for local summarization. algorithm may achieve diversity by iteratively se-
Mti,j is the transition probability of si to sj based lecting the most prestigious vertex and then penal-
on the perspective of date t, i.e., p(si → sj|t): izing the vertices “covered” by the already selected
( f(si—+sj|t) if Pf =60 ones, such as Maximum Marginal Relevance and its
p(si → sj |t) = E |C |f (si—+sk |t) applications in Wan et al. (2007b; 2007a). Most re-
0 if tsi = tsj = t cently diversity rank DivRank is another solution
(1) to diversity penalization in (Mei et al., 2010).
f(si → sj|t) is defined as the temporal weighted We incorporate DivRank in our general ranking
cosine similarity between two sentences: framework, which creates a dynamic M during each
f(si → sj|t) = X π(w, si|t) · π(w, sj|t) (2) iteration, rather than a static one. After z times of
wEsinsj iteration, the matrix M becomes:
where the weight π associated with term w is calcu- M(z) = µM(z−1) · ~λ(z−1) + 1 − µe~ (5)
lated with the temporal weighted tf.isf formula: |C|
π(w, s|t) = Γ|t − ts|· tf(w,s)(1 + log( |C |Equation (5) raises the probability for nodes with
qP. higher centrality and nodes already having high
|s|(tf(w, s)(1 + log(Nw weights are likely to “absorb” the weights of its
|C  |)))2 neighbors directly, and the weights of neighbors’
Nw neighbors indirectly. The process is to iteratively ad-
)) ~
(3) just matrix M according to λ and then to update λ
where ts is the timestamp of sentence s, and according to the changed M. As iteration increases
tf(w, s) is the term frequency of w in s. ts can be
436
there emerges a rich-gets-richer phenomenon (Fig-
ure 3 (c) and (d)). By incorporating DivRank, we
obtain rank r†i and the global biased ranking score
Gi for sentence si from date t to summarize Ct.
</bodyText>
<subsectionHeader confidence="0.999819">
3.2 Local Biased Summarization
</subsectionHeader>
<bodyText confidence="0.99997325">
Naturally, the component summary for date t should
be informative within Ct. Given the sentence col-
lection Ct = {sti|1 ≤ i ≤ |Ct|}, we build an affin-
ity matrix for Figure 1 (b), with the entry of intra-
date transition probability calculated from standard
cosine similarity. We incorporate DivRank within
local summarization and we obtain the local biased
rank and ranking score for si, denoted as r‡i and Li.
</bodyText>
<subsectionHeader confidence="0.999828">
3.3 Optimization of Global/Local Combination
</subsectionHeader>
<bodyText confidence="0.999986625">
We do not directly add the global biased ranking
score and local biased ranking score, as many previ-
ous works did (Wan et al., 2007b; Wan et al., 2007a),
because even the same ranking score gap may indi-
cate different rank gaps in two ranking lists.
Given subset Ct, let R = {ri}(i = 1,... ,|Ct|), ri
is the final ranking of si to estimate, optimize the
following objective cost function O(R),
</bodyText>
<equation confidence="0.981773428571429">
†
i
Gik ri −rk2
Ψi Gi
(6)
‡
Lik Ψ− Li i k2
</equation>
<bodyText confidence="0.98001125">
where Gi is the global biased ranking score while Li
is the local biased ranking score. Ψi is expected to
be the merged ranking score, namely sentence im-
portance, which will be defined later. Among the
two components in the objective function, the first
component means that the refined rank should not
deviate too much from the global biased rank. We
use kriΨi − �† i k2 instead ofkri − rz k2 in order to dis-
tinguish the differences between sentences from the
same rank gap. The second component is similar by
refining rank from local biased summarization.
Our goal is to find R = R∗ to minimize the cost
function, i.e., R∗ = argmin{O(R)}. R∗ is the final
rank merged by our algorithm. To minimize O(R),
we compute its first-order partial derivatives.
∂O(R)
</bodyText>
<equation confidence="0.996983222222222">
∂ri 2α ( Gi ( Li
Ψi Ψi Ψi Ψi
ri − r† i) + 2β ri − r‡ i ) (7)
Let ∂O(R)
∂ri = 0, we get
αΨir† i + βΨir‡
r∗ i
i = (8)
αGi + βLi
</equation>
<bodyText confidence="0.998488">
Two special cases are that if (1) α = 0, β =6 0:
we obtain ri = Ψir‡i /Li, indicating we only use the
local ranking score. (2) α =6 0, β = 0, indicating we
ignore local ranking score and only consider global
biased summarization using inter-date dependency.
There can be many ways to calculate the sen-
tence importance Ψi. Here we define Ψi as the
weighted combination of itself with ranking scores
from global biased and local biased summarization:
</bodyText>
<equation confidence="0.900905666666667">
Ψ(z) i = αGi + βLi + γΨ(z−1)
i . (9)
α + β + γ
To save one parameter we let α+β+γ = 1. In the z-
th iteration, r(z)
i is dependent on Ψ(z−1) and Ψ(z)
i is
i
indirectly dependent on r(z)
</equation>
<bodyText confidence="0.86375425">
i via Ψ(z−1) i . Ψ(0) i= 0.
We iteratively approximate final Ψi for the ultimate
rank list R∗. The expectation of stable Ψi is obtained
when Ψ(z)
</bodyText>
<equation confidence="0.9997312">
i = Ψ(z−1) i . Final Ψi is expected to satisfy
Ψi = αGi + βLi + γΨi:
αGi + βLi αGi + βLi
Ψi = = (10)
1 − γ α + β
</equation>
<bodyText confidence="0.9838635">
Final Ψi is dependent only on original global/local
biased ranking scores. Equation (8) becomes more
concise with no Ψ or γ: r∗ is a weighted combina-
tion of global and local ranks by αβ (α =6 0, β =6 0):
</bodyText>
<equation confidence="0.99459425">
(11)
1 r† i + 1
1 + α/β r‡ i
1 + β/α
</equation>
<sectionHeader confidence="0.990825" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.844247">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99999225">
There is no existing standard test set for ETTS meth-
ods. We randomly choose 6 news subjects with
special coverage and handcrafted timelines by ed-
itors from 10 selected news websites: these 6 test
sets consist of news datasets and golden standards to
evaluate our proposed framework empirically, which
amount to 10251 news articles. As shown in Ta-
ble 2, three of the sources are in UK, one of them
</bodyText>
<equation confidence="0.9456854">
O(R) =α � |Ct|
i=1
|Ct|
+ β
i=1
i
α + βr† i + β
α + βr‡
α
r∗i =
</equation>
<page confidence="0.573078">
437
</page>
<figure confidence="0.978608">
(a) An illustrative network. (a) PageRank on t. (b) DivRank on t (c) DivRank on t&apos;
</figure>
<figureCaption confidence="0.772652333333333">
Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from
DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect
temporal biased processes on t {1,9} in (c) and t&apos; {2,12} in (d).
</figureCaption>
<bodyText confidence="0.9958385">
is in China and the rest are in the US. We choose
these sites because many of them provide timelines
edited by professional editors, which serve as refer-
ence summaries. The news belongs to different cate-
gories of Rule of Interpretation (ROI) (Kumaran and
Allan, 2004). More detailed statistics are in Table 3.
</bodyText>
<tableCaption confidence="0.987675">
Table 2: News sources of 6 datasets
</tableCaption>
<table confidence="0.775903">
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
</table>
<tableCaption confidence="0.979469">
Table 3: Detailed basic information of 6 datasets.
</tableCaption>
<table confidence="0.942539818181818">
News Subjects #size #docs #stamps #RT AL
1.Influenza A 115026 2557 331 5 83
2.Financial Crisis 176435 2894 427 2 118
3.BP Oil Spill 63021 1468 135 6 76
4.Haiti Earthquake 12073 247 83 2 32
5.Jackson Death 37819 925 168 3 64
6.Obama Presidency 79761 2160 349 5 92
size: the whole sentence counts; #stamps: the number of timestamps;
Note average size of subsets is calculated as: avg.size=#size/#stamps;
RT: reference timelines; AL: avg. length of RT measured in sentences.
4.2 Experimental System Setups
</table>
<listItem confidence="0.943846">
• Preprocessing. As ETTS faces with much larger
</listItem>
<bodyText confidence="0.824338888888889">
corpus compared with traditional MDS, we apply
further data preprocessing besides stemming and
stop-word removal. We extract text snippets repre-
senting atomic “events” from all documents with a
toolkit provided by Yan et al. (2010; 2011a), by
which we attempt to assign more fine-grained and
accurate timestamps for every sentence within the
text snippets. After the snippet extraction procedure,
we filter the corpora by discarding non-event texts.
</bodyText>
<listItem confidence="0.610351923076923">
• Compression Rate and Date Selection. After
preprocessing, we obtain numerous snippets with
fine-grained timestamps, and then decompose them
into temporally tagged sentences as the global col-
lection C. We partition C according to timestamps
of sentences, i.e., C = C1 U C2 U · · · U C|T |.
Each component summary is generated from its cor-
responding sub-collection. The sizes of component
summaries are not necessarily equal, and moreover,
not all dates may be represented, so date selection
is also important. We apply a simple mechanism
that users specify the overall compression rate φ, and
we extract more sentences for important dates while
</listItem>
<bodyText confidence="0.887337">
fewer sentences for others. The importance of dates
is measured by the burstiness, which indicates prob-
able significant occurrences (Chieu and Lee, 2004).
The compression rate on ti is set as φi = |Ci|
|C |.
</bodyText>
<subsectionHeader confidence="0.990628">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9952713">
The ROUGE measure is widely used for evaluation
(Lin and Hovy, 2003): the DUC contests usually of-
ficially employ ROUGE for automatic summariza-
tion evaluation. In ROUGE evaluation, the summa-
rization quality is measured by counting the num-
ber of overlapping units, such as N-gram, word se-
quences, and word pairs between the candidate time-
lines CT and the reference timelines RT. There are
several kinds of ROUGE metrics, of which the most
important one is ROUGE-N with 3 sub-metrics:
</bodyText>
<equation confidence="0.92480345">
1 ROUGE-N-R is an N-gram recall metric:
Countmatch(N-gram)
ROUGE-N-R =
E
IERT
E
N-gramEI
E
IERT
Count (N-gram)
E
N-gramEI
438
2 ROUGE-N-P is an N-gram precision metric:
Countmatch(N-gram)
ROUGE-N-P =
3 ROUGE-N-F is an N-gram F1 metric:
ROUGE-N-F =
2 × ROUGE-N-P × ROUGE-N-R
ROUGE-N-P + ROUGE-N-R
</equation>
<bodyText confidence="0.999898210526316">
I denotes a timeline. N in these metrics stands for
the length of N-gram and N-gramERT denotes the
N-grams in reference timelines while N-gramECT
denotes the N-grams in the candidate timeline.
Countmatch(N-gram) is the maximum number of N-
gram in the candidate timeline and in the set of ref-
erence timelines. Count(N-gram) is the number of N-
grams in reference timelines or candidate timelines.
According to (Lin and Hovy, 2003), among all
sub-metrics, unigram-based ROUGE (ROUGE-1)
has been shown to agree with human judgment most
and bigram-based ROUGE (ROUGE-2) fits summa-
rization well. We report three ROUGE F-measure
scores: ROUGE-1, ROUGE-2, and ROUGE-W,
where ROUGE-W is based on the weighted longest
common subsequence. The weight W is set to be
1.2 in our experiments by ROUGE package (version
1.55). Intuitively, the higher the ROUGE scores, the
similar the two summaries are.
</bodyText>
<subsectionHeader confidence="0.912126">
4.4 Algorithms for Comparison
</subsectionHeader>
<bodyText confidence="0.999852058823529">
We implement the following widely used sum-
marization algorithms as baseline systems. They
are designed for traditional summarization without
trans-temporal dimension. The first intuitive way to
generate timelines by these methods is via a global
summarization on collection C and then distribu-
tion of selected sentences to their source dates. The
other one is via an equal summarization on all local
sub-collections. For baselines, we average both in-
tuitions as their performance scores. For fairness we
conduct the same preprocessing for all baselines.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al., 2004) to extract sentences according
to the following three parameters: centroid value,
positional value, and first-sentence overlap.
GMDS: The graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
Chieu: (Chieu and Lee, 2004) present a simi-
lar timeline system with different goals and frame-
works, utilizing interest and burstiness ranking but
neglecting trans-temporal news evolution.
ETTS: ETTS is an algorithm with optimized
combination of global/local biased summarization.
RefTL: As we have used multiple human time-
lines as references, we not only provide ROUGE
evaluations of the competing systems but also of the
human timelines against each other, which provides
a good indicator as to the upper bound ROUGE
score that any system could achieve.
</bodyText>
<subsectionHeader confidence="0.994666">
4.5 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999239142857143">
We use a cross validation manner among 6 datasets,
i.e., train parameters on one subject set and exam-
ine the performance on the others. After 6 training-
testing processes, we take the average F-score per-
formance in terms of ROUGE-1, ROUGE-2, and
ROUGE-W on all sets. The overall results are shown
in Figure 4 and details are listed in Tables 4∼6.
</bodyText>
<figureCaption confidence="0.986765">
Figure 4: Overall performance on 6 datasets.
</figureCaption>
<bodyText confidence="0.955824">
From the results, we have following observations:
</bodyText>
<listItem confidence="0.945320666666667">
• Random has the worst performance as expected.
• The results of Centroid are better than those of
Random, mainly because the Centroid method takes
</listItem>
<figure confidence="0.998521222222222">
E
IECT
E
N-gramEI
E
IECT
Count (N-gram)
E
N-gramEI
</figure>
<page confidence="0.755485">
439
</page>
<tableCaption confidence="0.99737">
Table 4: Overall performance comparison on Influenza
</tableCaption>
<table confidence="0.9256641">
A (ROI∗ category: Science) and Financial Crisis (ROI
category: Finance). α=0.4, kernel=Gaussian, u=60.
1. Influenza A 2. Financial Crisis
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.491 0.114 0.161 0.458 0.112 0.159
Random 0.257 0.039 0.081 0.230 0.030 0.071
Centroid 0.331 0.050 0.114 0.305 0.041 0.108
GMDS 0.364 0.062 0.130 0.327 0.054 0.110
Chieu 0.350 0.059 0.128 0.325 0.052 0.109
ETTS 0.375 0.071 0.132 0.339 0.058 0.112
</table>
<tableCaption confidence="0.639624">
Table 5: Overall performance comparison on BP Oil
(ROI category: Accidents) and Haiti Quake (ROI cate-
gory: Disasters). α=0.4, kernel=Gaussian, u=30.
</tableCaption>
<table confidence="0.9998295">
3. BP Oil 4. Haiti Quake
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.517 0.135 0.183 0.528 0.139 0.187
Random 0.262 0.041 0.096 0.266 0.043 0.093
Centroid 0.369 0.062 0.128 0.362 0.060 0.129
GMDS 0.389 0.084 0.139 0.380 0.106 0.137
Chieu 0.384 0.083 0.139 0.383 0.110 0.138
ETTS 0.441 0.107 0.158 0.436 0.111 0.145
</table>
<tableCaption confidence="0.556232">
Table 6: Overall performance comparison on Jackson
Death (ROI category: Legal Cases) and Obama Presi-
dency (ROI category: Politics). α=0.4, kernel=Gaussian,
u=30.
</tableCaption>
<table confidence="0.939278888888889">
5. Jackson Death 6. Obama Presidency
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.482 0.113 0.161 0.495 0.115 0.163
Random 0.232 0.033 0.080 0.254 0.039 0.084
Centroid 0.320 0.051 0.109 0.325 0.053 0.111
GMDS 0.341 0.059 0.127 0.359 0.061 0.129
Chieu 0.344 0.059 0.128 0.346 0.060 0.125
ETTS 0.358 0.061 0.130 0.369 0.074 0.133
∗ROI: news categorization defined by Linguistic Data Consortium.
</table>
<bodyText confidence="0.940430235294118">
into account positional value and first-sentence over-
lap, which facilitate main aspects summarization.
• The GMDS system outperforms centroid-based
summarization methods. This is due to the fact that
PageRank-based framework ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
Traditional MDS only consider sentence selection
from either the global or the local scope, and hence
bias occurs. Mis-selected sentences result in a low
recall. Generally the performance of global priority
intuition (i.e. only global summarization and then
distribution to temporal subsets) is better than local
priority methods (only local summarization). Proba-
ble bias is enlarged by searching for worthy sentence
in single dates. However, precision drops due to ex-
cessive choice of global timeline-worthy sentences.
</bodyText>
<figureCaption confidence="0.999962">
Figure 5: α/0: global/local combination.
Figure 7: u on short topics (&lt;1 year).
</figureCaption>
<listItem confidence="0.99673675">
• In general, the result of Chieu is better than
Centroid but unexpectedly, worse than GMDS. The
reason may be that Chieu does not capture suffi-
cient timeline attributes. The “interest” modeled
</listItem>
<figureCaption confidence="0.984607">
Figure 6: u on long topics (≥1 year).
</figureCaption>
<page confidence="0.66161">
440
</page>
<bodyText confidence="0.9985475">
in the algorithms actually performs flat clustering-
based summarization which is proved to be less use-
ful (Wang and Li, 2010). GMDS utilizes sentence
linkage, and partly captures “correlativeness”.
</bodyText>
<listItem confidence="0.933702888888889">
• ETTS under our proposed framework outper-
forms baselines, indicating that the properties we
use for timeline generation are beneficial. We also
add a direct comparison between ETTS and ETS
(Yan et al., 2011b). We notice that both balanced
algorithms achieve comparable performance (0.386
v.s. 0.412: a gap of 0.026 in terms of ROUGE-
1), but ETTS is much faster than ETS. It is under-
standable that ETS refines timelines based on neigh-
</listItem>
<bodyText confidence="0.818852857142857">
boring component summaries iteratively while for
ETTS neighboring information is incorporated in
temporal projection and hence there is no such pro-
cedure. Furthermore, ETS has 8 free parameters to
tune while ETTS has only 2 parameters. In other
words, ETTS is more simple to control.
• The performance on intensive focused news
within short time range (|last timestamp−first times-
tamp |&lt;1 year) is better than on long lasting news.
Having proved the effectiveness of our proposed
methods, we carry the next move to identity how
global−local combination ratio α/0 and projection
kernels take effects to enhance the quality of a sum-
mary in parameter tuning.
</bodyText>
<subsectionHeader confidence="0.976274">
4.6 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.9999722">
Each time we tune one parameter while others are
fixed. To identify how global and local biased sum-
marization combine, we provide experiments on the
performance of varying α/0 in Figure 5. Results in-
dicate that a balance between global and local biased
summarization is essential for timeline generation
because the performance is best when αβ ∈ [10,100]
and outperforms global and local summarization in
isolation, i.e., when α=0 or 0 = 0 in Figure 5. Inter-
estingly, we conclude an opposite observation com-
pared with ETS. Different approaches might lead to
different optimum of global/local combination.
Another key parameter σ measures the temporal
projection influence from global collection to local
collection and hence the size of neighboring sen-
tence set. 6 datasets are classified into two groups.
Subject 1, 2, 6 are grouped as long news with a time
span of more than one year and the others are short
news. The effect of σ varies on long news sets and
short news sets. In Figure 6 σ is best around 60 and
in Figure 7 it is best at about 20∼40, indicating long
news has relatively wider semantic scope.
We then examine the effect of different projection
kernels. Generally, Gaussian kernel outperforms
others and window kernel is the worst, probably be-
cause Gaussian kernel provides the best smoothing
effect with no arbitrary cutoffs. Window kernel fails
to distinguish different weights of neighboring sets
by temporal proximity, so its performance is as ex-
pected. Other 3 kernels are comparable.
</bodyText>
<subsectionHeader confidence="0.994616">
4.7 Sample Output and Case Study
</subsectionHeader>
<bodyText confidence="0.999990115384615">
Sample output is presented in Table 7 and it shares
major information similarity with the human time-
line in Table 1. Besides, we notice that a dynamic
φi is reasonable. Important burstiness is worthy of
more attention. Fewer sentences are selected on the
dates when nothing new occurs.
Interesting Findings. We notice that humans have
biases to generate timelines for they have (1) pref-
erence on local occurrences and (2) different writ-
ing styles. For instance, news outlets from United
States tend to summarize reactions by US govern-
ment while UK websites tend to summarize British
affairs. Some editors favor statistical reports while
others prefer narrative style, and some timelines
have detailed explanations while others are ex-
tremely concise with no more than two sentences for
each entry. Our system-generated timelines have a
large variance among all golden standards. Proba-
bly a new evaluation metric should be introduced to
measure the quality of human generated timelines
to mitigate the corresponding biases. A third in-
teresting observation is that subjects have different
volume patterns, e.g., H1N1 has a slow start and a
bursty evolution and BP Oil has a bursty start and a
quick decay. Obama is different in nature because
the report volume is temporally stable and scattered.
</bodyText>
<sectionHeader confidence="0.997706" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999936">
We present a novel solution for the important
web mining problem, Evolutionary Trans-Temporal
Summarization (ETTS), which generates trajectory
timelines for news subjects from massive data. We
formally formulate ETTS as a combination of global
and local summarization, incorporating affinity and
</bodyText>
<page confidence="0.924243">
441
</page>
<tableCaption confidence="0.999414">
Table 7: Selected part of timeline generated by ETTS for BP Oil.
</tableCaption>
<note confidence="0.955849625">
April 20, 2010 April 24, 2010
s1: An explosion on the Deepwater Horizon offshore oil drilling rig in s1: Oil is found to be leaking from the well.
the Gulf of Mexico, around 40 miles south east of Louisiana, causing
several kills and injuries.
s2: The rig was drilling in about 5,000ft (1,525m) of water, pushing
the boundaries of deepwater drilling technology.
s3: The rig is owned and operated by Transocean, a company hired by
BP to carry out the drilling work.
</note>
<table confidence="0.884183676470588">
s4: Deepwater Horizon oil rig fire leaves 11 missing.
April 26, 2010
s1: BP’s shares fall 2% amid fears that the cost of cleanup and legal
claims will hit the London-based company hard.
s2: Roughly 15,000 gallons of dispersants and 21,000ft of containment
boom are placed at the spill site.
April 27, 2010
April 22, 2010
s1: BP reports a rise in profits, due in large part to oil price increases,
as shares rise again.
s2: The US departments of interior and homeland security announce
plans for a joint investigation of the explosion and fire.
s3: Minerals Management Service (MMS) approves a plan for two re-
lief wells.
s4: BP chairman Tony Hayward says the company will take full re-
sponsibility for the spill, paying for legitimate claims and cleanup cost.
s1: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s2: The Deepwater Horizon sinks to the bottom of the Gulf after burn-
ing for 36 hours, raising concerns of a catastrophic oil spill.
s3: Deepwater Horizon rig sinks in 5,000ft of water.
April 23, 2010
s1: The US coast guard suspends the search for missing workers, who
are all presumed dead.
s2: The Coast Guard says it had no indication that oil was leaking from
the well 5,000ft below the surface of the Gulf.
s3: Underwater robots try to shut valves on the blowout preventer to
stop the leak, but BP abandons that failed effort two weeks later.
s4: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s5: Deepwater Horizon clean-up workers fight to prevent disaster.
April 28, 2010
s1: The coast guard says the flow of oil is 5,000bpd, five times greater
than first estimated, after a third leak is discovered.
</table>
<bodyText confidence="0.908392769230769">
s2: BP’s attempts to repair a hydraulic leak on the blowout preventer
valve are unsuccessful.
s3: BP reports that its first-quarter profits more than double to £3.65
billion following a rise in oil prices.
s4: Controlled burns begin on the giant oil slick.
diversity into a unified ranking framework. We im-
plement a system under such framework for ex-
periments on real web datasets to compare all ap-
proaches. Through our experiment we notice that
the combination plays an important role in timeline
generation, and global optimization weights slightly
higher (α/β ∈ [10, 100]), but auxiliary local infor-
mation does help to enhance performance in ETTS.
</bodyText>
<sectionHeader confidence="0.997484" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8453004">
This work was partially supported by NSFC with
Grant No.61073082, 60933004, 70903008 and
61073081, and Xiaojun Wan was supported by
NSFC with Grant No.60873155 and Beijing Nova
Program (2008B03).
</bodyText>
<sectionHeader confidence="0.958943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99943025">
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’01, pages 10–18.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ’04, pages 425–432.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 121–128.
Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010.
Topic initiator detection on the world wide web. In
Proceedings of the 19th international conference on
WWW’10, pages 481–490.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR’04, pages 297–304.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 457–464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL’03, pages 71–78.
</reference>
<page confidence="0.552334">
442
</page>
<reference confidence="0.996845392156863">
Yuanhua Lv and ChengXiang Zhai. 2009. Positional lan-
guage models for information retrieval. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’09, pages 299–306.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD’10, pages 1009–1018.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919–938.
Russell Swan and James Allan. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR’00, pages 49–56.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ’08, pages 299–306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903–2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI’07, pages 931–936.
Dingding Wang and Tao Li. 2010. Document update
summarization using incremental hierarchical cluster-
ing. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ’10, pages 279–288.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In Information Retrieval Tech-
nology - 6th Asia Information Retrieval Societies Con-
ference, AIRS 2010, pages 490–501.
Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming
Li. 2011a. A fine-grained digestion of news webpages
through event snippet extraction. In Proceedings of
the 20th international conference companion on world
wide web, WWW ’11, pages 157–158.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings of
the 34th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’11.
</reference>
<page confidence="0.976561">
443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875264">
<title confidence="0.99976">Timeline Generation through Evolutionary Trans-Temporal Summarization</title>
<author confidence="0.998246">Liang Congrui Xiaojun Xiaoming Yan</author>
<affiliation confidence="0.997685333333333">of Electronics Engineering and Computer Science, Peking University, of Computer Science and Technology, Peking University, bState Key Laboratory of Virtual Reality Technology and Systems, Beihang University,</affiliation>
<email confidence="0.979815">wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn</email>
<abstract confidence="0.99579656">We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection of time-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for using sentence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Rahul Gupta</author>
<author>Vikas Khandelwal</author>
</authors>
<title>Temporal summaries of new topics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="9014" citStr="Allan et al., 2001" startWordPosition="1332" endWordPosition="1335">434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 2011b) but the refining process based on generated component summaries is time consuming. We aim to seek for more efficient summarizing approach. To the best of our knowledge, neither update summarization nor traditional systems have considered the relationship among “component summaries”, or have utilized trans-temporal properties. ETTS approach can also naturally and simultaneously take i</context>
</contexts>
<marker>Allan, Gupta, Khandelwal, 2001</marker>
<rawString>James Allan, Rahul Gupta, and Vikas Khandelwal. 2001. Temporal summaries of new topics. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Query based event extraction along a timeline.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’04,</booktitle>
<pages>425--432</pages>
<contexts>
<context position="9079" citStr="Chieu and Lee, 2004" startWordPosition="1343" endWordPosition="1346">nt links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 2011b) but the refining process based on generated component summaries is time consuming. We aim to seek for more efficient summarizing approach. To the best of our knowledge, neither update summarization nor traditional systems have considered the relationship among “component summaries”, or have utilized trans-temporal properties. ETTS approach can also naturally and simultaneously take into account global/local summarization with biased information ri</context>
<context position="21557" citStr="Chieu and Lee, 2004" startWordPosition="3558" endWordPosition="3561">he global collection C. We partition C according to timestamps of sentences, i.e., C = C1 U C2 U · · · U C|T |. Each component summary is generated from its corresponding sub-collection. The sizes of component summaries are not necessarily equal, and moreover, not all dates may be represented, so date selection is also important. We apply a simple mechanism that users specify the overall compression rate φ, and we extract more sentences for important dates while fewer sentences for others. The importance of dates is measured by the burstiness, which indicates probable significant occurrences (Chieu and Lee, 2004). The compression rate on ti is set as φi = |Ci| |C |. 4.3 Evaluation Metrics The ROUGE measure is widely used for evaluation (Lin and Hovy, 2003): the DUC contests usually officially employ ROUGE for automatic summarization evaluation. In ROUGE evaluation, the summarization quality is measured by counting the number of overlapping units, such as N-gram, word sequences, and word pairs between the candidate timelines CT and the reference timelines RT. There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N with 3 sub-metrics: 1 ROUGE-N-R is an N-gram recall metric: </context>
<context position="24397" citStr="Chieu and Lee, 2004" startWordPosition="4005" endWordPosition="4008">ntuitions as their performance scores. For fairness we conduct the same preprocessing for all baselines. Random: The method selects sentences randomly for each document collection. Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. GMDS: The graph-based MDS proposed by (Wan and Yang, 2008) first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality. Chieu: (Chieu and Lee, 2004) present a similar timeline system with different goals and frameworks, utilizing interest and burstiness ranking but neglecting trans-temporal news evolution. ETTS: ETTS is an algorithm with optimized combination of global/local biased summarization. RefTL: As we have used multiple human timelines as references, we not only provide ROUGE evaluations of the competing systems but also of the human timelines against each other, which provides a good indicator as to the upper bound ROUGE score that any system could achieve. 4.5 Overall Performance Comparison We use a cross validation manner among</context>
</contexts>
<marker>Chieu, Lee, 2004</marker>
<rawString>Hai Leong Chieu and Yoong Keok Lee. 2004. Query based event extraction along a timeline. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’04, pages 425–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<contexts>
<context position="8229" citStr="Erkan and Radev, 2004" startWordPosition="1221" endWordPosition="1224"> for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel c</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In Proceedings of EMNLP, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="7999" citStr="Goldstein et al., 1999" startWordPosition="1187" endWordPosition="1190">n-based methods, which usually involve assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting the units with highest scores. To date, various extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relati</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: sentence selection and evaluation metrics. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Jin</author>
<author>Scott Spangler</author>
<author>Rui Ma</author>
<author>Jiawei Han</author>
</authors>
<title>Topic initiator detection on the world wide web.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on WWW’10,</booktitle>
<pages>481--490</pages>
<contexts>
<context position="2431" citStr="Jin et al., 2010" startWordPosition="339" endWordPosition="342">al search engines can rank these ∗Corresponding author. news webpages by relevance to a user specified aspect, i.e., a query such as “first relief effort for BP Oil Spill”, but search engines are not quite capable of ranking documents given the whole news subject without particular aspects. Faced with thousands of news documents, people usually have a myriad of interest aspects about the beginning, the development or the latest situation. However, traditional information retrieval techniques can only rank webpages according to their understanding of relevance, which is obviously insufficient (Jin et al., 2010). Even if the ranked documents could be in a satisfying order to help users understand news evolution, readers prefer to monitor the evolutionary trajectories by simply browsing rather than navigate every document in the overwhelming collection. Summarization is an ideal solution to provide an abbreviated, informative reorganization for faster and better representation of news documents. Particularly, a timeline (see Table 1) can summarize evolutionary news as a series of individual but correlated component summaries (items in Table 1) and offer an option to understand the big picture of evolu</context>
</contexts>
<marker>Jin, Spangler, Ma, Han, 2010</marker>
<rawString>Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010. Topic initiator detection on the world wide web. In Proceedings of the 19th international conference on WWW’10, pages 481–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Text classification and named entities for new event detection.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR’04,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="19472" citStr="Kumaran and Allan, 2004" startWordPosition="3219" endWordPosition="3222">(a) PageRank on t. (b) DivRank on t (c) DivRank on t&apos; Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect temporal biased processes on t {1,9} in (c) and t&apos; {2,12} in (d). is in China and the rest are in the US. We choose these sites because many of them provide timelines edited by professional editors, which serve as reference summaries. The news belongs to different categories of Rule of Interpretation (ROI) (Kumaran and Allan, 2004). More detailed statistics are in Table 3. Table 2: News sources of 6 datasets News Sources Nation News Sources Nation BBC UK Fox News US Xinhua China MSNBC US CNN US Guardian UK ABC US New York Times US Reuters UK Washington Post US Table 3: Detailed basic information of 6 datasets. News Subjects #size #docs #stamps #RT AL 1.Influenza A 115026 2557 331 5 83 2.Financial Crisis 176435 2894 427 2 118 3.BP Oil Spill 63021 1468 135 6 76 4.Haiti Earthquake 12073 247 83 2 32 5.Jackson Death 37819 925 168 3 64 6.Obama Presidency 79761 2160 349 5 92 size: the whole sentence counts; #stamps: the number</context>
</contexts>
<marker>Kumaran, Allan, 2004</marker>
<rawString>Giridhar Kumaran and James Allan. 2004. Text classification and named entities for new event detection. In Proceedings of the 27th annual international ACM SIGIR’04, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>From single to multi-document summarization: a prototype system and its evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>457--464</pages>
<contexts>
<context position="7871" citStr="Lin and Hovy, 2002" startWordPosition="1166" endWordPosition="1169">rization (e.g. NewsBlaster2) usually needs information fusion, sentence compression and reformulation. We focus on extraction-based methods, which usually involve assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting the units with highest scores. To date, various extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links bet</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: a prototype system and its evaluation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 457–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL’03,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="21703" citStr="Lin and Hovy, 2003" startWordPosition="3586" endWordPosition="3589"> from its corresponding sub-collection. The sizes of component summaries are not necessarily equal, and moreover, not all dates may be represented, so date selection is also important. We apply a simple mechanism that users specify the overall compression rate φ, and we extract more sentences for important dates while fewer sentences for others. The importance of dates is measured by the burstiness, which indicates probable significant occurrences (Chieu and Lee, 2004). The compression rate on ti is set as φi = |Ci| |C |. 4.3 Evaluation Metrics The ROUGE measure is widely used for evaluation (Lin and Hovy, 2003): the DUC contests usually officially employ ROUGE for automatic summarization evaluation. In ROUGE evaluation, the summarization quality is measured by counting the number of overlapping units, such as N-gram, word sequences, and word pairs between the candidate timelines CT and the reference timelines RT. There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N with 3 sub-metrics: 1 ROUGE-N-R is an N-gram recall metric: Countmatch(N-gram) ROUGE-N-R = E IERT E N-gramEI E IERT Count (N-gram) E N-gramEI 438 2 ROUGE-N-P is an N-gram precision metric: Countmatch(N-gram</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the Human Language Technology Conference of the NAACL’03, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanhua Lv</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Positional language models for information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="10894" citStr="Lv and Zhai, 2009" startWordPosition="1613" endWordPosition="1616">the collection onto the time horizon of t to construct a global affinity graph, using temporal decaying kernels. 3.1.1 Temporal Proximity Based Projection Clearly, a major technical challenge in ETTS is how to define the temporal biased projection function Γ(Δt), where Δt is the distance between the Figure 1: Construct global/local biased graphs. Solid circles denote intra-date sentences on the pending date t and dash ones represent inter-date sentences from other dates. Figure 2: Proximity-based kernel functions, where σ=10. pending date t and neighboring date t&apos;, i.e., Δt = |t&apos; − t|. As in (Lv and Zhai, 2009), we present 5 representative kernel functions: Gaussian, Triangle, Cosine, Circle, and Window, shown in Figure 2. Different kernels lead to different projections. 1. Gaussian kernel −Δt2 2σ2 ] 2. Triangle kernel � 1 − Δt σ if Δt ≤ σ Γ(Δt) = 0 otherwise 3. Cosine (Hamming) kernel Γ(Δt) = 12 [1 + cos(` or %`)] if Δt ≤ σ 0 otherwise 4. Circle kernel (Δt) = V1 − (Δtσ )2 if Δt ≤ σ 0 otherwise Γ(Δt) = exp[ 435 5. Window kernel any date from T. |C |is the sentences set size and ( Nw is the number of sentences containing term w. 1 if Δt ≤ σ We let p(si → si|t)=0 to avoid self transition. Γ(Δt) = Note</context>
</contexts>
<marker>Lv, Zhai, 2009</marker>
<rawString>Yuanhua Lv and ChengXiang Zhai. 2009. Positional language models for information retrieval. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Jian Guo</author>
<author>Dragomir Radev</author>
</authors>
<title>Divrank: the interplay of prestige and diversity in information networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD’10,</booktitle>
<pages>1009--1018</pages>
<contexts>
<context position="14158" citStr="Mei et al., 2010" startWordPosition="2202" endWordPosition="2205">). A greedy vertex selection the timestamp t is set to 0 for local summarization. algorithm may achieve diversity by iteratively seMti,j is the transition probability of si to sj based lecting the most prestigious vertex and then penalon the perspective of date t, i.e., p(si → sj|t): izing the vertices “covered” by the already selected ( f(si—+sj|t) if Pf =60 ones, such as Maximum Marginal Relevance and its p(si → sj |t) = E |C |f (si—+sk |t) applications in Wan et al. (2007b; 2007a). Most re0 if tsi = tsj = t cently diversity rank DivRank is another solution (1) to diversity penalization in (Mei et al., 2010). f(si → sj|t) is defined as the temporal weighted We incorporate DivRank in our general ranking cosine similarity between two sentences: framework, which creates a dynamic M during each f(si → sj|t) = X π(w, si|t) · π(w, sj|t) (2) iteration, rather than a static one. After z times of wEsinsj iteration, the matrix M becomes: where the weight π associated with term w is calcu- M(z) = µM(z−1) · ~λ(z−1) + 1 − µe~ (5) lated with the temporal weighted tf.isf formula: |C| π(w, s|t) = Γ|t − ts|· tf(w,s)(1 + log( |C |Equation (5) raises the probability for nodes with qP. higher centrality and nodes al</context>
</contexts>
<marker>Mei, Guo, Radev, 2010</marker>
<rawString>Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD’10, pages 1009–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<volume>5</volume>
<contexts>
<context position="8189" citStr="Mihalcea and Tarau, 2005" startWordPosition="1214" endWordPosition="1217">extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt wi</context>
</contexts>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>R. Mihalcea and P. Tarau. 2005. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Sty</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>40</volume>
<issue>6</issue>
<contexts>
<context position="7701" citStr="Radev et al., 2004" startWordPosition="1138" endWordPosition="1141">dvanced the technology and produced several experimental systems. Generally speaking, MDS methods can be either extractive or abstractive summarization. Abstractive summarization (e.g. NewsBlaster2) usually needs information fusion, sentence compression and reformulation. We focus on extraction-based methods, which usually involve assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting the units with highest scores. To date, various extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importa</context>
<context position="24022" citStr="Radev et al., 2004" startWordPosition="3951" endWordPosition="3954"> systems. They are designed for traditional summarization without trans-temporal dimension. The first intuitive way to generate timelines by these methods is via a global summarization on collection C and then distribution of selected sentences to their source dates. The other one is via an equal summarization on all local sub-collections. For baselines, we average both intuitions as their performance scores. For fairness we conduct the same preprocessing for all baselines. Random: The method selects sentences randomly for each document collection. Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. GMDS: The graph-based MDS proposed by (Wan and Yang, 2008) first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality. Chieu: (Chieu and Lee, 2004) present a similar timeline system with different goals and frameworks, utilizing interest and burstiness ranking but neglecting trans-temporal news evolution. ETTS: ETTS is an algorithm with optimized combination of global/l</context>
</contexts>
<marker>Radev, Jing, Sty, 2004</marker>
<rawString>D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40(6):919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell Swan</author>
<author>James Allan</author>
</authors>
<title>Automatic generation of overview timelines.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR’00,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="8968" citStr="Swan and Allan, 2000" startWordPosition="1323" endWordPosition="1326">g 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 2011b) but the refining process based on generated component summaries is time consuming. We aim to seek for more efficient summarizing approach. To the best of our knowledge, neither update summarization nor traditional systems have considered the relationship among “component summaries”, or have utilized trans-temporal properties. ETTS approac</context>
</contexts>
<marker>Swan, Allan, 2000</marker>
<rawString>Russell Swan and James Allan. 2000. Automatic generation of overview timelines. In Proceedings of the 23rd annual international ACM SIGIR’00, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="24209" citStr="Wan and Yang, 2008" startWordPosition="3977" endWordPosition="3980"> collection C and then distribution of selected sentences to their source dates. The other one is via an equal summarization on all local sub-collections. For baselines, we average both intuitions as their performance scores. For fairness we conduct the same preprocessing for all baselines. Random: The method selects sentences randomly for each document collection. Centroid: The method applies MEAD algorithm (Radev et al., 2004) to extract sentences according to the following three parameters: centroid value, positional value, and first-sentence overlap. GMDS: The graph-based MDS proposed by (Wan and Yang, 2008) first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality. Chieu: (Chieu and Lee, 2004) present a similar timeline system with different goals and frameworks, utilizing interest and burstiness ranking but neglecting trans-temporal news evolution. ETTS: ETTS is an algorithm with optimized combination of global/local biased summarization. RefTL: As we have used multiple human timelines as references, we not only provide ROUGE evaluations of the competing systems but also of the human timelines ag</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multi-document summarization using cluster-based link analysis. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Manifold-ranking based topic-focused multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<volume>7</volume>
<pages>2903--2908</pages>
<contexts>
<context position="8624" citStr="Wan et al., 2007" startWordPosition="1268" endWordPosition="1271">e redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 201</context>
<context position="14020" citStr="Wan et al. (2007" startWordPosition="2176" endWordPosition="2179">refore, intra- high salient scores to closely connected node comdate transition probability between sentences with munities (Figure 3 (b)). A greedy vertex selection the timestamp t is set to 0 for local summarization. algorithm may achieve diversity by iteratively seMti,j is the transition probability of si to sj based lecting the most prestigious vertex and then penalon the perspective of date t, i.e., p(si → sj|t): izing the vertices “covered” by the already selected ( f(si—+sj|t) if Pf =60 ones, such as Maximum Marginal Relevance and its p(si → sj |t) = E |C |f (si—+sk |t) applications in Wan et al. (2007b; 2007a). Most re0 if tsi = tsj = t cently diversity rank DivRank is another solution (1) to diversity penalization in (Mei et al., 2010). f(si → sj|t) is defined as the temporal weighted We incorporate DivRank in our general ranking cosine similarity between two sentences: framework, which creates a dynamic M during each f(si → sj|t) = X π(w, si|t) · π(w, sj|t) (2) iteration, rather than a static one. After z times of wEsinsj iteration, the matrix M becomes: where the weight π associated with term w is calcu- M(z) = µM(z−1) · ~λ(z−1) + 1 − µe~ (5) lated with the temporal weighted tf.isf form</context>
<context position="15989" citStr="Wan et al., 2007" startWordPosition="2526" endWordPosition="2529"> 3.2 Local Biased Summarization Naturally, the component summary for date t should be informative within Ct. Given the sentence collection Ct = {sti|1 ≤ i ≤ |Ct|}, we build an affinity matrix for Figure 1 (b), with the entry of intradate transition probability calculated from standard cosine similarity. We incorporate DivRank within local summarization and we obtain the local biased rank and ranking score for si, denoted as r‡i and Li. 3.3 Optimization of Global/Local Combination We do not directly add the global biased ranking score and local biased ranking score, as many previous works did (Wan et al., 2007b; Wan et al., 2007a), because even the same ranking score gap may indicate different rank gaps in two ranking lists. Given subset Ct, let R = {ri}(i = 1,... ,|Ct|), ri is the final ranking of si to estimate, optimize the following objective cost function O(R), † i Gik ri −rk2 Ψi Gi (6) ‡ Lik Ψ− Li i k2 where Gi is the global biased ranking score while Li is the local biased ranking score. Ψi is expected to be the merged ranking score, namely sentence importance, which will be defined later. Among the two components in the objective function, the first component means that the refined rank sho</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking based topic-focused multi-document summarization. In Proceedings of IJCAI, volume 7, pages 2903–2908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Single document summarization with document expansion.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd AAAI’07,</booktitle>
<pages>931--936</pages>
<contexts>
<context position="8624" citStr="Wan et al., 2007" startWordPosition="1268" endWordPosition="1271">e redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2http://www1.cs.columbia.edu/nlp/newsblaster/ 434 algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 201</context>
<context position="14020" citStr="Wan et al. (2007" startWordPosition="2176" endWordPosition="2179">refore, intra- high salient scores to closely connected node comdate transition probability between sentences with munities (Figure 3 (b)). A greedy vertex selection the timestamp t is set to 0 for local summarization. algorithm may achieve diversity by iteratively seMti,j is the transition probability of si to sj based lecting the most prestigious vertex and then penalon the perspective of date t, i.e., p(si → sj|t): izing the vertices “covered” by the already selected ( f(si—+sj|t) if Pf =60 ones, such as Maximum Marginal Relevance and its p(si → sj |t) = E |C |f (si—+sk |t) applications in Wan et al. (2007b; 2007a). Most re0 if tsi = tsj = t cently diversity rank DivRank is another solution (1) to diversity penalization in (Mei et al., 2010). f(si → sj|t) is defined as the temporal weighted We incorporate DivRank in our general ranking cosine similarity between two sentences: framework, which creates a dynamic M during each f(si → sj|t) = X π(w, si|t) · π(w, sj|t) (2) iteration, rather than a static one. After z times of wEsinsj iteration, the matrix M becomes: where the weight π associated with term w is calcu- M(z) = µM(z−1) · ~λ(z−1) + 1 − µe~ (5) lated with the temporal weighted tf.isf form</context>
<context position="15989" citStr="Wan et al., 2007" startWordPosition="2526" endWordPosition="2529"> 3.2 Local Biased Summarization Naturally, the component summary for date t should be informative within Ct. Given the sentence collection Ct = {sti|1 ≤ i ≤ |Ct|}, we build an affinity matrix for Figure 1 (b), with the entry of intradate transition probability calculated from standard cosine similarity. We incorporate DivRank within local summarization and we obtain the local biased rank and ranking score for si, denoted as r‡i and Li. 3.3 Optimization of Global/Local Combination We do not directly add the global biased ranking score and local biased ranking score, as many previous works did (Wan et al., 2007b; Wan et al., 2007a), because even the same ranking score gap may indicate different rank gaps in two ranking lists. Given subset Ct, let R = {ri}(i = 1,... ,|Ct|), ri is the final ranking of si to estimate, optimize the following objective cost function O(R), † i Gik ri −rk2 Ψi Gi (6) ‡ Lik Ψ− Li i k2 where Gi is the global biased ranking score while Li is the local biased ranking score. Ψi is expected to be the merged ranking score, namely sentence importance, which will be defined later. Among the two components in the objective function, the first component means that the refined rank sho</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang, and J. Xiao. 2007b. Single document summarization with document expansion. In Proceedings of the 22nd AAAI’07, pages 931–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
</authors>
<title>Document update summarization using incremental hierarchical clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10,</booktitle>
<pages>279--288</pages>
<contexts>
<context position="28398" citStr="Wang and Li, 2010" startWordPosition="4634" endWordPosition="4637">summarization). Probable bias is enlarged by searching for worthy sentence in single dates. However, precision drops due to excessive choice of global timeline-worthy sentences. Figure 5: α/0: global/local combination. Figure 7: u on short topics (&lt;1 year). • In general, the result of Chieu is better than Centroid but unexpectedly, worse than GMDS. The reason may be that Chieu does not capture sufficient timeline attributes. The “interest” modeled Figure 6: u on long topics (≥1 year). 440 in the algorithms actually performs flat clusteringbased summarization which is proved to be less useful (Wang and Li, 2010). GMDS utilizes sentence linkage, and partly captures “correlativeness”. • ETTS under our proposed framework outperforms baselines, indicating that the properties we use for timeline generation are beneficial. We also add a direct comparison between ETTS and ETS (Yan et al., 2011b). We notice that both balanced algorithms achieve comparable performance (0.386 v.s. 0.412: a gap of 0.026 in terms of ROUGE1), but ETTS is much faster than ETS. It is understandable that ETS refines timelines based on neighboring component summaries iteratively while for ETTS neighboring information is incorporated </context>
</contexts>
<marker>Wang, Li, 2010</marker>
<rawString>Dingding Wang and Tao Li. 2010. Document update summarization using incremental hierarchical clustering. In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10, pages 279–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Yu Li</author>
<author>Yan Zhang</author>
<author>Xiaoming Li</author>
</authors>
<title>Event recognition from news webpages through latent ingredients extraction.</title>
<date>2010</date>
<booktitle>In Information Retrieval Technology - 6th Asia Information Retrieval Societies Conference, AIRS</booktitle>
<pages>490--501</pages>
<contexts>
<context position="20538" citStr="Yan et al. (2010" startWordPosition="3398" endWordPosition="3401">quake 12073 247 83 2 32 5.Jackson Death 37819 925 168 3 64 6.Obama Presidency 79761 2160 349 5 92 size: the whole sentence counts; #stamps: the number of timestamps; Note average size of subsets is calculated as: avg.size=#size/#stamps; RT: reference timelines; AL: avg. length of RT measured in sentences. 4.2 Experimental System Setups • Preprocessing. As ETTS faces with much larger corpus compared with traditional MDS, we apply further data preprocessing besides stemming and stop-word removal. We extract text snippets representing atomic “events” from all documents with a toolkit provided by Yan et al. (2010; 2011a), by which we attempt to assign more fine-grained and accurate timestamps for every sentence within the text snippets. After the snippet extraction procedure, we filter the corpora by discarding non-event texts. • Compression Rate and Date Selection. After preprocessing, we obtain numerous snippets with fine-grained timestamps, and then decompose them into temporally tagged sentences as the global collection C. We partition C according to timestamps of sentences, i.e., C = C1 U C2 U · · · U C|T |. Each component summary is generated from its corresponding sub-collection. The sizes of c</context>
</contexts>
<marker>Yan, Li, Zhang, Li, 2010</marker>
<rawString>Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010. Event recognition from news webpages through latent ingredients extraction. In Information Retrieval Technology - 6th Asia Information Retrieval Societies Conference, AIRS 2010, pages 490–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Yu Li</author>
<author>Yan Zhang</author>
<author>Xiaoming Li</author>
</authors>
<title>A fine-grained digestion of news webpages through event snippet extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference companion on world wide web, WWW ’11,</booktitle>
<pages>157--158</pages>
<contexts>
<context position="9225" citStr="Yan et al., 2011" startWordPosition="1365" endWordPosition="1368">an et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 2011b) but the refining process based on generated component summaries is time consuming. We aim to seek for more efficient summarizing approach. To the best of our knowledge, neither update summarization nor traditional systems have considered the relationship among “component summaries”, or have utilized trans-temporal properties. ETTS approach can also naturally and simultaneously take into account global/local summarization with biased information richness and information novelty, and combine both summarization in optimization. 3 Trans-temporal Summarization We conduct trans-temporal summariza</context>
<context position="28678" citStr="Yan et al., 2011" startWordPosition="4676" endWordPosition="4679"> of Chieu is better than Centroid but unexpectedly, worse than GMDS. The reason may be that Chieu does not capture sufficient timeline attributes. The “interest” modeled Figure 6: u on long topics (≥1 year). 440 in the algorithms actually performs flat clusteringbased summarization which is proved to be less useful (Wang and Li, 2010). GMDS utilizes sentence linkage, and partly captures “correlativeness”. • ETTS under our proposed framework outperforms baselines, indicating that the properties we use for timeline generation are beneficial. We also add a direct comparison between ETTS and ETS (Yan et al., 2011b). We notice that both balanced algorithms achieve comparable performance (0.386 v.s. 0.412: a gap of 0.026 in terms of ROUGE1), but ETTS is much faster than ETS. It is understandable that ETS refines timelines based on neighboring component summaries iteratively while for ETTS neighboring information is incorporated in temporal projection and hence there is no such procedure. Furthermore, ETS has 8 free parameters to tune while ETTS has only 2 parameters. In other words, ETTS is more simple to control. • The performance on intensive focused news within short time range (|last timestamp−first</context>
</contexts>
<marker>Yan, Kong, Li, Zhang, Li, 2011</marker>
<rawString>Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming Li. 2011a. A fine-grained digestion of news webpages through event snippet extraction. In Proceedings of the 20th international conference companion on world wide web, WWW ’11, pages 157–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’11.</booktitle>
<contexts>
<context position="9225" citStr="Yan et al., 2011" startWordPosition="1365" endWordPosition="1368">an et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates. Further related work includes similar timeline systems proposed by (Swan and Allan, 2000) using named entities, by (Allan et al., 2001) measured in usefulness and novelty, and by (Chieu and Lee, 2004) measured in interest and burstiness. We have proposed a timeline algorithm named “Evolutionary Timeline Summarization (ETS)” in (Yan et al., 2011b) but the refining process based on generated component summaries is time consuming. We aim to seek for more efficient summarizing approach. To the best of our knowledge, neither update summarization nor traditional systems have considered the relationship among “component summaries”, or have utilized trans-temporal properties. ETTS approach can also naturally and simultaneously take into account global/local summarization with biased information richness and information novelty, and combine both summarization in optimization. 3 Trans-temporal Summarization We conduct trans-temporal summariza</context>
<context position="28678" citStr="Yan et al., 2011" startWordPosition="4676" endWordPosition="4679"> of Chieu is better than Centroid but unexpectedly, worse than GMDS. The reason may be that Chieu does not capture sufficient timeline attributes. The “interest” modeled Figure 6: u on long topics (≥1 year). 440 in the algorithms actually performs flat clusteringbased summarization which is proved to be less useful (Wang and Li, 2010). GMDS utilizes sentence linkage, and partly captures “correlativeness”. • ETTS under our proposed framework outperforms baselines, indicating that the properties we use for timeline generation are beneficial. We also add a direct comparison between ETTS and ETS (Yan et al., 2011b). We notice that both balanced algorithms achieve comparable performance (0.386 v.s. 0.412: a gap of 0.026 in terms of ROUGE1), but ETTS is much faster than ETS. It is understandable that ETS refines timelines based on neighboring component summaries iteratively while for ETTS neighboring information is incorporated in temporal projection and hence there is no such procedure. Furthermore, ETS has 8 free parameters to tune while ETTS has only 2 parameters. In other words, ETTS is more simple to control. • The performance on intensive focused news within short time range (|last timestamp−first</context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. In Proceedings of the 34th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>