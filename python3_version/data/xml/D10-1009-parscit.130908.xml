<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000247">
<title confidence="0.948724">
Handling Noisy Queries In Cross Language FAQ Retrieval
</title>
<author confidence="0.7857695">
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
</author>
<affiliation confidence="0.845519">
IBM Research India
</affiliation>
<address confidence="0.672276">
Vasant Kunj, Institutional Area
New Delhi, India
</address>
<email confidence="0.998862">
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
</email>
<sectionHeader confidence="0.996942" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993491857142857">
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999995487804878">
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al., 2005), access to Yellow Page
services (Kopparapu et al., 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&amp;A archive (Xue et al.,
2008) or the Web (Jijkoun et al., 2005), in a FAQ
database the questions and answers are already pro-
</bodyText>
<footnote confidence="0.9999745">
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
</footnote>
<page confidence="0.985904">
87
</page>
<note confidence="0.9211395">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87–96,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.995626">
Figure 1: Sample SMS queries with Hindi FAQs
</figureCaption>
<bodyText confidence="0.999960780487805">
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al., 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al., 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al., 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al., 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al., 2007) (Byun et al.,
2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus
et al., 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al., 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
’gud’ − &gt; ’god’,’good’,’guide’ ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also, naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
</bodyText>
<sectionHeader confidence="0.968032" genericHeader="method">
2 Problem formulation
</sectionHeader>
<bodyText confidence="0.9990888">
Consider an input SMS 5e in a source language
e. We view 5e as a sequence of n tokens 5e —
s1, s2,. . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
</bodyText>
<page confidence="0.996907">
88
</page>
<listItem confidence="0.54581325">
of questions in the FAQ corpus of a target language
h. Each question Qh E Qh is also viewed as a se-
quence of tokens. We want to find the question Qh�
from the corpus Qh that best matches the SMS Se.
</listItem>
<bodyText confidence="0.99997896">
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure α(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure α(th, te, si) &gt; E.
We denote this as th — si.
We define a weight function ω(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpus Qh using
the weight function. Consider a question Qh E Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
</bodyText>
<equation confidence="0.8404285">
thEQh,teEDe &amp; th-si
max ω(th, te, si)
</equation>
<bodyText confidence="0.974537">
Our goal is to efficiently find the question Qh � having
the maximum score.
</bodyText>
<sectionHeader confidence="0.9614" genericHeader="method">
3 Noise removal from queries
</sectionHeader>
<bodyText confidence="0.9999392">
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
</bodyText>
<subsectionHeader confidence="0.999388">
3.1 Similarity Measure
</subsectionHeader>
<bodyText confidence="0.9999755">
For a term te E De and token si of the SMS input
Se, the similarity measure γ(te, si) between them is
</bodyText>
<equation confidence="0.941947">
γ(te, si) � I 0 otherwise
EditDistanceSMS(te,si) if te and si share
LCSRatio(te,si)
character *
same starting
(1)
Where LCSRatio(te si) = tength(LCS(te,si)) and LCS(te sj)
r a tength(te) r a
is the Longest common subsequence between te and si.
</equation>
<bodyText confidence="0.995759966666667">
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al., 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al., 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then γ(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token “gud” whose
most likely correct form is “good”. The longest
common subsequence for “good” and “guided” with
“gud” is “gd”. Hence the two dictionary terms
“good” and “guided” have the same LCSRatio of 0.5
w.r.t “gud”, but the EditDistanceSMS of “good”
is 1 which is less than that of “guided”, which has
EditDistanceSMS of 2 w.r.t “gud”. As a result the
similarity measure between “gud” and “good” will
be higher than that of “gud” and “guided”. Higher
the LCSRatio and lower the EditDistanceSMS,
higher will be the similarity measure. Hence, for
a given SMS token “byk”, the similarity measure of
word “bike“ is higher than that of “break”.
</bodyText>
<sectionHeader confidence="0.946486" genericHeader="method">
4 Cross lingual similarity
</sectionHeader>
<bodyText confidence="0.9986825">
Once we have potential candidates which are the
likely disambiguated representations of the noisy
</bodyText>
<equation confidence="0.958242">
n
Score(Qh) _
i=1
</equation>
<page confidence="0.996674">
89
</page>
<figure confidence="0.968237166666667">
Procedure EditDistanceSmS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
//(fall → fal)
Step 2. remove all vowels in t
//(painting → pntng, threat → thrt)
return t
End
</figure>
<figureCaption confidence="0.999464">
Figure 2: EditDistanceSmS
</figureCaption>
<bodyText confidence="0.999986666666667">
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
</bodyText>
<subsectionHeader confidence="0.998705">
4.1 Statistical Dictionary
</subsectionHeader>
<bodyText confidence="0.990793227272727">
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al., 2007). Under IBM model 2 the transla-
tion probability of source language sentence e¯ =
{t1e, ... , tje, ... , tme } and a target language sentence
h¯ = {t1h,... , tih, ..., tle} is given by
τ(tih|tje)a(j|i, m, l).
(2)
Here the word translation model τ(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i, m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, ... , tkh, ...}
from the target language such that word translation
probability τ(tkh|tie) &gt; ε. We rank these terms ac-
cording to the probability given by the word trans-
lation model τ(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ∈ Dh.
</bodyText>
<subsectionHeader confidence="0.997262">
4.2 Cross lingual similarity measure
</subsectionHeader>
<bodyText confidence="0.999973428571428">
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure γ(te, si) &gt; φ. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
</bodyText>
<equation confidence="0.571744">
α(si, te, th) = γ(te, si).τ(th, te) (3)
</equation>
<bodyText confidence="0.999723">
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
</bodyText>
<subsectionHeader confidence="0.997323">
4.3 Cross lingual similarity weight
</subsectionHeader>
<bodyText confidence="0.977056333333333">
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
ω(th, te, si) as
</bodyText>
<equation confidence="0.836222">
ω(th, te, si) = α(th, te, si).idf(th) (4)
</equation>
<bodyText confidence="0.9998757">
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token “bck” if a word translation model produces
a translation output “wapas” (as in came back) or
“peet” or “qamar” (as in back pain) then idf will
weigh “peet” more as it is relatively more discrim-
inative compared to “wapas” which is used fre-
quently.
</bodyText>
<sectionHeader confidence="0.935434" genericHeader="method">
5 Pruning and matching
</sectionHeader>
<bodyText confidence="0.999879">
In this section we describe our search algorithm and
the preprocessing needed to find the best question
ˆQh for a given SMS query.
</bodyText>
<subsectionHeader confidence="0.934491">
5.1 Indexing
</subsectionHeader>
<bodyText confidence="0.996555625">
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qht� that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
</bodyText>
<equation confidence="0.919252142857143">
5http://lucene.apache.org/java/docs/
Pr( h|¯e) = φ(l|m)
�l
i=1
m
E
j=0
</equation>
<page confidence="0.966273">
90
</page>
<bodyText confidence="0.993092074074074">
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
7e in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability τ(thjte) &gt; ε, where
th E Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al., 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al., 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
</bodyText>
<subsectionHeader confidence="0.998375">
5.2 List Creation
</subsectionHeader>
<bodyText confidence="0.999586">
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, ... , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
</bodyText>
<equation confidence="0.857561">
γ(te, si) &gt; φ (5)
</equation>
<bodyText confidence="0.999839">
The threshold value φ is determined experimen-
tally. For every te E Lei we retrieve 7e and then
retrieve the idf scores for every th E 7e. Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
</bodyText>
<equation confidence="0.92898">
τ(thjte) &gt; 0.1 (6)
</equation>
<bodyText confidence="0.999919952380952">
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term “list” im-
plies a sorted list.
For example given a SMS query “hw mch ds it cst
to stdy in india” as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token “cst” we get
dictionary words lik “cost, cast, case, close”. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
</bodyText>
<subsectionHeader confidence="0.999576">
5.3 Search algorithm
</subsectionHeader>
<bodyText confidence="0.99919048">
Given Se containing n tokens, we create n sorted
lists Lh1, Lh2, ... , Lhn containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Qh
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Qh. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1, Lh2, ... , Lhn. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth. The set Qth is added to the candi-
</bodyText>
<page confidence="0.99853">
91
</page>
<figureCaption confidence="0.999688">
Figure 3: List creation
</figureCaption>
<bodyText confidence="0.998650071428571">
date set C. For each question Q E Qt, we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question ˆQh.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1[1], Lh�[1], ... , Lhn[1] from every list. Thus, the
upper bound UB on the score of Q is
</bodyText>
<equation confidence="0.968943666666667">
n
Score(Q) G E ω(Lh i [1]).
i=0
</equation>
<bodyText confidence="0.9982981">
Let Q* be the question in C having the maximum
score. Notice that if Q* &gt; UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition “Q* &gt; UB” serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
</bodyText>
<figure confidence="0.970524862068965">
Procedure Search Algorithm
Input: SMS S = s1, s2, ... , sn
Output: Maximum scoring question ˆQh.
Begin
dsi, construct Lei for which γ(si, te) &gt; e
// Li lists variants of si
Construct lists Lh1, Lh2, ... , Lh n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = 0.
repeat
j* = argmaxiω(Lhi [1])
t*h = Lhj•[1]
// t*h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t*h from the list Lhj,.
Retrieve Qth using the index
// Qth: the set of all questions in Qh
//having the term t*h
For each Q E Qth
Compute Score(Q) and
add Q with its score into C
UB = E1 ω(Lhi[1])
Q = argm_axQccScore(Q).
if Score(Q) &gt; UB, then
// Termination condition satisfied
Output Q and exit.
forever
End
</figure>
<figureCaption confidence="0.999842">
Figure 4: Search Algorithm with Pruning
</figureCaption>
<sectionHeader confidence="0.997887" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999920625">
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al.,
</bodyText>
<page confidence="0.990059">
92
</page>
<bodyText confidence="0.9976408">
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
</bodyText>
<figureCaption confidence="0.996825">
Figure 5: Sample SMS queries
</figureCaption>
<bodyText confidence="0.999606571428571">
2005) and the rest from the “out-of-domain” queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
</bodyText>
<subsectionHeader confidence="0.999813">
6.1 Translation System
</subsectionHeader>
<bodyText confidence="0.999981647058823">
We used the Moses toolkit (Koehn et al., 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold 0 de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
</bodyText>
<subsectionHeader confidence="0.999852">
6.2 System Evaluation
</subsectionHeader>
<bodyText confidence="0.999835">
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
</bodyText>
<subsubsectionHeader confidence="0.711739">
6.2.1 Experiment 1
</subsubsectionHeader>
<bodyText confidence="0.999985">
For Experiment 1 the threshold 0 in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
</bodyText>
<subsubsectionHeader confidence="0.580623">
6.2.2 Experiment 2
</subsubsectionHeader>
<bodyText confidence="0.999954">
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it’s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
</bodyText>
<subsectionHeader confidence="0.509538">
6.2.3 Experiment 3
</subsectionHeader>
<bodyText confidence="0.999603875">
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
</bodyText>
<page confidence="0.999121">
93
</page>
<tableCaption confidence="0.996569">
Table 1: MRR Scores
</tableCaption>
<table confidence="0.9996425">
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
</table>
<tableCaption confidence="0.997057">
Table 2: F1 Measure
</tableCaption>
<bodyText confidence="0.994701230769231">
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
</bodyText>
<equation confidence="0.9964845">
MRR = 1/1QJ � Q 1/ranki (7)
i=1
</equation>
<bodyText confidence="0.99416265">
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
</bodyText>
<figureCaption confidence="0.9999965">
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
</figureCaption>
<subsectionHeader confidence="0.999518">
6.3 Measuring noise level in SMS queries
</subsectionHeader>
<bodyText confidence="0.999965625">
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
</bodyText>
<table confidence="0.992704666666667">
Cleaned SMS Noisy SMS
English FAQ collection bigram 16.64 55.19
trigram 9.75 69.41
</table>
<tableCaption confidence="0.998319">
Table 3: Perplexity for Cleaned and Noisy SMS
</tableCaption>
<figure confidence="0.998989428571429">
Experiment 1
Experiment 2
Experiment 3
MRR Score
0.41
0.68
0.83
</figure>
<page confidence="0.996116">
94
</page>
<bodyText confidence="0.999961466666667">
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. “hw 2 prvnt ty-
phd” we manually created a clean SMS query “how
to prevent typhoid”. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99995988">
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
</bodyText>
<sectionHeader confidence="0.998262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998092767857143">
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
</reference>
<page confidence="0.980573">
95
</page>
<reference confidence="0.999721632653061">
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ’98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
</reference>
<page confidence="0.99846">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.266942">
<title confidence="0.88201">Handling Noisy Queries In Cross Language FAQ Retrieval Danish Contractor Govind Kothari Tanveer A.</title>
<author confidence="0.891702">L Venkata Subramaniam Sumit</author>
<affiliation confidence="0.985443">IBM Research</affiliation>
<address confidence="0.5220765">Vasant Kunj, Institutional New Delhi,</address>
<abstract confidence="0.999529863636364">Recent times have seen a tremendous growth in mobile based data services that allow people to use Short Message Service (SMS) to access these data services. In a multilingual society it is essential that data services that were developed for a specific language be made accessible through other local languages also. In this paper, we present a service that allows a user to query a Frequently- Asked-Questions (FAQ) database built in a local language (Hindi) using Noisy SMS English queries. The inherent noise in the SMS queries, along with the language mismatch makes this a challenging problem. We handle these two problems by formulating the query similarity over FAQ questions as a combinatorial search problem where the search space consists of combinations of dictionary variations of the noisy query and its top-N translations. We demonstrate the effectiveness of our approach on a real-life dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sreangsu Acharyya</author>
<author>Sumit Negi</author>
<author>L Venkata Subramaniam</author>
<author>Shourya Roy</author>
</authors>
<title>Language independent unsupervised learning of short message service dialect.</title>
<date>2009</date>
<booktitle>International Journal on Document Analysis and Recognition,</booktitle>
<pages>175--184</pages>
<contexts>
<context position="5617" citStr="Acharyya et al., 2009" startWordPosition="881" endWordPosition="884">language and clean sentences in another language are not available and it would be hard to build such large parallel corpora to train a machine translation system. There exists some work to remove noise from SMS (Choudhury et al., 2007) (Byun et al., 2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus et al., 2008). However, all of these techniques require an aligned corpus of SMS and conventional language for training. Such data is extremely hard to create. Unsupervised techniques require huge amounts of SMS data to learn mappings of nonstandard words to their corresponding conventional form (Acharyya et al., 2009). Removal of noise from SMS without the use of parallel data has been studied but the methods used are highly dependent on the language model and the degree of noise present in the SMS (Contractor et al., 2010). These systems are not very effective if the SMSes contain grammatical errors (or the system would require large amounts of training data in the language model to be able to deal with all possible types of noise) in addition to misspellings etc. Thus, the translation of a cleaned SMS, into a second language, will not be very accurate and it would not give good results if such a translat</context>
</contexts>
<marker>Acharyya, Negi, Subramaniam, Roy, 2009</marker>
<rawString>Sreangsu Acharyya, Sumit Negi, L Venkata Subramaniam, Shourya Roy. 2009. Language independent unsupervised learning of short message service dialect. International Journal on Document Analysis and Recognition, pp. 175-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aiti Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5269" citStr="Aw et al., 2006" startWordPosition="825" endWordPosition="828">allel corpora to learn the translation probabilities (Brown et al., 2007). Traditionally such corpora have consisted of news articles and other well written articles. Since the translation systems are not trained on SMS language they perform very poorly when translating noisy SMS language. Parallel corpora comprising noisy sentences in one language and clean sentences in another language are not available and it would be hard to build such large parallel corpora to train a machine translation system. There exists some work to remove noise from SMS (Choudhury et al., 2007) (Byun et al., 2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus et al., 2008). However, all of these techniques require an aligned corpus of SMS and conventional language for training. Such data is extremely hard to create. Unsupervised techniques require huge amounts of SMS data to learn mappings of nonstandard words to their corresponding conventional form (Acharyya et al., 2009). Removal of noise from SMS without the use of parallel data has been studied but the methods used are highly dependent on the language model and the degree of noise present in the SMS (Contractor et al., 2010). These systems are not very effective if </context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proceedings of COLING-ACL, pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation Computational Linguistics,</journal>
<pages>263--311</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, Robert. L. Mercer 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation Computational Linguistics, pp. 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeunghyun Byun</author>
</authors>
<title>Seung-Wook Lee, Young-In Song, Hae-Chang Rim.</title>
<date>2008</date>
<booktitle>AAAI Workshop on Enhanced Messaging.</booktitle>
<marker>Byun, 2008</marker>
<rawString>Jeunghyun Byun, Seung-Wook Lee, Young-In Song, Hae-Chang Rim. 2008. Two Phase Model for SMS Text Messages Refinement. AAAI Workshop on Enhanced Messaging.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
</authors>
<title>Animesh Mukherjee, Sudeshna Sarkar, Anupam Basu.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>157--174</pages>
<contexts>
<context position="5231" citStr="Choudhury et al., 2007" startWordPosition="817" endWordPosition="820">cal machine translation systems use large parallel corpora to learn the translation probabilities (Brown et al., 2007). Traditionally such corpora have consisted of news articles and other well written articles. Since the translation systems are not trained on SMS language they perform very poorly when translating noisy SMS language. Parallel corpora comprising noisy sentences in one language and clean sentences in another language are not available and it would be hard to build such large parallel corpora to train a machine translation system. There exists some work to remove noise from SMS (Choudhury et al., 2007) (Byun et al., 2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus et al., 2008). However, all of these techniques require an aligned corpus of SMS and conventional language for training. Such data is extremely hard to create. Unsupervised techniques require huge amounts of SMS data to learn mappings of nonstandard words to their corresponding conventional form (Acharyya et al., 2009). Removal of noise from SMS without the use of parallel data has been studied but the methods used are highly dependent on the language model and the degree of noise present in the SMS (Contractor et al., 2010). Th</context>
</contexts>
<marker>Choudhury, Saraf, Jain, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, pp. 157-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit versus latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In Proceeding of IJCAI,</booktitle>
<pages>1513--1518</pages>
<contexts>
<context position="4301" citStr="Cimiano et al., 2009" startWordPosition="668" endWordPosition="671">(Song et al., 2007). The high level of noise in SMS queries makes this a difficult problem (Kothari et al., 2009). In a multilingual setting this problem is even more formidable. Natural language FAQ services built for users in one language cannot be accessed in another language. In this paper we present a FAQ-based question answering system over a SMS interface that solves this problem for two languages. We allow the FAQ to be in one language and the SMS query to be in another. Multi-lingual question answering and information retrieval has been studied in the past (Sekine and Grishman, 2003)(Cimiano et al., 2009). Such systems resort to machine translation so that the search can be performed over a single language space. In the two language setting, it involves building a machine translation system engine and using it such that the question answering system built for a single language can be used. Typical statistical machine translation systems use large parallel corpora to learn the translation probabilities (Brown et al., 2007). Traditionally such corpora have consisted of news articles and other well written articles. Since the translation systems are not trained on SMS language they perform very p</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, Steffen Staab. 2009. Explicit versus latent concept models for cross-language information retrieval. In Proceeding of IJCAI, pp. 1513-1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
<author>Tanveer A Faruquie</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Unsupervised cleansing of noisy text.</title>
<date>2010</date>
<booktitle>In Proceeding of COLING 2010: Posters,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="5827" citStr="Contractor et al., 2010" startWordPosition="919" endWordPosition="922">SMS (Choudhury et al., 2007) (Byun et al., 2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus et al., 2008). However, all of these techniques require an aligned corpus of SMS and conventional language for training. Such data is extremely hard to create. Unsupervised techniques require huge amounts of SMS data to learn mappings of nonstandard words to their corresponding conventional form (Acharyya et al., 2009). Removal of noise from SMS without the use of parallel data has been studied but the methods used are highly dependent on the language model and the degree of noise present in the SMS (Contractor et al., 2010). These systems are not very effective if the SMSes contain grammatical errors (or the system would require large amounts of training data in the language model to be able to deal with all possible types of noise) in addition to misspellings etc. Thus, the translation of a cleaned SMS, into a second language, will not be very accurate and it would not give good results if such a translated SMS is used to query an FAQ collection. Token based noise-correction techniques (such as those using edit-distance, LCS etc) cannot be directly applied to handle the noise present in the SMS query. These noi</context>
</contexts>
<marker>Contractor, Faruquie, Subramaniam, 2010</marker>
<rawString>Danish Contractor, Tanveer A. Faruquie, L. Venkata Subramaniam. 2010. Unsupervised cleansing of noisy text. In Proceeding of COLING 2010: Posters, pp. 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fagin</author>
<author>A Lotem</author>
<author>M Naor</author>
</authors>
<title>Optimal aggregation algorithms for middleware.</title>
<date>2001</date>
<booktitle>In Proceedings of the 20th ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,</booktitle>
<pages>102--113</pages>
<contexts>
<context position="18779" citStr="Fagin et al., 2001" startWordPosition="3208" endWordPosition="3211">containing n tokens, we create n sorted lists Lh1, Lh2, ... , Lhn containing terms from the domain dictionary and sorted according to their cross lingual weights as explained in the previous section. A naive approach would be to query the index using each term appearing in all Lhi to build a Collection set C of questions. The best matching question Qh will be contained in this collection. We compute the score of each question in C using Score(Q) and the question with highest score is treated as Qh. However the naive approach suffers from high runtime cost. Inspired by the Threshold Algorithm (Fagin et al., 2001) we propose using a pruning algorithm that maintains a much smaller candidate set C of questions that can potentially contain the maximum scoring question. The algorithm is shown in Figure 4. The algorithm works in an iterative manner. In each iteration, it picks the term that has maximum weight among all the terms appearing in the lists Lh1, Lh2, ... , Lhn. As the lists are sorted in the descending order of the weights, this amounts to picking the maximum weight term amongst the first terms of the n lists. The chosen term th is queried to find the set Qth. The set Qth is added to the candi91 </context>
</contexts>
<marker>Fagin, Lotem, Naor, 2001</marker>
<rawString>R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggregation algorithms for middleware. In Proceedings of the 20th ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pp. 102-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones. In</title>
<date>2005</date>
<booktitle>Proc. of Human Computer Interfaces International,Lawrence Erlbaum Associates</booktitle>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of Human Computer Interfaces International,Lawrence Erlbaum Associates</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth ACM Conference on Information and Knowledge Management,CIKM,</booktitle>
<pages>76--83</pages>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. In Proceedings of the Tenth ACM Conference on Information and Knowledge Management,CIKM, pp. 76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Francois Yvon</author>
<author>Grraldine Damnati</author>
</authors>
<title>Normalizing SMS: Are two metaphors better than one?</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="5310" citStr="Kobus et al., 2008" startWordPosition="833" endWordPosition="836"> probabilities (Brown et al., 2007). Traditionally such corpora have consisted of news articles and other well written articles. Since the translation systems are not trained on SMS language they perform very poorly when translating noisy SMS language. Parallel corpora comprising noisy sentences in one language and clean sentences in another language are not available and it would be hard to build such large parallel corpora to train a machine translation system. There exists some work to remove noise from SMS (Choudhury et al., 2007) (Byun et al., 2008) (Aw et al., 2006) (Neef et al., 2007) (Kobus et al., 2008). However, all of these techniques require an aligned corpus of SMS and conventional language for training. Such data is extremely hard to create. Unsupervised techniques require huge amounts of SMS data to learn mappings of nonstandard words to their corresponding conventional form (Acharyya et al., 2009). Removal of noise from SMS without the use of parallel data has been studied but the methods used are highly dependent on the language model and the degree of noise present in the SMS (Contractor et al., 2010). These systems are not very effective if the SMSes contain grammatical errors (or </context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Francois Yvon and Grraldine Damnati. 2008. Normalizing SMS: Are two metaphors better than one? In Proceedings of COLING, pp. 441-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation. Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst</location>
<contexts>
<context position="23191" citStr="Koehn et al., 2007" startWordPosition="4027" endWordPosition="4030">a size was 440. Fig 5 shows some of the sample queries. Our objective was to retrieve the correct Hindi FAQ response given a noisy English SMS query. A given English SMS query was matched against the list of indexed FAQs and the best matching FAQ was returned by the Pruning Algorithm described in Section 5. A score of 1 was assigned if the retrieved answer was indeed the response to the posed SMS query else we assigned a score of 0. In case of out of domain queries a score of 1 was assigned if the output was NULL else we assigned a score of 0. 6.1 Translation System We used the Moses toolkit (Koehn et al., 2007) to build an English-Hindi statistical machine translation system. The system was trained on a collection of 150, 000 English and Hindi parallel sentences sourced from a publishing house. The 150, 000 sentences were on a varied range of subjects such as news, literature, history etc. Apart from this the training data also contained an aligned parallel corpus of English and Hindi FAQs. The FAQs were collected from government websites on topics such as health, education, travel services etc. Since an MT system trained solely on a collection of sentences would not be very accurate in translating </context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst 2007. Moses: Open source toolkit for statistical machine translation. Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunil Kumar Kopparapu</author>
<author>Akhilesh Srivastava</author>
<author>Arun Pande</author>
</authors>
<title>SMS based Natural Language Interface to Yellow Pages Directory.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th international conference on mobile technology, applications, and systems and the 1st international symposium on Computer human interaction in mobile technology,</booktitle>
<pages>558--563</pages>
<contexts>
<context position="2087" citStr="Kopparapu et al., 2007" startWordPosition="323" endWordPosition="326">mobile provides unmatched portability. This has encouraged the proliferation of information services built around SMS technology. Several applications, traditionally available on Internet, are now being made available on mobile devices using SMS. Examples include SMS short code services. Short codes are numbers where a short message in a predesignated format can be sent to get specific information. For example, to get the closing stock price of a particular share, the user has to send a message IBMSTOCKPR. Other examples are search (Schusteritsch et al., 2005), access to Yellow Page services (Kopparapu et al., 2007), Email 1, Blog 2 , FAQ retrieval 3 etc. The SMS-based FAQ retrieval services use human experts to answer SMS questions. Recent studies have shown that instant messaging is emerging as the preferred mode of communication after speech and email.4 Millions of users of instant messaging (IM) services and short message service (SMS) generate electronic content in a dialect that does not adhere to conventional grammar, punctuation and spelling standards. Words are intentionally compressed by non-standard spellings, abbreviations and phonetic transliteration are used. Typical question answering syst</context>
</contexts>
<marker>Kopparapu, Srivastava, Pande, 2007</marker>
<rawString>Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun Pande. 2007. SMS based Natural Language Interface to Yellow Pages Directory. In Proceedings of the 4th international conference on mobile technology, applications, and systems and the 1st international symposium on Computer human interaction in mobile technology, pp. 558-563 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Govind Kothari</author>
</authors>
<title>Sumit Negi, Tanveer Faruquie, Venkat Chakravarthy and L V Subramaniam</title>
<date>2009</date>
<marker>Kothari, 2009</marker>
<rawString>Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat Chakravarthy and L V Subramaniam 2009. SMS based Interface for FAQ Retrieval. Annual Meeting of the Association for Computation Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>107--130</pages>
<marker>Melamed, 1999</marker>
<rawString>I. D. Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, pp. 107-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guimier de Neef</author>
<author>Arnaud Debeurme Emilie</author>
<author>Jungyeul Park</author>
</authors>
<title>TILT correcteur de SMS : Evaluation et bilan quantitatif.</title>
<date>2007</date>
<booktitle>In Actes de TALN,</booktitle>
<pages>123--132</pages>
<marker>de Neef, Emilie, Park, 2007</marker>
<rawString>Guimier de Neef, Emilie, Arnaud Debeurme, and Jungyeul Park. 2007. TILT correcteur de SMS : Evaluation et bilan quantitatif. In Actes de TALN, pp. 123-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
</authors>
<title>Funda Ertunc.</title>
<date>2002</date>
<booktitle>In Proceedings of the ECIR,</booktitle>
<pages>324--333</pages>
<marker>Oard, 2002</marker>
<rawString>Douglas W. Oard, Funda Ertunc. 2002. TranslationBased Indexing for Cross-Language Retrieval In Proceedings of the ECIR, pp. 324-333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pirkola</author>
</authors>
<title>The Effects of Query Structure and Dictionary Setups in Dictionary-Based CrossLanguage Information Retrieval</title>
<date>1998</date>
<booktitle>SIGIR ’98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="15657" citStr="Pirkola, 1998" startWordPosition="2640" endWordPosition="2641">erm th. We query on this index to determine the number of documents f that contain th. The idf of each term in Dh is precomputed and stored in a hashtable with th as the key. The cross lingual similarity measure calculation requires the word translation probability for a given term te. For every te in dictionary De, we store 7e in a hashmap that contains a list of terms in the target language along with their statistically determined translation probability τ(thjte) &gt; ε, where th E Dh. Since the query and the FAQs use terms from different languages, the computation of IDF becomes a challenge (Pirkola, 1998) (Oard et al., 2007). Prior work uses a bilingual dictionary for translations for calculating the IDF. We on the other hand rely on a statistical dictionary that has translation probabilities. Applying the method suggested in the prior work on a statistical dictionary leads to errors as the translations may themselves be inaccurate. We therefore calculate IDFs for target language term (translation) and use it in the weight measure calculation. The method suggested by Oard et al (Oard et al., 2007) is more useful in retrieval tasks for multiple documents, while in our case we need to retrieve a</context>
</contexts>
<marker>Pirkola, 1998</marker>
<rawString>A. Pirkola 1998. The Effects of Query Structure and Dictionary Setups in Dictionary-Based CrossLanguage Information Retrieval SIGIR ’98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 55-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Prochasson</author>
<author>C Viard-Gaudin</author>
<author>E Morin</author>
</authors>
<title>Language models for handwritten short message services.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th International Conference on Document Analysis and Recognition,</booktitle>
<pages>83--87</pages>
<contexts>
<context position="10643" citStr="Prochasson et al., 2007" startWordPosition="1776" endWordPosition="1779">tween te and si. * The intuition behind this measure is that people typically type the first few characters of a word in an SMS correctly. This way we limit the possible variants for a particular noisy token The Longest Common Subsequence Ratio (LCSRatio) (Melamed et al., 1999) of two strings is the ratio of the length of their LCS and the length of the longer string. Since in the SMS scenario, the dictionary term will always be longer than the SMS token, the denominator of LCSRatio is taken as the length of the dictionary term. The EditDistanceSMS (Figure 2) compares the Consonant Skeletons (Prochasson et al., 2007) of the dictionary term and the SMS token. If the Levenshtein distance between consonant skeletons is small then γ(te, si) will be high. The intuition behind using EditDistanceSMS can be explained through an example. Consider an SMS token “gud” whose most likely correct form is “good”. The longest common subsequence for “good” and “guided” with “gud” is “gd”. Hence the two dictionary terms “good” and “guided” have the same LCSRatio of 0.5 w.r.t “gud”, but the EditDistanceSMS of “good” is 1 which is less than that of “guided”, which has EditDistanceSMS of 2 w.r.t “gud”. As a result the similari</context>
</contexts>
<marker>Prochasson, Viard-Gaudin, Morin, 2007</marker>
<rawString>E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007. Language models for handwritten short message services. In Proceedings of the 9th International Conference on Document Analysis and Recognition, pp. 83-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudy Schusteritsch</author>
<author>Shailendra Rao</author>
<author>Kerry Rodden</author>
</authors>
<title>Mobile Search with Text Messages: Designing the User Experience for Google SMS.</title>
<date>2005</date>
<booktitle>In Proceedings of ACM SIGCHI,</booktitle>
<pages>1777--1780</pages>
<contexts>
<context position="2030" citStr="Schusteritsch et al., 2005" startWordPosition="314" endWordPosition="317">Even for users familiar with computers and the internet, the mobile provides unmatched portability. This has encouraged the proliferation of information services built around SMS technology. Several applications, traditionally available on Internet, are now being made available on mobile devices using SMS. Examples include SMS short code services. Short codes are numbers where a short message in a predesignated format can be sent to get specific information. For example, to get the closing stock price of a particular share, the user has to send a message IBMSTOCKPR. Other examples are search (Schusteritsch et al., 2005), access to Yellow Page services (Kopparapu et al., 2007), Email 1, Blog 2 , FAQ retrieval 3 etc. The SMS-based FAQ retrieval services use human experts to answer SMS questions. Recent studies have shown that instant messaging is emerging as the preferred mode of communication after speech and email.4 Millions of users of instant messaging (IM) services and short message service (SMS) generate electronic content in a dialect that does not adhere to conventional grammar, punctuation and spelling standards. Words are intentionally compressed by non-standard spellings, abbreviations and phonetic </context>
</contexts>
<marker>Schusteritsch, Rao, Rodden, 2005</marker>
<rawString>Rudy Schusteritsch, Shailendra Rao, Kerry Rodden. 2005. Mobile Search with Text Messages: Designing the User Experience for Google SMS. In Proceedings of ACM SIGCHI, pp. 1777-1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Hindi-English cross-lingual question-answering system.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>181--192</pages>
<contexts>
<context position="4279" citStr="Sekine and Grishman, 2003" startWordPosition="665" endWordPosition="668">nt answer (Sneiders, 1999) (Song et al., 2007). The high level of noise in SMS queries makes this a difficult problem (Kothari et al., 2009). In a multilingual setting this problem is even more formidable. Natural language FAQ services built for users in one language cannot be accessed in another language. In this paper we present a FAQ-based question answering system over a SMS interface that solves this problem for two languages. We allow the FAQ to be in one language and the SMS query to be in another. Multi-lingual question answering and information retrieval has been studied in the past (Sekine and Grishman, 2003)(Cimiano et al., 2009). Such systems resort to machine translation so that the search can be performed over a single language space. In the two language setting, it involves building a machine translation system engine and using it such that the question answering system built for a single language can be used. Typical statistical machine translation systems use large parallel corpora to learn the translation probabilities (Brown et al., 2007). Traditionally such corpora have consisted of news articles and other well written articles. Since the translation systems are not trained on SMS langua</context>
</contexts>
<marker>Sekine, Grishman, 2003</marker>
<rawString>Satoshi Sekine, Ralph Grishman. 2003. Hindi-English cross-lingual question-answering system. ACM Transactions on Asian Language Information Processing, pp. 181-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sneiders</author>
</authors>
<title>Automated FAQ Answering: Continued Experience with Shallow Language Understanding Question Answering Systems. Papers from the 1999 AAAI Fall Symposium.</title>
<date>1999</date>
<tech>Technical Report FS-99-02,</tech>
<pages>97--107</pages>
<publisher>AAAI Press,</publisher>
<contexts>
<context position="3679" citStr="Sneiders, 1999" startWordPosition="564" endWordPosition="565">ollection, Q&amp;A archive (Xue et al., 2008) or the Web (Jijkoun et al., 2005), in a FAQ database the questions and answers are already pro1http://www.sms2email.com/ 2http://www.letmeparty.com/ 3http://www.chacha.com/ 4http://www.whyconverge.com/ 87 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87–96, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: Sample SMS queries with Hindi FAQs vided by an expert. The main task is then to identify the best matching question to retrieve the relevant answer (Sneiders, 1999) (Song et al., 2007). The high level of noise in SMS queries makes this a difficult problem (Kothari et al., 2009). In a multilingual setting this problem is even more formidable. Natural language FAQ services built for users in one language cannot be accessed in another language. In this paper we present a FAQ-based question answering system over a SMS interface that solves this problem for two languages. We allow the FAQ to be in one language and the SMS query to be in another. Multi-lingual question answering and information retrieval has been studied in the past (Sekine and Grishman, 2003)</context>
</contexts>
<marker>Sneiders, 1999</marker>
<rawString>E. Sneiders. 1999. Automated FAQ Answering: Continued Experience with Shallow Language Understanding Question Answering Systems. Papers from the 1999 AAAI Fall Symposium. Technical Report FS-99-02, AAAI Press, pp. 97-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Song</author>
<author>M Feng</author>
<author>N Gu</author>
<author>L Wenyin</author>
</authors>
<title>Question similarity calculation for FAQ answering.</title>
<date>2007</date>
<booktitle>In Proceeding of SKG 07,</booktitle>
<pages>298--301</pages>
<contexts>
<context position="3699" citStr="Song et al., 2007" startWordPosition="566" endWordPosition="569">chive (Xue et al., 2008) or the Web (Jijkoun et al., 2005), in a FAQ database the questions and answers are already pro1http://www.sms2email.com/ 2http://www.letmeparty.com/ 3http://www.chacha.com/ 4http://www.whyconverge.com/ 87 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87–96, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: Sample SMS queries with Hindi FAQs vided by an expert. The main task is then to identify the best matching question to retrieve the relevant answer (Sneiders, 1999) (Song et al., 2007). The high level of noise in SMS queries makes this a difficult problem (Kothari et al., 2009). In a multilingual setting this problem is even more formidable. Natural language FAQ services built for users in one language cannot be accessed in another language. In this paper we present a FAQ-based question answering system over a SMS interface that solves this problem for two languages. We allow the FAQ to be in one language and the SMS query to be in another. Multi-lingual question answering and information retrieval has been studied in the past (Sekine and Grishman, 2003)(Cimiano et al., 200</context>
</contexts>
<marker>Song, Feng, Gu, Wenyin, 2007</marker>
<rawString>W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Question similarity calculation for FAQ answering. In Proceeding of SKG 07, pp. 298-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval Models for Question and Answer Archives.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="3105" citStr="Xue et al., 2008" startWordPosition="482" endWordPosition="485">ntional grammar, punctuation and spelling standards. Words are intentionally compressed by non-standard spellings, abbreviations and phonetic transliteration are used. Typical question answering systems are built for use with languages which are free from such errors. It is difficult to build an automated question answering system around SMS technology. This is true even for questions whose answers are well documented like in a Frequently-Asked-Questions (FAQ) database. Unlike other automatic question answering systems that focus on searching answers from a given text collection, Q&amp;A archive (Xue et al., 2008) or the Web (Jijkoun et al., 2005), in a FAQ database the questions and answers are already pro1http://www.sms2email.com/ 2http://www.letmeparty.com/ 3http://www.chacha.com/ 4http://www.whyconverge.com/ 87 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87–96, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: Sample SMS queries with Hindi FAQs vided by an expert. The main task is then to identify the best matching question to retrieve the relevant answer (Sneiders, 1999) (Song et al., 2007). The </context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models for Question and Answer Archives. In Proceedings of SIGIR, pp. 475-482.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>