<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034299">
<note confidence="0.789618">
Computational Linguistics Volume 24, Number 4
</note>
<title confidence="0.977599">
The Architecture of the Language Faculty
</title>
<author confidence="0.80615">
Ray Jackendoff
</author>
<bodyText confidence="0.740238166666667">
(Brandeis University)
Cambridge, MA: The MIT Press
(Linguistic Inquiry monographs, edited
by Samuel Jay Keyser, volume 28),
1997, xvi+262 pp; paperbound, ISBN
0-262-60025-0, $20.00
</bodyText>
<figure confidence="0.763310666666667">
Reviewed by
Suzanne Stevenson
Rutgers University
</figure>
<bodyText confidence="0.963180885714285">
Much current research on natural language emphasizes the critical contribution of lex-
ical information to the definition and computation of linguistic structures. Motivations
for the focus on the lexicon cross the fields of theoretical linguistics, computational
linguistics, and psycholinguistics—for example, to explain the interaction of broad
syntactic principles with individual lexical information (e.g., Grimshaw 1990), to cap-
ture the syntactic and statistical relationships among words (e.g., Srinivas 1997), and
to relate lexical and syntactic processing within a cognitive theory of language (e.g.,
MacDonald, Pearlmutter, and Seidenberg 1994).
The convergence of this focus on the lexicon across diverse fields of linguistic
study has led to a tremendous potential for cross-disciplinary fertilization, to which
Jackendoff is no stranger. Many computational linguists will be familiar with his past
work on lexical-conceptual structure (e.g., Jackendoff 1983), which has been influential
in computational lexical semantics and machine translation (e.g., Dorr 1993). Within
this context, Jackendoff&apos;s new book, The Architecture of the Language Faculty, offers a
new path to the conclusion of the centrality of the lexicon within a broad theory of
human language, in which his notion of lexical structure is extended both in scope and
power. While focusing on arguments from theoretical linguistics to support his views,
Jackendoff explores the psycholinguistic and computational implications for many of
his conclusions. The book addresses issues ranging from expletive infixation to the
nature of consciousness, but here I&apos;ll focus on the lexical aspects of the proposal.
Working from fundamental assumptions about the modularity of cognitive sub-
systems, Jackendoff explores the ensuing constraints on the basic architecture of lin-
guistic theory, and consequences for the nature of the lexicon. The proposal hinges on
a concept that he terms here Representational Modularity—the idea that the mind is
divided into modules on the basis of the representational format that a cognitive sys-
tem uses.1 For example, phonology, syntax, and semantics will comprise three separate
representational modules, because the structures they manipulate require different for-
mal primitives and combinatorial principles. Because representational modules cannot
communicate directly with each other (since, by definition, they don&apos;t understand each
other&apos;s &amp;quot;language&amp;quot;), Jackendoff further proposes the existence of interface modules—
specialized components of the mind that translate between relevant aspects of two
1 The book thus relates to another thread of computational linguistic research—that of modularity in
Government-Binding parsing (e.g., Crocker [1992], Merlo [1996], among many others). Jackendoff&apos;s
sense of modularity differs in that it applies across linguistic subsystems, and across cognitive
subsystems more generally, rather than within the syntactic component of grammar.
</bodyText>
<page confidence="0.992853">
652
</page>
<subsectionHeader confidence="0.466271">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99997668627451">
or more cognitive subsystems. Only the outputs of the representational modules are
accessible to the interface module that translates between them. The characterization
of the interface module within the linguistic faculty, rather than the representational
modules themselves, is the focus of Jackendoff&apos;s present study.
As a starting point, Jackendoff assumes that an interface module consists of a set
of correspondence rules between representational formats, which may be specified as
obligatory optional, or default. The book presents and reviews a number of linguis-
tic phenomena and analyses to explore the relationship between the representational
modules of grammar (phonology / syntax / semantics), thereby determining the nec-
essary properties of the correspondence rules that relate them. Jackendoff shows that
the correspondences mediated by the interface module are complex, partial, and non-
derivational. He further concludes that the lexicon, rather than occurring as a separate
representational module, is an important component of this interface, playing a crucial
role in linking together and licensing structures across the submodules of the linguistic
system.
Jackendoff&apos;s general argument about the nature and role of the lexicon can be
briefly summarized as follows. Representational Modularity demands that the lan-
guage faculty must be split into three modules—phonology, syntax, and semantics—
each of which manipulates structures that are stated in its own representational format.
Lexical items, which include phonological, syntactic, and semantic content, cannot be
inserted as a whole into a structure in any one of these modules, since doing so would
violate Representational Modularity. Instead, a lexical item is a set of three structures
(phonological, syntactic, and semantic) that are linked by correspondence rules. Each
component of the lexical item is unified with the output structure of the appropriate
representational module, and the correspondence rules ensure that the correct linkages
between the pieces of a lexical item are enforced. The lexical items thus are crucial both
in licensing and linking the outputs of the individual representational modules. Since
each lexical item makes reference to phonological, syntactic, and semantic structures,
the lexicon must be conceived of as a part of the linguistic interface module, rather
than as a representational module itself.
Simply put, the claim is that what we call the lexicon is not a distinct entity but
rather a subset of the interface relations between the three grammatical subsystems.
This formulation leads to some advantages for how we factor information among the
components of grammar. For example, note that in addition to word-level linkages,
the interface module must also include correspondence rules that relate higher-level
phrase structures, as well as lower-level (intraword) morphological structures. Since
lexical items are part of this general interface, there is then no need to restrict them
to word-sized elements—they can be affixes, single words, compound words, or even
whole constructions. Jackendoff&apos;s proposal thus has the potential to provide a uniform
characterization of morphological, lexical, and phrase-level knowledge and processes,
within a highly lexicalized framework.
To see whether this potential is realizable, some important aspects of the approach
must be fleshed out. One consequence of Jackendoff&apos;s proposal is that much of the
work of linguistic theory is accomplished by the correspondence rules that relate the
various levels of representation. Thus, the interface module that encapsulates these
rules is a source of tremendous formal power—at least in addition to, if not instead
of, the representational modules encoding the traditional components of grammar. Yet,
while representational modules are explicitly constrained (to manipulate only a single
representational format), the formal restrictions on interface modules are not made
precise in the book. For example, Jackendoff asserts that some aspects of the output
structures of the representational modules are &amp;quot;visible&amp;quot; to the interface and some are
</bodyText>
<page confidence="0.997946">
653
</page>
<note confidence="0.639976">
Computational Linguistics Volume 24, Number 4
</note>
<bodyText confidence="0.999947805555556">
not, and furthermore that some aspects must be visible to a subset of the rules and not
to others—in neither case providing a formal characterization of how this distinction
is made. The lack of restrictions on the linguistic interface module—the content of
which is mediated only by what Jackendoff calls a balancing of power among relevant
modules—means that it isn&apos;t clear what couldn&apos;t go into the interface. If there are no
restrictions on the correspondence rules in terms of what information—and sets of
information across modules—they have access to, then there&apos;s no modularity left at
all—the interface module is completely &amp;quot;interactive.&amp;quot; Given that so much grammatical
knowledge is being pushed into the interface module, this kind of issue will be crucial
to a final evaluation of Jackendoff&apos;s proposal.
For the most part, the book is extremely well-written, and full of fascinating ex-
amples and analyses of phenomena crossing the boundaries of phonology syntax,
and semantics. Its breadth of scope means that the technical details of specific ex-
amples might not have the depth or sophistication that would satisfy an expert in
that area, but the intent to examine the boundaries—both between subcomponents
within the language faculty as well as between the language faculty and other cog-
nitive systems—leads to interesting insights and proposals on topics that are often
neglected in more-narrowly focused research. For example, although highly specula-
tive, the epilogue on the implications of Representational Modularity for the relations
between language and thought, and between attention and consciousness, is highly
thought-provoking. I found the book enjoyable and engaging, and believe that it will
appeal to computational linguists with an interest in linguistic theory and cognitive
science, and especially the relation between the two.
One aspect of the book that I think may detract from the enjoyment of some
readers is the (in my view, unwarranted) focus on comparisons to assumptions and
proposals of Chomsky. While this perspective may make sense for an audience of
theoretical syntacticians in the Chomskyan tradition, the intent of the book to situate
linguistics within the broader cognitive sciences makes this emphasis a bit baffling. By
contrast, more discussion could have been devoted to exploring in detail the relation
to more similar frameworks (such as HPSG, LFG, and Optimality Theory), in which
the assumption of multiple levels of representations and (nonderivational) correspon-
dences between them is not new. I suspect that computational linguists will find much
to admire in Jackendoff&apos;s proposed widening of the perspective within theoretical lin-
guistics, and in his concern with psychological and computational issues, but will
be puzzled by the perceived need for detailed arguments against such Chomskyan
assumptions as syntactocentrism and the emphasis on derivational relations.
</bodyText>
<sectionHeader confidence="0.99072" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.999615269230769">
Crocker, Matthew W. 1992. A Logical Model of
Competence and Performance in the Human
Sentence Processor. Doctoral dissertation,
University of Edinburgh.
Dorr, Bonnie. 1993. Machine Translation: A
View from the Lexicon. The MIT Press,
Cambridge, MA.
Grirnshaw, Jane. 1990. Argument Structure.
The MIT Press, Cambridge, MA.
Jackendoff, Ray. 1983. Semantics and
Cognition. The MIT Press, Cambridge, MA.
MacDonald, Maryellen, Neal Pearlmutter,
and Mark Seidenberg. 1994. The lexical
nature of syntactic ambiguity resolution.
Psychological Review, 101(4):676-703.
Merlo, Paola. 1996. Parsing with Principles
and Classes of Information. Kluwer
Academic Publishers, Dordrecht.
Srinivas, Bangalore. 1997. Complexity of
Lexical Descriptions and its Relevance to
Parsing. Doctoral dissertation, Department
of Computer and Information Science,
University of Pennsylvania. (Available as
technical report 97-10, Institute for
Research in Cognitive Science, University
of Pennsylvania.)
</reference>
<page confidence="0.998821">
654
</page>
<subsectionHeader confidence="0.740842">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.985755">
Suzanne Stevenson is an assistant professor in the Department of Computer Science, and in
the Center for Cognitive Science (RuCCS), at Rutgers University. Her research is in the area
of computational models of human syntactic processing, focusing on the relation of lexical
and syntactic theories to cognitively motivated parsing mechanisms. Stevenson&apos;s address is
Department of Computer Science, Rutgers University, CoRE Building, Busch Campus, New
Brunswick, NJ 08903; e-mail: suzanne@cs.rutgers.edu
</bodyText>
<page confidence="0.998493">
655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000605">
<title confidence="0.7908795">Computational Linguistics Volume 24, Number 4 The Architecture of the Language Faculty</title>
<author confidence="0.999952">Ray Jackendoff</author>
<affiliation confidence="0.99433">(Brandeis University)</affiliation>
<address confidence="0.969323">Cambridge, MA: The MIT Press</address>
<note confidence="0.7455296">(Linguistic Inquiry monographs, edited by Samuel Jay Keyser, volume 28), 1997, xvi+262 pp; paperbound, ISBN 0-262-60025-0, $20.00 Reviewed by</note>
<author confidence="0.98448">Suzanne Stevenson</author>
<affiliation confidence="0.999967">Rutgers University</affiliation>
<abstract confidence="0.996554531746032">Much current research on natural language emphasizes the critical contribution of lexical information to the definition and computation of linguistic structures. Motivations for the focus on the lexicon cross the fields of theoretical linguistics, computational linguistics, and psycholinguistics—for example, to explain the interaction of broad syntactic principles with individual lexical information (e.g., Grimshaw 1990), to capture the syntactic and statistical relationships among words (e.g., Srinivas 1997), and to relate lexical and syntactic processing within a cognitive theory of language (e.g., MacDonald, Pearlmutter, and Seidenberg 1994). The convergence of this focus on the lexicon across diverse fields of linguistic study has led to a tremendous potential for cross-disciplinary fertilization, to which Jackendoff is no stranger. Many computational linguists will be familiar with his past work on lexical-conceptual structure (e.g., Jackendoff 1983), which has been influential in computational lexical semantics and machine translation (e.g., Dorr 1993). Within context, Jackendoff&apos;s new book, Architecture of the Language Faculty, a new path to the conclusion of the centrality of the lexicon within a broad theory of human language, in which his notion of lexical structure is extended both in scope and power. While focusing on arguments from theoretical linguistics to support his views, Jackendoff explores the psycholinguistic and computational implications for many of his conclusions. The book addresses issues ranging from expletive infixation to the nature of consciousness, but here I&apos;ll focus on the lexical aspects of the proposal. from fundamental assumptions about of cognitive subsystems, Jackendoff explores the ensuing constraints on the basic architecture of lintheory, and consequences for the nature of lexicon. The proposal hinges on concept that he terms here Modularity—the that the mind is divided into modules on the basis of the representational format that a cognitive sys- For example, phonology, syntax, and semantics will comprise three separate representational modules, because the structures they manipulate require different formal primitives and combinatorial principles. Because representational modules cannot communicate directly with each other (since, by definition, they don&apos;t understand each &amp;quot;language&amp;quot;), Jackendoff further proposes the existence of modules— specialized components of the mind that translate between relevant aspects of two 1 The book thus relates to another thread of computational linguistic research—that of modularity in Government-Binding parsing (e.g., Crocker [1992], Merlo [1996], among many others). Jackendoff&apos;s sense of modularity differs in that it applies across linguistic subsystems, and across cognitive more generally, rather than syntactic component of grammar. 652 Book Reviews or more cognitive subsystems. Only the outputs of the representational modules are accessible to the interface module that translates between them. The characterization of the interface module within the linguistic faculty, rather than the representational modules themselves, is the focus of Jackendoff&apos;s present study. As a starting point, Jackendoff assumes that an interface module consists of a set of correspondence rules between representational formats, which may be specified as obligatory optional, or default. The book presents and reviews a number of linguistic phenomena and analyses to explore the relationship between the representational modules of grammar (phonology / syntax / semantics), thereby determining the necessary properties of the correspondence rules that relate them. Jackendoff shows that the correspondences mediated by the interface module are complex, partial, and nonderivational. He further concludes that the lexicon, rather than occurring as a separate representational module, is an important component of this interface, playing a crucial role in linking together and licensing structures across the submodules of the linguistic system. Jackendoff&apos;s general argument about the nature and role of the lexicon can be briefly summarized as follows. Representational Modularity demands that the language faculty must be split into three modules—phonology, syntax, and semantics— each of which manipulates structures that are stated in its own representational format. Lexical items, which include phonological, syntactic, and semantic content, cannot be a whole a structure in any one of these modules, since doing so would violate Representational Modularity. Instead, a lexical item is a set of three structures (phonological, syntactic, and semantic) that are linked by correspondence rules. Each component of the lexical item is unified with the output structure of the appropriate representational module, and the correspondence rules ensure that the correct linkages between the pieces of a lexical item are enforced. The lexical items thus are crucial both in licensing and linking the outputs of the individual representational modules. Since each lexical item makes reference to phonological, syntactic, and semantic structures, the lexicon must be conceived of as a part of the linguistic interface module, rather than as a representational module itself. Simply put, the claim is that what we call the lexicon is not a distinct entity but rather a subset of the interface relations between the three grammatical subsystems. This formulation leads to some advantages for how we factor information among the components of grammar. For example, note that in addition to word-level linkages, the interface module must also include correspondence rules that relate higher-level phrase structures, as well as lower-level (intraword) morphological structures. Since lexical items are part of this general interface, there is then no need to restrict them to word-sized elements—they can be affixes, single words, compound words, or even whole constructions. Jackendoff&apos;s proposal thus has the potential to provide a uniform characterization of morphological, lexical, and phrase-level knowledge and processes, within a highly lexicalized framework. To see whether this potential is realizable, some important aspects of the approach must be fleshed out. One consequence of Jackendoff&apos;s proposal is that much of the work of linguistic theory is accomplished by the correspondence rules that relate the various levels of representation. Thus, the interface module that encapsulates these rules is a source of tremendous formal power—at least in addition to, if not instead of, the representational modules encoding the traditional components of grammar. Yet, while representational modules are explicitly constrained (to manipulate only a single representational format), the formal restrictions on interface modules are not made precise in the book. For example, Jackendoff asserts that some aspects of the output structures of the representational modules are &amp;quot;visible&amp;quot; to the interface and some are 653 Computational Linguistics Volume 24, Number 4 and furthermore that some aspects must be visible to a the rules and not to others—in neither case providing a formal characterization of how this distinction is made. The lack of restrictions on the linguistic interface module—the content of which is mediated only by what Jackendoff calls a balancing of power among relevant that it isn&apos;t clear what into the interface. If there are no restrictions on the correspondence rules in terms of what information—and sets of information across modules—they have access to, then there&apos;s no modularity left at all—the interface module is completely &amp;quot;interactive.&amp;quot; Given that so much grammatical knowledge is being pushed into the interface module, this kind of issue will be crucial to a final evaluation of Jackendoff&apos;s proposal. For the most part, the book is extremely well-written, and full of fascinating examples and analyses of phenomena crossing the boundaries of phonology syntax, and semantics. Its breadth of scope means that the technical details of specific examples might not have the depth or sophistication that would satisfy an expert in that area, but the intent to examine the boundaries—both between subcomponents within the language faculty as well as between the language faculty and other cognitive systems—leads to interesting insights and proposals on topics that are often neglected in more-narrowly focused research. For example, although highly speculative, the epilogue on the implications of Representational Modularity for the relations between language and thought, and between attention and consciousness, is highly thought-provoking. I found the book enjoyable and engaging, and believe that it will appeal to computational linguists with an interest in linguistic theory and cognitive science, and especially the relation between the two. One aspect of the book that I think may detract from the enjoyment of some readers is the (in my view, unwarranted) focus on comparisons to assumptions and proposals of Chomsky. While this perspective may make sense for an audience of theoretical syntacticians in the Chomskyan tradition, the intent of the book to situate linguistics within the broader cognitive sciences makes this emphasis a bit baffling. By contrast, more discussion could have been devoted to exploring in detail the relation to more similar frameworks (such as HPSG, LFG, and Optimality Theory), in which the assumption of multiple levels of representations and (nonderivational) correspondences between them is not new. I suspect that computational linguists will find much to admire in Jackendoff&apos;s proposed widening of the perspective within theoretical linguistics, and in his concern with psychological and computational issues, but will be puzzled by the perceived need for detailed arguments against such Chomskyan assumptions as syntactocentrism and the emphasis on derivational relations.</abstract>
<title confidence="0.775398">References</title>
<author confidence="0.767054">Logical Model of</author>
<title confidence="0.768732">Competence and Performance in the Human</title>
<author confidence="0.753073">dissertation</author>
<affiliation confidence="0.99641">University of Edinburgh.</affiliation>
<address confidence="0.959337">Bonnie. 1993. Translation: A</address>
<affiliation confidence="0.731727">from the Lexicon. MIT Press,</affiliation>
<address confidence="0.899545">Cambridge, MA.</address>
<note confidence="0.931212416666667">Jane. 1990. Structure. The MIT Press, Cambridge, MA. Ray. 1983. and MIT Press, Cambridge, MA. MacDonald, Maryellen, Neal Pearlmutter, and Mark Seidenberg. 1994. The lexical nature of syntactic ambiguity resolution. Review, Paola. 1996. with Principles Classes of Information. Academic Publishers, Dordrecht. Bangalore. 1997. of</note>
<affiliation confidence="0.697229">Lexical Descriptions and its Relevance to dissertation, Department of Computer and Information Science, University of Pennsylvania. (Available as technical report 97-10, Institute for Research in Cognitive Science, University</affiliation>
<address confidence="0.5505355">of Pennsylvania.) 654</address>
<title confidence="0.556949">Book Reviews</title>
<abstract confidence="0.80608275">Stevenson an assistant professor in the Department of Computer Science, and in the Center for Cognitive Science (RuCCS), at Rutgers University. Her research is in the area of computational models of human syntactic processing, focusing on the relation of lexical and syntactic theories to cognitively motivated parsing mechanisms. Stevenson&apos;s address is</abstract>
<affiliation confidence="0.941092">Department of Computer Science, Rutgers University, CoRE Building, Busch Campus, New</affiliation>
<address confidence="0.7587155">Brunswick, NJ 08903; e-mail: suzanne@cs.rutgers.edu 655</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
</authors>
<title>A Logical Model of Competence and Performance in the Human Sentence Processor. Doctoral dissertation,</title>
<date>1992</date>
<institution>University of Edinburgh.</institution>
<marker>Crocker, 1992</marker>
<rawString>Crocker, Matthew W. 1992. A Logical Model of Competence and Performance in the Human Sentence Processor. Doctoral dissertation, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>Machine Translation: A View from the Lexicon.</title>
<date>1993</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1406" citStr="Dorr 1993" startWordPosition="188" endWordPosition="189">al relationships among words (e.g., Srinivas 1997), and to relate lexical and syntactic processing within a cognitive theory of language (e.g., MacDonald, Pearlmutter, and Seidenberg 1994). The convergence of this focus on the lexicon across diverse fields of linguistic study has led to a tremendous potential for cross-disciplinary fertilization, to which Jackendoff is no stranger. Many computational linguists will be familiar with his past work on lexical-conceptual structure (e.g., Jackendoff 1983), which has been influential in computational lexical semantics and machine translation (e.g., Dorr 1993). Within this context, Jackendoff&apos;s new book, The Architecture of the Language Faculty, offers a new path to the conclusion of the centrality of the lexicon within a broad theory of human language, in which his notion of lexical structure is extended both in scope and power. While focusing on arguments from theoretical linguistics to support his views, Jackendoff explores the psycholinguistic and computational implications for many of his conclusions. The book addresses issues ranging from expletive infixation to the nature of consciousness, but here I&apos;ll focus on the lexical aspects of the pr</context>
</contexts>
<marker>Dorr, 1993</marker>
<rawString>Dorr, Bonnie. 1993. Machine Translation: A View from the Lexicon. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grirnshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Grirnshaw, 1990</marker>
<rawString>Grirnshaw, Jane. 1990. Argument Structure. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1301" citStr="Jackendoff 1983" startWordPosition="174" endWordPosition="175">ic principles with individual lexical information (e.g., Grimshaw 1990), to capture the syntactic and statistical relationships among words (e.g., Srinivas 1997), and to relate lexical and syntactic processing within a cognitive theory of language (e.g., MacDonald, Pearlmutter, and Seidenberg 1994). The convergence of this focus on the lexicon across diverse fields of linguistic study has led to a tremendous potential for cross-disciplinary fertilization, to which Jackendoff is no stranger. Many computational linguists will be familiar with his past work on lexical-conceptual structure (e.g., Jackendoff 1983), which has been influential in computational lexical semantics and machine translation (e.g., Dorr 1993). Within this context, Jackendoff&apos;s new book, The Architecture of the Language Faculty, offers a new path to the conclusion of the centrality of the lexicon within a broad theory of human language, in which his notion of lexical structure is extended both in scope and power. While focusing on arguments from theoretical linguistics to support his views, Jackendoff explores the psycholinguistic and computational implications for many of his conclusions. The book addresses issues ranging from </context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray. 1983. Semantics and Cognition. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryellen MacDonald</author>
<author>Neal Pearlmutter</author>
<author>Mark Seidenberg</author>
</authors>
<title>The lexical nature of syntactic ambiguity resolution.</title>
<date>1994</date>
<journal>Psychological Review,</journal>
<pages>101--4</pages>
<marker>MacDonald, Pearlmutter, Seidenberg, 1994</marker>
<rawString>MacDonald, Maryellen, Neal Pearlmutter, and Mark Seidenberg. 1994. The lexical nature of syntactic ambiguity resolution. Psychological Review, 101(4):676-703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
</authors>
<title>Parsing with Principles and Classes of Information.</title>
<date>1996</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>Merlo, 1996</marker>
<rawString>Merlo, Paola. 1996. Parsing with Principles and Classes of Information. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangalore Srinivas</author>
</authors>
<title>Complexity of Lexical Descriptions and its Relevance to Parsing. Doctoral dissertation,</title>
<date>1997</date>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<note>Available as technical report 97-10,</note>
<contexts>
<context position="846" citStr="Srinivas 1997" startWordPosition="110" endWordPosition="111">xvi+262 pp; paperbound, ISBN 0-262-60025-0, $20.00 Reviewed by Suzanne Stevenson Rutgers University Much current research on natural language emphasizes the critical contribution of lexical information to the definition and computation of linguistic structures. Motivations for the focus on the lexicon cross the fields of theoretical linguistics, computational linguistics, and psycholinguistics—for example, to explain the interaction of broad syntactic principles with individual lexical information (e.g., Grimshaw 1990), to capture the syntactic and statistical relationships among words (e.g., Srinivas 1997), and to relate lexical and syntactic processing within a cognitive theory of language (e.g., MacDonald, Pearlmutter, and Seidenberg 1994). The convergence of this focus on the lexicon across diverse fields of linguistic study has led to a tremendous potential for cross-disciplinary fertilization, to which Jackendoff is no stranger. Many computational linguists will be familiar with his past work on lexical-conceptual structure (e.g., Jackendoff 1983), which has been influential in computational lexical semantics and machine translation (e.g., Dorr 1993). Within this context, Jackendoff&apos;s new </context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>Srinivas, Bangalore. 1997. Complexity of Lexical Descriptions and its Relevance to Parsing. Doctoral dissertation, Department of Computer and Information Science, University of Pennsylvania. (Available as technical report 97-10, Institute for Research in Cognitive Science, University of Pennsylvania.)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>