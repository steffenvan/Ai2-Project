<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978402">
Transition-based Dependency Parsing with Selectional Branching
</title>
<author confidence="0.988794">
Jinho D. Choi
</author>
<affiliation confidence="0.831417">
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
</affiliation>
<email confidence="0.996006">
jdchoi@cs.umass.edu
</email>
<author confidence="0.995648">
Andrew McCallum
</author>
<affiliation confidence="0.831111333333333">
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
</affiliation>
<email confidence="0.998135">
mccallum@cs.umass.edu
</email>
<sectionHeader confidence="0.994788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847210526316">
We present a novel approach, called selec-
tional branching, which uses confidence es-
timates to decide when to employ a beam,
providing the accuracy of beam search at
speeds close to a greedy transition-based
dependency parsing approach. Selectional
branching is guaranteed to perform a fewer
number of transitions than beam search yet
performs as accurately. We also present a
new transition-based dependency parsing
algorithm that gives a complexity of O(n)
for projective parsing and an expected lin-
ear time speed for non-projective parsing.
With the standard setup, our parser shows
an unlabeled attachment score of 92.96%
and a parsing speed of 9 milliseconds per
sentence, which is faster and more accurate
than the current state-of-the-art transition-
based parser that uses beam search.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999251125">
Transition-based dependency parsing has gained
considerable interest because it runs fast and per-
forms accurately. Transition-based parsing gives
complexities as low as O(n) and O(n2) for projec-
tive and non-projective parsing, respectively (Nivre,
2008).1 The complexity is lower for projective pars-
ing because a parser can deterministically skip to-
kens violating projectivity, while this property is
not assumed for non-projective parsing. Nonethe-
less, it is possible to perform non-projective parsing
in expected linear time because the amount of non-
projective dependencies is notably smaller (Nivre
and Nilsson, 2005) so a parser can assume projec-
tivity for most cases while recognizing ones for
which projectivity should not be assumed (Nivre,
2009; Choi and Palmer, 2011).
</bodyText>
<footnote confidence="0.835445">
1We refer parsing approaches that produce only projective
dependency trees as projective parsing and both projective and
non-projective dependency trees as non-projective parsing.
</footnote>
<bodyText confidence="0.999821677419355">
Greedy transition-based dependency parsing has
been widely deployed because of its speed (Cer et
al., 2010); however, state-of-the-art accuracies have
been achieved by globally optimized parsers using
beam search (Zhang and Clark, 2008; Huang and
Sagae, 2010; Zhang and Nivre, 2011; Bohnet and
Nivre, 2012). These approaches generate multiple
transition sequences given a sentence, and pick one
with the highest confidence. Coupled with dynamic
programming, transition-based dependency parsing
with beam search can be done very efficiently and
gives significant improvement to parsing accuracy.
One downside of beam search is that it always
uses a fixed size of beam even when a smaller size
of beam is sufficient for good results. In our exper-
iments, a greedy parser performs as accurately as a
parser that uses beam search for about 64% of time.
Thus, it is preferred if the beam size is not fixed but
proportional to the number of low confidence pre-
dictions that a greedy parser makes, in which case,
fewer transition sequences need to be explored to
produce the same or similar parse output.
We first present a new transition-based parsing
algorithm that gives a complexity of O(n) for pro-
jective parsing and an expected linear time speed
for non-projective parsing. We then introduce se-
lectional branching that uses confidence estimates
to decide when to employ a beam. With our new ap-
proach, we achieve a higher parsing accuracy than
the current state-of-the-art transition-based parser
that uses beam search and a much faster speed.
</bodyText>
<sectionHeader confidence="0.662156" genericHeader="method">
2 Transition-based dependency parsing
</sectionHeader>
<bodyText confidence="0.99994625">
We introduce a transition-based dependency pars-
ing algorithm that is a hybrid between Nivre’s arc-
eager and list-based algorithms (Nivre, 2003; Nivre,
2008). Nivre’s arc-eager is a projective parsing al-
gorithm showing a complexity of O(n). Nivre’s
list-based algorithm is a non-projective parsing al-
gorithm showing a complexity of O(n2). Table 1
shows transitions in our algorithm. The top 4 and
</bodyText>
<page confidence="0.966004">
1052
</page>
<note confidence="0.9383665">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.99810275">
Transition Current state ⇒ Resulting state
LEFTl-REDUCE ([σ|i],δ,[j|β],A) ⇒ (σ,δ,[j|β],A∪{i←lj})
RIGHTl-SHIFT ([σ|i],δ,[j|β],A) ⇒ ([σ|i|δ|j],[],β,A∪{i→ l j})
NO-SHIFT ( [σ|i], δ, [j|β], A ) ⇒ ( [σ|i|δ|j], [ ], β, A )
NO-REDUCE ( [σ|i], δ, [j|β], A ) ⇒ ( σ, δ, [j|β], A )
LEFTl-PASS ([σ|i],δ,[j|β],A) ⇒ (σ,[i|δ],[j|β],A∪{i←lj})
RIGHTl-PASS ([σ|i],δ,[j|β],A) ⇒ (σ,[i|δ],[j|β],A∪{i→lj})
NO-PASS ( [σ|i], δ, [j|β], A ) ⇒ ( σ, [i|δ], [j|β], A )
</table>
<tableCaption confidence="0.860236">
Table 1: Transitions in our dependency parsing algorithm.
Transition Preconditions
Table 2: Preconditions of the transitions in Table 1 (* is a wildcard representing any transition).
</tableCaption>
<equation confidence="0.624843111111111">
LEFTl-∗
RIGHTl-∗
∗-SHIFT
∗-REDUCE
[i =6 0] ∧ ¬[∃k. (i ← k) ∈ A] ∧ ¬[(i →∗ j) ∈ A]
¬[∃k. (k → j) ∈ A] ∧ ¬[(i ←
∗ j) ∈ A]
¬[∃k ∈ σ. (k =6 i) ∧ ((k ← j) ∨ (k → j))]
[∃h. (h → i) ∈ A] ∧ ¬[∃k ∈ β. (i → k)]
</equation>
<bodyText confidence="0.993662433333333">
the bottom 3 transitions are inherited from Nivre’s
arc-eager and list-based algorithms, respectively.2
Each parsing state is represented as a tuple (a,
S, 0, A), where a is a stack containing processed
tokens, S is a deque containing tokens popped out
of a but will be pushed back into a in later parsing
states to handle non-projectivity, and 0 is a buffer
containing unprocessed tokens. A is a set of labeled
arcs. (i, j) represent indices of their corresponding
tokens (wi, wj), l is a dependency label, and the 0
identifier corresponds to w0, introduced as the root
of a tree. The initial state is ([0], [ ], [1, ... , n], 0),
and the final state is (a, S, [ ], A). At any parsing
state, a decision is made by comparing the top of
a, wi, and the first element of 0, wj. This decision
is consulted by gold-standard trees during training
and a classifier during decoding.
LEFTl-* and RIGHTl-* are performed when wj
is the head of wi with a dependency label l, and vice
versa. After LEFTl-* or RIGHTl-*, an arc is added
to A. NO-* is performed when no dependency is
found for wi and wj. *-SHIFT is performed when
no dependency is found for wj and any token in
a other than wi. After *-SHIFT, all tokens in S
as well as wj are pushed into a. *-REDUCE is
performed when wi already has the head, and wi is
not the head of any token in 0. After *-REDUCE,
wi is popped out of a. *-PASS is performed when
neither *-SHIFT nor *-REDUCE can be performed.
After *-PASS, wi is moved to the front of S so it
</bodyText>
<footnote confidence="0.97102075">
2The parsing complexity of a transition-based dependency
parsing algorithm is determined by the number of transitions
performed with respect to the number of tokens in a sentence,
say n (Kübler et al., 2009).
</footnote>
<bodyText confidence="0.9968458125">
can be compared to other tokens in 0 later. Each
transition needs to satisfy certain preconditions to
ensure the properties of a well-formed dependency
graph (Nivre, 2008); they are described in Table 2.
(i � j) and (i �
∗ j) indicate that wj is the head
and an ancestor of wi with any label, respectively.
When a parser is trained on only projective trees,
our algorithm learns only the top 4 transitions and
produces only projective trees during decoding. In
this case, it performs at most 2n − 1 transitions
per sentence so the complexity is O(n). When a
parser is trained on a mixture of projective and non-
projective trees, our algorithm learns all transitions
and produces both kinds of trees during decoding.
In this case, it performs at most n(n+1)
</bodyText>
<sectionHeader confidence="0.799661" genericHeader="method">
2 transitions
</sectionHeader>
<bodyText confidence="0.9999178">
so the complexity is O(n2). However, because of
the presence of *-SHIFT and *-REDUCE, our al-
gorithm is capable of skipping or removing tokens
during non-projective parsing, which allows it to
show a linear time parsing speed in practice.
</bodyText>
<figure confidence="0.954265555555556">
130
100
80
60
40
20
0
0 10 20 30 40 50 60 70
Sentence length
</figure>
<figureCaption confidence="0.998101">
Figure 1: The # of transitions performed during
training with respect to sentence lengths for Dutch.
</figureCaption>
<figure confidence="0.76583095">
Transitions
1053
Transition Q δ β A
0 Initialization [0] [ ] [1|0] 0
1 NO-SHIFT [a|1] [ ] [2|0]
2 NO-SHIFT [a|2] [ ] [3|0]
3 NO-SHIFT [a|3] [ ] [4|0]
4 LEFT-REDUCE [a|2] [ ] [4|0] A U {3 +-NSUBJ− 4}
5 NO-PASS [a|1] [2] [4|0]
6 RIGHT-SHIFT [a|4] [ ] [5|0] A U {1 −RCMOD-+ 4}
7 NO-SHIFT [a|5] [ ] [6|0]
8 LEFT-REDUCE [a|4] [ ] [6|0] A U {5 +-AUX− 6}
9 RIGHT-PASS [a|2] [4] [6|0] A U {4 −XCOMP-+ 6}
10 LEFT-REDUCE [a|1] [4] [6|0] A U {2 +-DOBJ− 6}
11 NO-SHIFT [a|6] [ ] [7|0]
12 NO-REDUCE [a|4] [ ] [7|0]
13 NO-REDUCE [a|1] [ ] [7|0]
14 LEFT-REDUCE [0] [ ] [7|0] A U {1 +-NSUBJ− 7}
15 RIGHT-SHIFT [a|7] [ ] [8] A U {0 −ROOT-+ 7}
16 RIGHT-SHIFT [a|8] [ ] [ ] A U {7 −ADV-+ 8}
</figure>
<tableCaption confidence="0.978409">
Table 3: A transition sequence generated by our parsing algorithm using gold-standard decisions.
</tableCaption>
<bodyText confidence="0.998947809523809">
Figure 1 shows the total number of transitions per-
formed during training with respect to sentence
lengths for Dutch. Among all languages distributed
by the CoNLL-X shared task (Buchholz and Marsi,
2006), Dutch consists of the highest number of
non-projective dependencies (5.4% in arcs, 36.4%
in trees). Even with such a high number of non-
projective dependencies, our parsing algorithm still
shows a linear growth in transitions.
Table 3 shows a transition sequence generated
by our parsing algorithm using gold-standard deci-
sions. After w3 and w4 are compared, w3 is popped
out of Q (state 4) so it is not compared to any other
token in 0 (states 9 and 13). After w2 and w4 are
compared, w2 is moved to δ (state 5) so it can be
compared to other tokens in 0 (state 10). After w4
and w6 are compared, RIGHT-PASS is performed
(state 9) because there is a dependency between
w6 and w2 in Q (state 10). After w6 and w7 are
compared, w6 is popped out of Q (state 12) because
it is not needed for later parsing states.
</bodyText>
<sectionHeader confidence="0.915405" genericHeader="method">
3 Selectional branching
</sectionHeader>
<subsectionHeader confidence="0.998106">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999806681818182">
For transition-based parsing, state-of-the-art accu-
racies have been achieved by parsers optimized on
multiple transition sequences using beam search,
which can be done very efficiently when it is cou-
pled with dynamic programming (Zhang and Clark,
2008; Huang and Sagae, 2010; Zhang and Nivre,
2011; Huang et al., 2012; Bohnet and Nivre, 2012).
Despite all the benefits, there is one downside of
this approach; it generates a fixed number of tran-
sition sequences no matter how confident the one-
best sequence is.3 If every prediction leading to
the one-best sequence is confident, it may not be
necessary to explore more sequences to get the best
output. Thus, it is preferred if the beam size is not
fixed but proportional to the number of low confi-
dence predictions made for the one-best sequence.
The selectional branching method presented here
performs at most d · t − e transitions, where t is the
maximum number of transitions performed to gen-
erate a transition sequence, d = min(b, |λ|+1), bis
the beam size, |λ |is the number of low confidence
predictions made for the one-best sequence, and
</bodyText>
<equation confidence="0.9920355">
d(d−1)
e = 2 . Compared to beam search that always
</equation>
<bodyText confidence="0.9925734">
performs b · t transitions, selectional branching is
guaranteed to perform fewer transitions given the
same beam size because d &lt; b and e &gt; 0 except for
d = 1, in which case, no branching happens. With
selectional branching, our parser shows slightly
</bodyText>
<footnote confidence="0.995538">
3The ‘one-best sequence’ is a transition sequence gener-
ated by taking only the best prediction at each parsing state.
</footnote>
<page confidence="0.99722">
1054
</page>
<bodyText confidence="0.999021">
higher parsing accuracy than the current state-of-
the-art transition-based parser using beam search,
and performs about 3 times faster.
</bodyText>
<subsectionHeader confidence="0.999972">
3.2 Branching strategy
</subsectionHeader>
<bodyText confidence="0.99774925">
Figure 2 shows an overview of our branching strat-
egy. sij represents a parsing state, where i is the
index of the current transition sequence and j is
the index of the current parsing state (e.g., s12 rep-
resents the 2nd parsing state in the 1st transition
sequence). pkj represents the k’th best prediction
(in our case, it is a predicted transition) given s1j
(e.g., p21 is the 2nd-best prediction given s11).
</bodyText>
<figureCaption confidence="0.933786">
Figure 2: An overview of our branching strategy.
Each sequence Ti&gt;1 branches from T1.
</figureCaption>
<bodyText confidence="0.979766888888889">
Initially, the one-best sequence T1 = [s11, ... , s1t]
is generated by a greedy parser. While generating
T1, the parser adds tuples (s1j,p2j), ... , (s1j,pkj)
to a list A for each low confidence prediction p1j
given s1j.4 Then, new transition sequences are gen-
erated by using the b highest scoring predictions in
A, where b is the beam size. If |A |&lt; b, all predic-
tions in A are used. The same greedy parser is used
to generate these new sequences although it now
starts with s1j instead of an initial parsing state,
applies pkj to s1j, and performs further transitions.
Once all transition sequences are generated, a parse
tree is built from a sequence with the highest score.
For our experiments, we set k = 2, which gave
noticeably more accurate results than k = 1. We
also experimented with k &gt; 2, which did not show
significant improvement over k = 2. Note that as-
signing a greater k may increase |A |but not the total
number of transition sequences generated, which
is restricted by the beam size, b. Since each se-
quence Ti&gt;1 branches from T1, selectional branch-
ing performs fewer transitions than beam search:
at least d(d�1)
2 transitions are inherited from T1,
4λ is initially empty, which is hidden in Figure 2.
where d = min(b, |A |+ 1); thus, it performs that
many transitions less than beam search (see the
left lower triangle in Figure 2). Furthermore, se-
lectional branching generates a d number of se-
quences, where d is proportional to the number of
low confidence predictions made by T1. To sum up,
selectional branching generates the same or fewer
transition sequences than beam search and each
sequence Ti&gt;1 performs fewer transitions than T1;
thus, it performs faster than beam search in general
given the same beam size.
</bodyText>
<subsectionHeader confidence="0.999844">
3.3 Finding low confidence predictions
</subsectionHeader>
<bodyText confidence="0.999942875">
For each parsing state sij, a prediction is made by
generating a feature vector xij E X, feeding it into
a classifier C1 that uses a feature map Φ(x, y) and
a weight vector w to measure a score for each label
y E Y, and choosing a label with the highest score.
When there is a tie between labels with the highest
score, the first one is chosen. This can be expressed
as a logistic regression:
</bodyText>
<equation confidence="0.999239">
C1(x) = arg maYx{ f (x, y) }
yE
exp(w · Φ(x, y))
f(x, y) =
Ey�EY exp(w · Φ(x, y&apos;))
</equation>
<bodyText confidence="0.999761285714286">
To find low confidence predictions, we use the mar-
gins (score differences) between the best prediction
and the other predictions. If all margins are greater
than a threshold, the best prediction is considered
highly confident; otherwise, it is not. Given this
analogy, the k-best predictions can be found as
follows (m &gt; 0 is a margin threshold):
</bodyText>
<equation confidence="0.863692666666667">
Ck(x, m) = K arg max{f(x, y)yEY
}
s.t. f(x, C1(x)) − f(x, y) &lt; m
</equation>
<bodyText confidence="0.957218571428571">
‘K arg max’ returns a set of k&apos; labels whose mar-
gins to C1(x) are smaller than any other label’s
margin to C1(x) and also &lt; m, where k&apos; &lt; k.
When m = 0, it returns a set of the highest scoring
labels only, including C1(x). When m = 1, it re-
turns a set of all labels. Given this, a prediction is
considered not confident if |Ck(x, m) |&gt; 1.
</bodyText>
<subsectionHeader confidence="0.996473">
3.4 Finding the best transition sequence
</subsectionHeader>
<bodyText confidence="0.9998822">
Let Pi be a list of all predictions that lead to gen-
erate a transition sequence Ti. The predictions in
Pi are either inherited from T1 or made specifi-
cally for Ti. In Figure 2, P3 consists of p11 as its
first prediction, p22 as its second prediction, and
</bodyText>
<equation confidence="0.781740444444444">
T1 =
T2 =
T3 =
Td =
s11 s12
p21
p11
s22 ... ... s2t
p12 p1j
</equation>
<figure confidence="0.91941525">
... ... s1t
p22
s33 ... s3t
p2j
...
É
sdt
É
</figure>
<page confidence="0.973616">
1055
</page>
<bodyText confidence="0.999878">
further predictions made specifically for T3. The
score of each prediction is measured by f(x, y) in
Section 3.3. Then, the score of Ti is measured by
averaging scores of all predictions in Pi.
</bodyText>
<equation confidence="0.992724">
E
pEPi score(p)
|Pi|
</equation>
<bodyText confidence="0.999590125">
Unlike Zhang and Clark (2008), we take the av-
erage instead of the sum of all prediction scores.
This is because our algorithm does not guarantee
the same number of transitions for every sequence,
so the sum of all scores would weigh more on se-
quences with more transitions. We experimented
with both the sum and the average, and taking the
average led to slightly higher parsing accuracy.
</bodyText>
<subsectionHeader confidence="0.996374">
3.5 Bootstrapping transition sequences
</subsectionHeader>
<bodyText confidence="0.9999115">
During training, a training instance is generated
for each parsing state sij by taking a feature vec-
tor xij and its true label yij. To generate multiple
transition sequences during training, the bootstrap-
ping technique of Choi and Palmer (2011) is used,
which is described in Algorithm 1.5
</bodyText>
<construct confidence="0.582915666666667">
Algorithm 1 Bootstrapping
Input: Dt: training set, Dd: development set.
Output: A model M.
</construct>
<listItem confidence="0.9260832">
1: r + -0
2: I +- getTrainingInstances(Dt)
3: M0 +- buildModel(I)
4: S0 +- getScore(Dd, M0)
5: while (r = 0) or (Sr_1 &lt; Sr) do
6: r +- r + 1
7: I +- getTrainingInstances(Dt, Mr_1)
8: Mr +- buildModel(I)
9: Sr +- getScore(Dd, Mr)
10: return Mr_1
</listItem>
<bodyText confidence="0.9969885">
First, an initial model M0 is trained on all data by
taking the one-best sequences, and its score is mea-
sured by testing on a development set (lines 2-4).
Then, the next model Mr is trained on all data but
this time, Mr−1 is used to generate multiple tran-
sition sequences (line 7-8). Among all transition
sequences generated by Mr−1, training instances
from only T1 and Tg are used to train Mr, where T1
is the one-best sequence and Tg is a sequence giv-
ing the most accurate parse output compared to the
gold-standard tree. The score of Mr is measured
(line 9), and repeat the procedure if Sr−1 &lt; Sr;
otherwise, return the previous model Mr−1.
5Alternatively, the dynamic oracle approach of Goldberg
and Nivre (2012) can be used to generate multiple transition
sequences, which is expected to show similar results.
</bodyText>
<subsectionHeader confidence="0.988933">
3.6 Adaptive subgradient algorithm
</subsectionHeader>
<bodyText confidence="0.999648625">
To build each model during bootstrapping, we use
a stochastic adaptive subgradient algorithm called
ADAGRAD that uses per-coordinate learning rates
to exploit rarely seen features while remaining scal-
able (Duchi et al., 2011).This is suitable for NLP
tasks where rarely seen features often play an im-
portant role and training data consists of a large
number of instances with high dimensional features.
Algorithm 2 shows our adaptation of ADAGRAD
with logistic regression for multi-class classifica-
tion. Note that when used with logistic regression,
ADAGRAD takes a regular gradient instead of a sub-
gradient method for updating weights. For our ex-
periments, ADAGRAD slightly outperformed learn-
ing algorithms such as average perceptron (Collins,
2002) or Liblinear SVM (Hsieh et al., 2008).
</bodyText>
<construct confidence="0.7444372">
Algorithm 2 ADAGRAD + logistic regression
Input: D = {(xi, yi)}ni=1 s.t. xi E X, yi E Y
Φ(x, y) E Rd s.t. d = dimension(X) x |Y|
T: iterations, α: learning rate, p: ridge
Output: A weight vector w E Rd.
</construct>
<listItem confidence="0.994387">
1: w +- 0, where w E Rd
2: G +- 0, where G E Rd
3: for t +- 1 ... T do
4: for i +- 1 ... n do
5: QvyEY +- I(yi, y) − f(xi, y), s.t. Q E R|Y|
∂ +- �
6: yEY(Φ(xi, y) &apos; Qy)
7: G +- G + ∂ o ∂
8: for j +- 1 ... d do
9: wj +- wj + α &apos; 1
</listItem>
<bodyText confidence="0.971663611111111">
ρ+�/Gj &apos;∂j
The algorithm takes three hyper-parameters; T is
the number of iterations, α is the learning rate, and
ρ is the ridge (T &gt; 0, α &gt; 0, ρ ≥ 0). G is our run-
ning estimate of a diagonal covariance matrix for
the gradients (per-coordinate learning rates). For
each instance, scores for all labels are measured
by the logistic regression function f(x, y) in Sec-
tion 3.3. These scores are subtracted from an output
of the indicator function I(y, y&apos;), which forces our
model to keep learning this instance until the pre-
diction is 100% confident (in other words, until
the score of yi becomes 1). Then, a subgradient
is measured by taking all feature vectors together
weighted by Q (line 6). This subgradient is used to
update G and w, where o is the Hadamard product
(lines 7-9). ρ is a ridge term to keep the inverse
covariance well-conditioned.
</bodyText>
<equation confidence="0.991951333333333">
I(y, y&apos;) =
ise
�
1 y = y&apos;
0 otherw
score(Ti) =
</equation>
<page confidence="0.969589">
1056
</page>
<sectionHeader confidence="0.998633" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.966215">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.9999618">
For projective parsing experiments, the Penn En-
glish Treebank (Marcus et al., 1993) is used with
the standard split: sections 2-21 for training, 22 for
development, and 23 for evaluation. All constituent
trees are converted with the head-finding rules of
Yamada and Matsumoto (2003) and the labeling
rules of Nivre (2006). For non-projective pars-
ing experiments, four languages from the CoNLL-
X shared task are used: Danish, Dutch, Slovene,
and Swedish (Buchholz and Marsi, 2006). These
languages are selected because they contain non-
projective trees and are publicly available from the
CoNLL-X webpage.6 Since the CoNLL-X data we
have does not come with development sets, the last
10% of each training set is used for development.
</bodyText>
<subsectionHeader confidence="0.989034">
4.2 Feature engineering
</subsectionHeader>
<bodyText confidence="0.999976578947368">
For English, we mostly adapt features from Zhang
and Nivre (2011) who have shown state-of-the-art
parsing accuracy for transition-based dependency
parsing. Their distance features are not included
in our approach because they do not seem to show
meaningful improvement. Feature selection is done
on the English development set.
For the other languages, the same features are
used with the addition of morphological features
provided by CoNLL-X; specifically, morphological
features from the top of σ and the front of β are
added as unigram features. Moreover, all POS tag
features from English are duplicated with coarse-
grained POS tags provided by CoNLL-X. No more
feature engineering is done for these languages; it
is possible to achieve higher performance by using
different features, especially when these languages
contain non-projective dependencies whereas En-
glish does not, which we will explore in the future.
</bodyText>
<subsectionHeader confidence="0.996734">
4.3 Development
</subsectionHeader>
<bodyText confidence="0.990944777777778">
Several parameters need to be optimized during de-
velopment. For ADAGRAD, T, α, and p need to be
tuned (Section 3.6). For bootstrapping, the number
of iterations, say r, needs to be tuned (Section 3.5).
For selectional branching, the margin threshold m
and the beam size b need to be tuned (Section 3.3).
First, all parameters are tuned on the English devel-
opment set by using grid search on T = [1, ... ,10],
α = [0, 01, 0, 02], p = [0.1, 0.2], r = [1, 2, 3],
</bodyText>
<equation confidence="0.644036">
6http://ilk.uvt.nl/conll/
m = [0.83, ... , 0.92], and b = [16, 32, 64, 80].
</equation>
<bodyText confidence="0.9991145">
As a result, the following parameters are found:
α = 0.02, p = 0.1, m = 0.88, and b = 64180. For
this development set, the beam size of 64 and 80
gave the exact same result, so we kept the one with
</bodyText>
<figure confidence="0.996104818181818">
a larger beam size (b = 80).
91.2
91.16
91.12
b =
64|80
32
16
91
0.83 0.86 0.88 0.9 0.92
Margin
</figure>
<figureCaption confidence="0.974239">
Figure 3: Parsing accuracies with respect to mar-
gins and beam sizes on the English development set.
</figureCaption>
<figure confidence="0.528124">
b = 6480: the black solid line with solid circles,
b = 32: the blue dotted line with hollow circles,
b = 16: the red dotted line with solid circles.
</figure>
<bodyText confidence="0.954942">
Figure 3 shows parsing accuracies with respect to
different margins and beam sizes on the English de-
velopment set. These parameters need to be tuned
jointly because different margins prefer different
beam sizes. For instance, m = 0.85 gives the high-
est accuracy with b = 32, but m = 0.88 gives the
highest accuracy with b = 64180.
</bodyText>
<figure confidence="0.984170333333333">
UAS
91.5
LAS
90.5
90
89.5
89
0 2 4 6 8 10 12 14
Iteration
</figure>
<figureCaption confidence="0.99597">
Figure 4: Parsing accuracies with respect to ADA-
</figureCaption>
<bodyText confidence="0.969643222222222">
GRAD and bootstrap iterations on the English de-
velopment set when α = 0.02, p = 0.1, m = 0.88,
and b = 6480. UAS: unlabeled attachment score,
LAS: labeled attachment score.
Figure 4 shows parsing accuracies with respect to
ADAGRAD and bootstrap iterations on the English
development set. The range 1-5 shows results of
5 ADAGRAD iterations before bootstrapping, the
range 6-9 shows results of 4 iterations during the
</bodyText>
<figure confidence="0.976340428571429">
Accuracy
91.08
91.04
Accuracy
88.5
92
91
</figure>
<page confidence="0.993423">
1057
</page>
<bodyText confidence="0.99953675">
first bootstrapping, and the range 10-14 shows re-
sults of 5 iterations during the second bootstrap-
ping. Thus, the number of bootstrap iteration is
2 where each bootstrapping takes a different num-
ber of ADAGRAD iterations. Using an Intel Xeon
2.57GHz machine, it takes less than 40 minutes
to train the entire Penn Treebank, which includes
times for IO, feature extraction and bootstrapping.
</bodyText>
<figure confidence="0.998446888888889">
Transitions 1,200,000
1,000,000
800,000
600,000
400,000
200,000
0
0 10 20 30 40 50 60 70 80
Beam size = 1, 2, 4, 8, 16, 32, 64, 80
</figure>
<figureCaption confidence="0.996492">
Figure 5: The total number of transitions performed
</figureCaption>
<bodyText confidence="0.977995842105263">
during decoding with respect to beam sizes on the
English development set.
Figure 5 shows the total number of transitions per-
formed during decoding with respect to beam sizes
on the English development set (1,700 sentences,
40,117 tokens). With selectional branching, the
number of transitions grows logarithmically as the
beam size increases whereas it would have grown
linearly if beam search were used. We also checked
how often the one best sequence is chosen as the
final sequence during decoding. Out of 1,700 sen-
tences, the one best sequences are chosen for 1,095
sentences. This implies that about 64% of time,
our greedy parser performs as accurately as our
non-greedy parser using selectional branching.
For the other languages, we use the same values
as English for α, p, m, and b; only the ADAGRAD
and bootstrap iterations are tuned on the develop-
ment sets of the other languages.
</bodyText>
<subsectionHeader confidence="0.99798">
4.4 Projective parsing experiments
</subsectionHeader>
<bodyText confidence="0.999725705882353">
Before parsing, POS tags were assigned to the train-
ing set by using 20-way jackknifing. For the auto-
matic generation of POS tags, we used the domain-
specific model of Choi and Palmer (2012a)’s tagger,
which gave 97.5% accuracy on the English evalua-
tion set (0.2% higher than Collins (2002)’s tagger).
Table 4 shows comparison between past and cur-
rent state-of-the-art parsers and our approach. The
first block shows results from transition-based de-
pendency parsers using beam search. The second
block shows results from other kinds of parsing
approaches (e.g., graph-based parsing, ensemble
parsing, linear programming, dual decomposition).
The third block shows results from parsers using
external data. The last block shows results from
our approach. The Time column show how many
seconds per sentence each parser takes.7
</bodyText>
<table confidence="0.998446">
Approach UAS LAS Time
Zhang and Clark (2008) 92.1
Huang and Sagae (2010) 92.1 0.04
Zhang and Nivre (2011) 92.9 91.8 0.03
Bohnet and Nivre (2012) 93.38 92.44 0.4
McDonald et al. (2005) 90.9
Mcdonald and Pereira (2006) 91.5
Sagae and Lavie (2006) 92.7
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06 91.86
Martins et al. (2010) 93.26
Rush et al. (2010) 93.8
Koo et al. (2008) 93.16
Carreras et al. (2008) 93.54
Bohnet and Nivre (2012) 93.67 92.68
Suzuki et al. (2009) 93.79
bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009
bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008
bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006
bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004
bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003
bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002
bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002
</table>
<tableCaption confidence="0.984986">
Table 4: Parsing accuracies and speeds on the En-
</tableCaption>
<bodyText confidence="0.970138388888889">
glish evaluation set, excluding tokens containing
only punctuation. bt and bd indicate the beam sizes
used during training and decoding, respectively.
UAS: unlabeled attachment score, LAS: labeled
attachment score, Time: seconds per sentence.
For evaluation, we use the model trained with b =
80 and m = 0.88, which is the best setting found
during development. Our parser shows higher ac-
curacy than Zhang and Nivre (2011), which is
the current state-of-the-art transition-based parser
that uses beam search. Bohnet and Nivre (2012)’s
transition-based system jointly performs POS tag-
ging and dependency parsing, which shows higher
accuracy than ours. Our parser gives a comparative
accuracy to Koo and Collins (2010) that is a 3rd-
order graph-based parsing approach. In terms of
speed, our parser outperforms all other transition-
based parsers; it takes about 9 milliseconds per
</bodyText>
<footnote confidence="0.80908675">
7Dhillon et al. (2012) and Rush and Petrov (2012) also
have shown good results on this data but they are excluded
from our comparison because they use different kinds of
constituent-to-dependency conversion methods.
</footnote>
<page confidence="0.922969">
1058
</page>
<table confidence="0.9979334">
Approach Danish Dutch Slovene Swedish
LAS UAS LAS UAS LAS UAS LAS UAS
Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50
McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93
Nivre (2009) 84.2 - - - 75.2 - - -
F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30
Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66
Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80
bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12
bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36
</table>
<tableCaption confidence="0.999819">
Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation.
</tableCaption>
<bodyText confidence="0.999692785714286">
sentence using the beam size of 80. Our parser is
implemented in Java and tested on an Intel Xeon
2.57GHz. Note that we do not include input/output
time for our speed comparison.
For a proof of concept, we run the same model,
trained with bt = 80, but decode with different
beam sizes using the same margin. Surprisingly,
our parser gives the same accuracy (0.01% higher
for labeled attachment score) on this data even with
bd = 16. More importantly, bd = 16 shows about
the same parsing speed as bd = 80, which indicates
that selectional branching automatically reduced
down the beam size by estimating low confidence
predictions, so even if we assigned a larger beam
size for decoding, it would have performed as effi-
ciently. This implies that we no longer need to be
so conscious about the beam size during decoding.
Another interesting part is that (bt = 80, bd = 1)
shows higher accuracy than (bt = 1, bd = 1); this
implies that our training method of bootstrapping
transition sequences can improve even a greedy
parser. Notice that our greedy parser shows higher
accuracy than many other greedy parsers (Hall et
al., 2006; Goldberg and Elhadad, 2010) because
it uses the non-local features of Zhang and Nivre
(2011) and the bootstrapping technique of Choi
and Palmer (2011) that had not been used for most
other greedy parsing approaches.
</bodyText>
<subsectionHeader confidence="0.998197">
4.5 Non-projective parsing experiments
</subsectionHeader>
<bodyText confidence="0.999492625">
Table 5 shows comparison between state-of-the-art
parsers and our approach for four languages with
non-projective dependencies. Nivre et al. (2006)
uses a pseudo-projective transition-based parsing
approach. McDonald et al. (2006) uses a 2nd-order
maximum spanning tree approach. Nivre (2009)
and Fernández-González and Gómez-Rodríguez
(2012) use different non-projective transition-based
parsing approaches. Nivre and McDonald (2008)
uses an ensemble model between transition-based
and graph-based parsing approaches. Martins et
al. (2010) uses integer linear programming for the
optimization of their parsing model.
Some of these approaches use greedy parsers, so
we include our results from models using (bt = 80,
bd = 1, m = 0.88), which finds only the one-best
sequences during decoding although it is trained on
multiple transition sequences (see Section 4.4). Our
parser shows higher accuracies for most languages
except for unlabeled attachment scores in Danish
and Slovene. Our greedy approach outperforms
both Nivre (2009) and Fernández-González and
Gómez-Rodríguez (2012) who use different non-
projective parsing algorithms.
</bodyText>
<figure confidence="0.979194222222222">
130
100
80
60
40
20
0
0 10 20 30 40 50 60
Sentence length
</figure>
<figureCaption confidence="0.9728845">
Figure 6: The # of transitions performed during de-
coding with respect to sentence lengths for Dutch.
</figureCaption>
<bodyText confidence="0.82328725">
Figure 6 shows the number of transitions performed
during decoding with respect to sentence lengths
for Dutch using bd = 1. Our parser still shows a
linear growth in transition during decoding.
</bodyText>
<sectionHeader confidence="0.999859" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.94415575">
Our parsing algorithm is most similar to Choi and
Palmer (2011) who integrated our LEFT-REDUCE
transition into Nivre’s list-based algorithm. Our
algorithm is distinguished from theirs because ours
gives different parsing complexities of O(n) and
O(n2) for projective and non-projective parsing,
respectively, whereas their algorithm gives O(n2)
Transitions
</bodyText>
<page confidence="0.967275">
1059
</page>
<bodyText confidence="0.999958">
for both cases; this is possible because of the new
integration of the RIGHT-SHIFT and NO-REDUCE
transitions. There are other transition-based de-
pendency parsing algorithms that take a similar ap-
proach; Nivre (2009) integrated a SWAP transition
into Nivre’s arc-standard algorithm (Nivre, 2004)
and Fernández-González and Gómez-Rodríguez
(2012) integrated a buffer transition into Nivre’s
arc-eager algorithm to handle non-projectivity.
Our selectional branching method is most rele-
vant to Zhang and Clark (2008) who introduced
a transition-based dependency parsing model that
uses beam search. Huang and Sagae (2010) later
applied dynamic programming to this approach
and showed improved efficiency. Zhang and Nivre
(2011) added non-local features to this approach
and showed improved parsing accuracy. Bohnet
and Nivre (2012) introduced a transition-based sys-
tem that jointly performed POS tagging and de-
pendency parsing. Our work is distinguished from
theirs because we use selectional branching instead.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969428571429">
We present selectional branching that uses confi-
dence estimates to decide when to employ a beam.
Coupled with our new hybrid parsing algorithm,
ADAGRAD, rich non-local features, and bootstrap-
ping, our parser gives higher parsing accuracy than
most other transition-based dependency parsers in
multiple languages and shows faster parsing speed.
It is interesting to see that our greedy parser out-
performed most other greedy dependency parsers.
This is because our parser used both bootstrapping
and Zhang and Nivre (2011)’s non-local features,
which had not been used by other greedy parsers.
In the future, we will experiment with more ad-
vanced dependency representations (de Marneffe
and Manning, 2008; Choi and Palmer, 2012b) to
show robustness of our approach. Furthermore, we
will evaluate individual methods of our approach
separately to show impact of each method on pars-
ing performance. We also plan to implement the
typical beam search approach to make a direct com-
parison to our selectional branching.8
</bodyText>
<sectionHeader confidence="0.99815" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.916352">
Special thanks are due to Luke Vilnis of the Uni-
versity of Massachusetts Amherst for insights on
</bodyText>
<footnote confidence="0.9302705">
8Our parser is publicly available under an open source
project, ClearNLP (clearnlp.googlecode.com).
</footnote>
<bodyText confidence="0.7679975">
the ADAGRAD derivation. We gratefully acknowl-
edge a grant from the Defense Advanced Research
Projects Agency (DARPA) under the DEFT project,
solicitation #: DARPA-BAA-12-47.
</bodyText>
<sectionHeader confidence="0.967055" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997199145833333">
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP’12, pages 1455–1465.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL’06,
pages 149–164.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing. In Proceedings
of the 12th Conference on Computational Natural
Language Learning, CoNLL’08, pages 9–16.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC’10.
Jinho D. Choi and Martha Palmer. 2011. Getting the
Most out of Transition-based Dependency Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL:HLT’11, pages 687–
692.
Jinho D. Choi and Martha Palmer. 2012a. Fast and Ro-
bust Part-of-Speech Tagging Using Dynamic Model
Selection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
ACL’12, pages 363–367.
Jinho D. Choi and Martha Palmer. 2012b. Guidelines
for the Clear Style Constituent to Dependency Con-
version. Technical Report 01-12, University of Col-
orado Boulder.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP’02, pages 1–8.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
</reference>
<page confidence="0.645101">
1060
</page>
<reference confidence="0.995714625">
Paramveer S. Dhillon, Jordan Rodu, Michael Collins,
Dean P. Foster, and Lyle H. Ungar. 2012. Spectral
Dependency Parsing with Latent Variables. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP’12, pages
205–213.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12(39):2121–2159.
Daniel Fernández-González and Carlos Gómez-
Rodríguez. 2012. Improving Transition-Based
Dependency Parsing with Buffer Transitions. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP’12, pages 308–319.
Yoav Goldberg and Michael Elhadad. 2010. An Effi-
cient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT:NAACL’10, pages 742–750.
Yoav Goldberg and Joakim Nivre. 2012. A Dynamic
Oracle for Arc-Eager Dependency Parsing. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING’12.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, COLING-ACL’06, pages 316–
323.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
Dual Coordinate Descent Method for Large-scale
Linear SVM. In Proceedings of the 25th interna-
tional conference on Machine learning, ICML’08,
pages 408–415.
Liang Huang and Kenji Sagae. 2010. Dynamic Pro-
gramming for Linear-Time Incremental Parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL’10.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured Perceptron with Inexact Search. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT’12, pages 142–151.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL’10.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL:HLT’08,
pages 595–603.
Sandra Kübler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan &amp; Clay-
pool Publishers.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.
André F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Mário A. T. Figueiredo. 2010.
Turbo Parsers: Dependency Parsing by Approximate
Variational Inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP’10, pages 34–44.
Ryan Mcdonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proceedings of the Annual Meeting of the
European American Chapter of the Association for
Computational Linguistics, EACL’06, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91–98.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Analysis with a
Two-Stage Discriminative Parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, CoNLL’06, pages 216–220.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-based and Transition-based Dependency
Parsers. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, ACL:HLT’08,
pages 950–958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL’05, pages 99–106.
Joakim Nivre, Johan Hall, Jens Nilsson, Gül¸sen Eryiˇgit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vec-
tor machines. In Proceedings of the 10th Confer-
ence on Computational Natural Language Learning,
CoNLL’06, pages 221–225.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, IWPT’03, pages 149–160.
1061
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Proceedings of the ACL’04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50–57.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL-
IJCNLP’09, pages 351–359.
Alexander M. Rush and Slav Petrov. 2012. Vine Prun-
ing for Efficient Multi-Pass Dependency Parsing. In
Proceedings of the 12th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, NAACL:HLT’12.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On Dual Decomposi-
tion and Linear Programming Relaxations for Nat-
ural Language Processing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP’10, pages 1–11.
Kenji Sagae and Alon Lavie. 2006. Parser Combina-
tion by Reparsing. In In Proceedings of the Human
Language Technology Conference of the NAACL,
NAACL’06, pages 129–132.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP’09, pages 551–560.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chine. In Proceedings of the 8th International Work-
shop on Parsing Technologies, IWPT’03, pages 195–
206.
Yue Zhang and Stephen Clark. 2008. A Tale of
Two Parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP’08, pages 562–571.
Hao Zhang and Ryan McDonald. 2012. Generalized
Higher-Order Dependency Parsing with Cube Prun-
ing. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL’12, pages 320–331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL’11, pages 188–193.
</reference>
<page confidence="0.980706">
1062
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959926">
<title confidence="0.999057">Transition-based Dependency Parsing with Selectional Branching</title>
<author confidence="0.993818">D Jinho</author>
<affiliation confidence="0.999974">Department of Computer University of Massachusetts</affiliation>
<address confidence="0.999818">Amherst, MA, 01003,</address>
<email confidence="0.999883">jdchoi@cs.umass.edu</email>
<author confidence="0.986853">Andrew</author>
<affiliation confidence="0.9999715">Department of Computer University of Massachusetts</affiliation>
<address confidence="0.999335">Amherst, MA, 01003,</address>
<email confidence="0.999886">mccallum@cs.umass.edu</email>
<abstract confidence="0.9990274">We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing that gives a complexity of for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP’12,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="2396" citStr="Bohnet and Nivre, 2012" startWordPosition="340" endWordPosition="343">me projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about 64% of time. Thus, it is preferred if the beam size is not fixed but pr</context>
<context position="10122" citStr="Bohnet and Nivre, 2012" startWordPosition="1710" endWordPosition="1713">fter w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in Q (state 10). After w6 and w7 are compared, w6 is popped out of Q (state 12) because it is not needed for later parsing states. 3 Selectional branching 3.1 Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is the maximum number of transitions performed t</context>
<context position="25779" citStr="Bohnet and Nivre (2012)" startWordPosition="4483" endWordPosition="4486">rrent state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.</context>
<context position="27099" citStr="Bohnet and Nivre (2012)" startWordPosition="4742" endWordPosition="4745">bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding tokens containing only punctuation. bt and bd indicate the beam sizes used during training and decoding, respectively. UAS: unlabeled attachment score, LAS: labeled attachment score, Time: seconds per sentence. For evaluation, we use the model trained with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swed</context>
<context position="32413" citStr="Bohnet and Nivre (2012)" startWordPosition="5585" endWordPosition="5588">ivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. Our work is distinguished from theirs because we use selectional branching instead. 6 Conclusion We present selectional branching that uses confidence estimates to decide when to employ a beam. Coupled with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed. It is interesting to see that our greedy parser outperformed m</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP’12, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL’06,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="8929" citStr="Buchholz and Marsi, 2006" startWordPosition="1501" endWordPosition="1504">} 9 RIGHT-PASS [a|2] [4] [6|0] A U {4 −XCOMP-+ 6} 10 LEFT-REDUCE [a|1] [4] [6|0] A U {2 +-DOBJ− 6} 11 NO-SHIFT [a|6] [ ] [7|0] 12 NO-REDUCE [a|4] [ ] [7|0] 13 NO-REDUCE [a|1] [ ] [7|0] 14 LEFT-REDUCE [0] [ ] [7|0] A U {1 +-NSUBJ− 7} 15 RIGHT-SHIFT [a|7] [ ] [8] A U {0 −ROOT-+ 7} 16 RIGHT-SHIFT [a|8] [ ] [ ] A U {7 −ADV-+ 8} Table 3: A transition sequence generated by our parsing algorithm using gold-standard decisions. Figure 1 shows the total number of transitions performed during training with respect to sentence lengths for Dutch. Among all languages distributed by the CoNLL-X shared task (Buchholz and Marsi, 2006), Dutch consists of the highest number of non-projective dependencies (5.4% in arcs, 36.4% in trees). Even with such a high number of nonprojective dependencies, our parsing algorithm still shows a linear growth in transitions. Table 3 shows a transition sequence generated by our parsing algorithm using gold-standard decisions. After w3 and w4 are compared, w3 is popped out of Q (state 4) so it is not compared to any other token in 0 (states 9 and 13). After w2 and w4 are compared, w2 is moved to δ (state 5) so it can be compared to other tokens in 0 (state 10). After w4 and w6 are compared, R</context>
<context position="20118" citStr="Buchholz and Marsi, 2006" startWordPosition="3516" endWordPosition="3519">. ρ is a ridge term to keep the inverse covariance well-conditioned. I(y, y&apos;) = ise � 1 y = y&apos; 0 otherw score(Ti) = 1056 4 Experiments 4.1 Corpora For projective parsing experiments, the Penn English Treebank (Marcus et al., 1993) is used with the standard split: sections 2-21 for training, 22 for development, and 23 for evaluation. All constituent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006). For non-projective parsing experiments, four languages from the CoNLLX shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006). These languages are selected because they contain nonprojective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10% of each training set is used for development. 4.2 Feature engineering For English, we mostly adapt features from Zhang and Nivre (2011) who have shown state-of-the-art parsing accuracy for transition-based dependency parsing. Their distance features are not included in our approach because they do not seem to show meaningful improvement. Feature selection is done on the English development </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL’06, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning, CoNLL’08,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="26050" citStr="Carreras et al. (2008)" startWordPosition="4531" endWordPosition="4534">g, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding tokens containing only punctuation. bt and bd indic</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In Proceedings of the 12th Conference on Computational Natural Language Learning, CoNLL’08, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to Stanford Dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC’10.</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to Stanford Dependencies: Trade-offs between speed and accuracy. In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Getting the Most out of Transition-based Dependency Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL:HLT’11,</booktitle>
<pages>687--692</pages>
<contexts>
<context position="1908" citStr="Choi and Palmer, 2011" startWordPosition="272" endWordPosition="275">ties as low as O(n) and O(n2) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confid</context>
<context position="16257" citStr="Choi and Palmer (2011)" startWordPosition="2815" endWordPosition="2818"> instead of the sum of all prediction scores. This is because our algorithm does not guarantee the same number of transitions for every sequence, so the sum of all scores would weigh more on sequences with more transitions. We experimented with both the sum and the average, and taking the average led to slightly higher parsing accuracy. 3.5 Bootstrapping transition sequences During training, a training instance is generated for each parsing state sij by taking a feature vector xij and its true label yij. To generate multiple transition sequences during training, the bootstrapping technique of Choi and Palmer (2011) is used, which is described in Algorithm 1.5 Algorithm 1 Bootstrapping Input: Dt: training set, Dd: development set. Output: A model M. 1: r + -0 2: I +- getTrainingInstances(Dt) 3: M0 +- buildModel(I) 4: S0 +- getScore(Dd, M0) 5: while (r = 0) or (Sr_1 &lt; Sr) do 6: r +- r + 1 7: I +- getTrainingInstances(Dt, Mr_1) 8: Mr +- buildModel(I) 9: Sr +- getScore(Dd, Mr) 10: return Mr_1 First, an initial model M0 is trained on all data by taking the one-best sequences, and its score is measured by testing on a development set (lines 2-4). Then, the next model Mr is trained on all data but this time, M</context>
<context position="29616" citStr="Choi and Palmer (2011)" startWordPosition="5178" endWordPosition="5181">larger beam size for decoding, it would have performed as efficiently. This implies that we no longer need to be so conscious about the beam size during decoding. Another interesting part is that (bt = 80, bd = 1) shows higher accuracy than (bt = 1, bd = 1); this implies that our training method of bootstrapping transition sequences can improve even a greedy parser. Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-ba</context>
<context position="31287" citStr="Choi and Palmer (2011)" startWordPosition="5430" endWordPosition="5433">tachment scores in Danish and Slovene. Our greedy approach outperforms both Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) who use different nonprojective parsing algorithms. 130 100 80 60 40 20 0 0 10 20 30 40 50 60 Sentence length Figure 6: The # of transitions performed during decoding with respect to sentence lengths for Dutch. Figure 6 shows the number of transitions performed during decoding with respect to sentence lengths for Dutch using bd = 1. Our parser still shows a linear growth in transition during decoding. 5 Related work Our parsing algorithm is most similar to Choi and Palmer (2011) who integrated our LEFT-REDUCE transition into Nivre’s list-based algorithm. Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2) Transitions 1059 for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fe</context>
</contexts>
<marker>Choi, Palmer, 2011</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2011. Getting the Most out of Transition-based Dependency Parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL:HLT’11, pages 687– 692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL’12,</booktitle>
<pages>363--367</pages>
<contexts>
<context position="24999" citStr="Choi and Palmer (2012" startWordPosition="4363" endWordPosition="4366">ecoding. Out of 1,700 sentences, the one best sequences are chosen for 1,095 sentences. This implies that about 64% of time, our greedy parser performs as accurately as our non-greedy parser using selectional branching. For the other languages, we use the same values as English for α, p, m, and b; only the ADAGRAD and bootstrap iterations are tuned on the development sets of the other languages. 4.4 Projective parsing experiments Before parsing, POS tags were assigned to the training set by using 20-way jackknifing. For the automatic generation of POS tags, we used the domainspecific model of Choi and Palmer (2012a)’s tagger, which gave 97.5% accuracy on the English evaluation set (0.2% higher than Collins (2002)’s tagger). Table 4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many secon</context>
<context position="33332" citStr="Choi and Palmer, 2012" startWordPosition="5724" endWordPosition="5727"> with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed. It is interesting to see that our greedy parser outperformed most other greedy dependency parsers. This is because our parser used both bootstrapping and Zhang and Nivre (2011)’s non-local features, which had not been used by other greedy parsers. In the future, we will experiment with more advanced dependency representations (de Marneffe and Manning, 2008; Choi and Palmer, 2012b) to show robustness of our approach. Furthermore, we will evaluate individual methods of our approach separately to show impact of each method on parsing performance. We also plan to implement the typical beam search approach to make a direct comparison to our selectional branching.8 Acknowledgments Special thanks are due to Luke Vilnis of the University of Massachusetts Amherst for insights on 8Our parser is publicly available under an open source project, ClearNLP (clearnlp.googlecode.com). the ADAGRAD derivation. We gratefully acknowledge a grant from the Defense Advanced Research Project</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012a. Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL’12, pages 363–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Guidelines for the Clear Style Constituent to Dependency Conversion.</title>
<date>2012</date>
<tech>Technical Report 01-12,</tech>
<institution>University of Colorado Boulder.</institution>
<contexts>
<context position="24999" citStr="Choi and Palmer (2012" startWordPosition="4363" endWordPosition="4366">ecoding. Out of 1,700 sentences, the one best sequences are chosen for 1,095 sentences. This implies that about 64% of time, our greedy parser performs as accurately as our non-greedy parser using selectional branching. For the other languages, we use the same values as English for α, p, m, and b; only the ADAGRAD and bootstrap iterations are tuned on the development sets of the other languages. 4.4 Projective parsing experiments Before parsing, POS tags were assigned to the training set by using 20-way jackknifing. For the automatic generation of POS tags, we used the domainspecific model of Choi and Palmer (2012a)’s tagger, which gave 97.5% accuracy on the English evaluation set (0.2% higher than Collins (2002)’s tagger). Table 4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many secon</context>
<context position="33332" citStr="Choi and Palmer, 2012" startWordPosition="5724" endWordPosition="5727"> with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed. It is interesting to see that our greedy parser outperformed most other greedy dependency parsers. This is because our parser used both bootstrapping and Zhang and Nivre (2011)’s non-local features, which had not been used by other greedy parsers. In the future, we will experiment with more advanced dependency representations (de Marneffe and Manning, 2008; Choi and Palmer, 2012b) to show robustness of our approach. Furthermore, we will evaluate individual methods of our approach separately to show impact of each method on parsing performance. We also plan to implement the typical beam search approach to make a direct comparison to our selectional branching.8 Acknowledgments Special thanks are due to Luke Vilnis of the University of Massachusetts Amherst for insights on 8Our parser is publicly available under an open source project, ClearNLP (clearnlp.googlecode.com). the ADAGRAD derivation. We gratefully acknowledge a grant from the Defense Advanced Research Project</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012b. Guidelines for the Clear Style Constituent to Dependency Conversion. Technical Report 01-12, University of Colorado Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Empirical methods in natural language processing, EMNLP’02,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="18239" citStr="Collins, 2002" startWordPosition="3148" endWordPosition="3149">rning rates to exploit rarely seen features while remaining scalable (Duchi et al., 2011).This is suitable for NLP tasks where rarely seen features often play an important role and training data consists of a large number of instances with high dimensional features. Algorithm 2 shows our adaptation of ADAGRAD with logistic regression for multi-class classification. Note that when used with logistic regression, ADAGRAD takes a regular gradient instead of a subgradient method for updating weights. For our experiments, ADAGRAD slightly outperformed learning algorithms such as average perceptron (Collins, 2002) or Liblinear SVM (Hsieh et al., 2008). Algorithm 2 ADAGRAD + logistic regression Input: D = {(xi, yi)}ni=1 s.t. xi E X, yi E Y Φ(x, y) E Rd s.t. d = dimension(X) x |Y| T: iterations, α: learning rate, p: ridge Output: A weight vector w E Rd. 1: w +- 0, where w E Rd 2: G +- 0, where G E Rd 3: for t +- 1 ... T do 4: for i +- 1 ... n do 5: QvyEY +- I(yi, y) − f(xi, y), s.t. Q E R|Y| ∂ +- � 6: yEY(Φ(xi, y) &apos; Qy) 7: G +- G + ∂ o ∂ 8: for j +- 1 ... d do 9: wj +- wj + α &apos; 1 ρ+�/Gj &apos;∂j The algorithm takes three hyper-parameters; T is the number of iterations, α is the learning rate, and ρ is the rid</context>
<context position="25100" citStr="Collins (2002)" startWordPosition="4381" endWordPosition="4382">out 64% of time, our greedy parser performs as accurately as our non-greedy parser using selectional branching. For the other languages, we use the same values as English for α, p, m, and b; only the ADAGRAD and bootstrap iterations are tuned on the development sets of the other languages. 4.4 Projective parsing experiments Before parsing, POS tags were assigned to the training set by using 20-way jackknifing. For the automatic generation of POS tags, we used the domainspecific model of Choi and Palmer (2012a)’s tagger, which gave 97.5% accuracy on the English evaluation set (0.2% higher than Collins (2002)’s tagger). Table 4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the conference on Empirical methods in natural language processing, EMNLP’02, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Proceedings of the COLING workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Jordan Rodu</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Spectral Dependency Parsing with Latent Variables.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’12,</booktitle>
<pages>205--213</pages>
<contexts>
<context position="27466" citStr="Dhillon et al. (2012)" startWordPosition="4799" endWordPosition="4802">ained with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.</context>
</contexts>
<marker>Dhillon, Rodu, Collins, Foster, Ungar, 2012</marker>
<rawString>Paramveer S. Dhillon, Jordan Rodu, Michael Collins, Dean P. Foster, and Lyle H. Ungar. 2012. Spectral Dependency Parsing with Latent Variables. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’12, pages 205–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<issue>39</issue>
<contexts>
<context position="17714" citStr="Duchi et al., 2011" startWordPosition="3066" endWordPosition="3069"> the most accurate parse output compared to the gold-standard tree. The score of Mr is measured (line 9), and repeat the procedure if Sr−1 &lt; Sr; otherwise, return the previous model Mr−1. 5Alternatively, the dynamic oracle approach of Goldberg and Nivre (2012) can be used to generate multiple transition sequences, which is expected to show similar results. 3.6 Adaptive subgradient algorithm To build each model during bootstrapping, we use a stochastic adaptive subgradient algorithm called ADAGRAD that uses per-coordinate learning rates to exploit rarely seen features while remaining scalable (Duchi et al., 2011).This is suitable for NLP tasks where rarely seen features often play an important role and training data consists of a large number of instances with high dimensional features. Algorithm 2 shows our adaptation of ADAGRAD with logistic regression for multi-class classification. Note that when used with logistic regression, ADAGRAD takes a regular gradient instead of a subgradient method for updating weights. For our experiments, ADAGRAD slightly outperformed learning algorithms such as average perceptron (Collins, 2002) or Liblinear SVM (Hsieh et al., 2008). Algorithm 2 ADAGRAD + logistic regr</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 12(39):2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fernández-González</author>
<author>Carlos GómezRodríguez</author>
</authors>
<title>Improving Transition-Based Dependency Parsing with Buffer Transitions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP’12,</booktitle>
<pages>308--319</pages>
<marker>Fernández-González, GómezRodríguez, 2012</marker>
<rawString>Daniel Fernández-González and Carlos GómezRodríguez. 2012. Improving Transition-Based Dependency Parsing with Buffer Transitions. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP’12, pages 308–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT:NAACL’10,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="29493" citStr="Goldberg and Elhadad, 2010" startWordPosition="5158" endWordPosition="5161">lectional branching automatically reduced down the beam size by estimating low confidence predictions, so even if we assigned a larger beam size for decoding, it would have performed as efficiently. This implies that we no longer need to be so conscious about the beam size during decoding. Another interesting part is that (bt = 80, bd = 1) shows higher accuracy than (bt = 1, bd = 1); this implies that our training method of bootstrapping transition sequences can improve even a greedy parser. Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective </context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT:NAACL’10, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A Dynamic Oracle for Arc-Eager Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics, COLING’12.</booktitle>
<contexts>
<context position="17355" citStr="Goldberg and Nivre (2012)" startWordPosition="3014" endWordPosition="3017"> score is measured by testing on a development set (lines 2-4). Then, the next model Mr is trained on all data but this time, Mr−1 is used to generate multiple transition sequences (line 7-8). Among all transition sequences generated by Mr−1, training instances from only T1 and Tg are used to train Mr, where T1 is the one-best sequence and Tg is a sequence giving the most accurate parse output compared to the gold-standard tree. The score of Mr is measured (line 9), and repeat the procedure if Sr−1 &lt; Sr; otherwise, return the previous model Mr−1. 5Alternatively, the dynamic oracle approach of Goldberg and Nivre (2012) can be used to generate multiple transition sequences, which is expected to show similar results. 3.6 Adaptive subgradient algorithm To build each model during bootstrapping, we use a stochastic adaptive subgradient algorithm called ADAGRAD that uses per-coordinate learning rates to exploit rarely seen features while remaining scalable (Duchi et al., 2011).This is suitable for NLP tasks where rarely seen features often play an important role and training data consists of a large number of instances with high dimensional features. Algorithm 2 shows our adaptation of ADAGRAD with logistic regre</context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A Dynamic Oracle for Arc-Eager Dependency Parsing. In Proceedings of the 24th International Conference on Computational Linguistics, COLING’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Discriminative Classifiers for Deterministic Dependency Parsing. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLING-ACL’06,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="29464" citStr="Hall et al., 2006" startWordPosition="5154" endWordPosition="5157">h indicates that selectional branching automatically reduced down the beam size by estimating low confidence predictions, so even if we assigned a larger beam size for decoding, it would have performed as efficiently. This implies that we no longer need to be so conscious about the beam size during decoding. Another interesting part is that (bt = 80, bd = 1) shows higher accuracy than (bt = 1, bd = 1); this implies that our training method of bootstrapping transition sequences can improve even a greedy parser. Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) </context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2006</marker>
<rawString>Johan Hall, Joakim Nivre, and Jens Nilsson. 2006. Discriminative Classifiers for Deterministic Dependency Parsing. In In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLING-ACL’06, pages 316– 323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Kai-Wei Chang</author>
<author>Chih-Jen Lin</author>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A Dual Coordinate Descent Method for Large-scale Linear SVM.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning, ICML’08,</booktitle>
<pages>408--415</pages>
<contexts>
<context position="18277" citStr="Hsieh et al., 2008" startWordPosition="3153" endWordPosition="3156">n features while remaining scalable (Duchi et al., 2011).This is suitable for NLP tasks where rarely seen features often play an important role and training data consists of a large number of instances with high dimensional features. Algorithm 2 shows our adaptation of ADAGRAD with logistic regression for multi-class classification. Note that when used with logistic regression, ADAGRAD takes a regular gradient instead of a subgradient method for updating weights. For our experiments, ADAGRAD slightly outperformed learning algorithms such as average perceptron (Collins, 2002) or Liblinear SVM (Hsieh et al., 2008). Algorithm 2 ADAGRAD + logistic regression Input: D = {(xi, yi)}ni=1 s.t. xi E X, yi E Y Φ(x, y) E Rd s.t. d = dimension(X) x |Y| T: iterations, α: learning rate, p: ridge Output: A weight vector w E Rd. 1: w +- 0, where w E Rd 2: G +- 0, where G E Rd 3: for t +- 1 ... T do 4: for i +- 1 ... n do 5: QvyEY +- I(yi, y) − f(xi, y), s.t. Q E R|Y| ∂ +- � 6: yEY(Φ(xi, y) &apos; Qy) 7: G +- G + ∂ o ∂ 8: for j +- 1 ... d do 9: wj +- wj + α &apos; 1 ρ+�/Gj &apos;∂j The algorithm takes three hyper-parameters; T is the number of iterations, α is the learning rate, and ρ is the ridge (T &gt; 0, α &gt; 0, ρ ≥ 0). G is our run</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. 2008. A Dual Coordinate Descent Method for Large-scale Linear SVM. In Proceedings of the 25th international conference on Machine learning, ICML’08, pages 408–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic Programming for Linear-Time Incremental Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL’10.</booktitle>
<contexts>
<context position="2348" citStr="Huang and Sagae, 2010" startWordPosition="332" endWordPosition="335">(Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about 64% of time. Thus, it i</context>
<context position="10054" citStr="Huang and Sagae, 2010" startWordPosition="1698" endWordPosition="1701">(state 5) so it can be compared to other tokens in 0 (state 10). After w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in Q (state 10). After w6 and w7 are compared, w6 is popped out of Q (state 12) because it is not needed for later parsing states. 3 Selectional branching 3.1 Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e t</context>
<context position="25707" citStr="Huang and Sagae (2010)" startWordPosition="4470" endWordPosition="4473"> Collins (2002)’s tagger). Table 4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80</context>
<context position="32203" citStr="Huang and Sagae (2010)" startWordPosition="5555" endWordPosition="5558">1059 for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. Our work is distinguished from theirs because we use selectional branching instead. 6 Conclusion We present selectional branching that uses confidence estimates to decide when to employ a beam. Coupled with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping,</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic Programming for Linear-Time Incremental Parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured Perceptron with Inexact Search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT’12,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="10097" citStr="Huang et al., 2012" startWordPosition="1706" endWordPosition="1709">s in 0 (state 10). After w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in Q (state 10). After w6 and w7 are compared, w6 is popped out of Q (state 12) because it is not needed for later parsing states. 3 Selectional branching 3.1 Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is the maximum number o</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured Perceptron with Inexact Search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT’12, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient Thirdorder Dependency Parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL’10.</booktitle>
<contexts>
<context position="25907" citStr="Koo and Collins (2010)" startWordPosition="4505" endWordPosition="4508">m search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 </context>
<context position="27283" citStr="Koo and Collins (2010)" startWordPosition="4769" endWordPosition="4772">m sizes used during training and decoding, respectively. UAS: unlabeled attachment score, LAS: labeled attachment score, Time: seconds per sentence. For evaluation, we use the model trained with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (20</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient Thirdorder Dependency Parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL:HLT’08,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="26021" citStr="Koo et al. (2008)" startWordPosition="4526" endWordPosition="4529">rsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding tokens containing only</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL:HLT’08, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kübler</author>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="6786" citStr="Kübler et al., 2009" startWordPosition="1101" endWordPosition="1104">wj. *-SHIFT is performed when no dependency is found for wj and any token in a other than wi. After *-SHIFT, all tokens in S as well as wj are pushed into a. *-REDUCE is performed when wi already has the head, and wi is not the head of any token in 0. After *-REDUCE, wi is popped out of a. *-PASS is performed when neither *-SHIFT nor *-REDUCE can be performed. After *-PASS, wi is moved to the front of S so it 2The parsing complexity of a transition-based dependency parsing algorithm is determined by the number of transitions performed with respect to the number of tokens in a sentence, say n (Kübler et al., 2009). can be compared to other tokens in 0 later. Each transition needs to satisfy certain preconditions to ensure the properties of a well-formed dependency graph (Nivre, 2008); they are described in Table 2. (i � j) and (i � ∗ j) indicate that wj is the head and an ancestor of wi with any label, respectively. When a parser is trained on only projective trees, our algorithm learns only the top 4 transitions and produces only projective trees during decoding. In this case, it performs at most 2n − 1 transitions per sentence so the complexity is O(n). When a parser is trained on a mixture of projec</context>
</contexts>
<marker>Kübler, McDonald, Nivre, 2009</marker>
<rawString>Sandra Kübler, Ryan T. McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="19723" citStr="Marcus et al., 1993" startWordPosition="3454" endWordPosition="3457">e subtracted from an output of the indicator function I(y, y&apos;), which forces our model to keep learning this instance until the prediction is 100% confident (in other words, until the score of yi becomes 1). Then, a subgradient is measured by taking all feature vectors together weighted by Q (line 6). This subgradient is used to update G and w, where o is the Hadamard product (lines 7-9). ρ is a ridge term to keep the inverse covariance well-conditioned. I(y, y&apos;) = ise � 1 y = y&apos; 0 otherw score(Ti) = 1056 4 Experiments 4.1 Corpora For projective parsing experiments, the Penn English Treebank (Marcus et al., 1993) is used with the standard split: sections 2-21 for training, 22 for development, and 23 for evaluation. All constituent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006). For non-projective parsing experiments, four languages from the CoNLLX shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006). These languages are selected because they contain nonprojective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>Mário A T Figueiredo</author>
</authors>
<title>Turbo Parsers: Dependency Parsing by Approximate Variational Inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10,</booktitle>
<pages>34--44</pages>
<contexts>
<context position="25973" citStr="Martins et al. (2010)" startWordPosition="4516" endWordPosition="4519">g approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English </context>
<context position="28052" citStr="Martins et al. (2010)" startWordPosition="4902" endWordPosition="4905">iseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80 bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12 bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. sentence using the beam size of 80. Our parser is implemented in Java and tested on an Intel Xeon 2.57GHz. Note that we do not include input/output time for our speed comparison. For a proof of concept, we run the same model, trained with bt = 80, but decode with different beam sizes using the same margin. Surpris</context>
<context position="30261" citStr="Martins et al. (2010)" startWordPosition="5261" endWordPosition="5264">for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-based parsing approaches. Martins et al. (2010) uses integer linear programming for the optimization of their parsing model. Some of these approaches use greedy parsers, so we include our results from models using (bt = 80, bd = 1, m = 0.88), which finds only the one-best sequences during decoding although it is trained on multiple transition sequences (see Section 4.4). Our parser shows higher accuracies for most languages except for unlabeled attachment scores in Danish and Slovene. Our greedy approach outperforms both Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) who use different nonprojective parsing algorithms. 130 1</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>André F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2010. Turbo Parsers: Dependency Parsing by Approximate Variational Inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10, pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Mcdonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the European American Chapter of the Association for Computational Linguistics, EACL’06,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="25851" citStr="Mcdonald and Pereira (2006)" startWordPosition="4495" endWordPosition="4498">ws results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m </context>
</contexts>
<marker>Mcdonald, Pereira, 2006</marker>
<rawString>Ryan Mcdonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of the Annual Meeting of the European American Chapter of the Association for Computational Linguistics, EACL’06, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="25818" citStr="McDonald et al. (2005)" startWordPosition="4490" endWordPosition="4493">pproach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.5</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual Dependency Analysis with a Two-Stage Discriminative Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL’06,</booktitle>
<pages>216--220</pages>
<contexts>
<context position="27825" citStr="McDonald et al. (2006)" startWordPosition="4858" endWordPosition="4861">acy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80 bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12 bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. sentence using the beam size of 80. Our parser is implemented in Java and tested on an I</context>
<context position="29951" citStr="McDonald et al. (2006)" startWordPosition="5223" endWordPosition="5226">can improve even a greedy parser. Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-based parsing approaches. Martins et al. (2010) uses integer linear programming for the optimization of their parsing model. Some of these approaches use greedy parsers, so we include our results from models using (bt = 80, bd = 1, m = 0.88), which finds only the one-best sequences during decoding although it is trained on multiple tra</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL’06, pages 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating Graph-based and Transition-based Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL:HLT’08,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="28002" citStr="Nivre and McDonald (2008)" startWordPosition="4892" endWordPosition="4895">l other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80 bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12 bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. sentence using the beam size of 80. Our parser is implemented in Java and tested on an Intel Xeon 2.57GHz. Note that we do not include input/output time for our speed comparison. For a proof of concept, we run the same model, trained with bt = 80, but decode with d</context>
<context position="30155" citStr="Nivre and McDonald (2008)" startWordPosition="5247" endWordPosition="5250">es of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-based parsing approaches. Martins et al. (2010) uses integer linear programming for the optimization of their parsing model. Some of these approaches use greedy parsers, so we include our results from models using (bt = 80, bd = 1, m = 0.88), which finds only the one-best sequences during decoding although it is trained on multiple transition sequences (see Section 4.4). Our parser shows higher accuracies for most languages except for unlabeled attachment scores in Danish and Slovene. Our greedy approach outperforms both Nivre (2009) a</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating Graph-based and Transition-based Dependency Parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL:HLT’08, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>PseudoProjective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL’05,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="1752" citStr="Nivre and Nilsson, 2005" startWordPosition="246" endWordPosition="249">ion Transition-based dependency parsing has gained considerable interest because it runs fast and performs accurately. Transition-based parsing gives complexities as low as O(n) and O(n2) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zh</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. PseudoProjective Dependency Parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL’05, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gül¸sen Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning, CoNLL’06,</booktitle>
<pages>221--225</pages>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gül¸sen Eryiˇgit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of the 10th Conference on Computational Natural Language Learning, CoNLL’06, pages 221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An Efficient Algorithm for Projective Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies, IWPT’03,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="3809" citStr="Nivre, 2003" startWordPosition="568" endWordPosition="569">sent a new transition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Transition Current state ⇒ Resulting state LEFTl-REDUCE ([σ|i],δ,[j|β],A) ⇒ (σ,δ,[j|β],A∪{i←lj}) RIGHTl-SHIFT ([σ|i],δ,[j|β],A) ⇒ ([σ|i|δ|j],[],β,A∪{i→ </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An Efficient Algorithm for Projective Dependency Parsing. In Proceedings of the 8th International Workshop on Parsing Technologies, IWPT’03, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL’04 Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="31880" citStr="Nivre, 2004" startWordPosition="5513" endWordPosition="5514">i and Palmer (2011) who integrated our LEFT-REDUCE transition into Nivre’s list-based algorithm. Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2) Transitions 1059 for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS ta</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in Deterministic Dependency Parsing. In Proceedings of the ACL’04 Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="19961" citStr="Nivre (2006)" startWordPosition="3494" endWordPosition="3495">ing all feature vectors together weighted by Q (line 6). This subgradient is used to update G and w, where o is the Hadamard product (lines 7-9). ρ is a ridge term to keep the inverse covariance well-conditioned. I(y, y&apos;) = ise � 1 y = y&apos; 0 otherw score(Ti) = 1056 4 Experiments 4.1 Corpora For projective parsing experiments, the Penn English Treebank (Marcus et al., 1993) is used with the standard split: sections 2-21 for training, 22 for development, and 23 for evaluation. All constituent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006). For non-projective parsing experiments, four languages from the CoNLLX shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006). These languages are selected because they contain nonprojective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10% of each training set is used for development. 4.2 Feature engineering For English, we mostly adapt features from Zhang and Nivre (2011) who have shown state-of-the-art parsing accuracy for transition-based dependency parsing. Their dist</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1385" citStr="Nivre, 2008" startWordPosition="194" endWordPosition="195">ty of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search. 1 Introduction Transition-based dependency parsing has gained considerable interest because it runs fast and performs accurately. Transition-based parsing gives complexities as low as O(n) and O(n2) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees </context>
<context position="3823" citStr="Nivre, 2008" startWordPosition="570" endWordPosition="571">ansition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Transition Current state ⇒ Resulting state LEFTl-REDUCE ([σ|i],δ,[j|β],A) ⇒ (σ,δ,[j|β],A∪{i←lj}) RIGHTl-SHIFT ([σ|i],δ,[j|β],A) ⇒ ([σ|i|δ|j],[],β,A∪{i→ l j}) NO-SHIFT</context>
<context position="6959" citStr="Nivre, 2008" startWordPosition="1130" endWordPosition="1131">hen wi already has the head, and wi is not the head of any token in 0. After *-REDUCE, wi is popped out of a. *-PASS is performed when neither *-SHIFT nor *-REDUCE can be performed. After *-PASS, wi is moved to the front of S so it 2The parsing complexity of a transition-based dependency parsing algorithm is determined by the number of transitions performed with respect to the number of tokens in a sentence, say n (Kübler et al., 2009). can be compared to other tokens in 0 later. Each transition needs to satisfy certain preconditions to ensure the properties of a well-formed dependency graph (Nivre, 2008); they are described in Table 2. (i � j) and (i � ∗ j) indicate that wj is the head and an ancestor of wi with any label, respectively. When a parser is trained on only projective trees, our algorithm learns only the top 4 transitions and produces only projective trees during decoding. In this case, it performs at most 2n − 1 transitions per sentence so the complexity is O(n). When a parser is trained on a mixture of projective and nonprojective trees, our algorithm learns all transitions and produces both kinds of trees during decoding. In this case, it performs at most n(n+1) 2 transitions s</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-Projective Dependency Parsing in Expected Linear Time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACLIJCNLP’09,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="1884" citStr="Nivre, 2009" startWordPosition="270" endWordPosition="271">ives complexities as low as O(n) and O(n2) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one</context>
<context position="27886" citStr="Nivre (2009)" startWordPosition="4870" endWordPosition="4871">ns (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80 bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12 bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. sentence using the beam size of 80. Our parser is implemented in Java and tested on an Intel Xeon 2.57GHz. Note that we do not include input/output t</context>
<context position="30013" citStr="Nivre (2009)" startWordPosition="5234" endWordPosition="5235">gher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-based parsing approaches. Martins et al. (2010) uses integer linear programming for the optimization of their parsing model. Some of these approaches use greedy parsers, so we include our results from models using (bt = 80, bd = 1, m = 0.88), which finds only the one-best sequences during decoding although it is trained on multiple transition sequences (see Section 4.4). Our parser shows higher a</context>
<context position="31801" citStr="Nivre (2009)" startWordPosition="5503" endWordPosition="5504">on during decoding. 5 Related work Our parsing algorithm is most similar to Choi and Palmer (2011) who integrated our LEFT-REDUCE transition into Nivre’s list-based algorithm. Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2) Transitions 1059 for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and </context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-Projective Dependency Parsing in Expected Linear Time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACLIJCNLP’09, pages 351–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine Pruning for Efficient Multi-Pass Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 12th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>12</pages>
<contexts>
<context position="27493" citStr="Rush and Petrov (2012)" startWordPosition="4804" endWordPosition="4807">0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Danish Dutch Slovene Swedish LAS UAS LAS UAS LAS UAS LAS UAS Nivre et al. (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50 McDonald et al. (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93 Nivre (2009) 84.2 - - - 75.2 - - - F.-González and G.-Rodríguez (2012) 85.17 90.10 - - - - 83.55 89.30 Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66 Martins et al. (2010) - 91.50 - 84.91 - 85.53 - 89.80 bt = 80,</context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander M. Rush and Slav Petrov. 2012. Vine Pruning for Efficient Multi-Pass Dependency Parsing. In Proceedings of the 12th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL:HLT’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="25998" citStr="Rush et al. (2010)" startWordPosition="4521" endWordPosition="4524">sed parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP’10, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser Combination by Reparsing. In</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, NAACL’06,</booktitle>
<pages>129--132</pages>
<contexts>
<context position="25879" citStr="Sagae and Lavie (2006)" startWordPosition="4500" endWordPosition="4503">dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt </context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser Combination by Reparsing. In In Proceedings of the Human Language Technology Conference of the NAACL, NAACL’06, pages 129–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP’09,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="26113" citStr="Suzuki et al. (2009)" startWordPosition="4542" endWordPosition="4545">s using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding tokens containing only punctuation. bt and bd indicate the beam sizes used during training and decoding, respectiv</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP’09, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machine.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies, IWPT’03,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="19922" citStr="Yamada and Matsumoto (2003)" startWordPosition="3485" endWordPosition="3488"> yi becomes 1). Then, a subgradient is measured by taking all feature vectors together weighted by Q (line 6). This subgradient is used to update G and w, where o is the Hadamard product (lines 7-9). ρ is a ridge term to keep the inverse covariance well-conditioned. I(y, y&apos;) = ise � 1 y = y&apos; 0 otherw score(Ti) = 1056 4 Experiments 4.1 Corpora For projective parsing experiments, the Penn English Treebank (Marcus et al., 1993) is used with the standard split: sections 2-21 for training, 22 for development, and 23 for evaluation. All constituent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006). For non-projective parsing experiments, four languages from the CoNLLX shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006). These languages are selected because they contain nonprojective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10% of each training set is used for development. 4.2 Feature engineering For English, we mostly adapt features from Zhang and Nivre (2011) who have shown state-of-the-art parsing accuracy for transiti</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machine. In Proceedings of the 8th International Workshop on Parsing Technologies, IWPT’03, pages 195– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A Tale of Two Parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="2325" citStr="Zhang and Clark, 2008" startWordPosition="328" endWordPosition="331">ies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about </context>
<context position="10031" citStr="Zhang and Clark, 2008" startWordPosition="1694" endWordPosition="1697">ared, w2 is moved to δ (state 5) so it can be compared to other tokens in 0 (state 10). After w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in Q (state 10). After w6 and w7 are compared, w6 is popped out of Q (state 12) because it is not needed for later parsing states. 3 Selectional branching 3.1 Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here perfo</context>
<context position="15614" citStr="Zhang and Clark (2008)" startWordPosition="2708" endWordPosition="2711">tion sequence Let Pi be a list of all predictions that lead to generate a transition sequence Ti. The predictions in Pi are either inherited from T1 or made specifically for Ti. In Figure 2, P3 consists of p11 as its first prediction, p22 as its second prediction, and T1 = T2 = T3 = Td = s11 s12 p21 p11 s22 ... ... s2t p12 p1j ... ... s1t p22 s33 ... s3t p2j ... É sdt É 1055 further predictions made specifically for T3. The score of each prediction is measured by f(x, y) in Section 3.3. Then, the score of Ti is measured by averaging scores of all predictions in Pi. E pEPi score(p) |Pi| Unlike Zhang and Clark (2008), we take the average instead of the sum of all prediction scores. This is because our algorithm does not guarantee the same number of transitions for every sequence, so the sum of all scores would weigh more on sequences with more transitions. We experimented with both the sum and the average, and taking the average led to slightly higher parsing accuracy. 3.5 Bootstrapping transition sequences During training, a training instance is generated for each parsing state sij by taking a feature vector xij and its true label yij. To generate multiple transition sequences during training, the bootst</context>
<context position="25679" citStr="Zhang and Clark (2008)" startWordPosition="4465" endWordPosition="4468">uation set (0.2% higher than Collins (2002)’s tagger). Table 4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.</context>
<context position="32098" citStr="Zhang and Clark (2008)" startWordPosition="5540" endWordPosition="5543">for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2) Transitions 1059 for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. Our work is distinguished from theirs because we use selectional branching instead. 6 Conclusion We present selectional branching that uses confidence estimates to decide when to employ a </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A Tale of Two Parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized Higher-Order Dependency Parsing with Cube Pruning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12,</booktitle>
<pages>320--331</pages>
<contexts>
<context position="25939" citStr="Zhang and McDonald (2012)" startWordPosition="4510" endWordPosition="4513">ows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006 bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing acc</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012. Generalized Higher-Order Dependency Parsing with Cube Pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12, pages 320–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based Dependency Parsing with Rich Non-local Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL’11,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="2371" citStr="Zhang and Nivre, 2011" startWordPosition="336" endWordPosition="339">5) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about 64% of time. Thus, it is preferred if the beam</context>
<context position="10077" citStr="Zhang and Nivre, 2011" startWordPosition="1702" endWordPosition="1705">compared to other tokens in 0 (state 10). After w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in Q (state 10). After w6 and w7 are compared, w6 is popped out of Q (state 12) because it is not needed for later parsing states. 3 Selectional branching 3.1 Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is </context>
<context position="20460" citStr="Zhang and Nivre (2011)" startWordPosition="3571" endWordPosition="3574">tuent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006). For non-projective parsing experiments, four languages from the CoNLLX shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006). These languages are selected because they contain nonprojective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10% of each training set is used for development. 4.2 Feature engineering For English, we mostly adapt features from Zhang and Nivre (2011) who have shown state-of-the-art parsing accuracy for transition-based dependency parsing. Their distance features are not included in our approach because they do not seem to show meaningful improvement. Feature selection is done on the English development set. For the other languages, the same features are used with the addition of morphological features provided by CoNLL-X; specifically, morphological features from the top of σ and the front of β are added as unigram features. Moreover, all POS tag features from English are duplicated with coarsegrained POS tags provided by CoNLL-X. No more</context>
<context position="25740" citStr="Zhang and Nivre (2011)" startWordPosition="4476" endWordPosition="4479">4 shows comparison between past and current state-of-the-art parsers and our approach. The first block shows results from transition-based dependency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach UAS LAS Time Zhang and Clark (2008) 92.1 Huang and Sagae (2010) 92.1 0.04 Zhang and Nivre (2011) 92.9 91.8 0.03 Bohnet and Nivre (2012) 93.38 92.44 0.4 McDonald et al. (2005) 90.9 Mcdonald and Pereira (2006) 91.5 Sagae and Lavie (2006) 92.7 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 91.86 Martins et al. (2010) 93.26 Rush et al. (2010) 93.8 Koo et al. (2008) 93.16 Carreras et al. (2008) 93.54 Bohnet and Nivre (2012) 93.67 92.68 Suzuki et al. (2009) 93.79 bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009 bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009 bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008 bt = 80, bd = 8, m = 0.88 92.89 91.87 0.</context>
<context position="26989" citStr="Zhang and Nivre (2011)" startWordPosition="4727" endWordPosition="4730">88 92.76 91.76 0.004 bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003 bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002 bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002 Table 4: Parsing accuracies and speeds on the English evaluation set, excluding tokens containing only punctuation. bt and bd indicate the beam sizes used during training and decoding, respectively. UAS: unlabeled attachment score, LAS: labeled attachment score, Time: seconds per sentence. For evaluation, we use the model trained with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because the</context>
<context position="29558" citStr="Zhang and Nivre (2011)" startWordPosition="5169" endWordPosition="5172">ting low confidence predictions, so even if we assigned a larger beam size for decoding, it would have performed as efficiently. This implies that we no longer need to be so conscious about the beam size during decoding. Another interesting part is that (bt = 80, bd = 1) shows higher accuracy than (bt = 1, bd = 1); this implies that our training method of bootstrapping transition sequences can improve even a greedy parser. Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies. Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach. McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach. Nivre (2009) and Fernández-González and Gómez-Rodríguez (2012) use different non-projective transition-based parsing approaches. Nivre and McDonald (2008) us</context>
<context position="32309" citStr="Zhang and Nivre (2011)" startWordPosition="5570" endWordPosition="5573">sitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. Our work is distinguished from theirs because we use selectional branching instead. 6 Conclusion We present selectional branching that uses confidence estimates to decide when to employ a beam. Coupled with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL’11, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>