<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000449">
<note confidence="0.950417666666667">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 79-86
Edmonton, May-June 2003
</note>
<bodyText confidence="0.999906857142857">
translational equivalence between their components.
Every link generated by a D-MTG has D compo-
nents. Some (but not all) components of a link may
be empty. An empty component indicates that an
expression vanishes in translation. To express empty
components, we add a special terminal E to T and
a special nonterminal E to N. In MTG applications,
the different components of a link will typically come
from largely disjoint subsets of T or N, representing
vocabularies or sets of grammatical categories from
different languages.
Each MTG also has a set of production rules (or
just &amp;quot;productions&amp;quot; for short), which fall into one of
two categories.&apos; YIELD productions have the form
</bodyText>
<equation confidence="0.9979862">
X t (1)
where X is a link of D nonterminals and t is a link of
D terminals. td is empty if and only if Xd is empty,
1 &lt;d &lt; D. DEPEND productions have the form
X=NPM (2)
</equation>
<bodyText confidence="0.996153931034483">
where M is a non-empty vector of nonterminal links,
P is a non-empty vector of D permutations, and N
(&amp;quot;join&amp;quot;) is a rendering function, explained below.2
The rank of an MTG production is the number
of nonterminal links on its RHS. The rank of an
MTG is the maximum rank of its production rules.
MTG(R) is the class of MTGs of rank R.
Each row of P and M corresponds to a different
component of multitext. Each permutation is writ-
ten as a row in P, and each link is written as a column
in M, as in Equation 3 below. If Xd is empty, then
the dth component of every link in M must be empty
too. If Xd is not empty, then at least one of the links
in M must have a non-empty dth component. The
position of a non-empty terminal or nonterminal rel-
ative to other non-empty elements of its component
is its role. If there are m non-empty nonterminals
in component (row) d of M then Pd is a permutation
of roles from 1 to Tn. Pd is empty if and only if Xd
is empty.
The D-MTG derivation process begins with the
start link $, which is a vector of D copies of the
special start symbol $ E N. The derivation contin-
ues with nondeterministic application of production
rules. The semantics of = are the usual semantics
of rewriting systems, i.e., that the expression on the
LHS can be rewritten as the expression on the RHS.
Following convention, we let be the reflexive and
transitive closure of
</bodyText>
<footnote confidence="0.975035">
1-This dichotomy imposes a convenient normal form,
without loss of generality.
2The rendering function is a notational convenience;
MTGs can be defined without it.
</footnote>
<bodyText confidence="0.983854270833333">
When no more productions can be applied, i.e.,
when all nonterminals have been rewritten into ter-
minals, the rendering functions are evaluated in
inside-out order. The N function rearranges the non-
empty terminals in each row of a link vector accord-
ing to that row&apos;s permutation. For example,
[1,2,3] (c ab c abc
N [1,3,2,4] wyxcz =wxyz
[3,2,1] t Env c
vut
By reordering the terminals independently in each
component, the join operator hides information
about which terminals were derived from the same
link. Thus, the translational equivalence represented
by links is not observable in MTG yields, just as it
is not observable in raw multitext.
To avoid spurious ambiguity, we stipulate a nor-
mal form for components of P: In each permutation,
the first appearance of role x must precede the first
appearance of role y for all x &lt; y, except where the
arrangement is incompatible with a preceding per-
mutation in P. We could, for example, obtain the
same result above if we put EZE first, put ewt last,
and switch their roles in the 2nd and 3rd permuta-
tions. However, the normal form requires the 2nd
permutation to be [1, 3, 2, 4], not [4, 3, 2, 1], so EZE
must be listed last.
Let Q be an MTG derivation where no more pro-
duction rules can be applied. Let Render(Q) be the
result of evaluating all the N&apos;s in Q. The (formal)
language L(G) of an MTG G is the set of multi-
texts that can be generated by applying z to the
start link of G and then evaluating all the joins. I.e.,
L(G) = {Render(Q) : $ z Q}.
Due to the importance of lexical information in
disambiguating syntactic structure, we shall pay spe-
cial attention to lexicalized MTGs (LMTGs) of the
bilexical variety (L2MTGs). A bilexical MTG has
a set A of &amp;quot;delexicalized&amp;quot; nonterminal labels. Intu-
itively, A corresponds to the nonterminal set of an
ordinary CFG. Then, every nonterminal in N has
the form L[t] for some terminal t E T and some label
L E A.3 The terminal t is the lexical head of its
constituent, or just the head. One link on the RHS
of each L2MTG production serves as the heir of the
link on the LHS. Each component of the heir link in-
herits the lexical head of its parent nonterminal. An
example of a 2-L2MTG derivation is in Figure 1.
</bodyText>
<footnote confidence="0.956692333333333">
3The nonterminal e is always lexicalized with the ter-
minal E. Other nonterminals may also be lexicalized
with c to represent empty categories. The special start
nonterminal $ is lexicalized with the special start termi-
nal S. Following Eisner ..4z Satta (1999), we can then
define G so that the language of interest is actually
</footnote>
<equation confidence="0.9877005">
L&apos;(G)= {Q&apos; : Q&apos;$ E L(G)}.
(3)
S[S] ix [1, 2] (S[fed] $[S] 0.1 [1,2] (ix [1, 2, 3] ( Pro[I] V[fed] NP[cat] $[S]
S[S] [1, 2] S[kormil] $[S]) [1, 2] [1,3, 2] Pro[ya] V[kormil] NP[kota]) S[S])
0.1 [1, 2] (ix [1, 2,3] ( Pro[I] V[fed] 0.1 [1,2] (D[the] N[cat] $[S])
[1, 2] [1, 3, 2] Pro[ya] V[kormil] [1] e[e] N[kota]) ) S[S])
ix [1, 2] (ix [1, 2, 3] ( I fed ) (the cat )) $) 0.1 [1, 2] ( (I fed the cat) $) I fed the cat $
[1, 2] [1, 3, 2] ya kormil [1] E kota)) $ [1,2] (ya kota kormil) s) = ya kota kormil
</equation>
<figureCaption confidence="0.991933">
Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD
productions, followed by rendering.
</figureCaption>
<bodyText confidence="0.966878916666666">
Some subclasses and superclasses of MTG have
been studied before. The non-lexicalized class
2-MTG(2) is equivalent to ITG (Wu, 1997). Al-
shawi et al. (2000)&apos;s &amp;quot;collections of finite-state head
transducers&amp;quot; can be viewed as a subclass of 2-LMTG
where, among other restrictions, A contains only one
(dummy) nonterminal label. &amp;quot;Syntax-directed trans-
lations of order k&amp;quot; (Aho &amp; Ullman, 1969) are equiva-
lent to k-MTG(2). On the other hand, MTG is a sub-
class of Multiple CFG (Seki et al., 1991) where the
functions that render the RHS of production rules
may not mix symbols from different components.
</bodyText>
<sectionHeader confidence="0.987838" genericHeader="abstract">
3 Synchronous Parsers
</sectionHeader>
<bodyText confidence="0.999558526315789">
Inference of synchronous structures requires a syn-
chronous parser. A synchronous parser is an al-
gorithm that can infer the syntactic structure of each
component text in a multitext and simultaneously in-
fer the correspondence relation between these struc-
tures.4 To facilitate complexity analysis (below), we
specify our parsers using declarative inference rules.&apos;
&amp;quot;X :- Y, Z&amp;quot; means that X can be inferred from Y
and Z. V means the same thing. An item that
appears in an inference rule stands for the proposi-
tion that the item is in the parse chart. A produc-
tion rule that appears in an inference rule stands for
the proposition that the production is in the gram-
mar. Such specifications are nondeterministic: they
do not indicate the order in which a parser should
attempt inferences. A deterministic parsing strategy
can always be chosen later, to suit the application.
Any reasonable parsing strategy will have the same
asymptotic complexity (McAllester, 2002).
</bodyText>
<subsectionHeader confidence="0.996117">
3.1 Naive Synchronous Bilexical Parsers
</subsectionHeader>
<bodyText confidence="0.984165666666667">
For expository purposes, we begin with
Parser R2D2A, which is a naive CKY-style
chart parser for 2-L2MTG(2).6 The chart of
</bodyText>
<footnote confidence="0.994469333333333">
4A suitable set of monolingual parsers can also infer
the syntactic structure of each component, but cannot in-
fer the correspondence relation between these structures.
5We use both Horn clauses and sequents to save space.
6Parser R2D2A can be compared to Wu (1997)&apos;s pro-
cedure for parsing non-lexicalized ITGs, which runs in
</footnote>
<figure confidence="0.462554125">
—1 22 j1
j1 j2
h1 22 X7. [hl] Xl[hi]/Zi. [hi]
X2 [h2] X2 [h2]/Z2 [h2]
i2 —1
h2 j2
iz hook
seeds hedge
</figure>
<figureCaption confidence="0.995512">
Figure 2: Items used by our parsers for 2-L2MTG(2).
</figureCaption>
<bodyText confidence="0.999653">
Parser R2D2A is initialized with &amp;quot;seed&amp;quot; items,
illustrated in Figure 2. A one-dimensional seed is
put in the chart for every word in every component
of the input.
After initialization, the parser can
translational equivalence between seeds
components by firing Y inference rules:
</bodyText>
<equation confidence="0.8833725">
—1
i2-1 —1
hi
i2
</equation>
<bodyText confidence="0.9997215">
Y inference rules infer YIELD production rules.
Each two-dimensional instantiation expresses the
translational equivalence of two word tokens, h1
and h2, at positions i1 and i2 in their respective
components. One-dimensional Y inferences assert
that a word vanishes in translation. E.g.:
</bodyText>
<equation confidence="0.897936333333333">
i2 -1 6[6] i2 - 1h2 „E[f]
i2 X2 [h2] i2 &apos; 21_2 [(12] 122
• „ L
</equation>
<bodyText confidence="0.999754230769231">
Parser R2D2A spends most of its time compos-
ing pairs of non-seed items into larger items.7 A
bottom-up one-dimensional parser composes one-
dimensional items until it infers an item that cov-
ers the input text. A bottom-up synchronous parser
composes multi-dimensional items until it infers
an item that covers the multitext space spanned
by the input multitext. The items composed by
naive synchronous parsers are called hyperedges,
or hedges for short. The 2D hedges composed by
Parser R2D2A are shown in Figure 2. The particu-
lar hedge in the figure represents a constituent be-
tween word boundaries i1 ji of the first compo-
</bodyText>
<footnote confidence="0.82125675">
0(n6). As Eisner &amp; Satta (1999) have shown, yields of
bilexical grammars are generally more expensive to parse
than their nonlexicalized counterparts.
7The term item refers to any partial parse.
</footnote>
<figure confidence="0.999629954545455">
[pi.]
[p2]
assert the
in different
X1 [hl]
X2 [h2]
i2 —1 h2 Xi [hi] h1
7 X2 [h2] h2
22
j1
k1
i1
j1
k1
i1
j1
k1
i1
j1
k1
i1
i2
i2
i2
Z 1 h 1
Z h
2 2
Z h
1 1
Z h
2 2
i2
1
Y g
1
Y g
1 1
Y 2 g 2
Y 2 g 2
k2
k2
k2
k2
Y g
1
1
j2
Z h
1 1
Z h
2 2
Z h
1 1
Z h
2 2
Y 2 g 2
Y g
1 1
Y 2 g 2
j2
j2
j2
I go there quite often
3
2
4
1
I
J’
often
quite
go there
y
vais
1
2
J’
3
souvent
y vais
2
1
souvent
3
4
Pat went home early
Pat went
home early
ghar
Pat−nay
ghar Pat−nay juldee gayee
juldee
gayee
a gift for you from France
a gift
from France
for you
un cadeau de France pour vous
un
cadeau
de
France
pour
vous
rank with distinguished heir without
2 2 2
3 3 2
4 or 5 3 3
6 4 3
7 to 9 4 4
</figure>
<tableCaption confidence="0.64745525">
Table 1: Highest possible cardinality of minimizing de-
compositions over all 2D productions of the given rank.
Figures 6 and 5 exemplify highest-cardinality produc-
tions of ranks 3 and 4, respectively.
</tableCaption>
<bodyText confidence="0.99996">
As we shall see in Section 5, however, bad bina-
rization can worsen recognition complexity. The Bi-
narization Rules apply deterministically,&apos; but there
are multiple ways to decompose the RHS of a non-
binary DEPEND production into nested joins.11
Some decompositions may give rise to more discon-
tinuities than others. Let the cardinality of an
RTV be the total number of partitions in all its com-
ponents, and let the cardinality of a decomposi-
tion be the maximum cardinality of the RTVs that
it contains. A minimizing decomposition for a
given production is one of those with lowest cardi-
nality. Then, the cardinality of a production is
the cardinality of its minimizing decomposition. The
cardinality of a production is bounded by its rank,
as Table 1 shows for the 2D case. Finally, the car-
dinality C(G) of an MTG G is the maximum of
the cardinalities of its productions.
</bodyText>
<sectionHeader confidence="0.83512" genericHeader="keywords">
5 Inference of Discontinuous
Constituents
</sectionHeader>
<bodyText confidence="0.999665909090909">
Parser A is a parser for arbitrary MTGs. It initializes
its chart and fires Y inferences just like Parser R2A.
It then composes pairs of items into larger items us-
ing inference rule A.0 (see below). Just like items
in ordinary parsers, Parser A items need to know
their positions in the input multitext, but not their
internal structure. However, items with disconti-
nuities need to remember all their boundaries, not
just the outermost ones. Expanding on Johnson
(1985), we define a discontinuous span (or d-
span, for short) as a list of zero or more intervals
</bodyText>
<listItem confidence="0.910289666666667">
a = r,;. .; r,m), where
• the I, are left boundaries and the ri are right
boundaries between word positions in a text, so
that /, &lt;ri for 1 &lt;i &lt;m;
• ri &lt; 4+1 for 1 &lt; i &lt; m — 1, which means that
the intervals do not overlap.
</listItem>
<footnote confidence="0.968125166666667">
1°Given predefined equivalence classes for new nonter-
minals.
&amp;quot;For correct binarization of productions with a dis-
tinguished heir, the decomposition must put the heir in
the most deeply nested DLV. This requirement tends to
increase the cardinality of L2MTGs, as shown in Table 1.
</footnote>
<bodyText confidence="0.99985685">
In addition, we say that a d-span is in normal form
if all the inequalities between ri and 4+1 are strict,
i.e. there is a gap between each pair of consecutive
intervals. Now, a hedge item X(a) in Parser A is
a d-link X together with a vector of d-spans a in
normal form. The cardinality of an item is the
total number of intervals in its d-span vector.
Binarized MTG productions can be inferred un-
der generalizations of the ID and LP constraints de-
scribed in Section 3. We use two helper functions
to express these constraints. + is the concatena-
tion operator for d-spans: Given two d-spans, it out-
puts the union of their intervals in normal form.12
The 0 function computes the role template that de-
scribes the relative positions of the intervals in two
d-spans. E.g., if v = (1, 3; 8, 9) and a = (7,8), then
v + o- = (1, 3; 7, 9) and v 0 o- = [1], [2, 1]. Both op-
erators apply componentwise to vectors of d-spans.
With their help, we state the composition inference
rule of Parser A:
</bodyText>
<equation confidence="0.907879">
Y(v), Z(o-), X =N [v o-] (Y, Z)
X(v + o-)
</equation>
<bodyText confidence="0.957543621621622">
The space complexity of Parser A is a function
of the maximum number of boundaries stored in its
item signatures, and the number INI of nontermi-
nals in the grammar. The maximum number of re-
quired boundaries is exactly twice the cardinality of
the MTG, and each of the boundaries can range over
0(n) possible positions. Thus, the space complexity
of Parser A for an MTG G is in 0(1Ni D n2C (G)
) If G
is bilexical, then the number of possible nonterminals
hides a factor of nD , raising the space complexity of
Parser A to 0 (ID np +2C (G))
The time complexity of Parser A depends on
how many boundaries are shared between antecedent
items in A.0 rules. In the best case, all the bound-
aries are shared except the two outermost boundaries
in each dimension, and the inferred item is contigu-
ous. In the worst case, no boundaries are shared,
and the inferred item stores all the boundaries of
the antecedent items. In any case, if y and z are
the cardinalities of the composed items, and x is the
cardinality of the inferred item, then the number of
free boundaries in an A.0 inference is x + y + z.
Thus, in the worst case, the number of free bound-
aries involved in an A.0 inference is 3C(G). As be-
fore, each boundary can range over 0(n) possible
values, where n is the length of the longest compo-
nent of the input multitext. We still have 3 nonter-
minal labels per dimension per inference. Also, each
inference now needs to compute an RTV at a cost
12The inputs of ± must have no overlapping intervals,
or else the output is undefined.
A.C:
in 0 (C (G)). Thus, the time complexity of Parser A
is in 0 (C (G) INI3Dn3c(G)). For a binarized L2MTG,
which also needs to keep track of two lexical heads
per dimension per inference, this complexity rises to
</bodyText>
<equation confidence="0.770701">
0(C (G)13D n2D+3C(G)).
</equation>
<bodyText confidence="0.965354666666667">
Parser B is a generalization of Parser R2B for bi-
narized L2MTGs of arbitrary rank. It decomposes
inference rule A.0 into ID and LP subrules, using
generalized hooks that carry an RTV. The decompo-
sition can happen in one of two ways, depending on
the heir&apos;s role (1 or 2) in the DLV.
</bodyText>
<equation confidence="0.811939875">
Y[g](o-), X[h] =N [v o-] (Z[h], Y[g])
X[h] (o-)[v 0 c)-] \ Z [h]
Z[N(v), X[h](o-)[v 0 c)-] \ Z [h]
X[11](v + a)
Y[g](v), X[h] =N [v a] (Y[g], Z [II])
X[h](v)[v 0 o-]/Z[h]
Z[N(o-), X[h](v)[v 0 o-]/Z[h]
X[11](v + a)
</equation>
<bodyText confidence="0.999771">
The rules in Section 3.2 are simple examples of B.ID1
and B.LP1.
Parser B is faster than Parser A, but takes more
space. The hooks of Parser B must keep track of one
more nonterminal label per dimension than hedges.
The size of an RTV is bounded by the cardinal-
ity of the grammar. Thus, the space complexity of
Parser B is in 0 (C (G)12D np-F2C(G)N
) On the other
hand, The B.ID rules involve only one d-span in-
stead of two, reducing the number of free variables by
0 (C (G)) . The B.LP rules again involve only one lex-
ical head instead of two, reducing the number of free
variables by a factor of D. Since D &lt; C(G), it turns
out that the worst-case running time of Parser B is
less than that of Parser A by a factor of nD under
L2MTGs of any rank and dimensionality.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990272727273">
We have proposed Multitext Grammars (MTGs)
as a convenient and relatively expressive founda-
tion for building practical models of translational
equivalence. To encourage their use for this pur-
pose, we have explored algorithms for parsing bilex-
ical MTGs of arbitrary rank and dimensionality.
Our exploration highlighted some little-known prop-
erties of synchronous parsing: (1) some optimiza-
tions of monolingual parsers generalize to the syn-
chronous case, but others do not; (2) discontinuous
constituents are essential for parsing bitexts even in
similar Western languages; (3) different binarization
schemes lead to different time and space complexity.
There are many aspects of translational equiva-
lence that MTG cannot express, such as some of
those described by Dorr (1994). In future work,
we hope to extend the formalism to cover some of
the aspects that would not raise the computational
complexity of its recognition, such as discontinuous
and/or phrasal terminals. Concurrently, we shall ex-
plore the empirical properties of MTG, by inducing
stochastic MTGs from real multitexts.
</bodyText>
<sectionHeader confidence="0.995592" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989">
Thanks to Jason Eisner, Sanjeev Khudanpur, Owen
Rambow, Giorgio Satta, and members of NYU&apos;s Pro-
teus Project for helpful discussions. The idea of
treating binarization as an optimization problem is
due to Wei Wang. Dan Klein proposed the term
&amp;quot;hook.&amp;quot; This research was supported by the DARPA
TIDES program, by an NSF CAREER award, and
by a gift from Sun Microsystems.
</bodyText>
<sectionHeader confidence="0.998106" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702588235294">
A. Abeille, Y. Schabes, and A. Joshi (1990) &amp;quot;Using Lex-
icalized Tree Adjoining Grammars for Machine Trans-
lation,&amp;quot; Proceedings of COLING.
A. Aho and J. Ullman (1969) &amp;quot;Syntax Directed Transla-
tions and the Pushdown Assembler,&amp;quot; Journal of Com-
puter and System Sciences 3, 37-56.
H. Alshawi, S. Bangalore, and S. Douglas (2000) &amp;quot;Learn-
ing Dependency Translation Models as Collections of
Finite State Head Transducers,&amp;quot; Computational Lin-
guistics 26(445-60.
B. Dorr (1994) &amp;quot;Machine Translation Divergences: A
Formal Description and Proposed Solution,&amp;quot; Compu-
tational Linguistics, 20:4, 597-633.
J. Eisner and G. Satta (1999) &amp;quot;Efficient Parsing for Bilex-
ical Context-Free Grammars and Head-Automaton
Grammars,&amp;quot; Proceedings of the ACL.
M. Johnson (1985) &amp;quot;Parsing with Discontinuous Con-
stituents,&amp;quot; Proceedings of the ACL.
D. McAllester (2002) &amp;quot;On The Complexity Analysis of
Static Analyses&amp;quot; Journal of the ACM 49(4).
0. Rambow and G. Satta (1999) &amp;quot;Independent Paral-
lelism in Finite Copying Parallel Rewriting Systems,&amp;quot;
Theoretical Computer Science 223:87-120.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami (1991)
&amp;quot;On Multiple Context-Free Grammars,&amp;quot; Theoretical
Computer Science 88:191-229.
S. Shieber and Y. Schabes (1990) &amp;quot;Synchronous Tree-
Adjoining Grammars,&amp;quot; Proceedings of COLING.
S. Shieber, Y. Schabes, and F. Pereira (1995) &amp;quot;Principles
and Implementation of Deductive Parsing,&amp;quot; Journal of
Logic Programming 24:3-36.
D. Wu (1997) &amp;quot;Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora,&amp;quot; Com-
putational Linguistics 23(3):377-404.
</reference>
<figure confidence="0.963787">
B.ID1:
B.LP1:
B.ID2:
B.LP2:
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000583">
<note confidence="0.888717">Proceedings of HLT-NAACL 2003 Main Papers , pp. 79-86 Edmonton, May-June 2003</note>
<abstract confidence="0.984147677631579">translational equivalence between their components. link generated by a D-MTG has components. Some (but not all) components of a link may be empty. An empty component indicates that an expression vanishes in translation. To express empty we add a special terminal to special nonterminal MTG applications, the different components of a link will typically come largely disjoint subsets of vocabularies or sets of grammatical categories from different languages. Each MTG also has a set of production rules (or just &amp;quot;productions&amp;quot; for short), which fall into one of two categories.&apos; YIELD productions have the form t X is a link of and t is a link of is empty if and only if empty, &lt; D. productions have the form X=NPM (2) where M is a non-empty vector of nonterminal links, is a non-empty vector of and is a rendering function, explained The rank of an MTG production is the number of nonterminal links on its RHS. The rank of an MTG is the maximum rank of its production rules. is the class of MTGs of rank Each row of P and M corresponds to a different component of multitext. Each permutation is written as a row in P, and each link is written as a column M, as in Equation 3 below. If empty, then the dth component of every link in M must be empty If not empty, then at least one of the links in M must have a non-empty dth component. The position of a non-empty terminal or nonterminal relative to other non-empty elements of its component is its role. If there are m non-empty nonterminals component (row) M then a permutation roles from 1 to Pd empty if and only if is empty. The D-MTG derivation process begins with the link $, which is a vector of of the start symbol $ derivation continues with nondeterministic application of production rules. The semantics of = are the usual semantics of rewriting systems, i.e., that the expression on the LHS can be rewritten as the expression on the RHS. Following convention, we let be the reflexive transitive closure of dichotomy imposes a convenient normal form, without loss of generality. rendering function is a notational convenience; MTGs can be defined without it. When no more productions can be applied, i.e., when all nonterminals have been rewritten into terminals, the rendering functions are evaluated in order. The rearranges the nonempty terminals in each row of a link vector according to that row&apos;s permutation. For example, c [1,3,2,4] =wxyz t Env vut By reordering the terminals independently in each component, the join operator hides information about which terminals were derived from the same link. Thus, the translational equivalence represented by links is not observable in MTG yields, just as it is not observable in raw multitext. To avoid spurious ambiguity, we stipulate a normal form for components of P: In each permutation, first appearance of role precede the first of role y for all y, except where the arrangement is incompatible with a preceding permutation in P. We could, for example, obtain the result above if we put put and switch their roles in the 2nd and 3rd permutations. However, the normal form requires the 2nd to be [1, 3, 2, 4], not [4, 3, 2, 1], so must be listed last. Let Q be an MTG derivation where no more production rules can be applied. Let Render(Q) be the result of evaluating all the N&apos;s in Q. The (formal) an MTG the set of multithat can be generated by applying zto the link of then evaluating all the joins. I.e., = : $ z Due to the importance of lexical information in disambiguating syntactic structure, we shall pay special attention to lexicalized MTGs (LMTGs) of the variety A bilexical MTG has a set A of &amp;quot;delexicalized&amp;quot; nonterminal labels. Intuitively, A corresponds to the nonterminal set of an CFG. Then, every nonterminal in form some terminal some label The terminal the lexical head of its constituent, or just the head. One link on the RHS each production serves as the heir of the link on the LHS. Each component of the heir link inherits the lexical head of its parent nonterminal. An of a derivation is in Figure 1. nonterminal always lexicalized with the ternonterminals may also be lexicalized represent empty categories. The special start nonterminal $ is lexicalized with the special start terminal S. Following Eisner ..4z Satta (1999), we can then that the language of interest is actually : Q&apos;$ E (3) ix[1, 2] (S[fed] $[S] 0.1[1,2] [1, 2, 3] ( NP[cat] [1, 2] S[kormil] [1, 2] [1,3, 2] NP[kota]) S[S]) 0.1[1, 2] [1, 0.1[1,2] N[cat] 2] [1, 3, 2] Pro[ya] V[kormil] [1] ) S[S]) ix[1, 2] [1, 2, 3] ( I fed ) (the cat )) $) 0.1[1, 2] ( (I fed the cat) $) I fed the cat $ 2] [1, 3, 2] ya kormil [1] $ kota kormil) s) kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. and superclasses of MTG have been studied before. The non-lexicalized 2-MTG(2) is equivalent to ITG (Wu, 1997). Alal. &amp;quot;collections of finite-state head transducers&amp;quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &amp;quot;Syntax-directed transof order 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subof Multiple CFG (Seki al., where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synparser. A parser an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these struc- To facilitate complexity analysis (below), we specify our parsers using declarative inference rules.&apos; &amp;quot;X :- Y, Z&amp;quot; means that X can be inferred from Y and Z. V means the same thing. An item that appears in an inference rule stands for the proposition that the item is in the parse chart. A production rule that appears in an inference rule stands for the proposition that the production is in the grammar. Such specifications are nondeterministic: they do not indicate the order in which a parser should attempt inferences. A deterministic parsing strategy can always be chosen later, to suit the application. Any reasonable parsing strategy will have the same asymptotic complexity (McAllester, 2002). 3.1 Naive Synchronous Bilexical Parsers For expository purposes, we begin with Parser R2D2A, which is a naive CKY-style parser for The chart of suitable set of monolingual parsers can also infer the syntactic structure of each component, but cannot infer the correspondence relation between these structures. use both Horn clauses and sequents to save space. R2D2A can be compared to Wu (1997)&apos;s procedure for parsing non-lexicalized ITGs, which runs in —1 22 j1 j1 j2 X7. [hl] X2 [h2] X2 [h2]/Z2 [h2] i2 —1 iz hook seeds hedge Figure 2: Items used by our parsers for 2-L2MTG(2). R2D2A is with &amp;quot;seed&amp;quot; items, illustrated in Figure 2. A one-dimensional seed is put in the chart for every word in every component of the input. After initialization, the parser can translational equivalence between seeds components by firing Y inference rules: —1 —1 hi i2 inference rules infer rules. Each two-dimensional instantiation expresses the equivalence of two word tokens, at positions and in their respective components. One-dimensional Y inferences assert that a word vanishes in translation. E.g.: i2 6[6] i2 i2 X2 [h2] &apos; 21_2 [(12] 122 „ L R2D2A most of its time compospairs of non-seed items into larger A bottom-up one-dimensional parser composes onedimensional items until it infers an item that covers the input text. A bottom-up synchronous parser composes multi-dimensional items until it infers an item that covers the multitext space spanned by the input multitext. The items composed by synchronous parsers are called short. The 2D hedges composed by Parser R2D2A are shown in Figure 2. The particular hedge in the figure represents a constituent beword boundaries ji of the first compo- As Eisner &amp; Satta (1999) have shown, yields of bilexical grammars are generally more expensive to parse than their nonlexicalized counterparts. term to any partial parse. [pi.] [p2] assert in different X1 [hl] X2 [h2] i2 —1 h2 X2 [h2] h2 7 22 1h Z h 2 2 Z h 1 1 Z h 2 2 1 Y g 1 Y g 1 1 2g 2g Y g 1 1 Z h 1 1 Z h 2 2 Z h 1 1 Z h 2 2 2g Y g 1 1 2g I go there quite often 3 2 4 1 I J’ often quite go there y vais 1 2 3 souvent y vais 2 1 souvent 3 4 Pat went home early Pat went home early ghar Pat−nay ghar Pat−nay juldee gayee juldee gayee a gift for you from France a gift from France for you un cadeau de France pour vous un cadeau de France pour vous rank with distinguished heir without 2 2 2 3 3 2 4 or 5 3 3 6 4 3 7 to 9 4 4 Table 1: Highest possible cardinality of minimizing decompositions over all 2D productions of the given rank. Figures 6 and 5 exemplify highest-cardinality productions of ranks 3 and 4, respectively. As we shall see in Section 5, however, bad binarization can worsen recognition complexity. The Binarization Rules apply deterministically,&apos; but there are multiple ways to decompose the RHS of a non- DEPEND production into nested Some decompositions may give rise to more discontinuities than others. Let the cardinality of an RTV be the total number of partitions in all its components, and let the cardinality of a decomposithe cardinality of the RTVs that it contains. A minimizing decomposition for a given production is one of those with lowest cardi- Then, the cardinality of a production cardinality of its minimizing decomposition. cardinality of a production is bounded by its rank, as Table 1 shows for the 2D case. Finally, the caran MTG the maximum of the cardinalities of its productions. 5 Inference of Discontinuous Constituents Parser A is a parser for arbitrary MTGs. It initializes its chart and fires Y inferences just like Parser R2A. It then composes pairs of items into larger items using inference rule A.0 (see below). Just like items in ordinary parsers, Parser A items need to know their positions in the input multitext, but not their internal structure. However, items with discontinuities need to remember all their boundaries, not just the outermost ones. Expanding on Johnson (1985), we define a discontinuous span (or dshort) as a of zero or more intervals = .; where the left boundaries and the are right boundaries between word positions in a text, so /, for 1 • &lt; for 1 &lt; &lt; m which means that the intervals do not overlap. predefined equivalence classes for new nonterminals. &amp;quot;For correct binarization of productions with a distinguished heir, the decomposition must put the heir in the most deeply nested DLV. This requirement tends to increase the cardinality of L2MTGs, as shown in Table 1. In addition, we say that a d-span is in normal form all the inequalities between and are strict, i.e. there is a gap between each pair of consecutive Now, a hedge item Parser A is d-link with a vector of d-spans normal form. The cardinality of an item is the total number of intervals in its d-span vector. Binarized MTG productions can be inferred under generalizations of the ID and LP constraints described in Section 3. We use two helper functions to express these constraints. + is the concatenation operator for d-spans: Given two d-spans, it outthe union of their intervals in normal The 0 function computes the role template that describes the relative positions of the intervals in two E.g., if v = (1, 3; 8, 9) and = then + = (1, 3; 7, 9) and v 0 = [1], [2, 1]. Both operators apply componentwise to vectors of d-spans. With their help, we state the composition inference rule of Parser A: Y(v),X =N (Y,Z) + The space complexity of Parser A is a function of the maximum number of boundaries stored in its signatures, and the number INIof nonterminals in the grammar. The maximum number of required boundaries is exactly twice the cardinality of the MTG, and each of the boundaries can range over positions. Thus, the space complexity Parser A for an MTG in 0(1Ni (G) is bilexical, then the number of possible nonterminals a factor of , the space complexity of A to +2C (G)) The time complexity of Parser A depends on how many boundaries are shared between antecedent items in A.0 rules. In the best case, all the boundaries are shared except the two outermost boundaries in each dimension, and the inferred item is contiguous. In the worst case, no boundaries are shared, and the inferred item stores all the boundaries of antecedent items. In any case, if cardinalities of the composed items, and the cardinality of the inferred item, then the number of boundaries in an A.0 inference is + z. Thus, in the worst case, the number of free boundinvolved in an A.0 inference is beeach boundary can range over values, where n is the length of the longest component of the input multitext. We still have 3 nonterminal labels per dimension per inference. Also, each inference now needs to compute an RTV at a cost inputs of ± must have no overlapping intervals, or else the output is undefined. A.C: (C (G)). the time complexity of Parser A in (C (G) For a binarized which also needs to keep track of two lexical heads per dimension per inference, this complexity rises to (G)13D Parser B is a generalization of Parser R2B for biof arbitrary rank. It decomposes inference rule A.0 into ID and LP subrules, using generalized hooks that carry an RTV. The decomposition can happen in one of two ways, depending on the heir&apos;s role (1 or 2) in the DLV. X[h] Y[g]) Z [h] 0 \Z [h] + X[h] Z [II]) 0 X[h](v)[v + The rules in Section 3.2 are simple examples of B.ID1 and B.LP1. Parser B is faster than Parser A, but takes more space. The hooks of Parser B must keep track of one more nonterminal label per dimension than hedges. The size of an RTV is bounded by the cardinality of the grammar. Thus, the space complexity of B is in (C (G)12D ) On the other hand, The B.ID rules involve only one d-span instead of two, reducing the number of free variables by (C (G)) . B.LP rules again involve only one lexical head instead of two, reducing the number of free by a factor of &lt; turns out that the worst-case running time of Parser B is than that of Parser A by a factor of under of any rank and dimensionality. 6 Conclusion We have proposed Multitext Grammars (MTGs) as a convenient and relatively expressive foundation for building practical models of translational equivalence. To encourage their use for this purpose, we have explored algorithms for parsing bilexical MTGs of arbitrary rank and dimensionality. Our exploration highlighted some little-known properties of synchronous parsing: (1) some optimizations of monolingual parsers generalize to the synchronous case, but others do not; (2) discontinuous constituents are essential for parsing bitexts even in similar Western languages; (3) different binarization schemes lead to different time and space complexity. There are many aspects of translational equivalence that MTG cannot express, such as some of those described by Dorr (1994). In future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals. Concurrently, we shall explore the empirical properties of MTG, by inducing stochastic MTGs from real multitexts. Acknowledgments Thanks to Jason Eisner, Sanjeev Khudanpur, Owen Rambow, Giorgio Satta, and members of NYU&apos;s Proteus Project for helpful discussions. The idea of treating binarization as an optimization problem is due to Wei Wang. Dan Klein proposed the term &amp;quot;hook.&amp;quot; This research was supported by the DARPA TIDES program, by an NSF CAREER award, and by a gift from Sun Microsystems.</abstract>
<note confidence="0.740772458333333">References A. Abeille, Y. Schabes, and A. Joshi (1990) &amp;quot;Using Lexicalized Tree Adjoining Grammars for Machine Transof COLING. A. Aho and J. Ullman (1969) &amp;quot;Syntax Directed Translaand the Pushdown Assembler,&amp;quot; of Comand System Sciences 3, H. Alshawi, S. Bangalore, and S. Douglas (2000) &amp;quot;Learning Dependency Translation Models as Collections of State Head Transducers,&amp;quot; Lin- B. Dorr (1994) &amp;quot;Machine Translation Divergences: A Description and Proposed Solution,&amp;quot; Compu- Linguistics, 20:4, J. Eisner and G. Satta (1999) &amp;quot;Efficient Parsing for Bilexical Context-Free Grammars and Head-Automaton of the ACL. M. Johnson (1985) &amp;quot;Parsing with Discontinuous Conof the ACL. D. McAllester (2002) &amp;quot;On The Complexity Analysis of Analyses&amp;quot; of the ACM 49(4). 0. Rambow and G. Satta (1999) &amp;quot;Independent Parallelism in Finite Copying Parallel Rewriting Systems,&amp;quot; Computer Science H. Seki, T. Matsumura, M. Fujii, and T. Kasami (1991)</note>
<title confidence="0.9480925">Multiple Context-Free Grammars,&amp;quot; Science</title>
<author confidence="0.886315">S Shieber</author>
<author confidence="0.886315">Y Schabes Synchronous Tree-</author>
<affiliation confidence="0.619639">Grammars,&amp;quot; of COLING.</affiliation>
<note confidence="0.7989749">S. Shieber, Y. Schabes, and F. Pereira (1995) &amp;quot;Principles Implementation of Deductive Parsing,&amp;quot; of Programming D. Wu (1997) &amp;quot;Stochastic inversion transduction gramand bilingual parsing of parallel corpora,&amp;quot; Com- Linguistics B.ID1: B.LP1: B.ID2: B.LP2:</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeille</author>
<author>Y Schabes</author>
<author>A Joshi</author>
</authors>
<title>Using Lexicalized Tree Adjoining Grammars for Machine Translation,&amp;quot;</title>
<date>1990</date>
<booktitle>Proceedings of COLING.</booktitle>
<marker>Abeille, Schabes, Joshi, 1990</marker>
<rawString>A. Abeille, Y. Schabes, and A. Joshi (1990) &amp;quot;Using Lexicalized Tree Adjoining Grammars for Machine Translation,&amp;quot; Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aho</author>
<author>J Ullman</author>
</authors>
<title>Syntax Directed Translations and the Pushdown Assembler,&amp;quot;</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences</journal>
<volume>3</volume>
<pages>37--56</pages>
<contexts>
<context position="6035" citStr="Aho &amp; Ullman, 1969" startWordPosition="1096" endWordPosition="1099"> cat $ [1, 2] [1, 3, 2] ya kormil [1] E kota)) $ [1,2] (ya kota kormil) s) = ya kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. Some subclasses and superclasses of MTG have been studied before. The non-lexicalized class 2-MTG(2) is equivalent to ITG (Wu, 1997). Alshawi et al. (2000)&apos;s &amp;quot;collections of finite-state head transducers&amp;quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &amp;quot;Syntax-directed translations of order k&amp;quot; (Aho &amp; Ullman, 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subclass of Multiple CFG (Seki et al., 1991) where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synchronous parser. A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures.4 To facilitate complexity analysis (below), we specify our parsers using declarative inference rules.&apos; &amp;quot;X :- </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A. Aho and J. Ullman (1969) &amp;quot;Syntax Directed Translations and the Pushdown Assembler,&amp;quot; Journal of Computer and System Sciences 3, 37-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning Dependency Translation Models as Collections of Finite State Head Transducers,&amp;quot;</title>
<date>2000</date>
<journal>Computational Linguistics</journal>
<pages>26--445</pages>
<contexts>
<context position="5805" citStr="Alshawi et al. (2000)" startWordPosition="1061" endWordPosition="1065">a]) S[S]) 0.1 [1, 2] (ix [1, 2,3] ( Pro[I] V[fed] 0.1 [1,2] (D[the] N[cat] $[S]) [1, 2] [1, 3, 2] Pro[ya] V[kormil] [1] e[e] N[kota]) ) S[S]) ix [1, 2] (ix [1, 2, 3] ( I fed ) (the cat )) $) 0.1 [1, 2] ( (I fed the cat) $) I fed the cat $ [1, 2] [1, 3, 2] ya kormil [1] E kota)) $ [1,2] (ya kota kormil) s) = ya kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. Some subclasses and superclasses of MTG have been studied before. The non-lexicalized class 2-MTG(2) is equivalent to ITG (Wu, 1997). Alshawi et al. (2000)&apos;s &amp;quot;collections of finite-state head transducers&amp;quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &amp;quot;Syntax-directed translations of order k&amp;quot; (Aho &amp; Ullman, 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subclass of Multiple CFG (Seki et al., 1991) where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synchronous parser. A synchronous parser is an algorithm that can infer the syntactic struct</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas (2000) &amp;quot;Learning Dependency Translation Models as Collections of Finite State Head Transducers,&amp;quot; Computational Linguistics 26(445-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
</authors>
<title>Machine Translation Divergences: A Formal Description and Proposed Solution,&amp;quot;</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>597--633</pages>
<contexts>
<context position="17037" citStr="Dorr (1994)" startWordPosition="3120" endWordPosition="3121">rage their use for this purpose, we have explored algorithms for parsing bilexical MTGs of arbitrary rank and dimensionality. Our exploration highlighted some little-known properties of synchronous parsing: (1) some optimizations of monolingual parsers generalize to the synchronous case, but others do not; (2) discontinuous constituents are essential for parsing bitexts even in similar Western languages; (3) different binarization schemes lead to different time and space complexity. There are many aspects of translational equivalence that MTG cannot express, such as some of those described by Dorr (1994). In future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals. Concurrently, we shall explore the empirical properties of MTG, by inducing stochastic MTGs from real multitexts. Acknowledgments Thanks to Jason Eisner, Sanjeev Khudanpur, Owen Rambow, Giorgio Satta, and members of NYU&apos;s Proteus Project for helpful discussions. The idea of treating binarization as an optimization problem is due to Wei Wang. Dan Klein proposed the term &amp;quot;hook.&amp;quot; This research was suppo</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>B. Dorr (1994) &amp;quot;Machine Translation Divergences: A Formal Description and Proposed Solution,&amp;quot; Computational Linguistics, 20:4, 597-633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient Parsing for Bilexical Context-Free Grammars and Head-Automaton Grammars,&amp;quot;</title>
<date>1999</date>
<booktitle>Proceedings of the ACL.</booktitle>
<contexts>
<context position="9172" citStr="Eisner &amp; Satta (1999)" startWordPosition="1620" endWordPosition="1623">time composing pairs of non-seed items into larger items.7 A bottom-up one-dimensional parser composes onedimensional items until it infers an item that covers the input text. A bottom-up synchronous parser composes multi-dimensional items until it infers an item that covers the multitext space spanned by the input multitext. The items composed by naive synchronous parsers are called hyperedges, or hedges for short. The 2D hedges composed by Parser R2D2A are shown in Figure 2. The particular hedge in the figure represents a constituent between word boundaries i1 ji of the first compo0(n6). As Eisner &amp; Satta (1999) have shown, yields of bilexical grammars are generally more expensive to parse than their nonlexicalized counterparts. 7The term item refers to any partial parse. [pi.] [p2] assert the in different X1 [hl] X2 [h2] i2 —1 h2 Xi [hi] h1 7 X2 [h2] h2 22 j1 k1 i1 j1 k1 i1 j1 k1 i1 j1 k1 i1 i2 i2 i2 Z 1 h 1 Z h 2 2 Z h 1 1 Z h 2 2 i2 1 Y g 1 Y g 1 1 Y 2 g 2 Y 2 g 2 k2 k2 k2 k2 Y g 1 1 j2 Z h 1 1 Z h 2 2 Z h 1 1 Z h 2 2 Y 2 g 2 Y g 1 1 Y 2 g 2 j2 j2 j2 I go there quite often 3 2 4 1 I J’ often quite go there y vais 1 2 J’ 3 souvent y vais 2 1 souvent 3 4 Pat went home early Pat went home early ghar </context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta (1999) &amp;quot;Efficient Parsing for Bilexical Context-Free Grammars and Head-Automaton Grammars,&amp;quot; Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Parsing with Discontinuous Constituents,&amp;quot;</title>
<date>1985</date>
<journal>Journal of the ACM</journal>
<booktitle>Proceedings of the ACL. D. McAllester</booktitle>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="11591" citStr="Johnson (1985)" startWordPosition="2112" endWordPosition="2113">ase. Finally, the cardinality C(G) of an MTG G is the maximum of the cardinalities of its productions. 5 Inference of Discontinuous Constituents Parser A is a parser for arbitrary MTGs. It initializes its chart and fires Y inferences just like Parser R2A. It then composes pairs of items into larger items using inference rule A.0 (see below). Just like items in ordinary parsers, Parser A items need to know their positions in the input multitext, but not their internal structure. However, items with discontinuities need to remember all their boundaries, not just the outermost ones. Expanding on Johnson (1985), we define a discontinuous span (or dspan, for short) as a list of zero or more intervals a = r,;. .; r,m), where • the I, are left boundaries and the ri are right boundaries between word positions in a text, so that /, &lt;ri for 1 &lt;i &lt;m; • ri &lt; 4+1 for 1 &lt; i &lt; m — 1, which means that the intervals do not overlap. 1°Given predefined equivalence classes for new nonterminals. &amp;quot;For correct binarization of productions with a distinguished heir, the decomposition must put the heir in the most deeply nested DLV. This requirement tends to increase the cardinality of L2MTGs, as shown in Table 1. In add</context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>M. Johnson (1985) &amp;quot;Parsing with Discontinuous Constituents,&amp;quot; Proceedings of the ACL. D. McAllester (2002) &amp;quot;On The Complexity Analysis of Static Analyses&amp;quot; Journal of the ACM 49(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rambow</author>
<author>G Satta</author>
</authors>
<title>Independent Parallelism in Finite Copying Parallel Rewriting Systems,&amp;quot;</title>
<date>1999</date>
<journal>Theoretical Computer Science</journal>
<pages>223--87</pages>
<marker>Rambow, Satta, 1999</marker>
<rawString>0. Rambow and G. Satta (1999) &amp;quot;Independent Parallelism in Finite Copying Parallel Rewriting Systems,&amp;quot; Theoretical Computer Science 223:87-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seki</author>
<author>T Matsumura</author>
<author>M Fujii</author>
<author>T Kasami</author>
</authors>
<title>On Multiple Context-Free Grammars,&amp;quot;</title>
<date>1991</date>
<journal>Theoretical Computer Science</journal>
<pages>88--191</pages>
<contexts>
<context position="6136" citStr="Seki et al., 1991" startWordPosition="1117" endWordPosition="1120">A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. Some subclasses and superclasses of MTG have been studied before. The non-lexicalized class 2-MTG(2) is equivalent to ITG (Wu, 1997). Alshawi et al. (2000)&apos;s &amp;quot;collections of finite-state head transducers&amp;quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &amp;quot;Syntax-directed translations of order k&amp;quot; (Aho &amp; Ullman, 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subclass of Multiple CFG (Seki et al., 1991) where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synchronous parser. A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures.4 To facilitate complexity analysis (below), we specify our parsers using declarative inference rules.&apos; &amp;quot;X :- Y, Z&amp;quot; means that X can be inferred from Y and Z. V means the same thing. An item that appears in an i</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>H. Seki, T. Matsumura, M. Fujii, and T. Kasami (1991) &amp;quot;On Multiple Context-Free Grammars,&amp;quot; Theoretical Computer Science 88:191-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>Y Schabes</author>
</authors>
<title>Synchronous TreeAdjoining Grammars,&amp;quot;</title>
<date>1990</date>
<booktitle>Proceedings of COLING.</booktitle>
<marker>Shieber, Schabes, 1990</marker>
<rawString>S. Shieber and Y. Schabes (1990) &amp;quot;Synchronous TreeAdjoining Grammars,&amp;quot; Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>Y Schabes</author>
<author>F Pereira</author>
</authors>
<title>Principles and Implementation of Deductive Parsing,&amp;quot;</title>
<date>1995</date>
<journal>Journal of Logic Programming</journal>
<pages>24--3</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>S. Shieber, Y. Schabes, and F. Pereira (1995) &amp;quot;Principles and Implementation of Deductive Parsing,&amp;quot; Journal of Logic Programming 24:3-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora,&amp;quot;</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<pages>23--3</pages>
<contexts>
<context position="5782" citStr="Wu, 1997" startWordPosition="1059" endWordPosition="1060">mil] NP[kota]) S[S]) 0.1 [1, 2] (ix [1, 2,3] ( Pro[I] V[fed] 0.1 [1,2] (D[the] N[cat] $[S]) [1, 2] [1, 3, 2] Pro[ya] V[kormil] [1] e[e] N[kota]) ) S[S]) ix [1, 2] (ix [1, 2, 3] ( I fed ) (the cat )) $) 0.1 [1, 2] ( (I fed the cat) $) I fed the cat $ [1, 2] [1, 3, 2] ya kormil [1] E kota)) $ [1,2] (ya kota kormil) s) = ya kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. Some subclasses and superclasses of MTG have been studied before. The non-lexicalized class 2-MTG(2) is equivalent to ITG (Wu, 1997). Alshawi et al. (2000)&apos;s &amp;quot;collections of finite-state head transducers&amp;quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &amp;quot;Syntax-directed translations of order k&amp;quot; (Aho &amp; Ullman, 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subclass of Multiple CFG (Seki et al., 1991) where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synchronous parser. A synchronous parser is an algorithm that can inf</context>
<context position="7656" citStr="Wu (1997)" startWordPosition="1364" endWordPosition="1365">s. A deterministic parsing strategy can always be chosen later, to suit the application. Any reasonable parsing strategy will have the same asymptotic complexity (McAllester, 2002). 3.1 Naive Synchronous Bilexical Parsers For expository purposes, we begin with Parser R2D2A, which is a naive CKY-style chart parser for 2-L2MTG(2).6 The chart of 4A suitable set of monolingual parsers can also infer the syntactic structure of each component, but cannot infer the correspondence relation between these structures. 5We use both Horn clauses and sequents to save space. 6Parser R2D2A can be compared to Wu (1997)&apos;s procedure for parsing non-lexicalized ITGs, which runs in —1 22 j1 j1 j2 h1 22 X7. [hl] Xl[hi]/Zi. [hi] X2 [h2] X2 [h2]/Z2 [h2] i2 —1 h2 j2 iz hook seeds hedge Figure 2: Items used by our parsers for 2-L2MTG(2). Parser R2D2A is initialized with &amp;quot;seed&amp;quot; items, illustrated in Figure 2. A one-dimensional seed is put in the chart for every word in every component of the input. After initialization, the parser can translational equivalence between seeds components by firing Y inference rules: —1 i2-1 —1 hi i2 Y inference rules infer YIELD production rules. Each two-dimensional instantiation expre</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu (1997) &amp;quot;Stochastic inversion transduction grammars and bilingual parsing of parallel corpora,&amp;quot; Computational Linguistics 23(3):377-404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>