<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.994847">
An extractive supervised two-stage method for sentence compression
</title>
<author confidence="0.997757">
Dimitrios Galanis* and Ion Androutsopoulos*+
</author>
<affiliation confidence="0.941768">
*Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit – IMIS, Research Centre “Athena”, Greece
</affiliation>
<sectionHeader confidence="0.95583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872">
We present a new method that compresses
sentences by removing words. In a first stage,
it generates candidate compressions by re-
moving branches from the source sentence’s
dependency tree using a Maximum Entropy
classifier. In a second stage, it chooses the
best among the candidate compressions using
a Support Vector Machine Regression model.
Experimental results show that our method
achieves state-of-the-art performance without
requiring any manually written rules.
</bodyText>
<sectionHeader confidence="0.995164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998023127659575">
Sentence compression is the task of producing a
shorter form of a single given sentence, so that the
new form is grammatical and retains the most im-
portant information of the original one (Jing, 2000).
Sentence compression is valuable in many applica-
tions, for example when displaying texts on small
screens (Corston-Oliver, 2001), in subtitle genera-
tion (Vandeghinste and Pan, 2004), and in text sum-
marization (Madnani et al., 2007).
People use various methods to shorten sentences,
including word or phrase removal, using shorter
paraphrases, and common sense knowledge. How-
ever, reasonable machine-generated sentence com-
pressions can often be obtained by only remov-
ing words. We use the term extractive to refer to
methods that compress sentences by only removing
words, as opposed to abstractive methods, where
more elaborate transformations are also allowed.
Most of the existing compression methods are ex-
tractive (Jing, 2000; Knight and Marcu, 2002; Mc-
Donald, 2006; Clarke and Lapata, 2008; Cohn and
Lapata, 2009). Although abstractive methods have
also been proposed (Cohn and Lapata, 2008), and
they may shed more light on how people compress
sentences, they do not always manage to outperform
extractive methods (Nomoto, 2009). Hence, from an
engineering perspective, it is still important to inves-
tigate how extractive methods can be improved.
In this paper, we present a new extractive sentence
compression method that relies on supervised ma-
chine learning.1 In a first stage, the method gener-
ates candidate compressions by removing branches
from the source sentence’s dependency tree using a
Maximum Entropy classifier (Berger et al., 2006). In
a second stage, it chooses the best among the candi-
date compressions using a Support Vector Machine
Regression (SVR) model (Chang and Lin, 2001). We
show experimentally that our method compares fa-
vorably to a state-of-the-art extractive compression
method (Cohn and Lapata, 2007; Cohn and Lapata,
2009), without requiring any manually written rules,
unlike other recent work (Clarke and Lapata, 2008;
Nomoto, 2009). In essence, our method is a two-
tier over-generate and select (or rerank) approach to
sentence compression; similar approaches have been
adopted in natural language generation and parsing
(Paiva and Evans, 2005; Collins and Koo, 2005).
</bodyText>
<sectionHeader confidence="0.99853" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.993629333333333">
Knight and Marcu (2002) presented a noisy channel
sentence compression method that uses a language
model P(y) and a channel model P(xly), where x
</bodyText>
<footnote confidence="0.7108635">
1An implementation of our method will be freely available
from http://nlp.cs.aueb.gr/software.html
</footnote>
<note confidence="0.836101333333333">
885
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999831543478261">
is the source sentence and y the compressed one.
P(x|y) is calculated as the product of the proba-
bilities of the parse tree tranformations required to
expand y to x. The best compression of x is the
one that maximizes P(x|y) · P(y), and it is found
using a noisy channel decoder. In a second, alter-
native method Knight and Marcu (2002) use a tree-
to-tree transformation algorithm that tries to rewrite
directly x to the best y. This second method uses
C4.5 (Quinlan, 1993) to learn when to perform tree
rewriting actions (e.g., dropping subtrees, combin-
ing subtrees) that transform larger trees to smaller
trees. Both methods were trained and tested on
data from the Ziff-Davis corpus (Knight and Marcu,
2002), and they achieved very similar grammatical-
ity and meaning preservation scores, with no statis-
tically significant difference. However, their com-
pression rates (counted in words) were very dif-
ferent: 70.37% for the noisy-channel method and
57.19% for the C4.5-based one.
McDonald (2006) ranks each candidate compres-
sion using a function based on the dot product of a
vector of weights with a vector of features extracted
from the candidate’s n-grams, POS tags, and depen-
dency tree. The weights were learnt from the Ziff-
Davis corpus. The best compression is found us-
ing a Viterbi-like algorithm that looks for the best
sequence of source words that maximizes the scor-
ing function. The method outperformed Knight and
Marcu’s (2002) tree-to-tree method in grammatical-
ity and meaning preservation on data from the Ziff-
Davis corpus, with a similar compression rate.
Clarke and Lapata (2008) presented an unsuper-
vised method that finds the best compression using
Integer Linear Programming (ILP). The ILP obejc-
tive function takes into account a language model
that indicates which n-grams are more likely to be
deleted, and a significance model that shows which
words of the input sentence are important. Man-
ually defined constraints (in effect, rules) that op-
erate on dependency trees indicate which syntactic
constituents can be deleted. This method outper-
formed McDonald’s (2006) in grammaticality and
meaning preservation on test sentences from Edin-
burgh’s “written” and “spoken” corpora.2 However,
the compression rates of the two systems were dif-
</bodyText>
<footnote confidence="0.766511">
2See http://homepages.inf.ed.ac.uk/s0460084/data/.
</footnote>
<bodyText confidence="0.99996902631579">
ferent (72.0% vs. 63.7% for McDonald’s method,
both on the written corpus).
We compare our method against Cohn and Lap-
ata’s T3 system (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art extractive sentence
compression system that learns parse tree transduc-
tion operators from a parallel extractive corpus of
source-compressed trees. T3 uses a chart-based de-
coding algorithm and a Structured Support Vector
Machine (Tsochantaridis et al., 2005) to learn to
select the best compression among those licensed
by the operators learnt.3 T3 outperformed McDon-
ald’s (2006) system in grammaticality and meaning
preservation on Edinburgh’s “written” and “spoken”
corpora, achieving comparable compression rates
(Cohn and Lapata, 2009). Cohn and Lapata (2008)
have also developed an abstractive version of T3,
which was reported to outperform the original, ex-
tractive T3 in meaning preservation; there was no
statistically significant difference in grammaticality.
Finally, Nomoto (2009) presented a two-stage ex-
tractive method. In the first stage, candidate com-
pressions are generated by chopping the source sen-
tence’s dependency tree. Many ungrammatical com-
pressions are avoided using hand-crafted drop-me-
not rules for dependency subtrees. The candidate
compressions are then ranked using a function that
takes into account the inverse document frequen-
cies of the words, and their depths in the source
dependency tree. Nomoto’s extractive method was
reported to outperform Cohn and Lapata’s abstrac-
tive version of T3 on a corpus collected via RSS
feeds. Our method is similar to Nomoto’s, in that
it uses two stages, one that chops the source depen-
dency tree generating candidate compressions, and
one that ranks the candidates. However, we experi-
mented with more elaborate ranking models, and our
method does not employ any manually crafted rules.
</bodyText>
<sectionHeader confidence="0.992473" genericHeader="method">
3 Our method
</sectionHeader>
<bodyText confidence="0.869149285714286">
As already mentioned, our method first generates
candidate compressions, which are then ranked. The
candidate compressions generator operates by re-
moving branches from the dependency tree of the
3T3 appears to be the only previous sentence compres-
sion method whose implementation is publicly available; see
http://www.dcs.shef.ac.uk/∼tcohn/t3/.
</bodyText>
<page confidence="0.882184">
886
</page>
<bodyText confidence="0.9997206">
input sentence (figure 1); this stage is discussed in
section 3.1 below. We experimented with different
ranking functions, discussed in section 3.2, which
use features extracted from the source sentence s
and the candidate compressions ci, ... , ck.
</bodyText>
<subsectionHeader confidence="0.999497">
3.1 Generating candidate compressions
</subsectionHeader>
<bodyText confidence="0.992565608695652">
Our method requires a parallel training corpus con-
sisting of sentence-compression pairs (s, g). The
compressed sentences g must have been formed by
only deleting words from the corresponding source
sentences s. The (s, g) training pairs are used to es-
timate the propability that a dependency edge e of a
dependency tree T3 of an input sentence s is retained
or not in the dependency tree Tg of the compressed
sentence g. More specifically, we want to estimate
the probabilities P(Xi|context(ei)) for every edge
ei of T3, where Xi is a variable that can take one
of the following three values: not del, for not delet-
ing ei; del u for deleting ei along with its head; and
del l for deleting e along with its modifier. The head
(respectively, modifier) of ei is the node ei originates
from (points to) in the dependency tree. context(ei)
is a set of features that represents ei’s local context
in T3, as well as the local context of the head and
modifier of ei in s.
The propabilities above can be estimated using
the Maximum Entropy (ME) framework (Berger et
al., 2006), a method for learning the distribution
P(X|V ) from training data, where X is discrete-
</bodyText>
<listItem confidence="0.807439181818182">
valued variable and V = (Vi, ... , Vn) is a real or
discrete-valued vector. Here, V = context(ei) and
X = Xi. We use the following features in V :
• The label of the dependency edge ei, as well as
the POS tag of the head and modifier of ei.
• The entire head-label-modifier triple of ei. This
feature overlaps with the previous two features,
but it is common in ME models to use feature
combinations as additional features, since they
may indicate a category more strongly than the
individual initial features.4
• The POS tag of the father of ei’s head, and the
label of the dependency that links the father to
ei’s head.
4http://nlp.stanford.edu/pubs/maxent-tutorial-slides.pdf.
• The POS tag of each one of the three previous
and the three following words of ei’s head and
modifier in s (12 features).
• The POS tag bi-grams of the previous two and
the following two words of ei’s head and mod-
ifier in s (4 features).
• Binary features that show which of the possible
</listItem>
<bodyText confidence="0.896224605263158">
labels occur (or not) among the labels of the
edges that have the same head as ei in T3 (one
feature for each possible dependency label).
• Two binary features that show if the subtree
rooted at the modifier of ei or ei’s uptree (the
rest of the tree, when ei’s subtree is removed)
contain an important word. A word is consid-
ered important if it appears in the document s
was drawn from significantly more often than
in a background corpus. In summarization,
such words are called signature terms and are
thought to be descriptive of the input; they can
be identified using the log-likelihood ratio A of
each word (Lin and Hovy, 2000; Gupta et al.,
2007).
For each dependency edge ei of a source training
sentence s, we create a training vector V with the
above features. If ei is retained in the dependency
tree of the corresponding compressed sentence g in
the corpus, V is assigned the category not del. If
ei is not retained, it is assigned the category del l
or del u, depending on whether the head (as in the
ccomp of “said” in Figure 1) or the modifier (as in
the dobj of “attend”) of ei has also been removed.
When the modifier of an edge is removed, the entire
subtree rooted at the modifier is removed, and simi-
larly for the uptree, when the head is removed. We
do not create training vectors for the edges of the
removed subtree of a modifier or the edges of the
removed uptree of a head.
Given an input sentence s and its dependency tree
T3, the candidate compressions generator produces
the candidate compressed sentences ci, ... , cn by
deleting branches of T3 and putting the remaining
words of the dependency tree in the same order as in
s. The candidates ci, ... , cn correspond to possible
assignments of values to the Xi variables (recall that
Xi = not del|del l|del u) of the edges ei of T3.
</bodyText>
<figure confidence="0.985971367346939">
887
source: gold:
said attend
ns$4bj Bux�,
he
Mother
will
attend
k+k+k
+
k+k
hearing
+k+�+
will
82
on
Mother
superior
�
pobj�&amp;quot;
measure
�
det���#
the
�� ���
num � num ��� ��$ amod �����*
Catherine
mother
82
superior
Friday
�
ccomp
� nsubj
y
�� k+k+k+k+k+
j*j*
nusbj
� aux �prep
��% dobj *�*j*j
�� ����
num num ��
� ��% amod �����*
Catherine
measure det�
�
mother the
det�
the
</figure>
<figureCaption confidence="0.94139625">
Figure 1: Dependency trees of a source sentence and its compression by a human (taken from Edinburgh’s “written”
corpus). The source sentence is: “Mother Catherine, 82, the mother superior, will attend the hearing on Friday, he
said.” The compressed one is: “Mother Catherine, 82, the mother superior, will attend.” Deleted edges and words are
shown curled and underlined, respectively.
</figureCaption>
<bodyText confidence="0.999995088235294">
Hence, there are at most 3&apos;−1 candidate compres-
sions, where m is the number of words in s. This
is a large number of candidates, even for modestly
long input sentences. In practice, the candidates are
fewer, because del l removes an entire subtree and
del u an entire uptree, and we do not need to make
decisions Xz about the edges of the deleted subtrees
and uptrees. To reduce the number of candidates
further, we ignore possible assignments that contain
decisions Xz = x to which the ME model assigns
probabilities below a threshold t; i.e., the ME model
is used to prune the space of possible assignments.
When generating the possible assignments to the
Xz variables, we examine the edges ez of T3 in a
top-down breadth-first manner. In the source tree of
Figure 1, for example, we first consider the edges
of “said”; the left-to-right order is random, but let
us assume that we consider first the ccomp edge.
There are three possible actions: retain the edge
(not del), remove it along with the head “said”
(del u), or remove it along with the modifier “at-
tend” and its subtree (del l). If the ME model assigns
a low probability to one of the three actions, that ac-
tion is ignored. For each one of the (remaining) ac-
tions, we obtain a new form of T3, and we continue
to consider its (other) edges. We process the edges
in a top-down fashion, because the ME model allows
del l actions much more often than del u actions,
and when del l actions are performed near the root
of T3, they prune large parts of the space of possible
assignments to the Xz variables. Some of the candi-
date compressions that were generated for an input
sentence by setting t = 0.2 are shown in Table 1,
along with the gold (human-authored) compression.
</bodyText>
<subsectionHeader confidence="0.999697">
3.2 Ranking candidate compressions
</subsectionHeader>
<bodyText confidence="0.99997275">
Given that we now have a method that generates
candidate compressions ci, ... , ck for a sentence s,
we need a function F (cz s) that will rank the candi-
date compressions. Many of them are ungrammat-
ical and/or do not convey the most important infor-
mation of s. F (cz s) should help us select a short
candidate that is grammatical and retains the most
important information of s.
</bodyText>
<subsubsectionHeader confidence="0.502284">
3.2.1 Grammaticality and importance rate
</subsubsectionHeader>
<bodyText confidence="0.999970666666667">
A simple way to rank the candidate compressions
is to assign to each one a score intended to measure
its grammaticality and importance rate. By gram-
maticality, Gramm(cz), we mean how grammati-
cally well-formed candidate cz is. A common way
to obtain such a measure is to use an n-gram lan-
</bodyText>
<page confidence="0.589182">
888
</page>
<table confidence="0.686347714285714">
s: Then last week a second note, in the same handwriting, informed Mrs Allan that the search was
on the wrong side of the bridge.
g: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c1: Last week a second note informed Mrs Allan that the search was on the side.
c2: Last week a second note informed Mrs Allan that the search was.
c3: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c4: Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge.
</table>
<tableCaption confidence="0.997912">
Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1, ... , c4.
</tableCaption>
<bodyText confidence="0.999658375">
guage model trained on a large background corpus.
However, language models tend to assign smaller
probabilities to longer sentences; therefore they fa-
vor short sentences, but not necessarily the most ap-
propriate compressions. To overcome this problem,
we follow Cordeiro et al. (2009) and normalize the
score of a trigram language model as shown below,
where w1, ... , wm are the words of candidate ci.
</bodyText>
<equation confidence="0.99926425">
Gramm(ci) = log PLM(ci)1/m =
m
(1/m) • log( P(wj|wj−1, wj−2)) (1)
j=1
</equation>
<bodyText confidence="0.98244725">
The importance rate ImpRate(ci|s), defined be-
low, estimates how much information of the original
sentence s is retained in candidate ci. tf(wi) is the
term frequency of wi in the document that contained
� (� = ci, s), and idf(wi) is the inverse document
frequency of wi in a background corpus. We actu-
ally compute idf(wi) only for nouns and verbs, and
set idf(wi) = 0 for other words.
</bodyText>
<equation confidence="0.996820666666667">
ImpRate(ci|s) = Imp(ci)/Imp(s) (2)
Imp(ξ) = � tf(wi) • idf(wi) (3)
wiEξ
</equation>
<bodyText confidence="0.976409">
The ranking F(c|s) is then defined as a linear
combination of grammaticality and importance rate:
</bodyText>
<equation confidence="0.881484">
F(ci|s) = A • Gramm(ci) + (1 − A) �
• ImpRate(ci|s) − α • CR(ci|s) (4)
</equation>
<bodyText confidence="0.999978428571428">
A compression rate penalty factor CR(ci|s) =
|c|/|s |is included, to bias our method towards gen-
erating shorter or longer compressions;  |•  |denotes
word length in words (punctuation is ignored). We
explain how the weigths A, α are tuned in following
sections. We call LM-IMP the configuration of our
method that uses the ranking function of equation 4.
</bodyText>
<subsectionHeader confidence="0.94766">
3.2.2 Support Vector Regression
</subsectionHeader>
<bodyText confidence="0.999645909090909">
A more sophisticated way to select the best com-
pression is to train a Support Vector Machines Re-
gression (SVR) model to assign scores to feature vec-
tors, with each vector representing a candidate com-
pression. SVR models (Chang and Lin, 2001) are
trained using l training vectors (x1, y1), ... , (xl, yl),
where xi E Rn and yi E R, and learn a function
f : Rn —* R that generalizes the training data. In
our case, xi is a feature vector representing a candi-
date compression ci, and yi is a score indicating how
good a compression ci is. We use 98 features:
</bodyText>
<listItem confidence="0.808458846153846">
• Gramm(ci) and ImpRate(ci|s), as above.
• 2 features indicating the ratio of important and
unimportant words of s, identified as in section
3.1, that were deleted.
• 2 features that indicate the average depth of
the deleted and not deleted words in the depen-
dency tree of s.
• 92 features that indicate which POS tags appear
in s and how many of them were deleted in ci.
For every POS tag label, we use two features,
one that shows how many POS tags of that la-
bel are contained in s and one that shows how
many of these POS tags were deleted in ci.
</listItem>
<bodyText confidence="0.99906125">
To assign a regression score yi to each training
vector xi, we experimented with the following func-
tions that measure how similar ci is to the gold com-
pression g, and how grammatical ci is.
</bodyText>
<listItem confidence="0.737323">
• Grammatical relations overlap: In this case, yi
</listItem>
<bodyText confidence="0.7496805">
is the F1-score of the dependencies of ci against
those of the gold compression g. This measure
has been shown to correlate well with human
judgements (Clarke and Lapata, 2006). As in
</bodyText>
<page confidence="0.851179">
889
</page>
<bodyText confidence="0.996007">
the ranking function of section 3.2.1, we add a
compression rate penalty factor.
</bodyText>
<equation confidence="0.999858">
yi = F1(d(ci)), d(g)) − a • CR(ci|s) (5)
</equation>
<bodyText confidence="0.995551666666667">
d(•) denotes the set of dependencies. We call
SVR-F1 the configuration of our system that
uses equation 5 to rank the candidates.
</bodyText>
<listItem confidence="0.914993214285714">
• Tokens accuracy and grammaticality: Tokens
accuracy, TokAcc(ci|s, g), is the percentage of
tokens of s that were correctly retained or re-
moved in ci; a token was correctly retained
or removed, if it was also retained (or re-
moved) in the gold compression g. To cal-
culate TokAcc(ci|s, g), we need the word-to-
word alignment of s to g, and s to ci. These
alignments were obtained as a by-product of
computing the corresponding (word) edit dis-
tances. We also want the regression model to
favor grammatical compressions. Hence, we
use a linear combination of tokens accuracy
and grammaticality of ci:
</listItem>
<equation confidence="0.99919">
yi = A • TokAcc(ci|s, g) +
(1 − A) • Gramm(ci) − a • CR(ci|s) (6)
</equation>
<bodyText confidence="0.999882">
Again, we add a compression rate penalty, to
be able to generate shorter or longer compres-
sions. We call SVR-TOKACC-LM the config-
uration of our system that uses equation 6.
</bodyText>
<sectionHeader confidence="0.644946" genericHeader="method">
4 Baseline and T3
</sectionHeader>
<bodyText confidence="0.999992173913044">
As a baseline, we use a simple algorithm based on
the ME classifier of section 3.1. The baseline pro-
duces a single compression c for every source sen-
tence s by considering sequentially the edges ei of
s’s dependency tree in a random order, and perform-
ing at each ei the single action (not del, del u, or
del l) that the ME model considers more probable;
the words of the chopped dependency tree are then
put in the same order as in s. We call this system
Greedy-Baseline.
We compare our method against the extractive
version of T3 (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art sentence compres-
sion system that applies sequences of transduction
operators to the syntax trees of the source sentences.
The available tranduction operators are learnt from
the syntax trees of a set of source-gold pairs. Ev-
ery operator transforms a subtree a to a subtree -y,
rooted at symbols X and Y , respectively.
To find the best sequence of transduction opera-
tors that can be applied to a source syntax tree, a
chart-based dynamic programming decoder is used,
which finds the best scoring sequence q*:
</bodyText>
<equation confidence="0.8815915">
q* = arg max score(q; w) (7)
q
</equation>
<bodyText confidence="0.999773">
where score(q; w) is the dot product (&apos;F(q), w).
IF(q) is a vector-valued feature function, and w is a
vector of weights learnt using a Structured Support
Vector Machine (Tsochantaridis et al., 2005).
IF(q) consists of: (i) the log-probability of the re-
sulting candidate, as returned by a tri-gram language
model; and (ii) features that describe how the opera-
tors of q are applied, for example the number of the
terminals in each operator’s a and -y subtrees, the
POS tags of the X and Y roots of a and -y etc.
</bodyText>
<sectionHeader confidence="0.997305" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999966454545455">
We used Stanford’s parser (de Marneffe et al., 2006)
and ME classifier (Manning et al., 2003).5 For
the (trigram) language model, we used SRILM with
modified Kneser-Ney smoothing (Stolcke, 2002).6
The language model was trained on approximately
4.5 million sentences of the TIPSTER corpus. To
obtain idf(wi) values, we used approximately 19.5
million verbs and nouns from the TIPSTER corpus.
T3 requires the syntax trees of the source-gold
pairs in Penn Treebank format, as well as a trigram
language model. We obtained T3’s trees using Stan-
ford’s parser, as in our system, unlike Cohn and La-
pata (2009) that use Bikel’s (2002) parser. The lan-
guage models in T3 and our system are trained on
the same data and with the same options used by
Cohn and Lapata (2009). T3 also needs a word-to-
word alignment of the source-gold pairs, which was
obtained by computing the edit distance, as in Cohn
and Lapata (2009) and SVR-TOKACC-LM.
We used Edinburgh’s “written” sentence com-
pression corpus (section 2), which consists of
source-gold pairs (one gold compression per source
</bodyText>
<footnote confidence="0.999517">
5Both available from http://nlp.stanford.edu/.
6See http://www.speech.sri.com/projects/srilm/.
</footnote>
<page confidence="0.871781">
890
</page>
<bodyText confidence="0.999443833333333">
sentence). The gold compressions were created by
deleting words. We split the corpus in 3 parts: 1024
training, 324 development, and 291 testing pairs.
perfomance of the two SVR configurations might be
improved further by using more training examples,
whereas LM-IMP contains no learning component.
</bodyText>
<subsectionHeader confidence="0.995">
5.1 Best configuration of our method
</subsectionHeader>
<bodyText confidence="0.998956186046511">
We first evaluated experimentally the three configu-
rations of our method (LM-IMP, SVR-F1, SVR-
TOKACC-LM), using the F1-measure of the de-
pendencies of the machine-generated compressions
against those of the gold compressions as an auto-
matic evaluation measure. This measure has been
shown to correlate well with human judgements
(Clarke and Lapata, 2006).
In all three configurations, we trained the ME
model of section 3.1 on the dependency trees of the
source-gold pairs of the training part of the corpus.
We then used the trained ME classifier to generate
the candidate compressions of each source sentence
of the training part. We set t = 0.2, which led to
at most 10,000 candidates for almost every source
sentence. We kept up to 1.000 candidates for each
source sentence, and we selected randonly approx-
imately 10% of them, obtaining 18,385 candidates,
which were used to train the two SVR configurations;
LM-IMP requires no training.
To tune the A parameters of LM-IMP and SVR-
TOKACC-LM in equations 4 and 6, we initially set
α = 0 and we experimented with different val-
ues of A. For each one of the two configurations
and for every different A value, we computed the
average compression rate of the machine-generated
compressions on the development set. In the rest
of the experiments, we set A to the value that gave
an average compression rate approximatelly equal to
that of the gold compressions of the training part.
We then experimented with different values of α
in all three configurations, in equations 4–6, to pro-
duce smaller or longer compression rates. The α pa-
rameter provides a uniform mechanism to fine-tune
the compression rate in all three configurations, even
in SVR-F1 that has no A. The results on the de-
velopment part are shown in Figure 2, along with
the baseline’s results. The baseline has no param-
eters to tune; hence, its results are shown as a sin-
gle point. Both SVR models outperform LM-IMP,
which in turn outperforms the baseline. Also, SVR-
TOKACC-LM performs better or as well as SVR-
F1 for all compression rates. Note, also, that the
</bodyText>
<figureCaption confidence="0.970745">
Figure 2: Evaluation results on the development set.
</figureCaption>
<subsectionHeader confidence="0.979629">
5.2 Our method against T3
</subsectionHeader>
<bodyText confidence="0.99998024">
We then evaluated the best configuration of our
method (SVR-TOKACC-LM) against T3, both au-
tomatically (F1-measure) and with human judges.
We trained both systems on the training set of the
corpus. In our system, we used the same A value that
we had obtained from the experiments of the previ-
ous section. We then varied the values of our sys-
tem’s α parameter to obtain approximately the same
compression rate as T3.
For the evaluation with the human judges, we se-
lected randomly 80 sentences from the test part. For
each source sentence s, we formed three pairs, con-
taining s, the gold compression, the compression
of SVR-TOKACC-LM, and the compression of T3,
repsectively, 240 pairs in total. Four judges (grad-
uate students) were used. Each judge was given 60
pairs in a random sequence; they did not know how
the compressed sentenes were obtained and no judge
saw more than one compression of the same source
sentence. The judges were told to rate (in a scale
from 1 to 5) the compressed sentences in terms of
grammaticality, meaning preservation, and overall
quality. Their average judgements are shown in Ta-
ble 2, where the F1-scores are also included. Cohn
and Lapata (2009) have reported very similar scores
</bodyText>
<page confidence="0.730023">
891
</page>
<bodyText confidence="0.763281">
for T3 on a different split of the corpus (F1: 49.48%,
CR: 61.09%).
</bodyText>
<table confidence="0.9952175">
system G M Ov F1 (%) CR (%)
T3 3.83 3.28 3.23 47.34 59.16
SVR 4.20 3.43 3.57 52.09 59.85
gold 4.73 4.27 4.43 100.00 78.80
</table>
<tableCaption confidence="0.970589666666667">
Table 2: Results on 80 test sentences. G: grammaticality,
M: meaning preservation, Ov: overall score, CR: com-
pression rate, SVR: SVR-TOKACC-LM.
</tableCaption>
<bodyText confidence="0.9992545625">
Our system outperforms T3 in all evaluation mea-
sures. We used Analysis of Variance (ANOVA) fol-
lowed by post-hoc Tukey tests to check whether the
judge ratings differ significantly (p &lt; 0.1); all judge
ratings of gold compressions are significantly differ-
ent from T3’s and those of our system; also, our sys-
tem differs significantly from T3 in grammaticality,
but not in meaning preservation and overall score.
We also performed Wilcoxon tests, which showed
that the difference in the F1 scores of the two sys-
tems is statistically significant (p &lt; 0.1) on the 80
test sentences. Table 3 shows the F1 scores and the
average compression rates for all 291 test sentences.
Both systems have comparable compression rates,
but again our system outperforms T3 in F1, with a
statistically significant difference (p &lt; 0.001).
</bodyText>
<table confidence="0.909512666666667">
system F1 CR
SVR-TOKACC-LM 53.75 63.72
T3 47.52 64.16
</table>
<tableCaption confidence="0.999678">
Table 3: F1 scores on the entire test set.
</tableCaption>
<bodyText confidence="0.999943125">
Finally, we computed the Pearson correlation r of
the overall (Ov) scores that the judges assigned to
the machine-generated compressions with the corre-
sponding F1 scores. The two measures were found
to corellate reliably (r = 0.526). Similar results
have been reported (Clarke and Lapata, 2006) for
Edinburgh’s “spoken” corpus (r = 0.532) and the
Ziff-Davis corpus (r = 0.575).
</bodyText>
<sectionHeader confidence="0.991535" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999979365853659">
We presented a new two-stage extractive method
for sentence compression. The first stage gener-
ates candidate compressions by removing or not
edges from the source sentence’s dependency tree;
an ME model is used to prune unlikely edge deletion
or non-deletions. The second stage ranks the can-
didate compressions; we experimented with three
ranking models, achieving the best results with an
SVR model trained with an objective function that
combines token accuracy and a language model.
We showed experimentally, via automatic evalua-
tion and with human judges, that our method com-
pares favorably to a state-of-the-art extractive sys-
tem. Unlike other recent approaches, our system
uses no hand-crafted rules. In future work, we plan
to support more complex tranformations, instead of
only removing words and experiment with different
sizes of training data.
The work reported in this paper was carried out in
the context of project INDIGO, where an autonomous
robotic guide for museum collections is being devel-
oped. The guide engages the museum’s visitors in
spoken dialogues, and it describes the exhibits that
the visitors select by generating textual descriptions,
which are passed on to a speech synthesizer. The
texts are generated from logical facts stored in an on-
tology (Galanis et al., 2009) and from canned texts;
the latter are used when the corresponding informa-
tion is difficult to encode in symbolic form (e.g., to
store short stories about the exhibits). The descrip-
tions of the exhibits are tailored depending on the
type of the visitor (e.g., child vs. adult), and an im-
portant tailoring aspect is the generation of shorter
or longer descriptions. The parts of the descrip-
tions that are generated from logical facts can be
easily made shorter or longer, by conveying fewer
or more facts. The methods of this paper are used
to automatically shorten the parts of the descrip-
tions that are canned texts, instead of requiring mul-
tiple (shorter and longer) hand-written versions of
the canned texts.
</bodyText>
<sectionHeader confidence="0.990279" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.8729114">
This work was carried out in INDIGO, an FP6 IST
project funded by the European Union, with addi-
tional funding provided by the Greek General Sec-
retariat of Research and Technology.7
7Consulthttp://www.ics.forth.gr/indigo/.
</bodyText>
<page confidence="0.791762">
892
</page>
<sectionHeader confidence="0.989712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922526315789">
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
2006. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of the 2nd International Conference on Human Lan-
guage Technology Research, pages 24–27.
C.C Chang and C.J Lin. 2001. LIBSVM: a library for
Support Vector Machines. Technical report. Software
available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of COLING, pages 377–384.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Artificial Intelligence Research, 1(31):399–
429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoNLL,
pages 73–82.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING,
pages 137–144.
T. Cohn and M. Lapata. 2009. Sentence compression
as tree to tree tranduction. Artificial Intelligence Re-
search, 34:637–674.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25–69.
J. Cordeiro, G. Dias, and P. Brazdil. 2009. Unsupervised
induction of sentence compression rules. In Proceed-
ings of the ACL Workshop on Language Generation
and Summarisation, pages 391–399.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization, pages 89–98.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC,
pages 449–454.
Dimitrios Galanis, George Karakatsiotis, Gerasimos
Lampouras, and Ion Androutsopoulos. 2009. An
open-source natural language generator for OWL on-
tologies and its use in protege and second life. In Pro-
ceedings of the Demonstrations Session at EACL 2009,
pages 17–20, Athens, Greece, April. Association for
Computational Linguistics.
S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measur-
ing importance and query relevance in topic-focused
multi-document summarization. In Proceedings of
ACL, pages 193–196.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP, pages 310–
315.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probalistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91–
107.
C.W. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of ACL, pages 495–501.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C. D. Manning, D. Klein, and C. Manning. 2003. Op-
timization, maxent models, and conditional estimation
without magic. In tutorial notes of HLT-NAACL 2003
and ACL 2003.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, pages 297–304.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP, pages 391–399.
D. Paiva and R. Evans. 2005. Empirically-based con-
trol of natural language generation. In Proceedings of
ACL.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901–
904.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453–1484.
V. Vandeghinste and Y. Pan. 2004. Sentence compres-
sion for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop “Text Summariza-
tion Branches Out”, pages 89–95.
</reference>
<page confidence="0.969993">
893
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.371068">
<note confidence="0.442622">An extractive supervised two-stage method for sentence compression of Informatics, Athens University of Economics and Business, Curation Unit – Research Centre “Athena”, Greece</note>
<abstract confidence="0.999676166666667">We present a new method that compresses sentences by removing words. In a first stage, it generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier. In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2403" citStr="Berger et al., 2006" startWordPosition="355" endWordPosition="358">stractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and </context>
<context position="9398" citStr="Berger et al., 2006" startWordPosition="1450" endWordPosition="1453"> to estimate the probabilities P(Xi|context(ei)) for every edge ei of T3, where Xi is a variable that can take one of the following three values: not del, for not deleting ei; del u for deleting ei along with its head; and del l for deleting e along with its modifier. The head (respectively, modifier) of ei is the node ei originates from (points to) in the dependency tree. context(ei) is a set of features that represents ei’s local context in T3, as well as the local context of the head and modifier of ei in s. The propabilities above can be estimated using the Maximum Entropy (ME) framework (Berger et al., 2006), a method for learning the distribution P(X|V ) from training data, where X is discretevalued variable and V = (Vi, ... , Vn) is a real or discrete-valued vector. Here, V = context(ei) and X = Xi. We use the following features in V : • The label of the dependency edge ei, as well as the POS tag of the head and modifier of ei. • The entire head-label-modifier triple of ei. This feature overlaps with the previous two features, but it is common in ME models to use feature combinations as additional features, since they may indicate a category more strongly than the individual initial features.4 </context>
</contexts>
<marker>Berger, Pietra, Pietra, 2006</marker>
<rawString>A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. 2006. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Design of a multi-lingual, parallelprocessing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology Research,</booktitle>
<pages>24--27</pages>
<marker>Bikel, 2002</marker>
<rawString>D. Bikel. 2002. Design of a multi-lingual, parallelprocessing statistical parsing engine. In Proceedings of the 2nd International Conference on Human Language Technology Research, pages 24–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for Support Vector Machines. Technical report. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="2552" citStr="Chang and Lin, 2001" startWordPosition="380" endWordPosition="383">manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method </context>
<context position="17917" citStr="Chang and Lin, 2001" startWordPosition="2983" endWordPosition="2986">ression rate penalty factor CR(ci|s) = |c|/|s |is included, to bias our method towards generating shorter or longer compressions; |• |denotes word length in words (punctuation is ignored). We explain how the weigths A, α are tuned in following sections. We call LM-IMP the configuration of our method that uses the ranking function of equation 4. 3.2.2 Support Vector Regression A more sophisticated way to select the best compression is to train a Support Vector Machines Regression (SVR) model to assign scores to feature vectors, with each vector representing a candidate compression. SVR models (Chang and Lin, 2001) are trained using l training vectors (x1, y1), ... , (xl, yl), where xi E Rn and yi E R, and learn a function f : Rn —* R that generalizes the training data. In our case, xi is a feature vector representing a candidate compression ci, and yi is a score indicating how good a compression ci is. We use 98 features: • Gramm(ci) and ImpRate(ci|s), as above. • 2 features indicating the ratio of important and unimportant words of s, identified as in section 3.1, that were deleted. • 2 features that indicate the average depth of the deleted and not deleted words in the dependency tree of s. • 92 feat</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.C Chang and C.J Lin. 2001. LIBSVM: a library for Support Vector Machines. Technical report. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>377--384</pages>
<contexts>
<context position="19198" citStr="Clarke and Lapata, 2006" startWordPosition="3227" endWordPosition="3230">y of them were deleted in ci. For every POS tag label, we use two features, one that shows how many POS tags of that label are contained in s and one that shows how many of these POS tags were deleted in ci. To assign a regression score yi to each training vector xi, we experimented with the following functions that measure how similar ci is to the gold compression g, and how grammatical ci is. • Grammatical relations overlap: In this case, yi is the F1-score of the dependencies of ci against those of the gold compression g. This measure has been shown to correlate well with human judgements (Clarke and Lapata, 2006). As in 889 the ranking function of section 3.2.1, we add a compression rate penalty factor. yi = F1(d(ci)), d(g)) − a • CR(ci|s) (5) d(•) denotes the set of dependencies. We call SVR-F1 the configuration of our system that uses equation 5 to rank the candidates. • Tokens accuracy and grammaticality: Tokens accuracy, TokAcc(ci|s, g), is the percentage of tokens of s that were correctly retained or removed in ci; a token was correctly retained or removed, if it was also retained (or removed) in the gold compression g. To calculate TokAcc(ci|s, g), we need the word-toword alignment of s to g, an</context>
<context position="23833" citStr="Clarke and Lapata, 2006" startWordPosition="4010" endWordPosition="4013"> We split the corpus in 3 parts: 1024 training, 324 development, and 291 testing pairs. perfomance of the two SVR configurations might be improved further by using more training examples, whereas LM-IMP contains no learning component. 5.1 Best configuration of our method We first evaluated experimentally the three configurations of our method (LM-IMP, SVR-F1, SVRTOKACC-LM), using the F1-measure of the dependencies of the machine-generated compressions against those of the gold compressions as an automatic evaluation measure. This measure has been shown to correlate well with human judgements (Clarke and Lapata, 2006). In all three configurations, we trained the ME model of section 3.1 on the dependency trees of the source-gold pairs of the training part of the corpus. We then used the trained ME classifier to generate the candidate compressions of each source sentence of the training part. We set t = 0.2, which led to at most 10,000 candidates for almost every source sentence. We kept up to 1.000 candidates for each source sentence, and we selected randonly approximately 10% of them, obtaining 18,385 candidates, which were used to train the two SVR configurations; LM-IMP requires no training. To tune the </context>
<context position="28387" citStr="Clarke and Lapata, 2006" startWordPosition="4788" endWordPosition="4791"> 3 shows the F1 scores and the average compression rates for all 291 test sentences. Both systems have comparable compression rates, but again our system outperforms T3 in F1, with a statistically significant difference (p &lt; 0.001). system F1 CR SVR-TOKACC-LM 53.75 63.72 T3 47.52 64.16 Table 3: F1 scores on the entire test set. Finally, we computed the Pearson correlation r of the overall (Ov) scores that the judges assigned to the machine-generated compressions with the corresponding F1 scores. The two measures were found to corellate reliably (r = 0.526). Similar results have been reported (Clarke and Lapata, 2006) for Edinburgh’s “spoken” corpus (r = 0.532) and the Ziff-Davis corpus (r = 0.575). 6 Conclusions and future work We presented a new two-stage extractive method for sentence compression. The first stage generates candidate compressions by removing or not edges from the source sentence’s dependency tree; an ME model is used to prune unlikely edge deletion or non-deletions. The second stage ranks the candidate compressions; we experimented with three ranking models, achieving the best results with an SVR model trained with an objective function that combines token accuracy and a language model. </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of COLING, pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Artificial Intelligence Research,</journal>
<volume>1</volume>
<issue>31</issue>
<pages>429</pages>
<contexts>
<context position="1746" citStr="Clarke and Lapata, 2008" startWordPosition="254" endWordPosition="257">Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree</context>
<context position="5114" citStr="Clarke and Lapata (2008)" startWordPosition="784" endWordPosition="787">ased one. McDonald (2006) ranks each candidate compression using a function based on the dot product of a vector of weights with a vector of features extracted from the candidate’s n-grams, POS tags, and dependency tree. The weights were learnt from the ZiffDavis corpus. The best compression is found using a Viterbi-like algorithm that looks for the best sequence of source words that maximizes the scoring function. The method outperformed Knight and Marcu’s (2002) tree-to-tree method in grammaticality and meaning preservation on data from the ZiffDavis corpus, with a similar compression rate. Clarke and Lapata (2008) presented an unsupervised method that finds the best compression using Integer Linear Programming (ILP). The ILP obejctive function takes into account a language model that indicates which n-grams are more likely to be deleted, and a significance model that shows which words of the input sentence are important. Manually defined constraints (in effect, rules) that operate on dependency trees indicate which syntactic constituents can be deleted. This method outperformed McDonald’s (2006) in grammaticality and meaning preservation on test sentences from Edinburgh’s “written” and “spoken” corpora</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Artificial Intelligence Research, 1(31):399– 429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="2686" citStr="Cohn and Lapata, 2007" startWordPosition="399" endWordPosition="402">how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P(y) and a channel model P(xly), where x 1An implementation of our method will be freely available from htt</context>
<context position="5982" citStr="Cohn and Lapata, 2007" startWordPosition="914" endWordPosition="917"> model that shows which words of the input sentence are important. Manually defined constraints (in effect, rules) that operate on dependency trees indicate which syntactic constituents can be deleted. This method outperformed McDonald’s (2006) in grammaticality and meaning preservation on test sentences from Edinburgh’s “written” and “spoken” corpora.2 However, the compression rates of the two systems were dif2See http://homepages.inf.ed.ac.uk/s0460084/data/. ferent (72.0% vs. 63.7% for McDonald’s method, both on the written corpus). We compare our method against Cohn and Lapata’s T3 system (Cohn and Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art extractive sentence compression system that learns parse tree transduction operators from a parallel extractive corpus of source-compressed trees. T3 uses a chart-based decoding algorithm and a Structured Support Vector Machine (Tsochantaridis et al., 2005) to learn to select the best compression among those licensed by the operators learnt.3 T3 outperformed McDonald’s (2006) system in grammaticality and meaning preservation on Edinburgh’s “written” and “spoken” corpora, achieving comparable compression rates (Cohn and Lapata, 2009). Cohn and Lapata</context>
<context position="20869" citStr="Cohn and Lapata, 2007" startWordPosition="3530" endWordPosition="3533">configuration of our system that uses equation 6. 4 Baseline and T3 As a baseline, we use a simple algorithm based on the ME classifier of section 3.1. The baseline produces a single compression c for every source sentence s by considering sequentially the edges ei of s’s dependency tree in a random order, and performing at each ei the single action (not del, del u, or del l) that the ME model considers more probable; the words of the chopped dependency tree are then put in the same order as in s. We call this system Greedy-Baseline. We compare our method against the extractive version of T3 (Cohn and Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art sentence compression system that applies sequences of transduction operators to the syntax trees of the source sentences. The available tranduction operators are learnt from the syntax trees of a set of source-gold pairs. Every operator transforms a subtree a to a subtree -y, rooted at symbols X and Y , respectively. To find the best sequence of transduction operators that can be applied to a source syntax tree, a chart-based dynamic programming decoder is used, which finds the best scoring sequence q*: q* = arg max score(q; w) (7) q where score(q; </context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>T. Cohn and M. Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of EMNLP-CoNLL, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="1848" citStr="Cohn and Lapata, 2008" startWordPosition="269" endWordPosition="272">tences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best amon</context>
<context position="6589" citStr="Cohn and Lapata (2008)" startWordPosition="999" endWordPosition="1002">nd Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art extractive sentence compression system that learns parse tree transduction operators from a parallel extractive corpus of source-compressed trees. T3 uses a chart-based decoding algorithm and a Structured Support Vector Machine (Tsochantaridis et al., 2005) to learn to select the best compression among those licensed by the operators learnt.3 T3 outperformed McDonald’s (2006) system in grammaticality and meaning preservation on Edinburgh’s “written” and “spoken” corpora, achieving comparable compression rates (Cohn and Lapata, 2009). Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. In the first stage, candidate compressions are generated by chopping the source sentence’s dependency tree. Many ungrammatical compressions are avoided using hand-crafted drop-menot rules for dependency subtrees. The candidate compressions are then ranked using a function that takes into account the inverse document frequencies </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>T. Cohn and M. Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of COLING, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression as tree to tree tranduction.</title>
<date>2009</date>
<journal>Artificial Intelligence Research,</journal>
<pages>34--637</pages>
<contexts>
<context position="1770" citStr="Cohn and Lapata, 2009" startWordPosition="258" endWordPosition="261">ummarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy</context>
<context position="6006" citStr="Cohn and Lapata, 2009" startWordPosition="918" endWordPosition="921"> words of the input sentence are important. Manually defined constraints (in effect, rules) that operate on dependency trees indicate which syntactic constituents can be deleted. This method outperformed McDonald’s (2006) in grammaticality and meaning preservation on test sentences from Edinburgh’s “written” and “spoken” corpora.2 However, the compression rates of the two systems were dif2See http://homepages.inf.ed.ac.uk/s0460084/data/. ferent (72.0% vs. 63.7% for McDonald’s method, both on the written corpus). We compare our method against Cohn and Lapata’s T3 system (Cohn and Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art extractive sentence compression system that learns parse tree transduction operators from a parallel extractive corpus of source-compressed trees. T3 uses a chart-based decoding algorithm and a Structured Support Vector Machine (Tsochantaridis et al., 2005) to learn to select the best compression among those licensed by the operators learnt.3 T3 outperformed McDonald’s (2006) system in grammaticality and meaning preservation on Edinburgh’s “written” and “spoken” corpora, achieving comparable compression rates (Cohn and Lapata, 2009). Cohn and Lapata (2008) have also develo</context>
<context position="20893" citStr="Cohn and Lapata, 2009" startWordPosition="3534" endWordPosition="3537">stem that uses equation 6. 4 Baseline and T3 As a baseline, we use a simple algorithm based on the ME classifier of section 3.1. The baseline produces a single compression c for every source sentence s by considering sequentially the edges ei of s’s dependency tree in a random order, and performing at each ei the single action (not del, del u, or del l) that the ME model considers more probable; the words of the chopped dependency tree are then put in the same order as in s. We call this system Greedy-Baseline. We compare our method against the extractive version of T3 (Cohn and Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art sentence compression system that applies sequences of transduction operators to the syntax trees of the source sentences. The available tranduction operators are learnt from the syntax trees of a set of source-gold pairs. Every operator transforms a subtree a to a subtree -y, rooted at symbols X and Y , respectively. To find the best sequence of transduction operators that can be applied to a source syntax tree, a chart-based dynamic programming decoder is used, which finds the best scoring sequence q*: q* = arg max score(q; w) (7) q where score(q; w) is the dot product (&apos;</context>
<context position="22582" citStr="Cohn and Lapata (2009)" startWordPosition="3820" endWordPosition="3824">etc. 5 Experiments We used Stanford’s parser (de Marneffe et al., 2006) and ME classifier (Manning et al., 2003).5 For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002).6 The language model was trained on approximately 4.5 million sentences of the TIPSTER corpus. To obtain idf(wi) values, we used approximately 19.5 million verbs and nouns from the TIPSTER corpus. T3 requires the syntax trees of the source-gold pairs in Penn Treebank format, as well as a trigram language model. We obtained T3’s trees using Stanford’s parser, as in our system, unlike Cohn and Lapata (2009) that use Bikel’s (2002) parser. The language models in T3 and our system are trained on the same data and with the same options used by Cohn and Lapata (2009). T3 also needs a word-toword alignment of the source-gold pairs, which was obtained by computing the edit distance, as in Cohn and Lapata (2009) and SVR-TOKACC-LM. We used Edinburgh’s “written” sentence compression corpus (section 2), which consists of source-gold pairs (one gold compression per source 5Both available from http://nlp.stanford.edu/. 6See http://www.speech.sri.com/projects/srilm/. 890 sentence). The gold compressions were</context>
<context position="26807" citStr="Cohn and Lapata (2009)" startWordPosition="4523" endWordPosition="4526">airs, containing s, the gold compression, the compression of SVR-TOKACC-LM, and the compression of T3, repsectively, 240 pairs in total. Four judges (graduate students) were used. Each judge was given 60 pairs in a random sequence; they did not know how the compressed sentenes were obtained and no judge saw more than one compression of the same source sentence. The judges were told to rate (in a scale from 1 to 5) the compressed sentences in terms of grammaticality, meaning preservation, and overall quality. Their average judgements are shown in Table 2, where the F1-scores are also included. Cohn and Lapata (2009) have reported very similar scores 891 for T3 on a different split of the corpus (F1: 49.48%, CR: 61.09%). system G M Ov F1 (%) CR (%) T3 3.83 3.28 3.23 47.34 59.16 SVR 4.20 3.43 3.57 52.09 59.85 gold 4.73 4.27 4.43 100.00 78.80 Table 2: Results on 80 test sentences. G: grammaticality, M: meaning preservation, Ov: overall score, CR: compression rate, SVR: SVR-TOKACC-LM. Our system outperforms T3 in all evaluation measures. We used Analysis of Variance (ANOVA) followed by post-hoc Tukey tests to check whether the judge ratings differ significantly (p &lt; 0.1); all judge ratings of gold compressio</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>T. Cohn and M. Lapata. 2009. Sentence compression as tree to tree tranduction. Artificial Intelligence Research, 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="3057" citStr="Collins and Koo, 2005" startWordPosition="455" endWordPosition="458">the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P(y) and a channel model P(xly), where x 1An implementation of our method will be freely available from http://nlp.cs.aueb.gr/software.html 885 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics is the source sentence and y the compressed one. P(x|y) is calculated as the product of the probabilities of the parse tree tranformatio</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cordeiro</author>
<author>G Dias</author>
<author>P Brazdil</author>
</authors>
<title>Unsupervised induction of sentence compression rules.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Workshop on Language Generation and Summarisation,</booktitle>
<pages>391--399</pages>
<contexts>
<context position="16476" citStr="Cordeiro et al. (2009)" startWordPosition="2730" endWordPosition="2733">llan that the search was. c3: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge. c4: Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge. Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1, ... , c4. guage model trained on a large background corpus. However, language models tend to assign smaller probabilities to longer sentences; therefore they favor short sentences, but not necessarily the most appropriate compressions. To overcome this problem, we follow Cordeiro et al. (2009) and normalize the score of a trigram language model as shown below, where w1, ... , wm are the words of candidate ci. Gramm(ci) = log PLM(ci)1/m = m (1/m) • log( P(wj|wj−1, wj−2)) (1) j=1 The importance rate ImpRate(ci|s), defined below, estimates how much information of the original sentence s is retained in candidate ci. tf(wi) is the term frequency of wi in the document that contained � (� = ci, s), and idf(wi) is the inverse document frequency of wi in a background corpus. We actually compute idf(wi) only for nouns and verbs, and set idf(wi) = 0 for other words. ImpRate(ci|s) = Imp(ci)/Im</context>
</contexts>
<marker>Cordeiro, Dias, Brazdil, 2009</marker>
<rawString>J. Cordeiro, G. Dias, and P. Brazdil. 2009. Unsupervised induction of sentence compression rules. In Proceedings of the ACL Workshop on Language Generation and Summarisation, pages 391–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
</authors>
<title>Text compaction for display on very small screens.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarization,</booktitle>
<pages>89--98</pages>
<contexts>
<context position="1080" citStr="Corston-Oliver, 2001" startWordPosition="154" endWordPosition="155"> Maximum Entropy classifier. In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 200</context>
</contexts>
<marker>Corston-Oliver, 2001</marker>
<rawString>S. Corston-Oliver. 2001. Text compaction for display on very small screens. In Proceedings of the NAACL Workshop on Automatic Summarization, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C de Marneffe</author>
<author>B MacCartney</author>
<author>C Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.C. de Marneffe, B. MacCartney, and C. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>George Karakatsiotis</author>
<author>Gerasimos Lampouras</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>An open-source natural language generator for OWL ontologies and its use in protege and second life.</title>
<date>2009</date>
<booktitle>In Proceedings of the Demonstrations Session at EACL</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="29800" citStr="Galanis et al., 2009" startWordPosition="5011" endWordPosition="5014"> uses no hand-crafted rules. In future work, we plan to support more complex tranformations, instead of only removing words and experiment with different sizes of training data. The work reported in this paper was carried out in the context of project INDIGO, where an autonomous robotic guide for museum collections is being developed. The guide engages the museum’s visitors in spoken dialogues, and it describes the exhibits that the visitors select by generating textual descriptions, which are passed on to a speech synthesizer. The texts are generated from logical facts stored in an ontology (Galanis et al., 2009) and from canned texts; the latter are used when the corresponding information is difficult to encode in symbolic form (e.g., to store short stories about the exhibits). The descriptions of the exhibits are tailored depending on the type of the visitor (e.g., child vs. adult), and an important tailoring aspect is the generation of shorter or longer descriptions. The parts of the descriptions that are generated from logical facts can be easily made shorter or longer, by conveying fewer or more facts. The methods of this paper are used to automatically shorten the parts of the descriptions that </context>
</contexts>
<marker>Galanis, Karakatsiotis, Lampouras, Androutsopoulos, 2009</marker>
<rawString>Dimitrios Galanis, George Karakatsiotis, Gerasimos Lampouras, and Ion Androutsopoulos. 2009. An open-source natural language generator for OWL ontologies and its use in protege and second life. In Proceedings of the Demonstrations Session at EACL 2009, pages 17–20, Athens, Greece, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A Nenkova</author>
<author>D Jurafsky</author>
</authors>
<title>Measuring importance and query relevance in topic-focused multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>193--196</pages>
<contexts>
<context position="11110" citStr="Gupta et al., 2007" startWordPosition="1760" endWordPosition="1763">els of the edges that have the same head as ei in T3 (one feature for each possible dependency label). • Two binary features that show if the subtree rooted at the modifier of ei or ei’s uptree (the rest of the tree, when ei’s subtree is removed) contain an important word. A word is considered important if it appears in the document s was drawn from significantly more often than in a background corpus. In summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio A of each word (Lin and Hovy, 2000; Gupta et al., 2007). For each dependency edge ei of a source training sentence s, we create a training vector V with the above features. If ei is retained in the dependency tree of the corresponding compressed sentence g in the corpus, V is assigned the category not del. If ei is not retained, it is assigned the category del l or del u, depending on whether the head (as in the ccomp of “said” in Figure 1) or the modifier (as in the dobj of “attend”) of ei has also been removed. When the modifier of an edge is removed, the entire subtree rooted at the modifier is removed, and similarly for the uptree, when the he</context>
</contexts>
<marker>Gupta, Nenkova, Jurafsky, 2007</marker>
<rawString>S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measuring importance and query relevance in topic-focused multi-document summarization. In Proceedings of ACL, pages 193–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP,</booktitle>
<pages>310--315</pages>
<contexts>
<context position="950" citStr="Jing, 2000" startWordPosition="136" endWordPosition="137">first stage, it generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier. In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>H. Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of ANLP, pages 310– 315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probalistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<pages>107</pages>
<contexts>
<context position="1705" citStr="Knight and Marcu, 2002" startWordPosition="247" endWordPosition="250">n subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches f</context>
<context position="3097" citStr="Knight and Marcu (2002)" startWordPosition="462" endWordPosition="465">ns using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P(y) and a channel model P(xly), where x 1An implementation of our method will be freely available from http://nlp.cs.aueb.gr/software.html 885 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics is the source sentence and y the compressed one. P(x|y) is calculated as the product of the probabilities of the parse tree tranformations required to expand y to x. The best c</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probalistic approach to sentence compression. Artificial Intelligence, 139(1):91– 107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="11089" citStr="Lin and Hovy, 2000" startWordPosition="1756" endWordPosition="1759">r not) among the labels of the edges that have the same head as ei in T3 (one feature for each possible dependency label). • Two binary features that show if the subtree rooted at the modifier of ei or ei’s uptree (the rest of the tree, when ei’s subtree is removed) contain an important word. A word is considered important if it appears in the document s was drawn from significantly more often than in a background corpus. In summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio A of each word (Lin and Hovy, 2000; Gupta et al., 2007). For each dependency edge ei of a source training sentence s, we create a training vector V with the above features. If ei is retained in the dependency tree of the corresponding compressed sentence g in the corpus, V is assigned the category not del. If ei is not retained, it is assigned the category del l or del u, depending on whether the head (as in the ccomp of “said” in Figure 1) or the modifier (as in the dobj of “attend”) of ei has also been removed. When the modifier of an edge is removed, the entire subtree rooted at the modifier is removed, and similarly for th</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C.W. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of ACL, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>D Zajic</author>
<author>B Dorr</author>
<author>N F Ayan</author>
<author>J Lin</author>
</authors>
<title>Multiple alternative sentence compressions for automatic text summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of DUC.</booktitle>
<contexts>
<context position="1183" citStr="Madnani et al., 2007" startWordPosition="169" endWordPosition="172">ing a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although ab</context>
</contexts>
<marker>Madnani, Zajic, Dorr, Ayan, Lin, 2007</marker>
<rawString>N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin. 2007. Multiple alternative sentence compressions for automatic text summarization. In Proceedings of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In tutorial notes of HLT-NAACL 2003 and ACL</booktitle>
<contexts>
<context position="22072" citStr="Manning et al., 2003" startWordPosition="3739" endWordPosition="3742"> q where score(q; w) is the dot product (&apos;F(q), w). IF(q) is a vector-valued feature function, and w is a vector of weights learnt using a Structured Support Vector Machine (Tsochantaridis et al., 2005). IF(q) consists of: (i) the log-probability of the resulting candidate, as returned by a tri-gram language model; and (ii) features that describe how the operators of q are applied, for example the number of the terminals in each operator’s a and -y subtrees, the POS tags of the X and Y roots of a and -y etc. 5 Experiments We used Stanford’s parser (de Marneffe et al., 2006) and ME classifier (Manning et al., 2003).5 For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002).6 The language model was trained on approximately 4.5 million sentences of the TIPSTER corpus. To obtain idf(wi) values, we used approximately 19.5 million verbs and nouns from the TIPSTER corpus. T3 requires the syntax trees of the source-gold pairs in Penn Treebank format, as well as a trigram language model. We obtained T3’s trees using Stanford’s parser, as in our system, unlike Cohn and Lapata (2009) that use Bikel’s (2002) parser. The language models in T3 and our system are trained on t</context>
</contexts>
<marker>Manning, Klein, Manning, 2003</marker>
<rawString>C. D. Manning, D. Klein, and C. Manning. 2003. Optimization, maxent models, and conditional estimation without magic. In tutorial notes of HLT-NAACL 2003 and ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="1721" citStr="McDonald, 2006" startWordPosition="251" endWordPosition="253">andeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source s</context>
<context position="4515" citStr="McDonald (2006)" startWordPosition="687" endWordPosition="688">that tries to rewrite directly x to the best y. This second method uses C4.5 (Quinlan, 1993) to learn when to perform tree rewriting actions (e.g., dropping subtrees, combining subtrees) that transform larger trees to smaller trees. Both methods were trained and tested on data from the Ziff-Davis corpus (Knight and Marcu, 2002), and they achieved very similar grammaticality and meaning preservation scores, with no statistically significant difference. However, their compression rates (counted in words) were very different: 70.37% for the noisy-channel method and 57.19% for the C4.5-based one. McDonald (2006) ranks each candidate compression using a function based on the dot product of a vector of weights with a vector of features extracted from the candidate’s n-grams, POS tags, and dependency tree. The weights were learnt from the ZiffDavis corpus. The best compression is found using a Viterbi-like algorithm that looks for the best sequence of source words that maximizes the scoring function. The method outperformed Knight and Marcu’s (2002) tree-to-tree method in grammaticality and meaning preservation on data from the ZiffDavis corpus, with a similar compression rate. Clarke and Lapata (2008) </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic constraints. In Proceedings of EACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>A comparison of model free versus model intensive approaches to sentence compression.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>391--399</pages>
<contexts>
<context position="1986" citStr="Nomoto, 2009" startWordPosition="293" endWordPosition="294">pressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved. In this paper, we present a new extractive sentence compression method that relies on supervised machine learning.1 In a first stage, the method generates candidate compressions by removing branches from the source sentence’s dependency tree using a Maximum Entropy classifier (Berger et al., 2006). In a second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our </context>
<context position="6817" citStr="Nomoto (2009)" startWordPosition="1033" endWordPosition="1034"> algorithm and a Structured Support Vector Machine (Tsochantaridis et al., 2005) to learn to select the best compression among those licensed by the operators learnt.3 T3 outperformed McDonald’s (2006) system in grammaticality and meaning preservation on Edinburgh’s “written” and “spoken” corpora, achieving comparable compression rates (Cohn and Lapata, 2009). Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. In the first stage, candidate compressions are generated by chopping the source sentence’s dependency tree. Many ungrammatical compressions are avoided using hand-crafted drop-menot rules for dependency subtrees. The candidate compressions are then ranked using a function that takes into account the inverse document frequencies of the words, and their depths in the source dependency tree. Nomoto’s extractive method was reported to outperform Cohn and Lapata’s abstractive version of T3 on a corpus collected via RSS feeds. Our method is similar to Nomoto</context>
</contexts>
<marker>Nomoto, 2009</marker>
<rawString>T. Nomoto. 2009. A comparison of model free versus model intensive approaches to sentence compression. In Proceedings of EMNLP, pages 391–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Paiva</author>
<author>R Evans</author>
</authors>
<title>Empirically-based control of natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3033" citStr="Paiva and Evans, 2005" startWordPosition="451" endWordPosition="454">cond stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model (Chang and Lin, 2001). We show experimentally that our method compares favorably to a state-of-the-art extractive compression method (Cohn and Lapata, 2007; Cohn and Lapata, 2009), without requiring any manually written rules, unlike other recent work (Clarke and Lapata, 2008; Nomoto, 2009). In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing (Paiva and Evans, 2005; Collins and Koo, 2005). 2 Related work Knight and Marcu (2002) presented a noisy channel sentence compression method that uses a language model P(y) and a channel model P(xly), where x 1An implementation of our method will be freely available from http://nlp.cs.aueb.gr/software.html 885 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885–893, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics is the source sentence and y the compressed one. P(x|y) is calculated as the product of the probabilities of the</context>
</contexts>
<marker>Paiva, Evans, 2005</marker>
<rawString>D. Paiva and R. Evans. 2005. Empirically-based control of natural language generation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3992" citStr="Quinlan, 1993" startWordPosition="608" endWordPosition="609"> of the North American Chapter of the ACL, pages 885–893, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics is the source sentence and y the compressed one. P(x|y) is calculated as the product of the probabilities of the parse tree tranformations required to expand y to x. The best compression of x is the one that maximizes P(x|y) · P(y), and it is found using a noisy channel decoder. In a second, alternative method Knight and Marcu (2002) use a treeto-tree transformation algorithm that tries to rewrite directly x to the best y. This second method uses C4.5 (Quinlan, 1993) to learn when to perform tree rewriting actions (e.g., dropping subtrees, combining subtrees) that transform larger trees to smaller trees. Both methods were trained and tested on data from the Ziff-Davis corpus (Knight and Marcu, 2002), and they achieved very similar grammaticality and meaning preservation scores, with no statistically significant difference. However, their compression rates (counted in words) were very different: 70.37% for the noisy-channel method and 57.19% for the C4.5-based one. McDonald (2006) ranks each candidate compression using a function based on the dot product o</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="22173" citStr="Stolcke, 2002" startWordPosition="3755" endWordPosition="3756">ctor of weights learnt using a Structured Support Vector Machine (Tsochantaridis et al., 2005). IF(q) consists of: (i) the log-probability of the resulting candidate, as returned by a tri-gram language model; and (ii) features that describe how the operators of q are applied, for example the number of the terminals in each operator’s a and -y subtrees, the POS tags of the X and Y roots of a and -y etc. 5 Experiments We used Stanford’s parser (de Marneffe et al., 2006) and ME classifier (Manning et al., 2003).5 For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002).6 The language model was trained on approximately 4.5 million sentences of the TIPSTER corpus. To obtain idf(wi) values, we used approximately 19.5 million verbs and nouns from the TIPSTER corpus. T3 requires the syntax trees of the source-gold pairs in Penn Treebank format, as well as a trigram language model. We obtained T3’s trees using Stanford’s parser, as in our system, unlike Cohn and Lapata (2009) that use Bikel’s (2002) parser. The language models in T3 and our system are trained on the same data and with the same options used by Cohn and Lapata (2009). T3 also needs a word-toword al</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for independent and structured output spaces.</title>
<date>2005</date>
<booktitle>Machine Learning Research,</booktitle>
<pages>6--1453</pages>
<contexts>
<context position="6284" citStr="Tsochantaridis et al., 2005" startWordPosition="956" endWordPosition="959"> sentences from Edinburgh’s “written” and “spoken” corpora.2 However, the compression rates of the two systems were dif2See http://homepages.inf.ed.ac.uk/s0460084/data/. ferent (72.0% vs. 63.7% for McDonald’s method, both on the written corpus). We compare our method against Cohn and Lapata’s T3 system (Cohn and Lapata, 2007; Cohn and Lapata, 2009), a state-of-the-art extractive sentence compression system that learns parse tree transduction operators from a parallel extractive corpus of source-compressed trees. T3 uses a chart-based decoding algorithm and a Structured Support Vector Machine (Tsochantaridis et al., 2005) to learn to select the best compression among those licensed by the operators learnt.3 T3 outperformed McDonald’s (2006) system in grammaticality and meaning preservation on Edinburgh’s “written” and “spoken” corpora, achieving comparable compression rates (Cohn and Lapata, 2009). Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. In the first stage, candi</context>
<context position="21653" citStr="Tsochantaridis et al., 2005" startWordPosition="3662" endWordPosition="3665">sentences. The available tranduction operators are learnt from the syntax trees of a set of source-gold pairs. Every operator transforms a subtree a to a subtree -y, rooted at symbols X and Y , respectively. To find the best sequence of transduction operators that can be applied to a source syntax tree, a chart-based dynamic programming decoder is used, which finds the best scoring sequence q*: q* = arg max score(q; w) (7) q where score(q; w) is the dot product (&apos;F(q), w). IF(q) is a vector-valued feature function, and w is a vector of weights learnt using a Structured Support Vector Machine (Tsochantaridis et al., 2005). IF(q) consists of: (i) the log-probability of the resulting candidate, as returned by a tri-gram language model; and (ii) features that describe how the operators of q are applied, for example the number of the terminals in each operator’s a and -y subtrees, the POS tags of the X and Y roots of a and -y etc. 5 Experiments We used Stanford’s parser (de Marneffe et al., 2006) and ME classifier (Manning et al., 2003).5 For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing (Stolcke, 2002).6 The language model was trained on approximately 4.5 million sentences of the </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2005. Support vector machine learning for independent and structured output spaces. Machine Learning Research, 6:1453–1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vandeghinste</author>
<author>Y Pan</author>
</authors>
<title>Sentence compression for automated subtitling: A hybrid approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop “Text Summarization Branches Out”,</booktitle>
<pages>89--95</pages>
<contexts>
<context position="1133" citStr="Vandeghinste and Pan, 2004" startWordPosition="160" endWordPosition="163"> it chooses the best among the candidate compressions using a Support Vector Machine Regression model. Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules. 1 Introduction Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is valuable in many applications, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle generation (Vandeghinste and Pan, 2004), and in text summarization (Madnani et al., 2007). People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and common sense knowledge. However, reasonable machine-generated sentence compressions can often be obtained by only removing words. We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed. Most of the existing compression methods are extractive (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and</context>
</contexts>
<marker>Vandeghinste, Pan, 2004</marker>
<rawString>V. Vandeghinste and Y. Pan. 2004. Sentence compression for automated subtitling: A hybrid approach. In Proceedings of the ACL Workshop “Text Summarization Branches Out”, pages 89–95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>