<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001826">
<title confidence="0.991126">
Measuring the Similarity between Automatically Generated Topics
</title>
<author confidence="0.989592">
Nikolaos Aletras and Mark Stevenson
</author>
<affiliation confidence="0.997447">
Department of Computer Science,
University of Sheffield,
</affiliation>
<address confidence="0.891342666666667">
Regent Court, 211 Portobello,
Sheffield,
United Kingdom S1 4DP
</address>
<email confidence="0.998338">
{n.aletras, m.stevenson}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824714285714">
Previous approaches to the problem of
measuring similarity between automati-
cally generated topics have been based on
comparison of the topics’ word probability
distributions. This paper presents alterna-
tive approaches, including ones based on
distributional semantics and knowledge-
based measures, evaluated by compari-
son with human judgements. The best
performing methods provide reliable esti-
mates of topic similarity comparable with
human performance and should be used in
preference to the word probability distri-
bution measures used previously.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999715595744681">
Topic models (Blei et al., 2010) have proved to be
useful for interpreting and organising the contents
of large document collections. It seems intuitively
plausible that some automatically generated topics
will be similar while others are dis-similar. For ex-
ample, a topic about basketball (team game james
season player nba play knicks coach league) is
more similar to a topic about football (world cup
team soccer africa player south game match goal)
than one about the global finance (fed financial
banks federal reserve bank bernanke rule crisis
credit). Methods for automatically determining
the similarity between topics have several poten-
tial applications, such as analysis of corpora to de-
termine topics being discussed (Hall et al., 2008)
or within topic browsers to decide which topics
should be shown together (Chaney and Blei, 2012;
Gretarsson et al., 2012; Hinneburg et al., 2012).
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a popular type of topic model but can-
not capture such correlations unless the seman-
tic similarity between topics is measured. Other
topic models, such as the Correlated Topic Model
(CTM) (Blei and Lafferty, 2006), overcome this
limitation and identify correlations between top-
ics.
Approaches to identifying similar topics for a
range of tasks have been described in the litera-
ture but they have been restricted to using informa-
tion from the word probability distribution to com-
pare topics and have not been directly evaluated.
Word distributions have been compared using a
variety of measures such as KL-divergence (Li and
McCallum, 2006; Wang et al., 2009; Newman et
al., 2009), cosine measure (He et al., 2009; Ram-
age et al., 2009) and the average Log Odds Ratio
(Chaney and Blei, 2012). Kim and Oh (2011) also
applied the cosine measure and KL-Divergence
which were compared with four other measures:
Jaccard’s Coefficient, Kendall’s τ coefficient, Dis-
count Cumulative Gain and Jensen Shannon Di-
vergence (JSD).
This paper compares a wider range of ap-
proaches to measuring topic similarity than pre-
vious work. In addition these measures are eval-
uated directly by comparing them against human
judgements.
</bodyText>
<sectionHeader confidence="0.844755" genericHeader="method">
2 Measuring Topic Similarity
</sectionHeader>
<bodyText confidence="0.9970254">
We compare measures based on word probability
distributions (Section 2.1), distributional semantic
methods (Sections 2.2-2.4), knowledge-based ap-
proaches (Section 2.5) and their combination (Sec-
tion 2.6).
</bodyText>
<subsectionHeader confidence="0.979772">
2.1 Topic Word Probability Distribution
</subsectionHeader>
<bodyText confidence="0.9997768">
We first experimented with measures based on
comparison of the topics’ word distributions (see
Section 1), by applying the JSD, KL-divergence
and Cosine approaches and the Log Odds Ratio
(Chaney and Blei, 2012).
</bodyText>
<page confidence="0.973456">
22
</page>
<note confidence="0.8251745">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 22–27,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.950088">
2.2 Topic Model Semantic Space
</subsectionHeader>
<bodyText confidence="0.9999862">
The semantic space generated by the topic model
can be used to represent the topics and the topic
words. By definition each topic is a probability
distribution over the words in the training corpus.
For a corpus with D documents and V words, a
topic model learns a relation between words and
topics, T, as a T ×V matrix, W, that indicates the
probability of each word in each topic. W is the
topic model semantic space and each topic word
can be represented as a vector, Vi, with topics as
features weighted by the probability of the word
in each topic. The similarity between two topics
is computed as the average pairwise cosine sim-
ilarity between their top-10 most probable words
(TS-Cos).
</bodyText>
<subsectionHeader confidence="0.994109">
2.3 Reference Corpus Semantic Space
</subsectionHeader>
<bodyText confidence="0.996953366666667">
Topic words can also be represented as vectors
in a semantic space constructed from an external
source. We adapt the method proposed by Aletras
and Stevenson (2013) for measuring topic coher-
ence using distributional semantics1.
Top-N Features A semantic space is con-
structed considering only the top n most frequent
words in Wikipedia (excluding stop words) as con-
text features. Each topic word is represented as a
vector of n features weighted by computing the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1989) between the topic word and each
context feature, PMI(wi, wj)γ. -y is a variable for
assigning more importance to higher PMI values.
In our experiments, we set -y = 3 and found that
the best performance is obtained for n = 5000.
Similarity between two topics is defined as the av-
erage cosine similarity of the topic word vectors
(RCS-Cos-N).
Topic Word Space Alternatively, we consider
only the top-10 topic words from the two topics
as context features to generate topic word vectors.
Then, topic similarity is computed as the pairwise
cosine similarity of the topic word vectors (RCS-
Cos-TWS).
Word Association Topic similarity can also be
computed by applying word association measures
directly. Newman et al. (2010) measure topic
coherence as the average PMI between the topic
words. This approach can be adapted to measure
</bodyText>
<footnote confidence="0.954687">
1Wikipedia is used as a reference corpus to count word
co-occurrences and frequencies using a context window of
±10 words centred on a topic word.
</footnote>
<bodyText confidence="0.992641">
topic similarity by computing the average pairwise
PMI between the topic words in two topics (PMI).
</bodyText>
<subsectionHeader confidence="0.996245">
2.4 Training Corpus Semantic Space
</subsectionHeader>
<bodyText confidence="0.981944333333333">
Term-Document Space A matrix X can be cre-
ated using the training corpus. Each term (row)
represents a topic word vector. Element xij in X
is the tf.idf of the term i in document j. Topic
similarity is computed as the pairwise cosine sim-
ilarity of the topic word vectors (TCS-Cos-TD).
Word Co-occurrence in Training Documents
Alternatively, we generate a matrix Z of co-
document frequencies. The matrix Z consists of
V rows and columns representing the V vocab-
ulary words. Element zij is the log of the num-
ber of documents that contains the words i and
j normalised by the document frequency, DF, of
the word j. Mimno et al. (2011) introduced that
metric to measure topic coherence. We adapted
it to estimate topic similarity by aggregating the
co-document frequency of the words between two
topics (Doc-Co-occ).
</bodyText>
<sectionHeader confidence="0.753087" genericHeader="method">
2.5 Knowledge-based Methods
</sectionHeader>
<bodyText confidence="0.999897722222222">
UKB (Agirre et al., 2009) is used to generate a
probability distribution over WordNet synsets for
each word in the vocabulary V of the topic model
using the Personalized PageRank algorithm. The
similarity between two topic words is calculated
by transforming these distributions into vectors
and computing the cosine metric. The similar-
ity between two topics is computed by measur-
ing pairwise similarity between their top-10 topic
words and selecting the highest score.
Explicit Semantic Analysis (ESA) proposed by
Gabrilovich and Markovitch (2007) transforms
the topic keywords into vectors that consist of
Wikipedia article titles weighted by their relevance
to the keyword. For each topic, the centroid is
computed from the keyword vectors. Similarity
between topics is computed as the cosine similar-
ity of the ESA centroid vectors.
</bodyText>
<subsectionHeader confidence="0.981661">
2.6 Feature Combination Using SVR
</subsectionHeader>
<bodyText confidence="0.9999084">
We also evaluate the performance of a support
vector regression system (SVR) (Vapnik, 1998)
with a linear kernel using a combination of ap-
proaches described above as features2. The system
is trained and tested using 10-fold cross validation.
</bodyText>
<footnote confidence="0.904572">
2With the exception of JSD, features based on the topics’
word probability distributions were not used by SVR since it
was found that including them reduced performance.
</footnote>
<page confidence="0.99872">
23
</page>
<sectionHeader confidence="0.997895" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.997228936170213">
Data We created a data set consisting of pairs of
topics generated by two topic models (LDA and
CTM) over two document collections using differ-
ent numbers of topics. The first consists of 47,229
news articles from New York Times (NYT) in the
GigaWord corpus and the second contains 50,000
articles from ukWAC (Baroni et al., 2009). Each
article is tokenised then stop words and words ap-
pearing fewer than five times in the corpora re-
moved. This results in a total of 57,651 unique to-
kens for the NYT corpus and 72,672 for ukWAC.
LDA Topics are learned by training LDA mod-
els over the two corpora using gensim3. The num-
ber of topics is set to T = 50,100, 200 and hy-
perparameters, α and β, are set to T . Randomly
selecting pairs of topics will result to a data set
in which the majority of pairs would not be simi-
lar. We overcome that problem by assuming that
the JSD between likely relevant pairs will be low
while it will be higher for less relevant pairs of
topics. We selected 800 pairs of topics. 600 pairs
represent topics with similar word distributions (in
the top 6 most relevant topics ranked by JSD). The
remaining 200 pairs were selected randomly.
CTM is trained using the EM algorithm4. The
number of topics to learn is set to T =
50,100, 200 and the rest of the settings are set to
their default values. The topic graph generated by
CTM was used to create all the possible pairs be-
tween topics that are connected. This results in a
total of 70, 468 and 695 pairs in NYT, and a total
of 80, 246 and 258 pairs in ukWAC for the 50, 100
and 200 topics respectively.
Incoherent topics are removed using an ap-
proach based on distributional semantics (Aletras
and Stevenson, 2013). Each topic is represented
using the top 10 words with the highest marginal
probability.
Human Judgements of Topic Similarity were
obtained using an online crowdsourcing platform,
Crowdflower. Annotators were provided with
pairs of topics and were asked to judge how simi-
lar the topics are by providing a rating on a scale of
0 (completely unrelated) to 5 (identical). The av-
erage response for each pair was calculated in or-
der to create the final similarity judgement for use
as a gold-standard. The average Inter-Annotator
</bodyText>
<footnote confidence="0.975747">
3http://radimrehurek.com/gensim
4http://www.cs.princeton.edu/˜blei/
ctm-c/index.html
</footnote>
<bodyText confidence="0.99339725">
agreement (IAA) across all pairs for all of the col-
lections is in the range of 0.53-0.68. The data set
together with gold-standard annotations is freely
available5.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99994625">
Table 1 shows the correlation (Spearman) between
the topic similarity metrics described in Section 2
and average human judgements for the LDA and
CTM topic pairs. It also shows the performance
of a Word Overlap baseline which measures the
number of terms that two topics have in common
normalised by the total number of topic terms.
The correlations obtained using the topics’
word probability distributions (Section 2.1), i.e.
JSD, KL-divergence and Cos, are comparable with
the baseline for all of the topic collections and
topic models. The metric proposed by Chaney
and Blei (2012) also compares probability distri-
butions and fails to perform well on either data
set. These results suggest that these metrics may
be sensitive to the high dimensionality of the vo-
cabulary. They also assign high similarity to top-
ics that contain ambiguous words, resulting in low
correlations with human judgements.
Performance of the cosine of the word vec-
tor (TS-Cos) in the Topic Model Semantic Space
(Section 2.2) varies implying that the quality of the
latent space generated by LDA and CTM is sensi-
tive to the number of topics.
The similarity metrics that use the reference
corpus (Section 2.3) consistently produce good
correlations for topic pairs generated using both
LDA and CTM. The best overall correlation for a
single feature in most cases is obtained using av-
erage PMI (in a range of 0.43-0.74). The perfor-
mance of the distributional semantic metric using
the Topic Word Space (RCS-Cos-TWS) is com-
parable and slightly lower for the top-N features
(RCS-Cos-N). This indicates that the reference
corpus covers a broader range of semantic subjects
than the latent space produced by the topic model.
When the term-document matrix from the train-
ing corpus is used as a vector space (Section 2.4)
performance is worse than when the reference
corpus is used. In addition, using co-document
frequency derived from the training corpus does
not correlate particularly well with human judge-
ments. These methods are sensitive to the size
of the corpus, which may be too small to gener-
</bodyText>
<footnote confidence="0.980145333333333">
5http://staffwww.dcs.shef.ac.uk/
people/N.Aletras/resources/topicSim.
tar.gz
</footnote>
<page confidence="0.997655">
24
</page>
<figure confidence="0.866363357142857">
Spearman’s r
LDA CTM
NYT ukWAC NYT ukWAC
Method 50 100 200 50 100 200 50 100 200 50 100 200
Word Overlap
Baseline
0.32
0.49
0.35
0.32 0.40 0.51 0.22
0.41 0.56 0.45
0.53
0.33
Topic Word Probability Distribution
</figure>
<table confidence="0.8847825">
JSD 0.37 0.44 0.53 0.29 0.30 0.34 0.59 0.43 0.49 0.38 0.34 0.60
KL-Divergence 0.29 0.29 0.41 0.20 0.24 0.33 0.54 0.39 0.56 0.31 0.29 0.47
Cos 0.31 0.37 0.59 0.30 0.30 0.36 0.58 0.45 0.52 0.50 0.40 0.58
Chaney and Blei (2012) 0.16 0.26 0.18 0.29 0.21 0.25 0.29 0.40 0.31 -0.23 0.12 0.61
</table>
<figure confidence="0.97214240909091">
Topic Model Semantic Space
0.51
0.57
0.60
0.58
0.42
0.35 0.41 0.67 0.29 0.35 0.42 0.67 0.51 0.49
Reference Corpus Semantic Space
0.37 0.46 0.61 0.35 0.32 0.39 0.60 0.47 0.61
0.40 0.54 0.70 0.38 0.43 0.51 0.63 0.59 0.62
0.43 0.63 0.74 0.43 0.53 0.64 0.68 0.70 0.64
TS-Cos
RCS-Cos-N
RCS-Cos-TWS
PMI
0.42
0.41
0.54
0.64
0.42
0.55
0.62
</figure>
<table confidence="0.911226133333333">
Training Corpus Semantic Space
0.42 0.67 0.29 0.31 0.40 0.64 0.54 0.58
0.29 0.45 0.28 0.22 0.30 0.65 0.36 0.57
Knowledge-based
0.38 0.56 0.22 0.35 0.41 0.52 0.41 0.40
0.58 0.71 0.46 0.55 0.61 0.69 0.67 0.64
Feature Combination
0.64 0.75 0.46 0.58 0.66 0.72 0.71 0.62
0.58 0.61 0.53 0.56 0.60 0.68 0.68 0.64
TCS-Cos-TD 0.36
Doc-Co-occ 0.28
UKB 0.25
ESA 0.43
SVR 0.46
IAA 0.54
</table>
<figure confidence="0.9895925">
0.49
0.31
0.41
0.70
0.60
0.67
0.43
0.26
0.43
0.62
0.65
0.63
0.43
0.34
0.42
0.61
0.66
0.64
</figure>
<tableCaption confidence="0.8424845">
Table 1: Results for various approaches to topic similarity. All correlations are significant p &lt; 0.001.
Underlined scores denote best performance of a single feature. Bold denotes best overall performance.
</tableCaption>
<bodyText confidence="0.999414888888889">
ate reliable estimates of tf.idf or co-document fre-
quency.
ESA, one of the knowledge-based methods
(Section 2.5), performs well and is comparable to
(or in some cases better than) PMI. UKB does
not perform particularly well because the topics
often contain named entities that do not exist in
WordNet. ESA is based on Wikipedia and does
not suffer from this problem. Overall, metrics for
computing topic similarity based on rich semantic
resources (e.g. Wikipedia) are more appropriate
than metrics based on the topic model itself be-
cause of the limited size of the training corpus.
Combining the features using SVR gives the
best overall result for LDA (in the range 0.46-
0.75) and CTM (0.60-0.72). However, the fea-
ture combination performs slightly lower than the
best single feature in two cases when CTM is
used (T=200, NYT and T=50, ukWAC). Analy-
sis of the coefficients produced by the SVR in
each fold demonstrated that including JSD and
the Word Overlap reduce SVR performance. We
repeated the experiments by removing these fea-
tures6 which resulted in higher correlations (0.64
and 0.65 respectively).
Another interesting observation is that using
LDA the correlations of the various similarity met-
</bodyText>
<footnote confidence="0.870949">
6These features are useful for the other experiments since
performance drops when they are removed.
</footnote>
<bodyText confidence="0.999261833333333">
rics with human judgements increase with the
number of topics for both corpora. This result
is consistent with the findings of Stevens et al.
(2012) that topic model coherence increases with
the number of topics. Fewer topics makes the task
of identifying similar topics more difficult because
it is likely that they will contain some terms that do
not relate to the topic’s main subject. Correlations
in CTM are more stable for different number of
topics because of the nature of the model, the pairs
have been generated using the topic graph which
by definition contains correlated topics.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998525">
We explored the task of determining the similar-
ity between pairs of automatically generated top-
ics and described a range of approaches to the
problem. We constructed a data set of pairs of
topics generated by two topic models, LDA and
CTM, together with human judgements of simi-
larity. The data set was used to evaluate a wide
range of approaches. The most interesting finding
is the poor performance of the metrics based on
word probability distributions previously used for
this task. Our results demonstrate that word asso-
ciation measures, such as PMI, and state-of-the-art
textual similarity metrics, such as ESA, are more
appropriate.
</bodyText>
<page confidence="0.997389">
25
</page>
<sectionHeader confidence="0.989099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997377398148148">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL-HLT ’09), pages 19–27, Boulder, Col-
orado.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) – Long
Papers, pages 13–22, Potsdam, Germany.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209–226.
David Blei and John Lafferty. 2006. Correlated topic
models. In Y. Weiss, B. Sch¨olkopf, and J. Platt,
editors, Advances in Neural Information Processing
Systems 18, pages 147–154. MIT Press, Cambridge,
MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
David Blei, Lawrence Carin, and David Dunson. 2010.
Probabilistic topic models. Signal Processing Mag-
azine, IEEE, 27(6):55–65.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 76–83, Vancouver, British Columbia, Canada.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI ’07), pages 1606–1611.
Brynjar Gretarsson, John O’Donovan, Svetlin Bostand-
jiev, Tobias H¨ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 363–371, Honolulu, Hawaii.
Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mi-
tra, and Lee Giles. 2009. Detecting topic evolution
in scientific literature: how can citations help? In
Proceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM ’09),
pages 957–966, Hong Kong, China.
Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838–841. Springer Berlin Heidelberg.
Dongwoo Kim and Alice Oh. 2011. Topic chains
for understanding a news corpus. In Computational
Linguistics and Intelligent Text Processing, pages
163–176. Springer.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning (ICML ’06), pages
577–584.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
262–272, Edinburgh, Scotland, UK.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. J. Mach. Learn. Res., 10:1801–1828.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT ’10), pages 100–108, Los
Angeles, California.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA:
A supervised topic model for credit attribution in
multi-labeled corpora. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ’09), pages 248–256,
Singapore.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring topic co-
herence over many models and many topics. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP
’12), pages 952–961, Jeju Island, Korea.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
</reference>
<page confidence="0.949408">
26
</page>
<reference confidence="0.996075166666667">
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Sec-
ond ACM International Conference on Web Search
and Data Mining (WSDM ’09), pages 192–201,
Barcelona, Spain.
</reference>
<page confidence="0.998808">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272734">
<title confidence="0.998218">Measuring the Similarity between Automatically Generated Topics</title>
<author confidence="0.820728">Aletras</author>
<affiliation confidence="0.986684">Department of Computer University of</affiliation>
<note confidence="0.517948">Regent Court, 211 United Kingdom S1</note>
<abstract confidence="0.997970333333334">Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topics’ word probability distributions. This paper presents alternative approaches, including ones based on distributional semantics and knowledgebased measures, evaluated by comparison with human judgements. The best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT ’09),</booktitle>
<pages>pages</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6943" citStr="Agirre et al., 2009" startWordPosition="1091" endWordPosition="1094">of the topic word vectors (TCS-Cos-TD). Word Co-occurrence in Training Documents Alternatively, we generate a matrix Z of codocument frequencies. The matrix Z consists of V rows and columns representing the V vocabulary words. Element zij is the log of the number of documents that contains the words i and j normalised by the document frequency, DF, of the word j. Mimno et al. (2011) introduced that metric to measure topic coherence. We adapted it to estimate topic similarity by aggregating the co-document frequency of the words between two topics (Doc-Co-occ). 2.5 Knowledge-based Methods UKB (Agirre et al., 2009) is used to generate a probability distribution over WordNet synsets for each word in the vocabulary V of the topic model using the Personalized PageRank algorithm. The similarity between two topic words is calculated by transforming these distributions into vectors and computing the cosine metric. The similarity between two topics is computed by measuring pairwise similarity between their top-10 topic words and selecting the highest score. Explicit Semantic Analysis (ESA) proposed by Gabrilovich and Markovitch (2007) transforms the topic keywords into vectors that consist of Wikipedia article</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT ’09), pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Aletras</author>
<author>Mark Stevenson</author>
</authors>
<title>Evaluating topic coherence using distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>13--22</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="4613" citStr="Aletras and Stevenson (2013)" startWordPosition="706" endWordPosition="709">relation between words and topics, T, as a T ×V matrix, W, that indicates the probability of each word in each topic. W is the topic model semantic space and each topic word can be represented as a vector, Vi, with topics as features weighted by the probability of the word in each topic. The similarity between two topics is computed as the average pairwise cosine similarity between their top-10 most probable words (TS-Cos). 2.3 Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1. Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi, wj)γ. -y is a variable for assigning more importance to higher PMI values. In our experiments, we set -y = 3 and found that the best performance is obtained for n = 5000. Similarity bet</context>
<context position="9902" citStr="Aletras and Stevenson, 2013" startWordPosition="1600" endWordPosition="1603">s (in the top 6 most relevant topics ranked by JSD). The remaining 200 pairs were selected randomly. CTM is trained using the EM algorithm4. The number of topics to learn is set to T = 50,100, 200 and the rest of the settings are set to their default values. The topic graph generated by CTM was used to create all the possible pairs between topics that are connected. This results in a total of 70, 468 and 695 pairs in NYT, and a total of 80, 246 and 258 pairs in ukWAC for the 50, 100 and 200 topics respectively. Incoherent topics are removed using an approach based on distributional semantics (Aletras and Stevenson, 2013). Each topic is represented using the top 10 words with the highest marginal probability. Human Judgements of Topic Similarity were obtained using an online crowdsourcing platform, Crowdflower. Annotators were provided with pairs of topics and were asked to judge how similar the topics are by providing a rating on a scale of 0 (completely unrelated) to 5 (identical). The average response for each pair was calculated in order to create the final similarity judgement for use as a gold-standard. The average Inter-Annotator 3http://radimrehurek.com/gensim 4http://www.cs.princeton.edu/˜blei/ ctm-c/</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>Nikolaos Aletras and Mark Stevenson. 2013. Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 13–22, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="8546" citStr="Baroni et al., 2009" startWordPosition="1344" endWordPosition="1347">proaches described above as features2. The system is trained and tested using 10-fold cross validation. 2With the exception of JSD, features based on the topics’ word probability distributions were not used by SVR since it was found that including them reduced performance. 23 3 Evaluation Data We created a data set consisting of pairs of topics generated by two topic models (LDA and CTM) over two document collections using different numbers of topics. The first consists of 47,229 news articles from New York Times (NYT) in the GigaWord corpus and the second contains 50,000 articles from ukWAC (Baroni et al., 2009). Each article is tokenised then stop words and words appearing fewer than five times in the corpora removed. This results in a total of 57,651 unique tokens for the NYT corpus and 72,672 for ukWAC. LDA Topics are learned by training LDA models over the two corpora using gensim3. The number of topics is set to T = 50,100, 200 and hyperparameters, α and β, are set to T . Randomly selecting pairs of topics will result to a data set in which the majority of pairs would not be similar. We overcome that problem by assuming that the JSD between likely relevant pairs will be low while it will be high</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>147--154</pages>
<editor>In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1997" citStr="Blei and Lafferty, 2006" startWordPosition="290" endWordPosition="293">le crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. 2006. Correlated topic models. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 147–154. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1781" citStr="Blei et al., 2003" startWordPosition="254" endWordPosition="257">knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures suc</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Lawrence Carin</author>
<author>David Dunson</author>
</authors>
<date>2010</date>
<booktitle>Probabilistic topic models. Signal Processing Magazine, IEEE,</booktitle>
<pages>27--6</pages>
<contexts>
<context position="865" citStr="Blei et al., 2010" startWordPosition="112" endWordPosition="115">s.shef.ac.uk Abstract Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topics’ word probability distributions. This paper presents alternative approaches, including ones based on distributional semantics and knowledgebased measures, evaluated by comparison with human judgements. The best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously. 1 Introduction Topic models (Blei et al., 2010) have proved to be useful for interpreting and organising the contents of large document collections. It seems intuitively plausible that some automatically generated topics will be similar while others are dis-similar. For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have </context>
</contexts>
<marker>Blei, Carin, Dunson, 2010</marker>
<rawString>David Blei, Lawrence Carin, and David Dunson. 2010. Probabilistic topic models. Signal Processing Magazine, IEEE, 27(6):55–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison June-Barlow Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International AAAI Conference on Weblogs and Social</booktitle>
<location>Media, Dublin, Ireland.</location>
<contexts>
<context position="1676" citStr="Chaney and Blei, 2012" startWordPosition="238" endWordPosition="241">while others are dis-similar. For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics an</context>
<context position="3475" citStr="Chaney and Blei, 2012" startWordPosition="518" endWordPosition="521">suring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), distributional semantic methods (Sections 2.2-2.4), knowledge-based approaches (Section 2.5) and their combination (Section 2.6). 2.1 Topic Word Probability Distribution We first experimented with measures based on comparison of the topics’ word distributions (see Section 1), by applying the JSD, KL-divergence and Cosine approaches and the Log Odds Ratio (Chaney and Blei, 2012). 22 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 22–27, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2.2 Topic Model Semantic Space The semantic space generated by the topic model can be used to represent the topics and the topic words. By definition each topic is a probability distribution over the words in the training corpus. For a corpus with D documents and V words, a topic model learns a relation between words and topics, T, as a T ×V matrix, W, that indicates the probability </context>
<context position="11273" citStr="Chaney and Blei (2012)" startWordPosition="1811" endWordPosition="1814">ions is freely available5. 4 Results Table 1 shows the correlation (Spearman) between the topic similarity metrics described in Section 2 and average human judgements for the LDA and CTM topic pairs. It also shows the performance of a Word Overlap baseline which measures the number of terms that two topics have in common normalised by the total number of topic terms. The correlations obtained using the topics’ word probability distributions (Section 2.1), i.e. JSD, KL-divergence and Cos, are comparable with the baseline for all of the topic collections and topic models. The metric proposed by Chaney and Blei (2012) also compares probability distributions and fails to perform well on either data set. These results suggest that these metrics may be sensitive to the high dimensionality of the vocabulary. They also assign high similarity to topics that contain ambiguous words, resulting in low correlations with human judgements. Performance of the cosine of the word vector (TS-Cos) in the Topic Model Semantic Space (Section 2.2) varies implying that the quality of the latent space generated by LDA and CTM is sensitive to the number of topics. The similarity metrics that use the reference corpus (Section 2.3</context>
<context position="13275" citStr="Chaney and Blei (2012)" startWordPosition="2143" endWordPosition="2146">ith human judgements. These methods are sensitive to the size of the corpus, which may be too small to gener5http://staffwww.dcs.shef.ac.uk/ people/N.Aletras/resources/topicSim. tar.gz 24 Spearman’s r LDA CTM NYT ukWAC NYT ukWAC Method 50 100 200 50 100 200 50 100 200 50 100 200 Word Overlap Baseline 0.32 0.49 0.35 0.32 0.40 0.51 0.22 0.41 0.56 0.45 0.53 0.33 Topic Word Probability Distribution JSD 0.37 0.44 0.53 0.29 0.30 0.34 0.59 0.43 0.49 0.38 0.34 0.60 KL-Divergence 0.29 0.29 0.41 0.20 0.24 0.33 0.54 0.39 0.56 0.31 0.29 0.47 Cos 0.31 0.37 0.59 0.30 0.30 0.36 0.58 0.45 0.52 0.50 0.40 0.58 Chaney and Blei (2012) 0.16 0.26 0.18 0.29 0.21 0.25 0.29 0.40 0.31 -0.23 0.12 0.61 Topic Model Semantic Space 0.51 0.57 0.60 0.58 0.42 0.35 0.41 0.67 0.29 0.35 0.42 0.67 0.51 0.49 Reference Corpus Semantic Space 0.37 0.46 0.61 0.35 0.32 0.39 0.60 0.47 0.61 0.40 0.54 0.70 0.38 0.43 0.51 0.63 0.59 0.62 0.43 0.63 0.74 0.43 0.53 0.64 0.68 0.70 0.64 TS-Cos RCS-Cos-N RCS-Cos-TWS PMI 0.42 0.41 0.54 0.64 0.42 0.55 0.62 Training Corpus Semantic Space 0.42 0.67 0.29 0.31 0.40 0.64 0.54 0.58 0.29 0.45 0.28 0.22 0.30 0.65 0.36 0.57 Knowledge-based 0.38 0.56 0.22 0.35 0.41 0.52 0.41 0.40 0.58 0.71 0.46 0.55 0.61 0.69 0.67 0.64</context>
</contexts>
<marker>Chaney, Blei, 2012</marker>
<rawString>Allison June-Barlow Chaney and David M. Blei. 2012. Visualizing topic models. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4970" citStr="Church and Hanks, 1989" startWordPosition="762" endWordPosition="765">ine similarity between their top-10 most probable words (TS-Cos). 2.3 Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1. Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi, wj)γ. -y is a variable for assigning more importance to higher PMI values. In our experiments, we set -y = 3 and found that the best performance is obtained for n = 5000. Similarity between two topics is defined as the average cosine similarity of the topic word vectors (RCS-Cos-N). Topic Word Space Alternatively, we consider only the top-10 topic words from the two topics as context features to generate topic word vectors. Then, topic similarity is computed as the pairwise cosine similarity of the topic word vectors (RCSCos-TWS). Word </context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 76–83, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI ’07),</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="7466" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="1169" endWordPosition="1172"> frequency of the words between two topics (Doc-Co-occ). 2.5 Knowledge-based Methods UKB (Agirre et al., 2009) is used to generate a probability distribution over WordNet synsets for each word in the vocabulary V of the topic model using the Personalized PageRank algorithm. The similarity between two topic words is calculated by transforming these distributions into vectors and computing the cosine metric. The similarity between two topics is computed by measuring pairwise similarity between their top-10 topic words and selecting the highest score. Explicit Semantic Analysis (ESA) proposed by Gabrilovich and Markovitch (2007) transforms the topic keywords into vectors that consist of Wikipedia article titles weighted by their relevance to the keyword. For each topic, the centroid is computed from the keyword vectors. Similarity between topics is computed as the cosine similarity of the ESA centroid vectors. 2.6 Feature Combination Using SVR We also evaluate the performance of a support vector regression system (SVR) (Vapnik, 1998) with a linear kernel using a combination of approaches described above as features2. The system is trained and tested using 10-fold cross validation. 2With the exception of JSD, features</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI ’07), pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brynjar Gretarsson</author>
<author>John O’Donovan</author>
<author>Svetlin Bostandjiev</author>
<author>Tobias H¨ollerer</author>
<author>Arthur Asuncion</author>
<author>David Newman</author>
<author>Padhraic Smyth</author>
</authors>
<title>TopicNets: Visual analysis of large text corpora with topic modeling.</title>
<date>2012</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>3</volume>
<issue>2</issue>
<marker>Gretarsson, O’Donovan, Bostandjiev, H¨ollerer, Asuncion, Newman, Smyth, 2012</marker>
<rawString>Brynjar Gretarsson, John O’Donovan, Svetlin Bostandjiev, Tobias H¨ollerer, Arthur Asuncion, David Newman, and Padhraic Smyth. 2012. TopicNets: Visual analysis of large text corpora with topic modeling. ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Studying the history of ideas using topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>363--371</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1580" citStr="Hall et al., 2008" startWordPosition="222" endWordPosition="225">ons. It seems intuitively plausible that some automatically generated topics will be similar while others are dis-similar. For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have </context>
</contexts>
<marker>Hall, Jurafsky, Manning, 2008</marker>
<rawString>David Hall, Daniel Jurafsky, and Christopher D. Manning. 2008. Studying the history of ideas using topic models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363–371, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi He</author>
<author>Bi Chen</author>
<author>Jian Pei</author>
<author>Baojun Qiu</author>
<author>Prasenjit Mitra</author>
<author>Lee Giles</author>
</authors>
<title>Detecting topic evolution in scientific literature: how can citations help?</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM ’09),</booktitle>
<pages>957--966</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="2496" citStr="He et al., 2009" startWordPosition="371" endWordPosition="374">y between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), d</context>
</contexts>
<marker>He, Chen, Pei, Qiu, Mitra, Giles, 2009</marker>
<rawString>Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mitra, and Lee Giles. 2009. Detecting topic evolution in scientific literature: how can citations help? In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM ’09), pages 957–966, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Hinneburg</author>
<author>Rico Preiss</author>
<author>Ren´e Schr¨oder</author>
</authors>
<title>TopicExplorer: Exploring document collections with topic models.</title>
<date>2012</date>
<booktitle>Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>7524</volume>
<pages>838--841</pages>
<editor>In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Hinneburg, Preiss, Schr¨oder, 2012</marker>
<rawString>Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder. 2012. TopicExplorer: Exploring document collections with topic models. In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors, Machine Learning and Knowledge Discovery in Databases, volume 7524 of Lecture Notes in Computer Science, pages 838–841. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongwoo Kim</author>
<author>Alice Oh</author>
</authors>
<title>Topic chains for understanding a news corpus.</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>163--176</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2592" citStr="Kim and Oh (2011)" startWordPosition="390" endWordPosition="393">ei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), distributional semantic methods (Sections 2.2-2.4), knowledge-based approaches (Section 2.5) and </context>
</contexts>
<marker>Kim, Oh, 2011</marker>
<rawString>Dongwoo Kim and Alice Oh. 2011. Topic chains for understanding a news corpus. In Computational Linguistics and Intelligent Text Processing, pages 163–176. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning (ICML ’06),</booktitle>
<pages>577--584</pages>
<contexts>
<context position="2422" citStr="Li and McCallum, 2006" startWordPosition="357" endWordPosition="360">f topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd International Conference on Machine Learning (ICML ’06), pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>262--272</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="6708" citStr="Mimno et al. (2011)" startWordPosition="1057" endWordPosition="1060">ent Space A matrix X can be created using the training corpus. Each term (row) represents a topic word vector. Element xij in X is the tf.idf of the term i in document j. Topic similarity is computed as the pairwise cosine similarity of the topic word vectors (TCS-Cos-TD). Word Co-occurrence in Training Documents Alternatively, we generate a matrix Z of codocument frequencies. The matrix Z consists of V rows and columns representing the V vocabulary words. Element zij is the log of the number of documents that contains the words i and j normalised by the document frequency, DF, of the word j. Mimno et al. (2011) introduced that metric to measure topic coherence. We adapted it to estimate topic similarity by aggregating the co-document frequency of the words between two topics (Doc-Co-occ). 2.5 Knowledge-based Methods UKB (Agirre et al., 2009) is used to generate a probability distribution over WordNet synsets for each word in the vocabulary V of the topic model using the Personalized PageRank algorithm. The similarity between two topic words is calculated by transforming these distributions into vectors and computing the cosine metric. The similarity between two topics is computed by measuring pairwi</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262–272, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>10--1801</pages>
<contexts>
<context position="2463" citStr="Newman et al., 2009" startWordPosition="365" endWordPosition="368">elations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures based on word probabili</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. J. Mach. Learn. Res., 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT ’10),</booktitle>
<pages>100--108</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="5688" citStr="Newman et al. (2010)" startWordPosition="880" endWordPosition="883">e importance to higher PMI values. In our experiments, we set -y = 3 and found that the best performance is obtained for n = 5000. Similarity between two topics is defined as the average cosine similarity of the topic word vectors (RCS-Cos-N). Topic Word Space Alternatively, we consider only the top-10 topic words from the two topics as context features to generate topic word vectors. Then, topic similarity is computed as the pairwise cosine similarity of the topic word vectors (RCSCos-TWS). Word Association Topic similarity can also be computed by applying word association measures directly. Newman et al. (2010) measure topic coherence as the average PMI between the topic words. This approach can be adapted to measure 1Wikipedia is used as a reference corpus to count word co-occurrences and frequencies using a context window of ±10 words centred on a topic word. topic similarity by computing the average pairwise PMI between the topic words in two topics (PMI). 2.4 Training Corpus Semantic Space Term-Document Space A matrix X can be created using the training corpus. Each term (row) represents a topic word vector. Element xij in X is the tf.idf of the term i in document j. Topic similarity is computed</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT ’10), pages 100–108, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP ’09),</booktitle>
<pages>248--256</pages>
<contexts>
<context position="2518" citStr="Ramage et al., 2009" startWordPosition="375" endWordPosition="379">is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), distributional semantic</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP ’09), pages 248–256, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Stevens</author>
<author>Philip Kegelmeyer</author>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Exploring topic coherence over many models and many topics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP ’12),</booktitle>
<pages>952--961</pages>
<location>Jeju Island,</location>
<contexts>
<context position="15795" citStr="Stevens et al. (2012)" startWordPosition="2565" endWordPosition="2568">T=50, ukWAC). Analysis of the coefficients produced by the SVR in each fold demonstrated that including JSD and the Word Overlap reduce SVR performance. We repeated the experiments by removing these features6 which resulted in higher correlations (0.64 and 0.65 respectively). Another interesting observation is that using LDA the correlations of the various similarity met6These features are useful for the other experiments since performance drops when they are removed. rics with human judgements increase with the number of topics for both corpora. This result is consistent with the findings of Stevens et al. (2012) that topic model coherence increases with the number of topics. Fewer topics makes the task of identifying similar topics more difficult because it is likely that they will contain some terms that do not relate to the topic’s main subject. Correlations in CTM are more stable for different number of topics because of the nature of the model, the pairs have been generated using the topic graph which by definition contains correlated topics. 5 Conclusions We explored the task of determining the similarity between pairs of automatically generated topics and described a range of approaches to the </context>
</contexts>
<marker>Stevens, Kegelmeyer, Andrzejewski, Buttler, 2012</marker>
<rawString>Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP ’12), pages 952–961, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="7879" citStr="Vapnik, 1998" startWordPosition="1235" endWordPosition="1236">two topics is computed by measuring pairwise similarity between their top-10 topic words and selecting the highest score. Explicit Semantic Analysis (ESA) proposed by Gabrilovich and Markovitch (2007) transforms the topic keywords into vectors that consist of Wikipedia article titles weighted by their relevance to the keyword. For each topic, the centroid is computed from the keyword vectors. Similarity between topics is computed as the cosine similarity of the ESA centroid vectors. 2.6 Feature Combination Using SVR We also evaluate the performance of a support vector regression system (SVR) (Vapnik, 1998) with a linear kernel using a combination of approaches described above as features2. The system is trained and tested using 10-fold cross validation. 2With the exception of JSD, features based on the topics’ word probability distributions were not used by SVR since it was found that including them reduced performance. 23 3 Evaluation Data We created a data set consisting of pairs of topics generated by two topic models (LDA and CTM) over two document collections using different numbers of topics. The first consists of 47,229 news articles from New York Times (NYT) in the GigaWord corpus and t</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N Vapnik. 1998. Statistical learning theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Wang</author>
<author>Kai Zhang</author>
<author>Xiaoming Jin</author>
<author>Dou Shen</author>
</authors>
<title>Mining common topics from multiple asynchronous text streams.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM ’09),</booktitle>
<pages>192--201</pages>
<location>Barcelona,</location>
<contexts>
<context position="2441" citStr="Wang et al., 2009" startWordPosition="361" endWordPosition="364">t capture such correlations unless the semantic similarity between topics is measured. Other topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. 2 Measuring Topic Similarity We compare measures b</context>
</contexts>
<marker>Wang, Zhang, Jin, Shen, 2009</marker>
<rawString>Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen. 2009. Mining common topics from multiple asynchronous text streams. In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM ’09), pages 192–201, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>