<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.996788">
Real-Time Spoken Language Translation
Using Associative Processors
</title>
<author confidence="0.784345">
Kozo 0i, Eiichiro Sumita, Osamu Furuse, Hitoshi Iida and Tetsuya Higuchit
</author>
<affiliation confidence="0.624901">
ATR Interpreting Telecommunications Research Laboratories
</affiliation>
<address confidence="0.706841">
2-2 Hikaridai, Seika, Souraku, Kyoto 619-02, JAPAN
</address>
<email confidence="0.625616">
foi,sumita,furuse,iidalOitl.atr.co.jp
</email>
<affiliation confidence="0.47117">
fElectrotechnical Laboratory
</affiliation>
<address confidence="0.914543">
1-1-4 Umezono, Tsukuba, Ibaraki 305, Japan
</address>
<email confidence="0.953716">
higuchifttl.go.jp
</email>
<sectionHeader confidence="0.996682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989772727273">
This paper proposes a model using associative
processors (APs) for real-time spoken language
translation. Spoken language translation re-
quires (1) an accurate translation and (2) a real-
time response. We have already proposed a
model, TDMT (Transfer-Driven Machine Trans-
lation), that translates a sentence utilizing ex-
amples effectively and performs accurate struc-
tural disambiguation and target word selection.
This paper will concentrate on the second re-
quirement. In TDMT, example-retrieval (ER),
i.e., retrieving examples most similar to an in-
put expression, is the most dominant part of the
total processing time. Our study has concluded
that we only need to implement the ER for ex-
pressions including a frequent word on APs. Ex-
perimental results show that the ER can be dras-
tically speeded up. Moreover, a study on com-
munications between APs demonstrates the scal-
ability against vocabulary size by extrapolation.
Thus, our model, TDMT on APs, meets the vital
requirements of spoken language translation.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811357142857">
Research on speech translation that began in the
mid-1980s has been challenging. Such research has
resulted in several prototype systems (Morimoto et
al., 1993; Kitano, 1991; Waibel et al., 1991). Speech
translation consists of a sequence of processes, i.e.,
speech recognition, spoken language translation and
speech synthesis. Each process must be accelerated
in order to achieve real-time response. This pa-
per focuses on the second process, spoken language
translation, which requires (1) an accurate trans-
lation and (2) a real-time response. We have al-
ready proposed a model that utilizes examples and
translates a sentence by combining pieces of trans-
fer knowledge, i.e., target language expressions that
correspond to source language expressions that cover
the sentence jointly. The model is called Transfer-
Driven Machine Translation (TDMT) (Furuse and
Iida, 1992; Furuse et al., 1994) (see subsection 2.1 for
details). A prototype system of TDMT which trans-
lates a Japanese spoken sentence into English, has
performed accurate structural disambiguation and
target word selectionl.
This paper will focus on the second requirement.
First, we will outline TDMT and analyze its com-
putational cost. Second, we will describe the con-
figuration, experimental results and scalability of
TDMT on associative processors (APs). Finally, we
will touch on related works and conclude.
</bodyText>
<sectionHeader confidence="0.93885" genericHeader="introduction">
2 TDMT and its Cost Analysis
</sectionHeader>
<subsectionHeader confidence="0.89716">
2.1 Outline of TDMT
</subsectionHeader>
<bodyText confidence="0.999016857142857">
In TDMT, transfer knowledge is the primary knowl-
edge, which is described by an example-based frame-
work (Nagao, 1984). A piece of transfer knowledge
describes the correspondence between source lan-
guage expressions (SEs) and target language expres-
sions (TEs) as follows, to preserve the translational
equivalence:
</bodyText>
<equation confidence="0.731288">
SE =&gt; TEi (Eii,
TEr, (En1, En2,...)
</equation>
<bodyText confidence="0.876831615384615">
Eij indicates the j-th example of TEL. For exam-
ple, the transfer knowledge for source expression &amp;quot;X
no Y&amp;quot; is described as follows2:
X no Y =&gt;
Y&apos; of X&apos; ((ronbun[paper],daimoku[titleD,- •
Y&apos; for X&apos; ((hoteru[hotel],yoyaku[reservation]),• •
Y&apos; in X&apos; ((Kyouto[Kyoto],kaigi[conference]),• •
The translation success rate for 825 sentences used as
learning data in a conference registration task, is about
98%. The translation success rate for 1,056 sentences,
amassed through arbitary inputs in the same domain, is
about 71%. The translation success rate increases as the
number of examples increases.
</bodyText>
<footnote confidence="0.617875">
&apos;X and Y are variables for Japanese words and X&apos;
and Y&apos; are the English translations of X and Y, respec-
tively; &amp;quot;no&amp;quot; is an adnominal particle that corresponds to
such English prepositions as &amp;quot;of,&amp;quot; &amp;quot;for,&amp;quot; &amp;quot;in,&amp;quot; and so on.
</footnote>
<page confidence="0.998381">
101
</page>
<bodyText confidence="0.99295275">
TDMT utilizes the semantic distance calculation
proposed by Sumita and Iida (Sumita and Iida,
1992). Let us suppose that an input, I, and each
example, Eij, consist of t words as follows:
</bodyText>
<equation confidence="0.999617">
I = (II, • • • ,I)
= (Eii1, • • • , Eiit)
</equation>
<bodyText confidence="0.817568">
Then, the distance between I and Eij is calculated
as follows:
</bodyText>
<equation confidence="0.99891">
d(I , Eii) = d((I1, • • • , (Eiji., • • • , Etit))
= E duk, Eiik) x Wk
k=1
</equation>
<bodyText confidence="0.995818571428571">
The semantic distance d(ik, Ek) between words
is reduced to the distance between concepts in a the-
saurus (see subsection 3.2 for details). The weight
Wk is the degree to which the word influences the
selection of the translation3.
The flow of selecting the most plausible TE is as
follows:
</bodyText>
<listItem confidence="0.94457385">
(1) The distance from the input is calculated for all
examples.
(2) The example with the minimum distance from
the input is chosen.
(3) The corresponding TE of the chosen example is
extracted.
Processes (1) and (2) are called ER (Example-
Retrieval) hereafter.
Now, we can explain the top-level TDMT algo-
rithm:
(a) Apply the transfer knowledge to an input sen-
tence and produce possible source structures in
which SEs of the transfer knowledge are com-
bined.
(b) Transfer all SEs of the source structures to the
most appropriate TEs by the processes (1)—(3)
above, to produce the target structures.
(c) Select the most appropriate target structure
from among all target structures on the basis
of the total semantic distance.
</listItem>
<bodyText confidence="0.99854925">
For example, the source structure of the following
Japanese sentence is represented by a combination
of SEs with forms such as (X no Y), (X ni Y), (X
de Y), (X ga Y) and so on:
</bodyText>
<subsectionHeader confidence="0.999787">
2.2 The Analysis of Computational Cost
</subsectionHeader>
<bodyText confidence="0.916928">
Here, we briefly investigate the TDMT processing
time on sequential machines.
For 746 test sentences (average sentence length:
about 10 words) comprising representative Japanese
</bodyText>
<footnote confidence="0.802816">
3In the TDMT prototype, Wk is 1/2.
</footnote>
<figure confidence="0.983669133333333">
100
90
80
70
60
Rate
(eye) 50
40
30
20
10
0
0-2 2-4 4-6 6-8 8-1010-
Translation time
in sequential TDMT (seconds)
</figure>
<figureCaption confidence="0.999973">
Figure 1: Rates for ER time in sequential TDMT
</figureCaption>
<bodyText confidence="0.980799824561404">
sentences4 in a conference registration task, the av-
erage translation time per sentence is about 3.53
seconds in the TDMT prototype on a sequential
machine (SPARCstation2). ER is embedded as a
subroutine call and is called many times during the
translation of one sentence. The average number of
ER calls per sentence is about 9.5. Figure 1 shows
rates for the ER time and other processing time.
The longer the total processing time, the higher the
rate for the ER time; the rate rises from about 43%
to about 85%. The average rate is 71%. Thus, ER is
the most dominant part of the total processing time.
In the ATR dialogue database (Ehara et al., 1990),
which contains about 13,000 sentences for a confer-
ence registration task, the average sentence length is
about 14 words. We therefore assume in the remain-
der of this subsection and subsection 3.5 that the av-
erage sentence length of a Japanese spoken sentence
is 14 words, and use statistics for 14-word sentences
when calculating the times of a large-vocabulary
TDMT system. The expected translation time of
each 14-word sentence is about 5.95 seconds, which
is much larger than the utterance time. The ex-
pected number of ER calls for each 14-word sen-
tence is about 15. The expected time and rate for
ER of the 14-word sentence are about 4.32 seconds
and about 73%, respectively.
Here, we will consider whether a large-vocabulary
TDMT system can attain a real-time response.
In the TDMT prototype, the vocabulary size and
the number of examples, N, are about 1,500 and
12,500, respectively. N depends on the vocab-
ulary size. The vocabulary size of the average
commercially-available machine translation system
is about 100,000. Thus, in the large-vocabulary sys-
&apos;We have 825 test sentences as described in footnote 1
in section 1. These sentences cover basic expressions that
are used in Japanese ability tests conducted by the gov-
ernment and Japanese education courses used by many
schools for foreigners (Uratani et al., 1992). The sen-
tences were reviewed by Japanese linguists. In the ex-
periments in this paper, we used 746 sentences excluding
sentences translated by exact-match.
dainihan
second version,
kaigi
conference,
no annaisyo
particle, announcement,
de happyou-sareru
particle, be presented,
ni
particle,
ronbun
paper,
no daimoku ga notte-orimasu
particle, title, particle, be written
</bodyText>
<figure confidence="0.975770333333334">
Other
processing
time
Example-
retrieval
time
</figure>
<page confidence="0.987769">
102
</page>
<bodyText confidence="0.999713086956522">
tern, N is about 830,000 12, 500 x 100, 000/1, 500)
in direct proportion to the vocabulary size. For the
sake of convenience, we assume N = 1, 000, 000.
The ER time is nearly proportional to N due to
process (1) described in subsection 2.1. Therefore,
the expected translation time of a 14-word sentence
in the large-vocabulary system using a SPARCsta-
tion2 (28.5 MIPS) is about 347.2 (=[ER time]d[other
processing time5]=[4.32 x 1, 000, 000/12, 500]-1[5.95 —4.32]=345.6+1.63) seconds. ER consumes 99.5% of
the translation time.
A 4,000 MIPS sequential machine will be avail-
able in 10 years, since MIPS is increasing at a rate
of about 35 % per year; we already have a 200
MIPS machine (i.e. DEC alpha/7000). The trans-
lation time of the large-vocabulary system with the
4,000 MIPS machine is expected to be about 2.474
(a., 347.2 x 28.5/4, 000) seconds. Of the time, 2.462
(a., 345.6 x 28.5/4, 000) seconds will be for ER. There-
fore, although the 1500-word TDMT prototype will
run quickly on the 4,000 MIPS machine, sequential
implementation will not be scalable, in other words,
the translation time will still be insufficient for real-
time application. Therefore, we have decided to uti-
lize the parallelism of associative processors.
Careful analysis of the computational cost in the
sequential TDMT prototype has revealed that the
ER for the top 10 SEs (source language expressions)
accounts for nearly 96% of the entire ER time. The
expected number of ER calls for the top 10 SEs of
each 14-word sentence is about 6. Table 1 shows
rates of the ER time against each SE in the trans-
fer knowledge. Function words, such as &amp;quot;wa&amp;quot;, &amp;quot;no&amp;quot;,
&amp;quot;o&amp;quot;, &amp;quot;ni&amp;quot; and &amp;quot;ga&amp;quot;, in the SEs are often used in
Japanese sentences. They are polysemous, thus,
their translations are complicated. For that rea-
son, the number of examples associated with these
SEs is very large. In sum, the computational cost
of retrieving examples including function words is
proportional to the square of the frequency of the
function words. In an English-to-Japanese version
of TDMT, the number of examples associated with
the SEs, which include function words such as &amp;quot;by&amp;quot;,
&amp;quot;to&amp;quot; and &amp;quot;of&amp;quot;, is very large as well.
With this rationale, we decided to parallelize ER
for the top 10 SEs of the Japanese-to-English trans-
fer knowledge.
</bodyText>
<tableCaption confidence="0.998818">
Table 1: Rates of ER time against each SE
</tableCaption>
<table confidence="0.99967125">
SE Rate(%) Accumulative(%)
X wa Y 25.20 25.20
X no Y 20.60 45.80
X o Y 19.61 65.41
X ni Y 11.13 76.54
X ga Y 8.90 85.44
. . •
.
</table>
<footnote confidence="0.620535">
5This time does not depend on N.
</footnote>
<sectionHeader confidence="0.4690775" genericHeader="method">
3 TDMT Using Associative
Processors
</sectionHeader>
<subsectionHeader confidence="0.741881">
3.1 ER on Associative Processors (APs)
</subsectionHeader>
<bodyText confidence="0.9998814">
As described in the previous subsection, parallelizing
ER is inevitable but promising. Preliminary experi-
ments of ER on a massively parallel associative pro-
cessor IXM2 (Higuchi et al., 1991a; Higuchi et al.,
1991b) have been successful (Sumita et al., 1993).
The IXM2 is the first massively parallel associative
processor that clearly demonstrates the computing
power of a large Associative Memory (AM). The AM
not only features storage operations but also logical
operations such as retrieving by content. Parallel
search and parallel write are particularly important
operations. The IXM2 consists of associative pro-
cessors (APs) and communication processors. Each
AP has an AM of 4K words of 40 bits, plus an IMS
T801 Transputer (25 MHz).
</bodyText>
<subsectionHeader confidence="0.994081">
3.2 Semantic Distance Calculation on APs
</subsectionHeader>
<bodyText confidence="0.9997871">
As described in subsection 2.1, the semantic distance
between words is reduced to the distance between
concepts in a thesaurus. The distance between con-
cepts is determined according to their positions in
the thesaurus hierarchy. The distance varies from 0
to 1. When the thesaurus is (n + 1) layered, (kin)
is connected to the classes in the k-th layer from the
bottom (0 &lt; k &lt; n). In Figure 2, n is 3, k is from
0 to 3, and the distance d is 0/3 (=0), 1/3, 2/3 and
3/3 (=1) from the bottom.
The semantic distance is calculated based on the
thesaurus code, which clearly represents the the-
saurus hierarchy, as in Table 2, instead of travers-
ing the hierarchy. Our n is 3 and the width of each
layer is 10. Thus, each word is assigned a three-
digit decimal code of the concept to which the word
corresponds.
Here, we briefly introduce the semantic distance
calculation on an AM (Associative Memory) refer-
ring to Figure 3. The input data is 344 which is the
</bodyText>
<figureCaption confidence="0.980968">
Figure 2: Thesaurus (portion) and distance
</figureCaption>
<equation confidence="0.572096588235294">
d( &apos;W1, &amp;quot;W3&amp;quot;)
=d( &amp;quot;C1&amp;quot;, &amp;quot;C2&amp;quot; )
=1/3
WORD
CONCEPT
thesaurus root
d(W1&amp;quot;, &amp;quot;W5&amp;quot;)
= 1
d(&amp;quot;W1&amp;quot;, &amp;quot;W4&amp;quot;)
= 2/3
d(W1&amp;quot;, &amp;quot;W3&amp;quot;)
=1/3
d(&amp;quot;W1&amp;quot;, &amp;quot;W2&amp;quot;)
= 0
841
&amp;quot; &amp;quot; &amp;quot;W3&amp;quot;
&amp;quot;C
</equation>
<page confidence="0.720595">
85
850
103
</page>
<tableCaption confidence="0.961823">
Table 2: Semantic distance by thesaurus code.
The input code and example code are CI
CI1Cl2CI3, CE=CEiCE2CE3.
</tableCaption>
<table confidence="0.9887806">
Condition Example Dist.
C/IC/2C/3 = C.E10E2CE3 347 , 347 0
ail C/2 = CEICE2, C/3 0 CE3 347 , 346 1/3
C.Ti = CEi, C./2 0 CE2 347 , 337 2/3
C/i 0 CEi 347 , 247 1
</table>
<figureCaption confidence="0.978138">
Figure 3: Semantic distance calculation on an Asso-
ciative Memory
</figureCaption>
<bodyText confidence="0.9989551875">
thesaurus code of the word &amp;quot;uchiawase[meetine
Each code (316, 344) of the examples such as
&amp;quot;teisha[stoppine , &amp;quot;kaigi[conference]&amp;quot; , and so on is
stored in each word of the AM. The algorithm for
searching for examples whose distance from the in-
put is 0, is as follows6:
(I) Give a command that searches for the words
whose three-digit code matches the input. (The
search is performed on all words simultaneously
and matched words are marked.)
(II) Get the addresses of the matched words one by
one and add the distance, 0, to the variable that
corresponds to each address.
The search in process (I) is done only by the AM
and causes the acceleration of ER. Process (II) is
done by a transputer and is a sequential process.
</bodyText>
<subsectionHeader confidence="0.999844">
3.3 Configuration of TDMT Using APs
</subsectionHeader>
<bodyText confidence="0.99932">
According to the performance analysis in subsection
2.2, we have implemented the ER of the top 10 SEs.
Figure 4 shows a TDMT configuration using APs
in which the ER of the top 10 SEs are imple-
mented. The 10 APs AP2, • • • , AP10) and
the transputer (TP) directly connected to the host
machine (SPARCstation2) are connected in a tree
configuration7.
</bodyText>
<footnote confidence="0.995958428571429">
6An algorithm that searches for examples whose dis-
tance from the input is 1/3, 2/3 or 3/3, is similar.
&apos;The tree is 3-array because the transputer has four
connectors. The TDMT main program is described with
Lisp language and is executed on the host machine. The
ER routine is programmed with Occam2 language, which
is called by the main program and runs on the TP and
</footnote>
<figureCaption confidence="0.999957">
Figure 4: Configuration of TDMT using 10 APs
</figureCaption>
<bodyText confidence="0.9704375">
The algorithm for ER in the TDMT using APs is
as follows:
</bodyText>
<listItem confidence="0.990625222222222">
(i) Get input data and send the input data from
the host to TP.
(ii) Distribute the input data to all APs.
(iii) Each AP carries out ER, and gets the minimum
distance and the example number whose dis-
tance is minimum.
(iv) Each AP and the TP receive the data from the
lower APs (if they exist), merge them and their
own result, and send the merged result upward.
</listItem>
<bodyText confidence="0.9996072">
With the configuration shown in Figure 4, we
studied two different methods of storing examples.
The two methods of storing examples are as follows:
Homo-loading (HM) Examples associated with
one SE are stored in one AP. That is, each AP
is loaded with examples of the same SE.
Hetero-loading (HT) Examples associated with
one SE are divided equally and stored in 10
APs. That is, each AP is loaded with exam-
ples of 10 different SEs.
</bodyText>
<subsectionHeader confidence="0.975215">
3.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.994702833333334">
Figure 5 plots the speedup of ER for TDMT using
APs over sequential TDMT, with the two methods.
It can be seen that the speedup for the HT method
is greater than that for the HM method, partly be-
cause the sequential part of ER is proportional to the
example number in question. With the HT method,
</bodyText>
<figure confidence="0.5429165">
0-2 2-4 4-6 6-8 8-10 10-
Translation time in sequential TDMT (seconds)
</figure>
<figureCaption confidence="0.991664">
Figure 5: Speedup of ER in TDMT using APs over
sequential TDMT
</figureCaption>
<bodyText confidence="0.569647">
on transputers in the APs.
</bodyText>
<figure confidence="0.995936818181818">
uchiawase
[ meeting ]
Thesaurus code
(4,pite43)
3 4 4
Input data
Mark Address
Associative Memory
111
teisha
[ stopping ]
kaigi
[ conferenc
one wordHH
NICK3111111
EMU ofl
Host
machine
30
25
0. 20
5
</figure>
<page confidence="0.996516">
104
</page>
<bodyText confidence="0.997479571428571">
the average speedup is about 16.4 (=[the average time
per sentence in the sequential TDM11/[the average time
per sentence in the HT method] =-2 2489.7/152.2(msec.)).
For the 14-word sentences, the average speedup is
about 20.8 4324.7/208.0(msec.)) and the ER time
for the top 10 SEs is about 85.4 milliseconds out of
the total 208.0 milliseconds.
</bodyText>
<figureCaption confidence="0.971948625">
Figure 6 shows a screen giving a comparison be-
tween TDMT using APs and sequential TDMT.
Figure 6: A comparison of TDMT using APs and se-
quential TDMT — This is a snapshot of a race between
two machines. The sentence numbers and run times cor-
respond to sentences that have been translated. The
average times cover all sentences that have been trans-
lated.
</figureCaption>
<subsectionHeader confidence="0.946611">
3.5 Scalability
</subsectionHeader>
<bodyText confidence="0.997139863636363">
In this subsection, we consider the scalability of
TDMT using APs in the HT method. Here, we
will estimate the ER time using 1,000,000 examples
which are necessary for a large-vocabulary TDMT
system (see subsection 2.2).
Assuming that the number of examples in each
AP is the same as that in the experiment, 800 (=
1, 000, 000/12,500) APs are needed to store 1,000,000
examples. Figure 7 shows 800 APs in a tree struc-
ture (EL 3X &gt; 800; L(minimum)=6 layers). In the
remainder of this subsection, we will use the statis-
tics (time, etc.) for the 14-word8 sentences.
The translation time is divided into the ER time
on APs and the processing time on the host machine.
The former is divided into the computing time on
each AP and the communication time between APs.
The ER time on APs in the experiment is about
85.4 milliseconds as described in subsection 3.4. The
computing time per sentence on each AP is the same
as that in the experiment and is approximately 84.1
milliseconds out of the 85.4 milliseconds. The com-
munication time between APs is vital and increases
</bodyText>
<footnote confidence="0.7131215">
8This is the average sentence length in the ATR dia-
logue database. See subsection 2.2.
</footnote>
<figureCaption confidence="0.999316">
Figure 7: Configuration of large-vocabulary TDMT
using 800 APs
</figureCaption>
<bodyText confidence="0.962628921052632">
as the number of APs increases. There are two kinds
of communication processes: distribution of input
datas and collection of the resulting data of ER&apos;s.
The input data distribution time is the sum of
distribution times TP--0. APi , APi AP2, • • •,
AP4-4P5 and AP5—A136, that is, 6 multiplied by
the distribution time between two APs that are di-
rectly connected (see Figure 7), because a transputer
can send the data to the other transputers directly
connected in parallel (e.g., AP5.A136, AP5--■AP7,
AP5.A138). The average number of ER calls is
about 6 and the average distribution time between
directly-connected APs is about 0.05 milliseconds.
Therefore, the total input data distribution time per
sentence in the configuration of Figure 7 is nearly 1.8
(= 0.05 x 6 x 6) milliseconds.
The time required to collect the resulting data
is the sum of the processing times in process (iv),
which is explained in subsection 3.3, at the TP,
API, • • • , AP4 and AP5, illustrated in Figure 7. It
takes about 0.04 milliseconds, on average, for each
AP to receive the resulting data from the lower
APs and it takes about 0.02 milliseconds, on av-
erage, for the AP to merge the minimum distance
and the example numbers. Therefore, it is ex-
pected that the total collection time is about 2.2
(= (0.04 + 0.02) x 6 x 6) milliseconds.
Thus, the total communication time is about 4.0
(= 1.8 + 2.2) milliseconds. Consequently, the pro-
cessing time on APs is about 88.1 (= 84.1 + 4.0) mil-
liseconds. This is 3,920 (z-. 345.6/0.0881) times faster
than the SPARCstation211. It is clear then that the
communication has little impact on the scalability
because it is controlled by the tree depth and small
coefficient.
Therefore, the TDMT using APs becomes more
scalable as the number of examples increases and
can attain a real-time response.
</bodyText>
<footnote confidence="0.99801">
9Process (ii) described in subsection 3.3.
19Process (iv) described in subsection 3.3.
11See the data described in subsection 2.2.
</footnote>
<figure confidence="0.994042033333333">
Trarsfer-Drtggn Nachine Translation *gtes(TCOM• •
Translate(T) Display(0)
&apos;.&apos;..Se4ngntinf.transtation(l&apos;ARcstation2)..
Sentence Number! 8
Input sentence iifigilgtC-T4in t&apos;LtF86
- •
Translation Result!Please proceed with the registration form
FILeTiii.(NOIlseconds) 7 Mirage Time(Milliseconan)
toko xoe xoo .0;00 5000 ROO 7000 mos
11103.1111111:22
&apos;Massively Para/lel Trans1it1an(jW2)
Sentence klA.1
Input Sentence ;...t.11.-Ci4.1 L.
Translation Resultigood -bye
Run Thw(Ninrsecon.cts)
Average Tine(Millisecondn)
.6
o 3030 ■ON. Xoo mix moo aioo
• • 0
• • • • • • • •
lViVi \ I VI
0 : AP
(Associative
Processor)
6 layers
Host
machine
• • 0
••
!
</figure>
<page confidence="0.995066">
105
</page>
<sectionHeader confidence="0.999443" genericHeader="method">
4 Related works
</sectionHeader>
<bodyText confidence="0.999889933333333">
Up to now, some systems using a massively par-
allel machine in the field of natural language pro-
cessing, such as a parsing system (Kitano and
Higuchi, 1991b) and translation systems, e.g., Dm-
SNAP (Kitano et al., 1991), ASTRAL (Kitano and
Higuchi, 1991a), MBT3n (Sato, 1993), have been
proposed. They have demonstrated good perfor-
mance; nonetheless, they differ from our proposal.
For the first three systems, their domain is much
smaller than our domain and they do not perform
structural disambiguation or target word selection
based on the semantic distance between an input
expression and each example. For the last system,
it translates technical terms i.e. noun phrases, but
not sentences.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962190476191">
This paper has proposed TDMT (Transfer-Driven
Machine Translation) on APs (Associative Proces-
sors) for real-time spoken language translation. In
TDMT, a sentence is translated by combining pieces
of transfer knowledge that are associated with ex-
amples, i.e., source word sequences. We showed that
the ER (example-retrieval) for source expressions in-
cluding a frequent word, such as a function word,
are predominant and are drastically speeded up us-
ing APs. That the TDMT using APs is scalable
against vocabulary size has also been confirmed by
extrapolation, i.e., a 10-AP sustained performance
to an 800-AP expected performance, through analy-
sis on communications between APs. Consequently,
the TDMT can achieve real-time performance even
with a large-vocabulary system. In addition, as our
previous papers have shown, the TDMT achieves
accurate structural disambiguation and target word
selection. Thus, our model, TDMT on APs, meets
the vital requirements for real-time spoken language
translation.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999092027777778">
Terumasa Ehara, Kentaro Ogura, and Tsuyoshi Mo-
rimoto. 1990. ATR Dialogue Database. In Proc.
of ICSLP&apos;90, pages 1093-1096, November.
Osamu Furuse and Hitoshi Iida. 1992. Cooperation
Between Transfer and Analysis in Example-Based
Framework. In Proc. of COLING&apos;92, pages 645-
651, July.
Osamu Furuse, Eiichiro Sumita, and Hitoshi Iida.
1994. Transfer Driven Machine Translation Uti-
lizing Empirical Knowledge. Transactions of In-
formation Processing Society of Japan, 35(3):414-
425, March.
Tetsuya Higuchi, Tatsumi Furuya, Kenichi Handa,
Naoto Takahashi, Hiroyasu Nishiyama, and Akio
Kokubu. 1991a. IXM2 : A Parallel Associative
Processor. In Proc. of the 18th International Sym-
posium on Computer Architecture, May.
Tetsuya Higuchi, Hiroaki Kitano, Tatsumi Furuya,
Ken-ichi Handa, Naoto Takahashi, and Akio
Kokubu. 1991b. IXM2 : A Parallel Associative
Processor for Knowledge Processing. In Proc. of
AAAI&apos;91, pages 296-303, July.
Hiroaki Kitano and Tetsuya Higuchi. 1991a. High
Performance Memory-Based Translation on IXM2
Massively Parallel Associative Memory Processor.
In Proc. of AAAI&apos;91, pages 149-154, July.
Hiroaki Kitano and Tetsuya Higuchi. 1991b. Mas-
sively Parallel Memory-Based Parsing. In Proc.
of IJCAI&apos;91, pages 918-924.
Hiroaki Kitano, Dan Moldovan, and Seungho Cha.
1991. High Performance Natural Language Pro-
cessing on Semantic Network Array Processor. In
Proc. of IJCAI&apos;91, pages 911-917.
Hiroaki Kitano. 1991. (I)DM-Dialog: An Experi-
mental Speech-to-Speech Dialog Translation Sys-
tem. IEEE Computer, 24(6):36-50, June.
Tsuyoshi Morimoto, Toshiyuki Takezawa, Fumi-
hiro Yato, Shigeki Sagayama, Toshihisa Tashiro,
Masaaki Nagata, and Akira Kurematsu. 1993.
ATR&apos;s Speech Translation System: ASURA. In
Proc. of EUROSPEECH&apos;93, pages 1291-1294,
September.
Makoto Nagao. 1984. A Framework of a Mechani-
cal Translation between Japanese and English by
Analogy Principle. In A. Elithorn and R. Banerji,
editors, Artificial and Human Intelligence, pages
173-180. North-Holland.
Satoshi Sato. 1993. MIMD Implementation of
MBT3. In Proc. of the Second International
Workshop on Parallel Processing for Artificial In-
telligence, pages 28-35. IJCAI&apos;93, August.
Eiichiro Sumita and Hitoshi Iida. 1992. Example-
Based Transfer of Japanese Adnominal Particles
into English. IEICE TRANS. INF. &amp; SYST.,
E75-D(4):585-594, April.
Eiichiro Sumita, Kozo 0i, Osamu Furuse, Hitoshi
Iida, Tetsuya Higuchi, Naoto Takahashi, and Hi-
roaki Kitano. 1993. Example-Based Machine
Translation on Massively Parallel Processors. In
Proc. of IJCAI&apos;93, pages 1283-1288, August.
Noriyoshi Uratani, Masami Suzuki, Masaaki Na-
gata, Tsuyoshi Morimoto, Yukinori Takubo,
Toshiyuki Sadanobu, and Hajime Narita. 1992. A
Function Evaluation Method for Analysis System
of Goal-Directed Dialogue. In IEICE Technical
Report, NLC92-10, July.
Alex Waibel, Ajay N. Jain, Arthur E. McNair, Hi-
roaki Saito, Alexander G. Hauptmann, and Joe
Tebelskis. 1991. JANUS: A Speech-to-speech
Translation Using Connectionist and Symbolic
Processing Strategies. In Proc. of ICASSP&apos;91,
pages 793-796, May.
</reference>
<page confidence="0.997323">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.673491">
<title confidence="0.998208">Real-Time Spoken Language Translation Using Associative Processors</title>
<author confidence="0.74302">Eiichiro Sumita i</author>
<author confidence="0.74302">Osamu Furuse</author>
<author confidence="0.74302">Hitoshi Iida Higuchit</author>
<affiliation confidence="0.995576">ATR Interpreting Telecommunications Research Laboratories</affiliation>
<address confidence="0.99744">2-2 Hikaridai, Seika, Souraku, Kyoto 619-02, JAPAN</address>
<email confidence="0.969714">foi,sumita,furuse,iidalOitl.atr.co.jp</email>
<affiliation confidence="0.999687">fElectrotechnical Laboratory</affiliation>
<address confidence="0.983896">1-1-4 Umezono, Tsukuba, Ibaraki 305, Japan</address>
<email confidence="0.965305">higuchifttl.go.jp</email>
<abstract confidence="0.999272391304348">This paper proposes a model using associative processors (APs) for real-time spoken language translation. Spoken language translation requires (1) an accurate translation and (2) a realtime response. We have already proposed a model, TDMT (Transfer-Driven Machine Translation), that translates a sentence utilizing examples effectively and performs accurate structural disambiguation and target word selection. This paper will concentrate on the second requirement. In TDMT, example-retrieval (ER), i.e., retrieving examples most similar to an input expression, is the most dominant part of the total processing time. Our study has concluded that we only need to implement the ER for expressions including a frequent word on APs. Experimental results show that the ER can be drastically speeded up. Moreover, a study on communications between APs demonstrates the scalability against vocabulary size by extrapolation. Thus, our model, TDMT on APs, meets the vital requirements of spoken language translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Terumasa Ehara</author>
<author>Kentaro Ogura</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>ATR Dialogue Database.</title>
<date>1990</date>
<booktitle>In Proc. of ICSLP&apos;90,</booktitle>
<pages>1093--1096</pages>
<contexts>
<context position="6628" citStr="Ehara et al., 1990" startWordPosition="1067" endWordPosition="1070">tion task, the average translation time per sentence is about 3.53 seconds in the TDMT prototype on a sequential machine (SPARCstation2). ER is embedded as a subroutine call and is called many times during the translation of one sentence. The average number of ER calls per sentence is about 9.5. Figure 1 shows rates for the ER time and other processing time. The longer the total processing time, the higher the rate for the ER time; the rate rises from about 43% to about 85%. The average rate is 71%. Thus, ER is the most dominant part of the total processing time. In the ATR dialogue database (Ehara et al., 1990), which contains about 13,000 sentences for a conference registration task, the average sentence length is about 14 words. We therefore assume in the remainder of this subsection and subsection 3.5 that the average sentence length of a Japanese spoken sentence is 14 words, and use statistics for 14-word sentences when calculating the times of a large-vocabulary TDMT system. The expected translation time of each 14-word sentence is about 5.95 seconds, which is much larger than the utterance time. The expected number of ER calls for each 14-word sentence is about 15. The expected time and rate f</context>
</contexts>
<marker>Ehara, Ogura, Morimoto, 1990</marker>
<rawString>Terumasa Ehara, Kentaro Ogura, and Tsuyoshi Morimoto. 1990. ATR Dialogue Database. In Proc. of ICSLP&apos;90, pages 1093-1096, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osamu Furuse</author>
<author>Hitoshi Iida</author>
</authors>
<title>Cooperation Between Transfer and Analysis in Example-Based Framework.</title>
<date>1992</date>
<booktitle>In Proc. of COLING&apos;92,</booktitle>
<pages>645--651</pages>
<contexts>
<context position="2280" citStr="Furuse and Iida, 1992" startWordPosition="325" endWordPosition="328">ses, i.e., speech recognition, spoken language translation and speech synthesis. Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language expressions that correspond to source language expressions that cover the sentence jointly. The model is called TransferDriven Machine Translation (TDMT) (Furuse and Iida, 1992; Furuse et al., 1994) (see subsection 2.1 for details). A prototype system of TDMT which translates a Japanese spoken sentence into English, has performed accurate structural disambiguation and target word selectionl. This paper will focus on the second requirement. First, we will outline TDMT and analyze its computational cost. Second, we will describe the configuration, experimental results and scalability of TDMT on associative processors (APs). Finally, we will touch on related works and conclude. 2 TDMT and its Cost Analysis 2.1 Outline of TDMT In TDMT, transfer knowledge is the primary </context>
</contexts>
<marker>Furuse, Iida, 1992</marker>
<rawString>Osamu Furuse and Hitoshi Iida. 1992. Cooperation Between Transfer and Analysis in Example-Based Framework. In Proc. of COLING&apos;92, pages 645-651, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osamu Furuse</author>
<author>Eiichiro Sumita</author>
<author>Hitoshi Iida</author>
</authors>
<title>Transfer Driven Machine Translation Utilizing Empirical Knowledge.</title>
<date>1994</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>35--3</pages>
<contexts>
<context position="2302" citStr="Furuse et al., 1994" startWordPosition="329" endWordPosition="332">nition, spoken language translation and speech synthesis. Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language expressions that correspond to source language expressions that cover the sentence jointly. The model is called TransferDriven Machine Translation (TDMT) (Furuse and Iida, 1992; Furuse et al., 1994) (see subsection 2.1 for details). A prototype system of TDMT which translates a Japanese spoken sentence into English, has performed accurate structural disambiguation and target word selectionl. This paper will focus on the second requirement. First, we will outline TDMT and analyze its computational cost. Second, we will describe the configuration, experimental results and scalability of TDMT on associative processors (APs). Finally, we will touch on related works and conclude. 2 TDMT and its Cost Analysis 2.1 Outline of TDMT In TDMT, transfer knowledge is the primary knowledge, which is de</context>
</contexts>
<marker>Furuse, Sumita, Iida, 1994</marker>
<rawString>Osamu Furuse, Eiichiro Sumita, and Hitoshi Iida. 1994. Transfer Driven Machine Translation Utilizing Empirical Knowledge. Transactions of Information Processing Society of Japan, 35(3):414-425, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Higuchi</author>
</authors>
<title>Tatsumi Furuya, Kenichi Handa, Naoto Takahashi, Hiroyasu Nishiyama, and Akio Kokubu. 1991a. IXM2 : A Parallel Associative Processor.</title>
<date></date>
<booktitle>In Proc. of the 18th International Symposium on Computer Architecture,</booktitle>
<marker>Higuchi, </marker>
<rawString>Tetsuya Higuchi, Tatsumi Furuya, Kenichi Handa, Naoto Takahashi, Hiroyasu Nishiyama, and Akio Kokubu. 1991a. IXM2 : A Parallel Associative Processor. In Proc. of the 18th International Symposium on Computer Architecture, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Higuchi</author>
</authors>
<title>Hiroaki Kitano, Tatsumi Furuya, Ken-ichi Handa, Naoto Takahashi, and Akio Kokubu. 1991b. IXM2 : A Parallel Associative Processor for Knowledge Processing.</title>
<date></date>
<booktitle>In Proc. of AAAI&apos;91,</booktitle>
<pages>296--303</pages>
<marker>Higuchi, </marker>
<rawString>Tetsuya Higuchi, Hiroaki Kitano, Tatsumi Furuya, Ken-ichi Handa, Naoto Takahashi, and Akio Kokubu. 1991b. IXM2 : A Parallel Associative Processor for Knowledge Processing. In Proc. of AAAI&apos;91, pages 296-303, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Kitano</author>
<author>Tetsuya Higuchi</author>
</authors>
<title>High Performance Memory-Based Translation on IXM2 Massively Parallel Associative Memory Processor.</title>
<date>1991</date>
<booktitle>In Proc. of AAAI&apos;91,</booktitle>
<pages>149--154</pages>
<contexts>
<context position="20973" citStr="Kitano and Higuchi, 1991" startWordPosition="3538" endWordPosition="3541">Result!Please proceed with the registration form FILeTiii.(NOIlseconds) 7 Mirage Time(Milliseconan) toko xoe xoo .0;00 5000 ROO 7000 mos 11103.1111111:22 &apos;Massively Para/lel Trans1it1an(jW2) Sentence klA.1 Input Sentence ;...t.11.-Ci4.1 L. Translation Resultigood -bye Run Thw(Ninrsecon.cts) Average Tine(Millisecondn) .6 o 3030 ■ON. Xoo mix moo aioo • • 0 • • • • • • • • lViVi \ I VI 0 : AP (Associative Processor) 6 layers Host machine • • 0 •• ! 105 4 Related works Up to now, some systems using a massively parallel machine in the field of natural language processing, such as a parsing system (Kitano and Higuchi, 1991b) and translation systems, e.g., DmSNAP (Kitano et al., 1991), ASTRAL (Kitano and Higuchi, 1991a), MBT3n (Sato, 1993), have been proposed. They have demonstrated good performance; nonetheless, they differ from our proposal. For the first three systems, their domain is much smaller than our domain and they do not perform structural disambiguation or target word selection based on the semantic distance between an input expression and each example. For the last system, it translates technical terms i.e. noun phrases, but not sentences. 5 Conclusion This paper has proposed TDMT (Transfer-Driven M</context>
</contexts>
<marker>Kitano, Higuchi, 1991</marker>
<rawString>Hiroaki Kitano and Tetsuya Higuchi. 1991a. High Performance Memory-Based Translation on IXM2 Massively Parallel Associative Memory Processor. In Proc. of AAAI&apos;91, pages 149-154, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Kitano</author>
<author>Tetsuya Higuchi</author>
</authors>
<title>Massively Parallel Memory-Based Parsing.</title>
<date>1991</date>
<booktitle>In Proc. of IJCAI&apos;91,</booktitle>
<pages>918--924</pages>
<contexts>
<context position="20973" citStr="Kitano and Higuchi, 1991" startWordPosition="3538" endWordPosition="3541">Result!Please proceed with the registration form FILeTiii.(NOIlseconds) 7 Mirage Time(Milliseconan) toko xoe xoo .0;00 5000 ROO 7000 mos 11103.1111111:22 &apos;Massively Para/lel Trans1it1an(jW2) Sentence klA.1 Input Sentence ;...t.11.-Ci4.1 L. Translation Resultigood -bye Run Thw(Ninrsecon.cts) Average Tine(Millisecondn) .6 o 3030 ■ON. Xoo mix moo aioo • • 0 • • • • • • • • lViVi \ I VI 0 : AP (Associative Processor) 6 layers Host machine • • 0 •• ! 105 4 Related works Up to now, some systems using a massively parallel machine in the field of natural language processing, such as a parsing system (Kitano and Higuchi, 1991b) and translation systems, e.g., DmSNAP (Kitano et al., 1991), ASTRAL (Kitano and Higuchi, 1991a), MBT3n (Sato, 1993), have been proposed. They have demonstrated good performance; nonetheless, they differ from our proposal. For the first three systems, their domain is much smaller than our domain and they do not perform structural disambiguation or target word selection based on the semantic distance between an input expression and each example. For the last system, it translates technical terms i.e. noun phrases, but not sentences. 5 Conclusion This paper has proposed TDMT (Transfer-Driven M</context>
</contexts>
<marker>Kitano, Higuchi, 1991</marker>
<rawString>Hiroaki Kitano and Tetsuya Higuchi. 1991b. Massively Parallel Memory-Based Parsing. In Proc. of IJCAI&apos;91, pages 918-924.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Kitano</author>
<author>Dan Moldovan</author>
<author>Seungho Cha</author>
</authors>
<date>1991</date>
<booktitle>High Performance Natural Language Processing on Semantic Network Array Processor. In Proc. of IJCAI&apos;91,</booktitle>
<pages>911--917</pages>
<contexts>
<context position="21035" citStr="Kitano et al., 1991" startWordPosition="3548" endWordPosition="3551">onds) 7 Mirage Time(Milliseconan) toko xoe xoo .0;00 5000 ROO 7000 mos 11103.1111111:22 &apos;Massively Para/lel Trans1it1an(jW2) Sentence klA.1 Input Sentence ;...t.11.-Ci4.1 L. Translation Resultigood -bye Run Thw(Ninrsecon.cts) Average Tine(Millisecondn) .6 o 3030 ■ON. Xoo mix moo aioo • • 0 • • • • • • • • lViVi \ I VI 0 : AP (Associative Processor) 6 layers Host machine • • 0 •• ! 105 4 Related works Up to now, some systems using a massively parallel machine in the field of natural language processing, such as a parsing system (Kitano and Higuchi, 1991b) and translation systems, e.g., DmSNAP (Kitano et al., 1991), ASTRAL (Kitano and Higuchi, 1991a), MBT3n (Sato, 1993), have been proposed. They have demonstrated good performance; nonetheless, they differ from our proposal. For the first three systems, their domain is much smaller than our domain and they do not perform structural disambiguation or target word selection based on the semantic distance between an input expression and each example. For the last system, it translates technical terms i.e. noun phrases, but not sentences. 5 Conclusion This paper has proposed TDMT (Transfer-Driven Machine Translation) on APs (Associative Processors) for real-t</context>
</contexts>
<marker>Kitano, Moldovan, Cha, 1991</marker>
<rawString>Hiroaki Kitano, Dan Moldovan, and Seungho Cha. 1991. High Performance Natural Language Processing on Semantic Network Array Processor. In Proc. of IJCAI&apos;91, pages 911-917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Kitano</author>
</authors>
<title>(I)DM-Dialog: An Experimental Speech-to-Speech Dialog Translation System.</title>
<date>1991</date>
<journal>IEEE Computer,</journal>
<pages>24--6</pages>
<contexts>
<context position="1584" citStr="Kitano, 1991" startWordPosition="223" endWordPosition="224">rt of the total processing time. Our study has concluded that we only need to implement the ER for expressions including a frequent word on APs. Experimental results show that the ER can be drastically speeded up. Moreover, a study on communications between APs demonstrates the scalability against vocabulary size by extrapolation. Thus, our model, TDMT on APs, meets the vital requirements of spoken language translation. 1 Introduction Research on speech translation that began in the mid-1980s has been challenging. Such research has resulted in several prototype systems (Morimoto et al., 1993; Kitano, 1991; Waibel et al., 1991). Speech translation consists of a sequence of processes, i.e., speech recognition, spoken language translation and speech synthesis. Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language expressions that correspond to source language expressions that cover the senten</context>
</contexts>
<marker>Kitano, 1991</marker>
<rawString>Hiroaki Kitano. 1991. (I)DM-Dialog: An Experimental Speech-to-Speech Dialog Translation System. IEEE Computer, 24(6):36-50, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>Toshiyuki Takezawa, Fumihiro Yato, Shigeki Sagayama, Toshihisa Tashiro, Masaaki Nagata, and Akira Kurematsu.</title>
<date>1993</date>
<booktitle>In Proc. of EUROSPEECH&apos;93,</booktitle>
<pages>1291--1294</pages>
<marker>Morimoto, 1993</marker>
<rawString>Tsuyoshi Morimoto, Toshiyuki Takezawa, Fumihiro Yato, Shigeki Sagayama, Toshihisa Tashiro, Masaaki Nagata, and Akira Kurematsu. 1993. ATR&apos;s Speech Translation System: ASURA. In Proc. of EUROSPEECH&apos;93, pages 1291-1294, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A Framework of a Mechanical Translation between Japanese and English by Analogy Principle.</title>
<date>1984</date>
<booktitle>Artificial and Human Intelligence,</booktitle>
<pages>173--180</pages>
<editor>In A. Elithorn and R. Banerji, editors,</editor>
<publisher>North-Holland.</publisher>
<contexts>
<context position="2953" citStr="Nagao, 1984" startWordPosition="433" endWordPosition="434">ototype system of TDMT which translates a Japanese spoken sentence into English, has performed accurate structural disambiguation and target word selectionl. This paper will focus on the second requirement. First, we will outline TDMT and analyze its computational cost. Second, we will describe the configuration, experimental results and scalability of TDMT on associative processors (APs). Finally, we will touch on related works and conclude. 2 TDMT and its Cost Analysis 2.1 Outline of TDMT In TDMT, transfer knowledge is the primary knowledge, which is described by an example-based framework (Nagao, 1984). A piece of transfer knowledge describes the correspondence between source language expressions (SEs) and target language expressions (TEs) as follows, to preserve the translational equivalence: SE =&gt; TEi (Eii, TEr, (En1, En2,...) Eij indicates the j-th example of TEL. For example, the transfer knowledge for source expression &amp;quot;X no Y&amp;quot; is described as follows2: X no Y =&gt; Y&apos; of X&apos; ((ronbun[paper],daimoku[titleD,- • Y&apos; for X&apos; ((hoteru[hotel],yoyaku[reservation]),• • Y&apos; in X&apos; ((Kyouto[Kyoto],kaigi[conference]),• • The translation success rate for 825 sentences used as learning data in a conferenc</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Makoto Nagao. 1984. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle. In A. Elithorn and R. Banerji, editors, Artificial and Human Intelligence, pages 173-180. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sato</author>
</authors>
<title>MIMD Implementation of MBT3.</title>
<date>1993</date>
<booktitle>In Proc. of the Second International Workshop on Parallel Processing for Artificial Intelligence,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="21091" citStr="Sato, 1993" startWordPosition="3558" endWordPosition="3559">0 mos 11103.1111111:22 &apos;Massively Para/lel Trans1it1an(jW2) Sentence klA.1 Input Sentence ;...t.11.-Ci4.1 L. Translation Resultigood -bye Run Thw(Ninrsecon.cts) Average Tine(Millisecondn) .6 o 3030 ■ON. Xoo mix moo aioo • • 0 • • • • • • • • lViVi \ I VI 0 : AP (Associative Processor) 6 layers Host machine • • 0 •• ! 105 4 Related works Up to now, some systems using a massively parallel machine in the field of natural language processing, such as a parsing system (Kitano and Higuchi, 1991b) and translation systems, e.g., DmSNAP (Kitano et al., 1991), ASTRAL (Kitano and Higuchi, 1991a), MBT3n (Sato, 1993), have been proposed. They have demonstrated good performance; nonetheless, they differ from our proposal. For the first three systems, their domain is much smaller than our domain and they do not perform structural disambiguation or target word selection based on the semantic distance between an input expression and each example. For the last system, it translates technical terms i.e. noun phrases, but not sentences. 5 Conclusion This paper has proposed TDMT (Transfer-Driven Machine Translation) on APs (Associative Processors) for real-time spoken language translation. In TDMT, a sentence is </context>
</contexts>
<marker>Sato, 1993</marker>
<rawString>Satoshi Sato. 1993. MIMD Implementation of MBT3. In Proc. of the Second International Workshop on Parallel Processing for Artificial Intelligence, pages 28-35. IJCAI&apos;93, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
<author>Hitoshi Iida</author>
</authors>
<date>1992</date>
<booktitle>ExampleBased Transfer of Japanese Adnominal Particles into English. IEICE TRANS. INF. &amp; SYST.,</booktitle>
<pages>75--4</pages>
<contexts>
<context position="4103" citStr="Sumita and Iida, 1992" startWordPosition="611" endWordPosition="614">tion success rate for 825 sentences used as learning data in a conference registration task, is about 98%. The translation success rate for 1,056 sentences, amassed through arbitary inputs in the same domain, is about 71%. The translation success rate increases as the number of examples increases. &apos;X and Y are variables for Japanese words and X&apos; and Y&apos; are the English translations of X and Y, respectively; &amp;quot;no&amp;quot; is an adnominal particle that corresponds to such English prepositions as &amp;quot;of,&amp;quot; &amp;quot;for,&amp;quot; &amp;quot;in,&amp;quot; and so on. 101 TDMT utilizes the semantic distance calculation proposed by Sumita and Iida (Sumita and Iida, 1992). Let us suppose that an input, I, and each example, Eij, consist of t words as follows: I = (II, • • • ,I) = (Eii1, • • • , Eiit) Then, the distance between I and Eij is calculated as follows: d(I , Eii) = d((I1, • • • , (Eiji., • • • , Etit)) = E duk, Eiik) x Wk k=1 The semantic distance d(ik, Ek) between words is reduced to the distance between concepts in a thesaurus (see subsection 3.2 for details). The weight Wk is the degree to which the word influences the selection of the translation3. The flow of selecting the most plausible TE is as follows: (1) The distance from the input is calcul</context>
</contexts>
<marker>Sumita, Iida, 1992</marker>
<rawString>Eiichiro Sumita and Hitoshi Iida. 1992. ExampleBased Transfer of Japanese Adnominal Particles into English. IEICE TRANS. INF. &amp; SYST., E75-D(4):585-594, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
</authors>
<title>Kozo 0i, Osamu Furuse, Hitoshi Iida, Tetsuya Higuchi, Naoto Takahashi, and Hiroaki Kitano.</title>
<date>1993</date>
<booktitle>Example-Based Machine Translation on Massively Parallel Processors. In Proc. of IJCAI&apos;93,</booktitle>
<pages>1283--1288</pages>
<marker>Sumita, 1993</marker>
<rawString>Eiichiro Sumita, Kozo 0i, Osamu Furuse, Hitoshi Iida, Tetsuya Higuchi, Naoto Takahashi, and Hiroaki Kitano. 1993. Example-Based Machine Translation on Massively Parallel Processors. In Proc. of IJCAI&apos;93, pages 1283-1288, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriyoshi Uratani</author>
<author>Masami Suzuki</author>
<author>Masaaki Nagata</author>
</authors>
<title>Tsuyoshi Morimoto, Yukinori Takubo, Toshiyuki Sadanobu, and Hajime Narita.</title>
<date>1992</date>
<booktitle>In IEICE</booktitle>
<tech>Technical Report, NLC92-10,</tech>
<contexts>
<context position="7957" citStr="Uratani et al., 1992" startWordPosition="1284" endWordPosition="1287">ther a large-vocabulary TDMT system can attain a real-time response. In the TDMT prototype, the vocabulary size and the number of examples, N, are about 1,500 and 12,500, respectively. N depends on the vocabulary size. The vocabulary size of the average commercially-available machine translation system is about 100,000. Thus, in the large-vocabulary sys&apos;We have 825 test sentences as described in footnote 1 in section 1. These sentences cover basic expressions that are used in Japanese ability tests conducted by the government and Japanese education courses used by many schools for foreigners (Uratani et al., 1992). The sentences were reviewed by Japanese linguists. In the experiments in this paper, we used 746 sentences excluding sentences translated by exact-match. dainihan second version, kaigi conference, no annaisyo particle, announcement, de happyou-sareru particle, be presented, ni particle, ronbun paper, no daimoku ga notte-orimasu particle, title, particle, be written Other processing time Exampleretrieval time 102 tern, N is about 830,000 12, 500 x 100, 000/1, 500) in direct proportion to the vocabulary size. For the sake of convenience, we assume N = 1, 000, 000. The ER time is nearly proport</context>
</contexts>
<marker>Uratani, Suzuki, Nagata, 1992</marker>
<rawString>Noriyoshi Uratani, Masami Suzuki, Masaaki Nagata, Tsuyoshi Morimoto, Yukinori Takubo, Toshiyuki Sadanobu, and Hajime Narita. 1992. A Function Evaluation Method for Analysis System of Goal-Directed Dialogue. In IEICE Technical Report, NLC92-10, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Waibel</author>
<author>Ajay N Jain</author>
<author>Arthur E McNair</author>
<author>Hiroaki Saito</author>
<author>Alexander G Hauptmann</author>
<author>Joe Tebelskis</author>
</authors>
<title>JANUS: A Speech-to-speech Translation Using Connectionist and Symbolic Processing Strategies.</title>
<date>1991</date>
<booktitle>In Proc. of ICASSP&apos;91,</booktitle>
<pages>793--796</pages>
<contexts>
<context position="1606" citStr="Waibel et al., 1991" startWordPosition="225" endWordPosition="228">l processing time. Our study has concluded that we only need to implement the ER for expressions including a frequent word on APs. Experimental results show that the ER can be drastically speeded up. Moreover, a study on communications between APs demonstrates the scalability against vocabulary size by extrapolation. Thus, our model, TDMT on APs, meets the vital requirements of spoken language translation. 1 Introduction Research on speech translation that began in the mid-1980s has been challenging. Such research has resulted in several prototype systems (Morimoto et al., 1993; Kitano, 1991; Waibel et al., 1991). Speech translation consists of a sequence of processes, i.e., speech recognition, spoken language translation and speech synthesis. Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language expressions that correspond to source language expressions that cover the sentence jointly. The model </context>
</contexts>
<marker>Waibel, Jain, McNair, Saito, Hauptmann, Tebelskis, 1991</marker>
<rawString>Alex Waibel, Ajay N. Jain, Arthur E. McNair, Hiroaki Saito, Alexander G. Hauptmann, and Joe Tebelskis. 1991. JANUS: A Speech-to-speech Translation Using Connectionist and Symbolic Processing Strategies. In Proc. of ICASSP&apos;91, pages 793-796, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>