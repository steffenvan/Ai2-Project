<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000844">
<title confidence="0.97791">
Quadratic Features and Deep Architectures for Chunking
</title>
<author confidence="0.946031">
Joseph Turian and James Bergstra and Yoshua Bengio
</author>
<affiliation confidence="0.844295">
Dept. IRO, Universit´e de Montr´eal
</affiliation>
<sectionHeader confidence="0.993987" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999646833333333">
We experiment with several chunking models.
Deeper architectures achieve better gener-
alization. Quadratic filters, a simplification
of a theoretical model of V1 complex cells,
reliably increase accuracy. In fact, logistic
regression with quadratic filters outperforms
a standard single hidden layer neural network.
Adding quadratic filters to logistic regression
is almost as effective as feature engineering.
Despite predicting each output label indepen-
dently, our model is competitive with ones
that use previous decisions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947594594594">
There are three general approaches to improving
chunking performance: engineer better features,
improve inference, and improve the model.
Manual feature engineering is a common direc-
tion. One technique is to take primitive features
and manually compound them. This technique is
common, and most NLP systems use n-gram based
features (Carreras and M`arquez, 2003; Ando and
Zhang, 2005, for example). Another approach is
linguistically motivated feature engineering, e.g.
Charniak and Johnson (2005).
Other works have looked in the direction of
improving inference. Rather than predicting each
decision independently, previous decisions can be
included in the inference process. In this work,
we use the simplest approach of modeling each
decision independently.
The third direction is by using a better model. If
modeling capacity can be added without introducing
too many extra degrees of freedom, generalization
could be improved. One approach for compactly
increasing capacity is to automatically induce
intermediate features through the composition of
non-linearities, for example SVMs with a non-linear
kernel (Kudo and Matsumoto, 2001), inducing
compound features in a CRF (McCallum, 2003),
neural networks (Henderson, 2004; Bengio and Le-
Cun, 2007), and boosting decision trees (Turian and
Melamed, 2006). Recently, Bergstra et al. (2009)
showed that capacity can be increased by adding
quadratic filters, leading to improved generalization
on vision tasks. This work examines how well
quadratic filters work for an NLP task. Compared to
manual feature engineering, improved models are
appealing because they are less task-specific.
We experiment on the task of chunking (Sang and
Buchholz, 2000), a syntactic sequence labeling task.
</bodyText>
<sectionHeader confidence="0.757753" genericHeader="method">
2 Sequence labeling
</sectionHeader>
<bodyText confidence="0.993427533333334">
Besides Collobert and Weston (2008), previous
work on sequence labeling usually use previous
decisions in predicting output labels. Here we do
not take advantage of the dependency between suc-
cessive output labels. Our approach predicts each
output label independently of the others. This allows
us to ignore inference during training: The model
maximizes the conditional likelihood of each output
label independent of the output label of other tokens.
We use a sliding window approach. The output
label for a particular focus token xi is predicted
based upon k¯ tokens before and after xi. The entire
window is of size k = 2 · k¯ + 1. Nearly all work on
sequence labeling uses a sliding window approach
(Kudo and Matsumoto, 2001; Zhang et al., 2002;
</bodyText>
<page confidence="0.981962">
245
</page>
<note confidence="0.6206015">
Proceedings of NAACL HLT 2009: Short Papers, pages 245–248,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.8354375">
Figure 1: Illustration of our baseline I-T-W-O model (see
Secs. 4 and 5.1). The input layer comprises seven tokens
with 204 dimensions each. Each token is passed through
a shared 150-dimensional token feature extractor. These
7 · 150 features are concatenated and 400 features are
extracted from them in the window layer. These 400 fea-
tures are the input to the final 23-class output prediction.
Feature extractors σq and ψh are described in Section 3.
</figureCaption>
<bodyText confidence="0.999556096774193">
Carreras and M`arquez, 2003; Ando and Zhang,
2005, for example). We assume that each token x
can be transformed into a real-valued feature vector
φ(x) with l entries. The feature function will be
described in Section 4.
A standard approach is as follows: We first
concatenate the features of k tokens into one vector
[φ(xi−¯k), ... , φ(xi+¯k)] of length k · l entries. We can
then pass [φ(xi−¯k), ... , φ(xi+¯k)] to a feature extractor
over the entire window followed by an output
log-linear layer.
Convolutional architectures can help when there
is a position-invariant aspect to the input. In machine
vision, parameters related to one part of the image
are sometimes restricted to be equal to parameters
related to another part (LeCun et al., 1998). A
convolutional approach to sequence labeling is as
follows: At the lowest layer we extract features from
individual tokens using a shared feature extractor.
These higher-level individual token features are then
concatenated, and are passed to a feature extractor
over the entire window.
In our baseline approach, we apply one convolu-
tional layer of feature extraction to each token (one
token layer), followed by a concatenation, followed
by one layer of feature extraction over the entire
window (one window layer), followed by a 23-D
output prediction using multiclass logistic regres-
sion. We abbreviate this architecture as I-T-W-O
(input-+token-+window-+output). See Figure 1 for
an illustration of this architecture.
</bodyText>
<sectionHeader confidence="0.979605" genericHeader="method">
3 Quadratic feature extractors
</sectionHeader>
<bodyText confidence="0.994852666666667">
The most common feature extractor in the literature
is a linear filter h followed by a non-linear squashing
(activation) function σ:
</bodyText>
<equation confidence="0.993133">
f(x) = σ(h(x)), h(x) = b + Wx. (1)
</equation>
<bodyText confidence="0.99991325">
In our experiments, we use the softsign squash-
ing function σ(z) = z/(1 + |z|). n-class lo-
gistic regression predicts ψ(h(x)), where softmax
ψi(z) = exp(zi)/ Ek exp(zk). Rust et al. (2005) argues
that complex cells in the V1 area of visual cortex
are not well explained by Eq. 1, but are instead
better explained by a model that includes quadratic
interactions between regions of the receptive field.
Bergstra et al. (2009) approximate the model of
Rust et al. (2005) with a simpler model of the
form given in Eq. 2.† In this model, the pre-squash
transformation q includes J quadratic filters:
</bodyText>
<equation confidence="0.868679666666667">
��� J
b + Wx + Z (Vjx)2
j=1
</equation>
<bodyText confidence="0.993576705882353">
(2)
where b, W, and V1 ... VJ are tunable parameters.
In the vision experiments of Bergstra et al.
(2009), using quadratic filters improved the gen-
eralization of the trained architecture. We were
interested to see if the increased capacity would
also be beneficial in language tasks. For our logistic
regression (I-O) experiments, the architecture is
specifically I–ψq-+O, i.e. output O is the softmax
ψ applied to the quadratic transform q of the input
I. Like Bergstra et al. (2009), in architectures with
hidden layers, we apply the quadratic transform q
in all layers except the final layer, which uses linear
transform h. For example, I-T-W-O is specifically
I–σq-*T–σq-+W–ψh-+O, as shown in Figure 1. Future
work will explore if generalization is improved by
using q in the final layer.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.9704055">
Here is a detailed description of the types of features
we use, with number of dimensions:
</bodyText>
<listItem confidence="0.798259">
• embeddings. We map each word to a real-valued
50-dimensional embedding. These embeddings
were obtained by Collobert and Weston (2008), and
† Bergstra et al. (2009) do not use a sqrt in Eq. 2. We found that
sqrt improves optimization and gives better generalization.
</listItem>
<figure confidence="0.985017055555556">
out
win
tok
in
400
ψh
23
204
150 150 150 150 150 150 150
aq
204
204
204
aq
204
204
204
f(x) = σ(q(x)), q(x) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝
</figure>
<page confidence="0.992304">
246
</page>
<bodyText confidence="0.991435">
were induced based upon a purely unsupervised
training strategy over the 631 million words in the
English Wikipedia.
</bodyText>
<listItem confidence="0.975455357142857">
• POS-tag. Part-of-speech tags were assigned auto-
matically, and are part of the CoNLL data. 45 dim.
• label frequencies. Frequency of each label
assigned to this word in the training and validation
data. From Ando and Zhang (2005). 23 dim.
• type(first character). The type of the first charac-
ter of the word. type(x) = ‘A’ if x is a capital letter,
‘a’ if x is a lowercase letter, ‘n’ if x is a digit, and x
otherwise. From Collins (2002). 20 dim.
• word length. The length of the word. 20 dim.
• compressed word type. We convert each char-
acter of the word into its type. We then remove any
repeated consecutive type. For example, “Label-
making” ⇒ “Aa-a”. From Collins (2002). 46 dim.
</listItem>
<bodyText confidence="0.690981666666667">
The last three feature types are based upon ortho-
graphic information. There is a combined total of
204 features per token.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99983425">
We follow the conditions in the CoNLL-2000
shared task (Sang and Buchholz, 2000). Of the 8936
training sentences, we used 1000 randomly sampled
sentences (23615 words) for validation.
</bodyText>
<subsectionHeader confidence="0.994085">
5.1 Training details
</subsectionHeader>
<bodyText confidence="0.999982769230769">
The optimization criterion used during training is
the maximization of the sum (over word positions)
of the per-token log-likelihood of the correct deci-
sion. Stochastic gradient descent is performed using
a fixed learning rate η and early stopping. Gradients
are estimated using a minibatch of 8 examples. We
found that a learning rate of 0.01, 0.0032, or 0.001
was most effective.
In all our experiments we use a window size
of 7 tokens. In preliminary experiments, smaller
windows yielded poorer results, and larger ones
were no better. Layer sizes of extracted features
were chosen to optimize validation F1.
</bodyText>
<subsectionHeader confidence="0.688355">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.994172">
We report chunk F-measure (F1). In some tables
we also report Acc, the per-token label accuracy,
post-Viterbi decoding.
</bodyText>
<figureCaption confidence="0.480456333333333">
Figure 2 shows that using quadratic filters reliably
improves generalization on all architectures. For
the I-T-W-O architecture, quadratic filters increase
</figureCaption>
<figure confidence="0.454492">
# of quadratic filters
</figure>
<figureCaption confidence="0.99266275">
Figure 2: Validation F1 (y-axis) as we vary the number
of quadratic filters (x-axis), over different model archi-
tectures. Both architecture depth and quadratic filters
improve validation F1.
</figureCaption>
<table confidence="0.8598842">
Architecture #qf Acc F1
I-O 16 96.45 93.94
I-W(400)-O 4 96.66 94.39
I-T(150)-W(566)-O 2 96.85 94.77
I-T(150)-W(310)-W(310)-O 4 96.87 94.82
</table>
<tableCaption confidence="0.995416">
Table 1: Architecture experiments on validation data.
</tableCaption>
<bodyText confidence="0.965489347826087">
The first column describes the layers in the architecture.
(The architecture in Figure 1 is I-T(150)-W(400)-O.)
The second column gives the number of quadratic filters.
For each architecture, the layer sizes and number of
quadratic filters are chosen to maximize validation F1.
Deeper architectures achieve higher F1 scores.
validation F1 by an absolute 0.31. Most surpris-
ingly, logistic regression with 16 filters achieves
F1=93.94, which outperforms the 93.83 of a stan-
dard (0 filter) single hidden layer neural network.
With embeddings as the only features, logreg
with 0 filters achieves F1=85.36. By adding all
features, we can raise the F1 to 91.96. Alternately,
by adding 16 filters, we can raise the F1 to 91.60. In
other words, adding filters is nearly as effective as
our manual feature engineering.
Table 1 shows the result of varying the overall
architecture. Deeper architectures achieve higher
F1 scores. Table 2 compares the model as we lesion
off different features. POS tags and the embeddings
were the most important features.
We applied our best model overall (I-T-W-W-O
in Table 1) to the test data. Results are shown in
</bodyText>
<figure confidence="0.9990831">
0 1 2 4 8 16
94.5%
93.5%
92.5%
91.5%
95%
94%
93%
92%
I-T-W-O (baseline)
I-W-O (1 hidden layer NN)
I-O (LogReg)
95%
94.5%
94%
93.5%
93%
92.5%
92%
91.5%
</figure>
<page confidence="0.837085">
247
</page>
<table confidence="0.684887571428571">
Acc F1
96.81 94.69
96.84 94.62
96.77 94.58
96.60 94.22
96.40 93.97
96.18 93.53
Feature set
default
no orthographic features
no label frequencies
no POS tags
no embeddings
only embeddings
</table>
<tableCaption confidence="0.994513">
Table 2: Results on validation of varying the feature set,
for the architecture in Figure 1 with 4 quadratic filters.
</tableCaption>
<table confidence="0.999914">
NP F1 Prc Rcl F1
AZ05 94.70 94.57 94.20 94.39
KM01 94.39 93.89 93.92 93.91
I-T-W-W-O 94.44 93.72 93.91 93.81
CM03 94.41 94.19 93.29 93.74
SP03 94.38 - - -
Mc03 93.96 - - -
AZ05- - 93.83 93.37 93.60
ZDJ02 93.89 93.54 93.60 93.57
</table>
<tableCaption confidence="0.996097">
Table 3: Test set results for Ando and Zhang (2005), Kudo
</tableCaption>
<figureCaption confidence="0.864136333333333">
and Matsumoto (2001), our I-T-W-W-O model, Carreras
and M`arquez (2003), Sha and Pereira (2003), McCallum
(2003), Zhang et al. (2002), and our best I-O model.
AZ05- is Ando and Zhang (2005) using purely supervised
training, not semi-supervised training. Scores are noun
phrase F1, and overall chunk precision, recall, and F1.
</figureCaption>
<bodyText confidence="0.946629142857143">
Table 3. We are unable to compare to Collobert and
Weston (2008) because they use a different training
and test set. Our model predicts all labels in the
sequence independently. All other works in Table 3
use previous decisions when making the current
label decision. Our approach is nonetheless compet-
itive with approaches that use this extra information.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999960611111111">
Many NLP approaches underfit important linguistic
phenomena. We experimented with new techniques
for increasing chunker model capacity: adding
depth (automatically inducing intermediate features
through the composition of non-linearities), and
including quadratic filters. Higher accuracy was
achieved by deeper architectures, i.e. ones with
more intermediate layers of automatically tuned fea-
ture extractors. Although they are a simplification of
a theoretical model of V1 complex cells, quadratic
filters reliably improved generalization in all archi-
tectures. Most surprisingly, logistic regression with
quadratic filters outperformed a single hidden layer
neural network without. Also, with logistic regres-
sion, adding quadratic filters was almost as effective
as manual feature engineering. Despite predicting
each output label independently, our model is
competitive with ones that use previous decisions.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.984499333333333">
Thank you to Ronan Collobert, L´eon Bottou, and
NEC Labs for access to their word embeddings, and
to NSERC and MITACS for financial support.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755">
R. Ando and T. Zhang. A high-performance semi-
supervised learning method for text chunking. In ACL,
2005.
Y. Bengio and Y. LeCun. Scaling learning algorithms
towards AI. In Large Scale Kernel Machines. 2007.
J. Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio.
Quadratic polynomials learn better image features. TR
1337, DIRO, Universit´e de Montr´eal, 2009.
X. Carreras and L. M`arquez. Phrase recognition by
filtering and ranking with perceptrons. In RANLP, 2003.
E. Charniak and M. Johnson. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In ACL, 2005.
M. Collins. Ranking algorithms for named entity extrac-
tion: Boosting and the voted perceptron. In ACL, 2002.
R. Collobert and J. Weston. A unified architecture for
natural language processing: Deep neural networks with
multitask learning. In ICML, 2008.
J. Henderson. Discriminative training of a neural
network statistical parser. In ACL, 2004.
T. Kudo and Y. Matsumoto. Chunking with support
vector machines. In NAACL, 2001.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient
based learning applied to document recognition. IEEE,
86(11):2278–2324, November 1998.
A. McCallum. Efficiently inducing features of condi-
tional random fields. In UAI, 2003.
N. Rust, O. Schwartz, J. A. Movshon, and E. Simoncelli.
Spatiotemporal elements of macaque V1 receptive fields.
Neuron, 46(6):945–956, 2005.
E. T. Sang and S. Buchholz. Introduction to the
CoNLL-2000 shared task: Chunking. In CoNLL, 2000.
F. Sha and F. C. N. Pereira. Shallow parsing with
conditional random fields. In HLT-NAACL, 2003.
J. Turian and I. D. Melamed. Advances in discriminative
parsing. In ACL, 2006.
T. Zhang, F. Damerau, and D. Johnson. Text chunking
based on a generalization of Winnow. JMLR, 2, 2002.
</reference>
<page confidence="0.996986">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.826893">
<title confidence="0.999978">Quadratic Features and Deep Architectures for Chunking</title>
<author confidence="0.972837">Turian Bergstra</author>
<affiliation confidence="0.842609">Dept. IRO, Universit´e de Montr´eal</affiliation>
<abstract confidence="0.999217923076923">We experiment with several chunking models. Deeper architectures achieve better generalization. Quadratic filters, a simplification of a theoretical model of V1 complex cells, reliably increase accuracy. In fact, logistic regression with quadratic filters outperforms a standard single hidden layer neural network. Adding quadratic filters to logistic regression almost as as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A high-performance semisupervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="1075" citStr="Ando and Zhang, 2005" startWordPosition="148" endWordPosition="151">l network. Adding quadratic filters to logistic regression is almost as effective as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions. 1 Introduction There are three general approaches to improving chunking performance: engineer better features, improve inference, and improve the model. Manual feature engineering is a common direction. One technique is to take primitive features and manually compound them. This technique is common, and most NLP systems use n-gram based features (Carreras and M`arquez, 2003; Ando and Zhang, 2005, for example). Another approach is linguistically motivated feature engineering, e.g. Charniak and Johnson (2005). Other works have looked in the direction of improving inference. Rather than predicting each decision independently, previous decisions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is t</context>
<context position="3846" citStr="Ando and Zhang, 2005" startWordPosition="574" endWordPosition="577">HLT 2009: Short Papers, pages 245–248, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Figure 1: Illustration of our baseline I-T-W-O model (see Secs. 4 and 5.1). The input layer comprises seven tokens with 204 dimensions each. Each token is passed through a shared 150-dimensional token feature extractor. These 7 · 150 features are concatenated and 400 features are extracted from them in the window layer. These 400 features are the input to the final 23-class output prediction. Feature extractors σq and ψh are described in Section 3. Carreras and M`arquez, 2003; Ando and Zhang, 2005, for example). We assume that each token x can be transformed into a real-valued feature vector φ(x) with l entries. The feature function will be described in Section 4. A standard approach is as follows: We first concatenate the features of k tokens into one vector [φ(xi−¯k), ... , φ(xi+¯k)] of length k · l entries. We can then pass [φ(xi−¯k), ... , φ(xi+¯k)] to a feature extractor over the entire window followed by an output log-linear layer. Convolutional architectures can help when there is a position-invariant aspect to the input. In machine vision, parameters related to one part of the </context>
<context position="7727" citStr="Ando and Zhang (2005)" startWordPosition="1222" endWordPosition="1225">nd Weston (2008), and † Bergstra et al. (2009) do not use a sqrt in Eq. 2. We found that sqrt improves optimization and gives better generalization. out win tok in 400 ψh 23 204 150 150 150 150 150 150 150 aq 204 204 204 aq 204 204 204 f(x) = σ(q(x)), q(x) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝ 246 were induced based upon a purely unsupervised training strategy over the 631 million words in the English Wikipedia. • POS-tag. Part-of-speech tags were assigned automatically, and are part of the CoNLL data. 45 dim. • label frequencies. Frequency of each label assigned to this word in the training and validation data. From Ando and Zhang (2005). 23 dim. • type(first character). The type of the first character of the word. type(x) = ‘A’ if x is a capital letter, ‘a’ if x is a lowercase letter, ‘n’ if x is a digit, and x otherwise. From Collins (2002). 20 dim. • word length. The length of the word. 20 dim. • compressed word type. We convert each character of the word into its type. We then remove any repeated consecutive type. For example, “Labelmaking” ⇒ “Aa-a”. From Collins (2002). 46 dim. The last three feature types are based upon orthographic information. There is a combined total of 204 features per token. 5 Experiments We follo</context>
<context position="11711" citStr="Ando and Zhang (2005)" startWordPosition="1881" endWordPosition="1884">5% 94% 93.5% 93% 92.5% 92% 91.5% 247 Acc F1 96.81 94.69 96.84 94.62 96.77 94.58 96.60 94.22 96.40 93.97 96.18 93.53 Feature set default no orthographic features no label frequencies no POS tags no embeddings only embeddings Table 2: Results on validation of varying the feature set, for the architecture in Figure 1 with 4 quadratic filters. NP F1 Prc Rcl F1 AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 - - - Mc03 93.96 - - - AZ05- - 93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3: Test set results for Ando and Zhang (2005), Kudo and Matsumoto (2001), our I-T-W-W-O model, Carreras and M`arquez (2003), Sha and Pereira (2003), McCallum (2003), Zhang et al. (2002), and our best I-O model. AZ05- is Ando and Zhang (2005) using purely supervised training, not semi-supervised training. Scores are noun phrase F1, and overall chunk precision, recall, and F1. Table 3. We are unable to compare to Collobert and Weston (2008) because they use a different training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision.</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Ando and T. Zhang. A high-performance semisupervised learning method for text chunking. In ACL, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>Y LeCun</author>
</authors>
<title>Scaling learning algorithms towards AI. In Large Scale Kernel Machines.</title>
<date>2007</date>
<contexts>
<context position="1945" citStr="Bengio and LeCun, 2007" startWordPosition="271" endWordPosition="275">ions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2 Sequence labeling Besides Collobert and Weston (2008), previous work on sequence labeling usually use previous d</context>
</contexts>
<marker>Bengio, LeCun, 2007</marker>
<rawString>Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel Machines. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bergstra</author>
<author>G Desjardins</author>
<author>P Lamblin</author>
<author>Y Bengio</author>
</authors>
<title>Quadratic polynomials learn better image features.</title>
<date>2009</date>
<tech>TR 1337,</tech>
<institution>DIRO, Universit´e de Montr´eal,</institution>
<contexts>
<context position="2035" citStr="Bergstra et al. (2009)" startWordPosition="285" endWordPosition="288">f modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2 Sequence labeling Besides Collobert and Weston (2008), previous work on sequence labeling usually use previous decisions in predicting output labels. Here we do not take advantage of the dependency betw</context>
<context position="5892" citStr="Bergstra et al. (2009)" startWordPosition="905" endWordPosition="908">. 3 Quadratic feature extractors The most common feature extractor in the literature is a linear filter h followed by a non-linear squashing (activation) function σ: f(x) = σ(h(x)), h(x) = b + Wx. (1) In our experiments, we use the softsign squashing function σ(z) = z/(1 + |z|). n-class logistic regression predicts ψ(h(x)), where softmax ψi(z) = exp(zi)/ Ek exp(zk). Rust et al. (2005) argues that complex cells in the V1 area of visual cortex are not well explained by Eq. 1, but are instead better explained by a model that includes quadratic interactions between regions of the receptive field. Bergstra et al. (2009) approximate the model of Rust et al. (2005) with a simpler model of the form given in Eq. 2.† In this model, the pre-squash transformation q includes J quadratic filters: ��� J b + Wx + Z (Vjx)2 j=1 (2) where b, W, and V1 ... VJ are tunable parameters. In the vision experiments of Bergstra et al. (2009), using quadratic filters improved the generalization of the trained architecture. We were interested to see if the increased capacity would also be beneficial in language tasks. For our logistic regression (I-O) experiments, the architecture is specifically I–ψq-+O, i.e. output O is the softma</context>
<context position="7152" citStr="Bergstra et al. (2009)" startWordPosition="1117" endWordPosition="1120"> q of the input I. Like Bergstra et al. (2009), in architectures with hidden layers, we apply the quadratic transform q in all layers except the final layer, which uses linear transform h. For example, I-T-W-O is specifically I–σq-*T–σq-+W–ψh-+O, as shown in Figure 1. Future work will explore if generalization is improved by using q in the final layer. 4 Features Here is a detailed description of the types of features we use, with number of dimensions: • embeddings. We map each word to a real-valued 50-dimensional embedding. These embeddings were obtained by Collobert and Weston (2008), and † Bergstra et al. (2009) do not use a sqrt in Eq. 2. We found that sqrt improves optimization and gives better generalization. out win tok in 400 ψh 23 204 150 150 150 150 150 150 150 aq 204 204 204 aq 204 204 204 f(x) = σ(q(x)), q(x) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝ 246 were induced based upon a purely unsupervised training strategy over the 631 million words in the English Wikipedia. • POS-tag. Part-of-speech tags were assigned automatically, and are part of the CoNLL data. 45 dim. • label frequencies. Frequency of each label assigned to this word in the training and validation data. From Ando and Zhang (2005). 23 dim. • type(first ch</context>
</contexts>
<marker>Bergstra, Desjardins, Lamblin, Bengio, 2009</marker>
<rawString>J. Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio. Quadratic polynomials learn better image features. TR 1337, DIRO, Universit´e de Montr´eal, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Phrase recognition by filtering and ranking with perceptrons.</title>
<date>2005</date>
<booktitle>In RANLP, 2003. E. Charniak</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. Phrase recognition by filtering and ranking with perceptrons. In RANLP, 2003. E. Charniak and M. Johnson. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In ACL, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="7936" citStr="Collins (2002)" startWordPosition="1267" endWordPosition="1268"> 204 aq 204 204 204 f(x) = σ(q(x)), q(x) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝ 246 were induced based upon a purely unsupervised training strategy over the 631 million words in the English Wikipedia. • POS-tag. Part-of-speech tags were assigned automatically, and are part of the CoNLL data. 45 dim. • label frequencies. Frequency of each label assigned to this word in the training and validation data. From Ando and Zhang (2005). 23 dim. • type(first character). The type of the first character of the word. type(x) = ‘A’ if x is a capital letter, ‘a’ if x is a lowercase letter, ‘n’ if x is a digit, and x otherwise. From Collins (2002). 20 dim. • word length. The length of the word. 20 dim. • compressed word type. We convert each character of the word into its type. We then remove any repeated consecutive type. For example, “Labelmaking” ⇒ “Aa-a”. From Collins (2002). 46 dim. The last three feature types are based upon orthographic information. There is a combined total of 204 features per token. 5 Experiments We follow the conditions in the CoNLL-2000 shared task (Sang and Buchholz, 2000). Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for validation. 5.1 Training details The optimiza</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. Ranking algorithms for named entity extraction: Boosting and the voted perceptron. In ACL, 2002. R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="1920" citStr="Henderson, 2004" startWordPosition="269" endWordPosition="270">y, previous decisions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2 Sequence labeling Besides Collobert and Weston (2008), previous work on sequence labeli</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>J. Henderson. Discriminative training of a neural network statistical parser. In ACL, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In NAACL,</booktitle>
<contexts>
<context position="1832" citStr="Kudo and Matsumoto, 2001" startWordPosition="255" endWordPosition="258"> looked in the direction of improving inference. Rather than predicting each decision independently, previous decisions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2</context>
<context position="3179" citStr="Kudo and Matsumoto, 2001" startWordPosition="468" endWordPosition="471">n predicting output labels. Here we do not take advantage of the dependency between successive output labels. Our approach predicts each output label independently of the others. This allows us to ignore inference during training: The model maximizes the conditional likelihood of each output label independent of the output label of other tokens. We use a sliding window approach. The output label for a particular focus token xi is predicted based upon k¯ tokens before and after xi. The entire window is of size k = 2 · k¯ + 1. Nearly all work on sequence labeling uses a sliding window approach (Kudo and Matsumoto, 2001; Zhang et al., 2002; 245 Proceedings of NAACL HLT 2009: Short Papers, pages 245–248, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Figure 1: Illustration of our baseline I-T-W-O model (see Secs. 4 and 5.1). The input layer comprises seven tokens with 204 dimensions each. Each token is passed through a shared 150-dimensional token feature extractor. These 7 · 150 features are concatenated and 400 features are extracted from them in the window layer. These 400 features are the input to the final 23-class output prediction. Feature extractors σq and ψh are descri</context>
<context position="11738" citStr="Kudo and Matsumoto (2001)" startWordPosition="1885" endWordPosition="1888">92% 91.5% 247 Acc F1 96.81 94.69 96.84 94.62 96.77 94.58 96.60 94.22 96.40 93.97 96.18 93.53 Feature set default no orthographic features no label frequencies no POS tags no embeddings only embeddings Table 2: Results on validation of varying the feature set, for the architecture in Figure 1 with 4 quadratic filters. NP F1 Prc Rcl F1 AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 - - - Mc03 93.96 - - - AZ05- - 93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3: Test set results for Ando and Zhang (2005), Kudo and Matsumoto (2001), our I-T-W-W-O model, Carreras and M`arquez (2003), Sha and Pereira (2003), McCallum (2003), Zhang et al. (2002), and our best I-O model. AZ05- is Ando and Zhang (2005) using purely supervised training, not semi-supervised training. Scores are noun phrase F1, and overall chunk precision, recall, and F1. Table 3. We are unable to compare to Collobert and Weston (2008) because they use a different training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision. Our approach is nonetheles</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. Chunking with support vector machines. In NAACL, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Bottou</author>
<author>Y Bengio</author>
<author>P Haffner</author>
</authors>
<title>Gradient based learning applied to document recognition.</title>
<date>1998</date>
<journal>IEEE,</journal>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="4547" citStr="LeCun et al., 1998" startWordPosition="692" endWordPosition="695">feature vector φ(x) with l entries. The feature function will be described in Section 4. A standard approach is as follows: We first concatenate the features of k tokens into one vector [φ(xi−¯k), ... , φ(xi+¯k)] of length k · l entries. We can then pass [φ(xi−¯k), ... , φ(xi+¯k)] to a feature extractor over the entire window followed by an output log-linear layer. Convolutional architectures can help when there is a position-invariant aspect to the input. In machine vision, parameters related to one part of the image are sometimes restricted to be equal to parameters related to another part (LeCun et al., 1998). A convolutional approach to sequence labeling is as follows: At the lowest layer we extract features from individual tokens using a shared feature extractor. These higher-level individual token features are then concatenated, and are passed to a feature extractor over the entire window. In our baseline approach, we apply one convolutional layer of feature extraction to each token (one token layer), followed by a concatenation, followed by one layer of feature extraction over the entire window (one window layer), followed by a 23-D output prediction using multiclass logistic regression. We ab</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. IEEE, 86(11):2278–2324, November 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In UAI,</booktitle>
<contexts>
<context position="1886" citStr="McCallum, 2003" startWordPosition="265" endWordPosition="266">dicting each decision independently, previous decisions can be included in the inference process. In this work, we use the simplest approach of modeling each decision independently. The third direction is by using a better model. If modeling capacity can be added without introducing too many extra degrees of freedom, generalization could be improved. One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2 Sequence labeling Besides Collobert and Weston (2008)</context>
<context position="11830" citStr="McCallum (2003)" startWordPosition="1900" endWordPosition="1901">et default no orthographic features no label frequencies no POS tags no embeddings only embeddings Table 2: Results on validation of varying the feature set, for the architecture in Figure 1 with 4 quadratic filters. NP F1 Prc Rcl F1 AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 - - - Mc03 93.96 - - - AZ05- - 93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3: Test set results for Ando and Zhang (2005), Kudo and Matsumoto (2001), our I-T-W-W-O model, Carreras and M`arquez (2003), Sha and Pereira (2003), McCallum (2003), Zhang et al. (2002), and our best I-O model. AZ05- is Ando and Zhang (2005) using purely supervised training, not semi-supervised training. Scores are noun phrase F1, and overall chunk precision, recall, and F1. Table 3. We are unable to compare to Collobert and Weston (2008) because they use a different training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision. Our approach is nonetheless competitive with approaches that use this extra information. 6 Conclusions Many NLP approa</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>A. McCallum. Efficiently inducing features of conditional random fields. In UAI, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rust</author>
<author>O Schwartz</author>
<author>J A Movshon</author>
<author>E Simoncelli</author>
</authors>
<title>Spatiotemporal elements of macaque V1 receptive fields.</title>
<date>2005</date>
<journal>Neuron,</journal>
<volume>46</volume>
<issue>6</issue>
<contexts>
<context position="5657" citStr="Rust et al. (2005)" startWordPosition="865" endWordPosition="868"> window (one window layer), followed by a 23-D output prediction using multiclass logistic regression. We abbreviate this architecture as I-T-W-O (input-+token-+window-+output). See Figure 1 for an illustration of this architecture. 3 Quadratic feature extractors The most common feature extractor in the literature is a linear filter h followed by a non-linear squashing (activation) function σ: f(x) = σ(h(x)), h(x) = b + Wx. (1) In our experiments, we use the softsign squashing function σ(z) = z/(1 + |z|). n-class logistic regression predicts ψ(h(x)), where softmax ψi(z) = exp(zi)/ Ek exp(zk). Rust et al. (2005) argues that complex cells in the V1 area of visual cortex are not well explained by Eq. 1, but are instead better explained by a model that includes quadratic interactions between regions of the receptive field. Bergstra et al. (2009) approximate the model of Rust et al. (2005) with a simpler model of the form given in Eq. 2.† In this model, the pre-squash transformation q includes J quadratic filters: ��� J b + Wx + Z (Vjx)2 j=1 (2) where b, W, and V1 ... VJ are tunable parameters. In the vision experiments of Bergstra et al. (2009), using quadratic filters improved the generalization of the</context>
</contexts>
<marker>Rust, Schwartz, Movshon, Simoncelli, 2005</marker>
<rawString>N. Rust, O. Schwartz, J. A. Movshon, and E. Simoncelli. Spatiotemporal elements of macaque V1 receptive fields. Neuron, 46(6):945–956, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking. In CoNLL,</title>
<date>2000</date>
<contexts>
<context position="2393" citStr="Sang and Buchholz, 2000" startWordPosition="339" endWordPosition="342">xample SVMs with a non-linear kernel (Kudo and Matsumoto, 2001), inducing compound features in a CRF (McCallum, 2003), neural networks (Henderson, 2004; Bengio and LeCun, 2007), and boosting decision trees (Turian and Melamed, 2006). Recently, Bergstra et al. (2009) showed that capacity can be increased by adding quadratic filters, leading to improved generalization on vision tasks. This work examines how well quadratic filters work for an NLP task. Compared to manual feature engineering, improved models are appealing because they are less task-specific. We experiment on the task of chunking (Sang and Buchholz, 2000), a syntactic sequence labeling task. 2 Sequence labeling Besides Collobert and Weston (2008), previous work on sequence labeling usually use previous decisions in predicting output labels. Here we do not take advantage of the dependency between successive output labels. Our approach predicts each output label independently of the others. This allows us to ignore inference during training: The model maximizes the conditional likelihood of each output label independent of the output label of other tokens. We use a sliding window approach. The output label for a particular focus token xi is pred</context>
<context position="8399" citStr="Sang and Buchholz, 2000" startWordPosition="1346" endWordPosition="1349">he first character of the word. type(x) = ‘A’ if x is a capital letter, ‘a’ if x is a lowercase letter, ‘n’ if x is a digit, and x otherwise. From Collins (2002). 20 dim. • word length. The length of the word. 20 dim. • compressed word type. We convert each character of the word into its type. We then remove any repeated consecutive type. For example, “Labelmaking” ⇒ “Aa-a”. From Collins (2002). 46 dim. The last three feature types are based upon orthographic information. There is a combined total of 204 features per token. 5 Experiments We follow the conditions in the CoNLL-2000 shared task (Sang and Buchholz, 2000). Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for validation. 5.1 Training details The optimization criterion used during training is the maximization of the sum (over word positions) of the per-token log-likelihood of the correct decision. Stochastic gradient descent is performed using a fixed learning rate η and early stopping. Gradients are estimated using a minibatch of 8 examples. We found that a learning rate of 0.01, 0.0032, or 0.001 was most effective. In all our experiments we use a window size of 7 tokens. In preliminary experiments, smaller </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. T. Sang and S. Buchholz. Introduction to the CoNLL-2000 shared task: Chunking. In CoNLL, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F C N Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<journal>J. Turian</journal>
<booktitle>In HLT-NAACL,</booktitle>
<contexts>
<context position="11813" citStr="Sha and Pereira (2003)" startWordPosition="1896" endWordPosition="1899">97 96.18 93.53 Feature set default no orthographic features no label frequencies no POS tags no embeddings only embeddings Table 2: Results on validation of varying the feature set, for the architecture in Figure 1 with 4 quadratic filters. NP F1 Prc Rcl F1 AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 - - - Mc03 93.96 - - - AZ05- - 93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3: Test set results for Ando and Zhang (2005), Kudo and Matsumoto (2001), our I-T-W-W-O model, Carreras and M`arquez (2003), Sha and Pereira (2003), McCallum (2003), Zhang et al. (2002), and our best I-O model. AZ05- is Ando and Zhang (2005) using purely supervised training, not semi-supervised training. Scores are noun phrase F1, and overall chunk precision, recall, and F1. Table 3. We are unable to compare to Collobert and Weston (2008) because they use a different training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision. Our approach is nonetheless competitive with approaches that use this extra information. 6 Conclusion</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. C. N. Pereira. Shallow parsing with conditional random fields. In HLT-NAACL, 2003. J. Turian and I. D. Melamed. Advances in discriminative parsing. In ACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>JMLR,</journal>
<volume>2</volume>
<contexts>
<context position="3199" citStr="Zhang et al., 2002" startWordPosition="472" endWordPosition="475">. Here we do not take advantage of the dependency between successive output labels. Our approach predicts each output label independently of the others. This allows us to ignore inference during training: The model maximizes the conditional likelihood of each output label independent of the output label of other tokens. We use a sliding window approach. The output label for a particular focus token xi is predicted based upon k¯ tokens before and after xi. The entire window is of size k = 2 · k¯ + 1. Nearly all work on sequence labeling uses a sliding window approach (Kudo and Matsumoto, 2001; Zhang et al., 2002; 245 Proceedings of NAACL HLT 2009: Short Papers, pages 245–248, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Figure 1: Illustration of our baseline I-T-W-O model (see Secs. 4 and 5.1). The input layer comprises seven tokens with 204 dimensions each. Each token is passed through a shared 150-dimensional token feature extractor. These 7 · 150 features are concatenated and 400 features are extracted from them in the window layer. These 400 features are the input to the final 23-class output prediction. Feature extractors σq and ψh are described in Section 3. Ca</context>
<context position="11851" citStr="Zhang et al. (2002)" startWordPosition="1902" endWordPosition="1905">hographic features no label frequencies no POS tags no embeddings only embeddings Table 2: Results on validation of varying the feature set, for the architecture in Figure 1 with 4 quadratic filters. NP F1 Prc Rcl F1 AZ05 94.70 94.57 94.20 94.39 KM01 94.39 93.89 93.92 93.91 I-T-W-W-O 94.44 93.72 93.91 93.81 CM03 94.41 94.19 93.29 93.74 SP03 94.38 - - - Mc03 93.96 - - - AZ05- - 93.83 93.37 93.60 ZDJ02 93.89 93.54 93.60 93.57 Table 3: Test set results for Ando and Zhang (2005), Kudo and Matsumoto (2001), our I-T-W-W-O model, Carreras and M`arquez (2003), Sha and Pereira (2003), McCallum (2003), Zhang et al. (2002), and our best I-O model. AZ05- is Ando and Zhang (2005) using purely supervised training, not semi-supervised training. Scores are noun phrase F1, and overall chunk precision, recall, and F1. Table 3. We are unable to compare to Collobert and Weston (2008) because they use a different training and test set. Our model predicts all labels in the sequence independently. All other works in Table 3 use previous decisions when making the current label decision. Our approach is nonetheless competitive with approaches that use this extra information. 6 Conclusions Many NLP approaches underfit importa</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. Johnson. Text chunking based on a generalization of Winnow. JMLR, 2, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>