<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002876">
<title confidence="0.996711">
Heuristic search in a cognitive model of human parsing
</title>
<author confidence="0.994663">
John T. Hale
</author>
<affiliation confidence="0.984832">
Cornell University
</affiliation>
<address confidence="0.816442">
217 Morrill Hall
Ithaca, New York 14853
</address>
<email confidence="0.999366">
jthale@cornell.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851857142857">
We present a cognitive process model
of human sentence comprehension based
on generalized left-corner parsing. A
search heuristic based upon previously-
parsed corpora derives garden path effects,
garden path paradoxes, and the local co-
herence effect.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913321428571">
One of the most interesting applications of pars-
ing technology has, for some researchers, been
psycholinguistic models (Kay, 2005). Algorith-
mic models of language use have led in the past
to a variety of cognitive insights (Kaplan, 1972;
Marcus, 1980; Thibadeau et al., 1982; Pereira,
1985; Pulman, 1986; Johnson, 1989; Stabler,
1994). However they are challenged by a veritable
tidal wave of new data collected during the 1990s
and 2000s. Work during this later period reveals
phenomena, such as the local coherence effect dis-
cussed in section 5, that have yet to be truly inte-
grated into any particular theoretical framework.
This short paper presents a parsing system in-
tended to serve as a model of the syntactic part
of human sentence comprehension. Such a model
helps make sense of sentence-difficulty data from
self-paced reading, eye-tracking and other behav-
ioral studies. It also sketches a relationship be-
tween calculations carried out in the course of
automated syntactic analysis and the inferences
about linguistic structure taking place in our minds
during ordinary sentence-understanding.
Section 2 defines the model itself, highlight-
ing its relationship to generalized left-corner pars-
ing. Sections 3–5 apply this model to three contro-
versial phenomena that are well-established in the
psycholinguistics literature. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.49326" genericHeader="method">
2 Architecture of the model
</sectionHeader>
<subsectionHeader confidence="0.997569">
2.1 Problem states and Operators
</subsectionHeader>
<bodyText confidence="0.999888785714286">
We model the human sentence comprehen-
sion mechanism as search within a prob-
lem space (Newell and Simon, 1972). We as-
sume that all (incremental) parser states have
a (partial) grammatical interpretation (Chomsky,
1965, 9). In this paper, the grammatical inter-
pretation employs context-free grammar. An in-
ventory of operators carries the model from one
point in the problem space to another. In the in-
terest of simplicity, we place no bound on the
number of problem states the model can explore.
However, we do acknowledge with Johnson-Laird
(1983) and Resnik (1992) a pressure to minimize
memory consumption internal to a problem state.
The model’s within-problem state memory usage
should reflect human acceptability judgments with
embedded sentences. These considerations moti-
vate a generalized left-corner (GLC) parsing strat-
egy (Demers, 1977) whose stack consumption is
maximal on just the center-embedded examples
that are so difficult for people to understand. To
reflect the argument/adjunct distinction (Tutun-
jian and Boland, 2008) we adopt a mixed strat-
egy that is bottom-up for optional postmodifiers
but left-corner everywhere else. Leaving the arc-
eager/arc-standard decision (Abney and Johnson,
1991) to the control policy allows four possible
operators, schematized in Table 1.
</bodyText>
<subsectionHeader confidence="0.969765">
2.2 Informed Search
</subsectionHeader>
<bodyText confidence="0.999974375">
Informed search differs from uninformed search
procedures such as depth-first and breadth-first
by making use of heuristic knowledge about the
search domain. The strategy is to choose for ex-
pansion the node whose cost is lowest (Barr and
Feigenbaum, 1981, 61). In A∗ search (Hart et al.,
1968) this cost is divided up into a sum consisting
of the known cost to reach a search node and an
</bodyText>
<page confidence="0.935194">
230
</page>
<note confidence="0.3293765">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 230–233,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.973916384615385">
project a rule LHS → Trigger
Rest
↑
announce
point
shift a word W
project and match the sought parent LHS using
scan the sought word W
the rule LHS → Trigger
Rest
↑
announce
point
</figure>
<tableCaption confidence="0.999093">
Table 1: Four schema define the operators
</tableCaption>
<table confidence="0.999813875">
stack n E[steps] standard error
[VP] S [TOP] 55790 44.936 0.1572
S [TOP] 53991 10.542 0.0986
[NP] S [TOP] 43635 33.092 0.1633
NP [TOP] 38844 55.791 0.2126
NP [S] S [TOP] 34415 47.132 0.2122
[S] S [TOP] 33578 52.800 0.2195
[PP] S [TOP] 30693 34.454 0.1915
IN [PP] S [TOP] 27272 32.379 0.2031
DT [NP] S [TOP] 22375 34.478 0.2306
[AUX] [VP] S [TOP] 16447 46.536 0.2863
VBD [VP] S [TOP] 16224 43.057 0.2826
VB [VP] S [TOP] 13548 40.404 0.3074
the [NP] S [TOP] 12507 34.120 0.3046
NP [NP] S [TOP] 12092 43.821 0.3269
DT [TOP] 10440 66.452 0.3907
</table>
<tableCaption confidence="0.989034333333333">
Table 2: Popular left-corner parser states. Stacks
grow to the left. The categories are as described in
Table 3 of Marcus et al. (1993).
</tableCaption>
<bodyText confidence="0.999900772727273">
estimate of the costs involved in finishing search
from that node. In this work, rather than relying
on the guarantee provided by the A* theorem, we
examine the exploration pattern that results from
an inadmissable completion cost estimator. The
choice of estimator is Hypothesis 1.
Hypothesis 1 Search in parsing is informed by an
estimate of the expected number of steps to com-
pletion, given previous experience.
Table 2 writes out the expected number of
steps to completion (E[steps]) for a selection of
problem states binned together according to their
grammatical interpretation. Categories enclosed
in square brackets are predicted top-down whereas
unbracketed have been found bottom-up. These
states are some of the most popular states vis-
ited during a simulation of parsing the Brown cor-
pus (Kuˇcera and Francis, 1967; Marcus et al.,
1993) according to the mixed strategy introduced
above in subsection 2.1. The quantity E[steps]
serves in what follows as the completion cost esti-
mate in A* search.
</bodyText>
<sectionHeader confidence="0.965367" genericHeader="method">
3 Garden pathing
</sectionHeader>
<bodyText confidence="0.98726825">
Any model of human sentence comprehension
should address the garden path effect. The con-
trast between 1a and 1b is an example of this phe-
nomenon.
</bodyText>
<listItem confidence="0.9895635">
(1) a. while Mary was mending a sock fell on the floor
b. while Mary was mending, a sock fell on the floor
</listItem>
<bodyText confidence="0.998320695652174">
The control condition 1b includes a comma which,
in spoken language, would be expressed as a
prosodic break (Carroll and Slowiaczek, 1991;
Speer et al., 1996).
Figure 1 shows the search space explored in
the experimental condition 1a. In this picture,
ovals represent problem states. The number in-
side the oval encodes the vistation order. Arcs be-
tween ovals represent operator applications. The
path (14, 22, 23, 24, 25, 29, 27) is the garden path
which builds a grammatical interpretation where a
sock is attached as a direct object of the verb mend.
The grey line highlights the order in which A*
search considers this path. At state 21 after shift-
ing sock, experience with the Brown corpus sug-
gests reconsidering the garden path.
Whereas the model examines 45 search nodes
during the analysis of the temporarily ambiguous
item 1a, it dispatches the unambiguous item 1b af-
ter only 40 nodes despite that sentence having an
additional token (the comma). Garden paths, on
this view, are sequences of parser states explored
only in a temporarily ambiguous item.
</bodyText>
<sectionHeader confidence="0.933333" genericHeader="method">
4 Garden pathing counterexamples
</sectionHeader>
<bodyText confidence="0.992325">
Purely structural attachment preferences like
Right Association (Kimball, 1973) and Mini-
mal Attachment (Frazier and Fodor, 1978; Pereira,
1985) are threatened by paradoxical counterexam-
ples such as 2 from Gibson (1991, 22) where no
fixed principle yields correct predictions across
both examples.
</bodyText>
<listItem confidence="0.9293915">
(2) a. I gave her earrings on her birthday .
b. I gave her earrings to another girl .
</listItem>
<bodyText confidence="0.994810333333333">
A parser guided by Hypothesis 1 interleaves the
garden path attachment and the globally-correct
attachment in both cases, resulting in a search that
</bodyText>
<page confidence="0.987474">
231
</page>
<bodyText confidence="0.999792875">
is strictly committed to neither analysis. In 2a,
32% of discovered states represent the globally-
incorrect attachment of her. In 2b, 27% of states
represent the globally-incorrect attachment of her
to give as a one-word direct object. The para-
dox for purely structural attachment heuristics is
dissolved by the observation that neither pathway
fully achieves priority over the other.
</bodyText>
<sectionHeader confidence="0.990562" genericHeader="method">
5 Local Coherence
</sectionHeader>
<bodyText confidence="0.937497394736842">
Tabor et al. (2004) discovered1 a processing dif-
ficulty phenomenon called “local coherence.”
Among the stimuli they considered, the locally-
coherent condition is 3a where the substring the
player tossed a frisbee could be analyzed as a sen-
tence, if considered in isolation.
(3) a. The coach smiled at the player tossed a frisbee by
the opposing team.
b. The coach smiled at the player thrown a frisbee
by the opposing team
c. The coach smiled at the player who was tossed a
frisbee by the opposing team.
d. The coach smiled at the player who was thrown a
frisbee by the opposing team.
Tabor and colleagues observe an interaction be-
tween the degree of morphological ambiguity of
the embedded verb (tossed or thrown) and the
presence or absence of the relative-clause initial
words who was. These two factors are known as
±ambiguity and ±reduction, respectively. If the
human parser were making full use of the gram-
mar, its operation would reflect the impossibility
of continuing the coach smiled at... with a sen-
tence. The ungrammaticality of a sentence in this
left context would preclude any analysis of the
player as a subject of active voice toss. But greater
reading times observed on the ambiguous tossed
as compared to the unambiguous thrown suggest
contrariwise that this grammatical deduction is not
made uniformly based on the left context.
Table 3 shows how an informed parser’s step
counts, when guided by Hypothesis 1, derive
Tabor et al.’s observed pattern. The cell pre-
dicted to be hardest is the local coherence,
shaded gray. The degree of worsening due to rel-
ative clause reduction is greater in +ambiguous
than in −ambiguous. This derives the observed
interaction.
</bodyText>
<figureCaption confidence="0.997619">
Figure 1: Heavy line is the globally-correct path 1Konieczny and M¨uller (2006) documents a closely re-
lated form of local coherence in German.
</figureCaption>
<figure confidence="0.92561643902439">
0
1
2
3
4
5
6
8
7
9
10
11
12
13
15
14
16
17
22
18
23
19
24
20
25
21
26
29
27
28
31
30
32
33
35
34
36
37
38
39
40
</figure>
<page confidence="0.976162333333333">
41
42
43
45
44
232
</page>
<table confidence="0.952447">
+ambiguous −ambiguous
+reduced 119 84
−reduced 67 53
</table>
<tableCaption confidence="0.996095">
Table 3: Count of states examined
</tableCaption>
<reference confidence="0.952999833333333">
Mark Johnson. 1989. Parsing as deduction: the use of knowl-
edge of language. Journal of Psycholinguistic Research,
18(1):105–128.
Ronald M. Kaplan. 1972. Augmented transition networks as
psychological models of sentence comprehension. Artifi-
cial Intelligence, 3:77–100.
</reference>
<sectionHeader confidence="0.99729" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999968833333333">
When informed by experience with the Brown
corpus, the parsing system described in this pa-
per exhibits a pattern of “time-sharing” perfor-
mance that corresponds to human behavioral diffi-
culty in three controversial cases. The built-in el-
ements — context-free grammar, generalized left-
corner parsing and the A∗-type cost function —
are together adequate to address a range of com-
prehension difficulty phenomena without impos-
ing an a priori memory limit. The contribution is
an algorithmic-level account of the cognitive pro-
cesses involved in perceiving syntactic structure.
</bodyText>
<sectionHeader confidence="0.997422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998678397435897">
Steven Abney and Mark Johnson. 1991. Memory require-
ments and local ambiguities of parsing strategies. Journal
of Psycholinguistic Research, 20(3):233–249.
Avron Barr and Edward A. Feigenbaum, editors. 1981. The
Handbook of Artificial Intelligence. William Kaufmann.
Patrick J. Carroll and Maria L. Slowiaczek. 1991. Modes and
modules: multiple pathways to the language processor.
In Jay L. Garfield, editor, Modularity in Knowledge Rep-
resentation and Natural Language Understanding, pages
221–247. MIT Press.
Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press.
Alan J. Demers. 1977. Generalized left corner parsing. In
Conference Report of the 4th annual association for com-
puting machinery symposium on Principles of Program-
ming Languages, pages 170–181.
Lyn Frazier and Janet Dean Fodor. 1978. The sausage ma-
chine: anew two-stage parsing model. Cognition, 6:291–
325.
Edward Gibson. 1991. A Computational Theory of Human
Linguistic Processing: Memory Limitations and Process-
ing Breakdown. Ph.D. thesis, Carnegie Mellon University.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968. A
formal basis for the heuristic determination of minimum
cost paths. IEEE Transactions of systems science and cy-
bernetics, ssc-4(2):100–107.
Philip N. Johnson-Laird. 1983. Mental Models. Cambridge
University Press.
Martin Kay. 2005. A life of language. Computational Lin-
guistics, 31(4):425–438.
John P. Kimball. 1973. Seven principles of surface structure
parsing in natural language. Cognition, 2:15–48.
Lars Konieczny and Daniel M¨uller. 2006. Local coherences
in sentence processing. CUNY Conference on Human
Sentence Processing.
Henry Kuˇcera and W. Nelson Francis. 1967. Computational
Analysis of Present-day American English. Brown Uni-
versity Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19.
Mitchell P. Marcus. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Allen Newell and Herbert A. Simon. 1972. Human Problem
Solving. Prentice-Hall, Englewood Cliffs, New Jersey.
Fernando Pereira. 1985. A new characterization of attach-
ment preference. In David Dowty, Lauri Karttunen, and
Arnold Zwicky, editors, Natural Language Parsing: Psy-
chological, Computational and Theoretical Perspectives,
ACL Studies in Natural Language Processing, pages 307–
319. Cambridge University Press.
Steven G. Pulman. 1986. Grammars, parsers, and mem-
ory limitations. Language and Cognitive Processes,
1(3):197–2256.
Philip Resnik. 1992. Left-corner parsing and psychologi-
cal plausibility. In Proceedings of the Fourteenth Interna-
tional Conference on Computational Linguistics, Nantes,
France.
Shari R. Speer, Margaret M. Kjelgaard, and Kathryn M. Do-
broth. 1996. The influence of prosodic structure on
the resolution of temporary syntactic closure ambiguities.
Journal of Psycholinguistic Research, 25(2):249–271.
Edward Stabler. 1994. The finite connectivity of lin-
guistic structure. In Charles Clifton, Lyn Frazier, and
Keith Rayner, editors, Perspectives on Sentence Process-
ing, pages 303–336. Lawrence Erlbaum.
Whitney Tabor, Bruno Galantuccia, and Daniel Richardson.
2004. Effects of merely local syntactic coherence on
sentence processing. Journal of Memory and Language,
50(4):355–370.
Robert Thibadeau, Marcel A. Just, and Patricia Carpenter.
1982. A model of the time course and content of read-
ing. Cognitive Science, 6:157–203.
D. Tutunjian and J.E. Boland. 2008. Do We Need a Distinc-
tion between Arguments and Adjuncts? Evidence from
Psycholinguistic Studies of Comprehension. Language
and Linguistics Compass, 2(4):631–646.
</reference>
<page confidence="0.998951">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825204">
<title confidence="0.993313">Heuristic search in a cognitive model of human parsing</title>
<author confidence="0.997631">T John</author>
<affiliation confidence="0.942041">Cornell</affiliation>
<address confidence="0.9829365">217 Morrill Ithaca, New York</address>
<email confidence="0.999914">jthale@cornell.edu</email>
<abstract confidence="0.988245625">We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing as deduction: the use of knowledge of language.</title>
<date>1989</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="738" citStr="Johnson, 1989" startWordPosition="109" endWordPosition="110">3 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking and other behavioral studies. It also sketches a relationship betwe</context>
</contexts>
<marker>Johnson, 1989</marker>
<rawString>Mark Johnson. 1989. Parsing as deduction: the use of knowledge of language. Journal of Psycholinguistic Research, 18(1):105–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>Augmented transition networks as psychological models of sentence comprehension.</title>
<date>1972</date>
<journal>Artificial Intelligence,</journal>
<pages>3--77</pages>
<contexts>
<context position="656" citStr="Kaplan, 1972" startWordPosition="97" endWordPosition="98">an parsing John T. Hale Cornell University 217 Morrill Hall Ithaca, New York 14853 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading</context>
</contexts>
<marker>Kaplan, 1972</marker>
<rawString>Ronald M. Kaplan. 1972. Augmented transition networks as psychological models of sentence comprehension. Artificial Intelligence, 3:77–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="3045" citStr="Abney and Johnson, 1991" startWordPosition="457" endWordPosition="460"> memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) parsing strategy (Demers, 1977) whose stack consumption is maximal on just the center-embedded examples that are so difficult for people to understand. To reflect the argument/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Leaving the arceager/arc-standard decision (Abney and Johnson, 1991) to the control policy allows four possible operators, schematized in Table 1. 2.2 Informed Search Informed search differs from uninformed search procedures such as depth-first and breadth-first by making use of heuristic knowledge about the search domain. The strategy is to choose for expansion the node whose cost is lowest (Barr and Feigenbaum, 1981, 61). In A∗ search (Hart et al., 1968) this cost is divided up into a sum consisting of the known cost to reach a search node and an 230 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 230–233, Paris, Octobe</context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. Journal of Psycholinguistic Research, 20(3):233–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avron Barr</author>
<author>Edward A Feigenbaum</author>
<author>editors</author>
</authors>
<date>1981</date>
<booktitle>The Handbook of Artificial Intelligence.</booktitle>
<publisher>William Kaufmann.</publisher>
<marker>Barr, Feigenbaum, editors, 1981</marker>
<rawString>Avron Barr and Edward A. Feigenbaum, editors. 1981. The Handbook of Artificial Intelligence. William Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick J Carroll</author>
<author>Maria L Slowiaczek</author>
</authors>
<title>Modes and modules: multiple pathways to the language processor. In</title>
<date>1991</date>
<booktitle>Modularity in Knowledge Representation and Natural Language Understanding,</booktitle>
<pages>221--247</pages>
<editor>Jay L. Garfield, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6016" citStr="Carroll and Slowiaczek, 1991" startWordPosition="962" endWordPosition="965">Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1993) according to the mixed strategy introduced above in subsection 2.1. The quantity E[steps] serves in what follows as the completion cost estimate in A* search. 3 Garden pathing Any model of human sentence comprehension should address the garden path effect. The contrast between 1a and 1b is an example of this phenomenon. (1) a. while Mary was mending a sock fell on the floor b. while Mary was mending, a sock fell on the floor The control condition 1b includes a comma which, in spoken language, would be expressed as a prosodic break (Carroll and Slowiaczek, 1991; Speer et al., 1996). Figure 1 shows the search space explored in the experimental condition 1a. In this picture, ovals represent problem states. The number inside the oval encodes the vistation order. Arcs between ovals represent operator applications. The path (14, 22, 23, 24, 25, 29, 27) is the garden path which builds a grammatical interpretation where a sock is attached as a direct object of the verb mend. The grey line highlights the order in which A* search considers this path. At state 21 after shifting sock, experience with the Brown corpus suggests reconsidering the garden path. Whe</context>
</contexts>
<marker>Carroll, Slowiaczek, 1991</marker>
<rawString>Patrick J. Carroll and Maria L. Slowiaczek. 1991. Modes and modules: multiple pathways to the language processor. In Jay L. Garfield, editor, Modularity in Knowledge Representation and Natural Language Understanding, pages 221–247. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2050" citStr="Chomsky, 1965" startWordPosition="310" endWordPosition="311">linguistic structure taking place in our minds during ordinary sentence-understanding. Section 2 defines the model itself, highlighting its relationship to generalized left-corner parsing. Sections 3–5 apply this model to three controversial phenomena that are well-established in the psycholinguistics literature. Section 6 concludes. 2 Architecture of the model 2.1 Problem states and Operators We model the human sentence comprehension mechanism as search within a problem space (Newell and Simon, 1972). We assume that all (incremental) parser states have a (partial) grammatical interpretation (Chomsky, 1965, 9). In this paper, the grammatical interpretation employs context-free grammar. An inventory of operators carries the model from one point in the problem space to another. In the interest of simplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) pa</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan J Demers</author>
</authors>
<title>Generalized left corner parsing.</title>
<date>1977</date>
<booktitle>In Conference Report of the 4th annual association for computing machinery symposium on Principles of Programming Languages,</booktitle>
<pages>170--181</pages>
<contexts>
<context position="2679" citStr="Demers, 1977" startWordPosition="406" endWordPosition="407">per, the grammatical interpretation employs context-free grammar. An inventory of operators carries the model from one point in the problem space to another. In the interest of simplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) parsing strategy (Demers, 1977) whose stack consumption is maximal on just the center-embedded examples that are so difficult for people to understand. To reflect the argument/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Leaving the arceager/arc-standard decision (Abney and Johnson, 1991) to the control policy allows four possible operators, schematized in Table 1. 2.2 Informed Search Informed search differs from uninformed search procedures such as depth-first and breadth-first by making use of heuristic knowledge ab</context>
</contexts>
<marker>Demers, 1977</marker>
<rawString>Alan J. Demers. 1977. Generalized left corner parsing. In Conference Report of the 4th annual association for computing machinery symposium on Principles of Programming Languages, pages 170–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Janet Dean Fodor</author>
</authors>
<title>The sausage machine: anew two-stage parsing model.</title>
<date>1978</date>
<journal>Cognition,</journal>
<volume>6</volume>
<pages>325</pages>
<contexts>
<context position="7100" citStr="Frazier and Fodor, 1978" startWordPosition="1138" endWordPosition="1141">* search considers this path. At state 21 after shifting sock, experience with the Brown corpus suggests reconsidering the garden path. Whereas the model examines 45 search nodes during the analysis of the temporarily ambiguous item 1a, it dispatches the unambiguous item 1b after only 40 nodes despite that sentence having an additional token (the comma). Garden paths, on this view, are sequences of parser states explored only in a temporarily ambiguous item. 4 Garden pathing counterexamples Purely structural attachment preferences like Right Association (Kimball, 1973) and Minimal Attachment (Frazier and Fodor, 1978; Pereira, 1985) are threatened by paradoxical counterexamples such as 2 from Gibson (1991, 22) where no fixed principle yields correct predictions across both examples. (2) a. I gave her earrings on her birthday . b. I gave her earrings to another girl . A parser guided by Hypothesis 1 interleaves the garden path attachment and the globally-correct attachment in both cases, resulting in a search that 231 is strictly committed to neither analysis. In 2a, 32% of discovered states represent the globallyincorrect attachment of her. In 2b, 27% of states represent the globally-incorrect attachment </context>
</contexts>
<marker>Frazier, Fodor, 1978</marker>
<rawString>Lyn Frazier and Janet Dean Fodor. 1978. The sausage machine: anew two-stage parsing model. Cognition, 6:291– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>A Computational Theory of Human Linguistic</title>
<date>1991</date>
<booktitle>Processing: Memory Limitations and Processing Breakdown. Ph.D. thesis,</booktitle>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="7190" citStr="Gibson (1991" startWordPosition="1154" endWordPosition="1155">sts reconsidering the garden path. Whereas the model examines 45 search nodes during the analysis of the temporarily ambiguous item 1a, it dispatches the unambiguous item 1b after only 40 nodes despite that sentence having an additional token (the comma). Garden paths, on this view, are sequences of parser states explored only in a temporarily ambiguous item. 4 Garden pathing counterexamples Purely structural attachment preferences like Right Association (Kimball, 1973) and Minimal Attachment (Frazier and Fodor, 1978; Pereira, 1985) are threatened by paradoxical counterexamples such as 2 from Gibson (1991, 22) where no fixed principle yields correct predictions across both examples. (2) a. I gave her earrings on her birthday . b. I gave her earrings to another girl . A parser guided by Hypothesis 1 interleaves the garden path attachment and the globally-correct attachment in both cases, resulting in a search that 231 is strictly committed to neither analysis. In 2a, 32% of discovered states represent the globallyincorrect attachment of her. In 2b, 27% of states represent the globally-incorrect attachment of her to give as a one-word direct object. The paradox for purely structural attachment h</context>
</contexts>
<marker>Gibson, 1991</marker>
<rawString>Edward Gibson. 1991. A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Hart</author>
<author>Nils J Nilsson</author>
<author>Bertram Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<journal>IEEE Transactions of systems science and cybernetics,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="3437" citStr="Hart et al., 1968" startWordPosition="520" endWordPosition="523">ment/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Leaving the arceager/arc-standard decision (Abney and Johnson, 1991) to the control policy allows four possible operators, schematized in Table 1. 2.2 Informed Search Informed search differs from uninformed search procedures such as depth-first and breadth-first by making use of heuristic knowledge about the search domain. The strategy is to choose for expansion the node whose cost is lowest (Barr and Feigenbaum, 1981, 61). In A∗ search (Hart et al., 1968) this cost is divided up into a sum consisting of the known cost to reach a search node and an 230 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 230–233, Paris, October 2009. c�2009 Association for Computational Linguistics project a rule LHS → Trigger Rest ↑ announce point shift a word W project and match the sought parent LHS using scan the sought word W the rule LHS → Trigger Rest ↑ announce point Table 1: Four schema define the operators stack n E[steps] standard error [VP] S [TOP] 55790 44.936 0.1572 S [TOP] 53991 10.542 0.0986 [NP] S [TOP] 43635 3</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions of systems science and cybernetics, ssc-4(2):100–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip N Johnson-Laird</author>
</authors>
<title>Mental Models.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2380" citStr="Johnson-Laird (1983)" startWordPosition="365" endWordPosition="366">6 concludes. 2 Architecture of the model 2.1 Problem states and Operators We model the human sentence comprehension mechanism as search within a problem space (Newell and Simon, 1972). We assume that all (incremental) parser states have a (partial) grammatical interpretation (Chomsky, 1965, 9). In this paper, the grammatical interpretation employs context-free grammar. An inventory of operators carries the model from one point in the problem space to another. In the interest of simplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) parsing strategy (Demers, 1977) whose stack consumption is maximal on just the center-embedded examples that are so difficult for people to understand. To reflect the argument/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Lea</context>
</contexts>
<marker>Johnson-Laird, 1983</marker>
<rawString>Philip N. Johnson-Laird. 1983. Mental Models. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>A life of language.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Kay, 2005</marker>
<rawString>Martin Kay. 2005. A life of language. Computational Linguistics, 31(4):425–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John P Kimball</author>
</authors>
<title>Seven principles of surface structure parsing in natural language.</title>
<date>1973</date>
<journal>Cognition,</journal>
<pages>2--15</pages>
<contexts>
<context position="7052" citStr="Kimball, 1973" startWordPosition="1132" endWordPosition="1133">ey line highlights the order in which A* search considers this path. At state 21 after shifting sock, experience with the Brown corpus suggests reconsidering the garden path. Whereas the model examines 45 search nodes during the analysis of the temporarily ambiguous item 1a, it dispatches the unambiguous item 1b after only 40 nodes despite that sentence having an additional token (the comma). Garden paths, on this view, are sequences of parser states explored only in a temporarily ambiguous item. 4 Garden pathing counterexamples Purely structural attachment preferences like Right Association (Kimball, 1973) and Minimal Attachment (Frazier and Fodor, 1978; Pereira, 1985) are threatened by paradoxical counterexamples such as 2 from Gibson (1991, 22) where no fixed principle yields correct predictions across both examples. (2) a. I gave her earrings on her birthday . b. I gave her earrings to another girl . A parser guided by Hypothesis 1 interleaves the garden path attachment and the globally-correct attachment in both cases, resulting in a search that 231 is strictly committed to neither analysis. In 2a, 32% of discovered states represent the globallyincorrect attachment of her. In 2b, 27% of sta</context>
</contexts>
<marker>Kimball, 1973</marker>
<rawString>John P. Kimball. 1973. Seven principles of surface structure parsing in natural language. Cognition, 2:15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Konieczny</author>
<author>Daniel M¨uller</author>
</authors>
<title>Local coherences in sentence processing.</title>
<date>2006</date>
<booktitle>CUNY Conference on Human Sentence Processing.</booktitle>
<marker>Konieczny, M¨uller, 2006</marker>
<rawString>Lars Konieczny and Daniel M¨uller. 2006. Local coherences in sentence processing. CUNY Conference on Human Sentence Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kuˇcera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<marker>Kuˇcera, Francis, 1967</marker>
<rawString>Henry Kuˇcera and W. Nelson Francis. 1967. Computational Analysis of Present-day American English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="4600" citStr="Marcus et al. (1993)" startWordPosition="728" endWordPosition="731">6 0.1572 S [TOP] 53991 10.542 0.0986 [NP] S [TOP] 43635 33.092 0.1633 NP [TOP] 38844 55.791 0.2126 NP [S] S [TOP] 34415 47.132 0.2122 [S] S [TOP] 33578 52.800 0.2195 [PP] S [TOP] 30693 34.454 0.1915 IN [PP] S [TOP] 27272 32.379 0.2031 DT [NP] S [TOP] 22375 34.478 0.2306 [AUX] [VP] S [TOP] 16447 46.536 0.2863 VBD [VP] S [TOP] 16224 43.057 0.2826 VB [VP] S [TOP] 13548 40.404 0.3074 the [NP] S [TOP] 12507 34.120 0.3046 NP [NP] S [TOP] 12092 43.821 0.3269 DT [TOP] 10440 66.452 0.3907 Table 2: Popular left-corner parser states. Stacks grow to the left. The categories are as described in Table 3 of Marcus et al. (1993). estimate of the costs involved in finishing search from that node. In this work, rather than relying on the guarantee provided by the A* theorem, we examine the exploration pattern that results from an inadmissable completion cost estimator. The choice of estimator is Hypothesis 1. Hypothesis 1 Search in parsing is informed by an estimate of the expected number of steps to completion, given previous experience. Table 2 writes out the expected number of steps to completion (E[steps]) for a selection of problem states binned together according to their grammatical interpretation. Categories en</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>A theory of syntactic recognition for natural language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="670" citStr="Marcus, 1980" startWordPosition="99" endWordPosition="100">n T. Hale Cornell University 217 Morrill Hall Ithaca, New York 14853 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Mitchell P. Marcus. 1980. A theory of syntactic recognition for natural language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen Newell</author>
<author>Herbert A Simon</author>
</authors>
<title>Human Problem Solving. Prentice-Hall, Englewood Cliffs,</title>
<date>1972</date>
<location>New Jersey.</location>
<contexts>
<context position="1943" citStr="Newell and Simon, 1972" startWordPosition="293" endWordPosition="296">relationship between calculations carried out in the course of automated syntactic analysis and the inferences about linguistic structure taking place in our minds during ordinary sentence-understanding. Section 2 defines the model itself, highlighting its relationship to generalized left-corner parsing. Sections 3–5 apply this model to three controversial phenomena that are well-established in the psycholinguistics literature. Section 6 concludes. 2 Architecture of the model 2.1 Problem states and Operators We model the human sentence comprehension mechanism as search within a problem space (Newell and Simon, 1972). We assume that all (incremental) parser states have a (partial) grammatical interpretation (Chomsky, 1965, 9). In this paper, the grammatical interpretation employs context-free grammar. An inventory of operators carries the model from one point in the problem space to another. In the interest of simplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human accept</context>
</contexts>
<marker>Newell, Simon, 1972</marker>
<rawString>Allen Newell and Herbert A. Simon. 1972. Human Problem Solving. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>A new characterization of attachment preference. In</title>
<date>1985</date>
<booktitle>Natural Language Parsing: Psychological, Computational and Theoretical Perspectives, ACL Studies in Natural Language Processing,</booktitle>
<pages>307--319</pages>
<editor>David Dowty, Lauri Karttunen, and Arnold Zwicky, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="709" citStr="Pereira, 1985" startWordPosition="105" endWordPosition="106">ll Hall Ithaca, New York 14853 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking and other behavioral studies. It also </context>
<context position="7116" citStr="Pereira, 1985" startWordPosition="1142" endWordPosition="1143">ath. At state 21 after shifting sock, experience with the Brown corpus suggests reconsidering the garden path. Whereas the model examines 45 search nodes during the analysis of the temporarily ambiguous item 1a, it dispatches the unambiguous item 1b after only 40 nodes despite that sentence having an additional token (the comma). Garden paths, on this view, are sequences of parser states explored only in a temporarily ambiguous item. 4 Garden pathing counterexamples Purely structural attachment preferences like Right Association (Kimball, 1973) and Minimal Attachment (Frazier and Fodor, 1978; Pereira, 1985) are threatened by paradoxical counterexamples such as 2 from Gibson (1991, 22) where no fixed principle yields correct predictions across both examples. (2) a. I gave her earrings on her birthday . b. I gave her earrings to another girl . A parser guided by Hypothesis 1 interleaves the garden path attachment and the globally-correct attachment in both cases, resulting in a search that 231 is strictly committed to neither analysis. In 2a, 32% of discovered states represent the globallyincorrect attachment of her. In 2b, 27% of states represent the globally-incorrect attachment of her to give a</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Fernando Pereira. 1985. A new characterization of attachment preference. In David Dowty, Lauri Karttunen, and Arnold Zwicky, editors, Natural Language Parsing: Psychological, Computational and Theoretical Perspectives, ACL Studies in Natural Language Processing, pages 307– 319. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven G Pulman</author>
</authors>
<title>Grammars, parsers, and memory limitations.</title>
<date>1986</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>1--3</pages>
<contexts>
<context position="723" citStr="Pulman, 1986" startWordPosition="107" endWordPosition="108"> New York 14853 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking and other behavioral studies. It also sketches a rel</context>
</contexts>
<marker>Pulman, 1986</marker>
<rawString>Steven G. Pulman. 1986. Grammars, parsers, and memory limitations. Language and Cognitive Processes, 1(3):197–2256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="2398" citStr="Resnik (1992)" startWordPosition="368" endWordPosition="369">re of the model 2.1 Problem states and Operators We model the human sentence comprehension mechanism as search within a problem space (Newell and Simon, 1972). We assume that all (incremental) parser states have a (partial) grammatical interpretation (Chomsky, 1965, 9). In this paper, the grammatical interpretation employs context-free grammar. An inventory of operators carries the model from one point in the problem space to another. In the interest of simplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) parsing strategy (Demers, 1977) whose stack consumption is maximal on just the center-embedded examples that are so difficult for people to understand. To reflect the argument/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Leaving the arceager/</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In Proceedings of the Fourteenth International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari R Speer</author>
<author>Margaret M Kjelgaard</author>
<author>Kathryn M Dobroth</author>
</authors>
<title>The influence of prosodic structure on the resolution of temporary syntactic closure ambiguities.</title>
<date>1996</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="6037" citStr="Speer et al., 1996" startWordPosition="966" endWordPosition="969">cis, 1967; Marcus et al., 1993) according to the mixed strategy introduced above in subsection 2.1. The quantity E[steps] serves in what follows as the completion cost estimate in A* search. 3 Garden pathing Any model of human sentence comprehension should address the garden path effect. The contrast between 1a and 1b is an example of this phenomenon. (1) a. while Mary was mending a sock fell on the floor b. while Mary was mending, a sock fell on the floor The control condition 1b includes a comma which, in spoken language, would be expressed as a prosodic break (Carroll and Slowiaczek, 1991; Speer et al., 1996). Figure 1 shows the search space explored in the experimental condition 1a. In this picture, ovals represent problem states. The number inside the oval encodes the vistation order. Arcs between ovals represent operator applications. The path (14, 22, 23, 24, 25, 29, 27) is the garden path which builds a grammatical interpretation where a sock is attached as a direct object of the verb mend. The grey line highlights the order in which A* search considers this path. At state 21 after shifting sock, experience with the Brown corpus suggests reconsidering the garden path. Whereas the model examin</context>
</contexts>
<marker>Speer, Kjelgaard, Dobroth, 1996</marker>
<rawString>Shari R. Speer, Margaret M. Kjelgaard, and Kathryn M. Dobroth. 1996. The influence of prosodic structure on the resolution of temporary syntactic closure ambiguities. Journal of Psycholinguistic Research, 25(2):249–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>The finite connectivity of linguistic structure.</title>
<date>1994</date>
<booktitle>Perspectives on Sentence Processing,</booktitle>
<pages>303--336</pages>
<editor>In Charles Clifton, Lyn Frazier, and Keith Rayner, editors,</editor>
<publisher>Lawrence Erlbaum.</publisher>
<contexts>
<context position="754" citStr="Stabler, 1994" startWordPosition="111" endWordPosition="112">l.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking and other behavioral studies. It also sketches a relationship between calculations </context>
</contexts>
<marker>Stabler, 1994</marker>
<rawString>Edward Stabler. 1994. The finite connectivity of linguistic structure. In Charles Clifton, Lyn Frazier, and Keith Rayner, editors, Perspectives on Sentence Processing, pages 303–336. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
<author>Bruno Galantuccia</author>
<author>Daniel Richardson</author>
</authors>
<title>Effects of merely local syntactic coherence on sentence processing.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="7930" citStr="Tabor et al. (2004)" startWordPosition="1272" endWordPosition="1275">y . b. I gave her earrings to another girl . A parser guided by Hypothesis 1 interleaves the garden path attachment and the globally-correct attachment in both cases, resulting in a search that 231 is strictly committed to neither analysis. In 2a, 32% of discovered states represent the globallyincorrect attachment of her. In 2b, 27% of states represent the globally-incorrect attachment of her to give as a one-word direct object. The paradox for purely structural attachment heuristics is dissolved by the observation that neither pathway fully achieves priority over the other. 5 Local Coherence Tabor et al. (2004) discovered1 a processing difficulty phenomenon called “local coherence.” Among the stimuli they considered, the locallycoherent condition is 3a where the substring the player tossed a frisbee could be analyzed as a sentence, if considered in isolation. (3) a. The coach smiled at the player tossed a frisbee by the opposing team. b. The coach smiled at the player thrown a frisbee by the opposing team c. The coach smiled at the player who was tossed a frisbee by the opposing team. d. The coach smiled at the player who was thrown a frisbee by the opposing team. Tabor and colleagues observe an int</context>
</contexts>
<marker>Tabor, Galantuccia, Richardson, 2004</marker>
<rawString>Whitney Tabor, Bruno Galantuccia, and Daniel Richardson. 2004. Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50(4):355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Thibadeau</author>
<author>Marcel A Just</author>
<author>Patricia Carpenter</author>
</authors>
<title>A model of the time course and content of reading.</title>
<date>1982</date>
<journal>Cognitive Science,</journal>
<pages>6--157</pages>
<contexts>
<context position="694" citStr="Thibadeau et al., 1982" startWordPosition="101" endWordPosition="104">ell University 217 Morrill Hall Ithaca, New York 14853 jthale@cornell.edu Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. 1 Introduction One of the most interesting applications of parsing technology has, for some researchers, been psycholinguistic models (Kay, 2005). Algorithmic models of language use have led in the past to a variety of cognitive insights (Kaplan, 1972; Marcus, 1980; Thibadeau et al., 1982; Pereira, 1985; Pulman, 1986; Johnson, 1989; Stabler, 1994). However they are challenged by a veritable tidal wave of new data collected during the 1990s and 2000s. Work during this later period reveals phenomena, such as the local coherence effect discussed in section 5, that have yet to be truly integrated into any particular theoretical framework. This short paper presents a parsing system intended to serve as a model of the syntactic part of human sentence comprehension. Such a model helps make sense of sentence-difficulty data from self-paced reading, eye-tracking and other behavioral st</context>
</contexts>
<marker>Thibadeau, Just, Carpenter, 1982</marker>
<rawString>Robert Thibadeau, Marcel A. Just, and Patricia Carpenter. 1982. A model of the time course and content of reading. Cognitive Science, 6:157–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tutunjian</author>
<author>J E Boland</author>
</authors>
<title>Do We Need a Distinction between Arguments and Adjuncts?</title>
<date>2008</date>
<booktitle>Evidence from Psycholinguistic Studies of Comprehension. Language and Linguistics Compass,</booktitle>
<pages>2--4</pages>
<contexts>
<context position="2872" citStr="Tutunjian and Boland, 2008" startWordPosition="431" endWordPosition="435">mplicity, we place no bound on the number of problem states the model can explore. However, we do acknowledge with Johnson-Laird (1983) and Resnik (1992) a pressure to minimize memory consumption internal to a problem state. The model’s within-problem state memory usage should reflect human acceptability judgments with embedded sentences. These considerations motivate a generalized left-corner (GLC) parsing strategy (Demers, 1977) whose stack consumption is maximal on just the center-embedded examples that are so difficult for people to understand. To reflect the argument/adjunct distinction (Tutunjian and Boland, 2008) we adopt a mixed strategy that is bottom-up for optional postmodifiers but left-corner everywhere else. Leaving the arceager/arc-standard decision (Abney and Johnson, 1991) to the control policy allows four possible operators, schematized in Table 1. 2.2 Informed Search Informed search differs from uninformed search procedures such as depth-first and breadth-first by making use of heuristic knowledge about the search domain. The strategy is to choose for expansion the node whose cost is lowest (Barr and Feigenbaum, 1981, 61). In A∗ search (Hart et al., 1968) this cost is divided up into a sum</context>
</contexts>
<marker>Tutunjian, Boland, 2008</marker>
<rawString>D. Tutunjian and J.E. Boland. 2008. Do We Need a Distinction between Arguments and Adjuncts? Evidence from Psycholinguistic Studies of Comprehension. Language and Linguistics Compass, 2(4):631–646.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>