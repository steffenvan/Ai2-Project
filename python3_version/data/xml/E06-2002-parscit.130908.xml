<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002294">
<title confidence="0.997566">
A Web-based Demonstrator of a Multi-lingual Phrase-based
Translation System
</title>
<note confidence="0.476423666666667">
Roldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello Federico
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo - Trento, Italy
</note>
<bodyText confidence="0.868370153846154">
{surname}@itc.it
This paper describes a multi-lingual
phrase-based Statistical Machine Transla-
tion system accessible by means of a Web
page. The user can issue translation re-
quests from Arabic, Chinese or Spanish
into English. The same phrase-based sta-
tistical technology is employed to realize
the three supported language-pairs. New
language-pairs can be easily added to the
demonstrator. The Web-based interface al-
lows the use of the translation system to
any computer connected to the Internet.
</bodyText>
<figure confidence="0.861938571428571">
Abstract
�
= arg max
e
a
≈ arg max
e,a
</figure>
<sectionHeader confidence="0.591055" genericHeader="abstract">
2 SMT System Description
2.1 Log-Linear Model
</sectionHeader>
<bodyText confidence="0.999058714285714">
Given a string f in the source language, the goal of
the statistical machine translation is to select the
string e in the target language which maximizes
the posterior distribution Pr(e  |f). By introduc-
ing the hidden word alignment variable a, the fol-
lowing approximate optimization criterion can be
applied for that purpose:
</bodyText>
<equation confidence="0.97927025">
e∗ = arg max Pr(e  |f)
e
Pr(e, a  |f)
Pr(e, a  |f)
</equation>
<sectionHeader confidence="0.987371" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999038882352941">
At this time, Statistical Machine Translation
(SMT) has empirically proven to be the most
competitive approach in international competi-
tions like the NIST Evaluation Campaigns1 and
the International Workshops on Spoken Language
Translation (IWSLT-20042 and IWSLT-20053).
In this paper we describe our multi-lingual
phrase-based Statistical Machine Translation sys-
tem which can be accessed by means of a Web
page. Section 2 presents the general log-linear
framework to SMT and gives an overview of
our phrase-based SMT system. In section 3
the software architecture of the demo is out-
lined. Section 4 focuses on the currently supported
language-pairs: Arabic-to-English, Chinese-to-
English and Spanish-to-English. In section 5 the
Web-based interface of the demo is described.
</bodyText>
<footnote confidence="0.996937">
1http://www.nist.gov/speech/tests/mt/
2http://www.slt.atr.jp/IWSLT2004/
3http://www.is.cs.cmu.edu/iwslt2005/
</footnote>
<bodyText confidence="0.995969">
Exploiting the maximum entropy (Berger et
al., 1996) framework, the conditional distribu-
tion Pr(e, a  |f) can be determined through
suitable real valued functions (called features)
hr(e, f, a), r = 1... R, and takes the parametric
form:
</bodyText>
<equation confidence="0.981196">
R
pλ(e, a  |f) a exp{ E Arhr(e, f, a)}
r=1
</equation>
<bodyText confidence="0.999967923076923">
The ITC-irst system (Chen et al., 2005) is
based on a log-linear model which extends the
original IBM Model 4 (Brown et al., 1993)
to phrases (Koehn et al., 2003; Federico and
Bertoldi, 2005). In particular, target strings e are
built from sequences of phrases ˜e1 ... ˜el. For each
target phrase e˜ the corresponding source phrase
within the source string is identified through three
random quantities: the fertility φ, which estab-
lishes its length; the permutation πi, which sets
its first position; the tablet ˜f, which tells its word
string. Notice that target phrases might have fer-
tility equal to zero, hence they do not translate any
</bodyText>
<page confidence="0.878988">
91
</page>
<bodyText confidence="0.999972857142857">
source word. Moreover, uncovered source posi-
tions are associated to a special target word (null)
according to specific fertility and permutation ran-
dom variables.
The resulting log-linear model applies eight fea-
ture functions whose parameters are either esti-
mated from data (e.g. target language models,
phrase-based lexicon models) or empirically fixed
(e.g. permutation models). While feature func-
tions exploit statistics extracted from monolingual
or word-aligned texts from the training data, the
scaling factors A of the log-linear model are esti-
mated on the development data by applying a min-
imum error training procedure (Och, 2004).
</bodyText>
<subsectionHeader confidence="0.999868">
2.2 Decoding Strategy
</subsectionHeader>
<bodyText confidence="0.999987814814815">
The translation of an input string is performed by
the SMT system in two steps. In the first pass a
beam search algorithm (decoder) computes a word
graph of translation hypotheses. Hence, either
the best translation hypothesis is directly extracted
from the word graph and output, or an N-best list
of translations is computed (Tran et al., 1996). The
N-best translations are then re-ranked by applying
additional features and the top ranking translation
is finally output.
The decoder exploits dynamic programming,
that is the optimal solution is computed by expand-
ing and recombining previously computed partial
theories. A theory is described by its state which is
the only information needed for its expansion. Ex-
panded theories sharing the same state are recom-
bined, that is only the best scoring one is stored
for further expansions. In order to output a word
graph of translations, backpointers to all expanded
theories are mantained, too.
To cope with the large number of generated the-
ories some approximations are introduced during
the search: less promising theories are pruned off
(beam search) and a new source position is se-
lected by limiting the number of vacant positions
on the left-hand and the distance from the left most
vacant position (re-ordering constraints).
</bodyText>
<subsectionHeader confidence="0.998033">
2.3 Phrase extraction and model training
</subsectionHeader>
<bodyText confidence="0.99969415625">
Training of the phrase-based translation model
requires a parallel corpus provided with word-
alignments in both directions, i.e. from source
to target positions, and viceversa. This pre-
processing step can be accomplished by applying
the GIZA++ toolkit (Och and Ney, 2003) that pro-
vides Viterbi alignments based on IBM Model-4.
Starting from the parallel training corpus, pro-
vided with direct and inverted alignments, the so-
called union alignment (Och and Ney, 2003) is
computed.
Phrase-pairs are extracted from each sentence pair
which correspond to sub-intervals of the source
and target positions, J and I, such that the union
alignment links all positions of J into I and all
positions of I into J. In general, phrases are ex-
tracted with maximum length in the source and tar-
get defined by the parameters Jmax and Imax. All
such phrase-pairs are efficiently computed by an
algorithm with complexity O(lImaxJ2max) (Cet-
tolo et al., 2005).
Given all phrase-pairs extracted from the train-
ing corpus, lexicon probabilities and fertility prob-
abilities are estimated.
Target language models (LMs) used by the de-
coder and rescoring modules are, respectively,
estimated from 3-gram and 4-gram statistics
by applying the modified Kneser-Ney smoothing
method (Goodman and Chen, 1998). LMs are es-
timated with an in-house software toolkit which
also provides a compact binary representation of
the LM which is used by the decoder.
</bodyText>
<sectionHeader confidence="0.990319" genericHeader="method">
3 Demo Architecture
</sectionHeader>
<bodyText confidence="0.999683291666667">
Figure 1 shows the two-layer architecture of the
demo. At the bottom lie the programs that provide
the actual translation services: for each language-
pair a wrapper coordinates the activity of a special-
ized pre-processing tool and a MT decoder. The
translation programs run on a grid-based cluster
of high-end PCs to optimize the processing speed.
All the wrappers communicate with the MT front-
end whose main task is to forward translation re-
quests to the appropriate language-pair wrapper
and to report an error in case of wrong requests
(e.g. unsupported language-pair). It is worth
noticing here that a new language-pair can be eas-
ily added to the system with a minimal interven-
tion on the code of the MT front-end.
At the top of the architecture are the programs
that provide the interface with the user. This layer
is separated from the translation layer (hosted by
internal machines only) by means of a firewall.
The user interface is implemented as a Web page
in which a translation request (a source sentence
and a language-pair) is input by means of an
HTML form. The cgi script invocated by the form
manages the interaction with the MT front-end.
</bodyText>
<figure confidence="0.984874666666667">
92
Web Page
(form)
</figure>
<figureCaption confidence="0.999727">
Figure 1: Architecture of the demo. For each
</figureCaption>
<bodyText confidence="0.995707217391305">
language-pair a set of programs (in particular the
MT decoder) provides the translation service. The
request issued by the user on the Web page is
sent by the cgi script to the MT front-end. The
translation is then performed on the appropriate
language-pair service and the output sent back to
the Web browser.
When a user issues a translation request after
filling the form fields, the cgi script sends the re-
quest to the MT front-end and waits for its reply.
The input sentence is then forwarded to the wrap-
per of the appropriate language-pair. After a pre-
processing step, the actual translation is performed
by the specific MT decoder. The output in the tar-
get language is then sent back to the user’s Web
browser through the chain in the reverse order.
From a technical point of view, the inter-process
communication is realized by means of standard
TCP-IP sockets. As far as the encoding of texts is
concerned, all the languages are encoded in UTF-
8: this allows to manage the processing phase in
an uniform way and to render graphically different
character sets.
</bodyText>
<sectionHeader confidence="0.943384" genericHeader="method">
4 The supported language-pairs
</sectionHeader>
<bodyText confidence="0.999958">
Although there is no theoretical limit to the num-
ber of supported language-pairs, the current ver-
sion of the demo provides translations to English
from three source languages: Arabic, Chinese and
Spanish. For demonstration purpose, three differ-
ent application domains are covered too.
</bodyText>
<sectionHeader confidence="0.499895" genericHeader="method">
Arabic-to-English (Tourism)
</sectionHeader>
<bodyText confidence="0.999802181818182">
The Arabic-to-English system has been trained
with the data provided by the International Work-
shop on Spoken Language Translation 2005 The
context is that of the Basic Traveling Expres-
sion Corpus (BTEC) task (Takezawa et al., 2002).
BTEC is a multilingual speech corpus which con-
tains sentences coming from phrase books for
tourists. Training set includes 20k sentences con-
taining 159K Arabic and 182K English running
words; vocabulary size is 18K for Arabic, 7K for
English.
</bodyText>
<subsectionHeader confidence="0.551522">
Chinese-to-English (Newswire)
</subsectionHeader>
<bodyText confidence="0.999885">
The Chinese-to-English system has been trained
with the data provided by the NIST MT Evaluation
Campaign 2005 , large-data condition. In this case
parallel data are mainly news-wires provided by
news agencies. Training set includes 71M Chinese
and 77M English running words; vocabulary size
is 157K for Chinese, 214K for English.
</bodyText>
<subsectionHeader confidence="0.79355">
Spanish-to-English (European Parliament)
</subsectionHeader>
<bodyText confidence="0.999978777777778">
The Spanish-to-English system has been trained
with the data provided by the Evaluation Cam-
paign 2005 of the European integrated project TC-
STAR4. The context is that of the speeches of
the European Parliament Plenary sessions (EPPS)
from April 1996 to October 2004. Training set for
the Final Text Edition transcriptions includes 31M
Spanish and 30M English running words; vocabu-
lary size is 140K for Spanish, 94K for English.
</bodyText>
<sectionHeader confidence="0.840224" genericHeader="method">
5 The Web-based Interface
</sectionHeader>
<bodyText confidence="0.999669583333333">
Figure 2 shows a snapshot of the Web-based in-
terface of the demo – the URL has been removed
to make this submission anonymous. In the upper
part of the page the user provides the two informa-
tion required for the translation: the source sen-
tence can be input in a 80x5 textarea html struc-
ture, while the language-pair can be selected by
means of a set a radio-buttons. The user can reset
the input area or send the translation request by
means of standard reset and submit buttons. Some
examples of bilingual sentences are provided in
the lower part of the page.
</bodyText>
<figure confidence="0.984015413793104">
4http://www.tc-star.org
CGI
script
external host
firewall
internal hosts
MT
front−end
fast machines
lang 1
wrapper
wrapper
lang 2
...
wrapper
lang N
prepro−
cessing
prepro−
cessing
prepro−
cessing
MT
decoder
MT
decoder
MT
decoder
93
</figure>
<figureCaption confidence="0.952459">
Figure 3: Example of an Arabic sentence trans-
lated into English.
</figureCaption>
<note confidence="0.9955876">
Mauro Cettolo, Marcello Federico, Nicola Bertoldi,
Roldano Cattoni, and Boxing Chen. 2005. A look
inside the itc-irst smt system. In Proceedings of the
10th Machine Translation Summit, pages 451–457,
Phuket, Thailand, September.
</note>
<figureCaption confidence="0.985969">
Figure 2: A snapshot of the Web-based interface.
</figureCaption>
<bodyText confidence="0.999390083333333">
The user provides the sentence to be translated
in the desired language-pair. Some examples of
bilingual sentences are also available to the user.
The output of a translation request is simple: the
requested source sentence, the translation in the
target language and the selected language-pair are
presented to the user. Figure 3 shows an example
of an Arabic sentence translated into English.
We plan to extend the interface with the pos-
sibility for the user to ask additional information
about the translation – e.g. the number of explored
theories or the score of the first-best translation.
</bodyText>
<sectionHeader confidence="0.99937" genericHeader="conclusions">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99983725">
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech to Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999965472222222">
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39–71.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263–313.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and
M. Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceedings of the IWSLT 2005,
Pittsburgh, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-Phrase
Statistical Translation Model. ACM Transactions on
Speech and Language Processing. to appear.
J. Goodman and S. Chen. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACl 2003, pages 127–133, Edmonton, Canada.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
F.J. Och. 2004. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, Sapporo, Japan.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a Broad-Coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, pages 147–152, Las Palmas, Spain.
B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word
Graph based N-Best Search in Continuous Speech
Recognition. In Proceedings of ICLSP, Philadel-
phia, PA, USA.
</reference>
<page confidence="0.965042">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676396">
<title confidence="0.9995355">A Web-based Demonstrator of a Multi-lingual Phrase-based Translation System</title>
<author confidence="0.999767">Roldano Cattoni</author>
<author confidence="0.999767">Nicola Bertoldi</author>
<author confidence="0.999767">Mauro Cettolo</author>
<author confidence="0.999767">Boxing Chen</author>
<author confidence="0.999767">Marcello Federico</author>
<affiliation confidence="0.923944">ITC-irst - Centro per la Ricerca Scientifica e Tecnologica</affiliation>
<address confidence="0.984276">38050 Povo - Trento, Italy</address>
<abstract confidence="0.994355233333334">This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page. The user can issue translation requests from Arabic, Chinese or Spanish into English. The same phrase-based statistical technology is employed to realize the three supported language-pairs. New language-pairs can be easily added to the demonstrator. The Web-based interface allows the use of the translation system to any computer connected to the Internet. Abstract � = arg max e a max 2 SMT System Description 2.1 Log-Linear Model a string the source language, the goal of the statistical machine translation is to select the the target language which maximizes posterior distribution  |By introducthe hidden word the following approximate optimization criterion can be applied for that purpose: arg max | e |</abstract>
<intro confidence="0.838662"></intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2154" citStr="Berger et al., 1996" startWordPosition="314" endWordPosition="317">hrase-based Statistical Machine Translation system which can be accessed by means of a Web page. Section 2 presents the general log-linear framework to SMT and gives an overview of our phrase-based SMT system. In section 3 the software architecture of the demo is outlined. Section 4 focuses on the currently supported language-pairs: Arabic-to-English, Chinese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1http://www.nist.gov/speech/tests/mt/ 2http://www.slt.atr.jp/IWSLT2004/ 3http://www.is.cs.cmu.edu/iwslt2005/ Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f) can be determined through suitable real valued functions (called features) hr(e, f, a), r = 1... R, and takes the parametric form: R pλ(e, a |f) a exp{ E Arhr(e, f, a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases ˜e1 ... ˜el. For each target phrase e˜ the corresponding source phrase within the source string is identified thr</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2510" citStr="Brown et al., 1993" startWordPosition="379" endWordPosition="382">nese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1http://www.nist.gov/speech/tests/mt/ 2http://www.slt.atr.jp/IWSLT2004/ 3http://www.is.cs.cmu.edu/iwslt2005/ Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f) can be determined through suitable real valued functions (called features) hr(e, f, a), r = 1... R, and takes the parametric form: R pλ(e, a |f) a exp{ E Arhr(e, f, a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases ˜e1 ... ˜el. For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi, which sets its first position; the tablet ˜f, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target w</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>R Cattoni</author>
<author>N Bertoldi</author>
<author>M Cettolo</author>
<author>M Federico</author>
</authors>
<date>2005</date>
<booktitle>The ITC-irst SMT System for IWSLT-2005. In Proceedings of the IWSLT 2005,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="2419" citStr="Chen et al., 2005" startWordPosition="362" endWordPosition="365">lined. Section 4 focuses on the currently supported language-pairs: Arabic-to-English, Chinese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1http://www.nist.gov/speech/tests/mt/ 2http://www.slt.atr.jp/IWSLT2004/ 3http://www.is.cs.cmu.edu/iwslt2005/ Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f) can be determined through suitable real valued functions (called features) hr(e, f, a), r = 1... R, and takes the parametric form: R pλ(e, a |f) a exp{ E Arhr(e, f, a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases ˜e1 ... ˜el. For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi, which sets its first position; the tablet ˜f, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate an</context>
</contexts>
<marker>Chen, Cattoni, Bertoldi, Cettolo, Federico, 2005</marker>
<rawString>B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and M. Federico. 2005. The ITC-irst SMT System for IWSLT-2005. In Proceedings of the IWSLT 2005, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>A Word-to-Phrase Statistical Translation Model.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing.</journal>
<note>to appear.</note>
<contexts>
<context position="2571" citStr="Federico and Bertoldi, 2005" startWordPosition="389" endWordPosition="392">the Web-based interface of the demo is described. 1http://www.nist.gov/speech/tests/mt/ 2http://www.slt.atr.jp/IWSLT2004/ 3http://www.is.cs.cmu.edu/iwslt2005/ Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f) can be determined through suitable real valued functions (called features) hr(e, f, a), r = 1... R, and takes the parametric form: R pλ(e, a |f) a exp{ E Arhr(e, f, a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases ˜e1 ... ˜el. For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi, which sets its first position; the tablet ˜f, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to specific fertility and permutation ra</context>
</contexts>
<marker>Federico, Bertoldi, 2005</marker>
<rawString>M. Federico and N. Bertoldi. 2005. A Word-to-Phrase Statistical Translation Model. ACM Transactions on Speech and Language Processing. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
<author>S Chen</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="6284" citStr="Goodman and Chen, 1998" startWordPosition="974" endWordPosition="977">sitions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax. All such phrase-pairs are efficiently computed by an algorithm with complexity O(lImaxJ2max) (Cettolo et al., 2005). Given all phrase-pairs extracted from the training corpus, lexicon probabilities and fertility probabilities are estimated. Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). LMs are estimated with an in-house software toolkit which also provides a compact binary representation of the LM which is used by the decoder. 3 Demo Architecture Figure 1 shows the two-layer architecture of the demo. At the bottom lie the programs that provide the actual translation services: for each languagepair a wrapper coordinates the activity of a specialized pre-processing tool and a MT decoder. The translation programs run on a grid-based cluster of high-end PCs to optimize the processing speed. All the wrappers communicate with the MT frontend whose main task is to forward transla</context>
</contexts>
<marker>Goodman, Chen, 1998</marker>
<rawString>J. Goodman and S. Chen. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACl 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2541" citStr="Koehn et al., 2003" startWordPosition="385" endWordPosition="388">glish. In section 5 the Web-based interface of the demo is described. 1http://www.nist.gov/speech/tests/mt/ 2http://www.slt.atr.jp/IWSLT2004/ 3http://www.is.cs.cmu.edu/iwslt2005/ Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f) can be determined through suitable real valued functions (called features) hr(e, f, a), r = 1... R, and takes the parametric form: R pλ(e, a |f) a exp{ E Arhr(e, f, a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases ˜e1 ... ˜el. For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi, which sets its first position; the tablet ˜f, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to specifi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLTNAACl 2003, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5281" citStr="Och and Ney, 2003" startWordPosition="815" endWordPosition="818"> of generated theories some approximations are introduced during the search: less promising theories are pruned off (beam search) and a new source position is selected by limiting the number of vacant positions on the left-hand and the distance from the left most vacant position (re-ordering constraints). 2.3 Phrase extraction and model training Training of the phrase-based translation model requires a parallel corpus provided with wordalignments in both directions, i.e. from source to target positions, and viceversa. This preprocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax. All such phrase-pairs are efficiently computed by</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3663" citStr="Och, 2004" startWordPosition="561" endWordPosition="562">red source positions are associated to a special target word (null) according to specific fertility and permutation random variables. The resulting log-linear model applies eight feature functions whose parameters are either estimated from data (e.g. target language models, phrase-based lexicon models) or empirically fixed (e.g. permutation models). While feature functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors A of the log-linear model are estimated on the development data by applying a minimum error training procedure (Och, 2004). 2.2 Decoding Strategy The translation of an input string is performed by the SMT system in two steps. In the first pass a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed (Tran et al., 1996). The N-best translations are then re-ranked by applying additional features and the top ranking translation is finally output. The decoder exploits dynamic programming, that is the optimal solution is computed by expanding and re</context>
</contexts>
<marker>Och, 2004</marker>
<rawString>F.J. Och. 2004. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>H Yamamoto</author>
<author>S Yamamoto</author>
</authors>
<title>Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd LREC,</booktitle>
<pages>147--152</pages>
<location>Las Palmas,</location>
<contexts>
<context position="9313" citStr="Takezawa et al., 2002" startWordPosition="1482" endWordPosition="1485">o render graphically different character sets. 4 The supported language-pairs Although there is no theoretical limit to the number of supported language-pairs, the current version of the demo provides translations to English from three source languages: Arabic, Chinese and Spanish. For demonstration purpose, three different application domains are covered too. Arabic-to-English (Tourism) The Arabic-to-English system has been trained with the data provided by the International Workshop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). BTEC is a multilingual speech corpus which contains sentences coming from phrase books for tourists. Training set includes 20k sentences containing 159K Arabic and 182K English running words; vocabulary size is 18K for Arabic, 7K for English. Chinese-to-English (Newswire) The Chinese-to-English system has been trained with the data provided by the NIST MT Evaluation Campaign 2005 , large-data condition. In this case parallel data are mainly news-wires provided by news agencies. Training set includes 71M Chinese and 77M English running words; vocabulary size is 157K for Chinese, 214K for Engl</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and S. Yamamoto. 2002. Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World. In Proceedings of 3rd LREC, pages 147–152, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Tran</author>
<author>F Seide</author>
<author>V Steinbiss</author>
</authors>
<title>A Word Graph based N-Best Search in Continuous Speech Recognition.</title>
<date>1996</date>
<booktitle>In Proceedings of ICLSP,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="4033" citStr="Tran et al., 1996" startWordPosition="620" endWordPosition="623">ure functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors A of the log-linear model are estimated on the development data by applying a minimum error training procedure (Och, 2004). 2.2 Decoding Strategy The translation of an input string is performed by the SMT system in two steps. In the first pass a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed (Tran et al., 1996). The N-best translations are then re-ranked by applying additional features and the top ranking translation is finally output. The decoder exploits dynamic programming, that is the optimal solution is computed by expanding and recombining previously computed partial theories. A theory is described by its state which is the only information needed for its expansion. Expanded theories sharing the same state are recombined, that is only the best scoring one is stored for further expansions. In order to output a word graph of translations, backpointers to all expanded theories are mantained, too.</context>
</contexts>
<marker>Tran, Seide, Steinbiss, 1996</marker>
<rawString>B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word Graph based N-Best Search in Continuous Speech Recognition. In Proceedings of ICLSP, Philadelphia, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>