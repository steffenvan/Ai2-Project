<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007695">
<title confidence="0.99743">
Active Zipfian Sampling for Statistical Parser Training∗
</title>
<author confidence="0.953771">
Onur C¸ obano˘glu
</author>
<affiliation confidence="0.90862125">
Department of Computer Science
Sennott Square
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.998009">
onc3@pitt.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983424">
Active learning has proven to be a successful
strategy in quick development of corpora to be
used in training of statistical natural language
parsers. A vast majority of studies in this
field has focused on estimating informative-
ness of samples; however, representativeness
of samples is another important criterion to be
considered in active learning. We present a
novel metric for estimating representativeness
of sentences, based on a modification of Zipf’s
Principle of Least Effort. Experiments on
WSJ corpus with a wide-coverage parser show
that our method performs always at least as
good as and generally significantly better than
alternative representativeness-based methods.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986987414634146">
Wide coverage statistical parsers (Collins, 1997;
Charniak, 2000) have proven to require large
amounts of manually annotated data for training to
achieve substantial performance. However, build-
ing such large annotated corpora is very expensive
in terms of human effort, time and cost (Marcus et
al., 1993). Several alternatives of the standard super-
vised learning setting have been proposed to reduce
the annotation costs, one of which is active learning.
Active learning setting allows the learner to select its
own samples to be labeled and added to the training
data iteratively. The motive behind active learning
∗Vast majority of this work was done while the author was a
graduate student in Middle East Technical University, under the
funding from T ¨UB˙ITAK-B˙IDEB through 2210 National Schol-
arship Programme for MSc Students.
is that if the learner may select highly informative
samples, it can eliminate the redundancy generally
found in random data; however, informative sam-
ples can be very untypical (Tang et al., 2002). Un-
like random sampling, active learning has no guar-
antee of selecting representative samples and untyp-
ical training samples are expected to degrade test
performance of a classifier.
To get around this problem, several methods of
estimating representativeness of a sample have been
introduced. In this study, we propose a novel rep-
resentativeness estimator for a sentence, which is
based on a modification of Zipf’s Principle ofLeast
Effort (Zipf, 1949), theoretically sound and em-
pirically validated on Brown corpus (Francis and
Ku˘cera, 1967). Experiments conducted with a wide
coverage CCG parser (Clark and Curran, 2004;
Clark and Curran, 2007) on CCGbank (Hocken-
maier and Steedman, 2005) show that using our esti-
mator as a representativeness metric never performs
worse than and generally outperforms length bal-
anced sampling (Becker and Osborne, 2005), which
is another representativeness based active learn-
ing method, and pure informativeness based active
learning.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999559428571429">
In selective sampling setting, there are three criteria
to be considered while choosing a sample to add to
the training data (Dan, 2004; Tang et al., 2002): In-
formativeness (what will the expected contribution
of this sample to the current model be?), represen-
tativeness (what is the estimated probability of see-
ing this sample in the target population?) and diver-
</bodyText>
<page confidence="0.982382">
249
</page>
<note confidence="0.357946">
Proceedings of NAACL HLT 2009: Short Papers, pages 249–252,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99980345">
sity (how different are the samples in a batch from
each other?). The last criterion applies only to the
batch-mode setting, in which the training data is in-
cremented by multiple samples at each step for prac-
tical purposes.
Most of the active learning research in statistical
parser training domain has focused on informative-
ness measures developed for both single and multi-
learner settings. The informativeness measures for
single-learners that have exhibited significant per-
formance in well known experimental domains are
as follow: Selecting the sentences unparsable by the
current model (and if the batch does not get filled,
using a secondary method) (Thompson et al., 1999);
selecting the sentences with the highest tree entropy,
i.e. the Shannon entropy of parses the probabilistic
parser assigns to the sentence (Hwa, 2004); select-
ing the sentences having lowest best probabilities,
where best probability is the conditional probability
of the most probable parse, given the sentence and
the current model (Osborne and Baldridge, 2004);
primarily selecting the sentences that are expected
to include events observed with low frequency so
far with the help of bagging and filling the rest of
the batch according to tree entropy, which is named
as two-stage active learning by Becker and Os-
borne (2005). Proposed informativeness measures
for multiple learners and ensemble learners can be
found in (Baldridge and Osborne, 2003; Osborne
and Baldridge, 2004; Becker and Osborne, 2005;
Baldridge and Osborne, 2008).
As for representativeness measures, Tang et.
al. (2002) proposed using sample density, i.e. the
inverse of the average distance of the sample to the
other samples in the pool, according to some dis-
tance metric. Becker and Osborne (2005) introduced
length balanced sampling, in which the length his-
togram of the batch is kept equal to the length his-
togram of a random sample of batch size drawn from
the pool.
</bodyText>
<sectionHeader confidence="0.978658" genericHeader="method">
3 Description Of The Work
</sectionHeader>
<bodyText confidence="0.9999755">
We introduce a novel representativeness measure for
statistical parser training domain. Our measure is a
function proposed in (Sigurd et al., 2004), which es-
timates the relative frequencies of sentence lengths
in a natural language. Sigurd et. al. (2004) claimed
that the longer a sentence is, the less likely it will be
uttered; in accordance with Zipf’s Principle of Least
Effort (Zipf, 1935). However, too short sentences
will appear infrequently as well, since the number
of different statements that may be expressed using
relatively fewer words is relatively smaller. Authors
conjectured that there is a clash of expressivity and
effort over the frequency of sentence length, which
effort eventually wins. They formulated this behav-
ior with a Gamma distribution estimating the relative
frequencies of sentence lengths. Authors conducted
a parameter fit study for English using Brown cor-
pus (Francis and Ku˘cera, 1967) and reported that the
formula f(L) = 1.1 × Li × 0.90L, where L is the
sentence length, fits to the observations with very
high correlation.
We propose using this fitted formula (named
fzipf−eng from now on) as the measure of repre-
sentativeness of a sentence. This metric has sev-
eral nice features: It is model-independent (so it is
not affected from modeling errors), is both theoreti-
cally sound and empirically validated, can be used in
other NLP domains and is a numerical metric, pro-
viding flexibility in combining it with informative-
ness (and diversity) measures.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993258">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999993625">
We conducted experiments on CCGbank cor-
pus (Hockenmaier and Steedman, 2005) with the
wide coverage CCG parser of Clark and Cur-
ran (2004; 2007)1. C&amp;C parser was fast enough to
enable us to use the whole available training data
pool for sample selection in experiments, but not for
training (since training C&amp;C parser is not that fast).
Among the models implemented in the parser, the
normal-form model is used. We used the default set-
tings of the C&amp;C parser distribution for fair evalu-
ation. WSJ Sections 02-21 (39604 sentences) are
used for training and WSJ Section 23 (2407 sen-
tences) is used for testing. Following (Clark and
Curran, 2007), we evaluated the parser performance
using the labeled f-score of the predicate-argument
dependencies produced by the parser.
</bodyText>
<footnote confidence="0.988116666666667">
1Following (Baldridge and Osborne, 2004), we claim that
the performances of AL with C&amp;C parser and other state-of-
the-art wide coverage parsers will be similar
</footnote>
<page confidence="0.981671">
250
</page>
<figure confidence="0.998987346938776">
15000 20000 25000 30000 35000 40000 45000 50000
Annotation cost (number of brackets)
Labeled f-score
75
74
73
72
71
70
69
68
67
’random’
’entropy’
’
’entropy_lbs
’entropy_zipf’
15000 20000 25000 30000 35000 40000 45000 50000
Annotation cost (number of brackets)
Labeled f-score
75
74
73
72
71
70
69
68
67
’random’
’lbp’
’lbp_lbs’
’lbp_zipf’
15000 20000 25000 30000 35000 40000 45000 50000
Annotation cost (number of brackets)
Labeled f-score
75
74
73
72
71
70
69
68
67
’random’
twtwose tage
’ostag_lbs’’
’twostage_zipf’
</figure>
<figureCaption confidence="0.988782333333333">
Figure 1: Comparative performances of different representativeness measures. The informativeness measure used is
tree entropy in the leftmost graph, lowest best probability in the central graph and two-stage AL in the rightmost graph.
The line with the tag ’random’ always shows the random sampling baseline.
</figureCaption>
<bodyText confidence="0.6019752">
none lbs
entropy 30.99%(74.24%) 20.63%(74.31%) 30.11
lbp 22.34%(74.37%) 20.78%(74.49%) 30.19
unparsed/entropy 19.98%(74.32%) 19.34%(74.43%) 26.27
twostage 2.83%(73.94%) 11.13%(74.09%) 13.38
</bodyText>
<table confidence="0.8099916">
zipf random
%(74.36 %) N/A (74.35%)
%(74.43 %) N/A(74.50 %)
%(74.38 %) N/A(74.35 %)
%(74.05 %) N/A(73.94 %)
</table>
<tableCaption confidence="0.950724">
Table 1: PRUD values of different AL schemes. The row includes the informativeness measure and the column
includes the representativeness measure used. The column with the label random always includes the results for
random sampling. The numbers in parentheses are the labeled f-score values reached by the schemes.
</tableCaption>
<bodyText confidence="0.974942078431372">
For each active learning scheme and random sam-
pling, the size of the seed training set is 500 sen-
tences, the batch size is 100 sentences and itera-
tion stops after reaching 2000 sentences.2 For sta-
tistical significance, each experiment is replicated 5
times. We evaluate the active learning performance
in terms of Percentage Reduction in Utilized Data,
i.e. how many percents less data is used by AL
compared to random sampling, in order to reach a
certain performance score. Amount of used data is
measured with the number of brackets in the data. In
CCGbank, a bracket always corresponds to a parse
decision, so it is a reasonable approximation of the
amount of annotator effort.
Our measure is compared to length balanced sam-
pling and using no representativeness measures.
Since there is not a trivial distance metric between
CCG parses and we do not know a proposed one, we
could not test it against sample density method. We
limited the informativeness measures to be tested
to the four single-learner measures we mentioned
in Section 2. Multi-learner and ensemble methods
are excluded, since the success of such methods re-
2These values apply to the training of the parser and the
CCG supertagger. POS-tagger is trained with the whole avail-
able pool of 39604 sentences due to sparse data problem.
lies heavily on the diversity of the available mod-
els (Baldridge and Osborne, 2004; Baldridge and
Osborne, 2008). The models in C&amp;C parser are
not diverse enough and we left crafting such diverse
models to future work.
We combined fzipf−eng with the informativeness
measures as follow: With tree entropy, sentences
with the highest fzipf−eng(s) × fnte(s,G) (named
fzipf−entropy(s, G)) values are selected. fnte(s, G)
is the tree entropy of the sentence s under the cur-
rent model G, normalized by the binary logarithm of
the number of parses, following (Hwa, 2004). With
lowest best probability, sentences with the high-
est fzipf−eng(s) × (1 − fbp(s, G)) values are se-
lected, where fbp is the best probability function
(see Section 2). With unparsed/entropy, we primar-
ily chose the unparsable sentences having highest
fzipf−eng(s) values and filled the rest of the batch
according to fzipf−entropy. With two-stage active
learning, we primarily chose sentences that can be
parsed by the full parser but not the bagged parser
and have the highest fzipf−eng(s) values, we secon-
darily chose sentences that cannot be parsed by both
parsers and have the highest fzipf−eng(s) values, the
third priority is given to sentences having highest
</bodyText>
<page confidence="0.993782">
251
</page>
<bodyText confidence="0.9990246">
fzipf−entropy values.3 Combining length balanced
sampling with all of these informativeness measures
is straightforward. For statistical significance, a dif-
ferent random sample is used for length histogram
in each replication of experiment.
</bodyText>
<sectionHeader confidence="0.952745" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<subsectionHeader confidence="0.948408">
Results can be seen in Figure 1 and Table 1. Due
</subsectionHeader>
<bodyText confidence="0.999674076923077">
to lack of space and similarity of the graphs of un-
parsed/entropy and LBP, we excluded the graph of
unparsed/entropy (but its results are included in Ta-
ble 1). Since observation points in different lines do
not fall on the exactly same performance level (for
exact PRUD measurement), we took the points on as
closest f-score levels as possible. With tree entropy,
Zipfian sampling performs almost as good as pure
informativeness based AL and with two-stage AL,
length balanced sampling performs almost as good
as Zipfian sampling. In all other comparisons, Zip-
fian sampling outperforms its alternatives substan-
tially.
</bodyText>
<sectionHeader confidence="0.991039" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990181818182">
We introduced a representativeness measure for ac-
tive learning in statistical parser training domain,
based on an empirical sentence length frequency
model of English. Experiments on a wide cover-
age CCG parser show that this measure outperforms
the alternative measures most of the time and never
hinders. Our study can be extended via further ex-
perimentation with the methods we excluded in Sec-
tion 4.1, with other parsers, with other languages
and with other Zipfian cues of language (e.g. Zipf’s
law on word frequencies (Zipf, 1949)).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996">
We specially thank to Jason Baldridge, Cem
Bozs¸ahin, Ruken C¸akıcı, Rebecca Hwa, Miles Os-
borne and anonymous reviewers for their invaluable
support, advices and feedback.
</bodyText>
<sectionHeader confidence="0.98557" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.60987225">
Jason Baldridge and Miles Osborne. 2003. Active learn-
ing for HPSG parse selection. In Proceedings of
CoNLL.
3Note that our usage of two-stage AL is slightly different
from the original definition in (Becker and Osborne, 2005)
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP.
</bodyText>
<reference confidence="0.990193306122449">
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. In Natural Language Engineering, volume 14,
pages 199–222. Cambridge, UK.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings ofIJCAI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings ofNAACL.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings ofACL.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings ofACL.
Shen Dan. 2004. Multi-criteria based active learning for
named entity recognition. Master’s thesis, National
University of Singapore.
W. Nelson Francis and Henry Ku˘cera. 1967. Com-
putational Analysis of Present-day American English.
Brown University Press, Providence, RI.
Julia Hockenmaier and Mark Steedman. 2005. CCG-
bank. Linguistic Data Consortium, Philadelphia.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English:The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In Proceed-
ings ofHLT-NAACL.
Bengt Sigurd, Mats Eeg-Olofsson, and Joost van Weijer.
2004. Word length, sentence length and frequency -
Zipf revisited. StudiaLinguistica, 58(1):37–52.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive learning for statistical natural language parsing. In
Proceedings ofACL.
Cynthia A. Thompson, Mary E. Califf, and Raymond J.
Mooney. 1999. Active learning for natural language
parsing and information extraction. In Proceedings of
ICML.
George K. Zipf. 1935. The Psychobiology of Language.
MIT Press, Cambridge, MA. Reprinted in 1965.
George K. Zipf. 1949. Human Behavior and the Princi-
ple ofLeast Effort. Addison-Wesley, Cambridge, MA.
</reference>
<page confidence="0.997381">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.360923">
<author confidence="0.543632">Zipfian Sampling for Statistical Parser</author>
<affiliation confidence="0.816614666666667">Department of Computer Sennott University of</affiliation>
<address confidence="0.867464">Pittsburgh, PA 15260,</address>
<email confidence="0.938949">onc3@pitt.edu</email>
<abstract confidence="0.9994873125">Active learning has proven to be a successful strategy in quick development of corpora to be used in training of statistical natural language parsers. A vast majority of studies in this field has focused on estimating informativeness of samples; however, representativeness of samples is another important criterion to be considered in active learning. We present a novel metric for estimating representativeness of sentences, based on a modification of Zipf’s of Least Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally significantly better than alternative representativeness-based methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and logarithmic opinion pools for HPSG parse selection.</title>
<date>2008</date>
<journal>In Natural Language Engineering,</journal>
<volume>14</volume>
<pages>199--222</pages>
<location>Cambridge, UK.</location>
<contexts>
<context position="4938" citStr="Baldridge and Osborne, 2008" startWordPosition="748" endWordPosition="751">, where best probability is the conditional probability of the most probable parse, given the sentence and the current model (Osborne and Baldridge, 2004); primarily selecting the sentences that are expected to include events observed with low frequency so far with the help of bagging and filling the rest of the batch according to tree entropy, which is named as two-stage active learning by Becker and Osborne (2005). Proposed informativeness measures for multiple learners and ensemble learners can be found in (Baldridge and Osborne, 2003; Osborne and Baldridge, 2004; Becker and Osborne, 2005; Baldridge and Osborne, 2008). As for representativeness measures, Tang et. al. (2002) proposed using sample density, i.e. the inverse of the average distance of the sample to the other samples in the pool, according to some distance metric. Becker and Osborne (2005) introduced length balanced sampling, in which the length histogram of the batch is kept equal to the length histogram of a random sample of batch size drawn from the pool. 3 Description Of The Work We introduce a novel representativeness measure for statistical parser training domain. Our measure is a function proposed in (Sigurd et al., 2004), which estimate</context>
<context position="10692" citStr="Baldridge and Osborne, 2008" startWordPosition="1671" endWordPosition="1674">e is not a trivial distance metric between CCG parses and we do not know a proposed one, we could not test it against sample density method. We limited the informativeness measures to be tested to the four single-learner measures we mentioned in Section 2. Multi-learner and ensemble methods are excluded, since the success of such methods re2These values apply to the training of the parser and the CCG supertagger. POS-tagger is trained with the whole available pool of 39604 sentences due to sparse data problem. lies heavily on the diversity of the available models (Baldridge and Osborne, 2004; Baldridge and Osborne, 2008). The models in C&amp;C parser are not diverse enough and we left crafting such diverse models to future work. We combined fzipf−eng with the informativeness measures as follow: With tree entropy, sentences with the highest fzipf−eng(s) × fnte(s,G) (named fzipf−entropy(s, G)) values are selected. fnte(s, G) is the tree entropy of the sentence s under the current model G, normalized by the binary logarithm of the number of parses, following (Hwa, 2004). With lowest best probability, sentences with the highest fzipf−eng(s) × (1 − fbp(s, G)) values are selected, where fbp is the best probability func</context>
</contexts>
<marker>Baldridge, Osborne, 2008</marker>
<rawString>Jason Baldridge and Miles Osborne. 2008. Active learning and logarithmic opinion pools for HPSG parse selection. In Natural Language Engineering, volume 14, pages 199–222. Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Miles Osborne</author>
</authors>
<title>A two-stage method for active learning of statistical grammars.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCAI.</booktitle>
<contexts>
<context position="2783" citStr="Becker and Osborne, 2005" startWordPosition="416" endWordPosition="419">timating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be considered while choosing a sample to add to the training data (Dan, 2004; Tang et al., 2002): Informativeness (what will the expected contribution of this sample to the current model be?), representativeness (what is the estimated probability of seeing this sample in the target population?) and diver249 Proceedings of NAACL HLT 2009: Short Papers, pages 249–252, Boulder, Colorado, June 2009. c�2009 Asso</context>
<context position="4729" citStr="Becker and Osborne (2005)" startWordPosition="718" endWordPosition="722">lecting the sentences with the highest tree entropy, i.e. the Shannon entropy of parses the probabilistic parser assigns to the sentence (Hwa, 2004); selecting the sentences having lowest best probabilities, where best probability is the conditional probability of the most probable parse, given the sentence and the current model (Osborne and Baldridge, 2004); primarily selecting the sentences that are expected to include events observed with low frequency so far with the help of bagging and filling the rest of the batch according to tree entropy, which is named as two-stage active learning by Becker and Osborne (2005). Proposed informativeness measures for multiple learners and ensemble learners can be found in (Baldridge and Osborne, 2003; Osborne and Baldridge, 2004; Becker and Osborne, 2005; Baldridge and Osborne, 2008). As for representativeness measures, Tang et. al. (2002) proposed using sample density, i.e. the inverse of the average distance of the sample to the other samples in the pool, according to some distance metric. Becker and Osborne (2005) introduced length balanced sampling, in which the length histogram of the batch is kept equal to the length histogram of a random sample of batch size d</context>
</contexts>
<marker>Becker, Osborne, 2005</marker>
<rawString>Markus Becker and Miles Osborne. 2005. A two-stage method for active learning of statistical grammars. In Proceedings ofIJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="960" citStr="Charniak, 2000" startWordPosition="134" endWordPosition="135"> A vast majority of studies in this field has focused on estimating informativeness of samples; however, representativeness of samples is another important criterion to be considered in active learning. We present a novel metric for estimating representativeness of sentences, based on a modification of Zipf’s Principle of Least Effort. Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally significantly better than alternative representativeness-based methods. 1 Introduction Wide coverage statistical parsers (Collins, 1997; Charniak, 2000) have proven to require large amounts of manually annotated data for training to achieve substantial performance. However, building such large annotated corpora is very expensive in terms of human effort, time and cost (Marcus et al., 1993). Several alternatives of the standard supervised learning setting have been proposed to reduce the annotation costs, one of which is active learning. Active learning setting allows the learner to select its own samples to be labeled and added to the training data iteratively. The motive behind active learning ∗Vast majority of this work was done while the a</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2549" citStr="Clark and Curran, 2004" startWordPosition="380" endWordPosition="383">Unlike random sampling, active learning has no guarantee of selecting representative samples and untypical training samples are expected to degrade test performance of a classifier. To get around this problem, several methods of estimating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be considered while choosing a sample to add to the training data (Dan, 2004; Tang et al., 2002): Informativeness (what will the expected contribution of this sample to the cur</context>
<context position="7039" citStr="Clark and Curran (2004" startWordPosition="1089" endWordPosition="1093">ery high correlation. We propose using this fitted formula (named fzipf−eng from now on) as the measure of representativeness of a sentence. This metric has several nice features: It is model-independent (so it is not affected from modeling errors), is both theoretically sound and empirically validated, can be used in other NLP domains and is a numerical metric, providing flexibility in combining it with informativeness (and diversity) measures. 4 Experiments 4.1 Experimental Setup We conducted experiments on CCGbank corpus (Hockenmaier and Steedman, 2005) with the wide coverage CCG parser of Clark and Curran (2004; 2007)1. C&amp;C parser was fast enough to enable us to use the whole available training data pool for sample selection in experiments, but not for training (since training C&amp;C parser is not that fast). Among the models implemented in the parser, the normal-form model is used. We used the default settings of the C&amp;C parser distribution for fair evaluation. WSJ Sections 02-21 (39604 sentences) are used for training and WSJ Section 23 (2407 sentences) is used for testing. Following (Clark and Curran, 2007), we evaluated the parser performance using the labeled f-score of the predicate-argument depe</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2574" citStr="Clark and Curran, 2007" startWordPosition="384" endWordPosition="387">active learning has no guarantee of selecting representative samples and untypical training samples are expected to degrade test performance of a classifier. To get around this problem, several methods of estimating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be considered while choosing a sample to add to the training data (Dan, 2004; Tang et al., 2002): Informativeness (what will the expected contribution of this sample to the current model be?), represen</context>
<context position="7545" citStr="Clark and Curran, 2007" startWordPosition="1176" endWordPosition="1179">iments on CCGbank corpus (Hockenmaier and Steedman, 2005) with the wide coverage CCG parser of Clark and Curran (2004; 2007)1. C&amp;C parser was fast enough to enable us to use the whole available training data pool for sample selection in experiments, but not for training (since training C&amp;C parser is not that fast). Among the models implemented in the parser, the normal-form model is used. We used the default settings of the C&amp;C parser distribution for fair evaluation. WSJ Sections 02-21 (39604 sentences) are used for training and WSJ Section 23 (2407 sentences) is used for testing. Following (Clark and Curran, 2007), we evaluated the parser performance using the labeled f-score of the predicate-argument dependencies produced by the parser. 1Following (Baldridge and Osborne, 2004), we claim that the performances of AL with C&amp;C parser and other state-ofthe-art wide coverage parsers will be similar 250 15000 20000 25000 30000 35000 40000 45000 50000 Annotation cost (number of brackets) Labeled f-score 75 74 73 72 71 70 69 68 67 ’random’ ’entropy’ ’ ’entropy_lbs ’entropy_zipf’ 15000 20000 25000 30000 35000 40000 45000 50000 Annotation cost (number of brackets) Labeled f-score 75 74 73 72 71 70 69 68 67 ’rand</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="943" citStr="Collins, 1997" startWordPosition="132" endWordPosition="133">nguage parsers. A vast majority of studies in this field has focused on estimating informativeness of samples; however, representativeness of samples is another important criterion to be considered in active learning. We present a novel metric for estimating representativeness of sentences, based on a modification of Zipf’s Principle of Least Effort. Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally significantly better than alternative representativeness-based methods. 1 Introduction Wide coverage statistical parsers (Collins, 1997; Charniak, 2000) have proven to require large amounts of manually annotated data for training to achieve substantial performance. However, building such large annotated corpora is very expensive in terms of human effort, time and cost (Marcus et al., 1993). Several alternatives of the standard supervised learning setting have been proposed to reduce the annotation costs, one of which is active learning. Active learning setting allows the learner to select its own samples to be labeled and added to the training data iteratively. The motive behind active learning ∗Vast majority of this work was</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Dan</author>
</authors>
<title>Multi-criteria based active learning for named entity recognition. Master’s thesis,</title>
<date>2004</date>
<institution>National University of Singapore.</institution>
<contexts>
<context position="3049" citStr="Dan, 2004" startWordPosition="459" endWordPosition="460">us (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be considered while choosing a sample to add to the training data (Dan, 2004; Tang et al., 2002): Informativeness (what will the expected contribution of this sample to the current model be?), representativeness (what is the estimated probability of seeing this sample in the target population?) and diver249 Proceedings of NAACL HLT 2009: Short Papers, pages 249–252, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sity (how different are the samples in a batch from each other?). The last criterion applies only to the batch-mode setting, in which the training data is incremented by multiple samples at each step for practical purposes. Most</context>
</contexts>
<marker>Dan, 2004</marker>
<rawString>Shen Dan. 2004. Multi-criteria based active learning for named entity recognition. Master’s thesis, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Ku˘cera</author>
</authors>
<title>Computational Analysis of Present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<marker>Francis, Ku˘cera, 1967</marker>
<rawString>W. Nelson Francis and Henry Ku˘cera. 1967. Computational Analysis of Present-day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<date>2005</date>
<booktitle>CCGbank. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="2618" citStr="Hockenmaier and Steedman, 2005" startWordPosition="390" endWordPosition="394">selecting representative samples and untypical training samples are expected to degrade test performance of a classifier. To get around this problem, several methods of estimating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be considered while choosing a sample to add to the training data (Dan, 2004; Tang et al., 2002): Informativeness (what will the expected contribution of this sample to the current model be?), representativeness (what is the estimated probabilit</context>
<context position="6979" citStr="Hockenmaier and Steedman, 2005" startWordPosition="1078" endWordPosition="1081">.90L, where L is the sentence length, fits to the observations with very high correlation. We propose using this fitted formula (named fzipf−eng from now on) as the measure of representativeness of a sentence. This metric has several nice features: It is model-independent (so it is not affected from modeling errors), is both theoretically sound and empirically validated, can be used in other NLP domains and is a numerical metric, providing flexibility in combining it with informativeness (and diversity) measures. 4 Experiments 4.1 Experimental Setup We conducted experiments on CCGbank corpus (Hockenmaier and Steedman, 2005) with the wide coverage CCG parser of Clark and Curran (2004; 2007)1. C&amp;C parser was fast enough to enable us to use the whole available training data pool for sample selection in experiments, but not for training (since training C&amp;C parser is not that fast). Among the models implemented in the parser, the normal-form model is used. We used the default settings of the C&amp;C parser distribution for fair evaluation. WSJ Sections 02-21 (39604 sentences) are used for training and WSJ Section 23 (2407 sentences) is used for testing. Following (Clark and Curran, 2007), we evaluated the parser performa</context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. CCGbank. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="4252" citStr="Hwa, 2004" startWordPosition="645" endWordPosition="646">s. Most of the active learning research in statistical parser training domain has focused on informativeness measures developed for both single and multilearner settings. The informativeness measures for single-learners that have exhibited significant performance in well known experimental domains are as follow: Selecting the sentences unparsable by the current model (and if the batch does not get filled, using a secondary method) (Thompson et al., 1999); selecting the sentences with the highest tree entropy, i.e. the Shannon entropy of parses the probabilistic parser assigns to the sentence (Hwa, 2004); selecting the sentences having lowest best probabilities, where best probability is the conditional probability of the most probable parse, given the sentence and the current model (Osborne and Baldridge, 2004); primarily selecting the sentences that are expected to include events observed with low frequency so far with the help of bagging and filling the rest of the batch according to tree entropy, which is named as two-stage active learning by Becker and Osborne (2005). Proposed informativeness measures for multiple learners and ensemble learners can be found in (Baldridge and Osborne, 200</context>
<context position="11143" citStr="Hwa, 2004" startWordPosition="1746" endWordPosition="1747">of 39604 sentences due to sparse data problem. lies heavily on the diversity of the available models (Baldridge and Osborne, 2004; Baldridge and Osborne, 2008). The models in C&amp;C parser are not diverse enough and we left crafting such diverse models to future work. We combined fzipf−eng with the informativeness measures as follow: With tree entropy, sentences with the highest fzipf−eng(s) × fnte(s,G) (named fzipf−entropy(s, G)) values are selected. fnte(s, G) is the tree entropy of the sentence s under the current model G, normalized by the binary logarithm of the number of parses, following (Hwa, 2004). With lowest best probability, sentences with the highest fzipf−eng(s) × (1 − fbp(s, G)) values are selected, where fbp is the best probability function (see Section 2). With unparsed/entropy, we primarily chose the unparsable sentences having highest fzipf−eng(s) values and filled the rest of the batch according to fzipf−entropy. With two-stage active learning, we primarily chose sentences that can be parsed by the full parser but not the bagged parser and have the highest fzipf−eng(s) values, we secondarily chose sentences that cannot be parsed by both parsers and have the highest fzipf−eng</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary A Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English:The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1200" citStr="Marcus et al., 1993" startWordPosition="170" endWordPosition="173">timating representativeness of sentences, based on a modification of Zipf’s Principle of Least Effort. Experiments on WSJ corpus with a wide-coverage parser show that our method performs always at least as good as and generally significantly better than alternative representativeness-based methods. 1 Introduction Wide coverage statistical parsers (Collins, 1997; Charniak, 2000) have proven to require large amounts of manually annotated data for training to achieve substantial performance. However, building such large annotated corpora is very expensive in terms of human effort, time and cost (Marcus et al., 1993). Several alternatives of the standard supervised learning setting have been proposed to reduce the annotation costs, one of which is active learning. Active learning setting allows the learner to select its own samples to be labeled and added to the training data iteratively. The motive behind active learning ∗Vast majority of this work was done while the author was a graduate student in Middle East Technical University, under the funding from T ¨UB˙ITAK-B˙IDEB through 2210 National Scholarship Programme for MSc Students. is that if the learner may select highly informative samples, it can el</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English:The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldridge</author>
</authors>
<title>Ensemblebased active learning for parse selection.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="4464" citStr="Osborne and Baldridge, 2004" startWordPosition="674" endWordPosition="677">sures for single-learners that have exhibited significant performance in well known experimental domains are as follow: Selecting the sentences unparsable by the current model (and if the batch does not get filled, using a secondary method) (Thompson et al., 1999); selecting the sentences with the highest tree entropy, i.e. the Shannon entropy of parses the probabilistic parser assigns to the sentence (Hwa, 2004); selecting the sentences having lowest best probabilities, where best probability is the conditional probability of the most probable parse, given the sentence and the current model (Osborne and Baldridge, 2004); primarily selecting the sentences that are expected to include events observed with low frequency so far with the help of bagging and filling the rest of the batch according to tree entropy, which is named as two-stage active learning by Becker and Osborne (2005). Proposed informativeness measures for multiple learners and ensemble learners can be found in (Baldridge and Osborne, 2003; Osborne and Baldridge, 2004; Becker and Osborne, 2005; Baldridge and Osborne, 2008). As for representativeness measures, Tang et. al. (2002) proposed using sample density, i.e. the inverse of the average dista</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldridge. 2004. Ensemblebased active learning for parse selection. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bengt Sigurd</author>
<author>Mats Eeg-Olofsson</author>
<author>Joost van Weijer</author>
</authors>
<title>Word length, sentence length and frequency -Zipf revisited.</title>
<date>2004</date>
<journal>StudiaLinguistica,</journal>
<volume>58</volume>
<issue>1</issue>
<marker>Sigurd, Eeg-Olofsson, van Weijer, 2004</marker>
<rawString>Bengt Sigurd, Mats Eeg-Olofsson, and Joost van Weijer. 2004. Word length, sentence length and frequency -Zipf revisited. StudiaLinguistica, 58(1):37–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1925" citStr="Tang et al., 2002" startWordPosition="285" endWordPosition="288">ion costs, one of which is active learning. Active learning setting allows the learner to select its own samples to be labeled and added to the training data iteratively. The motive behind active learning ∗Vast majority of this work was done while the author was a graduate student in Middle East Technical University, under the funding from T ¨UB˙ITAK-B˙IDEB through 2210 National Scholarship Programme for MSc Students. is that if the learner may select highly informative samples, it can eliminate the redundancy generally found in random data; however, informative samples can be very untypical (Tang et al., 2002). Unlike random sampling, active learning has no guarantee of selecting representative samples and untypical training samples are expected to degrade test performance of a classifier. To get around this problem, several methods of estimating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary E Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="4100" citStr="Thompson et al., 1999" startWordPosition="620" endWordPosition="623">other?). The last criterion applies only to the batch-mode setting, in which the training data is incremented by multiple samples at each step for practical purposes. Most of the active learning research in statistical parser training domain has focused on informativeness measures developed for both single and multilearner settings. The informativeness measures for single-learners that have exhibited significant performance in well known experimental domains are as follow: Selecting the sentences unparsable by the current model (and if the batch does not get filled, using a secondary method) (Thompson et al., 1999); selecting the sentences with the highest tree entropy, i.e. the Shannon entropy of parses the probabilistic parser assigns to the sentence (Hwa, 2004); selecting the sentences having lowest best probabilities, where best probability is the conditional probability of the most probable parse, given the sentence and the current model (Osborne and Baldridge, 2004); primarily selecting the sentences that are expected to include events observed with low frequency so far with the help of bagging and filling the rest of the batch according to tree entropy, which is named as two-stage active learning</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary E. Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>The Psychobiology of Language.</title>
<date>1935</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<note>Reprinted in</note>
<contexts>
<context position="5769" citStr="Zipf, 1935" startWordPosition="889" endWordPosition="890">and Osborne (2005) introduced length balanced sampling, in which the length histogram of the batch is kept equal to the length histogram of a random sample of batch size drawn from the pool. 3 Description Of The Work We introduce a novel representativeness measure for statistical parser training domain. Our measure is a function proposed in (Sigurd et al., 2004), which estimates the relative frequencies of sentence lengths in a natural language. Sigurd et. al. (2004) claimed that the longer a sentence is, the less likely it will be uttered; in accordance with Zipf’s Principle of Least Effort (Zipf, 1935). However, too short sentences will appear infrequently as well, since the number of different statements that may be expressed using relatively fewer words is relatively smaller. Authors conjectured that there is a clash of expressivity and effort over the frequency of sentence length, which effort eventually wins. They formulated this behavior with a Gamma distribution estimating the relative frequencies of sentence lengths. Authors conducted a parameter fit study for English using Brown corpus (Francis and Ku˘cera, 1967) and reported that the formula f(L) = 1.1 × Li × 0.90L, where L is the </context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George K. Zipf. 1935. The Psychobiology of Language. MIT Press, Cambridge, MA. Reprinted in 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principle ofLeast Effort.</title>
<date>1949</date>
<publisher>Addison-Wesley,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2379" citStr="Zipf, 1949" startWordPosition="357" endWordPosition="358">y informative samples, it can eliminate the redundancy generally found in random data; however, informative samples can be very untypical (Tang et al., 2002). Unlike random sampling, active learning has no guarantee of selecting representative samples and untypical training samples are expected to degrade test performance of a classifier. To get around this problem, several methods of estimating representativeness of a sample have been introduced. In this study, we propose a novel representativeness estimator for a sentence, which is based on a modification of Zipf’s Principle ofLeast Effort (Zipf, 1949), theoretically sound and empirically validated on Brown corpus (Francis and Ku˘cera, 1967). Experiments conducted with a wide coverage CCG parser (Clark and Curran, 2004; Clark and Curran, 2007) on CCGbank (Hockenmaier and Steedman, 2005) show that using our estimator as a representativeness metric never performs worse than and generally outperforms length balanced sampling (Becker and Osborne, 2005), which is another representativeness based active learning method, and pure informativeness based active learning. 2 Related Work In selective sampling setting, there are three criteria to be con</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George K. Zipf. 1949. Human Behavior and the Principle ofLeast Effort. Addison-Wesley, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>