<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000144">
<title confidence="0.997598">
Learning Dictionaries for Named Entity Recognition using Minimal
Supervision
</title>
<author confidence="0.990328">
Arvind Neelakantan
</author>
<affiliation confidence="0.9987055">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<address confidence="0.868007">
Amherst, MA, 01003
</address>
<email confidence="0.998952">
arvind@cs.umass.edu
</email>
<author confidence="0.993326">
Michael Collins
</author>
<affiliation confidence="0.836836333333333">
Department of Computer Science
Columbia University
New-York, NY 10027, USA
</affiliation>
<email confidence="0.998883">
mcollins@cs.columbia.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920176470588">
This paper describes an approach for au-
tomatic construction of dictionaries for
Named Entity Recognition (NER) using
large amounts of unlabeled data and a few
seed examples. We use Canonical Cor-
relation Analysis (CCA) to obtain lower
dimensional embeddings (representations)
for candidate phrases and classify these
phrases using a small number of labeled
examples. Our method achieves 16.5%
and 11.3% F-1 score improvement over
co-training on disease and virus NER re-
spectively. We also show that by adding
candidate phrase embeddings as features
in a sequence tagger gives better perfor-
mance compared to using word embed-
dings.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917942307693">
Several works (e.g., Ratinov and Roth, 2009; Co-
hen and Sarawagi, 2004) have shown that inject-
ing dictionary matches as features in a sequence
tagger results in significant gains in NER perfor-
mance. However, building these dictionaries re-
quires a huge amount of human effort and it is of-
ten difficult to get good coverage for many named
entity types. The problem is more severe when we
consider named entity types such as gene, virus
and disease, because of the large (and growing)
number of names in use, the fact that the names are
heavily abbreviated and multiple names are used
to refer to the same entity (Leaman et al., 2010;
Dogan and Lu, 2012). Also, these dictionaries can
only be built by domain experts, making the pro-
cess very expensive.
This paper describes an approach for automatic
construction of dictionaries for NER using large
amounts of unlabeled data and a small number
of seed examples. Our approach consists of two
steps. First, we collect a high recall, low preci-
sion list of candidate phrases from the large unla-
beled data collection for every named entity type
using simple rules. In the second step, we con-
struct an accurate dictionary of named entities by
removing the noisy candidates from the list ob-
tained in the first step. This is done by learning a
classifier using the lower dimensional, real-valued
CCA (Hotelling, 1935) embeddings of the can-
didate phrases as features and training it using a
small number of labeled examples. The classifier
we use is a binary SVM which predicts whether a
candidate phrase is a named entity or not.
We compare our method to a widely used semi-
supervised algorithm based on co-training (Blum
and Mitchell, 1998). The dictionaries are first
evaluated on virus (GENIA, 2003) and disease
(Dogan and Lu, 2012) NER by using them directly
in dictionary based taggers. We also give results
comparing the dictionaries produced by the two
semi-supervised approaches with dictionaries that
are compiled manually. The effectiveness of the
dictionaries are also measured by injecting dictio-
nary matches as features in a Conditional Random
Field (CRF) based tagger. The results indicate
that our approach with minimal supervision pro-
duces dictionaries that are comparable to dictio-
naries compiled manually. Finally, we also com-
pare the quality of the candidate phrase embed-
dings with word embeddings (Dhillon et al., 2011)
by adding them as features in a CRF based se-
quence tagger.
</bodyText>
<sectionHeader confidence="0.982764" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9882575">
We first give background on Canonical Correla-
tion Analysis (CCA), and then give background on
</bodyText>
<page confidence="0.974282">
452
</page>
<note confidence="0.96186325">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452–461,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
CRFs for the NER problem. italized. Prefix and suffixes of the word wi
were also added.
</note>
<subsectionHeader confidence="0.885786">
2.1 Canonical Correlation Analysis (CCA)
</subsectionHeader>
<bodyText confidence="0.999296666666667">
The input to CCA consists of n paired observa-
tions (x1, z1), ... , (xn, zn) where xi ∈ Rd1, zi ∈
Rd2 (∀i ∈ {1, 2, ... , n}) are the feature represen-
tations for the two views of a data point. CCA
simultaneously learns projection matrices Φ1 ∈
Rd1×k, Φ2 ∈ Rd2×k (k is a small number) which
are used to obtain the lower dimensional represen-
tations (¯x1, ¯z1), ... , (¯xn, ¯zn) where ¯xi = ΦT1 xi ∈
Rk, ¯zi = ΦT2 zi ∈ Rk, ∀i ∈ {1, 2, ... , n}. Φ1,Φ2
are chosen to maximize the correlation between ¯xi
and ¯zi, ∀i ∈ {1, 2, ... , n}.
Consider the setting where we have a label for
the data point along with it’s two views and ei-
ther view is sufficient to make accurate predic-
tions. Kakade and Foster (2007) and Sridharan
and Kakade (2008) give strong theoretical guaran-
tees when the lower dimensional embeddings from
CCA are used for predicting the label of the data
point. This setting is similar to the one considered
in co-training (Collins and Singer, 1999) but there
is no assumption of independence between the two
views of the data point. Also, it is an exact al-
gorithm unlike the algorithm given in Collins and
Singer (1999). Since we are using lower dimen-
sional embeddings of the data point for prediction,
we can learn a predictor with fewer labeled exam-
ples.
</bodyText>
<subsectionHeader confidence="0.978954">
2.2 CRFs for Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.9988908125">
CRF based sequence taggers have been used for
a number of NER tasks (e.g., McCallum and Li,
2003) and in particular for biomedical NER (e.g.,
McDonald and Pereira, 2005; Burr Settles, 2004)
because they allow a great deal of flexibility in the
features which can be included. The input to a
CRF tagger is a sentence (w1, w2,... , wn) where
wi, ∀i ∈ {1, 2, ... , n} are words in the sentence.
The output is a sequence of tags y1, y2,. .. , yn
where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B
is the tag given to the first word in a named entity,
T is the tag given to all words except the first word
in a named entity and O is the tag given to all other
words. We used the standard NER baseline fea-
tures (e.g., Dhillon et al., 2011; Ratinov and Roth,
2009) which include:
</bodyText>
<listItem confidence="0.998999875">
• Current Word wi and its lexical features
which include whether the word is capital-
ized and whether all the characters are cap-
• Word tokens in window of size two
around the current word which include
wi−2, wi−1, wi+1, wi+2 and also the capital-
ization pattern in the window.
• Previous two predictions yi−1 and yi−2.
</listItem>
<bodyText confidence="0.999615444444445">
The effectiveness of the dictionaries are evaluated
by adding dictionary matches as features along
with the baseline features (Ratinov and Roth,
2009; Cohen and Sarawagi, 2004) in the CRF tag-
ger. We also compared the quality of the candi-
date phrase embeddings with the word-level em-
beddings by adding them as features (Dhillon et
al., 2011) along with the baseline features in the
CRF tagger.
</bodyText>
<sectionHeader confidence="0.990947" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999826333333333">
This section describes the two steps in our ap-
proach: obtaining candidate phrases and classify-
ing them.
</bodyText>
<subsectionHeader confidence="0.999874">
3.1 Obtaining Candidate Phrases
</subsectionHeader>
<bodyText confidence="0.999877208333333">
We used the full text of 110,369 biomedical pub-
lications in the BioMed Central corpus1 to get the
high recall, low precision list of candidate phrases.
The advantages of using this huge collection of
publications are obvious: almost all (including
rare) named entities related to the biomedical do-
main will be mentioned and contains more re-
cent developments than a structured resource like
Wikipedia. The challenge however is that these
publications are unstructured and hence it is a dif-
ficult task to construct accurate dictionaries using
them with minimal supervision.
The list of virus candidate phrases were ob-
tained by extracting phrases that occur between
“the” and “virus” in the simple pattern “the ...
virus” during a single pass over the unlabeled doc-
ument collection. This noisy list had a lot of virus
names such as influenza, human immunodeficiency
and Epstein-Barr along with phrases that are not
virus names, like mutant, same, new, and so on.
A similar rule like “the ... disease” did not give
a good coverage of disease names since it is not
the common way of how diseases are mentioned
in publications. So we took a different approach
</bodyText>
<footnote confidence="0.9977735">
1The corpus can be downloaded at
http://www.biomedcentral.com/about/datamining
</footnote>
<page confidence="0.999227">
453
</page>
<bodyText confidence="0.999981285714286">
to obtain the noisy list of disease names. We col-
lected every sentence in the unlabeled data col-
lection that has the word “disease” in it and ex-
tracted noun phrases2 following the patterns “dis-
eases like ....”, “diseases such as ....” , “diseases in-
cluding ....” , “diagnosed with ....”, “patients with
....” and “suffering from ....”.
</bodyText>
<subsectionHeader confidence="0.999964">
3.2 Classification of Candidate Phrases
</subsectionHeader>
<bodyText confidence="0.999982666666667">
Having found the list of candidate phrases, we
now describe how noisy words are filtered out
from them. We gather (spelling, context) pairs for
every instance of a candidate phrase in the unla-
beled data collection. spelling refers to the can-
didate phrase itself while context includes three
words each to the left and the right of the candidate
phrase in the sentence. The spelling and the con-
text of the candidate phrase provide a natural split
into two views which multi-view algorithms like
co-training and CCA can exploit. The only super-
vision in our method is to provide a few spelling
seed examples (10 in the case of virus, 18 in the
case of disease), for example, human immunodefi-
ciency is a virus and mutant is not a virus.
</bodyText>
<subsectionHeader confidence="0.96757">
3.2.1 Approach using CCA embeddings
</subsectionHeader>
<bodyText confidence="0.999426791666667">
We use CCA described in the previous section
to obtain lower dimensional embeddings for the
candidate phrases using the (spelling, context)
views. Unlike previous works such as Dhillon et
al. (2011) and Dhillon et al. (2012), we use CCA to
learn embeddings for candidate phrases instead of
all words in the vocabulary so that we don’t miss
named entities which have two or more words.
Let the number of (spelling, context) pairs be n
(sum of total number of instances of every can-
didate phrase in the unlabeled data collection).
First, we map the spelling and context to high-
dimensional feature vectors. For the spelling view,
we define a feature for every candidate phrase and
also a boolean feature which indicates whether the
phrase is capitalized or not. For the context view,
we use features similar to Dhillon et al. (2011)
where a feature for every word in the context in
conjunction with its position is defined. Each
of the n (spelling, context) pairs are mapped to
a pair of high-dimensional feature vectors to get
n paired observations (x1, z1), ... , (xn, zn) with
xi E Rd1, zi E Rd2, Vi E 11, 2, ... , n} (d1, d2
are the feature space dimensions of the spelling
</bodyText>
<footnote confidence="0.650436">
2Noun phrases were obtained using
http://www.umiacs.umd.edu/ hal/TagChunk/
</footnote>
<bodyText confidence="0.996118210526316">
and context view respectively). Using CCA3, we
learn the projection matrices Φ1 E Rd1xk, Φ2 E
Rd2xk (k &lt;&lt; d1 and k &lt;&lt; d2 ) and obtain
spelling view projections ¯xi = ΦT 1 xi E Rk, Vi E
11, 2, ... , n}. The k-dimensional spelling view
projection of any instance of a candidate phrase
is used as it’s embedding4.
The k-dimensional candidate phrase embed-
dings are used as features to learn a binary SVM
with the seed spelling examples given in figure 1
as training data. The binary SVM predicts whether
a candidate phrase is a named entity or not. Since
the value of k is small, a small number of labeled
examples are sufficient to train an accurate clas-
sifier. The learned SVM is used to filter out the
noisy phrases from the list of candidate phrases
obtained in the previous step.
To summarize, our approach for classifying
candidate phrases has the following steps:
</bodyText>
<listItem confidence="0.959657">
• Input: n (spelling, context) pairs, spelling
seed examples.
• Each of the n (spelling, context) pairs are
mapped to a pair of high-dimensional fea-
ture vectors to get n paired observations
(x1, z1), ... , (xn, zn) with xi E Rd1, zi E
Rd2, Vi E 11, 2, ... , n}.
• Using CCA, we learn the projection matri-
ces Φ1 E Rd1xk,Φ2 E Rd2xk and ob-
tain spelling view projections ¯xi = ΦT1 xi E
Rk, Vi E 11, 2, ... , n}.
• The embedding of a candidate phrase is given
by the k-dimensional spelling view projec-
tion of any instance of the candidate phrase.
• We learn a binary SVM with the candi-
date phrase embeddings as features and the
spelling seed examples given in figure 1 as
training data. Using this SVM, we predict
whether a candidate phrase is a named entity
or not.
</listItem>
<subsectionHeader confidence="0.945428">
3.2.2 Approach based on Co-training
</subsectionHeader>
<bodyText confidence="0.7237244">
We discuss here briefly the DL-CoTrain algorithm
(Collins and Singer, 1999) which is based on co-
training (Blum and Mitchell, 1998), to classify
3Similar to Dhillon et al. (2012) we used the method given
in Halko et al. (2011) to perform the SVD computation in
CCA for practical considerations.
4Note that a candidate phrase gets the same spelling view
projection across it’s different instances since the spelling
features of a candidate phrase are identical across it’s in-
stances.
</bodyText>
<page confidence="0.993517">
454
</page>
<listItem confidence="0.9929465">
• Virus seed spelling examples
– Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B
– Non-virus Names: mutant, same, wild type, parental, recombinant
• Disease seed spelling examples
– Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, tumors, VHL
– Non-disease Names: cells, patients, study, data, expression, breast, BRCA1, protein, mutant
</listItem>
<figureCaption confidence="0.998477">
Figure 1: Seed spelling examples
</figureCaption>
<bodyText confidence="0.9999358">
candidate phrases. We compare our approach us-
ing CCA embeddings with this approach. Here,
two decision list of rules are learned simultane-
ously one using the spelling view and the other
using the context view. The rules using the
spelling view are of the form: full-string=human
immunodeficiency —* Virus, full-string=mutant —*
Not a virus and so on. In the context view, we
used bigram5 rules where we considered all pos-
sible bigrams using the context. The rules are of
two types: one which gives a positive label, for
example, full-string=human immunodeficiency —*
Virus and the other which gives a negative label,
for example, full-string=mutant —* Not a virus.
The DL-CoTrain algorithm is as follows:
</bodyText>
<listItem confidence="0.978416307692308">
• Input: (spelling, context) pairs for every in-
stance of a candidate phrase in the corpus, m
specifying the number of rules to be added in
every iteration, precision threshold c, spelling
seed examples.
• Algorithm:
1. Initialize the spelling decision list using
the spelling seed examples given in fig-
ure 1 and set i = 1.
2. Label the entire input collection using the
learned decision list of spelling rules.
3. Add i x m new context rules of each
type to the decision list of context rules
</listItem>
<bodyText confidence="0.876420125">
using the current labeled data. The
rules are added using the same criterion
as given in Collins and Singer (1999),
i.e., among the rules whose strength is
greater than the precision threshold c,
the ones which are seen more often with
the corresponding label in the input data
collection are added.
</bodyText>
<footnote confidence="0.581348333333333">
5We tried using unigram rules but they were very weak
predictors and the performance of the algorithm was poor
when they were considered.
</footnote>
<listItem confidence="0.890343">
4. Label the entire input collection using the
learned decision list of context rules.
5. Add i x m new spelling rules of each
type to the decision list of spelling rules
</listItem>
<bodyText confidence="0.994005866666667">
using the current labeled data. The rules
are added using the same criterion as in
step 3. Set i = i+1. If rules were added
in the previous iteration, return to step 2.
The algorithm is run until no new rules are left to
be added. The spelling decision list along with
its strength (Collins and Singer, 1999) is used to
construct the dictionaries. The phrases present in
the spelling rules which give a positive label and
whose strength is greater than the precision thresh-
old, were added to the dictionary of named enti-
ties. We found the parameters m and c difficult
to tune and they could significantly affect the per-
formance of the algorithm. We give more details
regarding this in the experiments section.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.998950277777778">
Previously, Collins and Singer (1999) introduced
a multi-view, semi-supervised algorithm based on
co-training (Blum and Mitchell, 1998) for collect-
ing names of people, organizations and locations.
This algorithm makes a strong independence as-
sumption about the data and employs many heuris-
tics to greedily optimize an objective function.
This greedy approach also introduces new param-
eters that are often difficult to tune.
In other works such as Toral and Mu˜noz (2006)
and Kazama and Torisawa (2007) external struc-
tured resources like Wikipedia have been used to
construct dictionaries. Even though these meth-
ods are fairly successful they suffer from a num-
ber of drawbacks especially in the biomedical do-
main. The main drawback of these approaches is
that it is very difficult to accurately disambiguate
ambiguous entities especially when the entities are
</bodyText>
<page confidence="0.996328">
455
</page>
<bodyText confidence="0.999861657894737">
abbreviations (Kazama and Torisawa, 2007). For
example, DM is the abbreviation for the disease
Diabetes Mellitus and the disambiguation page for
DM in Wikipedia associates it to more than 50 cat-
egories since DM can be expanded to Doctor of
Management, Dichroic mirror, and so on, each of
it belonging to a different category. Due to the
rapid growth of Wikipedia, the number of enti-
ties that have disambiguation pages is growing fast
and it is increasingly difficult to retrieve the article
we want. Also, it is tough to understand these ap-
proaches from a theoretical standpoint.
Dhillon et al. (2011) used CCA to learn word
embeddings and added them as features in a se-
quence tagger. They show that CCA learns bet-
ter word embeddings than CW embeddings (Col-
lobert and Weston, 2008), Hierarchical log-linear
(HLBL) embeddings (Mnih and Hinton, 2007)
and embeddings learned from many other tech-
niques for NER and chunking. Unlike PCA, a
widely used dimensionality reduction technique,
CCA is invariant to linear transformations of the
data. Our approach is motivated by the theoreti-
cal result in Kakade and Foster (2007) which is
developed in the co-training setting. We directly
use the CCA embeddings to predict the label of
a data point instead of using them as features in
a sequence tagger. Also, we learn CCA embed-
dings for candidate phrases instead of all words in
the vocabulary since named entities often contain
more than one word. Dhillon et al. (2012) learn
a multi-class SVM using the CCA word embed-
dings to predict the POS tag of a word type. We
extend this technique to NER by learning a binary
SVM using the CCA embeddings of a high recall,
low precision list of candidate phrases to predict
whether a candidate phrase is a named entity or
not.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999048">
In this section, we give experimental results on
virus and disease NER.
</bodyText>
<subsectionHeader confidence="0.924668">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999995060606061">
The noisy lists of both virus and disease names
were obtained from the BioMed Central corpus.
This corpus was also used to get the collection of
(spelling, context) pairs which are the input to the
CCA procedure and the DL-CoTrain algorithm de-
scribed in the previous section. We obtained CCA
embeddings for the 100, 000 most frequently oc-
curring word types in this collection along with
every word type present in the training and de-
velopment data of the virus and the disease NER
dataset. These word embeddings are similar to the
ones described in Dhillon et al. (2011) and Dhillon
et al. (2012).
We used the virus annotations in the GE-
NIA corpus (GENIA, 2003) for our experiments.
The dataset contains 18,546 annotated sentences.
We randomly selected 8,546 sentences for train-
ing and the remaining sentences were randomly
split equally into development and testing sen-
tences. The training sentences are used only for
experiments with the sequence taggers. Previ-
ously, Zhang et al. (2004) tested their HMM-based
named entity recognizer on this data. For disease
NER, we used the recent disease corpus (Dogan
and Lu, 2012) and used the same training, devel-
opment and test data split given by them. We used
a sentence segmenter6 to get sentence segmented
data and Stanford Tokenizer7 to tokenize the data.
Similar to Dogan and Lu (2012), all the different
disease categories were flattened into one single
category of disease mentions. The development
data was used to tune the hyperparameters and the
methods were evaluated on the test data.
</bodyText>
<subsectionHeader confidence="0.98079">
5.2 Results using a dictionary-based tagger
</subsectionHeader>
<bodyText confidence="0.9999968">
First, we compare the dictionaries compiled us-
ing different methods by using them directly in
a dictionary-based tagger. This is a simple and
informative way to understand the quality of the
dictionaries before using them in a CRF-tagger.
Since these taggers can be trained using a hand-
ful of training examples, we can use them to build
NER systems even when there are no labeled sen-
tences to train. The input to a dictionary tagger is
a list of named entities and a sentence. If there is
an exact match between a phrase in the input list
to the words in the given sentence then it is tagged
as a named entity. All other words are labeled as
non-entities. We evaluated the performance of the
following methods for building dictionaries:
</bodyText>
<listItem confidence="0.941535333333333">
• Candidate List: This dictionary contains all
the candidate phrases that were obtained us-
ing the method described in Section 3.1. The
noisy list of virus candidates and disease can-
didates had 3,100 and 60,080 entries respec-
tively.
</listItem>
<footnote confidence="0.999818">
6https://pypi.python.org/pypi/text-sentence/0.13
7http://nlp.stanford.edu/software/tokenizer.shtml
</footnote>
<page confidence="0.995818">
456
</page>
<table confidence="0.999752666666667">
Method Virus NER Disease NER
Precision Recall F-1 Score Precision Recall F-1 Score
Candidate List 2.20 69.58 4.27 4.86 60.32 8.99
Manual 42.69 68.75 52.67 51.39 45.08 48.03
Co-Training 48.33 66.46 55.96 58.87 23.17 33.26
CCA 57.24 68.33 62.30 38.34 44.55 41.21
</table>
<tableCaption confidence="0.999179">
Table 1: Precision, recall, F- 1 scores of dictionary-based taggers
</tableCaption>
<listItem confidence="0.979961">
• Manual: Manually constructed dictionaries,
</listItem>
<bodyText confidence="0.8512079375">
which requires a large amount of human ef-
fort, are employed for the task. We used the
list of virus names given in Wikipedia8. Un-
fortunately, abbreviations of virus names are
not present in this list and we could not find
any other more complete list of virus names.
Hence, we constructed abbreviations by con-
catenating the first letters of all the strings in
a virus name, for every virus name given in
the Wikipedia list.
For diseases, we used the list of disease
names given in the Unified Medical Lan-
guage System (UMLS) Metathesaurus. This
dictionary has been widely used in disease
NER (e.g., Dogan and Lu, 2012; Leaman et
al., 2010)9.
</bodyText>
<listItem confidence="0.687409666666667">
• Co-Training: The dictionaries are con-
structed using the DL-CoTrain algorithm de-
scribed previously. The parameters used
</listItem>
<bodyText confidence="0.992651388888889">
were m = 5 and c = 0.95 as given in Collins
and Singer (1999). The phrases present in
the spelling rules which give a positive label
and whose strength is greater than the preci-
sion threshold, were added to the dictionary
of named entities.
In our experiment to construct a dictionary
of virus names, the algorithm stopped after
just 12 iterations and hence the dictionary had
only 390 virus names. This was because there
were no spelling rules with strength greater
than 0.95 to be added. We tried varying
both the parameters but in all cases, the algo-
rithm did not progress after a few iterations.
We adopted a simple heuristic to increase the
coverage of virus names by using the strength
of the spelling rules obtained after the 12th it-
eration. All spelling rules that give a positive
</bodyText>
<footnote confidence="0.997312666666667">
8http://en.wikipedia.org/wiki/List of viruses
9The list of disease names from UMLS can be found at
https://sites.google.com/site/fmchowdhury2/bioenex .
</footnote>
<bodyText confidence="0.999206923076923">
label and which has a strength greater than
0 were added to the decision list of spelling
rules. The phrases present in these rules are
added to the dictionary. We picked the 0 pa-
rameter from the set [0.1, 0.2, 0.3, 0.4, 0.5,
0.6, 0.7, 0.8, 0.9] using the development data.
The co-training algorithm for constructing
the dictionary of disease names ran for close
to 50 iterations and hence we obtained bet-
ter coverage for disease names. We still used
the same heuristic of adding more named en-
tities using the strength of the rule since it
performed better.
</bodyText>
<listItem confidence="0.858923">
• CCA: Using the CCA embeddings of the
</listItem>
<bodyText confidence="0.982936636363636">
candidate phrases10 as features we learned a
binary SVM11 to predict whether a candidate
phrase is a named entity or not. We consid-
ered using 10 to 30 dimensions of candidate
phrase embeddings and the regularizer was
picked from the set [0.0001, 0.001, 0.01, 0.1,
1, 10, 100]. Both the regularizer and the num-
ber of dimensions to be used were tuned us-
ing the development data.
Table 1 gives the results of the dictionary based
taggers using the different methods described
above. As expected, when the noisy list of candi-
date phrases are used as dictionaries the recall of
the system is quite high but the precision is very
low. The low precision of the Wikipedia virus
lists was due to the heuristic used to obtain ab-
breviations which produced a few noisy abbrevia-
tions but this heuristic was crucial to get a high re-
call. The list of disease names from UMLS gives
a low recall because the list does not contain many
disease abbreviations and composite disease men-
tions such as breast and ovarian cancer. The pres-
</bodyText>
<footnote confidence="0.9982358">
10The performance of the dictionaries learned from word
embeddings was very poor and we do not report it’s perfor-
mance here.
11we used LIBSVM (http://www.csie.ntu.edu.tw/ cjlin/libsvm/)
in our SVM experiments
</footnote>
<page confidence="0.996682">
457
</page>
<figure confidence="0.9997573">
Disease NER
F−1 Score
0.75
0.65
0.55
0.450
0.8
0.7
0.6
0.5
1000 2000 3000 4000 5000 6000
baseline
manual
co−training
cca
Virus NER
F−1 Score 0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
baseline
manual
co−training
cca
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
Number of Training Sentences Number of Training Sentences
</figure>
<figureCaption confidence="0.9951135">
Figure 2: Virus and Disease NER F-1 scores for varying training data size when dictionaries obtained
from different methods are injected
</figureCaption>
<bodyText confidence="0.996614958333333">
ence of ambiguous abbreviations affected the ac-
curacy of this dictionary.
The virus dictionary constructed using the CCA
embeddings was very accurate and the false pos-
itives were mainly due to ambiguous phrases,
for example, in the phrase HIV replication, HIV
which usually refers to the name of a virus is
tagged as a RNA molecule. The accuracy of the
disease dictionary produced using CCA embed-
dings was mainly affected by noisy abbreviations.
We can see that the dictionaries obtained us-
ing CCA embeddings perform better than the dic-
tionaries obtained from co-training on both dis-
ease and virus NER even after improving the co-
training algorithm’s coverage using the heuristic
described in this section. It is important to note
that the dictionaries constructed using the CCA
embeddings and a small number of labeled exam-
ples performs competitively with dictionaries that
are entirely built by domain experts. These re-
sults show that by using the CCA based approach
we can build NER systems that give reasonable
performance even for difficult named entity types
with almost no supervision.
</bodyText>
<subsectionHeader confidence="0.986242">
5.3 Results using a CRF tagger
</subsectionHeader>
<bodyText confidence="0.9999916">
We did two sets of experiments using a CRF tag-
ger. In the first experiment, we add dictionary fea-
tures to the CRF tagger while in the second ex-
periment we add the embeddings as features to the
CRF tagger. The same baseline model is used in
both the experiments whose features are described
in Section 2.2. For both the CRF12 experiments
the regularizers from the set [0.0001, 0.001, 0.01,
0.1, 1.0, 10.0] were considered and it was tuned
on the development set.
</bodyText>
<subsectionHeader confidence="0.619816">
5.3.1 Dictionary Features
</subsectionHeader>
<bodyText confidence="0.99979416">
Here, we inject dictionary matches as features
(e.g., Ratinov and Roth, 2009; Cohen and
Sarawagi, 2004) in the CRF tagger. Given a dic-
tionary of named entities, every word in the input
sentence has a dictionary feature associated with
it. When there is an exact match between a phrase
in the dictionary with the words in the input sen-
tence, the dictionary feature of the first word in
the named entity is set to B and the dictionary fea-
ture of the remaining words in the named entity
is set to I. The dictionary feature of all the other
words in the input sentence which are not part of
any named entity in the dictionary is set to O. The
effectiveness of the dictionaries constructed from
various methods are compared by adding dictio-
nary match features to the CRF tagger. These dic-
tionary match features were added along with the
baseline features.
Figure 2 indicates that the dictionary features in
general are helpful to the CRF model. We can see
that the dictionaries produced from our approach
using CCA are much more helpful than the dictio-
naries produced from co-training especially when
there are fewer labeled sentences to train. Simi-
lar to the dictionary tagger experiments discussed
</bodyText>
<footnote confidence="0.497225">
12We used CRFsuite (www.chokkan.org/software/crfsuite/)
for our experiments with CRFs.
</footnote>
<page confidence="0.992071">
458
</page>
<figure confidence="0.99963075">
0.450
Virus NER
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
Number of Training Sentences
Disease NER
1000 2000 3000 4000 5000 6000
Number of Training Sentences
baseline
cca−mrd
cca−phrase
baseline
cca−word
cca−phrase
F−1 Score 0.8
0.75
0.7
0.65
0.6
0.55
0.5
F−1 Score 0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
</figure>
<figureCaption confidence="0.994856">
Figure 3: Virus and Disease NER F-1 scores for varying training data size when embeddings obtained
from different methods are used as features
</figureCaption>
<bodyText confidence="0.999784666666667">
previously, the dictionaries produced from our ap-
proach performs competitively with dictionaries
that are entirely built by domain experts.
</bodyText>
<subsectionHeader confidence="0.727803">
5.3.2 Embedding Features
</subsectionHeader>
<bodyText confidence="0.99998782">
The quality of the candidate phrase embeddings
are compared with word embeddings by adding
the embeddings as features in the CRF tagger.
Along with the baseline features, CCA-word
model adds word embeddings as features while the
CCA-phrase model adds candidate phrase em-
beddings as features. CCA-word model is similar
to the one used in Dhillon et al. (2011).
We considered adding 10, 20, 30, 40 and 50 di-
mensional word embeddings as features for every
training data size and the best performing model
on the development data was picked for the exper-
iments on the test data. For candidate phrase em-
beddings we used the same number of dimensions
that was used for training the SVMs to construct
the best dictionary.
When candidate phrase embeddings are ob-
tained using CCA, we do not have embeddings
for words which are not in the list of candidate
phrases. Also, a candidate phrase having more
than one word has a joint representation, i.e., the
phrase “human immunodeficiency” has a lower
dimensional representation while the words “hu-
man” and “immunodeficiency” do not have their
own lower dimensional representations (assuming
they are not part of the candidate list). To over-
come this issue, we used a simple technique to dif-
ferentiate between candidate phrases and the rest
of the words. Let x be the highest real valued can-
didate phrase embedding and the candidate phrase
embedding be a d dimensional real valued vector.
If a candidate phrase occurs in a sentence, the em-
beddings of that candidate phrase are added as fea-
tures to the first word of that candidate phrase. If
the candidate phrase has more than one word, the
other words in the candidate phrase are given an
embedding of dimension d with each dimension
having the value 2 × x. All the other words are
given an embedding of dimension d with each di-
mension having the value 4 × x.
Figure 3 shows that almost always the candi-
date phrase embeddings help the CRF model. It is
also interesting to note that sometimes the word-
level embeddings have an adverse affect on the
performance of the CRF model. The CCA-phrase
model performs significantly better than the other
two models when there are fewer labeled sen-
tences to train and the separation of the candidate
phrases from the other words seems to have helped
the CRF model.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994777777778">
We described an approach for automatic construc-
tion of dictionaries for NER using minimal super-
vision. Compared to the previous approaches, our
method is free from overly-stringent assumptions
about the data, uses SVD that can be solved ex-
actly and achieves better empirical performance.
Our approach which uses a small number of seed
examples performs competitively with dictionar-
ies that are compiled manually.
</bodyText>
<page confidence="0.998884">
459
</page>
<sectionHeader confidence="0.998819" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999913">
We are grateful to Alexander Rush, Alexandre
Passos and the anonymous reviewers for their
useful feedback. This work was supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior Na-
tional Business Center (DoI/NBC) contract num-
ber D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719069767442">
Andrew McCallum and Wei Li. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. 2003. Conference on Natural Language
Learning (CoNLL).
Andriy Mnih and Geoffrey Hinton. Three New Graph-
ical Models for Statistical Language Modelling.
2007. International Conference on Machine learn-
ing (ICML).
Antonio Toral and Rafael Mu˜noz. A proposal to auto-
matically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. 2006.
Workshop On New Text Wikis And Blogs And
Other Dynamic Text Sources.
Avrin Blum and Tom M. Mitchell. Combining Labeled
and Unlabeled Data with Co-Training. 1998. Con-
ference on Learning Theory (COLT).
Burr Settles. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. 2004. International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
H. Hotelling. Canonical correlation analysis (cca)
1935. Journal of Educational Psychology.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su and
Chew-Lim Tan. Enhancing HMM-based Biomed-
ical Named Entity Recognition by Studying Special
Phenomena. 2004. Journal of Biomedical Informat-
ics.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi and
Jun’ichi Tsujii. GENIA corpus - a semantically an-
notated corpus for bio-textmining. 2003. ISMB.
Junichi Kazama and Kentaro Torisawa. Exploiting
Wikipedia as External Knowledge for Named Entity
Recognition. 2007. Association for Computational
Linguistics (ACL).
Karthik Sridharan and Sham M. Kakade. An Informa-
tion Theoretic Framework for Multi-view Learning.
2008. Conference on Learning Theory (COLT).
Lev Ratinov and Dan Roth. Design Challenges
and Misconceptions in Named Entity Recognition.
2009. Conference on Natural Language Learning
(CoNLL).
Michael Collins and Yoram Singer. Unsupervised
Models for Named Entity Classification. 1999. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Nathan Halko, Per-Gunnar Martinsson, Joel A. Tropp.
Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix de-
compositions. 2011. Society for Industrial and Ap-
plied Mathematics.
Paramveer S. Dhillon, Dean Foster and Lyle Ungar.
Multi-View Learning of Word Embeddings via CCA.
2011. Advances in Neural Information Processing
Systems (NIPS).
Paramveer Dhillon, Jordan Rodu, Dean Foster and Lyle
Ungar. Two Step CCA: A new spectral method for
estimating vector models of words. 2012. Interna-
tional Conference on Machine learning (ICML).
Rezarta Islamaj Dogan and Zhiyong Lu. An improved
corpus of disease mentions in PubMed citations.
2012. Workshop on Biomedical Natural Language
Processing, Association for Computational Linguis-
tics (ACL).
Robert Leaman, Christopher Miller and Graciela Gon-
zalez. Enabling Recognition of Diseases in Biomed-
ical Text with Machine Learning: Corpus and
Benchmark. 2010. Workshop on Biomedical Nat-
ural Language Processing, Association for Compu-
tational Linguistics (ACL).
Ronan Collobert and Jason Weston. A unified architec-
ture for natural language processing: deep neural
networks with multitask learning. 2008. Interna-
tional Conference on Machine learning (ICML).
Ryan McDonald and Fernando Pereira. Identifying
Gene and Protein Mentions in Text Using Condi-
tional Random Fields. 2005. BMC Bioinformatics.
Sham M. Kakade and Dean P. Foster. Multi-view re-
gression via canonical correlation analysis. 2007.
Conference on Learning Theory (COLT).
William W. Cohen and Sunita Sarawagi. Exploiting
Dictionaries in Named Entity Extraction: Combin-
ing Semi-Markov Extraction Processes and Data In-
tegration Methods. 2004. Semi-Markov Extraction
</reference>
<page confidence="0.989456">
460
</page>
<reference confidence="0.9724325">
Processes and Data Integration Methods, Proceed-
ings of KDD.
</reference>
<page confidence="0.99942">
461
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.380615">
<title confidence="0.9976685">Learning Dictionaries for Named Entity Recognition using Minimal Supervision</title>
<author confidence="0.913799">Arvind</author>
<affiliation confidence="0.999896">Department of Computer University of Massachusetts,</affiliation>
<address confidence="0.987318">Amherst, MA,</address>
<email confidence="0.999643">arvind@cs.umass.edu</email>
<author confidence="0.986288">Michael</author>
<affiliation confidence="0.790529">Department of Computer Columbia</affiliation>
<address confidence="0.884041">New-York, NY 10027,</address>
<email confidence="0.99966">mcollins@cs.columbia.edu</email>
<abstract confidence="0.993176444444444">This paper describes an approach for automatic construction of dictionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) phrases classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons.</title>
<date>2003</date>
<booktitle>Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="5258" citStr="McCallum and Li, 2003" startWordPosition="869" endWordPosition="872">lower dimensional embeddings from CCA are used for predicting the label of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1, w2,... , wn) where wi, ∀i ∈ {1, 2, ... , n} are words in the sentence. The output is a sequence of tags y1, y2,. .. , yn where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B is the tag given to the first word in a named entity, T is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline featur</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons. 2003. Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three New Graphical Models for Statistical Language Modelling.</title>
<date>2007</date>
<booktitle>International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="17239" citStr="Mnih and Hinton, 2007" startWordPosition="2929" endWordPosition="2932"> to Doctor of Management, Dichroic mirror, and so on, each of it belonging to a different category. Due to the rapid growth of Wikipedia, the number of entities that have disambiguation pages is growing fast and it is increasingly difficult to retrieve the article we want. Also, it is tough to understand these approaches from a theoretical standpoint. Dhillon et al. (2011) used CCA to learn word embeddings and added them as features in a sequence tagger. They show that CCA learns better word embeddings than CW embeddings (Collobert and Weston, 2008), Hierarchical log-linear (HLBL) embeddings (Mnih and Hinton, 2007) and embeddings learned from many other techniques for NER and chunking. Unlike PCA, a widely used dimensionality reduction technique, CCA is invariant to linear transformations of the data. Our approach is motivated by the theoretical result in Kakade and Foster (2007) which is developed in the co-training setting. We directly use the CCA embeddings to predict the label of a data point instead of using them as features in a sequence tagger. Also, we learn CCA embeddings for candidate phrases instead of all words in the vocabulary since named entities often contain more than one word. Dhillon </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. Three New Graphical Models for Statistical Language Modelling. 2007. International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Rafael Mu˜noz</author>
</authors>
<title>A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia.</title>
<date>2006</date>
<marker>Toral, Mu˜noz, 2006</marker>
<rawString>Antonio Toral and Rafael Mu˜noz. A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia. 2006. Workshop On New Text Wikis And Blogs And Other Dynamic Text Sources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrin Blum</author>
<author>Tom M Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="2650" citStr="Blum and Mitchell, 1998" startWordPosition="420" endWordPosition="423">ion for every named entity type using simple rules. In the second step, we construct an accurate dictionary of named entities by removing the noisy candidates from the list obtained in the first step. This is done by learning a classifier using the lower dimensional, real-valued CCA (Hotelling, 1935) embeddings of the candidate phrases as features and training it using a small number of labeled examples. The classifier we use is a binary SVM which predicts whether a candidate phrase is a named entity or not. We compare our method to a widely used semisupervised algorithm based on co-training (Blum and Mitchell, 1998). The dictionaries are first evaluated on virus (GENIA, 2003) and disease (Dogan and Lu, 2012) NER by using them directly in dictionary based taggers. We also give results comparing the dictionaries produced by the two semi-supervised approaches with dictionaries that are compiled manually. The effectiveness of the dictionaries are also measured by injecting dictionary matches as features in a Conditional Random Field (CRF) based tagger. The results indicate that our approach with minimal supervision produces dictionaries that are comparable to dictionaries compiled manually. Finally, we also </context>
<context position="12211" citStr="Blum and Mitchell, 1998" startWordPosition="2098" endWordPosition="2101">ces Φ1 E Rd1xk,Φ2 E Rd2xk and obtain spelling view projections ¯xi = ΦT1 xi E Rk, Vi E 11, 2, ... , n}. • The embedding of a candidate phrase is given by the k-dimensional spelling view projection of any instance of the candidate phrase. • We learn a binary SVM with the candidate phrase embeddings as features and the spelling seed examples given in figure 1 as training data. Using this SVM, we predict whether a candidate phrase is a named entity or not. 3.2.2 Approach based on Co-training We discuss here briefly the DL-CoTrain algorithm (Collins and Singer, 1999) which is based on cotraining (Blum and Mitchell, 1998), to classify 3Similar to Dhillon et al. (2012) we used the method given in Halko et al. (2011) to perform the SVD computation in CCA for practical considerations. 4Note that a candidate phrase gets the same spelling view projection across it’s different instances since the spelling features of a candidate phrase are identical across it’s instances. 454 • Virus seed spelling examples – Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B – Non-virus Names: mutant, same, wild type, parental, recombinant • Disease seed spelling examples – Disease Names: tumor, m</context>
<context position="15663" citStr="Blum and Mitchell, 1998" startWordPosition="2671" endWordPosition="2674">pelling decision list along with its strength (Collins and Singer, 1999) is used to construct the dictionaries. The phrases present in the spelling rules which give a positive label and whose strength is greater than the precision threshold, were added to the dictionary of named entities. We found the parameters m and c difficult to tune and they could significantly affect the performance of the algorithm. We give more details regarding this in the experiments section. 4 Related Work Previously, Collins and Singer (1999) introduced a multi-view, semi-supervised algorithm based on co-training (Blum and Mitchell, 1998) for collecting names of people, organizations and locations. This algorithm makes a strong independence assumption about the data and employs many heuristics to greedily optimize an objective function. This greedy approach also introduces new parameters that are often difficult to tune. In other works such as Toral and Mu˜noz (2006) and Kazama and Torisawa (2007) external structured resources like Wikipedia have been used to construct dictionaries. Even though these methods are fairly successful they suffer from a number of drawbacks especially in the biomedical domain. The main drawback of t</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrin Blum and Tom M. Mitchell. Combining Labeled and Unlabeled Data with Co-Training. 1998. Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets.</title>
<date>2004</date>
<booktitle>International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA).</booktitle>
<contexts>
<context position="5350" citStr="Settles, 2004" startWordPosition="885" endWordPosition="886">ing is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1, w2,... , wn) where wi, ∀i ∈ {1, 2, ... , n} are words in the sentence. The output is a sequence of tags y1, y2,. .. , yn where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B is the tag given to the first word in a named entity, T is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: • Current Word wi and</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Burr Settles. Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets. 2004. International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hotelling</author>
</authors>
<title>Canonical correlation analysis (cca)</title>
<date>1935</date>
<journal>Journal of Educational Psychology.</journal>
<contexts>
<context position="2327" citStr="Hotelling, 1935" startWordPosition="366" endWordPosition="367">xpensive. This paper describes an approach for automatic construction of dictionaries for NER using large amounts of unlabeled data and a small number of seed examples. Our approach consists of two steps. First, we collect a high recall, low precision list of candidate phrases from the large unlabeled data collection for every named entity type using simple rules. In the second step, we construct an accurate dictionary of named entities by removing the noisy candidates from the list obtained in the first step. This is done by learning a classifier using the lower dimensional, real-valued CCA (Hotelling, 1935) embeddings of the candidate phrases as features and training it using a small number of labeled examples. The classifier we use is a binary SVM which predicts whether a candidate phrase is a named entity or not. We compare our method to a widely used semisupervised algorithm based on co-training (Blum and Mitchell, 1998). The dictionaries are first evaluated on virus (GENIA, 2003) and disease (Dogan and Lu, 2012) NER by using them directly in dictionary based taggers. We also give results comparing the dictionaries produced by the two semi-supervised approaches with dictionaries that are comp</context>
</contexts>
<marker>Hotelling, 1935</marker>
<rawString>H. Hotelling. Canonical correlation analysis (cca) 1935. Journal of Educational Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Zhang</author>
<author>Dan Shen</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Enhancing HMM-based Biomedical Named Entity Recognition by Studying Special Phenomena.</title>
<date>2004</date>
<journal>Journal of Biomedical Informatics.</journal>
<contexts>
<context position="19234" citStr="Zhang et al. (2004)" startWordPosition="3269" endWordPosition="3272">ction along with every word type present in the training and development data of the virus and the disease NER dataset. These word embeddings are similar to the ones described in Dhillon et al. (2011) and Dhillon et al. (2012). We used the virus annotations in the GENIA corpus (GENIA, 2003) for our experiments. The dataset contains 18,546 annotated sentences. We randomly selected 8,546 sentences for training and the remaining sentences were randomly split equally into development and testing sentences. The training sentences are used only for experiments with the sequence taggers. Previously, Zhang et al. (2004) tested their HMM-based named entity recognizer on this data. For disease NER, we used the recent disease corpus (Dogan and Lu, 2012) and used the same training, development and test data split given by them. We used a sentence segmenter6 to get sentence segmented data and Stanford Tokenizer7 to tokenize the data. Similar to Dogan and Lu (2012), all the different disease categories were flattened into one single category of disease mentions. The development data was used to tune the hyperparameters and the methods were evaluated on the test data. 5.2 Results using a dictionary-based tagger Fir</context>
</contexts>
<marker>Zhang, Shen, Zhou, Su, Tan, 2004</marker>
<rawString>Jie Zhang, Dan Shen, Guodong Zhou, Jian Su and Chew-Lim Tan. Enhancing HMM-based Biomedical Named Entity Recognition by Studying Special Phenomena. 2004. Journal of Biomedical Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
</authors>
<title>Tomoko Ohta, Yuka Tateisi and Jun’ichi Tsujii. GENIA corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<publisher>ISMB.</publisher>
<marker>Kim, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi and Jun’ichi Tsujii. GENIA corpus - a semantically annotated corpus for bio-textmining. 2003. ISMB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as External Knowledge for Named Entity Recognition.</title>
<date>2007</date>
<contexts>
<context position="16029" citStr="Kazama and Torisawa (2007)" startWordPosition="2730" endWordPosition="2733">ntly affect the performance of the algorithm. We give more details regarding this in the experiments section. 4 Related Work Previously, Collins and Singer (1999) introduced a multi-view, semi-supervised algorithm based on co-training (Blum and Mitchell, 1998) for collecting names of people, organizations and locations. This algorithm makes a strong independence assumption about the data and employs many heuristics to greedily optimize an objective function. This greedy approach also introduces new parameters that are often difficult to tune. In other works such as Toral and Mu˜noz (2006) and Kazama and Torisawa (2007) external structured resources like Wikipedia have been used to construct dictionaries. Even though these methods are fairly successful they suffer from a number of drawbacks especially in the biomedical domain. The main drawback of these approaches is that it is very difficult to accurately disambiguate ambiguous entities especially when the entities are 455 abbreviations (Kazama and Torisawa, 2007). For example, DM is the abbreviation for the disease Diabetes Mellitus and the disambiguation page for DM in Wikipedia associates it to more than 50 categories since DM can be expanded to Doctor o</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Junichi Kazama and Kentaro Torisawa. Exploiting Wikipedia as External Knowledge for Named Entity Recognition. 2007. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Sridharan</author>
<author>Sham M Kakade</author>
</authors>
<title>An Information Theoretic Framework for Multi-view Learning.</title>
<date>2008</date>
<booktitle>Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="4591" citStr="Sridharan and Kakade (2008)" startWordPosition="754" endWordPosition="757">∈ {1, 2, ... , n}) are the feature representations for the two views of a data point. CCA simultaneously learns projection matrices Φ1 ∈ Rd1×k, Φ2 ∈ Rd2×k (k is a small number) which are used to obtain the lower dimensional representations (¯x1, ¯z1), ... , (¯xn, ¯zn) where ¯xi = ΦT1 xi ∈ Rk, ¯zi = ΦT2 zi ∈ Rk, ∀i ∈ {1, 2, ... , n}. Φ1,Φ2 are chosen to maximize the correlation between ¯xi and ¯zi, ∀i ∈ {1, 2, ... , n}. Consider the setting where we have a label for the data point along with it’s two views and either view is sufficient to make accurate predictions. Kakade and Foster (2007) and Sridharan and Kakade (2008) give strong theoretical guarantees when the lower dimensional embeddings from CCA are used for predicting the label of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognition CRF based sequence taggers hav</context>
</contexts>
<marker>Sridharan, Kakade, 2008</marker>
<rawString>Karthik Sridharan and Sham M. Kakade. An Information Theoretic Framework for Multi-view Learning. 2008. Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design Challenges and Misconceptions in Named Entity Recognition.</title>
<date>2009</date>
<booktitle>Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="1015" citStr="Ratinov and Roth, 2009" startWordPosition="139" endWordPosition="142">ctionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings. 1 Introduction Several works (e.g., Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) have shown that injecting dictionary matches as features in a sequence tagger results in significant gains in NER performance. However, building these dictionaries requires a huge amount of human effort and it is often difficult to get good coverage for many named entity types. The problem is more severe when we consider named entity types such as gene, virus and disease, because of the large (and growing) number of names in use, the fact that the names are heavily abbreviated and multiple names are used to refer to the same entity (Leaman et al., 2010; Dogan and Lu</context>
<context position="5913" citStr="Ratinov and Roth, 2009" startWordPosition="1006" endWordPosition="1009">l NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1, w2,... , wn) where wi, ∀i ∈ {1, 2, ... , n} are words in the sentence. The output is a sequence of tags y1, y2,. .. , yn where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B is the tag given to the first word in a named entity, T is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: • Current Word wi and its lexical features which include whether the word is capitalized and whether all the characters are cap• Word tokens in window of size two around the current word which include wi−2, wi−1, wi+1, wi+2 and also the capitalization pattern in the window. • Previous two predictions yi−1 and yi−2. The effectiveness of the dictionaries are evaluated by adding dictionary matches as features along with the baseline features (Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. We also compared the quality of the candidate phrase embeddings with th</context>
<context position="26942" citStr="Ratinov and Roth, 2009" startWordPosition="4556" endWordPosition="4559">ith almost no supervision. 5.3 Results using a CRF tagger We did two sets of experiments using a CRF tagger. In the first experiment, we add dictionary features to the CRF tagger while in the second experiment we add the embeddings as features to the CRF tagger. The same baseline model is used in both the experiments whose features are described in Section 2.2. For both the CRF12 experiments the regularizers from the set [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0] were considered and it was tuned on the development set. 5.3.1 Dictionary Features Here, we inject dictionary matches as features (e.g., Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. Given a dictionary of named entities, every word in the input sentence has a dictionary feature associated with it. When there is an exact match between a phrase in the dictionary with the words in the input sentence, the dictionary feature of the first word in the named entity is set to B and the dictionary feature of the remaining words in the named entity is set to I. The dictionary feature of all the other words in the input sentence which are not part of any named entity in the dictionary is set to O. The effectiveness of the dictionaries con</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. Design Challenges and Misconceptions in Named Entity Recognition. 2009. Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="4814" citStr="Collins and Singer, 1999" startWordPosition="790" endWordPosition="793">epresentations (¯x1, ¯z1), ... , (¯xn, ¯zn) where ¯xi = ΦT1 xi ∈ Rk, ¯zi = ΦT2 zi ∈ Rk, ∀i ∈ {1, 2, ... , n}. Φ1,Φ2 are chosen to maximize the correlation between ¯xi and ¯zi, ∀i ∈ {1, 2, ... , n}. Consider the setting where we have a label for the data point along with it’s two views and either view is sufficient to make accurate predictions. Kakade and Foster (2007) and Sridharan and Kakade (2008) give strong theoretical guarantees when the lower dimensional embeddings from CCA are used for predicting the label of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features </context>
<context position="12156" citStr="Collins and Singer, 1999" startWordPosition="2088" endWordPosition="2091"> 2, ... , n}. • Using CCA, we learn the projection matrices Φ1 E Rd1xk,Φ2 E Rd2xk and obtain spelling view projections ¯xi = ΦT1 xi E Rk, Vi E 11, 2, ... , n}. • The embedding of a candidate phrase is given by the k-dimensional spelling view projection of any instance of the candidate phrase. • We learn a binary SVM with the candidate phrase embeddings as features and the spelling seed examples given in figure 1 as training data. Using this SVM, we predict whether a candidate phrase is a named entity or not. 3.2.2 Approach based on Co-training We discuss here briefly the DL-CoTrain algorithm (Collins and Singer, 1999) which is based on cotraining (Blum and Mitchell, 1998), to classify 3Similar to Dhillon et al. (2012) we used the method given in Halko et al. (2011) to perform the SVD computation in CCA for practical considerations. 4Note that a candidate phrase gets the same spelling view projection across it’s different instances since the spelling features of a candidate phrase are identical across it’s instances. 454 • Virus seed spelling examples – Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B – Non-virus Names: mutant, same, wild type, parental, recombinant • D</context>
<context position="14308" citStr="Collins and Singer (1999)" startWordPosition="2439" endWordPosition="2442">hm is as follows: • Input: (spelling, context) pairs for every instance of a candidate phrase in the corpus, m specifying the number of rules to be added in every iteration, precision threshold c, spelling seed examples. • Algorithm: 1. Initialize the spelling decision list using the spelling seed examples given in figure 1 and set i = 1. 2. Label the entire input collection using the learned decision list of spelling rules. 3. Add i x m new context rules of each type to the decision list of context rules using the current labeled data. The rules are added using the same criterion as given in Collins and Singer (1999), i.e., among the rules whose strength is greater than the precision threshold c, the ones which are seen more often with the corresponding label in the input data collection are added. 5We tried using unigram rules but they were very weak predictors and the performance of the algorithm was poor when they were considered. 4. Label the entire input collection using the learned decision list of context rules. 5. Add i x m new spelling rules of each type to the decision list of spelling rules using the current labeled data. The rules are added using the same criterion as in step 3. Set i = i+1. I</context>
<context position="15565" citStr="Collins and Singer (1999)" startWordPosition="2659" endWordPosition="2662">us iteration, return to step 2. The algorithm is run until no new rules are left to be added. The spelling decision list along with its strength (Collins and Singer, 1999) is used to construct the dictionaries. The phrases present in the spelling rules which give a positive label and whose strength is greater than the precision threshold, were added to the dictionary of named entities. We found the parameters m and c difficult to tune and they could significantly affect the performance of the algorithm. We give more details regarding this in the experiments section. 4 Related Work Previously, Collins and Singer (1999) introduced a multi-view, semi-supervised algorithm based on co-training (Blum and Mitchell, 1998) for collecting names of people, organizations and locations. This algorithm makes a strong independence assumption about the data and employs many heuristics to greedily optimize an objective function. This greedy approach also introduces new parameters that are often difficult to tune. In other works such as Toral and Mu˜noz (2006) and Kazama and Torisawa (2007) external structured resources like Wikipedia have been used to construct dictionaries. Even though these methods are fairly successful </context>
<context position="22100" citStr="Collins and Singer (1999)" startWordPosition="3739" endWordPosition="3742">t and we could not find any other more complete list of virus names. Hence, we constructed abbreviations by concatenating the first letters of all the strings in a virus name, for every virus name given in the Wikipedia list. For diseases, we used the list of disease names given in the Unified Medical Language System (UMLS) Metathesaurus. This dictionary has been widely used in disease NER (e.g., Dogan and Lu, 2012; Leaman et al., 2010)9. • Co-Training: The dictionaries are constructed using the DL-CoTrain algorithm described previously. The parameters used were m = 5 and c = 0.95 as given in Collins and Singer (1999). The phrases present in the spelling rules which give a positive label and whose strength is greater than the precision threshold, were added to the dictionary of named entities. In our experiment to construct a dictionary of virus names, the algorithm stopped after just 12 iterations and hence the dictionary had only 390 virus names. This was because there were no spelling rules with strength greater than 0.95 to be added. We tried varying both the parameters but in all cases, the algorithm did not progress after a few iterations. We adopted a simple heuristic to increase the coverage of vir</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. Unsupervised Models for Named Entity Classification. 1999. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Halko</author>
<author>Per-Gunnar Martinsson</author>
<author>Joel A Tropp</author>
</authors>
<title>Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.</title>
<date>2011</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<contexts>
<context position="12306" citStr="Halko et al. (2011)" startWordPosition="2116" endWordPosition="2119">n}. • The embedding of a candidate phrase is given by the k-dimensional spelling view projection of any instance of the candidate phrase. • We learn a binary SVM with the candidate phrase embeddings as features and the spelling seed examples given in figure 1 as training data. Using this SVM, we predict whether a candidate phrase is a named entity or not. 3.2.2 Approach based on Co-training We discuss here briefly the DL-CoTrain algorithm (Collins and Singer, 1999) which is based on cotraining (Blum and Mitchell, 1998), to classify 3Similar to Dhillon et al. (2012) we used the method given in Halko et al. (2011) to perform the SVD computation in CCA for practical considerations. 4Note that a candidate phrase gets the same spelling view projection across it’s different instances since the spelling features of a candidate phrase are identical across it’s instances. 454 • Virus seed spelling examples – Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B – Non-virus Names: mutant, same, wild type, parental, recombinant • Disease seed spelling examples – Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, tumors, VHL – Non-disease Names: cells, patients,</context>
</contexts>
<marker>Halko, Martinsson, Tropp, 2011</marker>
<rawString>Nathan Halko, Per-Gunnar Martinsson, Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. 2011. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Multi-View Learning of Word Embeddings via CCA.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="3348" citStr="Dhillon et al., 2011" startWordPosition="526" endWordPosition="529">gan and Lu, 2012) NER by using them directly in dictionary based taggers. We also give results comparing the dictionaries produced by the two semi-supervised approaches with dictionaries that are compiled manually. The effectiveness of the dictionaries are also measured by injecting dictionary matches as features in a Conditional Random Field (CRF) based tagger. The results indicate that our approach with minimal supervision produces dictionaries that are comparable to dictionaries compiled manually. Finally, we also compare the quality of the candidate phrase embeddings with word embeddings (Dhillon et al., 2011) by adding them as features in a CRF based sequence tagger. 2 Background We first give background on Canonical Correlation Analysis (CCA), and then give background on 452 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452–461, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics CRFs for the NER problem. italized. Prefix and suffixes of the word wi were also added. 2.1 Canonical Correlation Analysis (CCA) The input to CCA consists of n paired observations (x1, z1), ... , (xn, zn) where xi ∈ Rd</context>
<context position="5888" citStr="Dhillon et al., 2011" startWordPosition="1002" endWordPosition="1005">rticular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1, w2,... , wn) where wi, ∀i ∈ {1, 2, ... , n} are words in the sentence. The output is a sequence of tags y1, y2,. .. , yn where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B is the tag given to the first word in a named entity, T is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: • Current Word wi and its lexical features which include whether the word is capitalized and whether all the characters are cap• Word tokens in window of size two around the current word which include wi−2, wi−1, wi+1, wi+2 and also the capitalization pattern in the window. • Previous two predictions yi−1 and yi−2. The effectiveness of the dictionaries are evaluated by adding dictionary matches as features along with the baseline features (Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. We also compared the quality of the candidate </context>
<context position="9364" citStr="Dhillon et al. (2011)" startWordPosition="1582" endWordPosition="1585">hrase in the sentence. The spelling and the context of the candidate phrase provide a natural split into two views which multi-view algorithms like co-training and CCA can exploit. The only supervision in our method is to provide a few spelling seed examples (10 in the case of virus, 18 in the case of disease), for example, human immunodeficiency is a virus and mutant is not a virus. 3.2.1 Approach using CCA embeddings We use CCA described in the previous section to obtain lower dimensional embeddings for the candidate phrases using the (spelling, context) views. Unlike previous works such as Dhillon et al. (2011) and Dhillon et al. (2012), we use CCA to learn embeddings for candidate phrases instead of all words in the vocabulary so that we don’t miss named entities which have two or more words. Let the number of (spelling, context) pairs be n (sum of total number of instances of every candidate phrase in the unlabeled data collection). First, we map the spelling and context to highdimensional feature vectors. For the spelling view, we define a feature for every candidate phrase and also a boolean feature which indicates whether the phrase is capitalized or not. For the context view, we use features s</context>
<context position="16992" citStr="Dhillon et al. (2011)" startWordPosition="2888" endWordPosition="2891">he entities are 455 abbreviations (Kazama and Torisawa, 2007). For example, DM is the abbreviation for the disease Diabetes Mellitus and the disambiguation page for DM in Wikipedia associates it to more than 50 categories since DM can be expanded to Doctor of Management, Dichroic mirror, and so on, each of it belonging to a different category. Due to the rapid growth of Wikipedia, the number of entities that have disambiguation pages is growing fast and it is increasingly difficult to retrieve the article we want. Also, it is tough to understand these approaches from a theoretical standpoint. Dhillon et al. (2011) used CCA to learn word embeddings and added them as features in a sequence tagger. They show that CCA learns better word embeddings than CW embeddings (Collobert and Weston, 2008), Hierarchical log-linear (HLBL) embeddings (Mnih and Hinton, 2007) and embeddings learned from many other techniques for NER and chunking. Unlike PCA, a widely used dimensionality reduction technique, CCA is invariant to linear transformations of the data. Our approach is motivated by the theoretical result in Kakade and Foster (2007) which is developed in the co-training setting. We directly use the CCA embeddings </context>
<context position="18815" citStr="Dhillon et al. (2011)" startWordPosition="3203" endWordPosition="3206">imental results on virus and disease NER. 5.1 Data The noisy lists of both virus and disease names were obtained from the BioMed Central corpus. This corpus was also used to get the collection of (spelling, context) pairs which are the input to the CCA procedure and the DL-CoTrain algorithm described in the previous section. We obtained CCA embeddings for the 100, 000 most frequently occurring word types in this collection along with every word type present in the training and development data of the virus and the disease NER dataset. These word embeddings are similar to the ones described in Dhillon et al. (2011) and Dhillon et al. (2012). We used the virus annotations in the GENIA corpus (GENIA, 2003) for our experiments. The dataset contains 18,546 annotated sentences. We randomly selected 8,546 sentences for training and the remaining sentences were randomly split equally into development and testing sentences. The training sentences are used only for experiments with the sequence taggers. Previously, Zhang et al. (2004) tested their HMM-based named entity recognizer on this data. For disease NER, we used the recent disease corpus (Dogan and Lu, 2012) and used the same training, development and tes</context>
<context position="29125" citStr="Dhillon et al. (2011)" startWordPosition="4918" endWordPosition="4921">r varying training data size when embeddings obtained from different methods are used as features previously, the dictionaries produced from our approach performs competitively with dictionaries that are entirely built by domain experts. 5.3.2 Embedding Features The quality of the candidate phrase embeddings are compared with word embeddings by adding the embeddings as features in the CRF tagger. Along with the baseline features, CCA-word model adds word embeddings as features while the CCA-phrase model adds candidate phrase embeddings as features. CCA-word model is similar to the one used in Dhillon et al. (2011). We considered adding 10, 20, 30, 40 and 50 dimensional word embeddings as features for every training data size and the best performing model on the development data was picked for the experiments on the test data. For candidate phrase embeddings we used the same number of dimensions that was used for training the SVMs to construct the best dictionary. When candidate phrase embeddings are obtained using CCA, we do not have embeddings for words which are not in the list of candidate phrases. Also, a candidate phrase having more than one word has a joint representation, i.e., the phrase “human</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean Foster and Lyle Ungar. Multi-View Learning of Word Embeddings via CCA. 2011. Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Jordan Rodu</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Two Step CCA: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="9390" citStr="Dhillon et al. (2012)" startWordPosition="1587" endWordPosition="1590"> spelling and the context of the candidate phrase provide a natural split into two views which multi-view algorithms like co-training and CCA can exploit. The only supervision in our method is to provide a few spelling seed examples (10 in the case of virus, 18 in the case of disease), for example, human immunodeficiency is a virus and mutant is not a virus. 3.2.1 Approach using CCA embeddings We use CCA described in the previous section to obtain lower dimensional embeddings for the candidate phrases using the (spelling, context) views. Unlike previous works such as Dhillon et al. (2011) and Dhillon et al. (2012), we use CCA to learn embeddings for candidate phrases instead of all words in the vocabulary so that we don’t miss named entities which have two or more words. Let the number of (spelling, context) pairs be n (sum of total number of instances of every candidate phrase in the unlabeled data collection). First, we map the spelling and context to highdimensional feature vectors. For the spelling view, we define a feature for every candidate phrase and also a boolean feature which indicates whether the phrase is capitalized or not. For the context view, we use features similar to Dhillon et al. (</context>
<context position="12258" citStr="Dhillon et al. (2012)" startWordPosition="2106" endWordPosition="2109"> projections ¯xi = ΦT1 xi E Rk, Vi E 11, 2, ... , n}. • The embedding of a candidate phrase is given by the k-dimensional spelling view projection of any instance of the candidate phrase. • We learn a binary SVM with the candidate phrase embeddings as features and the spelling seed examples given in figure 1 as training data. Using this SVM, we predict whether a candidate phrase is a named entity or not. 3.2.2 Approach based on Co-training We discuss here briefly the DL-CoTrain algorithm (Collins and Singer, 1999) which is based on cotraining (Blum and Mitchell, 1998), to classify 3Similar to Dhillon et al. (2012) we used the method given in Halko et al. (2011) to perform the SVD computation in CCA for practical considerations. 4Note that a candidate phrase gets the same spelling view projection across it’s different instances since the spelling features of a candidate phrase are identical across it’s instances. 454 • Virus seed spelling examples – Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B – Non-virus Names: mutant, same, wild type, parental, recombinant • Disease seed spelling examples – Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, t</context>
<context position="17852" citStr="Dhillon et al. (2012)" startWordPosition="3032" endWordPosition="3035">n, 2007) and embeddings learned from many other techniques for NER and chunking. Unlike PCA, a widely used dimensionality reduction technique, CCA is invariant to linear transformations of the data. Our approach is motivated by the theoretical result in Kakade and Foster (2007) which is developed in the co-training setting. We directly use the CCA embeddings to predict the label of a data point instead of using them as features in a sequence tagger. Also, we learn CCA embeddings for candidate phrases instead of all words in the vocabulary since named entities often contain more than one word. Dhillon et al. (2012) learn a multi-class SVM using the CCA word embeddings to predict the POS tag of a word type. We extend this technique to NER by learning a binary SVM using the CCA embeddings of a high recall, low precision list of candidate phrases to predict whether a candidate phrase is a named entity or not. 5 Experiments In this section, we give experimental results on virus and disease NER. 5.1 Data The noisy lists of both virus and disease names were obtained from the BioMed Central corpus. This corpus was also used to get the collection of (spelling, context) pairs which are the input to the CCA proce</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>Paramveer Dhillon, Jordan Rodu, Dean Foster and Lyle Ungar. Two Step CCA: A new spectral method for estimating vector models of words. 2012. International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rezarta Islamaj Dogan</author>
<author>Zhiyong Lu</author>
</authors>
<title>An improved corpus of disease mentions in PubMed citations.</title>
<date>2012</date>
<booktitle>Workshop on Biomedical Natural Language Processing, Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1622" citStr="Dogan and Lu, 2012" startWordPosition="247" endWordPosition="250">d Roth, 2009; Cohen and Sarawagi, 2004) have shown that injecting dictionary matches as features in a sequence tagger results in significant gains in NER performance. However, building these dictionaries requires a huge amount of human effort and it is often difficult to get good coverage for many named entity types. The problem is more severe when we consider named entity types such as gene, virus and disease, because of the large (and growing) number of names in use, the fact that the names are heavily abbreviated and multiple names are used to refer to the same entity (Leaman et al., 2010; Dogan and Lu, 2012). Also, these dictionaries can only be built by domain experts, making the process very expensive. This paper describes an approach for automatic construction of dictionaries for NER using large amounts of unlabeled data and a small number of seed examples. Our approach consists of two steps. First, we collect a high recall, low precision list of candidate phrases from the large unlabeled data collection for every named entity type using simple rules. In the second step, we construct an accurate dictionary of named entities by removing the noisy candidates from the list obtained in the first s</context>
<context position="19367" citStr="Dogan and Lu, 2012" startWordPosition="3291" endWordPosition="3294">beddings are similar to the ones described in Dhillon et al. (2011) and Dhillon et al. (2012). We used the virus annotations in the GENIA corpus (GENIA, 2003) for our experiments. The dataset contains 18,546 annotated sentences. We randomly selected 8,546 sentences for training and the remaining sentences were randomly split equally into development and testing sentences. The training sentences are used only for experiments with the sequence taggers. Previously, Zhang et al. (2004) tested their HMM-based named entity recognizer on this data. For disease NER, we used the recent disease corpus (Dogan and Lu, 2012) and used the same training, development and test data split given by them. We used a sentence segmenter6 to get sentence segmented data and Stanford Tokenizer7 to tokenize the data. Similar to Dogan and Lu (2012), all the different disease categories were flattened into one single category of disease mentions. The development data was used to tune the hyperparameters and the methods were evaluated on the test data. 5.2 Results using a dictionary-based tagger First, we compare the dictionaries compiled using different methods by using them directly in a dictionary-based tagger. This is a simpl</context>
<context position="21893" citStr="Dogan and Lu, 2012" startWordPosition="3703" endWordPosition="3706">, which requires a large amount of human effort, are employed for the task. We used the list of virus names given in Wikipedia8. Unfortunately, abbreviations of virus names are not present in this list and we could not find any other more complete list of virus names. Hence, we constructed abbreviations by concatenating the first letters of all the strings in a virus name, for every virus name given in the Wikipedia list. For diseases, we used the list of disease names given in the Unified Medical Language System (UMLS) Metathesaurus. This dictionary has been widely used in disease NER (e.g., Dogan and Lu, 2012; Leaman et al., 2010)9. • Co-Training: The dictionaries are constructed using the DL-CoTrain algorithm described previously. The parameters used were m = 5 and c = 0.95 as given in Collins and Singer (1999). The phrases present in the spelling rules which give a positive label and whose strength is greater than the precision threshold, were added to the dictionary of named entities. In our experiment to construct a dictionary of virus names, the algorithm stopped after just 12 iterations and hence the dictionary had only 390 virus names. This was because there were no spelling rules with stre</context>
</contexts>
<marker>Dogan, Lu, 2012</marker>
<rawString>Rezarta Islamaj Dogan and Zhiyong Lu. An improved corpus of disease mentions in PubMed citations. 2012. Workshop on Biomedical Natural Language Processing, Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Leaman</author>
<author>Christopher Miller</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Enabling Recognition of Diseases</title>
<booktitle>in Biomedical Text with Machine Learning: Corpus and Benchmark. 2010. Workshop on Biomedical Natural Language Processing, Association for Computational Linguistics (ACL).</booktitle>
<marker>Leaman, Miller, Gonzalez, </marker>
<rawString>Robert Leaman, Christopher Miller and Graciela Gonzalez. Enabling Recognition of Diseases in Biomedical Text with Machine Learning: Corpus and Benchmark. 2010. Workshop on Biomedical Natural Language Processing, Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="17172" citStr="Collobert and Weston, 2008" startWordPosition="2920" endWordPosition="2924">ipedia associates it to more than 50 categories since DM can be expanded to Doctor of Management, Dichroic mirror, and so on, each of it belonging to a different category. Due to the rapid growth of Wikipedia, the number of entities that have disambiguation pages is growing fast and it is increasingly difficult to retrieve the article we want. Also, it is tough to understand these approaches from a theoretical standpoint. Dhillon et al. (2011) used CCA to learn word embeddings and added them as features in a sequence tagger. They show that CCA learns better word embeddings than CW embeddings (Collobert and Weston, 2008), Hierarchical log-linear (HLBL) embeddings (Mnih and Hinton, 2007) and embeddings learned from many other techniques for NER and chunking. Unlike PCA, a widely used dimensionality reduction technique, CCA is invariant to linear transformations of the data. Our approach is motivated by the theoretical result in Kakade and Foster (2007) which is developed in the co-training setting. We directly use the CCA embeddings to predict the label of a data point instead of using them as features in a sequence tagger. Also, we learn CCA embeddings for candidate phrases instead of all words in the vocabul</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. 2008. International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Identifying Gene and Protein Mentions in Text Using Conditional Random Fields.</title>
<date>2005</date>
<journal>BMC Bioinformatics.</journal>
<contexts>
<context position="5329" citStr="McDonald and Pereira, 2005" startWordPosition="880" endWordPosition="883">abel of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognition CRF based sequence taggers have been used for a number of NER tasks (e.g., McCallum and Li, 2003) and in particular for biomedical NER (e.g., McDonald and Pereira, 2005; Burr Settles, 2004) because they allow a great deal of flexibility in the features which can be included. The input to a CRF tagger is a sentence (w1, w2,... , wn) where wi, ∀i ∈ {1, 2, ... , n} are words in the sentence. The output is a sequence of tags y1, y2,. .. , yn where yi ∈ {B, T, O}, ∀i ∈ {1, 2, ... , n}. B is the tag given to the first word in a named entity, T is the tag given to all words except the first word in a named entity and O is the tag given to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: </context>
</contexts>
<marker>McDonald, Pereira, 2005</marker>
<rawString>Ryan McDonald and Fernando Pereira. Identifying Gene and Protein Mentions in Text Using Conditional Random Fields. 2005. BMC Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sham M Kakade</author>
<author>Dean P Foster</author>
</authors>
<title>Multi-view regression via canonical correlation analysis.</title>
<date>2007</date>
<booktitle>Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="4559" citStr="Kakade and Foster (2007)" startWordPosition="749" endWordPosition="752">where xi ∈ Rd1, zi ∈ Rd2 (∀i ∈ {1, 2, ... , n}) are the feature representations for the two views of a data point. CCA simultaneously learns projection matrices Φ1 ∈ Rd1×k, Φ2 ∈ Rd2×k (k is a small number) which are used to obtain the lower dimensional representations (¯x1, ¯z1), ... , (¯xn, ¯zn) where ¯xi = ΦT1 xi ∈ Rk, ¯zi = ΦT2 zi ∈ Rk, ∀i ∈ {1, 2, ... , n}. Φ1,Φ2 are chosen to maximize the correlation between ¯xi and ¯zi, ∀i ∈ {1, 2, ... , n}. Consider the setting where we have a label for the data point along with it’s two views and either view is sufficient to make accurate predictions. Kakade and Foster (2007) and Sridharan and Kakade (2008) give strong theoretical guarantees when the lower dimensional embeddings from CCA are used for predicting the label of the data point. This setting is similar to the one considered in co-training (Collins and Singer, 1999) but there is no assumption of independence between the two views of the data point. Also, it is an exact algorithm unlike the algorithm given in Collins and Singer (1999). Since we are using lower dimensional embeddings of the data point for prediction, we can learn a predictor with fewer labeled examples. 2.2 CRFs for Named Entity Recognitio</context>
<context position="17509" citStr="Kakade and Foster (2007)" startWordPosition="2972" endWordPosition="2975">want. Also, it is tough to understand these approaches from a theoretical standpoint. Dhillon et al. (2011) used CCA to learn word embeddings and added them as features in a sequence tagger. They show that CCA learns better word embeddings than CW embeddings (Collobert and Weston, 2008), Hierarchical log-linear (HLBL) embeddings (Mnih and Hinton, 2007) and embeddings learned from many other techniques for NER and chunking. Unlike PCA, a widely used dimensionality reduction technique, CCA is invariant to linear transformations of the data. Our approach is motivated by the theoretical result in Kakade and Foster (2007) which is developed in the co-training setting. We directly use the CCA embeddings to predict the label of a data point instead of using them as features in a sequence tagger. Also, we learn CCA embeddings for candidate phrases instead of all words in the vocabulary since named entities often contain more than one word. Dhillon et al. (2012) learn a multi-class SVM using the CCA word embeddings to predict the POS tag of a word type. We extend this technique to NER by learning a binary SVM using the CCA embeddings of a high recall, low precision list of candidate phrases to predict whether a ca</context>
</contexts>
<marker>Kakade, Foster, 2007</marker>
<rawString>Sham M. Kakade and Dean P. Foster. Multi-view regression via canonical correlation analysis. 2007. Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Sunita Sarawagi</author>
</authors>
<title>Exploiting Dictionaries in Named Entity Extraction: Combining Semi-Markov Extraction Processes and Data Integration Methods.</title>
<date>2004</date>
<booktitle>Semi-Markov Extraction Processes and Data Integration Methods, Proceedings of KDD.</booktitle>
<contexts>
<context position="1042" citStr="Cohen and Sarawagi, 2004" startWordPosition="143" endWordPosition="147">ity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings. 1 Introduction Several works (e.g., Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) have shown that injecting dictionary matches as features in a sequence tagger results in significant gains in NER performance. However, building these dictionaries requires a huge amount of human effort and it is often difficult to get good coverage for many named entity types. The problem is more severe when we consider named entity types such as gene, virus and disease, because of the large (and growing) number of names in use, the fact that the names are heavily abbreviated and multiple names are used to refer to the same entity (Leaman et al., 2010; Dogan and Lu, 2012). Also, these dictio</context>
<context position="6422" citStr="Cohen and Sarawagi, 2004" startWordPosition="1091" endWordPosition="1094"> to all other words. We used the standard NER baseline features (e.g., Dhillon et al., 2011; Ratinov and Roth, 2009) which include: • Current Word wi and its lexical features which include whether the word is capitalized and whether all the characters are cap• Word tokens in window of size two around the current word which include wi−2, wi−1, wi+1, wi+2 and also the capitalization pattern in the window. • Previous two predictions yi−1 and yi−2. The effectiveness of the dictionaries are evaluated by adding dictionary matches as features along with the baseline features (Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. We also compared the quality of the candidate phrase embeddings with the word-level embeddings by adding them as features (Dhillon et al., 2011) along with the baseline features in the CRF tagger. 3 Method This section describes the two steps in our approach: obtaining candidate phrases and classifying them. 3.1 Obtaining Candidate Phrases We used the full text of 110,369 biomedical publications in the BioMed Central corpus1 to get the high recall, low precision list of candidate phrases. The advantages of using this huge collection of publications are obvious: almost all (</context>
<context position="26969" citStr="Cohen and Sarawagi, 2004" startWordPosition="4560" endWordPosition="4563">n. 5.3 Results using a CRF tagger We did two sets of experiments using a CRF tagger. In the first experiment, we add dictionary features to the CRF tagger while in the second experiment we add the embeddings as features to the CRF tagger. The same baseline model is used in both the experiments whose features are described in Section 2.2. For both the CRF12 experiments the regularizers from the set [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0] were considered and it was tuned on the development set. 5.3.1 Dictionary Features Here, we inject dictionary matches as features (e.g., Ratinov and Roth, 2009; Cohen and Sarawagi, 2004) in the CRF tagger. Given a dictionary of named entities, every word in the input sentence has a dictionary feature associated with it. When there is an exact match between a phrase in the dictionary with the words in the input sentence, the dictionary feature of the first word in the named entity is set to B and the dictionary feature of the remaining words in the named entity is set to I. The dictionary feature of all the other words in the input sentence which are not part of any named entity in the dictionary is set to O. The effectiveness of the dictionaries constructed from various metho</context>
</contexts>
<marker>Cohen, Sarawagi, 2004</marker>
<rawString>William W. Cohen and Sunita Sarawagi. Exploiting Dictionaries in Named Entity Extraction: Combining Semi-Markov Extraction Processes and Data Integration Methods. 2004. Semi-Markov Extraction Processes and Data Integration Methods, Proceedings of KDD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>