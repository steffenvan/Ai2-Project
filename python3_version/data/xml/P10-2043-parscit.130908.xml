<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001292">
<title confidence="0.984093">
Online Generation of Locality Sensitive Hash Signatures
</title>
<author confidence="0.94771">
Benjamin Van Durme Ashwin Lall
</author>
<affiliation confidence="0.712531">
HLTCOE College of Computing
Johns Hopkins University Georgia Institute of Technology
Baltimore, MD 21211 USA Atlanta, GA 30332 USA
</affiliation>
<sectionHeader confidence="0.966714" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855428571429">
Motivated by the recent interest in stream-
ing algorithms for processing large text
collections, we revisit the work of
Ravichandran et al. (2005) on using the
Locality Sensitive Hash (LSH) method of
Charikar (2002) to enable fast, approxi-
mate comparisons of vector cosine simi-
larity. For the common case of feature
updates being additive over a data stream,
we show that LSH signatures can be main-
tained online, without additional approxi-
mation error, and with lower memory re-
quirements than when using the standard
offline technique.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973166666667">
There has been a surge of interest in adapting re-
sults from the streaming algorithms community to
problems in processing large text collections. The
term streaming refers to a model where data is
made available sequentially, and it is assumed that
resource limitations preclude storing the entirety
of the data for offline (batch) processing. Statis-
tics of interest are approximated via online, ran-
domized algorithms. Examples of text applica-
tions include: collecting approximate counts (Tal-
bot, 2009; Van Durme and Lall, 2009a), finding
top-n elements (Goyal et al., 2009), estimating
term co-occurrence (Li et al., 2008), adaptive lan-
guage modeling (Levenberg and Osborne, 2009),
and building top-k ranklists based on pointwise
mutual information (Van Durme and Lall, 2009b).
Here we revisit the work of Ravichandran et al.
(2005) on building word similarity measures from
large text collections by using the Locality Sensi-
tive Hash (LSH) method of Charikar (2002). For
the common case of feature updates being addi-
tive over a data stream (such as when tracking
lexical co-occurrence), we show that LSH signa-
tures can be maintained online, without additional
approximation error, and with lower memory re-
quirements than when using the standard offline
technique.
We envision this method being used in conjunc-
tion with dynamic clustering algorithms, for a va-
riety of applications. For example, Petrovic et al.
(2010) made use of LSH signatures generated over
individual tweets, for the purpose of first story de-
tection. Streaming LSH should allow for the clus-
tering of Twitter authors, based on the tweets they
generate, with signatures continually updated over
the Twitter stream.
</bodyText>
<sectionHeader confidence="0.90391" genericHeader="method">
2 Locality Sensitive Hashing
</sectionHeader>
<bodyText confidence="0.99796425">
We are concerned with computing the cosine sim-
ilarity of feature vectors, defined for a pair of vec-
tors i and v� as the dot product normalized by their
lengths:
</bodyText>
<equation confidence="0.974363">
cosine−similarity(u,v) =
</equation>
<bodyText confidence="0.952895454545455">
This similarity is the cosine of the angle be-
tween these high-dimensional vectors and attains
a value of one (i.e., cos (0)) when the vectors are
parallel and zero (i.e., cos (7r/2)) when orthogo-
nal.
Building on the seminal work of Indyk and
Motwani (1998) on locality sensitive hashing
(LSH), Charikar (2002) presented an LSH that
maps high-dimensional vectors to a much smaller
dimensional space while still preserving (cosine)
similarity between vectors in the original space.
The LSH algorithm computes a succinct signature
of the feature set of the words in a corpus by com-
puting d independent dot products of each feature
vector v� with a random unit vector r, i.e., Ei viri,
and retaining the sign of the d resulting products.
Each entry of r� is drawn from the distribution
N(0,1), the normal distribution with zero mean
and unit variance. Charikar’s algorithm makes use
of the fact (proved by Goemans and Williamson
u· v
|u||�v|�
</bodyText>
<page confidence="0.961945">
231
</page>
<note confidence="0.5037745">
Proceedings of the ACL 2010 Conference Short Papers, pages 231–235,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999482230769231">
(1995) for an unrelated application) that the an-
gle between any two vectors summarized in this
fashion is proportional to the expected Hamming
distance of their signature vectors. Hence, we can
retain length d bit-signatures in the place of high
dimensional feature vectors, while preserving the
ability to (quickly) approximate cosine similarity
in the original space.
Ravichandran et al. (2005) made use of this al-
gorithm to reduce the computation in searching
for similar nouns by first computing signatures for
each noun and then computing similarity over the
signatures rather than the original feature space.
</bodyText>
<sectionHeader confidence="0.979947" genericHeader="method">
3 Streaming Algorithm
</sectionHeader>
<bodyText confidence="0.972815222222222">
In this work, we focus on features that can be
maintained additively, such as raw frequencies.1
Our streaming algorithm for this problem makes
use of the simple fact that the dot product of the
feature vector with random vectors is a linear op-
eration. This permits us to replace the vi · ri op-
eration by vi individual additions of ri, once for
each time the feature is encountered in the stream
(where vi is the frequency of a feature and ri is the
randomly chosen Gaussian-distributed value asso-
ciated with this feature). The result of the final
computation is identical to the dot products com-
puted by the algorithm of Charikar (2002), but
the processing can now be done online. A simi-
lar technique, for stable random projections, was
independently discussed by Li et al. (2008).
Since each feature may appear multiple times
in the stream, we need a consistent way to retrieve
the random values drawn from N(0,1) associated
with it. To avoid the expense of computing and
storing these values explicitly, as is the norm, we
propose the use of a precomputed pool of ran-
dom values drawn from this distribution that we
can then hash into. Hashing into a fixed pool en-
sures that the same feature will consistently be as-
sociated with the same value drawn from N(0,1).
This introduces some weak dependence in the ran-
dom vectors, but we will give some analysis show-
ing that this should have very limited impact on
the cosine similarity computation, which we fur-
ther support with experimental evidence (see Ta-
ble 3).
Our algorithm traverses a stream of words and
1Note that Ravichandran et al. (2005) used pointwise mu-
tual information features, which are not additive since they
require a global statistic to compute.
</bodyText>
<equation confidence="0.766012571428572">
Algorithm 1 STREAMING LSH ALGORITHM
Parameters:
m : size of pool
d : number of bits (size of resultant signature)
s : a random seed
hl, ..., hd : hash functions mapping (s, fi) to {0, ... , m−11
INITIALIZATION:
</equation>
<listItem confidence="0.990382117647059">
1: Initialize floating point array P[0, ... ,m − 1]
2: Initialize H, a hashtable mapping words to floating point
arrays of size d
3: for i := 0 ... m − 1 do
4: P[i] := random sample from N(0, 1), using s as seed
ONLINE:
1: for each word w in the stream do
2: for each feature fi associated with w do
3: for j := 1 ... d do
4: H[w][j] := H[w][j] + P[hj(s, fi)]
SIGNATURECOMPUTATION:
1: for each w E H do
2: for i := 1 ... d do
3: if H[w][i] &gt; 0 then
4: S[w][i] := 1
5: else
6: S[w][i] := 0
</listItem>
<bodyText confidence="0.999517857142857">
maintains some state for each possible word that
it encounters (cf. Algorithm 1). In particular, the
state maintained for each word is a vector of float-
ing point numbers of length d. Each element of the
vector holds the (partial) dot product of the feature
vector of the word with a random unit vector. Up-
dating the state for a feature seen in the stream for
a given word simply involves incrementing each
position in the word’s vector by the random value
associated with the feature, accessed by hash func-
tions h1 through hd. At any point in the stream,
the vector for each word can be processed (in time
O(d)) to create a signature computed by checking
the sign of each component of its vector.
</bodyText>
<subsectionHeader confidence="0.997902">
3.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999796076923077">
The update cost of the streaming algorithm, per
word in the stream, is O(df), where d is the target
signature size and f is the number of features asso-
ciated with each word in the stream.2 This results
in an overall cost of O(ndf) for the streaming al-
gorithm, where n is the length of the stream. The
memory footprint of our algorithm is O(n0d+m),
where n0 is the number of distinct words in the
stream and m is the size of the pool of normally
distributed values. In comparison, the original
LSH algorithm computes signatures at a cost of
O(nf + n0dF) updates and O(n0F + dF + n0d)
memory, where F is the (large) number of unique
</bodyText>
<footnote confidence="0.964599">
2For the bigram features used in § 4, f = 2.
</footnote>
<page confidence="0.985603">
232
</page>
<bodyText confidence="0.990716666666667">
features. Our algorithm is superior in terms of
memory (because of the pooling trick), and has the
benefit of supporting similarity queries online.
</bodyText>
<subsectionHeader confidence="0.999892">
3.2 Pooling Normally-distributed Values
</subsectionHeader>
<bodyText confidence="0.999980714285714">
We now discuss why it is possible to use a
fixed pool of random values instead of generating
unique ones for each feature. Let g be the c.d.f.
of the distribution N(0,1). It is easy to see that
picking x E (0, 1) uniformly results in g−1(x) be-
ing chosen with distribution N(0,1). Now, if we
select for our pool the values
</bodyText>
<equation confidence="0.474891">
g−1(1/m), g−1(2/m), ... , g−1(1 − 1/m),
(a)
</equation>
<bodyText confidence="0.9988604">
for some sufficiently large m, then this is identical
to sampling from N(0,1) with the caveat that the
accuracy of the sample is limited. More precisely,
the deviation from sampling from this pool is off
from the actual value by at most
</bodyText>
<equation confidence="0.998799">
Z=1mx−2 ((i
{g−1+ 1)/m) − g−1(i/m)}.
</equation>
<bodyText confidence="0.9999558125">
By choosing m to be sufficiently large, we can
bound the error of the approximate sample from
a true sample (i.e., the loss in precision expressed
above) to be a small fraction (e.g., 1%) of the ac-
tual value. This would result in the same relative
error in the computation of the dot product (i.e.,
1%), which would almost never affect the sign of
the final value. Hence, pooling as above should
give results almost identical to the case where all
the random values were chosen independently. Fi-
nally, we make the observation that, for large m,
randomly choosing m values from N(0,1) results
in a set of values that are distributed very similarly
to the pool described above. An interesting avenue
for future work is making this analysis more math-
ematically precise.
</bodyText>
<subsectionHeader confidence="0.993888">
3.3 Extensions
</subsectionHeader>
<bodyText confidence="0.999835">
Decay The algorithm can be extended to support
temporal decay in the stream, where recent obser-
vations are given higher relative weight, by mul-
tiplying the current sums by a decay value (e.g.,
0.9) on a regular interval (e.g., once an hour, once
a day, once a week, etc.).
Distributed The algorithm can be easily dis-
tributed across multiple machines in order to pro-
cess different parts of a stream, or multiple differ-
ent streams, in parallel, such as in the context of
the MapReduce framework (Dean and Ghemawat,
</bodyText>
<figure confidence="0.978725">
(b)
</figure>
<figureCaption confidence="0.999992333333333">
Figure 1: Predicted versus actual cosine values for 50,000
pairs, using LSH signatures generated online, with d = 32 in
Fig. 1(a) and d = 256 in Fig. 1(b).
</figureCaption>
<bodyText confidence="0.999911571428571">
2004). The underlying operation is a linear op-
erator that is easily composed (i.e., via addition),
and the randomness between machines can be tied
based on a shared seed s. At any point in process-
ing the stream(s), current results can be aggregated
by summing the d-dimensional vectors for each
word, from each machine.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999979125">
Similar to the experiments of Ravichandran et
al. (2005), we evaluated the fidelity of signature
generation in the context of calculating distribu-
tional similarity between words across a large
text collection: in our case, articles taken from
the NYTimes portion of the Gigaword corpus
(Graff, 2003). The collection was processed as a
stream, sentence by sentence, using bigram fea-
</bodyText>
<page confidence="0.997299">
233
</page>
<table confidence="0.997216333333333">
d 16 32 64 128 256
SLSH 0.2885 0.2112 0.1486 0.1081 0.0769
LSH 0.2892 0.2095 0.1506 0.1083 0.0755
</table>
<tableCaption confidence="0.9962915">
Table 1: Mean absolute error when using signatures gener-
ated online (StreamingLSH), compared to offline (LSH).
</tableCaption>
<bodyText confidence="0.999779777777778">
tures. This gave a stream of 773,185,086 tokens,
with 1,138,467 unique types. Given the number
of types, this led to a (sparse) feature space with
dimension on the order of 2.5 million.
After compiling signatures, fifty-thousand
(x, y) pairs of types were randomly sampled
by selecting x and y each independently, with
replacement, from those types with at least 10 to-
kens in the stream (where 310,327 types satisfied
this constraint). The true cosine values between
each such x and y was computed based on offline
calculation, and compared to the cosine similarity
predicted by the Hamming distance between the
signatures for x and y. Unless otherwise specified,
the random pool size was fixed at m = 10, 000.
Figure 1 visually reaffirms the trade-off in LSH
between the number of bits and the accuracy of
cosine prediction across the range of cosine val-
ues. As the underlying vectors are strictly posi-
tive, the true cosine is restricted to [0, 1]. Figure 2
shows the absolute error between truth and predic-
tion for a similar sample, measured using signa-
tures of a variety of bit lengths. Here we see hori-
zontal bands arising from truly orthogonal vectors
leading to step-wise absolute error values tracked
to Hamming distance.
Table 1 compares the online and batch LSH al-
gorithms, giving the mean absolute error between
predicted and actual cosine values, computed for
the fifty-thousand element sample, using signa-
tures of various lengths. These results confirm that
we achieve the same level of accuracy with online
updates as compared to the standard method.
Figure 3 shows how a pool size as low as m =
100 gives reasonable variation in random values,
and that m = 10, 000 is sufficient. When using a
standard 32 bit floating point representation, this
is just 40 KBytes of memory, as compared to, e.g.,
the 2.5 GBytes required to store 256 random vec-
tors each containing 2.5 million elements.
Table 2 is based on taking an example for each
of three part-of-speech categories, and reporting
the resultant top-5 words as according to approx-
imated cosine similarity. Depending on the in-
tended application, these results indicate a range
</bodyText>
<figureCaption confidence="0.9655145">
Figure 2: Absolute error between predicted and true co-
sine for a sample of pairs, when using signatures of length
</figureCaption>
<equation confidence="0.646005">
log2(d) E {4, 5, 6, 7, 81, drawn with added jitter to avoid
overplotting.
Pool Size
</equation>
<figureCaption confidence="0.99872">
Figure 3: Error versus pool size, when using d = 256.
</figureCaption>
<bodyText confidence="0.977728">
of potentially sufficient signature lengths.
</bodyText>
<sectionHeader confidence="0.997433" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999302384615384">
We have shown that when updates to a feature vec-
tor are additive, it is possible to convert the offline
LSH signature generation method into a stream-
ing algorithm. In addition to allowing for on-
line querying of signatures, our approach leads to
space efficiencies, as it does not require the ex-
plicit representation of either the feature vectors,
nor the random matrix. Possibilities for future
work include the pairing of this method with algo-
rithms for dynamic clustering, as well as exploring
algorithms for different distances (e.g., L2) and es-
timators (e.g., asymmetric estimators (Dong et al.,
2009)).
</bodyText>
<figure confidence="0.998270181818182">
101 102 103 104 105
Mean Absolute Error
0.8
0.6
0.4
0.2
●
●
●
●
● ● ●
</figure>
<page confidence="0.969566">
234
</page>
<table confidence="0.978116142857143">
London
Milan.97, Madrid.96, Stockholm.96, Manila.95, Moscow.95
ASHER0, Champaign0, MANS0, NOBLE0, come0
Prague1, Vienna1, suburban1, synchronism1, Copenhagen2
Frankfurt4, Prague4, Taszar5, Brussels6, Copenhagen6
Prague12, Stockholm12, Frankfurt14, Madrid14, Manila14
Stockholm20, Milan22, Madrid24, Taipei24, Frankfurt25
in
during.99, on.98, beneath.98, from.98, onto.97
Across0, Addressing0, Addy0, Against0, Allmon0
aboard0, mishandled0, overlooking0, Addressing1, Rejecting1
Rejecting2, beneath2, during2, from3, hamstringing3
during4, beneath5, of6, on7, overlooking7
during10, on13, beneath15, of17, overlooking17
sold
deployed.84, presented.83, sacrificed.82, held.82, installed.82
Bustin0, Diors0, Draining0, Kosses0, UNA0
delivered2, held2, marks2, seared2, Ranked3
delivered5, rendered5, presented6, displayed7, exhibited7
held18, rendered18, presented19, deployed20, displayed20
presented41, rendered42, held47, leased47, reopened47
</table>
<tableCaption confidence="0.9821175">
Table 2: Top-5 items based on true cosine (bold), then using
minimal Hamming distance, given in top-down order when
using signatures of length log2(d) 2 f4, 5, 6, 7, 8g. Ties bro-
ken lexicographically. Values given as subscripts.
</tableCaption>
<sectionHeader confidence="0.997413" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999606111111111">
Thanks to Deepak Ravichandran, Miles Osborne,
Sasa Petrovic, Ken Church, Glen Coppersmith,
and the anonymous reviewers for their feedback.
This work began while the first author was at the
University of Rochester, funded by NSF grant IIS-
1016735. The second author was supported in
part by NSF grant CNS-0905169, funded under
the American Recovery and Reinvestment Act of
2009.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999582666666667">
Moses Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of STOC.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified Data Processing on Large Clusters.
In Proceedings of OSDI.
Wei Dong, Moses Charikar, and Kai Li. 2009. Asym-
metric distance estimation with sketches for similar-
ity search in high-dimensional spaces. In Proceed-
ings of SIGIR.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum
cut and satisfiability problems using semidefinite
programming. JACM, 42:1115–1145.
Amit Goyal, Hal Daum´e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Abby Levenberg and Miles Osborne. 2009. Stream-
based Randomised Language Models for SMT. In
Proceedings of EMNLP.
Ping Li, Kenneth W. Church, and Trevor J. Hastie.
2008. One Sketch For All: Theory and Application
of Conditional Random Sampling. In Advances in
Neural Information Processing Systems 21.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with appli-
cation to Twitter. In Proceedings of NAACL.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized Algorithms and NLP:
Using Locality Sensitive Hash Functions for High
Speed Noun Clustering. In Proceedings of ACL.
David Talbot. 2009. Succinct approximate counting of
skewed data. In Proceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009a. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming Pointwise Mutual Information. In Ad-
vances in Neural Information Processing Systems
22.
</reference>
<page confidence="0.998483">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.683413">
<title confidence="0.999959">Online Generation of Locality Sensitive Hash Signatures</title>
<author confidence="0.999763">Benjamin Van_Durme Ashwin Lall</author>
<affiliation confidence="0.9985225">HLTCOE College of Computing Johns Hopkins University Georgia Institute of Technology</affiliation>
<address confidence="0.999652">Baltimore, MD 21211 USA Atlanta, GA 30332 USA</address>
<abstract confidence="0.977172357142857">Motivated by the recent interest in streaming algorithms for processing large text collections, we revisit the work of Ravichandran et al. (2005) on using the Locality Sensitive Hash (LSH) method of Charikar (2002) to enable fast, approximate comparisons of vector cosine similarity. For the common case of feature updates being additive over a data stream, we show that LSH signatures can be mainwithout additional approximation error, and with lower memory requirements than when using the standard</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="1743" citStr="Charikar (2002)" startWordPosition="267" endWordPosition="268">cs of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique. We envision this method being used in conjunction with dynamic clustering algorithms, for a variety of applications. For example, Petrovic et al. (2010) made use of LSH signatures generated over individual tweets, for the purpose of first story detection. Streaming LSH should allow for the clustering</context>
<context position="2984" citStr="Charikar (2002)" startWordPosition="469" endWordPosition="470"> on the tweets they generate, with signatures continually updated over the Twitter stream. 2 Locality Sensitive Hashing We are concerned with computing the cosine similarity of feature vectors, defined for a pair of vectors i and v� as the dot product normalized by their lengths: cosine−similarity(u,v) = This similarity is the cosine of the angle between these high-dimensional vectors and attains a value of one (i.e., cos (0)) when the vectors are parallel and zero (i.e., cos (7r/2)) when orthogonal. Building on the seminal work of Indyk and Motwani (1998) on locality sensitive hashing (LSH), Charikar (2002) presented an LSH that maps high-dimensional vectors to a much smaller dimensional space while still preserving (cosine) similarity between vectors in the original space. The LSH algorithm computes a succinct signature of the feature set of the words in a corpus by computing d independent dot products of each feature vector v� with a random unit vector r, i.e., Ei viri, and retaining the sign of the d resulting products. Each entry of r� is drawn from the distribution N(0,1), the normal distribution with zero mean and unit variance. Charikar’s algorithm makes use of the fact (proved by Goemans</context>
<context position="5042" citStr="Charikar (2002)" startWordPosition="806" endWordPosition="807">s on features that can be maintained additively, such as raw frequencies.1 Our streaming algorithm for this problem makes use of the simple fact that the dot product of the feature vector with random vectors is a linear operation. This permits us to replace the vi · ri operation by vi individual additions of ri, once for each time the feature is encountered in the stream (where vi is the frequency of a feature and ri is the randomly chosen Gaussian-distributed value associated with this feature). The result of the final computation is identical to the dot products computed by the algorithm of Charikar (2002), but the processing can now be done online. A similar technique, for stable random projections, was independently discussed by Li et al. (2008). Since each feature may appear multiple times in the stream, we need a consistent way to retrieve the random values drawn from N(0,1) associated with it. To avoid the expense of computing and storing these values explicitly, as is the norm, we propose the use of a precomputed pool of random values drawn from this distribution that we can then hash into. Hashing into a fixed pool ensures that the same feature will consistently be associated with the sa</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified Data Processing on Large Clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of OSDI.</booktitle>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified Data Processing on Large Clusters. In Proceedings of OSDI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Dong</author>
<author>Moses Charikar</author>
<author>Kai Li</author>
</authors>
<title>Asymmetric distance estimation with sketches for similarity search in high-dimensional spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="14475" citStr="Dong et al., 2009" startWordPosition="2461" endWordPosition="2464"> Conclusions We have shown that when updates to a feature vector are additive, it is possible to convert the offline LSH signature generation method into a streaming algorithm. In addition to allowing for online querying of signatures, our approach leads to space efficiencies, as it does not require the explicit representation of either the feature vectors, nor the random matrix. Possibilities for future work include the pairing of this method with algorithms for dynamic clustering, as well as exploring algorithms for different distances (e.g., L2) and estimators (e.g., asymmetric estimators (Dong et al., 2009)). 101 102 103 104 105 Mean Absolute Error 0.8 0.6 0.4 0.2 ● ● ● ● ● ● ● 234 London Milan.97, Madrid.96, Stockholm.96, Manila.95, Moscow.95 ASHER0, Champaign0, MANS0, NOBLE0, come0 Prague1, Vienna1, suburban1, synchronism1, Copenhagen2 Frankfurt4, Prague4, Taszar5, Brussels6, Copenhagen6 Prague12, Stockholm12, Frankfurt14, Madrid14, Manila14 Stockholm20, Milan22, Madrid24, Taipei24, Frankfurt25 in during.99, on.98, beneath.98, from.98, onto.97 Across0, Addressing0, Addy0, Against0, Allmon0 aboard0, mishandled0, overlooking0, Addressing1, Rejecting1 Rejecting2, beneath2, during2, from3, hamstri</context>
</contexts>
<marker>Dong, Charikar, Li, 2009</marker>
<rawString>Wei Dong, Moses Charikar, and Kai Li. 2009. Asymmetric distance estimation with sketches for similarity search in high-dimensional spaces. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel X Goemans</author>
<author>David P Williamson</author>
</authors>
<title>Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming.</title>
<date>1995</date>
<pages>42--1115</pages>
<publisher>JACM,</publisher>
<marker>Goemans, Williamson, 1995</marker>
<rawString>Michel X. Goemans and David P. Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. JACM, 42:1115–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language Modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language Modeling. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<contexts>
<context position="11123" citStr="Graff, 2003" startWordPosition="1902" endWordPosition="1903"> operation is a linear operator that is easily composed (i.e., via addition), and the randomness between machines can be tied based on a shared seed s. At any point in processing the stream(s), current results can be aggregated by summing the d-dimensional vectors for each word, from each machine. 4 Experiments Similar to the experiments of Ravichandran et al. (2005), we evaluated the fidelity of signature generation in the context of calculating distributional similarity between words across a large text collection: in our case, articles taken from the NYTimes portion of the Gigaword corpus (Graff, 2003). The collection was processed as a stream, sentence by sentence, using bigram fea233 d 16 32 64 128 256 SLSH 0.2885 0.2112 0.1486 0.1081 0.0769 LSH 0.2892 0.2095 0.1506 0.1083 0.0755 Table 1: Mean absolute error when using signatures generated online (StreamingLSH), compared to offline (LSH). tures. This gave a stream of 773,185,086 tokens, with 1,138,467 unique types. Given the number of types, this led to a (sparse) feature space with dimension on the order of 2.5 million. After compiling signatures, fifty-thousand (x, y) pairs of types were randomly sampled by selecting x and y each indepe</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="2931" citStr="Indyk and Motwani (1998)" startWordPosition="460" endWordPosition="463"> LSH should allow for the clustering of Twitter authors, based on the tweets they generate, with signatures continually updated over the Twitter stream. 2 Locality Sensitive Hashing We are concerned with computing the cosine similarity of feature vectors, defined for a pair of vectors i and v� as the dot product normalized by their lengths: cosine−similarity(u,v) = This similarity is the cosine of the angle between these high-dimensional vectors and attains a value of one (i.e., cos (0)) when the vectors are parallel and zero (i.e., cos (7r/2)) when orthogonal. Building on the seminal work of Indyk and Motwani (1998) on locality sensitive hashing (LSH), Charikar (2002) presented an LSH that maps high-dimensional vectors to a much smaller dimensional space while still preserving (cosine) similarity between vectors in the original space. The LSH algorithm computes a succinct signature of the feature set of the words in a corpus by computing d independent dot products of each feature vector v� with a random unit vector r, i.e., Ei viri, and retaining the sign of the d resulting products. Each entry of r� is drawn from the distribution N(0,1), the normal distribution with zero mean and unit variance. Charikar</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased Randomised Language Models for SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1457" citStr="Levenberg and Osborne, 2009" startWordPosition="220" endWordPosition="223">ts from the streaming algorithms community to problems in processing large text collections. The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statistics of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique. We envision thi</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased Randomised Language Models for SMT. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth W Church</author>
<author>Trevor J Hastie</author>
</authors>
<title>One Sketch For All: Theory and Application of Conditional Random Sampling.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<contexts>
<context position="1399" citStr="Li et al., 2008" startWordPosition="212" endWordPosition="215">has been a surge of interest in adapting results from the streaming algorithms community to problems in processing large text collections. The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statistics of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than </context>
<context position="5186" citStr="Li et al. (2008)" startWordPosition="828" endWordPosition="831">t that the dot product of the feature vector with random vectors is a linear operation. This permits us to replace the vi · ri operation by vi individual additions of ri, once for each time the feature is encountered in the stream (where vi is the frequency of a feature and ri is the randomly chosen Gaussian-distributed value associated with this feature). The result of the final computation is identical to the dot products computed by the algorithm of Charikar (2002), but the processing can now be done online. A similar technique, for stable random projections, was independently discussed by Li et al. (2008). Since each feature may appear multiple times in the stream, we need a consistent way to retrieve the random values drawn from N(0,1) associated with it. To avoid the expense of computing and storing these values explicitly, as is the norm, we propose the use of a precomputed pool of random values drawn from this distribution that we can then hash into. Hashing into a fixed pool ensures that the same feature will consistently be associated with the same value drawn from N(0,1). This introduces some weak dependence in the random vectors, but we will give some analysis showing that this should </context>
</contexts>
<marker>Li, Church, Hastie, 2008</marker>
<rawString>Ping Li, Kenneth W. Church, and Trevor J. Hastie. 2008. One Sketch For All: Theory and Application of Conditional Random Sampling. In Advances in Neural Information Processing Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming First Story Detection with application to Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2194" citStr="Petrovic et al. (2010)" startWordPosition="337" endWordPosition="340">he work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique. We envision this method being used in conjunction with dynamic clustering algorithms, for a variety of applications. For example, Petrovic et al. (2010) made use of LSH signatures generated over individual tweets, for the purpose of first story detection. Streaming LSH should allow for the clustering of Twitter authors, based on the tweets they generate, with signatures continually updated over the Twitter stream. 2 Locality Sensitive Hashing We are concerned with computing the cosine similarity of feature vectors, defined for a pair of vectors i and v� as the dot product normalized by their lengths: cosine−similarity(u,v) = This similarity is the cosine of the angle between these high-dimensional vectors and attains a value of one (i.e., cos</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming First Story Detection with application to Twitter. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1609" citStr="Ravichandran et al. (2005)" startWordPosition="244" endWordPosition="247">able sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statistics of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique. We envision this method being used in conjunction with dynamic clustering algorithms, for a variety of applications. For example, Petrovic et al. (2010) made use of LS</context>
<context position="4165" citStr="Ravichandran et al. (2005)" startWordPosition="653" endWordPosition="656">ithm makes use of the fact (proved by Goemans and Williamson u· v |u||�v|� 231 Proceedings of the ACL 2010 Conference Short Papers, pages 231–235, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics (1995) for an unrelated application) that the angle between any two vectors summarized in this fashion is proportional to the expected Hamming distance of their signature vectors. Hence, we can retain length d bit-signatures in the place of high dimensional feature vectors, while preserving the ability to (quickly) approximate cosine similarity in the original space. Ravichandran et al. (2005) made use of this algorithm to reduce the computation in searching for similar nouns by first computing signatures for each noun and then computing similarity over the signatures rather than the original feature space. 3 Streaming Algorithm In this work, we focus on features that can be maintained additively, such as raw frequencies.1 Our streaming algorithm for this problem makes use of the simple fact that the dot product of the feature vector with random vectors is a linear operation. This permits us to replace the vi · ri operation by vi individual additions of ri, once for each time the f</context>
<context position="5999" citStr="Ravichandran et al. (2005)" startWordPosition="970" endWordPosition="973">g and storing these values explicitly, as is the norm, we propose the use of a precomputed pool of random values drawn from this distribution that we can then hash into. Hashing into a fixed pool ensures that the same feature will consistently be associated with the same value drawn from N(0,1). This introduces some weak dependence in the random vectors, but we will give some analysis showing that this should have very limited impact on the cosine similarity computation, which we further support with experimental evidence (see Table 3). Our algorithm traverses a stream of words and 1Note that Ravichandran et al. (2005) used pointwise mutual information features, which are not additive since they require a global statistic to compute. Algorithm 1 STREAMING LSH ALGORITHM Parameters: m : size of pool d : number of bits (size of resultant signature) s : a random seed hl, ..., hd : hash functions mapping (s, fi) to {0, ... , m−11 INITIALIZATION: 1: Initialize floating point array P[0, ... ,m − 1] 2: Initialize H, a hashtable mapping words to floating point arrays of size d 3: for i := 0 ... m − 1 do 4: P[i] := random sample from N(0, 1), using s as seed ONLINE: 1: for each word w in the stream do 2: for each fea</context>
<context position="10880" citStr="Ravichandran et al. (2005)" startWordPosition="1863" endWordPosition="1866">llel, such as in the context of the MapReduce framework (Dean and Ghemawat, (b) Figure 1: Predicted versus actual cosine values for 50,000 pairs, using LSH signatures generated online, with d = 32 in Fig. 1(a) and d = 256 in Fig. 1(b). 2004). The underlying operation is a linear operator that is easily composed (i.e., via addition), and the randomness between machines can be tied based on a shared seed s. At any point in processing the stream(s), current results can be aggregated by summing the d-dimensional vectors for each word, from each machine. 4 Experiments Similar to the experiments of Ravichandran et al. (2005), we evaluated the fidelity of signature generation in the context of calculating distributional similarity between words across a large text collection: in our case, articles taken from the NYTimes portion of the Gigaword corpus (Graff, 2003). The collection was processed as a stream, sentence by sentence, using bigram fea233 d 16 32 64 128 256 SLSH 0.2885 0.2112 0.1486 0.1081 0.0769 LSH 0.2892 0.2095 0.1506 0.1083 0.0755 Table 1: Mean absolute error when using signatures generated online (StreamingLSH), compared to offline (LSH). tures. This gave a stream of 773,185,086 tokens, with 1,138,46</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
</authors>
<title>Succinct approximate counting of skewed data.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="1277" citStr="Talbot, 2009" startWordPosition="194" endWordPosition="196">mation error, and with lower memory requirements than when using the standard offline technique. 1 Introduction There has been a surge of interest in adapting results from the streaming algorithms community to problems in processing large text collections. The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statistics of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that</context>
</contexts>
<marker>Talbot, 2009</marker>
<rawString>David Talbot. 2009. Succinct approximate counting of skewed data. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic Counting with Randomized Storage.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic Counting with Randomized Storage. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming Pointwise Mutual Information.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming Pointwise Mutual Information. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>