<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.991271">
Exploring Content Models for Multi-Document Summarization
</title>
<author confidence="0.56407">
Aria Haghighi Lucy Vanderwende
</author>
<address confidence="0.546954">
UC Berkeley, CS Division Microsoft Research
</address>
<email confidence="0.988207">
aria42@cs.berkeley.edu Lucy.Vanderwende@microsoft.com
</email>
<sectionHeader confidence="0.993653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990225">
We present an exploration of generative prob-
abilistic models for multi-document summa-
rization. Beginning with a simple word fre-
quency based model (Nenkova and Vander-
wende, 2005), we construct a sequence of
models each injecting more structure into the
representation of document set content and ex-
hibiting ROUGE gains along the way. Our
final model, HIERSUM, utilizes a hierarchi-
cal LDA-style model (Blei et al., 2004) to
represent content specificity as a hierarchy of
topic vocabulary distributions. At the task
of producing generic DUC-style summaries,
HIERSUM yields state-of-the-art ROUGE per-
formance and in pairwise user evaluation
strongly outperforms Toutanova et al. (2007)’s
state-of-the-art discriminative system. We
also explore HIERSUM’s capacity to produce
multiple ‘topical summaries’ in order to facil-
itate content discovery and navigation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823">
Over the past several years, there has been much in-
terest in the task of multi-document summarization.
In the common Document Understanding Confer-
ence (DUC) formulation of the task, a system takes
as input a document set as well as a short descrip-
tion of desired summary focus and outputs a word
length limited summary.1 To avoid the problem of
generating cogent sentences, many systems opt for
an extractive approach, selecting sentences from the
document set which best reflect its core content.2
</bodyText>
<footnote confidence="0.973584">
1In this work, we ignore the summary focus. Here, the word
topic will refer to elements of our statistical model rather than
summary focus.
2Note that sentence extraction does not solve the problem of
selecting and ordering summary sentences to form a coherent
</footnote>
<bodyText confidence="0.999916310344828">
There are several approaches to modeling docu-
ment content: simple word frequency-based meth-
ods (Luhn, 1958; Nenkova and Vanderwende,
2005), graph-based approaches (Radev, 2004; Wan
and Yang, 2006), as well as more linguistically mo-
tivated techniques (Mckeown et al., 1999; Leskovec
et al., 2005; Harabagiu et al., 2007). Another strand
of work (Barzilay and Lee, 2004; Daum´e III and
Marcu, 2006; Eisenstein and Barzilay, 2008), has
explored the use of structured probabilistic topic
models to represent document content. However, lit-
tle has been done to directly compare the benefit of
complex content models to simpler surface ones for
generic multi-document summarization.
In this work we examine a series of content
models for multi-document summarization and ar-
gue that LDA-style probabilistic topic models (Blei
et al., 2003) can offer state-of-the-art summariza-
tion quality as measured by automatic metrics (see
section 5.1) and manual user evaluation (see sec-
tion 5.2). We also contend that they provide con-
venient building blocks for adding more structure
to a summarization model. In particular, we uti-
lize a variation of the hierarchical LDA topic model
(Blei et al., 2004) to discover multiple specific ‘sub-
topics’ within a document set. The resulting model,
HIERSUM (see section 3.4), can produce general
summaries as well as summaries for any of the
learned sub-topics.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.8668532">
The task we will consider is extractive multi-
document summarization. In this task we assume
a document collection D consisting of documents
Di, ... , D,,, describing the same (or closely related)
narrative (Lapata, 2003).
</bodyText>
<page confidence="0.944667">
362
</page>
<note confidence="0.8926375">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362–370,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999098">
set of events. Our task will be to propose a sum-
mary S consisting of sentences in D totaling at most
L words.3 Here as in much extractive summariza-
tion, we will view each sentence as a bag-of-words
or more generally a bag-of-ngrams (see section 5.1).
The most prevalent example of this data setting is
document clusters found on news aggregator sites.
</bodyText>
<subsectionHeader confidence="0.960363">
2.1 Automated Evaluation
</subsectionHeader>
<bodyText confidence="0.9999898">
For model development we will utilize the DUC
2006 evaluation set4 consisting of 50 document sets
each with 25 documents; final evaluation will utilize
the DUC 2007 evaluation set (section 5).
Automated evaluation will utilize the standard
DUC evaluation metric ROUGE (Lin, 2004) which
represents recall over various n-grams statistics from
a system-generated summary against a set of human-
generated peer summaries.5 We compute ROUGE
scores with and without stop words removed from
peer and proposed summaries. In particular, we
utilize R-1 (recall against unigrams), R-2 (recall
against bigrams), and R-SU4 (recall against skip-4
bigrams)6. We present R-2 without stop words in the
running text, but full development results are pre-
sented in table 1. Official DUC scoring utilizes the
jackknife procedure and assesses significance using
bootstrapping resampling (Lin, 2004). In addition to
presenting automated results, we also present a user
evaluation in section 5.2.
</bodyText>
<sectionHeader confidence="0.992507" genericHeader="method">
3 Summarization Models
</sectionHeader>
<bodyText confidence="0.999961666666667">
We present a progression of models for multi-
document summarization. Inference details are
given in section 4.
</bodyText>
<subsectionHeader confidence="0.99789">
3.1 SumBasic
</subsectionHeader>
<bodyText confidence="0.999876142857143">
The SUMBASIC algorithm, introduced in Nenkova
and Vanderwende (2005), is a simple effective pro-
cedure for multi-document extractive summariza-
tion. Its design is motivated by the observation that
the relative frequency of a non-stop word in a doc-
ument set is a good predictor of a word appearing
in a human summary. In SUMBASIC, each sentence
</bodyText>
<footnote confidence="0.9994712">
3For DUC summarization tasks, L is typically 250.
4http://www-nlpir.nist.gov/projects/duc/data.html
5All words from peer and proposed summaries are lower-
cased and stemmed.
6Bigrams formed by skipping at most two words.
</footnote>
<equation confidence="0.8604325">
S is assigned a score reflecting how many high-
frequency words it contains,
1] Score(S) =
wES
</equation>
<bodyText confidence="0.999451333333333">
where PD(·) initially reflects the observed unigram
probabilities obtained from the document collection
D. A summary S is progressively built by adding
the highest scoring sentence according to (1).7
In order to discourage redundancy, the words
in the selected sentence are updated PDnew (w) a
</bodyText>
<equation confidence="0.904726">
Pold
D (w)2. Sentences are selected in this manner un-
</equation>
<bodyText confidence="0.99869632">
til the summary word limit has been reached.
Despite its simplicity, SUMBASIC yields 5.3 R-2
without stop words on DUC 2006 (see table 1).8 By
comparison, the highest-performing ROUGE sys-
tem at the DUC 2006 evaluation, SUMFOCUS, was
built on top of SUMBASIC and yielded a 6.0, which
is not a statistically significant improvement (Van-
derwende et al., 2007).9
Intuitively, SUMBASIC is trying to select a sum-
mary which has sentences where most words have
high likelihood under the document set unigram dis-
tribution. One conceptual problem with this objec-
tive is that it inherently favors repetition of frequent
non-stop words despite the ‘squaring’ update. Ide-
ally, a summarization criterion should be more recall
oriented, penalizing summaries which omit moder-
ately frequent document set words and quickly di-
minishing the reward for repeated use of word.
Another more subtle shortcoming is the use of the
raw empirical unigram distribution to represent con-
tent significance. For instance, there is no distinc-
tion between a word which occurs many times in the
same document or the same number of times across
several documents. Intuitively, the latter word is
more indicative of significant document set content.
</bodyText>
<subsectionHeader confidence="0.998361">
3.2 KLSum
</subsectionHeader>
<bodyText confidence="0.999409">
The KLSUM algorithm introduces a criterion for se-
lecting a summary S given document collection D,
</bodyText>
<equation confidence="0.849415">
KL(PDJJPS) (2)
</equation>
<footnote confidence="0.9982595">
7Note that sentence order is determined by the order in
which sentences are selected according to (1).
8This result is presented as 0.053 with the official ROUGE
scorer (Lin, 2004). Results here are scaled by 1,000.
9To be fair obtaining statistical significance in ROUGE
scores is quite difficult.
</footnote>
<equation confidence="0.98726375">
1
|S|PD(w) (1)
S∗ = min
S:words(S)≤L
</equation>
<page confidence="0.997674">
363
</page>
<figureCaption confidence="0.984044666666667">
Figure 1: Graphical model depiction of TOPIC-
SUM model (see section 3.3). Note that many hyper-
parameter dependencies are omitted for compactness.
</figureCaption>
<bodyText confidence="0.999191333333333">
where PS is the empirical unigram distribution of
the candidate summary S and KL(P Q) repre-
sents the Kullback-Lieber (KL) divergence given by
</bodyText>
<equation confidence="0.3919295">
Ew P(w) log P (w)
Q(w).10 This quantity represents the
</equation>
<bodyText confidence="0.9998744375">
divergence between the true distribution P (here the
document set unigram distribution) and the approx-
imating distribution Q (the summary distribution).
This criterion casts summarization as finding a set
of summary sentences which closely match the doc-
ument set unigram distribution. Lin et al. (2006)
propose a related criterion for robust summarization
evaluation, but to our knowledge this criteria has
been unexplored in summarization systems. We ad-
dress optimizing equation (2) as well as summary
sentence ordering in section 4.
KLSUM yields 6.0 R-2 without stop words, beat-
ing SUMBASIC but not with statistical significance. It
is worth noting however that KLSUM’s performance
matches SUMFOCUS (Vanderwende et al., 2007), the
highest R-2 performing system at DUC 2006.
</bodyText>
<subsectionHeader confidence="0.99733">
3.3 TopicSum
</subsectionHeader>
<bodyText confidence="0.883811777777778">
As mentioned in section 3.2, the raw unigram dis-
tribution PD(·) may not best reflect the content of
D for the purpose of summary extraction. We
propose TOPICSUM, which uses a simple LDA-like
topic model (Blei et al., 2003) similar to Daum´e
III and Marcu (2006) to estimate a content distribu-
10In order to ensure finite values of KL-divergence we
smoothe PS(·) so that it has a small amount of mass on all doc-
ument set words.
</bodyText>
<table confidence="0.9988335">
System ROUGE -stop R-1 ROUGE all
R-1 R-2 R-SU4 R-2 R-SU4
SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3
KLSUM 30.6 6.0 8.9 38.9 8.3 13.7
TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6
HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3
</table>
<tableCaption confidence="0.99467625">
Table 1: ROUGE results on DUC2006 for models pre-
sented in section 3. Results in bold represent results sta-
tistically significantly different from SUMBASIC in the
appropriate metric.
</tableCaption>
<bodyText confidence="0.994220714285714">
tion for summary extraction.11 We extract summary
sentences as before using the KLSUM criterion (see
equation (2)), plugging in a learned content distribu-
tion in place of the raw unigram distribution.
First, we describe our topic model (see figure 1)
which generates a collection of document sets. We
assume a fixed vocabulary V :12
</bodyText>
<listItem confidence="0.998479333333333">
1. Draw a background vocabulary distribution OB
from DIRICHLET(V,AB) shared across docu-
ment collections13 representing the background
distribution over vocabulary words. This distri-
bution is meant to flexibly model stop words
which do not contribute content. We will refer
to this topic as BACKGROUND.
2. For each document set D, we draw a content
distribution OC from DIRICHLET(V,AC) repre-
senting the significant content of D that we
wish to summarize. We will refer to this topic
as CONTENT.
3. For each document D in D, we draw a
document-specific vocabulary distribution OD
from DIRICHLET(V,AD) representing words
which are local to a single document, but do
not appear across several documents. We will
refer to this topic as DOCSPECIFIC.
</listItem>
<bodyText confidence="0.9194637">
11A topic model is a probabilistic generative process that gen-
erates a collection of documents using a mixture of topic vo-
cabulary distributions (Steyvers and Griffiths, 2007). Note this
usage of topic is unrelated to the summary focus given for doc-
ument collections; this information is ignored by our models.
12In contrast to previous models, stop words are not removed
in pre-processing.
13DIRICHLET(V,A) represents the symmetric Dirichlet
prior distribution over V each with a pseudo-count of A. Con-
crete pseudo-count values will be given in section 4.
</bodyText>
<figure confidence="0.9925852">
OB
Document Set
Document
Word
Sentence
W
Z
Ot
OD
Oc
364
General Content Topic 0Cr
{ star: 0.21, wars: 0.15, phantom: 0.10, ... )
Specific Content Topic �C, &amp;quot;Financial&amp;quot;
{ $: 0.39, million: 0.15, record: 0.8, ... )
Specific Content Topic &amp;quot;Merchandise&amp;quot;
�C�
{ toys: 0.22, spend: 0.18, sell: 0.10, ... )
Specific Content Topic &amp;quot;Fans&amp;quot;
�C�
{ fans: 0.16, line: 0.12, film: 0.09, ... )
OB
Document Set
Document
Sentence
Word
VG
W
0C1 ......... OCK
Z
oCc
ZS
y)T
OD
(a) Content Distributions (b) HIERSUM Graphical Model
</figure>
<figureCaption confidence="0.9785655">
Figure 2: (a): Examples of general versus specific content distributions utilized by HIERSUM (see section 3.4). The
general content distribution φC,, will be used throughout a document collection and represents core concepts in a
story. The specific content distributions represent topical ‘sub-stories’ with vocabulary tightly clustered together but
consistently used across documents. Quoted names of specific topics are given manually to facilitate interpretation. (b)
Graphical model depiction of the HIERSUM model (see section 3.4). Similar to the TOPICSUM model (see section 3.3)
except for adding complexity in the content hierarchy as well as sentence-specific prior distributions between general
and specific content topics (early sentences should have more general content words). Several dependencies are
missing from this depiction; crucially, each sentence’s specific topic ZS depends on the last sentence’s ZS.
</figureCaption>
<bodyText confidence="0.971695428571429">
4. For each sentence S of each document
D, draw a distribution ψT over topics
(CONTENT, DOCSPECIFIC, BACKGROUND)
from a Dirichlet prior with pseudo-counts
(1.0, 5.0,10.0).14 For each word position in
the sentence, we draw a topic Z from ψT,
and a word W from the topic distribution Z
indicates.
Our intent is that φC represents the core con-
tent of a document set. Intuitively, φC does
not include words which are common amongst
several document collections (modeled with the
BACKGROUND topic), or words which don’t appear
across many documents (modeled with the DOCSPE-
CIFIC topic). Also, because topics are tied together
at the sentence level, words which frequently occur
with other content words are more likely to be con-
sidered content words.
We ran our topic model over the DUC 2006
document collections and estimated the distribution
φC(·) for each document set.15 Then we extracted
</bodyText>
<footnote confidence="0.65134375">
14The different pseudo-counts reflect the intuition that most
of the words in a document come from the BACKGROUND and
DOCSPECIFIC topics.
15While possible to obtain the predictive posterior CON-
</footnote>
<bodyText confidence="0.972047">
a summary using the KLSUM criterion with our es-
timated φC in place of the the raw unigram distribu-
tion. Doing so yielded 6.3 R-2 without stop words
(see TOPICSUM in table 1); while not a statistically
significant improvement over KLSUM, it is our first
model which outperforms SUMBASIC with statistical
significance.
Daum´e III and Marcu (2006) explore a topic
model similar to ours for query-focused multi-
document summarization.16 Crucially however,
Daum´e III and Marcu (2006) selected sentences with
the highest expected number of CONTENT words.17
We found that in our model using this extraction
criterion yielded 5.3 R-2 without stop words, sig-
nificantly underperforming our TOPICSUM model.
One reason for this may be that Daum´e III and
Marcu (2006)’s criterion encourages selecting sen-
tences which have words that are confidently gener-
ated by the CONTENT distribution, but not necessar-
ily sentences which contain a plurality of it’s mass.
TENT distribution by analytically integrating over φC (Blei et
al., 2003), doing so gave no benefit.
</bodyText>
<footnote confidence="0.9656874">
16Daum´e III and Marcu (2006) note their model could be
used outside of query-focused summarization.
17This is phrased as selecting the sentence which has the
highest posterior probability of emitting CONTENT topic
words, but this is equivalent.
</footnote>
<page confidence="0.991295">
365
</page>
<table confidence="0.749745796296296">
(a) HIERSUM output
The French government
Saturday announced sev-
eral emergency measures
to support the jobless
people, including sending
an additional 500 million
franc (84 million U.S. dol-
lars) unemployment aid
package. The unem-
ployment rate in France
dropped by 0.3 percent
to stand at 12.4 percent
in November, said the
Ministry of Employment
Tuesday.
(b) PYTHY output
Several hundred people
took part in the demon-
stration here today against
the policies of the world’s
most developed nations.
The 12.5 percent unem-
ployment rate is haunt-
ing the Christmas sea-
son in France as militants
and unionists staged sev-
eral protests over the past
week against unemploy-
ment.
(c) Ref output
High unemployment is
France’s main economic
problem, despite recent
improvements. A top
worry of French people,
it is a factor affecting
France’s high suicide rate.
Long-term unemployment
causes social exclusion
and threatens France’s
social cohesion.
(d) Reference Unigram Coverage
word Ref PYTHY HIERSUM
unemployment 8 9 10
france’s 6 1 4
francs 4 0 1
high 4 1 2
economic 2 0 1
french 2 1 3
problem 2 0 1
benefits 2 0 0
social 2 0 2
jobless 2 1 2
</table>
<tableCaption confidence="0.993047">
Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two
sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent
non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries.
Note that many content words in the reference summary absent from PYTHY’s proposal are present in HIERSUM’s.
</tableCaption>
<subsectionHeader confidence="0.929329">
3.4 HIERSUM
</subsectionHeader>
<bodyText confidence="0.923037196078431">
Previous sections have treated the content of a doc-
ument set as a single (perhaps learned) unigram dis-
tribution. However, as Barzilay and Lee (2004) ob-
serve, the content of document collections is highly
structured, consisting of several topical themes, each
with its own vocabulary and ordering preferences.
For concreteness consider the DUC 2006 docu-
ment collection describing the opening of Star Wars:
Episode 1 (see figure 2(a)).
While there are words which indicate the general
content of this document collection (e.g. star, wars),
there are several sub-stories with their own specific
vocabulary. For instance, several documents in this
collection spend a paragraph or two talking about
the financial aspect of the film’s opening and use a
specific vocabulary there (e.g. $, million, record). A
user may be interested in general content of a docu-
ment collection or, depending on his or her interests,
one or more of the sub-stories. We choose to adapt
our topic modeling approach to allow modeling this
aspect of document set content.
Rather than drawing a single CONTENT distribu-
tion 0C for a document collection, we now draw
a general content distribution 0C0 from DIRICH-
LET(V,AG) as well as specific content distribu-
tions 0Ci for i = 1, ... , K each from DIRICH-
LET(V,AS).18 Our intent is that 0C0 represents the
18We choose K=3 in our experiments, but one could flexibly
general content of the document collection and each
0Ci represents specific sub-stories.
As with TOPICSUM, each sentence
has a distribution ψT over topics
(BACKGROUND, DOCSPECIFIC, CONTENT). When
BACKGROUND or DOCSPECIFIC topics are chosen,
the model works exactly as in TOPICSUM. However
when the CONTENT topic is drawn, we must decide
whether to emit a general content word (from 0C0)
or from one of the specific content distributions
(from one of 0Ci for i = 1, ... , K). The generative
story of TOPICSUM is altered as follows in this case:
• General or Specific? We must first decide
whether to use a general or specific content
word. Each sentence draws a binomial distribu-
tion ψG determining whether a CONTENT word
in the sentence will be drawn from the general
or a specific topic distribution. Reflecting the
intuition that the earlier sentences in a docu-
ment19 describe the general content of a story,
we bias ψG to be drawn from BETA(5,2), pre-
ferring general content words, and every later
sentence from BETA(1,2).20
</bodyText>
<listItem confidence="0.995395">
• What Specific Topic? If ψG decides we are
</listItem>
<footnote confidence="0.970902">
choose K as Blei et al. (2004) does.
19In our experiments, the first 5 sentences.
20BETA(a,b) represents the beta prior over binomial random
variables with a and b being pseudo-counts for the first and sec-
ond outcomes respectively.
</footnote>
<page confidence="0.998798">
366
</page>
<bodyText confidence="0.999889325">
emitting a topic specific content word, we must
decide which of 0Cl, ... , 0CK to use. In or-
der to ensure tight lexical cohesion amongst the
specific topics, we assume that each sentence
draws a single specific topic ZS used for every
specific content word in that sentence. Reflect-
ing intuition that adjacent sentences are likely
to share specific content vocabulary, we uti-
lize a ‘sticky’ HMM as in Barzilay and Lee
(2004) over the each sentences’ ZS. Con-
cretely, ZS for the first sentence in a docu-
ment is drawn uniformly from 1, ... , K, and
each subsequent sentence’s ZS will be identi-
cal to the previous sentence with probability Q,
and with probability 1 − Q we select a succes-
sor topic from a learned transition distribution
amongst 1, ... , K.21
Our intent is that the general content distribution
0C0 now prefers words which not only appear in
many documents, but also words which appear con-
sistently throughout a document rather than being
concentrated in a small number of sentences. Each
specific content distribution 0Ci is meant to model
topics which are used in several documents but tend
to be used in concentrated locations.
HIERSUM can be used to extract several kinds
of summaries. It can extract a general summary
by plugging 0C0 into the KLSUM criterion. It can
also produce topical summaries for the learned spe-
cific topics by extracting a summary over each 0Ci
distribution; this might be appropriate for a user
who wants to know more about a particular sub-
story. While we found the general content distribu-
tion (from 0Co) to produce the best single summary,
we experimented with utilizing topical summaries
for other summarization tasks (see section 6.1). The
resulting system, HIERSUM yielded 6.4 R-2 without
stop words. While not a statistically significant im-
provement in ROUGE over TOPICSUM, we found the
summaries to be noticeably improved.
</bodyText>
<sectionHeader confidence="0.999042" genericHeader="method">
4 Inference and Model Details
</sectionHeader>
<bodyText confidence="0.996602333333334">
Since globally optimizing the KLSUM criterion in
equation (equation (2)) is exponential in the total
number of sentences in a document collection, we
21We choose σ = 0.75 in our experiments.
opted instead for a simple approximation where sen-
tences are greedily added to a summary so long as
they decrease KL-divergence. We attempted more
complex inference procedures such as McDonald
(2007), but these attempts only yielded negligible
performance gains. All summary sentence order-
ing was determined as follows: each sentence in the
proposed summary was assigned a number in [0, 1]
reflecting its relative sentence position in its source
document, and sorted by this quantity.
All topic models utilize Gibbs sampling for in-
ference (Griffiths, 2002; Blei et al., 2004). In gen-
eral for concentration parameters, the more specific
a distribution is meant to be, the smaller its con-
centration parameter. Accordingly for TOPICSUM,
AG = AD = 1 and AC = 0.1. For HIERSUM we
used AG = 0.1 and AS = 0.01. These parameters
were minimally tuned (without reference to ROUGE
results) in order to ensure that all topic distribution
behaved as intended.
</bodyText>
<sectionHeader confidence="0.996293" genericHeader="method">
5 Formal Experiments
</sectionHeader>
<bodyText confidence="0.99997655">
We present formal experiments on the DUC 2007
data main summarization task, proposing a general
summary of at most 250 words22 which will be eval-
uated automatically and manually in order to simu-
late as much as possible the DUC evaluation envi-
ronment.23 DUC 2007 consists of 45 document sets,
each consisting of 25 documents and 4 human refer-
ence summaries.
We primarily evaluate the HIERSUM model, ex-
tracting a single summary from the general con-
tent distribution using the KLSUM criterion (see sec-
tion 3.2). Although the differences in ROUGE be-
tween HIERSUM and TOPICSUM were minimal, we
found HIERSUM summary quality to be stronger.
In order to provide a reference for ROUGE
and manual evaluation results, we compare against
PYTHY, a state-of-the-art supervised sentence ex-
traction summarization system. PYTHY uses human-
generated summaries in order to train a sentence
ranking system which discriminatively maximizes
</bodyText>
<footnote confidence="0.58531">
22Since the ROUGE evaluation metric is recall-oriented, it is
always advantageous - with respect to ROUGE - to use all 250
words.
23Although the DUC 2007 main summarization task provides
an indication of user intent through topic focus queries, we ig-
nore this aspect of the data.
</footnote>
<page confidence="0.989744">
367
</page>
<table confidence="0.999423">
System ROUGE w/o stop ROUGE w/ stop
R-1 R-2 R-SU4 R-1 R-2 R-SU4
HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3
HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7
PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5
PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8
</table>
<tableCaption confidence="0.998734666666667">
Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1). While HIER-
SUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stop
words), HIERSUM bigram’s performance is comparable and statistically no worse.
</tableCaption>
<bodyText confidence="0.986800375">
ROUGE scores. PYTHY uses several features to
rank sentences including several variations of the
SUMBASIC score (see section 3.1). At DUC 2007,
PYTHY was ranked first overall in automatic ROUGE
evaluation and fifth in manual content judgments.
As PYTHY utilizes a sentence simplification com-
ponent, which we do not, we also compare against
PYTHY without sentence simplification.
</bodyText>
<subsectionHeader confidence="0.990875">
5.1 ROUGE Evaluation
</subsectionHeader>
<bodyText confidence="0.995434333333333">
ROUGE results comparing variants of HIERSUM and
PYTHY are given in table 3. The HIERSUM system
as described in section 3.4 yields 7.3 R-2 without
stop words, falling significantly short of the 8.7 that
PYTHY without simplification yields. Note that R-2
is a measure of bigram recall and HIERSUM does not
represent bigrams whereas PYTHY includes several
bigram and higher order n-gram statistics.
In order to put HIERSUM and PYTHY on equal-
footing with respect to R-2, we instead ran HIER-
SUM with each sentence consisting of a bag of bi-
grams instead of unigrams.24 All the details of the
model remain the same. Once a general content
distribution over bigrams has been determined by
hierarchical topic modeling, the KLSUM criterion
is used as before to extract a summary. This sys-
tem, labeled HIERSUM bigram in table 3, yields 9.3
R-2 without stop words, significantly outperform-
ing HIERSUM unigram. This model outperforms
PYTHY with and without sentence simplification, but
not with statistical significance. We conclude that
both PYTHY variants and HIERSUM bigram are com-
parable with respect to ROUGE performance.
24Note that by doing topic modeling in this way over bi-
grams, our model becomes degenerate as it can generate incon-
sistent bags of bigrams. Future work may look at topic models
over n-grams as suggested by Wang et al. (2007).
</bodyText>
<table confidence="0.9982356">
Question PYTHY HIERSUM
Overall 20 49
Non-Redundancy 21 48
Coherence 15 54
Focus 28 41
</table>
<tableCaption confidence="0.9967168">
Table 4: Results of manual user evaluation (see sec-
tion 5.2). 15 participants expressed 69 pairwise prefer-
ences between HIERSUM and PYTHY. For all attributes,
HIERSUM outperforms PYTHY; all results are statisti-
cally significant as determined by pairwise t-test.
</tableCaption>
<subsectionHeader confidence="0.995805">
5.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.958420428571429">
In order to obtain a more accurate measure of sum-
mary quality, we performed a simple user study. For
each document set in the DUC 2007 collection, a
user was given a reference summary, a PYTHY sum-
mary, and a HIERSUM summary;25 note that the orig-
inal documents in the set were not provided to the
user, only a reference summary. For this experiment
we use the bigram variant of HIERSUM and compare
it to PYTHY without simplification so both systems
have the same set of possible output summaries.
The reference summary for each document set
was selected according to highest R-2 without stop
words against the remaining peer summaries. Users
were presented with 4 questions drawn from the
DUC manual evaluation guidelines:26 (1) Overall
quality: Which summary was better overall? (2)
Non-Redundancy: Which summary was less redun-
dant? (3) Coherence: Which summary was more
coherent? (4) Focus: Which summary was more
25The system identifier was of course not visible to the user.
The order of automatic summaries was determined randomly.
</bodyText>
<footnote confidence="0.6826115">
26http://www-nlpir.nist.gov/projects/duc/duc2007/quality-
questions.txt
</footnote>
<page confidence="0.993412">
368
</page>
<figureCaption confidence="0.96128625">
Figure 3: Using HIERSUM to organize content of document set into topics (see section 6.1). The sidebar gives key
phrases salient in each of the specific content topics in HIERSUM (see section 3.4). When a topic is clicked in the right
sidebar, the main frame displays an extractive ‘topical summary’ with links into document set articles. Ideally, a user
could use this interface to quickly find content in a document collection that matches their interest.
</figureCaption>
<bodyText confidence="0.9998815">
focused in its content, not conveying irrelevant de-
tails? The study had 16 users and each was asked
to compare five summary pairs, although some did
fewer. A total of 69 preferences were solicited. Doc-
ument collections presented to users were randomly
selected from those evaluated fewest.
As seen in table 5.2, HIERSUM outperforms
PYTHY under all questions. All results are statis-
tically significant as judged by a simple pairwise
t-test with 95% confidence. It is safe to conclude
that users in this study strongly preferred the HIER-
SUM summaries over the PYTHY summaries.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999748125">
While it is difficult to qualitatively compare one
summarization system over another, we can broadly
characterize HIERSUM summaries compared to some
of the other systems discussed. For example out-
put from HIERSUM and PYTHY see table 2. On the
whole, HIERSUM summaries appear to be signifi-
cantly less redundant than PYTHY and moderately
less redundant than SUMBASIC. The reason for this
might be that PYTHY is discriminatively trained to
maximize ROUGE which does not directly penalize
redundancy. Another tendency is for HIERSUM to se-
lect longer sentences typically chosen from an early
sentence in a document. As discussed in section 3.4,
HIERSUM is biased to consider early sentences in
documents have a higher proportion of general con-
tent words and so this tendency is to be expected.
</bodyText>
<subsectionHeader confidence="0.992809">
6.1 Content Navigation
</subsectionHeader>
<bodyText confidence="0.999960842105263">
A common concern in multi-document summariza-
tion is that without any indication of user interest or
intent providing a single satisfactory summary to a
user may not be feasible. While many variants of
the general summarization task have been proposed
which utilize such information (Vanderwende et al.,
2007; Nastase, 2008), this presupposes that a user
knows enough of the content of a document collec-
tion in order to propose a query.
As Leuski et al. (2003) and Branavan et al. (2007)
suggest, a document summarization system should
facilitate content discovery and yield summaries rel-
evant to a user’s interests. We may use HIERSUM in
order to facilitate content discovery via presenting
a user with salient words or phrases from the spe-
cific content topics parametrized by 0C1, ... , 0CK
(for an example see figure 3). While these topics are
not adaptive to user interest, they typically reflect
lexically coherent vocabularies.
</bodyText>
<sectionHeader confidence="0.937937" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.993356888888889">
In this paper we have presented an exploration of
content models for multi-document summarization
and demonstrated that the use of structured topic
models can benefit summarization quality as mea-
sured by automatic and manual metrics.
Acknowledgements The authors would like to
thank Bob Moore, Chris Brockett, Chris Quirk, and
Kristina Toutanova for their useful discussions as
well as the reviewers for their helpful comments.
</bodyText>
<page confidence="0.998717">
369
</page>
<sectionHeader confidence="0.995885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849932432432">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. JMLR.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
NIPS.
S.R.K. Branavan, Pawan Deshpande, and Regina Barzi-
lay. 2007. Generating a table-of-contents. In ACL.
Hal Daum´e III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP-
SIGDAT.
Thomas Griffiths. 2002. Gibbs sampling in the genera-
tive model of latent dirichlet allocation.
Sanda Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2007. Satisfying information needs with multi-
document summaries. Inf. Process. Manage., 43(6).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL.
Jurij Leskovec, Natasa Milic-frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the se-
mantic graph coverage and learning of document ex-
tracts. In In AAAI 05.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: Interactive multi-document summarization. In
ACL.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In HLT-NAACL.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In ECIR.
Kathleen R. Mckeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by re-
formulation: Progress and prospects. In In Proceed-
ings of AAAI-99.
Vivi Nastase. 2008. Topic-driven multi-document sum-
marization with encyclopedic knowledge and spread-
ing activation. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, Mi-
crosoft Research.
Dragomir R. Radev. 2004. Lexrank: graph-based cen-
trality as salience in text summarization. Journal of
Artificial Intelligence Research (JAIR.
M. Steyvers and T. Griffiths, 2007. Probabilistic Topic
Models.
Kristina Toutanova, Chris Brockett, Michael Gamon Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In DUC.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lexi-
cal expansion. volume 43.
Xiaojun Wan and Jianwu Yang. 2006. Improved affinity
graph based multi-document summarization. In HLT-
NAACL.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In ICDM.
</reference>
<page confidence="0.998263">
370
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862549">
<title confidence="0.999952">Exploring Content Models for Multi-Document Summarization</title>
<author confidence="0.997641">Aria Haghighi Lucy Vanderwende</author>
<affiliation confidence="0.973959">UC Berkeley, CS Division Microsoft Research</affiliation>
<email confidence="0.935334">aria42@cs.berkeley.eduLucy.Vanderwende@microsoft.com</email>
<abstract confidence="0.997516904761905">We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system. We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2195" citStr="Barzilay and Lee, 2004" startWordPosition="322" endWordPosition="325">is work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see sec</context>
<context position="16935" citStr="Barzilay and Lee (2004)" startWordPosition="2666" endWordPosition="2669">2 0 2 jobless 2 1 2 Table 2: Example summarization output for systems compared in section 5.2. (a), (b), and (c) represent the first two sentences output from PYTHY, HIERSUM, and reference summary respectively. In (d), we present the most frequent non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries. Note that many content words in the reference summary absent from PYTHY’s proposal are present in HIERSUM’s. 3.4 HIERSUM Previous sections have treated the content of a document set as a single (perhaps learned) unigram distribution. However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. For concreteness consider the DUC 2006 document collection describing the opening of Star Wars: Episode 1 (see figure 2(a)). While there are words which indicate the general content of this document collection (e.g. star, wars), there are several sub-stories with their own specific vocabulary. For instance, several documents in this collection spend a paragraph or two talking about the financial aspect of the film’s opening and use a s</context>
<context position="19899" citStr="Barzilay and Lee (2004)" startWordPosition="3161" endWordPosition="3164">19In our experiments, the first 5 sentences. 20BETA(a,b) represents the beta prior over binomial random variables with a and b being pseudo-counts for the first and second outcomes respectively. 366 emitting a topic specific content word, we must decide which of 0Cl, ... , 0CK to use. In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence. Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a ‘sticky’ HMM as in Barzilay and Lee (2004) over the each sentences’ ZS. Concretely, ZS for the first sentence in a document is drawn uniformly from 1, ... , K, and each subsequent sentence’s ZS will be identical to the previous sentence with probability Q, and with probability 1 − Q we select a successor topic from a learned transition distribution amongst 1, ... , K.21 Our intent is that the general content distribution 0C0 now prefers words which not only appear in many documents, but also words which appear consistently throughout a document rather than being concentrated in a small number of sentences. Each specific content distri</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="2659" citStr="Blei et al., 2003" startWordPosition="392" endWordPosition="395"> linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics. 2 Experimental Setup The task we will consider </context>
<context position="9173" citStr="Blei et al., 2003" startWordPosition="1416" endWordPosition="1419"> criteria has been unexplored in summarization systems. We address optimizing equation (2) as well as summary sentence ordering in section 4. KLSUM yields 6.0 R-2 without stop words, beating SUMBASIC but not with statistical significance. It is worth noting however that KLSUM’s performance matches SUMFOCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006. 3.3 TopicSum As mentioned in section 3.2, the raw unigram distribution PD(·) may not best reflect the content of D for the purpose of summary extraction. We propose TOPICSUM, which uses a simple LDA-like topic model (Blei et al., 2003) similar to Daum´e III and Marcu (2006) to estimate a content distribu10In order to ensure finite values of KL-divergence we smoothe PS(·) so that it has a small amount of mass on all document set words. System ROUGE -stop R-1 ROUGE all R-1 R-2 R-SU4 R-2 R-SU4 SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3 KLSUM 30.6 6.0 8.9 38.9 8.3 13.7 TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6 HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3 Table 1: ROUGE results on DUC2006 for models presented in section 3. Results in bold represent results statistically significantly different from SUMBASIC in the appropriate metric. tion for summary </context>
<context position="14924" citStr="Blei et al., 2003" startWordPosition="2334" endWordPosition="2337">multidocument summarization.16 Crucially however, Daum´e III and Marcu (2006) selected sentences with the highest expected number of CONTENT words.17 We found that in our model using this extraction criterion yielded 5.3 R-2 without stop words, significantly underperforming our TOPICSUM model. One reason for this may be that Daum´e III and Marcu (2006)’s criterion encourages selecting sentences which have words that are confidently generated by the CONTENT distribution, but not necessarily sentences which contain a plurality of it’s mass. TENT distribution by analytically integrating over φC (Blei et al., 2003), doing so gave no benefit. 16Daum´e III and Marcu (2006) note their model could be used outside of query-focused summarization. 17This is phrased as selecting the sentence which has the highest posterior probability of emitting CONTENT topic words, but this is equivalent. 365 (a) HIERSUM output The French government Saturday announced several emergency measures to support the jobless people, including sending an additional 500 million franc (84 million U.S. dollars) unemployment aid package. The unemployment rate in France dropped by 0.3 percent to stand at 12.4 percent in November, said the </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="3012" citStr="Blei et al., 2004" startWordPosition="449" endWordPosition="452">mpare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics. 2 Experimental Setup The task we will consider is extractive multidocument summarization. In this task we assume a document collection D consisting of documents Di, ... , D,,, describing the same (or closely related) narrative (Lapata, 2003). 362 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362–370, Boulder, Colorado, June 2009. c�2009 Ass</context>
<context position="19269" citStr="Blei et al. (2004)" startWordPosition="3055" endWordPosition="3058">rative story of TOPICSUM is altered as follows in this case: • General or Specific? We must first decide whether to use a general or specific content word. Each sentence draws a binomial distribution ψG determining whether a CONTENT word in the sentence will be drawn from the general or a specific topic distribution. Reflecting the intuition that the earlier sentences in a document19 describe the general content of a story, we bias ψG to be drawn from BETA(5,2), preferring general content words, and every later sentence from BETA(1,2).20 • What Specific Topic? If ψG decides we are choose K as Blei et al. (2004) does. 19In our experiments, the first 5 sentences. 20BETA(a,b) represents the beta prior over binomial random variables with a and b being pseudo-counts for the first and second outcomes respectively. 366 emitting a topic specific content word, we must decide which of 0Cl, ... , 0CK to use. In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence. Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a ‘sticky’ HMM</context>
<context position="22142" citStr="Blei et al., 2004" startWordPosition="3530" endWordPosition="3533">e 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence. We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains. All summary sentence ordering was determined as follows: each sentence in the proposed summary was assigned a number in [0, 1] reflecting its relative sentence position in its source document, and sorted by this quantity. All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004). In general for concentration parameters, the more specific a distribution is meant to be, the smaller its concentration parameter. Accordingly for TOPICSUM, AG = AD = 1 and AC = 0.1. For HIERSUM we used AG = 0.1 and AS = 0.01. These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended. 5 Formal Experiments We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order to simulate as much </context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Pawan Deshpande</author>
<author>Regina Barzilay</author>
</authors>
<title>Generating a table-of-contents.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29838" citStr="Branavan et al. (2007)" startWordPosition="4777" endWordPosition="4780">sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies. Conclusion In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured </context>
</contexts>
<marker>Branavan, Deshpande, Barzilay, 2007</marker>
<rawString>S.R.K. Branavan, Pawan Deshpande, and Regina Barzilay. 2007. Generating a table-of-contents. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In EMNLPSIGDAT.</booktitle>
<contexts>
<context position="2255" citStr="Eisenstein and Barzilay, 2008" startWordPosition="331" endWordPosition="334"> topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient buil</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In EMNLPSIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
</authors>
<title>Gibbs sampling in the generative model of latent dirichlet allocation.</title>
<date>2002</date>
<contexts>
<context position="22122" citStr="Griffiths, 2002" startWordPosition="3528" endWordPosition="3529">ent collection, we 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence. We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains. All summary sentence ordering was determined as follows: each sentence in the proposed summary was assigned a number in [0, 1] reflecting its relative sentence position in its source document, and sorted by this quantity. All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004). In general for concentration parameters, the more specific a distribution is meant to be, the smaller its concentration parameter. Accordingly for TOPICSUM, AG = AD = 1 and AC = 0.1. For HIERSUM we used AG = 0.1 and AS = 0.01. These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended. 5 Formal Experiments We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order </context>
</contexts>
<marker>Griffiths, 2002</marker>
<rawString>Thomas Griffiths. 2002. Gibbs sampling in the generative model of latent dirichlet allocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<title>Satisfying information needs with multidocument summaries.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="2147" citStr="Harabagiu et al., 2007" startWordPosition="314" endWordPosition="317"> set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see </context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2007</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2007. Satisfying information needs with multidocument summaries. Inf. Process. Manage., 43(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3453" citStr="Lapata, 2003" startWordPosition="520" endWordPosition="521">convenient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set. The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics. 2 Experimental Setup The task we will consider is extractive multidocument summarization. In this task we assume a document collection D consisting of documents Di, ... , D,,, describing the same (or closely related) narrative (Lapata, 2003). 362 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362–370, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics set of events. Our task will be to propose a summary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1). The most prevalent example of this data setting is document clusters found on news aggregator sites. 2.1 Automated Evaluation For model development we </context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurij Leskovec</author>
<author>Natasa Milic-frayling</author>
<author>Marko Grobelnik</author>
</authors>
<title>Impact of linguistic analysis on the semantic graph coverage and learning of document extracts.</title>
<date>2005</date>
<booktitle>In In AAAI 05.</booktitle>
<contexts>
<context position="2122" citStr="Leskovec et al., 2005" startWordPosition="310" endWordPosition="313">ences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured b</context>
</contexts>
<marker>Leskovec, Milic-frayling, Grobelnik, 2005</marker>
<rawString>Jurij Leskovec, Natasa Milic-frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the semantic graph coverage and learning of document extracts. In In AAAI 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>ineats: Interactive multi-document summarization.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29811" citStr="Leuski et al. (2003)" startWordPosition="4772" endWordPosition="4775">biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies. Conclusion In this paper we have presented an exploration of content models for multi-document summarization and demonstrated </context>
</contexts>
<marker>Leuski, Lin, Hovy, 2003</marker>
<rawString>Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003. ineats: Interactive multi-document summarization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Guihong Cao</author>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
</authors>
<title>An information-theoretic approach to automatic evaluation of summaries.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="8464" citStr="Lin et al. (2006)" startWordPosition="1303" endWordPosition="1306">epiction of TOPICSUM model (see section 3.3). Note that many hyperparameter dependencies are omitted for compactness. where PS is the empirical unigram distribution of the candidate summary S and KL(P Q) represents the Kullback-Lieber (KL) divergence given by Ew P(w) log P (w) Q(w).10 This quantity represents the divergence between the true distribution P (here the document set unigram distribution) and the approximating distribution Q (the summary distribution). This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution. Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems. We address optimizing equation (2) as well as summary sentence ordering in section 4. KLSUM yields 6.0 R-2 without stop words, beating SUMBASIC but not with statistical significance. It is worth noting however that KLSUM’s performance matches SUMFOCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006. 3.3 TopicSum As mentioned in section 3.2, the raw unigram distribution PD(·) may not best reflect the content of D for the pur</context>
</contexts>
<marker>Lin, Cao, Gao, Nie, 2006</marker>
<rawString>Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. ACL workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="4307" citStr="Lin, 2004" startWordPosition="655" endWordPosition="656">ummary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1). The most prevalent example of this data setting is document clusters found on news aggregator sites. 2.1 Automated Evaluation For model development we will utilize the DUC 2006 evaluation set4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5). Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system-generated summary against a set of humangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries. In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6. We present R-2 without stop words in the running text, but full development results are presented in table 1. Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004). In </context>
<context position="7660" citStr="Lin, 2004" startWordPosition="1179" endWordPosition="1180">raw empirical unigram distribution to represent content significance. For instance, there is no distinction between a word which occurs many times in the same document or the same number of times across several documents. Intuitively, the latter word is more indicative of significant document set content. 3.2 KLSum The KLSUM algorithm introduces a criterion for selecting a summary S given document collection D, KL(PDJJPS) (2) 7Note that sentence order is determined by the order in which sentences are selected according to (1). 8This result is presented as 0.053 with the official ROUGE scorer (Lin, 2004). Results here are scaled by 1,000. 9To be fair obtaining statistical significance in ROUGE scores is quite difficult. 1 |S|PD(w) (1) S∗ = min S:words(S)≤L 363 Figure 1: Graphical model depiction of TOPICSUM model (see section 3.3). Note that many hyperparameter dependencies are omitted for compactness. where PS is the empirical unigram distribution of the candidate summary S and KL(P Q) represents the Kullback-Lieber (KL) divergence given by Ew P(w) log P (w) Q(w).10 This quantity represents the divergence between the true distribution P (here the document set unigram distribution) and the ap</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proc. ACL workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal.</journal>
<contexts>
<context position="1934" citStr="Luhn, 1958" startWordPosition="284" endWordPosition="285">esired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a seri</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H.P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In ECIR.</booktitle>
<contexts>
<context position="21766" citStr="McDonald (2007)" startWordPosition="3473" endWordPosition="3474">.1). The resulting system, HIERSUM yielded 6.4 R-2 without stop words. While not a statistically significant improvement in ROUGE over TOPICSUM, we found the summaries to be noticeably improved. 4 Inference and Model Details Since globally optimizing the KLSUM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence. We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains. All summary sentence ordering was determined as follows: each sentence in the proposed summary was assigned a number in [0, 1] reflecting its relative sentence position in its source document, and sorted by this quantity. All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004). In general for concentration parameters, the more specific a distribution is meant to be, the smaller its concentration parameter. Accordingly for TOPICSUM, AG = AD = 1 and AC = 0.1. For HIERSUM we used AG = 0.1 and AS = 0</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R Mckeown</author>
<author>Judith L Klavans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Regina Barzilay</author>
<author>Eleazar Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: Progress and prospects. In</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99.</booktitle>
<contexts>
<context position="2099" citStr="Mckeown et al., 1999" startWordPosition="306" endWordPosition="309">proach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarizatio</context>
</contexts>
<marker>Mckeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>Kathleen R. Mckeown, Judith L. Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In In Proceedings of AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Topic-driven multi-document summarization with encyclopedic knowledge and spreading activation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="29675" citStr="Nastase, 2008" startWordPosition="4748" endWordPosition="4749">HIERSUM to select longer sentences typically chosen from an early sentence in a document. As discussed in section 3.4, HIERSUM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent voca</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Topic-driven multi-document summarization with encyclopedic knowledge and spreading activation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="1966" citStr="Nenkova and Vanderwende, 2005" startWordPosition="286" endWordPosition="289">ry focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-d</context>
<context position="5213" citStr="Nenkova and Vanderwende (2005)" startWordPosition="786" endWordPosition="789">t unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6. We present R-2 without stop words in the running text, but full development results are presented in table 1. Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004). In addition to presenting automated results, we also present a user evaluation in section 5.2. 3 Summarization Models We present a progression of models for multidocument summarization. Inference details are given in section 4. 3.1 SumBasic The SUMBASIC algorithm, introduced in Nenkova and Vanderwende (2005), is a simple effective procedure for multi-document extractive summarization. Its design is motivated by the observation that the relative frequency of a non-stop word in a document set is a good predictor of a word appearing in a human summary. In SUMBASIC, each sentence 3For DUC summarization tasks, L is typically 250. 4http://www-nlpir.nist.gov/projects/duc/data.html 5All words from peer and proposed summaries are lowercased and stemmed. 6Bigrams formed by skipping at most two words. S is assigned a score reflecting how many highfrequency words it contains, 1] Score(S) = wES where PD(·) in</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005. The impact of frequency on summarization. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research</journal>
<contexts>
<context position="2003" citStr="Radev, 2004" startWordPosition="292" endWordPosition="293">avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that </context>
</contexts>
<marker>Radev, 2004</marker>
<rawString>Dragomir R. Radev. 2004. Lexrank: graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR. M. Steyvers and T. Griffiths, 2007. Probabilistic Topic Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Chris Brockett</author>
<author>Michael Gamon Jagadeesh Jagarlamudi</author>
<author>Hisami Suzuki</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The pythy summarization system: Microsoft research at duc</title>
<date>2007</date>
<booktitle>In DUC.</booktitle>
<contexts>
<context position="878" citStr="Toutanova et al. (2007)" startWordPosition="117" endWordPosition="120">s for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system. We also explore HIERSUM’s capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation. 1 Introduction Over the past several years, there has been much interest in the task of multi-document summarization. In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive a</context>
</contexts>
<marker>Toutanova, Brockett, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>Kristina Toutanova, Chris Brockett, Michael Gamon Jagadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization system: Microsoft research at duc 2007. In DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
<author>Chris Brockett</author>
<author>Ani Nenkova</author>
</authors>
<title>Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion.</title>
<date>2007</date>
<volume>43</volume>
<contexts>
<context position="6502" citStr="Vanderwende et al., 2007" startWordPosition="992" endWordPosition="996">m the document collection D. A summary S is progressively built by adding the highest scoring sentence according to (1).7 In order to discourage redundancy, the words in the selected sentence are updated PDnew (w) a Pold D (w)2. Sentences are selected in this manner until the summary word limit has been reached. Despite its simplicity, SUMBASIC yields 5.3 R-2 without stop words on DUC 2006 (see table 1).8 By comparison, the highest-performing ROUGE system at the DUC 2006 evaluation, SUMFOCUS, was built on top of SUMBASIC and yielded a 6.0, which is not a statistically significant improvement (Vanderwende et al., 2007).9 Intuitively, SUMBASIC is trying to select a summary which has sentences where most words have high likelihood under the document set unigram distribution. One conceptual problem with this objective is that it inherently favors repetition of frequent non-stop words despite the ‘squaring’ update. Ideally, a summarization criterion should be more recall oriented, penalizing summaries which omit moderately frequent document set words and quickly diminishing the reward for repeated use of word. Another more subtle shortcoming is the use of the raw empirical unigram distribution to represent cont</context>
<context position="8889" citStr="Vanderwende et al., 2007" startWordPosition="1367" endWordPosition="1370">imating distribution Q (the summary distribution). This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution. Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems. We address optimizing equation (2) as well as summary sentence ordering in section 4. KLSUM yields 6.0 R-2 without stop words, beating SUMBASIC but not with statistical significance. It is worth noting however that KLSUM’s performance matches SUMFOCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006. 3.3 TopicSum As mentioned in section 3.2, the raw unigram distribution PD(·) may not best reflect the content of D for the purpose of summary extraction. We propose TOPICSUM, which uses a simple LDA-like topic model (Blei et al., 2003) similar to Daum´e III and Marcu (2006) to estimate a content distribu10In order to ensure finite values of KL-divergence we smoothe PS(·) so that it has a small amount of mass on all document set words. System ROUGE -stop R-1 ROUGE all R-1 R-2 R-SU4 R-2 R-SU4 SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3 KLSUM 30.6 6.0 8.9 </context>
<context position="29659" citStr="Vanderwende et al., 2007" startWordPosition="4744" endWordPosition="4747">. Another tendency is for HIERSUM to select longer sentences typically chosen from an early sentence in a document. As discussed in section 3.4, HIERSUM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexical</context>
</contexts>
<marker>Vanderwende, Suzuki, Brockett, Nenkova, 2007</marker>
<rawString>Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and Ani Nenkova. 2007. Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion. volume 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Improved affinity graph based multi-document summarization.</title>
<date>2006</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="2024" citStr="Wan and Yang, 2006" startWordPosition="294" endWordPosition="297">blem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilist</context>
</contexts>
<marker>Wan, Yang, 2006</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2006. Improved affinity graph based multi-document summarization. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="26012" citStr="Wang et al. (2007)" startWordPosition="4164" endWordPosition="4167">criterion is used as before to extract a summary. This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram. This model outperforms PYTHY with and without sentence simplification, but not with statistical significance. We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance. 24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams. Future work may look at topic models over n-grams as suggested by Wang et al. (2007). Question PYTHY HIERSUM Overall 20 49 Non-Redundancy 21 48 Coherence 15 54 Focus 28 41 Table 4: Results of manual user evaluation (see section 5.2). 15 participants expressed 69 pairwise preferences between HIERSUM and PYTHY. For all attributes, HIERSUM outperforms PYTHY; all results are statistically significant as determined by pairwise t-test. 5.2 Manual Evaluation In order to obtain a more accurate measure of summary quality, we performed a simple user study. For each document set in the DUC 2007 collection, a user was given a reference summary, a PYTHY summary, and a HIERSUM summary;25 n</context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In ICDM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>