<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002204">
<title confidence="0.9948725">
Semi-Automatic Acquisition of
Domain-Specific Translation Lexicons
</title>
<author confidence="0.998272">
Philip Resnik
</author>
<affiliation confidence="0.997221">
Dept. of Linguistics and UMIACS
University of Maryland
</affiliation>
<address confidence="0.883078">
College Park, MD 20742 USA
</address>
<email confidence="0.901648">
resnikOumiacs.umd.edu
</email>
<author confidence="0.975204">
I. Dan Melamed
</author>
<affiliation confidence="0.9978205">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.481038">
Philadelphia, PA 19104 USA
</address>
<email confidence="0.886926">
melamedOunagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.991076" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930857142857">
We investigate the utility of an algo-
rithm for translation lexicon acquisition
(SABLE), used previously on a very large
corpus to acquire general translation lexi-
cons, when that algorithm is applied to a
much smaller corpus to produce candidates
for domain-specific translation lexicons.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999747578947368">
Reliable translation lexicons are useful in many ap-
plications, such as cross-language text retrieval. Al-
though general purpose machine readable bilingual
dictionaries are sometimes available, and although
some methods for acquiring translation lexicons au-
tomatically from large corpora have been proposed,
less attention has been paid to the problem of ac-
quiring bilingual terminology specific to a domain,
especially given domain-specific parallel corpora of
only limited size.
In this paper, we investigate the utility of an algo-
rithm for translation lexicon acquisition (Melamed,
1997), used previously on a very large corpus to ac-
quire general translation lexicons, when that algo-
rithm is applied to a much smaller corpus to produce
candidates for domain-specific translation lexicons.
The goal is to produce material suitable for post-
processing in a lexicon acquisition process like the
following:
</bodyText>
<listItem confidence="0.985328625">
1. Run the automatic lexicon acquisition algo-
rithm on a domain-specific parallel corpus.
2. Automatically filter out &amp;quot;general usage&amp;quot; entries
that already appear in a machine readable dic-
tionary (MRD) or other general usage lexical
resources.
3. Manually filter out incorrect or irrelevant en-
tries from the remaining list.
</listItem>
<bodyText confidence="0.999936625">
Our aim, therefore, is to achieve sufficient recall and
precision to make this process — in particular the
time and manual effort required in Step 3 — a viable
alternative to manual creation of translation lexicons
without automated assistance.
The literature on cross-lingual text retrieval
(CLTR) includes work that is closely related to this
research, in that recent approaches emphasize the
use of dictionary- and corpus-based techniques for
translating queries from a source language into the
language of the document collection (Oard, 1997).
Davis and Dunning (1995), for example, generate
target-language queries using a corpus-based tech-
nique that is similar in several respects to the work
described here. However, the approach does not at-
tempt to distinguish domain-specific from general
usage term pairs, and it involves no manual inter-
vention. The work reported here, focusing on semi-
automating the process of acquiring translation lexi-
cons specific to a domain, can be viewed as providing
bilingual dictionary entries for CLTR methods like
that used by Davis in later work (Davis, 1996), in
which dictionary-based generation of an ambiguous
target language query is followed by corpus-based
disambiguation of that query.
Turning to the literature on bilingual terminology
identification per se, although monolingual termi-
nology extraction is a problem that has been previ-
ously explored, often with respect to identifying rel-
evant multi-word terms (e.g. (Daille, 1996; Smadja,
1993)), less prior work exists for bilingual acquisi-
tion of domain-specific translations. Termight (Da-
gan and Church, 1994) is one method for analyzing
parallel corpora to discover translations in techni-
cal terminology; Dagan and Church report accuracy
of 40% given an English/German technical manual,
and observe that even this relatively low accuracy
permits the successful application of the system in a
translation bureau, when used in conjunction with
an appropriate user interface.
The Champollion system (Smadja, McKeown, and
Hatzivassiloglou, 1996) moves toward higher accu-
racy (around 73%) and considerably greater flex-
ibility in the handling of multi-word translations,
though the algorithm has been applied primarily to
very large corpora such as the Hansards (3-9 mil-
lion words; Smadja et al. observe that the method
has difficulty handling low-frequency cases), and no
</bodyText>
<page confidence="0.994861">
340
</page>
<bodyText confidence="0.99337059375">
attempt is made to distinguish corpus-dependent
translations from general ones.
Daille et al. (1994) report on a study in which a
small (200,000 word) corpus was used as the basis for
extracting bilingual terminology, using a combina-
tion of syntactic patterns for identifying simple two-
word terms monolingually, and a statistical measure
for selecting related terms across languages. Using a
manually constructed reference list, they report 70%
precision.
The SABLE system (Melamed, 1996b) makes no
attempt to handle collocations, but for single-word
to single-word translations it offers a very accurate
method for acquiring high quality translation lexi-
cons from very large parallel corpora: Melamed re-
ports 90+% precision at 90+% recall, when evalu-
ated on sets of Hansards data of 6-7 million words.
Previous work with SABLE does not attempt to ad-
dress the question of domain-specific vs. general
translations.
This paper applies the SABLE system to a much
smaller (approximately 400,000 word) corpus in a
technical domain, and assesses its potential con-
tribution to the semi-automatic acquisition process
outlined above, very much in the spirit of Dagan and
Church (1994) and Daille et al. (1994), but begin-
ning with a higher accuracy starting point and fo-
cusing on mono-word terms. In the remainder of the
paper we briefly outline translation lexicon acquisi-
tion in the SABLE system, describe its application
to a corpus of technical documentation, and provide
a quantitative assessment of its performance.
</bodyText>
<sectionHeader confidence="0.996403" genericHeader="introduction">
2 SABLE
</sectionHeader>
<bodyText confidence="0.9328993">
SABLE (Scalable Architecture for Bilingual LExi-
cography) is a turn-key system for producing clean
broad-coverage translation lexicons from raw, un-
aligned parallel texts (bitexts). Its design is mod-
ular and minimizes the need for language-specific
components, with no dependence on genre or word
order similarity, nor sentence boundaries or other
&amp;quot;anchors&amp;quot; in the input.
SABLE was designed with the following features
in mind:
</bodyText>
<listItem confidence="0.976326181818182">
• Independence from linguistic resources: SABLE
does not rely on any language-specific resources
other than tokenizers and a heuristic for iden-
tifying word pairs that are mutual translations,
though users can easily reconfigure the system
to take advantage of such resources as language-
specific stemmers, part-of-speech taggers, and
stop lists when they are available.
• Black box functionality: Automatic acquisition
of translation lexicons requires only that the
user provide the input bitexts and identify the
two languages involved.
• Robustness: The system performs well even in
the face of omissions or inversions in transla-
tions.
• Scalability: SABLE has been used successfully
on input bitexts larger than 130MB.
• Portability: SABLE was initially implemented
for French/English, then ported to Span-
ish/English and to Korean/English. The port-
ing process has been standardized and docu-
mented (Melamed, 1996c).
</listItem>
<bodyText confidence="0.968345">
The following is a brief description of SABLE&apos;s
main components. A more detailed description of
the entire system is available in (Melamed, 1997).
</bodyText>
<subsectionHeader confidence="0.998323">
2.1 Mapping Bitext Correspondence
</subsectionHeader>
<bodyText confidence="0.998587166666666">
After both halves of the input bitext(s) have been
tokenized, SABLE invokes the Smooth Injective Map
Recognizer (SIMR) algorithm (Melamed, 1996a) and
related components to produce a bitext map. A bi-
text map is an injective partial function between the
character positions in the two halves of the bitext.
Each point of correspondence (x, y) in the bitext
map indicates that the word centered around char-
acter position x in the first half of the bitext is a
translation of the word centered around character
position y in the second half. SIMR produces bitext
maps a few points at a time, by interleaving a point
generation phase and a point selection phase.
SIMR is equipped with several &amp;quot;plug-in&amp;quot; match-
ing heuristic modules which are based on cognates
(Davis et al., 1995; Simard et al., 1992; Melamed,
1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen,
1993). Correspondence points are generated using
a subset of these matching heuristics; the particular
subset depends on the language pair and the avail-
able resources. The matching heuristics all work at
the word level, which is a happy medium between
larger text units like sentences and smaller text units
like character n-grams. Algorithms that map bitext
correspondence at the phrase or sentences level are
limited in their applicability to bitexts that have
easily recognizable phrase or sentence boundaries,
and Church (1993) reports that such bitexts are far
more rare than one might expect. Moreover, even
when these larger text units can be found, their
size imposes an upper bound on the resolution of
the bitext map. On the other end of the spectrum,
character-based bitext mapping algorithms (Church,
1993; Davis et al., 1995) are limited to language pairs
where cognates are common; in addition, they may
easily be misled by superficial differences in format-
ting and page layout and must sacrifice precision to
be computationally tractable.
SIMR filters candidate points of correspondence
using a geometric pattern recognition algorithm.
The recognized patterns may contain non-monotonic
sequences of points of correspondence, to account for
</bodyText>
<page confidence="0.994964">
341
</page>
<figure confidence="0.99635825">
I-
100000
10000
t000
100
1
• point of corresponden0
character position in bitext half A
</figure>
<figureCaption confidence="0.994945">
Figure 1: Word token pairs whose co-ordinates
lie between the dashed boundaries count as co-
occurrences.
</figureCaption>
<bodyText confidence="0.9997996">
word order differences between languages. The fil-
tering algorithm can be efficiently interleaved with
the point generation algorithm so that SIMR runs
in linear time and space with respect to the size of
the input bitext.
</bodyText>
<subsectionHeader confidence="0.999163">
2.2 Translation Lexicon Extraction
</subsectionHeader>
<bodyText confidence="0.999944392857143">
Since bitext maps can represent crossing correspon-
dences, they are more general than &amp;quot;alignments&amp;quot;
(Melamed, 1996a). For the same reason, bitext
maps allow a more general definition of token co-
occurrence. Early efforts at extracting translation
lexicons from bitexts deemed two tokens to co-occur
if they occurred in aligned sentence pairs (Gale and
Church, 1991). SABLE counts two tokens as co-
occurring if their point of correspondence lies within
a short distance b of the interpolated bitext map in
the bitext space, as illustrated in Figure 1. To en-
sure that interpolation is well-defined, minimal sets
of non-monotonic points of correspondence are re-
placed by the lower left and upper right corners of
their minimum enclosing rectangles (MERs).
SABLE uses token co-occurrence statistics to in-
duce an initial translation lexicon, using the method
described in (Melamed, 1995). The iterative filtering
module then alternates between estimating the most
likely translations among word tokens in the bitext
and estimating the most likely translations between
word types. This re-estimation paradigm was pi-
oneered by Brown et al. (1993). However, their
models were not designed for human inspection, and
though some have tried, it is not clear how to extract
translation lexicons from their models (Wu and Xia,
1995). In contrast, SABLE automatically constructs
an explicit translation lexicon, the lexicon consisting
</bodyText>
<figure confidence="0.8374392">
3rd plateau
2nd plateau
1st plateau
2000 6000 8000 100 00 12000
Entry Number
</figure>
<figureCaption confidence="0.9940725">
Figure 2: Translation lexicon entries proposed by
SABLE exhibit plateaus of likelihood.
</figureCaption>
<bodyText confidence="0.9999692">
of word type pairs that are not filtered out during
the re-estimation cycle. Neither of the translation
lexicon construction modules pay any attention to
word order, so they work equally well for language
pairs with different word order.
</bodyText>
<subsectionHeader confidence="0.999453">
2.3 Thresholding
</subsectionHeader>
<bodyText confidence="0.999988571428571">
Translation lexicon recall can be automatically com-
puted with respect to the input bitext (Melamed,
1996b), so SABLE users have the option of specify-
ing the recall they desire in the output. As always,
there is a tradeoff between recall and precision; by
default, SABLE will choose a likelihood threshold
that is known to produce reasonably high precision.
</bodyText>
<sectionHeader confidence="0.966676" genericHeader="method">
3 Evaluation in a Technical Domain
</sectionHeader>
<subsectionHeader confidence="0.998777">
3.1 Materials Evaluated
</subsectionHeader>
<bodyText confidence="0.95225528">
The SABLE system was run on a corpus compris-
ing parallel versions of Sun Microsystems documen-
tation (&amp;quot;Answerbooks&amp;quot;) in French (219,158 words)
and English (191,162 words). As Melamed (1996b)
observes, SABLE&apos;s output groups naturally accord-
ing to &amp;quot;plateaus&amp;quot; of likelihood (see Figure 2). The
translation lexicon obtained by running SABLE
on the Answerbooks contained 6663 French-English
content-word entries on the 2nd plateau or higher,
including 5464 on the 3rd plateau or higher. Table 1
shows a sample of 20 entries selected at random from
the Answerbook corpus output on the 3rd plateau
and higher. Exact matches, such as cpio/cpio or
clock/clock, comprised roughly 18% of the system&apos;s
output.
In order to eliminate likely general usage entries
from the initial translation lexicon, we automat-
ically filtered out all entries that appeared in a
French-English machine-readable dictionary (MRD)
(Cousin, Allain, and Love, 1991). 4071 entries re-
mained on or above the 2nd likelihood plateau, in-
cluding 3135 on the 3rd likelihood plateau or higher.
character position in bitext half B
interpolated bitext map
MER
</bodyText>
<page confidence="0.992977">
342
</page>
<tableCaption confidence="0.9685275">
Table 1: Random sample of SABLE output on soft-
ware manuals.
</tableCaption>
<bodyText confidence="0.999858454545455">
In previous experiments on the Hansard corpus of
Canadian parliamentary proceedings, SABLE had
uncovered valid general usage entries that were not
present in the Collins MRD (e.g. pointilles/dotted).
Since entries obtained from the Hansard corpus are
unlikely to include relevant technical terms, we de-
cided to test the efficacy of a second filtering step,
deleting all entries that had also been obtained by
running SABLE on the Hansards. On the 2nd
plateau or higher, 3030 entries passed both the
Collins and the Hansard filters; 2224 remained on
or above the 3rd plateau.
Thus in total, we evaluated four lexicons de-
rived from all combinations of two independent vari-
ables: cutoff (after the 2nd plateau vs. after the 3rd
plateau) and Hansards filter (with filter vs. without).
Evaluations were performed on a random sample of
100 entries from each lexicon variation, interleaving
the four samples to obscure any possible regularities.
Thus from the evaluator&apos;s perspective the task ap-
peared to involve a single sample of 400 translation
lexicon entries.
</bodyText>
<subsectionHeader confidence="0.99952">
3.2 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.999628666666667">
Our assessment of the system was designed to rea-
sonably approximate the post-processing that would
be done in order to use this system for acquisition
of translation lexicons in a real-world setting, which
would necessarily involve subjective judgments. We
hired six fluent speakers of both French and English
at the University of Maryland; they were briefed on
the general nature of the task, and given a data sheet
containing the 400 candidate entries (pairs contain-
ing one French word and one English word) and a
&amp;quot;multiple choice&amp;quot; style format for the annotations,
along with the following instructions.
</bodyText>
<listItem confidence="0.8417404">
1. If the pair clearly cannot be of help in
constructing a glossary, circle &amp;quot;Invalid&amp;quot;
and go on to the next pair.
2. If the pair can be of help in constructing
a glossary, choose one of the following:1
</listItem>
<bodyText confidence="0.999402952380952">
V: The two words are of the &amp;quot;plain
vanilla&amp;quot; type you might find in a bilin-
gual dictionary.
P: The pair is a case where a word
changes its part of speech during
translation. For example, &amp;quot;to have
protection&amp;quot; in English is often trans-
lated as &amp;quot;etre protege&amp;quot; in Cana-
dian parliamentary proceedings, so
for that domain the pair protec-
tion/protege would be marked P.
I: The pair is a case where a direct
translation is incomplete because the
computer program only looked at sin-
gle words. For example, if French
&amp;quot;immediatement&amp;quot; were paired with
English &amp;quot;right&amp;quot;, you could select I be-
cause the pair is almost certainly the
computer&apos;s best but incomplete at-
tempt to be pairing &amp;quot;immediatement&amp;quot;
with &amp;quot;right away&amp;quot;.
</bodyText>
<listItem confidence="0.993703714285714">
3. Then choose one or both of the following:
• Specific. Leaving aside the relation-
ship between the two words (your
choice of P, V, or I), the word pair
would be of use in constructing a tech-
nical glossary.
• General. Leaving aside the rela-
</listItem>
<bodyText confidence="0.918523370370371">
tionship between the two words (your
choice of P, V, or I), the word pair
would be of use in constructing a gen-
eral usage glossary.
Notice that a word pair could make
sense in both. For example, &amp;quot;cor-
beille/wastebasket&amp;quot; makes sense in the
computer domain (in many popular
graphical interfaces there is a wastebas-
ket icon that is used for deleting files),
but also in more general usage. So in this
case you could in fact decide to choose
both &amp;quot;Specific&amp;quot; and &amp;quot;General&amp;quot;. If you
can&apos;t choose either &amp;quot;Specific&amp;quot; or &amp;quot;Gen-
eral&amp;quot;, chances are that you should recon-
sider whether or not to mark this word
pair &amp;quot;Invalid&amp;quot;.
&apos;Since part-of-speech tagging was used in the version
of SABLE that produced the candidates in this experi-
ment, entries presented to the annotator also included a
minimal form of part-of-speech information, e.g. distin-
guishing nouns from verbs. The annotator was informed
that these annotations were the computer&apos;s best attempt
to identify the part-of-speech for the words; it was sug-
gested that they could be used as a hint as to why that
word pair had been proposed, if so desired, and otherwise
ignored.
</bodyText>
<figure confidence="0.99940687804878">
French
English
const antes
multi-fenetrage
risque
extensions
exemple
reliche
rw-r
regus
prealable
cpio
sont
defaults
In
alphabetique
activee
machine
mettre
connectes
bernard
superutilisateur
constants
windows
may
extensions
such
released
received
first
cpio
will
defaults
fn
alphabetically
activates
workstation
turns
connected
spanky
root
</figure>
<page confidence="0.998229">
343
</page>
<bodyText confidence="0.999881121212121">
4. If you&apos;re completely at a loss to decide
whether or not the word pair is valid, just
put a slash through the number of the
example (the number at the beginning of
the line) and go on to the next pair.
Annotators also had the option of working electron-
ically rather than on hardcopy.
The assessment questionnaire was designed to
elicit information primarily of two kinds. First,
we were concerned with the overall accuracy of the
method; that is, its ability to produce reasonable
candidate entries whether they be general or domain
specific. The &amp;quot;Invalid&amp;quot; category captures the sys-
tem&apos;s mistakes on this dimension. We also explic-
itly annotated candidates that might be useful in
constructing a translation lexicon, but possibly re-
quire further elaboration. The V category captures
cases that require minimal or no additional effort,
and the P category covers cases where some addi-
tional work might need to be done to accommodate
the part-of-speech divergence, depending on the ap-
plication. The I category captures cases where the
correspondence that has been identified may not ap-
ply directly at the single-world level, but nonetheless
does capture potentially useful information. Daille
et al. (1994) also note the existence of &amp;quot;incomplete&amp;quot;
cases in their results, but collapse them together
with &amp;quot;wrong&amp;quot; pairings.
Second, we were concerned with domain speci-
ficity. Ultimately we intend to measure this in an
objective, quantitative way by comparing term us-
age across corpora; however, for this study we relied
on human judgments.
</bodyText>
<subsectionHeader confidence="0.999643">
3.3 Use of Context
</subsectionHeader>
<bodyText confidence="0.999629244444444">
Melamed (1996b) suggests that evaluation of trans-
lation lexicons requires that judges have access
to bilingual concordances showing the contexts in
which proposed word pairs appear; however, out-of-
context judgments would be easier to obtain in both
experimental and real-world settings. In a prelimi-
nary evaluation, we had three annotators (one pro-
fessional French/English translator and two gradu-
ate students at the University of Pennsylvania) per-
form a version of the annotation task just described:
they annotated a set of entries containing the out-
put of an earlier version of the SABLE system (one
that used aligned sub-sentence fragments to define
term co-occurrence; cf. Section 2.2). No bilingual
concordances were made available to them.
Analysis of the system&apos;s performance in this pi-
lot study, however, as well as annotator comments
in a post-study questionnaire, confirmed that con-
text is quite important. In order to quantify its im-
portance, we asked one of the pilot annotators to
repeat the evaluation on the same items, this time
giving her access to context in the form of the bilin-
gual concordances for each term pair. These concor-
dances contained up to the first ten instances of that
pair as used in context. For example, given the pair
diplacez/drag, one instance in that pair&apos;s bilingual
concordance would be:
Maintenez SELECT enfonce et deplacez le
dossier vers espace de travail .
Press SELECT and drag the folder onto the
workspace background .
The instructions for the in-context evaluation spec-
ify that the annotator should look at the context
for every word pair, pointing out that &amp;quot;word pairs
may be used in unexpected ways in technical text
and words you would not normally expect to be re-
lated sometimes turn out to be related in a technical
context.&amp;quot;
Although we have data from only one annota-
tor, Table 2 shows the clear differences between the
two results.2 In light of the results of the pilot
study, therefore, our six annotators were given ac-
cess to bilingual concordances for the entries they
were judging and instructed in their use as just de-
scribed.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999429">
4.1 Group Annotations
</subsectionHeader>
<bodyText confidence="0.999980266666667">
A &amp;quot;group annotation&amp;quot; was obtained for each candi-
date translation lexicon entry based on agreement
of at least three of the six annotators. &amp;quot;Tie scores&amp;quot;
or the absence of a 3-of-6 plurality were treated as
the absence of an annotation. For example, if an en-
try was annotated as &amp;quot;Invalid&amp;quot; by two annotators,
marked as category V and Specific by two annota-
tors, and marked as category P, Specific, and Gen-
eral by the other two annotators, then the group an-
notation would contain an &amp;quot;unclassified valid type&amp;quot;
(since four annotators chose a valid type, but there
was no agreement by at least three on the specific
subclasification) and a &amp;quot;Specific&amp;quot; annotation (agreed
on by four annotators). All summary statistics are
reported in terms of the group annotation.
</bodyText>
<subsectionHeader confidence="0.984383">
4.2 Precision
</subsectionHeader>
<bodyText confidence="0.9999686">
SABLE&apos;s precision on the Answerbooks bitext is
summarized in Figure 3.3 Each of the percentages
being derived from a random sample of 100 observa-
tions, we can compute confidence intervals under a
normality assumption; if we assume that the obser-
vations are independent, then 95% confidence inter-
vals are narrower than one twentieth of a percentage
point for all the statistics computed.
The results show that up to 89% of the translation
lexicon entries produced by SABLE on or above the
</bodyText>
<footnote confidence="0.940743">
2Again, this sample of data was produced by an older
and less accurate version of SABLE, and therefore the
percentages should only be analyzed relative to each
other, not as absolute measures of performance.
3The exact numbers gladly provided on request.
</footnote>
<page confidence="0.991769">
344
</page>
<table confidence="0.999139">
All Valid Domain-Specific General Usage Both
Entries Only Only
Out-of-Context 39.5 9.25 5.5 57.75 29.75 23.5 1
In-Context 46.75 5 13 69.5 38 23.25 3.5
</table>
<tableCaption confidence="0.998602">
Table 2: Effect of in-context vs. out-of-context evaluation. All numbers are in %. n = 400.
</tableCaption>
<figure confidence="0.981330142857143">
v Unclassified valid type
100
75
50
25
0
3rd plateau cutoff 2nd plateau cutoff
</figure>
<figureCaption confidence="0.9989805">
Figure 3: Summary of filtered translation lexicon va-
lidity statistics.
</figureCaption>
<bodyText confidence="0.992650210526316">
3rd likelihood plateau &amp;quot;can be of help in constructing
a glossary.&amp;quot; Up to 56% can be considered useful es-
sentially as-is (the V category alone). Including all
entries on the 2nd plateau or higher provides better
coverage, but reduces the fraction of useful entries
to 81%. The fraction of entries that are useful as-is
remains roughly the same, at 55%. At both recall
levels, the extra Hansards-based filter had a detri-
mental effect on precision.
Note that these figures are based on translation
lexicons from which many valid general usage en-
tries have been filtered out (see Section 3). We can
compute SABLE&apos;s precision on unfiltered transla-
tion lexicons for this corpus by assuming that en-
tries appearing in the Collins MRD are all correct.&apos;
However, these are not the real figures of interest
here, because we are mainly concerned in this study
with the acquisition of domain-specific translation
lexicons.
</bodyText>
<subsectionHeader confidence="0.998689">
4.3 Recall
</subsectionHeader>
<bodyText confidence="0.9825633">
Following Melamed (1996b), we adopt the following
approach to measuring recall: the upper bound is de-
fined by the number of different words in the bitext.
Thus, perfect recall implies at least one entry con-
taining each word in the corpus. This is a much more
conservative metric than that used by Daille et al.
(1994), who report recall with respect to a relatively
4Result: 88.4% precision at 37.0% recall or 93.7%
precision at 30.4% recall.
small, manually constructed reference set. Although
we do not expect to achieve perfect recall on this cri-
terion after general usage entries have been filtered
out, the number is useful insofar as it provides a
sense of how recall for this corpus correlates with
precision. We have no reason to expect this corre-
lation to change across domain-specific and general
lexicon entries. For the unfiltered translation lexi-
cons, recall on the 3rd likelihood plateau and above
was 30.4%. When all entries on and above the 2nd
plateau were considered, recall improved to 37.0%.
</bodyText>
<figure confidence="0.966185181818182">
domain-specific only both general only
100
75
a)
&apos;a
cci
c&amp;quot; 50
&amp;quot;6
25
0
3rd plateau cutoff 2nd plateau cutoff
</figure>
<figureCaption confidence="0.9976715">
Figure 4: Summary of filtered translation lexicon
domain-specificity statistics.
</figureCaption>
<table confidence="0.999450571428571">
% %
Hansards Plateau Domain General %
Filter? Cutoff Specific Usage Both
Yes 3rd 82 37 35
No 3rd 71 53 35
Yes 2nd 66 27 22
No 2nd 81 47 47
</table>
<tableCaption confidence="0.931022">
Table 3: Domain-specificity of filtered translation
lexicon entries.
</tableCaption>
<subsectionHeader confidence="0.99909">
4.4 Domain Specificity
</subsectionHeader>
<bodyText confidence="0.925451">
Figure 4 demonstrates the effectiveness of the MRD-
and corpus-based filters, with details in Table 3. If
we assume that translation pairs in the Collins MRD
are not specific to our chosen domain, then domain-
specific translation lexicon entries constituted only
</bodyText>
<table confidence="0.8559625">
with no with no
Hansard Hansard Hansard Hansard
filter filter filter filter
with no with no
Hansard Hansard Hansard Hansard
filter filter filter filter
345
Al A2 A3 A4 A5 A6
K1 0.70 0.44 0.59 0.82 0.90 0.82
K2 0.62 0.67 0.72 0.74 0.55 0.73
K3 0.28 0.19 0.50 0.00 0.00 0.56
1C4 0.67 0.69 0.68 0.74 0.61 0.81
</table>
<tableCaption confidence="0.999696">
Table 4: Inter-annotator agreement.
</tableCaption>
<bodyText confidence="0.999832692307692">
49% of SABLE&apos;s unfiltered output on or above the
2nd plateau and 41% on or above the 3rd plateau.
The MRD filter increased this ratio to 81% and 71%,
respectively. As noted in Section 4.2, the second fil-
ter, based on the Hansard bitext, reduced the overall
accuracy of the translation lexicons. Its effects on
the proportion of domain-specific entries was mixed:
an 11% increase for the entries more likely to be cor-
rect, but a 15% decrease overall. The corpus-based
filter is certainly useful in the absence of an MRD.
However, our results suggest that combining filters
does not always help, and more research is needed
to investigate optimal filter combination strategies.
</bodyText>
<subsectionHeader confidence="0.998869">
4.5 Consistency of Annotations
</subsectionHeader>
<bodyText confidence="0.995241423076923">
In order to assess the consistency of annotation, we
follow Carletta (1996) in using Cohen&apos;s K, a chance-
corrected measure of inter-rater agreement. The K
statistic was developed to distinguish among levels
of agreement such as &amp;quot;almost perfect, substantial,
moderate, fair, slight, poor&amp;quot; (Agresti, 1992), and
Carletta suggests that as a rule of thumb in the be-
havioral sciences, values of K greater than .8 indicate
good replicability, with values between .67 and .8 al-
lowing tentative conclusions to be drawn. For each
such comparison, four values of K were computed:
kr. agreement on the evaluation of whether or not a
pair should be immediately rejected or retained;
K2: agreement, for the retained pairs, on the type
V, P, or I assigned to the pair;
K3: agreement, for the retained pairs, on whether to
classify the pair as being useful for constructing
a domain-specific glossary;
K4: agreement, for the retained pairs, on whether to
classify the pair as being useful for constructing
a general usage glossary.
In each case, the computation of the agreement
statistic took into account those cases, if any, where
the annotator could not arrive at a decision for this
case and opted simply to throw it out. Resulting val-
ues for inter-rater reliability are shown in Table 4;
the six annotators are identified as Al, A2, ... A6,
and each value of K reflects the comparison between
that annotator and the group annotation.
With the exception of 1c3, these values of K indi-
cate that the reliability of the judgments is generally
reasonable, albeit not entirely beyond debate. The
outlandish values for K3, despite high rates of abso-
lute agreement on that dimension of annotation, are
explained by the fact that the K statistic is known
to be highly problematic as a measure of inter-rater
reliability when one of the categories that can be
chosen is overwhelmingly likely (Grove et al., 1981;
Spitznagel and Heizer, 1985). Intuitively this is not
surprising: we designed the experiment to yield a
predominance of domain-specific terms, by means
of the MRD and Hansards filters. Our having suc-
ceeded, there is a very high probability that the
&amp;quot;Specific&amp;quot; annotation will be selected by any two
annotators, because it appears so very frequently;
as a result the actual agreement rate for that anno-
tation doesn&apos;t actually look all that different from
what one would get by chance, and so the K values
are low. The values of x3 for annotators 4 and 5
emphasize quite clearly that K is measuring not the
level of absolute agreement, but the distinguishabil-
ity of that level of agreement from chance.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967961538462">
In this paper, we have investigated the application of
SABLE, a turn-key translation lexicon construction
system for non-technical users, to the problem of
identifying domain-specific word translations given
domain-specific corpora of limited size. Evaluated
on a very small (400,000 word) corpus, the system
shows real promise as a method of processing small
domain-specific corpora in order to propose candi-
date single-word translations: once likely general us-
age terms are automatically filtered out, the system
obtains precision up to 89% at levels of recall very
conservatively estimated in the range of 30-40% on
domain-specific terms.
Of the proposed entries not immediately suitable
for inclusion in a translation lexicon, many represent
part-of-speech divergences (of the protect/protege
variety) and a smaller number incomplete entries
(of the immediatement/right variety) that would
nonetheless be helpful if used as the basis for a bilin-
gual concordance search — for example, a search for
French segments containing immediatement in the
vicinity of English segments containing right would
most likely yield up the obvious correspondence be-
tween immediatement and right away. Going be-
yond single-word correspondences, however, is a pri-
ority for future work.
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.996859875">
The authors wish to acknowledge the support of Sun
Microsystems Laboratories, particularly the assis-
tance of Gary Adams, Cookie Callahan, and Bob
Kuhns, as well as useful input from Bonnie Dorr,
Ralph Grishman, Marti Hearst, Doug Oard, and
three anonymous reviewers. Melamed also acknowl-
edges grants ARPA N00014-904-1863 and ARPA
N6600194C 6043.
</bodyText>
<page confidence="0.99812">
346
</page>
<sectionHeader confidence="0.979275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566093457944">
Alan Agresti. 1992. Modeling patterns of agreement
and disagreement. Statistical methods in medical
research, 1:201-218.
P. F. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. &amp;quot;The Mathematics of Statisti-
cal Machine Translation: Parameter Estimation&amp;quot;.
Computational Linguistics 19:2.
Jean Carletta. 1996. Assessing agreement on clas-
sification tasks: the Kappa statistic. Computa-
tional Linguistics, 22(2):249-254, June.
S. Chen. 1993. &amp;quot;Aligning Sentences in Bilingual
Corpora Using Lexical Information&amp;quot;. Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics, Columbus, OH.
K. W. Church. 1993. &amp;quot;Char_align: A Program for
Aligning Parallel Texts at the Character Level&amp;quot;.
Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, Colum-
bus, OH.
P. H. Cousin, L. Sinclair, J. F. Allain, and C. E.
Love. 1991. The Collins Paperback French Dic-
tionary. Harper Collins Publishers, Glasgow.
Ido Dagan and Ken W. Church. 1994. TERMIGHT:
Identifying and translating technical terminology.
In Proceedings of the Fourth ACL Conference on
Applied Natural Language Processing (13-15 Oc-
tober 1994, Stuttgart). Association for Computa-
tional Linguistics, October.
I. Dagan, K. Church, and W. Gale. 1993. &amp;quot;Robust
Word Alignment for Machine Aided Translation&amp;quot;.
Proceedings of the Workshop on Very Large Cor-
pora: Academic and Industrial Perspectives, avail-
able from the ACL.
Beatrice Daille. 1994. Combined approach for termi-
nology extraction: lexical statistics and linguistic
filtering. Ph.D. thesis, University Paris 7.
Beatrice Daille. 1996. Study and implementation
of combined techniques for automatic extraction
of terminology. In Judith Klavans and Philip
Resnik, editors, The Balancing Act: Combining
Symbolic and Statistical Approaches to Language.
MIT Press.
Mark Davis. 1996. &amp;quot;New experiments in cross-
language text retrieval at NMSU&apos;s Computing Re-
search Lab&amp;quot;. Fifth Text Retrieval Conference
(TREC-5). NIST.
Mark Davis and Ted Dunning. 1995. &amp;quot;A TREC
evaluation of query translation methods for multi-
lingual text retrieval&amp;quot;. Fourth Text Retrieval Con-
ference (TREC-4). NIST.
Mark Davis, Ted Dunning, and William Ogden.
1995. Text alignment in the real world: improv-
ing alignments of noisy translation using common
lexical features, string matching strategies, and n-
gram comparisons. In EACL-95.
W. Gale and K. W. Church. 1991. &amp;quot;Identifying
Word Correspondences in Parallel Texts&amp;quot;. Pro-
ceedings of the DARPA SNL Workshop, 1991.
W. Grove, N. Andreasen, P. McDonald-Scott,
M. Keller, and R. Shapiro. 1981. Reliability stud-
ies of psychiatric diagnosis. Archives of General
Psychiatry, 38, April.
I. Dan Melamed, 1995. Automatic evaluation and
uniform filter cascades for inducing n-best trans-
lation lexicons. In Proceedings of the Third Work-
shop on Very Large Corpora, Cambridge, Mas-
sachusetts.
I. Dan Melamed. 1996a. A geometric approach to
mapping bitext correspondence. In Conference on
Empirical Methods in Natural Language Process-
ing, Philadelphia, Pennsylvania.
I. Dan Melamed. 1996b. Automatic construction of
clean broad-coverage translation lexicons. In Pro-
ceedings of the 2nd Conference of the Association
for Machine Translation in the Americas, Mon-
treal, Canada.
I. Dan Melamed. 1996c. Porting SIMR to new lan-
guage pairs. IRCS Technical Report 96-26. Uni-
versity of Pennsylvania.
I. Dan Melamed. 1997. A scalable architecture for
bilingual lexicography. Dept. of Computer and
Information Science Technical Report MS-CIS-97-
01. University of Pennsylvania.
Douglas W. Oard. 1997. &amp;quot;Cross-Language Text
Retrieval Research in the USA&amp;quot;. Third DELOS
Workshop. European Research Consortium for In-
formatics and Mathematics. March.
M. Simard, G. F. Foster and P. Isabelle. 1992. &amp;quot;Us-
ing Cognates to Align Sentences in Bilingual Cor-
pora&amp;quot;. In Proceedings of the Fourth International
Conference on Theoretical and Methodological Is-
sues in Machine Translation, Montreal, Canada.
Frank Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143-177.
Frank Smadja, Kathleen McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach.
Computational Linguistics, 22(1), March.
E. Spitznagel and J. Helzer. &amp;quot;A proposed solution
to the base rate problem in the kappa statistic&amp;quot;.
Archives of General Psychiatry, 42. July, 1985.
D. Wu and X. Xia. 1994. &amp;quot;Learning an English-
Chinese Lexicon from a Parallel Corpus&amp;quot;. Pro-
ceedings of the First Conference of the Associ-
ation for Machine Translation in the Americas,
Columbia, MD.
</reference>
<page confidence="0.998573">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948281">
<title confidence="0.999311">Semi-Automatic Acquisition of Domain-Specific Translation Lexicons</title>
<author confidence="0.999983">Philip Resnik</author>
<affiliation confidence="0.999934">Dept. of Linguistics and UMIACS University of Maryland</affiliation>
<address confidence="0.999546">College Park, MD 20742 USA</address>
<email confidence="0.999709">resnikOumiacs.umd.edu</email>
<author confidence="0.999095">I Dan Melamed</author>
<affiliation confidence="0.999907">Dept. of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.95828">Philadelphia, PA 19104 USA</address>
<email confidence="0.999797">melamedOunagi.cis.upenn.edu</email>
<abstract confidence="0.9991295">We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Modeling patterns of agreement and disagreement.</title>
<date>1992</date>
<booktitle>Statistical methods in medical research,</booktitle>
<pages>1--201</pages>
<contexts>
<context position="27115" citStr="Agresti, 1992" startWordPosition="4318" endWordPosition="4319">more likely to be correct, but a 15% decrease overall. The corpus-based filter is certainly useful in the absence of an MRD. However, our results suggest that combining filters does not always help, and more research is needed to investigate optimal filter combination strategies. 4.5 Consistency of Annotations In order to assess the consistency of annotation, we follow Carletta (1996) in using Cohen&apos;s K, a chancecorrected measure of inter-rater agreement. The K statistic was developed to distinguish among levels of agreement such as &amp;quot;almost perfect, substantial, moderate, fair, slight, poor&amp;quot; (Agresti, 1992), and Carletta suggests that as a rule of thumb in the behavioral sciences, values of K greater than .8 indicate good replicability, with values between .67 and .8 allowing tentative conclusions to be drawn. For each such comparison, four values of K were computed: kr. agreement on the evaluation of whether or not a pair should be immediately rejected or retained; K2: agreement, for the retained pairs, on the type V, P, or I assigned to the pair; K3: agreement, for the retained pairs, on whether to classify the pair as being useful for constructing a domain-specific glossary; K4: agreement, fo</context>
</contexts>
<marker>Agresti, 1992</marker>
<rawString>Alan Agresti. 1992. Modeling patterns of agreement and disagreement. Statistical methods in medical research, 1:201-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context position="10973" citStr="Brown et al. (1993)" startWordPosition="1665" endWordPosition="1668">e, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs). SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995). The iterative filtering module then alternates between estimating the most likely translations among word tokens in the bitext and estimating the most likely translations between word types. This re-estimation paradigm was pioneered by Brown et al. (1993). However, their models were not designed for human inspection, and though some have tried, it is not clear how to extract translation lexicons from their models (Wu and Xia, 1995). In contrast, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 3rd plateau 2nd plateau 1st plateau 2000 6000 8000 100 00 12000 Entry Number Figure 2: Translation lexicon entries proposed by SABLE exhibit plateaus of likelihood. of word type pairs that are not filtered out during the re-estimation cycle. Neither of the translation lexicon construction modules pay any attention to</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. &amp;quot;The Mathematics of Statistical Machine Translation: Parameter Estimation&amp;quot;. Computational Linguistics 19:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="26888" citStr="Carletta (1996)" startWordPosition="4285" endWordPosition="4286">noted in Section 4.2, the second filter, based on the Hansard bitext, reduced the overall accuracy of the translation lexicons. Its effects on the proportion of domain-specific entries was mixed: an 11% increase for the entries more likely to be correct, but a 15% decrease overall. The corpus-based filter is certainly useful in the absence of an MRD. However, our results suggest that combining filters does not always help, and more research is needed to investigate optimal filter combination strategies. 4.5 Consistency of Annotations In order to assess the consistency of annotation, we follow Carletta (1996) in using Cohen&apos;s K, a chancecorrected measure of inter-rater agreement. The K statistic was developed to distinguish among levels of agreement such as &amp;quot;almost perfect, substantial, moderate, fair, slight, poor&amp;quot; (Agresti, 1992), and Carletta suggests that as a rule of thumb in the behavioral sciences, values of K greater than .8 indicate good replicability, with values between .67 and .8 allowing tentative conclusions to be drawn. For each such comparison, four values of K were computed: kr. agreement on the evaluation of whether or not a pair should be immediately rejected or retained; K2: ag</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the Kappa statistic. Computational Linguistics, 22(2):249-254, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
</authors>
<title>Aligning Sentences in Bilingual Corpora Using Lexical Information&amp;quot;.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="8137" citStr="Chen, 1993" startWordPosition="1228" endWordPosition="1229">n the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &amp;quot;plug-in&amp;quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts are far more rare than one might expect. Moreov</context>
</contexts>
<marker>Chen, 1993</marker>
<rawString>S. Chen. 1993. &amp;quot;Aligning Sentences in Bilingual Corpora Using Lexical Information&amp;quot;. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>Char_align: A Program for Aligning Parallel Texts at the Character Level&amp;quot;.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="8663" citStr="Church (1993)" startWordPosition="1308" endWordPosition="1309">95; Simard et al., 1992; Melamed, 1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts are far more rare than one might expect. Moreover, even when these larger text units can be found, their size imposes an upper bound on the resolution of the bitext map. On the other end of the spectrum, character-based bitext mapping algorithms (Church, 1993; Davis et al., 1995) are limited to language pairs where cognates are common; in addition, they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable. SIMR filters candidate points of correspondence using a geometric pattern rec</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>K. W. Church. 1993. &amp;quot;Char_align: A Program for Aligning Parallel Texts at the Character Level&amp;quot;. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H Cousin</author>
<author>L Sinclair</author>
<author>J F Allain</author>
<author>C E Love</author>
</authors>
<title>The Collins Paperback French Dictionary.</title>
<date>1991</date>
<publisher>Harper Collins Publishers,</publisher>
<location>Glasgow.</location>
<marker>Cousin, Sinclair, Allain, Love, 1991</marker>
<rawString>P. H. Cousin, L. Sinclair, J. F. Allain, and C. E. Love. 1991. The Collins Paperback French Dictionary. Harper Collins Publishers, Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Ken W Church</author>
</authors>
<title>TERMIGHT: Identifying and translating technical terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing</booktitle>
<institution>Stuttgart). Association for Computational Linguistics,</institution>
<contexts>
<context position="3473" citStr="Dagan and Church, 1994" startWordPosition="505" endWordPosition="509">as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query. Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g. (Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations. Termight (Dagan and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface. The Champollion system (Smadja, McKeown, and Hatzivassiloglou, 1996) moves toward higher accuracy (around 73%) and considerably greater flexibility in the handling of multi-word translations, though the algorithm has been applied prima</context>
<context position="5411" citStr="Dagan and Church (1994)" startWordPosition="801" endWordPosition="804">o single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words. Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations. This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automatic acquisition process outlined above, very much in the spirit of Dagan and Church (1994) and Daille et al. (1994), but beginning with a higher accuracy starting point and focusing on mono-word terms. In the remainder of the paper we briefly outline translation lexicon acquisition in the SABLE system, describe its application to a corpus of technical documentation, and provide a quantitative assessment of its performance. 2 SABLE SABLE (Scalable Architecture for Bilingual LExicography) is a turn-key system for producing clean broad-coverage translation lexicons from raw, unaligned parallel texts (bitexts). Its design is modular and minimizes the need for language-specific componen</context>
</contexts>
<marker>Dagan, Church, 1994</marker>
<rawString>Ido Dagan and Ken W. Church. 1994. TERMIGHT: Identifying and translating technical terminology. In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing (13-15 October 1994, Stuttgart). Association for Computational Linguistics, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Robust Word Alignment for Machine Aided Translation&amp;quot;.</title>
<date>1993</date>
<booktitle>Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, available from the ACL.</booktitle>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>I. Dagan, K. Church, and W. Gale. 1993. &amp;quot;Robust Word Alignment for Machine Aided Translation&amp;quot;. Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, available from the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<title>Combined approach for terminology extraction: lexical statistics and linguistic filtering.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University Paris 7.</institution>
<marker>Daille, 1994</marker>
<rawString>Beatrice Daille. 1994. Combined approach for terminology extraction: lexical statistics and linguistic filtering. Ph.D. thesis, University Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1996</date>
<booktitle>In Judith Klavans and</booktitle>
<editor>Philip Resnik, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3340" citStr="Daille, 1996" startWordPosition="489" endWordPosition="490">ported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query. Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g. (Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations. Termight (Dagan and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface. The Champollion system (Smadja, McKeown, and Hatzivassiloglou, 1996) moves toward higher accuracy (aro</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>Beatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In Judith Klavans and Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Approaches to Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davis</author>
</authors>
<title>New experiments in crosslanguage text retrieval at NMSU&apos;s Computing Research Lab&amp;quot;.</title>
<date>1996</date>
<booktitle>Fifth Text Retrieval Conference (TREC-5).</booktitle>
<publisher>NIST.</publisher>
<contexts>
<context position="2960" citStr="Davis, 1996" startWordPosition="435" endWordPosition="436">e into the language of the document collection (Oard, 1997). Davis and Dunning (1995), for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here. However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention. The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query. Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g. (Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations. Termight (Dagan and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical ter</context>
</contexts>
<marker>Davis, 1996</marker>
<rawString>Mark Davis. 1996. &amp;quot;New experiments in crosslanguage text retrieval at NMSU&apos;s Computing Research Lab&amp;quot;. Fifth Text Retrieval Conference (TREC-5). NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davis</author>
<author>Ted Dunning</author>
</authors>
<title>A TREC evaluation of query translation methods for multilingual text retrieval&amp;quot;.</title>
<date>1995</date>
<booktitle>Fourth Text Retrieval Conference (TREC-4).</booktitle>
<publisher>NIST.</publisher>
<contexts>
<context position="2433" citStr="Davis and Dunning (1995)" startWordPosition="350" endWordPosition="353">rect or irrelevant entries from the remaining list. Our aim, therefore, is to achieve sufficient recall and precision to make this process — in particular the time and manual effort required in Step 3 — a viable alternative to manual creation of translation lexicons without automated assistance. The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionary- and corpus-based techniques for translating queries from a source language into the language of the document collection (Oard, 1997). Davis and Dunning (1995), for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here. However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention. The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language qu</context>
</contexts>
<marker>Davis, Dunning, 1995</marker>
<rawString>Mark Davis and Ted Dunning. 1995. &amp;quot;A TREC evaluation of query translation methods for multilingual text retrieval&amp;quot;. Fourth Text Retrieval Conference (TREC-4). NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davis</author>
<author>Ted Dunning</author>
<author>William Ogden</author>
</authors>
<title>Text alignment in the real world: improving alignments of noisy translation using common lexical features, string matching strategies, and ngram comparisons.</title>
<date>1995</date>
<booktitle>In EACL-95.</booktitle>
<contexts>
<context position="8052" citStr="Davis et al., 1995" startWordPosition="1214" endWordPosition="1217">ted components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &amp;quot;plug-in&amp;quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Ch</context>
</contexts>
<marker>Davis, Dunning, Ogden, 1995</marker>
<rawString>Mark Davis, Ted Dunning, and William Ogden. 1995. Text alignment in the real world: improving alignments of noisy translation using common lexical features, string matching strategies, and ngram comparisons. In EACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying Word Correspondences in Parallel Texts&amp;quot;.</title>
<date>1991</date>
<booktitle>Proceedings of the DARPA SNL Workshop,</booktitle>
<contexts>
<context position="10200" citStr="Gale and Church, 1991" startWordPosition="1545" endWordPosition="1548">s. word order differences between languages. The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext. 2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than &amp;quot;alignments&amp;quot; (Melamed, 1996a). For the same reason, bitext maps allow a more general definition of token cooccurrence. Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance b of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs). SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995). The iterative filtering module then alternates between estimating the most likely </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. Gale and K. W. Church. 1991. &amp;quot;Identifying Word Correspondences in Parallel Texts&amp;quot;. Proceedings of the DARPA SNL Workshop, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Grove</author>
<author>N Andreasen</author>
<author>P McDonald-Scott</author>
<author>M Keller</author>
<author>R Shapiro</author>
</authors>
<title>Reliability studies of psychiatric diagnosis.</title>
<date>1981</date>
<journal>Archives of General Psychiatry,</journal>
<volume>38</volume>
<contexts>
<context position="28703" citStr="Grove et al., 1981" startWordPosition="4587" endWordPosition="4590">e 4; the six annotators are identified as Al, A2, ... A6, and each value of K reflects the comparison between that annotator and the group annotation. With the exception of 1c3, these values of K indicate that the reliability of the judgments is generally reasonable, albeit not entirely beyond debate. The outlandish values for K3, despite high rates of absolute agreement on that dimension of annotation, are explained by the fact that the K statistic is known to be highly problematic as a measure of inter-rater reliability when one of the categories that can be chosen is overwhelmingly likely (Grove et al., 1981; Spitznagel and Heizer, 1985). Intuitively this is not surprising: we designed the experiment to yield a predominance of domain-specific terms, by means of the MRD and Hansards filters. Our having succeeded, there is a very high probability that the &amp;quot;Specific&amp;quot; annotation will be selected by any two annotators, because it appears so very frequently; as a result the actual agreement rate for that annotation doesn&apos;t actually look all that different from what one would get by chance, and so the K values are low. The values of x3 for annotators 4 and 5 emphasize quite clearly that K is measuring n</context>
</contexts>
<marker>Grove, Andreasen, McDonald-Scott, Keller, Shapiro, 1981</marker>
<rawString>W. Grove, N. Andreasen, P. McDonald-Scott, M. Keller, and R. Shapiro. 1981. Reliability studies of psychiatric diagnosis. Archives of General Psychiatry, 38, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="8089" citStr="Melamed, 1995" startWordPosition="1222" endWordPosition="1223"> bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &amp;quot;plug-in&amp;quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts</context>
<context position="10716" citStr="Melamed, 1995" startWordPosition="1629" endWordPosition="1630">s deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance b of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs). SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995). The iterative filtering module then alternates between estimating the most likely translations among word tokens in the bitext and estimating the most likely translations between word types. This re-estimation paradigm was pioneered by Brown et al. (1993). However, their models were not designed for human inspection, and though some have tried, it is not clear how to extract translation lexicons from their models (Wu and Xia, 1995). In contrast, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 3rd plateau 2nd plateau 1st plateau 2000 6000 8000 100 00 120</context>
</contexts>
<marker>Melamed, 1995</marker>
<rawString>I. Dan Melamed, 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A geometric approach to mapping bitext correspondence.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="4723" citStr="Melamed, 1996" startWordPosition="695" endWordPosition="696">e Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no 340 attempt is made to distinguish corpus-dependent translations from general ones. Daille et al. (1994) report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for identifying simple twoword terms monolingually, and a statistical measure for selecting related terms across languages. Using a manually constructed reference list, they report 70% precision. The SABLE system (Melamed, 1996b) makes no attempt to handle collocations, but for single-word to single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words. Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations. This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automati</context>
<context position="7095" citStr="Melamed, 1996" startWordPosition="1058" endWordPosition="1059">ers, part-of-speech taggers, and stop lists when they are available. • Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved. • Robustness: The system performs well even in the face of omissions or inversions in translations. • Scalability: SABLE has been used successfully on input bitexts larger than 130MB. • Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE&apos;s main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character positi</context>
<context position="9950" citStr="Melamed, 1996" startWordPosition="1508" endWordPosition="1509">ences of points of correspondence, to account for 341 I100000 10000 t000 100 1 • point of corresponden0 character position in bitext half A Figure 1: Word token pairs whose co-ordinates lie between the dashed boundaries count as cooccurrences. word order differences between languages. The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext. 2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than &amp;quot;alignments&amp;quot; (Melamed, 1996a). For the same reason, bitext maps allow a more general definition of token cooccurrence. Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance b of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their </context>
<context position="11779" citStr="Melamed, 1996" startWordPosition="1791" endWordPosition="1792">, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 3rd plateau 2nd plateau 1st plateau 2000 6000 8000 100 00 12000 Entry Number Figure 2: Translation lexicon entries proposed by SABLE exhibit plateaus of likelihood. of word type pairs that are not filtered out during the re-estimation cycle. Neither of the translation lexicon construction modules pay any attention to word order, so they work equally well for language pairs with different word order. 2.3 Thresholding Translation lexicon recall can be automatically computed with respect to the input bitext (Melamed, 1996b), so SABLE users have the option of specifying the recall they desire in the output. As always, there is a tradeoff between recall and precision; by default, SABLE will choose a likelihood threshold that is known to produce reasonably high precision. 3 Evaluation in a Technical Domain 3.1 Materials Evaluated The SABLE system was run on a corpus comprising parallel versions of Sun Microsystems documentation (&amp;quot;Answerbooks&amp;quot;) in French (219,158 words) and English (191,162 words). As Melamed (1996b) observes, SABLE&apos;s output groups naturally according to &amp;quot;plateaus&amp;quot; of likelihood (see Figure 2). Th</context>
<context position="19128" citStr="Melamed (1996" startWordPosition="2990" endWordPosition="2991">ergence, depending on the application. The I category captures cases where the correspondence that has been identified may not apply directly at the single-world level, but nonetheless does capture potentially useful information. Daille et al. (1994) also note the existence of &amp;quot;incomplete&amp;quot; cases in their results, but collapse them together with &amp;quot;wrong&amp;quot; pairings. Second, we were concerned with domain specificity. Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments. 3.3 Use of Context Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings. In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used </context>
<context position="24094" citStr="Melamed (1996" startWordPosition="3813" endWordPosition="3814">ly the same, at 55%. At both recall levels, the extra Hansards-based filter had a detrimental effect on precision. Note that these figures are based on translation lexicons from which many valid general usage entries have been filtered out (see Section 3). We can compute SABLE&apos;s precision on unfiltered translation lexicons for this corpus by assuming that entries appearing in the Collins MRD are all correct.&apos; However, these are not the real figures of interest here, because we are mainly concerned in this study with the acquisition of domain-specific translation lexicons. 4.3 Recall Following Melamed (1996b), we adopt the following approach to measuring recall: the upper bound is defined by the number of different words in the bitext. Thus, perfect recall implies at least one entry containing each word in the corpus. This is a much more conservative metric than that used by Daille et al. (1994), who report recall with respect to a relatively 4Result: 88.4% precision at 37.0% recall or 93.7% precision at 30.4% recall. small, manually constructed reference set. Although we do not expect to achieve perfect recall on this criterion after general usage entries have been filtered out, the number is u</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. 1996a. A geometric approach to mapping bitext correspondence. In Conference on Empirical Methods in Natural Language Processing, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4723" citStr="Melamed, 1996" startWordPosition="695" endWordPosition="696">e Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no 340 attempt is made to distinguish corpus-dependent translations from general ones. Daille et al. (1994) report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for identifying simple twoword terms monolingually, and a statistical measure for selecting related terms across languages. Using a manually constructed reference list, they report 70% precision. The SABLE system (Melamed, 1996b) makes no attempt to handle collocations, but for single-word to single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words. Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations. This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automati</context>
<context position="7095" citStr="Melamed, 1996" startWordPosition="1058" endWordPosition="1059">ers, part-of-speech taggers, and stop lists when they are available. • Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved. • Robustness: The system performs well even in the face of omissions or inversions in translations. • Scalability: SABLE has been used successfully on input bitexts larger than 130MB. • Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE&apos;s main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character positi</context>
<context position="9950" citStr="Melamed, 1996" startWordPosition="1508" endWordPosition="1509">ences of points of correspondence, to account for 341 I100000 10000 t000 100 1 • point of corresponden0 character position in bitext half A Figure 1: Word token pairs whose co-ordinates lie between the dashed boundaries count as cooccurrences. word order differences between languages. The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext. 2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than &amp;quot;alignments&amp;quot; (Melamed, 1996a). For the same reason, bitext maps allow a more general definition of token cooccurrence. Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance b of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their </context>
<context position="11779" citStr="Melamed, 1996" startWordPosition="1791" endWordPosition="1792">, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 3rd plateau 2nd plateau 1st plateau 2000 6000 8000 100 00 12000 Entry Number Figure 2: Translation lexicon entries proposed by SABLE exhibit plateaus of likelihood. of word type pairs that are not filtered out during the re-estimation cycle. Neither of the translation lexicon construction modules pay any attention to word order, so they work equally well for language pairs with different word order. 2.3 Thresholding Translation lexicon recall can be automatically computed with respect to the input bitext (Melamed, 1996b), so SABLE users have the option of specifying the recall they desire in the output. As always, there is a tradeoff between recall and precision; by default, SABLE will choose a likelihood threshold that is known to produce reasonably high precision. 3 Evaluation in a Technical Domain 3.1 Materials Evaluated The SABLE system was run on a corpus comprising parallel versions of Sun Microsystems documentation (&amp;quot;Answerbooks&amp;quot;) in French (219,158 words) and English (191,162 words). As Melamed (1996b) observes, SABLE&apos;s output groups naturally according to &amp;quot;plateaus&amp;quot; of likelihood (see Figure 2). Th</context>
<context position="19128" citStr="Melamed (1996" startWordPosition="2990" endWordPosition="2991">ergence, depending on the application. The I category captures cases where the correspondence that has been identified may not apply directly at the single-world level, but nonetheless does capture potentially useful information. Daille et al. (1994) also note the existence of &amp;quot;incomplete&amp;quot; cases in their results, but collapse them together with &amp;quot;wrong&amp;quot; pairings. Second, we were concerned with domain specificity. Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments. 3.3 Use of Context Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings. In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used </context>
<context position="24094" citStr="Melamed (1996" startWordPosition="3813" endWordPosition="3814">ly the same, at 55%. At both recall levels, the extra Hansards-based filter had a detrimental effect on precision. Note that these figures are based on translation lexicons from which many valid general usage entries have been filtered out (see Section 3). We can compute SABLE&apos;s precision on unfiltered translation lexicons for this corpus by assuming that entries appearing in the Collins MRD are all correct.&apos; However, these are not the real figures of interest here, because we are mainly concerned in this study with the acquisition of domain-specific translation lexicons. 4.3 Recall Following Melamed (1996b), we adopt the following approach to measuring recall: the upper bound is defined by the number of different words in the bitext. Thus, perfect recall implies at least one entry containing each word in the corpus. This is a much more conservative metric than that used by Daille et al. (1994), who report recall with respect to a relatively 4Result: 88.4% precision at 37.0% recall or 93.7% precision at 30.4% recall. small, manually constructed reference set. Although we do not expect to achieve perfect recall on this criterion after general usage entries have been filtered out, the number is u</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. 1996b. Automatic construction of clean broad-coverage translation lexicons. In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Porting SIMR to new language pairs.</title>
<date>1996</date>
<tech>IRCS Technical Report 96-26.</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4723" citStr="Melamed, 1996" startWordPosition="695" endWordPosition="696">e Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no 340 attempt is made to distinguish corpus-dependent translations from general ones. Daille et al. (1994) report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for identifying simple twoword terms monolingually, and a statistical measure for selecting related terms across languages. Using a manually constructed reference list, they report 70% precision. The SABLE system (Melamed, 1996b) makes no attempt to handle collocations, but for single-word to single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words. Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations. This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automati</context>
<context position="7095" citStr="Melamed, 1996" startWordPosition="1058" endWordPosition="1059">ers, part-of-speech taggers, and stop lists when they are available. • Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved. • Robustness: The system performs well even in the face of omissions or inversions in translations. • Scalability: SABLE has been used successfully on input bitexts larger than 130MB. • Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE&apos;s main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character positi</context>
<context position="9950" citStr="Melamed, 1996" startWordPosition="1508" endWordPosition="1509">ences of points of correspondence, to account for 341 I100000 10000 t000 100 1 • point of corresponden0 character position in bitext half A Figure 1: Word token pairs whose co-ordinates lie between the dashed boundaries count as cooccurrences. word order differences between languages. The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext. 2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than &amp;quot;alignments&amp;quot; (Melamed, 1996a). For the same reason, bitext maps allow a more general definition of token cooccurrence. Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance b of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their </context>
<context position="11779" citStr="Melamed, 1996" startWordPosition="1791" endWordPosition="1792">, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 3rd plateau 2nd plateau 1st plateau 2000 6000 8000 100 00 12000 Entry Number Figure 2: Translation lexicon entries proposed by SABLE exhibit plateaus of likelihood. of word type pairs that are not filtered out during the re-estimation cycle. Neither of the translation lexicon construction modules pay any attention to word order, so they work equally well for language pairs with different word order. 2.3 Thresholding Translation lexicon recall can be automatically computed with respect to the input bitext (Melamed, 1996b), so SABLE users have the option of specifying the recall they desire in the output. As always, there is a tradeoff between recall and precision; by default, SABLE will choose a likelihood threshold that is known to produce reasonably high precision. 3 Evaluation in a Technical Domain 3.1 Materials Evaluated The SABLE system was run on a corpus comprising parallel versions of Sun Microsystems documentation (&amp;quot;Answerbooks&amp;quot;) in French (219,158 words) and English (191,162 words). As Melamed (1996b) observes, SABLE&apos;s output groups naturally according to &amp;quot;plateaus&amp;quot; of likelihood (see Figure 2). Th</context>
<context position="19128" citStr="Melamed (1996" startWordPosition="2990" endWordPosition="2991">ergence, depending on the application. The I category captures cases where the correspondence that has been identified may not apply directly at the single-world level, but nonetheless does capture potentially useful information. Daille et al. (1994) also note the existence of &amp;quot;incomplete&amp;quot; cases in their results, but collapse them together with &amp;quot;wrong&amp;quot; pairings. Second, we were concerned with domain specificity. Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments. 3.3 Use of Context Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings. In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used </context>
<context position="24094" citStr="Melamed (1996" startWordPosition="3813" endWordPosition="3814">ly the same, at 55%. At both recall levels, the extra Hansards-based filter had a detrimental effect on precision. Note that these figures are based on translation lexicons from which many valid general usage entries have been filtered out (see Section 3). We can compute SABLE&apos;s precision on unfiltered translation lexicons for this corpus by assuming that entries appearing in the Collins MRD are all correct.&apos; However, these are not the real figures of interest here, because we are mainly concerned in this study with the acquisition of domain-specific translation lexicons. 4.3 Recall Following Melamed (1996b), we adopt the following approach to measuring recall: the upper bound is defined by the number of different words in the bitext. Thus, perfect recall implies at least one entry containing each word in the corpus. This is a much more conservative metric than that used by Daille et al. (1994), who report recall with respect to a relatively 4Result: 88.4% precision at 37.0% recall or 93.7% precision at 30.4% recall. small, manually constructed reference set. Although we do not expect to achieve perfect recall on this criterion after general usage entries have been filtered out, the number is u</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. 1996c. Porting SIMR to new language pairs. IRCS Technical Report 96-26. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A scalable architecture for bilingual lexicography.</title>
<date>1997</date>
<journal>Dept. of Computer and Information Science</journal>
<tech>Technical Report MS-CIS-97-01.</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1224" citStr="Melamed, 1997" startWordPosition="167" endWordPosition="168"> lexicons. 1 Introduction Reliable translation lexicons are useful in many applications, such as cross-language text retrieval. Although general purpose machine readable bilingual dictionaries are sometimes available, and although some methods for acquiring translation lexicons automatically from large corpora have been proposed, less attention has been paid to the problem of acquiring bilingual terminology specific to a domain, especially given domain-specific parallel corpora of only limited size. In this paper, we investigate the utility of an algorithm for translation lexicon acquisition (Melamed, 1997), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons. The goal is to produce material suitable for postprocessing in a lexicon acquisition process like the following: 1. Run the automatic lexicon acquisition algorithm on a domain-specific parallel corpus. 2. Automatically filter out &amp;quot;general usage&amp;quot; entries that already appear in a machine readable dictionary (MRD) or other general usage lexical resources. 3. Manually filter out incorrect or irrelev</context>
<context position="7244" citStr="Melamed, 1997" startWordPosition="1081" endWordPosition="1082">es only that the user provide the input bitexts and identify the two languages involved. • Robustness: The system performs well even in the face of omissions or inversions in translations. • Scalability: SABLE has been used successfully on input bitexts larger than 130MB. • Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE&apos;s main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps </context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A scalable architecture for bilingual lexicography. Dept. of Computer and Information Science Technical Report MS-CIS-97-01. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
</authors>
<title>Cross-Language Text Retrieval Research in the USA&amp;quot;. Third DELOS Workshop. European Research Consortium for Informatics and Mathematics.</title>
<date>1997</date>
<contexts>
<context position="2407" citStr="Oard, 1997" startWordPosition="348" endWordPosition="349">ter out incorrect or irrelevant entries from the remaining list. Our aim, therefore, is to achieve sufficient recall and precision to make this process — in particular the time and manual effort required in Step 3 — a viable alternative to manual creation of translation lexicons without automated assistance. The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionary- and corpus-based techniques for translating queries from a source language into the language of the document collection (Oard, 1997). Davis and Dunning (1995), for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here. However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention. The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an am</context>
</contexts>
<marker>Oard, 1997</marker>
<rawString>Douglas W. Oard. 1997. &amp;quot;Cross-Language Text Retrieval Research in the USA&amp;quot;. Third DELOS Workshop. European Research Consortium for Informatics and Mathematics. March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G F Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using Cognates to Align Sentences in Bilingual Corpora&amp;quot;.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="8073" citStr="Simard et al., 1992" startWordPosition="1218" endWordPosition="1221">oduce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x, y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &amp;quot;plug-in&amp;quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) and/or &amp;quot;seed&amp;quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports t</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>M. Simard, G. F. Foster and P. Isabelle. 1992. &amp;quot;Using Cognates to Align Sentences in Bilingual Corpora&amp;quot;. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="3355" citStr="Smadja, 1993" startWordPosition="491" endWordPosition="492">ocusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query. Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g. (Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations. Termight (Dagan and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface. The Champollion system (Smadja, McKeown, and Hatzivassiloglou, 1996) moves toward higher accuracy (around 73%) and co</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3905" citStr="Smadja, McKeown, and Hatzivassiloglou, 1996" startWordPosition="567" endWordPosition="571">ften with respect to identifying relevant multi-word terms (e.g. (Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations. Termight (Dagan and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface. The Champollion system (Smadja, McKeown, and Hatzivassiloglou, 1996) moves toward higher accuracy (around 73%) and considerably greater flexibility in the handling of multi-word translations, though the algorithm has been applied primarily to very large corpora such as the Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no 340 attempt is made to distinguish corpus-dependent translations from general ones. Daille et al. (1994) report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for ident</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Spitznagel</author>
<author>J Helzer</author>
</authors>
<title>A proposed solution to the base rate problem in the kappa statistic&amp;quot;.</title>
<date>1985</date>
<journal>Archives of General Psychiatry,</journal>
<volume>42</volume>
<marker>Spitznagel, Helzer, 1985</marker>
<rawString>E. Spitznagel and J. Helzer. &amp;quot;A proposed solution to the base rate problem in the kappa statistic&amp;quot;. Archives of General Psychiatry, 42. July, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>X Xia</author>
</authors>
<title>Learning an EnglishChinese Lexicon from a Parallel Corpus&amp;quot;.</title>
<date>1994</date>
<booktitle>Proceedings of the First Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Columbia, MD.</location>
<marker>Wu, Xia, 1994</marker>
<rawString>D. Wu and X. Xia. 1994. &amp;quot;Learning an EnglishChinese Lexicon from a Parallel Corpus&amp;quot;. Proceedings of the First Conference of the Association for Machine Translation in the Americas, Columbia, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>