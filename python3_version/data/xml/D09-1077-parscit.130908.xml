<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004990">
<title confidence="0.993308">
Word Buffering Models for Improved Speech Repair Parsing∗
</title>
<author confidence="0.996237">
Tim Miller
</author>
<affiliation confidence="0.994552">
University of Minnesota – Twin Cities
</affiliation>
<email confidence="0.99359">
tmill@cs.umn.edu
</email>
<sectionHeader confidence="0.997327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931333333333">
This paper describes a time-series model
for parsing transcribed speech containing
disfluencies. This model differs from pre-
vious parsers in its explicit modeling of a
buffer of recent words, which allows it to
recognize repairs more easily due to the
frequent overlap in words between errors
and their repairs. The parser implement-
ing this model is evaluated on the stan-
dard Switchboard transcribed speech pars-
ing task for overall parsing accuracy and
edited word detection.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.95612108">
Speech repair is a phenomenon in spontaneous
speech where a speaker interrupts the flow of
speech (at what’s called the interruption point),
backtracks some number of words (the reparan-
dum), and continues the utterance with material
meant to replace the reparandum (the alteration).1
The utterance can be rendered syntactically cor-
rect by excising all the words that the speaker
skipped over when backtracking. Speech with re-
pair is difficult for machines to process because in
addition to detecting repair, a system must know
what words are meant to be excised, and parsing
systems must determine how to form a grammat-
ical structure out of the set of words comprising
both the error speech and the correct speech.
Recent approaches to syntactic modeling of
speech with repairs have shown that significant
gains in parsing accuracy can be achieved by mod-
eling the syntax of repairs (Hale et al., 2006;
Core and Schubert, 1999). In addition, others
have shown that a parser based on a time-series
model that explicitly represents the incomplete
∗This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
</bodyText>
<subsectionHeader confidence="0.457523">
1This terminology follows Shriberg (1994).
</subsectionHeader>
<bodyText confidence="0.999961558823529">
constituents in fluent and disfluent speech can also
improve parsing accuracy (Miller and Schuler,
2008). However, these parsing approaches are still
not as accurate at detecting reparanda as classifica-
tion systems which use a variety of features to de-
tect repairs (Charniak and Johnson, 2001; Johnson
and Charniak, 2004; Heeman and Allen, 1999).
One highly salient feature which classification
systems use to detect repair is the repetition of
words between the error and the repair. Johnson
and Charniak report that 60% of words in the al-
terations are copies of words in reparanda in the
Switchboard corpus. Typically, this information
is not available to a parser trained on context-free
grammars.
Meanwhile, psycholinguistic models suggest
that the human language system makes use of
buffers both to keep track of recent input (Bad-
deley et al., 1998) and to smooth out generation
(Levelt, 1989). These buffers are hypothesized
to contain representations of recent phonological
events, suggesting that there is a short window
where new input might be compared to recent in-
put. This could be represented as a buffer which
predicts or detects repeated input in certain con-
strained circumstances.
This paper describes a hybrid parsing sys-
tem operating on transcribed speech which com-
bines an incremental parser implemented as a
probabilistic time-series model, as in Miller and
Schuler, with a buffer of recent words meant to
loosely model something like a phonological loop,
which should better account for word repetition ef-
fects in speech repair.
</bodyText>
<sectionHeader confidence="0.996997" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999872">
This work uses the Switchboard corpus (Godfrey
et al., 1992) for both training and testing. This
corpus contains transcribed and syntactically an-
notated conversations between human interlocu-
tors. The reparanda in speech repairs are ulti-
</bodyText>
<page confidence="0.924455">
737
</page>
<note confidence="0.9966235">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 737–745,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997482770491804">
mately dominated by the EDITED label, and in
cases where the reparandum ends with an unfin-
ished constituent, the lowest constituent label is
augmented with the -UNF tag. These annotations
provide necessary but not sufficient information
for parsing speech with repairs, and thus many im-
provements in performing this task come as the re-
sult of modifying these annotations in the training
data.
As mentioned above, both Hale and colleagues
(2006) and Miller and Schuler (2008) showed
that speech repairs contain syntactic regularities,
which can improve the parsing of transcribed
speech with repairs when modeled properly. Hale
et al. used ‘daughter annotation’, which adds the
label of an EDITED node’s child to the EDITED
label itself, and ‘-UNF propagation’, which la-
bels every node between an original -UNF node
and the EDITED with an -UNF tag. Miller and
Schuler used a ‘right-corner transform’ to convert
standard phrase structure trees of the Penn Tree-
bank into ‘right-corner trees’, which have highly
left-branching structure and non-standard tree cat-
egories representing incomplete constituents be-
ing recognized. These trees can be mapped into a
fixed-depth Hierarchical Hidden Markov Model to
achieve improved parsing and reparandum-finding
results over standard CYK parsers.
Work by Johnson and Charniak (2004; 2001)
uses much of the same structure, but is not a pars-
ing approach per se. In earlier work, they used a
boosting algorithm using word identity and cate-
gory features to classify individual words as part
of a reparandum or not, and achieved very im-
pressive accuracy. More recent work uses a tree-
adjoining grammar (TAG) to model the overlap in
words and part-of-speech tags between reparan-
dum and alteration as context sensitive syntax
trees. A parser is then used to rank the multiple
outputs of the TAG model with reparandum words
removed.
Another approach that makes use of the corre-
spondence between words in the reparandum and
alteration is Heeman and Allen (1999). This ap-
proach uses several sources of evidence, including
word and POS correspondence, to predict repair
beginnings and correct them (by predicting how
far back they are intended to retrace). This model
includes random variables between words that cor-
respond to repair state, and in a repair state, allows
words in the reparandum to ‘license’ words in the
Figure 1: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that
can be used to parse syntax. Circles denote ran-
dom variables, and edges denote conditional de-
pendencies. Shaded circles denote variables with
observed values.
alteration with high probability, accounting for the
high percentage of copied words and POS tags be-
tween reparandum and alteration.
</bodyText>
<sectionHeader confidence="0.982982" genericHeader="method">
3 Model Description
</sectionHeader>
<bodyText confidence="0.999850222222222">
This work is based on a standard Hierarchical Hid-
den Markov Model parser (Schuler, 2009), with
the addition of two new random variables for
tracking the state of speech repair. The HHMM
framework is a desirable starting point for this
work for two reasons: First, its definition in terms
of a graphical model makes it easy to think about
and to add new random variables. Second, the
HHMM parser operates incrementally in a left-to-
right fashion on word input, which allows this sys-
tem to run in a single pass, conditioning current
words on a hypothesized buffer and interruption
point variable. The incremental nature of this sys-
tem is a constraint that other systems are not bound
by, but makes this model more psycholinguisti-
cally plausible. In comparison, a CYK parsing
framework attempting to use the same probabilis-
tic model of word dependency between reparanda
and alterations would need to do a second pass af-
ter obtaining the most likely parses, in order to tell
if a particular word’s generation probability in a
specific parse is influenced by a recent repair.
The graphical model representation of this
framework is illustrated in Figures 1 and 4. The
original model, shown in Figure 1, has complex
variables Q and F broken down into several qdt
and fdt for time step t and depth d. These ran-
</bodyText>
<figure confidence="0.804607833333333">
. . .
. . .
. . .
. . .
f1t−1
f2t−1
f3t−1
ot−1
q1t−1
q2t−1
q3t−1
f1t
f2t
f3t
q1t
q2t
q3t
ot
</figure>
<page confidence="0.992473">
738
</page>
<bodyText confidence="0.999891375">
dom variables will be explained shortly, but for
now suffice it to say that in this work they are un-
altered from the original HHMM parsing frame-
work, while those labeled I and B (Figure 4) are
additions specific to the system described in this
paper. This section will next describe the stan-
dard HHMM parsing framework, before describ-
ing how this work augments it.
</bodyText>
<subsectionHeader confidence="0.995251">
3.1 Right-corner Transform
</subsectionHeader>
<bodyText confidence="0.999923028571429">
The HHMM parser consists of stacks of a fixed
depth, which contain hypotheses of constituents
that are being processed. In order to minimize
the number of stack levels needed in processing,
the phrase structure trees in the training set are
modified using a ‘right-corner transform’, which
converts right expansion in trees to left expansion,
leaving heavily left-branching structure requiring
little depth. The right-corner transform used in
this paper is simply the left-right dual of a left-
corner transform (Johnson, 1998a).
The right-corner transform can be defined as
a recursive algorithm on phrase-structure trees in
Chomsky Normal Form (CNF). Trees are con-
verted to CNF first by binarizing using stan-
dard linguistically-motivated techniques (Klein
and Manning, 2003; Johnson, 1998b). Remaining
unbinarized structure is binarized in a brute force
fashion, creating right-branching structure by cre-
ating a single node which dominates the two right-
most children of a ‘super-binary’ tree, with the la-
bel being the concatenation of its children’s labels
(see Figure 2).
Taking this CNF phrase structure tree as input,
the right-corner transform algorithm keeps track
of two separate trees, the original and the new
right-corner tree it is building. This process be-
gins at the right-most preterminal of the original
tree, and works its way up along the right ‘spine’,
while building its way down a corresponding left
spine of the new right-corner tree. The trees be-
low shows the first step of the algorithm, with the
tree on the left being disassembled, the tree on the
right being built from its parts, and the working
positions in the trees shown in bold.
</bodyText>
<figure confidence="0.8981125">
A
B X
b Y:T Z
z
</figure>
<bodyText confidence="0.996949264705882">
The bottom right corner of the original tree is
made the top right corner of the new tree, and the
left corner of the new tree is made the new working
position and given a ‘slash’ category A/Z. The
‘slash’ category label A/Z represents a tree that
is the start of a constituent of type A that needs
a right-child of type Z in order to complete. The
new right-corner of the original tree is the parent
(X) of the previous right corner, and its subtree is
now added to the right-corner derivation:
After the first step, the subtrees moved over to
the right-corner tree may have more complex sub-
structure than a single word (in this case, T rep-
resents that possibly complex structure). After be-
ing attached to the right-corner tree in the correct
place, the algorithm is recursively applied to that
now right-branching substructure.
Again, the left child is given a new slash cat-
egory: The ‘active constituent’ (the left side of a
slash category) is inherited from the root, and the
‘awaited constituent’ (the right side of a slash cat-
egory) is taken from the constituent label of the
right-corner it came from.
This algorithm proceeds iteratively up the right
spine of the original tree, moving structure to the
right-corner tree and recursively transforming it as
it is added. The final step occurs when the original
root (A in this case) is reduced to having a single
child, in which case its child is added as a child
of the leftmost current branch of the right-corner
tree, and it is transformed recursively.
Figures 2 and 3 show an example tree from
the Switchboard corpus before and after the right-
corner transform is applied.
</bodyText>
<figure confidence="0.986662789473684">
A
A
B
X
A/Z
Z
b
Y:T
A/X
Y:T
z
·
·
z
A
A/Z
Z
739
S
</figure>
<figureCaption confidence="0.986720428571428">
Figure 2: Input to the right-corner transform. This
tree also shows an example of the ‘brute-force’ bi-
narization done on super-binary branches that can-
not be otherwise be binarized with linguistically-
motivated rules.
Figure 3: Right-corner transformed version of the
tree in Figure 2.
</figureCaption>
<subsectionHeader confidence="0.999379">
3.2 Hierarchical Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.999187605263158">
A Hierarchical Hidden Markov Model is essen-
tially an HMM with a specific factorization that
is useful in many domains — the hidden state at
each time step is factored into d random variables
which function as a stack, and d additional ran-
dom variables which regulate the operations of the
stack through time. For the model of speech repair
presented here, an interruption point is identified
by one of these regulator variables firing earlier
than it would in fluent speech. This concept will
be formalized below. The stack regulating random
variables are typically marginalized out when per-
forming inference on a sequence.
While the vertical direction of the hidden sub-
states (at a fixed t) represents a stack at a sin-
gle point in time, the horizontal direction of the
hidden sub-states (at a fixed d) can be viewed as
a simple HMM at depth d, expanding the state
from the HMM above it across multiple time steps
and causing the HMM below it to expand its own
states. This interpretation will be useful when for-
mally defining the transitions between the stack el-
ements at different time steps below.
Formally, HMMs characterize speech or text as
a sequence of hidden states qt (which may con-
sist of speech sounds, words, and/or other hypoth-
esized syntactic or semantic information), and ob-
served states ot at corresponding time steps t (typ-
ically short, overlapping frames of an audio sig-
nal, or words or characters in a text processing
application). A most likely sequence of hidden
states ˆq1..T can then be hypothesized given any se-
quence of observed states o1..T, using Bayes’ Law
(Equation 2) and Markov independence assump-
tions (Equation 3) to define a full P(q1..T  |o1..T)
probability as the product of a Language Model
(OL) prior probability and an Observation Model
(OO) likelihood probability:
</bodyText>
<equation confidence="0.9992128">
P(q1..T |o1..T) (1)
P(q1..T) · P(o1..T  |q1..T) (2)
T POL(qt  |qt–1)·POO(ot  |qt)
H (3)
t=1
</equation>
<bodyText confidence="0.9984559375">
Language model transitions POL(qt  |qt−1) over
complex hidden states qt can be modeled us-
ing synchronized levels of stacked-up compo-
nent HMMs in a Hierarchic Hidden Markov
Model (HHMM) (Murphy and Paskin, 2001).
HHMM transition probabilities are calculated in
two phases: a ‘reduce’ phase (resulting in an in-
termediate, marginalized state ft), in which com-
ponent HMMs may terminate; and a ‘shift’ phase
(resulting in a modeled state qt), in which unter-
minated HMMs transition, and terminated HMMs
are re-initialized from their parent HMMs. Vari-
ables over intermediate ft and modeled qt states
are factored into sequences of depth-specific vari-
ables — one for each of D levels in the HMM hi-
erarchy:
</bodyText>
<equation confidence="0.999733666666667">
ft = hf1t ... fDt i (4)
D
qt = hqt1 ... qt i (5)
</equation>
<bodyText confidence="0.999600333333333">
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific ‘reduce’ OF and ‘shift’ OQ mod-
</bodyText>
<figure confidence="0.952569382352941">
INTJ
INTJ S
so
INTJ
S
uh
NP
VP
live
IN
NP
in
dallas
you
PP
VBP
S
S/NP NP
S/PP IN dallas
S/VP VBP in
S/S NP live
S/INTJ S
INTJ
so
INTJ
you
uh
ˆq1..T = argmax
q1..T
= argmax
q1..T
def
= argmax
q1..T
</figure>
<page confidence="0.768684">
740
</page>
<bodyText confidence="0.971983791666667">
with fD+1
t and q0t defined as constants.
Shift and reduce probabilities are now defined
in terms of finitely recursive FSAs with probabil-
ity distributions over transition, recursive expan-
sion, and final-state status of states at each hierar-
chy level. In the HHMM used in this paper, each
intermediate state variable is a reduction state vari-
able fdt ∈ G ∪ {0, 1} (where G is the set of all
nonterminal symbols from the original grammar),
representing a reduction to the final syntactic state
in G, a horizontal transition to a new awaited cate-
gory, or a top-down transition to a new active cat-
egory. Each modeled state variable is a syntactic
element (qdt ∈ G × G) with an active and awaited
category represented with the slash notation.
The intermediate variable fdt is probabilistically
determined given a reduction at the stack level be-
low, but is deterministically 0 in the case of a non-
reduction at the stack level below. 2
where fD+1 ∈ G and q0t = ROOT.
Shift probabilities at each level are defined
using level-specific transition ΘQ-T and expan-
sion ΘQ-E models:
</bodyText>
<equation confidence="0.928516384615385">
PΘQ(qd t  |fd+1
t fdt qd t−1qd−1
t
def
ifftd+1 /∈G, fdt /∈G: [qdt =qdt−1]
{ if
t ∈G, fdt /∈G: PΘQ-T(qd t  |fd+1
t )
t fd t qd t−1qd−1
if fd+1
if fd+1 t∈G, fdt ∈G: PΘQ-E(qdt  |qd−1
t )
(9)
</equation>
<bodyText confidence="0.9957936">
where fD+1 ∈ G and q0t = ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current HHMM level.
If there is no final state immediately below the cur-
rent level (the first case above), it deterministically
</bodyText>
<footnote confidence="0.7964735">
2Here [·] is an indicator function: [0] = 1 if 0 is true, 0
otherwise.
</footnote>
<bodyText confidence="0.999873409090909">
copies the current HHMM state forward to the
next time step. If there is a final state immediately
below the current level (the second case above),
it transitions the HHMM state at the current level,
according to the distribution ΘQ-T. And if the state
at the current level is final (the third case above), it
re-initializes this state given the state at the level
above, according to the distribution ΘQ-E. The
overall effect is that higher-level HMMs are al-
lowed to transition only when lower-level HMMs
terminate. An HHMM therefore behaves like a
probabilistic implementation of a pushdown au-
tomaton (or ‘shift-reduce’ parser) with a finite
stack, where the maximum stack depth is equal to
the number of levels in the HHMM hierarchy.
All of the probability distributions defined
above can be estimated by training on a corpus of
right-corner transformed trees, by mapping tree el-
ements onto the random variables in the HHMM
and computing conditional probability tables at
each random variable. This process is described in
more detail in other work (Schuler et al., in press).
</bodyText>
<subsectionHeader confidence="0.997954">
3.3 Interruption Point and Word Buffer
</subsectionHeader>
<bodyText confidence="0.99970244">
This paper expands upon this standard HHMM
parsing model by adding two new sub-models to
the hidden variables described above, an interrup-
tion point (I) variable, and a word buffer (B) .
This model is illustrated in Figure 4, which takes
Figure 1 as a starting point and adds random vari-
ables just mentioned.
Buffers are hypothesized to be used in the hu-
man language system to smooth out delivery of
speech (Levelt, 1989). In this work, a buffer of
that sort is placed between the syntax generating
elements and the observed evidence (words). Its
role in this model is not to smooth the flow of
speech, but to keep a short memory that enables
the speaker to conveniently and helpfully restart
when a repair is produced. This in turn gives as-
sistance to a listener trying to understand what the
speaker is saying, since the listener also has the
last few words in memory.
The I variable implements a state machine that
keeps track of the repair status at each time point.
The domain of this variable is {0, 1, ET}, where
1 indicates the first word of an alteration, ET in-
dicates editing terms in between reparandum and
alteration, and 0 indicating no repair.3
</bodyText>
<equation confidence="0.701268">
t−1 ) (8)
</equation>
<bodyText confidence="0.990476">
3Actually, 0 can occur during an alteration, but in those
cases that fact is indicated by the state of the buffer.
</bodyText>
<equation confidence="0.998781458333333">
P(ft|qt–1)·P(qt|ft qt–1) (6)
PΘF(fdt  |ftd+1qdt–1 qt_1 )·
PΘQ(qdt |fd+1
t fdt qd t–1qd–1
t )
(7)
els:
�
PΘ,(qt|qt–1) =
ft
�def =
f1..D
t
D
H
d=1
PΘF(fdt  |fd+1
t qdt−1qd−1
t−1 )def =
�ifif ftd+1 ∈/ G : [fd0]fd+1
t∈
: PΘF-Redu
G
ce(fdt  |qd t−1, qd−1
</equation>
<page confidence="0.994676">
741
</page>
<figureCaption confidence="0.985538">
Figure 4: Extended HHMM parsing model with variables for interruption points (I) and a modeled word
buffer (B). Arrows within and between complex hidden variables F and Q have been removed for clarity.
</figureCaption>
<figure confidence="0.996760484848485">
Ft−2
Ft−1
Ft
f1t−2
Qt−2
f1t−1
Qt−1
f1t
Qt
f2t−2
f3t−2
q1t−2
q2t−2
q3t−2
f2t−1
f3t−1
q1t−1
q2t−1
q3t−1
f2t
f3t
q1t
q2t
q3t
it−2
it−1
it
bt−2
bt−1
bt
ot−2
ot−1
ot
</figure>
<bodyText confidence="0.997616285714286">
The value of I is deterministically constrained
in this work by its inputs, but it can be conceived
as a conditional probability P(it  |it−1, qt, qt−1, rt)
to allow footholds for future research.4 While
depending formally on many values, in practice
its dependencies are highly context-dependent and
constrained:
</bodyText>
<equation confidence="0.9908605">
P(it  |it−1, qt, qt−1, qt) def=
{ if it−1 =1 : [it = 0]
</equation>
<construct confidence="0.967600666666667">
if it−1 = ET n (INT J V PRN) E qt : [it = ET]
if it−1 = ET : [it =1]
if it−1 = 0 n EDITED E (qt−1 U ft)
n(INTJ V PRN) E qt : [it =ET]
if it−1 = 0 n EDITED E (qt−1 U ft) : [it =1]
if it−1 = 0 : [it = 0]
</construct>
<bodyText confidence="0.972299814814815">
These conditions are meant to be evaluated in
a short-circuiting fashion, i.e., the first condition
which is true starting from the top is applied. The
default (last) case is most common, going from
non-repair to non-repair state. When the syntax
generated something with the category EDITED
at the last time step (as evidenced by either the
modeled state variable qt−1 or the reduction state
variable ft depending on the length of the reparan-
dum), the interruption point variable is triggered to
change, either to ET if an interjection (INTJ) or
4Most obviously, this variable could be made prior to its
conditions to be their cause, if a suitable model for the causa-
tion of interruption points was designed using prosodic cues.
For this work, it is simply an intermediary that is not strictly
necessary but makes the model design more intuitive.
parenthetical (PRN) followed, otherwise to 1 for
the first word of an alteration. The ET state con-
tinues as long as the syntax at the current level is
generating something containing INTJ or PRN.
The random variable for the word buffer is more
complex, containing at each time step t an integer
index for keeping track of a current position in the
buffer (ct E (0, 1, ... , n −1) for buffer size n),
and an array of several recently generated words
(wt). This can be represented as the following con-
ditional probability:
</bodyText>
<equation confidence="0.997524">
P(bt  |bt−1, it, qt) = P(ct  |ct−1, it)
P(wt  |wt−1, ct) (10)
</equation>
<bodyText confidence="0.988868222222222">
The operation of the buffer is governed by four
cases:
Case 1: During normal operation (i.e. for fluent
speech), the interruption point variable is 0 and
at the previous time step the buffer index points
at the end of the buffer (it = 0 n ct−1 = n −1). In
this simple case, the buffer pointer remains point-
ing at the end position in the buffer (ct = n −1),
and the last n −1 items in the buffer are determin-
istically copied backwards one position. A new
word is generated probabilistically to occupy the
last position in the buffer (where ct is pointing).
This probability is estimated empirically using the
same model used in a standard HHMM to gener-
ate words, by conditioning the word on the deepest
non-empty qt value in the stack.
Case 2: When an editing term is being gener-
ated, (it = ET), the buffer is not in use. Practi-
</bodyText>
<page confidence="0.993132">
742
</page>
<bodyText confidence="0.999938958762887">
cally, this means that the value of the index c and
all wj are just copied over from time t−1 to time
t. This makes sense psycholinguistically, because
a buffer used to smooth speech rates would by def-
inition not be used when speech is interrupted by
a repair. It also makes sense from a purely engi-
neering point of view, since words used as editing
terms are usually stock phrases and filled pauses
that are not likely to have much predictive value
for the alteration, and are thus not worth keeping in
the buffer. The probability of the actual observed
word is modeled the same way word probabilities
are modeled in a standard HHMM, conditioned on
the deepest non-empty qt value, and ignoring the
buffer.
Case 3: The alteration case applies to the first
word after the reparandum and optional editing
terms (it =1). In this case, the index ct for the cur-
rent position of the buffer is obtained by subtract-
ing a number of words to replace, with that num-
ber drawn from a prior distribution. This distribu-
tion is based on the function f(k) = 1.22 · 0.45k.
This function was taken from Shriberg (1996),
where it was estimated based on several differ-
ent training corpora, and provided a remarkable
fit to all of them. Since this model uses a fixed
size buffer, the values are precomputed and renor-
malized to form a probability distribution. With
a buffer size of only n = 4, approximately 96%
of the probability mass of the original function is
accounted for.
After the indices are computed, the buffer at po-
sition ct is given a word value. The model first
decides whether to substitute or copy the previous
word over. The probability governing this decision
is also determined empirically, by computing how
often the first word in a alteration in the Switch-
board training set is a copy of the first word it is
meant to replace. If the copy operation is selected,
the word is added to the buffer without further di-
luting its probability. If, however, the substitution
operation was selected, the word is added to the
buffer with probability distributed across all pos-
sible words.
Case 4: The final case to account for
is alterations of length greater than one
(it = 0 n ct−1 =� n−1). This occurs when the
current index was moved back more than one
position, and so even though i is set to 0, the
current index into the buffer is not pointing at the
end. In this case, again the index ct is selected
according to a prior probability distribution. The
value selected from the distribution corresponds
to different actions that may be selected when
retracing the words in the reparandum to generate
the alteration.
The first option is that the current index remains
in place, which corresponds to an insertion oper-
ation, where the alteration is given an extra word
relative to the reparandum at its current position.
Following an insertion, a new word is generated
and placed in the buffer at the current index, with
probability conditioned on the syntax at the most
recent time step. The second option is to continue
the alignment, moving the current index forward
one position in the buffer, and then either perform-
ing a substitution or copy operation in alignment
with a word from the alteration. Word probabil-
ities for the copy and substitution operations are
generated in the same way as for the first word of
an alteration. Finally, the current index may skip
forward more than one value, performing a dele-
tion operation. Deletion skips over words in the
reparandum that do not correspond to words in the
alteration. After the deletion moves the current in-
dex pointer forward, a word is again either copied
or substituted against the newly aligned word.
The prior probability distributions over align-
ment operations is estimated from data in the
Switchboard in a similar manner to Johnson and
Charniak (2004). Briefly, using the disfluency-
annotated section of the Switchboard corpus (.dps
files), a list of reparanda and alterations corre-
sponding to one another are compiled. For each
pair, the minimal cost alignment is computed,
where a copy operation has cost 0, substitution
has cost 4, and deletion and insertion each have
cost 7. Using these alignments, probabilities are
computed using relative frequency counts for both
the first word of an alteration, and for subsequent
operations. Copy and substitution are the most fre-
quent operations (copying gives information about
the repair itself, while substitution can correct the
reason for the error), insertion is somewhat less
frequent (presumably for specifying further infor-
mation), and deletion is relatively rare (usually a
repair is not made to remove information).
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999640333333333">
This model was evaluated on the Switchboard
corpus (Godfrey et al., 1992) of conversational
telephone speech between two human interlocu-
</bodyText>
<page confidence="0.995183">
743
</page>
<table confidence="0.9995775">
System Precision Recall F-Score
Plain CYK 18.01 17.73 17.87
Hale et al. CYK 40.90 35.41 37.96
Hale et al. Lex. n/a n/a 70.0
TAG 82.0 77.8 79.7
Plain HHMM 43.90 47.36 45.57
HHMM-Back 44.12 57.49 49.93
HHMM-Retrace 48.82 59.41 53.59
</table>
<tableCaption confidence="0.970258666666667">
Table 1: Table of results of edit-finding accuracy.
Italics indicate reported, rather than reproduced,
results.
</tableCaption>
<table confidence="0.999619714285714">
System Configuration Parseval-F Edited-F
Plain CYK 71.03 17.9
Hale et al. CYK 68.47 37.96
Hale et al. Lex. 80.16 70.0
Plain HHMM 74.23 45.57
HHMM-Back 74.58 49.93
HHMM-Retrace 74.23 53.59
</table>
<tableCaption confidence="0.999416">
Table 2: Table of parsing results.
</tableCaption>
<bodyText confidence="0.999915421052632">
tors. The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. The standard train/test breakdown was
used, with sections 2 and 3 used for training, and
subsections 0 and 1 of section 4 used for testing.
Several held-out sentences from the end of section
4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
and partial words – anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
Results are shown in Tables 1 and 2. Table 1
shows detailed results on edited word finding, with
two test systems and several related approaches.
The first two lines show results from a re-
implementation of Hale et al. parsers. In both
those cases, gold standard part-of-speech (POS)
tags were supplied to the parser. The follow-
ing two lines are reported results of a lexicalized
parser from Hale et al. and the TAG system of
Johnson and Charniak. The final three lines are
evaluations of HHMM systems. The first is an
implementation of Miller and Schuler, run with-
out gold standard POS tags as input. The second
HHMM result is a system much like that described
in this paper, but designed to approximate the best
result that can come from simply trying to match
the first word of an alteration with a recent word.
Levelt (1989) notes that in over 90% of repairs, the
first word of the alteration is either identical or a
member of the same category as the first word of
the reparandum, and this clue is enough for listen-
ers to understand what the alteration is meant to
replace. This implementation keeps the I variable
to model repair state, but rather than a modeled
buffer being part of the hidden state, it keeps an
observed buffer that simply tracks the last n words
seen (n = 4 in this experiment). This buffer is
used only to generate the first word of a repair, and
only when the syntactic state allows the word. Fi-
nally, the system described in Section 3 is shown
on the final line.
Table 2 shows overall parsing accuracy results,
with the same set of systems, with the exception
of the TAG system which did not report parsing
results.
</bodyText>
<sectionHeader confidence="0.998686" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.9999805">
These results first show that the main contribution
of this paper, a model for a buffer of recent words
which influences speech repairs, results in drastic
improvements in the ability of an HHMM system
to discover edited words. This model does this in
a single pass through the observed words, incre-
mentally forming hypotheses about the state of the
syntactic process as well as the state of repair, just
as humans must recognize spontaneous speech.
Another interesting result is the relative effec-
tiveness of a buffer that is not modeled, but rather
just a collection of words used to condition the first
words of repair (‘HHMM-Back’). While this re-
sult is superior to the plain HHMM system, it still
falls well short of the retracing model using a mod-
eled buffer. This suggests that, though one word
is sufficient to align a reparandum and alteration
when the existence of a repair is given, more in-
formation is often necessary when the task is not
just alignment of repair but also detection of re-
</bodyText>
<page confidence="0.99234">
744
</page>
<bodyText confidence="0.9999315">
pair. A model that takes into account information
sources that identify the existence of repair, such
as prosodic cues (Hale et al., 2006; Lickley, 1996),
may thus result in improved performance for the
simpler unmodeled buffer.
These results also confirm that parsing sponta-
neous speech with an HHMM can be far superior
to a CKY parser, even when the CKY parser is
given the advantage of correct POS tags as input.
Second, even the baseline HHMM system also
improves over the CYK parser in finding edited
words, again without the advantage of correct POS
tags as input.
In conclusion, the model described here uses a
buffer inspired by the phonological loop used in
the human auditory system to keep a short mem-
ory of recent input. This model, when used to as-
sist in the detection and correction of repair, re-
sults in a large increase in accuracy in detection
of repair over other most basic parsing systems.
This system does not reach the performance lev-
els of lexicalized parsers, nor multi-pass classifi-
cation systems. Future work will explore ways to
apply additional features of these systems or other
sources of information to account for the remain-
der of the performance gap.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831">
Alan Baddeley, Susan Gathercole, and Costanza Pa-
pagno. 1998. The phonological loop as a language
learning device. Psychological Review, 105(1):158–
173, January.
Eugene Charniak and Mark Johnson. 2001. Edit de-
tection and parsing for transcribed speech. In 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 118–
126.
Mark G. Core and Lenhart K. Schubert. 1999. A syn-
tactic framework for speech repairs and other disrup-
tions. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL
99).
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517–520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Peter A. Heeman and James F. Allen. 1999. Speech
repairs, intonational phrases, and discourse markers:
Modeling speakers’ utterances in spoken dialogue.
Computational Linguistics, 25:527–571.
Mark Johnson and Eugene Charniak. 2004. A tag-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL ’04),
pages 33–39, Barcelona, Spain.
Mark Johnson. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619–623.
Mark Johnson. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613–
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423–430.
Willem J.M. Levelt. 1989. Speaking: From Intention
to Articulation. MIT Press.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings of The Fourth International Conference
on Spoken Language Processing (ICSLP ’96), pages
2478–2481.
Tim Miller and William Schuler. 2008. A syntac-
tic time-series model for parsing fluent and dis-
fluent speech. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING’08).
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833–840.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. in press. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics.
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ’09), Boul-
der, Colorado.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California at Berkeley.
Elizabeth Shriberg. 1996. Disfluencies in Switch-
board. In Proceedings of International Conference
on Spoken Language Processing.
</reference>
<page confidence="0.99854">
745
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.887904">
<title confidence="0.999588">Buffering Models for Improved Speech Repair</title>
<author confidence="0.999884">Tim Miller</author>
<affiliation confidence="0.9997">University of Minnesota – Twin</affiliation>
<email confidence="0.998854">tmill@cs.umn.edu</email>
<abstract confidence="0.991340769230769">This paper describes a time-series model for parsing transcribed speech containing disfluencies. This model differs from previous parsers in its explicit modeling of a buffer of recent words, which allows it to recognize repairs more easily due to the frequent overlap in words between errors and their repairs. The parser implementing this model is evaluated on the standard Switchboard transcribed speech parsing task for overall parsing accuracy and edited word detection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Baddeley</author>
<author>Susan Gathercole</author>
<author>Costanza Papagno</author>
</authors>
<title>The phonological loop as a language learning device.</title>
<date>1998</date>
<journal>Psychological Review,</journal>
<volume>105</volume>
<issue>1</issue>
<pages>173</pages>
<contexts>
<context position="2690" citStr="Baddeley et al., 1998" startWordPosition="420" endWordPosition="424">ty of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Meanwhile, psycholinguistic models suggest that the human language system makes use of buffers both to keep track of recent input (Baddeley et al., 1998) and to smooth out generation (Levelt, 1989). These buffers are hypothesized to contain representations of recent phonological events, suggesting that there is a short window where new input might be compared to recent input. This could be represented as a buffer which predicts or detects repeated input in certain constrained circumstances. This paper describes a hybrid parsing system operating on transcribed speech which combines an incremental parser implemented as a probabilistic time-series model, as in Miller and Schuler, with a buffer of recent words meant to loosely model something like</context>
</contexts>
<marker>Baddeley, Gathercole, Papagno, 1998</marker>
<rawString>Alan Baddeley, Susan Gathercole, and Costanza Papagno. 1998. The phonological loop as a language learning device. Psychological Review, 105(1):158– 173, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In 2nd Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="2128" citStr="Charniak and Johnson, 2001" startWordPosition="331" endWordPosition="334">irs (Hale et al., 2006; Core and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Meanwhile, psycholinguistic models suggest that the human language system makes use of buffers both to keep track of recent input (Baddeley et al., 1998) and to smooth out generation (Levelt,</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pages 118– 126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>Lenhart K Schubert</author>
</authors>
<title>A syntactic framework for speech repairs and other disruptions.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 99).</booktitle>
<contexts>
<context position="1550" citStr="Core and Schubert, 1999" startWordPosition="243" endWordPosition="246">be rendered syntactically correct by excising all the words that the speaker skipped over when backtracking. Speech with repair is difficult for machines to process because in addition to detecting repair, a system must know what words are meant to be excised, and parsing systems must determine how to form a grammatical structure out of the set of words comprising both the error speech and the correct speech. Recent approaches to syntactic modeling of speech with repairs have shown that significant gains in parsing accuracy can be achieved by modeling the syntax of repairs (Hale et al., 2006; Core and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak</context>
</contexts>
<marker>Core, Schubert, 1999</marker>
<rawString>Mark G. Core and Lenhart K. Schubert. 1999. A syntactic framework for speech repairs and other disruptions. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="3459" citStr="Godfrey et al., 1992" startWordPosition="542" endWordPosition="545">at there is a short window where new input might be compared to recent input. This could be represented as a buffer which predicts or detects repeated input in certain constrained circumstances. This paper describes a hybrid parsing system operating on transcribed speech which combines an incremental parser implemented as a probabilistic time-series model, as in Miller and Schuler, with a buffer of recent words meant to loosely model something like a phonological loop, which should better account for word repetition effects in speech repair. 2 Background This work uses the Switchboard corpus (Godfrey et al., 1992) for both training and testing. This corpus contains transcribed and syntactically annotated conversations between human interlocutors. The reparanda in speech repairs are ulti737 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 737–745, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP mately dominated by the EDITED label, and in cases where the reparandum ends with an unfinished constituent, the lowest constituent label is augmented with the -UNF tag. These annotations provide necessary but not sufficient information for parsing speech with repairs</context>
<context position="27185" citStr="Godfrey et al., 1992" startWordPosition="4645" endWordPosition="4648">d deletion and insertion each have cost 7. Using these alignments, probabilities are computed using relative frequency counts for both the first word of an alteration, and for subsequent operations. Copy and substitution are the most frequent operations (copying gives information about the repair itself, while substitution can correct the reason for the error), insertion is somewhat less frequent (presumably for specifying further information), and deletion is relatively rare (usually a repair is not made to remove information). 4 Evaluation This model was evaluated on the Switchboard corpus (Godfrey et al., 1992) of conversational telephone speech between two human interlocu743 System Precision Recall F-Score Plain CYK 18.01 17.73 17.87 Hale et al. CYK 40.90 35.41 37.96 Hale et al. Lex. n/a n/a 70.0 TAG 82.0 77.8 79.7 Plain HHMM 43.90 47.36 45.57 HHMM-Back 44.12 57.49 49.93 HHMM-Retrace 48.82 59.41 53.59 Table 1: Table of results of edit-finding accuracy. Italics indicate reported, rather than reproduced, results. System Configuration Parseval-F Edited-F Plain CYK 71.03 17.9 Hale et al. CYK 68.47 37.96 Hale et al. Lex. 80.16 70.0 Plain HHMM 74.23 45.57 HHMM-Back 74.58 49.93 HHMM-Retrace 74.23 53.59 Ta</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proc. ICASSP, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
<author>Izhak Shafran</author>
<author>Lisa Yung</author>
<author>Bonnie Dorr</author>
<author>Mary Harper</author>
<author>Anna Krasnyanskaya</author>
<author>Matthew Lease</author>
<author>Yang Liu</author>
<author>Brian Roark</author>
<author>Matthew Snover</author>
<author>Robin Stewart</author>
</authors>
<title>PCFGs with syntactic and prosodic indicators of speech repairs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</booktitle>
<contexts>
<context position="1524" citStr="Hale et al., 2006" startWordPosition="239" endWordPosition="242"> The utterance can be rendered syntactically correct by excising all the words that the speaker skipped over when backtracking. Speech with repair is difficult for machines to process because in addition to detecting repair, a system must know what words are meant to be excised, and parsing systems must determine how to form a grammatical structure out of the set of words comprising both the error speech and the correct speech. Recent approaches to syntactic modeling of speech with repairs have shown that significant gains in parsing accuracy can be achieved by modeling the syntax of repairs (Hale et al., 2006; Core and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, </context>
<context position="31646" citStr="Hale et al., 2006" startWordPosition="5408" endWordPosition="5411">is not modeled, but rather just a collection of words used to condition the first words of repair (‘HHMM-Back’). While this result is superior to the plain HHMM system, it still falls well short of the retracing model using a modeled buffer. This suggests that, though one word is sufficient to align a reparandum and alteration when the existence of a repair is given, more information is often necessary when the task is not just alignment of repair but also detection of re744 pair. A model that takes into account information sources that identify the existence of repair, such as prosodic cues (Hale et al., 2006; Lickley, 1996), may thus result in improved performance for the simpler unmodeled buffer. These results also confirm that parsing spontaneous speech with an HHMM can be far superior to a CKY parser, even when the CKY parser is given the advantage of correct POS tags as input. Second, even the baseline HHMM system also improves over the CYK parser in finding edited words, again without the advantage of correct POS tags as input. In conclusion, the model described here uses a buffer inspired by the phonological loop used in the human auditory system to keep a short memory of recent input. This</context>
</contexts>
<marker>Hale, Shafran, Yung, Dorr, Harper, Krasnyanskaya, Lease, Liu, Roark, Snover, Stewart, 2006</marker>
<rawString>John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary Harper, Anna Krasnyanskaya, Matthew Lease, Yang Liu, Brian Roark, Matthew Snover, and Robin Stewart. 2006. PCFGs with syntactic and prosodic indicators of speech repairs. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>Speech repairs, intonational phrases, and discourse markers: Modeling speakers’ utterances in spoken dialogue.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--527</pages>
<contexts>
<context position="2181" citStr="Heeman and Allen, 1999" startWordPosition="339" endWordPosition="342">ition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Meanwhile, psycholinguistic models suggest that the human language system makes use of buffers both to keep track of recent input (Baddeley et al., 1998) and to smooth out generation (Levelt, 1989). These buffers are hypothesized to contain rep</context>
<context position="5780" citStr="Heeman and Allen (1999)" startWordPosition="907" endWordPosition="910"> not a parsing approach per se. In earlier work, they used a boosting algorithm using word identity and category features to classify individual words as part of a reparandum or not, and achieved very impressive accuracy. More recent work uses a treeadjoining grammar (TAG) to model the overlap in words and part-of-speech tags between reparandum and alteration as context sensitive syntax trees. A parser is then used to rank the multiple outputs of the TAG model with reparandum words removed. Another approach that makes use of the correspondence between words in the reparandum and alteration is Heeman and Allen (1999). This approach uses several sources of evidence, including word and POS correspondence, to predict repair beginnings and correct them (by predicting how far back they are intended to retrace). This model includes random variables between words that correspond to repair state, and in a repair state, allows words in the reparandum to ‘license’ words in the Figure 1: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies.</context>
</contexts>
<marker>Heeman, Allen, 1999</marker>
<rawString>Peter A. Heeman and James F. Allen. 1999. Speech repairs, intonational phrases, and discourse markers: Modeling speakers’ utterances in spoken dialogue. Computational Linguistics, 25:527–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A tagbased noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04),</booktitle>
<pages>33--39</pages>
<location>Barcelona,</location>
<contexts>
<context position="2156" citStr="Johnson and Charniak, 2004" startWordPosition="335" endWordPosition="338"> and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Meanwhile, psycholinguistic models suggest that the human language system makes use of buffers both to keep track of recent input (Baddeley et al., 1998) and to smooth out generation (Levelt, 1989). These buffers are hy</context>
<context position="5110" citStr="Johnson and Charniak (2004" startWordPosition="792" endWordPosition="795">ED node’s child to the EDITED label itself, and ‘-UNF propagation’, which labels every node between an original -UNF node and the EDITED with an -UNF tag. Miller and Schuler used a ‘right-corner transform’ to convert standard phrase structure trees of the Penn Treebank into ‘right-corner trees’, which have highly left-branching structure and non-standard tree categories representing incomplete constituents being recognized. These trees can be mapped into a fixed-depth Hierarchical Hidden Markov Model to achieve improved parsing and reparandum-finding results over standard CYK parsers. Work by Johnson and Charniak (2004; 2001) uses much of the same structure, but is not a parsing approach per se. In earlier work, they used a boosting algorithm using word identity and category features to classify individual words as part of a reparandum or not, and achieved very impressive accuracy. More recent work uses a treeadjoining grammar (TAG) to model the overlap in words and part-of-speech tags between reparandum and alteration as context sensitive syntax trees. A parser is then used to rank the multiple outputs of the TAG model with reparandum words removed. Another approach that makes use of the correspondence bet</context>
<context position="26279" citStr="Johnson and Charniak (2004)" startWordPosition="4506" endWordPosition="4509"> alteration. Word probabilities for the copy and substitution operations are generated in the same way as for the first word of an alteration. Finally, the current index may skip forward more than one value, performing a deletion operation. Deletion skips over words in the reparandum that do not correspond to words in the alteration. After the deletion moves the current index pointer forward, a word is again either copied or substituted against the newly aligned word. The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004). Briefly, using the disfluencyannotated section of the Switchboard corpus (.dps files), a list of reparanda and alterations corresponding to one another are compiled. For each pair, the minimal cost alignment is computed, where a copy operation has cost 0, substitution has cost 4, and deletion and insertion each have cost 7. Using these alignments, probabilities are computed using relative frequency counts for both the first word of an alteration, and for subsequent operations. Copy and substitution are the most frequent operations (copying gives information about the repair itself, while sub</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A tagbased noisy channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04), pages 33–39, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Finite state approximation of constraint-based grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>619--623</pages>
<contexts>
<context position="8902" citStr="Johnson, 1998" startWordPosition="1436" endWordPosition="1437">rsing framework, before describing how this work augments it. 3.1 Right-corner Transform The HHMM parser consists of stacks of a fixed depth, which contain hypotheses of constituents that are being processed. In order to minimize the number of stack levels needed in processing, the phrase structure trees in the training set are modified using a ‘right-corner transform’, which converts right expansion in trees to left expansion, leaving heavily left-branching structure requiring little depth. The right-corner transform used in this paper is simply the left-right dual of a leftcorner transform (Johnson, 1998a). The right-corner transform can be defined as a recursive algorithm on phrase-structure trees in Chomsky Normal Form (CNF). Trees are converted to CNF first by binarizing using standard linguistically-motivated techniques (Klein and Manning, 2003; Johnson, 1998b). Remaining unbinarized structure is binarized in a brute force fashion, creating right-branching structure by creating a single node which dominates the two rightmost children of a ‘super-binary’ tree, with the label being the concatenation of its children’s labels (see Figure 2). Taking this CNF phrase structure tree as input, the</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998a. Finite state approximation of constraint-based grammars using left-corner grammar transforms. In Proceedings of COLING/ACL, pages 619–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>632</pages>
<contexts>
<context position="8902" citStr="Johnson, 1998" startWordPosition="1436" endWordPosition="1437">rsing framework, before describing how this work augments it. 3.1 Right-corner Transform The HHMM parser consists of stacks of a fixed depth, which contain hypotheses of constituents that are being processed. In order to minimize the number of stack levels needed in processing, the phrase structure trees in the training set are modified using a ‘right-corner transform’, which converts right expansion in trees to left expansion, leaving heavily left-branching structure requiring little depth. The right-corner transform used in this paper is simply the left-right dual of a leftcorner transform (Johnson, 1998a). The right-corner transform can be defined as a recursive algorithm on phrase-structure trees in Chomsky Normal Form (CNF). Trees are converted to CNF first by binarizing using standard linguistically-motivated techniques (Klein and Manning, 2003; Johnson, 1998b). Remaining unbinarized structure is binarized in a brute force fashion, creating right-branching structure by creating a single node which dominates the two rightmost children of a ‘super-binary’ tree, with the label being the concatenation of its children’s labels (see Figure 2). Taking this CNF phrase structure tree as input, the</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998b. PCFG models of linguistic tree representation. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="9151" citStr="Klein and Manning, 2003" startWordPosition="1470" endWordPosition="1473">r of stack levels needed in processing, the phrase structure trees in the training set are modified using a ‘right-corner transform’, which converts right expansion in trees to left expansion, leaving heavily left-branching structure requiring little depth. The right-corner transform used in this paper is simply the left-right dual of a leftcorner transform (Johnson, 1998a). The right-corner transform can be defined as a recursive algorithm on phrase-structure trees in Chomsky Normal Form (CNF). Trees are converted to CNF first by binarizing using standard linguistically-motivated techniques (Klein and Manning, 2003; Johnson, 1998b). Remaining unbinarized structure is binarized in a brute force fashion, creating right-branching structure by creating a single node which dominates the two rightmost children of a ‘super-binary’ tree, with the label being the concatenation of its children’s labels (see Figure 2). Taking this CNF phrase structure tree as input, the right-corner transform algorithm keeps track of two separate trees, the original and the new right-corner tree it is building. This process begins at the right-most preterminal of the original tree, and works its way up along the right ‘spine’, whi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2734" citStr="Levelt, 1989" startWordPosition="430" endWordPosition="431">n, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Meanwhile, psycholinguistic models suggest that the human language system makes use of buffers both to keep track of recent input (Baddeley et al., 1998) and to smooth out generation (Levelt, 1989). These buffers are hypothesized to contain representations of recent phonological events, suggesting that there is a short window where new input might be compared to recent input. This could be represented as a buffer which predicts or detects repeated input in certain constrained circumstances. This paper describes a hybrid parsing system operating on transcribed speech which combines an incremental parser implemented as a probabilistic time-series model, as in Miller and Schuler, with a buffer of recent words meant to loosely model something like a phonological loop, which should better ac</context>
<context position="18166" citStr="Levelt, 1989" startWordPosition="3045" endWordPosition="3046">and computing conditional probability tables at each random variable. This process is described in more detail in other work (Schuler et al., in press). 3.3 Interruption Point and Word Buffer This paper expands upon this standard HHMM parsing model by adding two new sub-models to the hidden variables described above, an interruption point (I) variable, and a word buffer (B) . This model is illustrated in Figure 4, which takes Figure 1 as a starting point and adds random variables just mentioned. Buffers are hypothesized to be used in the human language system to smooth out delivery of speech (Levelt, 1989). In this work, a buffer of that sort is placed between the syntax generating elements and the observed evidence (words). Its role in this model is not to smooth the flow of speech, but to keep a short memory that enables the speaker to conveniently and helpfully restart when a repair is produced. This in turn gives assistance to a listener trying to understand what the speaker is saying, since the listener also has the last few words in memory. The I variable implements a state machine that keeps track of the repair status at each time point. The domain of this variable is {0, 1, ET}, where 1</context>
<context position="29659" citStr="Levelt (1989)" startWordPosition="5059" endWordPosition="5060">al. parsers. In both those cases, gold standard part-of-speech (POS) tags were supplied to the parser. The following two lines are reported results of a lexicalized parser from Hale et al. and the TAG system of Johnson and Charniak. The final three lines are evaluations of HHMM systems. The first is an implementation of Miller and Schuler, run without gold standard POS tags as input. The second HHMM result is a system much like that described in this paper, but designed to approximate the best result that can come from simply trying to match the first word of an alteration with a recent word. Levelt (1989) notes that in over 90% of repairs, the first word of the alteration is either identical or a member of the same category as the first word of the reparandum, and this clue is enough for listeners to understand what the alteration is meant to replace. This implementation keeps the I variable to model repair state, but rather than a modeled buffer being part of the hidden state, it keeps an observed buffer that simply tracks the last n words seen (n = 4 in this experiment). This buffer is used only to generate the first word of a repair, and only when the syntactic state allows the word. Finall</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>Willem J.M. Levelt. 1989. Speaking: From Intention to Articulation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Lickley</author>
</authors>
<title>Juncture cues to disfluency.</title>
<date>1996</date>
<booktitle>In Proceedings of The Fourth International Conference on Spoken Language Processing (ICSLP ’96),</booktitle>
<pages>2478--2481</pages>
<contexts>
<context position="31662" citStr="Lickley, 1996" startWordPosition="5412" endWordPosition="5413"> rather just a collection of words used to condition the first words of repair (‘HHMM-Back’). While this result is superior to the plain HHMM system, it still falls well short of the retracing model using a modeled buffer. This suggests that, though one word is sufficient to align a reparandum and alteration when the existence of a repair is given, more information is often necessary when the task is not just alignment of repair but also detection of re744 pair. A model that takes into account information sources that identify the existence of repair, such as prosodic cues (Hale et al., 2006; Lickley, 1996), may thus result in improved performance for the simpler unmodeled buffer. These results also confirm that parsing spontaneous speech with an HHMM can be far superior to a CKY parser, even when the CKY parser is given the advantage of correct POS tags as input. Second, even the baseline HHMM system also improves over the CYK parser in finding edited words, again without the advantage of correct POS tags as input. In conclusion, the model described here uses a buffer inspired by the phonological loop used in the human auditory system to keep a short memory of recent input. This model, when use</context>
</contexts>
<marker>Lickley, 1996</marker>
<rawString>R. J. Lickley. 1996. Juncture cues to disfluency. In Proceedings of The Fourth International Conference on Spoken Language Processing (ICSLP ’96), pages 2478–2481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Miller</author>
<author>William Schuler</author>
</authors>
<title>A syntactic time-series model for parsing fluent and disfluent speech.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08).</booktitle>
<contexts>
<context position="1940" citStr="Miller and Schuler, 2008" startWordPosition="301" endWordPosition="304">and the correct speech. Recent approaches to syntactic modeling of speech with repairs have shown that significant gains in parsing accuracy can be achieved by modeling the syntax of repairs (Hale et al., 2006; Core and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchboard corpus. Typically, this information is not available to a parser trained on context-free grammars. Mea</context>
<context position="4265" citStr="Miller and Schuler (2008)" startWordPosition="666" endWordPosition="669">roceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 737–745, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP mately dominated by the EDITED label, and in cases where the reparandum ends with an unfinished constituent, the lowest constituent label is augmented with the -UNF tag. These annotations provide necessary but not sufficient information for parsing speech with repairs, and thus many improvements in performing this task come as the result of modifying these annotations in the training data. As mentioned above, both Hale and colleagues (2006) and Miller and Schuler (2008) showed that speech repairs contain syntactic regularities, which can improve the parsing of transcribed speech with repairs when modeled properly. Hale et al. used ‘daughter annotation’, which adds the label of an EDITED node’s child to the EDITED label itself, and ‘-UNF propagation’, which labels every node between an original -UNF node and the EDITED with an -UNF tag. Miller and Schuler used a ‘right-corner transform’ to convert standard phrase structure trees of the Penn Treebank into ‘right-corner trees’, which have highly left-branching structure and non-standard tree categories represen</context>
</contexts>
<marker>Miller, Schuler, 2008</marker>
<rawString>Tim Miller and William Schuler. 2008. A syntactic time-series model for parsing fluent and disfluent speech. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>833--840</pages>
<contexts>
<context position="14160" citStr="Murphy and Paskin, 2001" startWordPosition="2333" endWordPosition="2336">idden states ˆq1..T can then be hypothesized given any sequence of observed states o1..T, using Bayes’ Law (Equation 2) and Markov independence assumptions (Equation 3) to define a full P(q1..T |o1..T) probability as the product of a Language Model (OL) prior probability and an Observation Model (OO) likelihood probability: P(q1..T |o1..T) (1) P(q1..T) · P(o1..T |q1..T) (2) T POL(qt |qt–1)·POO(ot |qt) H (3) t=1 Language model transitions POL(qt |qt−1) over complex hidden states qt can be modeled using synchronized levels of stacked-up component HMMs in a Hierarchic Hidden Markov Model (HHMM) (Murphy and Paskin, 2001). HHMM transition probabilities are calculated in two phases: a ‘reduce’ phase (resulting in an intermediate, marginalized state ft), in which component HMMs may terminate; and a ‘shift’ phase (resulting in a modeled state qt), in which unterminated HMMs transition, and terminated HMMs are re-initialized from their parent HMMs. Variables over intermediate ft and modeled qt states are factored into sequences of depth-specific variables — one for each of D levels in the HMM hierarchy: ft = hf1t ... fDt i (4) D qt = hqt1 ... qt i (5) Transition probabilities are then calculated as a product of tr</context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Kevin P. Murphy and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proc. NIPS, pages 833–840.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>in press. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics.</title>
<marker>Schuler, AbdelRahman, Miller, Schwartz, </marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. in press. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL ’09),</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6676" citStr="Schuler, 2009" startWordPosition="1052" endWordPosition="1053">in a repair state, allows words in the reparandum to ‘license’ words in the Figure 1: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values. alteration with high probability, accounting for the high percentage of copied words and POS tags between reparandum and alteration. 3 Model Description This work is based on a standard Hierarchical Hidden Markov Model parser (Schuler, 2009), with the addition of two new random variables for tracking the state of speech repair. The HHMM framework is a desirable starting point for this work for two reasons: First, its definition in terms of a graphical model makes it easy to think about and to add new random variables. Second, the HHMM parser operates incrementally in a left-toright fashion on word input, which allows this system to run in a single pass, conditioning current words on a hypothesized buffer and interruption point variable. The incremental nature of this system is a constraint that other systems are not bound by, but</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>William Schuler. 2009. Parsing with a bounded stack using a model-based right-corner transform. In Proceedings of the North American Association for Computational Linguistics (NAACL ’09), Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="1834" citStr="Shriberg (1994)" startWordPosition="288" endWordPosition="289">ne how to form a grammatical structure out of the set of words comprising both the error speech and the correct speech. Recent approaches to syntactic modeling of speech with repairs have shown that significant gains in parsing accuracy can be achieved by modeling the syntax of repairs (Hale et al., 2006; Core and Schubert, 1999). In addition, others have shown that a parser based on a time-series model that explicitly represents the incomplete ∗This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1This terminology follows Shriberg (1994). constituents in fluent and disfluent speech can also improve parsing accuracy (Miller and Schuler, 2008). However, these parsing approaches are still not as accurate at detecting reparanda as classification systems which use a variety of features to detect repairs (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Heeman and Allen, 1999). One highly salient feature which classification systems use to detect repair is the repetition of words between the error and the repair. Johnson and Charniak report that 60% of words in the alterations are copies of words in reparanda in the Switchbo</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Disfluencies in Switchboard.</title>
<date>1996</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="23570" citStr="Shriberg (1996)" startWordPosition="4042" endWordPosition="4043">ot worth keeping in the buffer. The probability of the actual observed word is modeled the same way word probabilities are modeled in a standard HHMM, conditioned on the deepest non-empty qt value, and ignoring the buffer. Case 3: The alteration case applies to the first word after the reparandum and optional editing terms (it =1). In this case, the index ct for the current position of the buffer is obtained by subtracting a number of words to replace, with that number drawn from a prior distribution. This distribution is based on the function f(k) = 1.22 · 0.45k. This function was taken from Shriberg (1996), where it was estimated based on several different training corpora, and provided a remarkable fit to all of them. Since this model uses a fixed size buffer, the values are precomputed and renormalized to form a probability distribution. With a buffer size of only n = 4, approximately 96% of the probability mass of the original function is accounted for. After the indices are computed, the buffer at position ct is given a word value. The model first decides whether to substitute or copy the previous word over. The probability governing this decision is also determined empirically, by computin</context>
</contexts>
<marker>Shriberg, 1996</marker>
<rawString>Elizabeth Shriberg. 1996. Disfluencies in Switchboard. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>