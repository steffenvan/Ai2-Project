<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<note confidence="0.438636">
CNRC-TMT:
</note>
<title confidence="0.991531">
Second Language Writing Assistant System Description
</title>
<author confidence="0.996124">
Cyril Goutte Michel Simard Marine Carpuat
</author>
<affiliation confidence="0.7869705">
National Research Council Canada
Multilingual Text Processing
</affiliation>
<address confidence="0.83749">
1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada
</address>
<email confidence="0.989757">
FirstName.LastName@nrc.ca
</email>
<sectionHeader confidence="0.993684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927857142857">
We describe the system entered by the
National Research Council Canada in
the SemEval-2014 L2 writing assistant
task. Our system relies on a standard
Phrase-Based Statistical Machine Transla-
tion trained on generic, publicly available
data. Translations are produced by taking
the already translated part of the sentence
as fixed context. We show that translation
systems can address the L2 writing assis-
tant task, reaching out-of-five word-based
accuracy above 80 percent for 3 out of 4
language pairs. We also present a brief
analysis of remaining errors.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994795595238096">
The Semeval L2 writing assistant task simulates
the situation of an L2 language learner trying to
translate a L1 fragment in a L2 context. This is
clearly motivated by a L2 language learning sce-
nario.
However, a very similar scenario can be en-
countered in Computer-Aided Translation. Trans-
lation memories retrieve from a large corpus of al-
ready translated documents the source segments
that best match a new sentence to be translated.
If an exact source match is found, the correspond-
ing target translation can be expected to be suit-
able with little or no post-editing. However, when
only approximate matches are found, post-editing
will typically be required to adapt the target side
of the partially matching source segment to the
source sentence under consideration. It is possible
to automate this process: standard string matching
algorithms and word alignment techniques can be
used to locate the parts of the source segment that
do not match the sentence to translate, and from
c�2014, The Crown in Right of Canada.
there the parts of the target segment that need to
be modified (Bic¸ici and Dymetman, 2008; Simard
and Isabelle, 2009; Koehn and Senellart, 2010).
The task of translating a L1 fragment in L2 con-
text therefore has much broader application than
language learning. This motivation also provides
a clear link of this task to the Machine Translation
setting. There are also connections to the code-
switching and mixed language translation prob-
lems (Fung et al., 1999).
In our work, we therefore investigate the use
of a standard Phrase-Based Statistical Machine
Translation (SMT) system to translate L1 frag-
ments in L2 context. In the next section, we de-
scribe the SMT system that we used in our submis-
sion. We then describe the corpora used to train
the SMT engine (Section 3), and our results on the
trial and test data, as well as a short error analysis
(Section 4).
section
</bodyText>
<sectionHeader confidence="0.988949" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.938576388888889">
The core Machine Translation engine used for all
our submissions is Portage (Larkin et al., 2010),
the NRC’s phrase-based SMT system. Given a
suitably trained SMT system, the Task 5 input is
processed as follows. For each sentence with an
L1 fragment to translate, the already translated
parts are set as left and right context. The L1 frag-
ment in L2 context is sent to the decoder. The
output is a full sentence translation that ensures 1)
that the context is left untouched, and 2) that the
L1 fragment is translated in a way that fits with the
L2 context.
We now describe the key components of the MT
system (language, translation and reordering mod-
els), as well as the decoding and parameter tuning.
Translation Models We use a single static
phrase table including phrase pairs extracted from
the symmetrized HMM word-alignment learned
</bodyText>
<page confidence="0.977482">
192
</page>
<note confidence="0.73069">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192–197,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9968186">
on the entire training data. The phrase table con-
tains four features per phrase pair: lexical esti-
mates of the forward and backward probabilities
obtained either by relative frequencies or using the
method of Zens and Ney (2004). These estimates
are derived by summing counts over all possible
alignments. This yields four corresponding pa-
rameters in the log-linear model.
Reordering Models We use standard reorder-
ing models: a distance-based distortion feature, as
well as a lexicalized distortion model (Tillmann,
2004; Koehn et al., 2005). For each phrase pair,
the orientation counts required for the lexicalized
distortion model are computed using HMM word-
alignment on the full training corpora. We esti-
mate lexicalized probabilities for monotone, swap,
and discontinuous ordering with respect to the pre-
vious and following target phrase. This results in
a total of 6 feature values per phrase pair, in addi-
tion to the distance-based distortion feature, hence
seven parameters to tune in the log-linear model.
Language Models When translating L1 frag-
ments in L2 context, the L2 language model (LM)
is particularly important as it is the only compo-
nent of the SMT system that scores how well the
translation of the L1 fragment fits in the existing
L2 context. We test two different LM configura-
tions. The first of these (run]) uses a single static
LM: a standard 4-gram, estimated using Kneser-
Ney smoothing (Kneser and Ney, 1995) on the tar-
get side of the bilingual corpora used for training
the translation models. In the second configuration
(runt), in order to further adapt the translations to
the test domain, a smaller LM trained on the L2
contexts of the test data is combined to the train-
ing corpus LM in a linear mixture model (Foster
and Kuhn, 2007). The linear mixture weights are
estimated on the L2 context of each test set in a
cross-validation fashion.
Decoding Algorithm and Parameter Tuning
Decoding uses the cube-pruning algorithm (Huang
and Chiang, 2007) with a 7-word distortion limit.
Log-linear parameter tuning is performed using a
lattice-based batch version of MIRA (Cherry and
Foster, 2012).
</bodyText>
<sectionHeader confidence="0.99722" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.993702666666667">
SMT systems require large amounts of data to
estimate model parameters. In addition, transla-
tion performance largely depends on having in-
</bodyText>
<table confidence="0.999427111111111">
Europarl News Total
train 1904k 177k 2081k
dev - 2000 2000
train 1959k 174k 2133k
dev - 2000 2000
train 2002k 157k 2158k
dev - 2000 2000
train 1974k - 1974k
dev 1984 - 1984
</table>
<tableCaption confidence="0.9498285">
Table 1: Number of training segments for each
language pair.
</tableCaption>
<bodyText confidence="0.996022521739131">
domain data to train on. As we had no informa-
tion on the domain of the test data for Task 5, we
chose to rely on general purpose publicly avail-
able data. Our main corpus is Europarl (Koehn,
2005), which is available for all 4 language pairs
of the evaluation. As Europarl covers parliamen-
tary proceedings, we added some news and com-
mentary (henceforth ”News”) data provided for
the 2013 workshop on Machine Translation shared
task (Bojar et al., 2013) for language pairs other
than nl-en. In all cases, we extracted from the cor-
pus a tuning (“dev”) set of around 2000 sentence
pairs. Statistics for the training data are given in
Table 1.
The trial and test data each consist of 500 sen-
tences with L1 fragments in L2 context provided
by the organizers. As the trial data came from Eu-
roparl, we filtered our training corpora in order to
remove close matches and avoid training on the
trial data (Table 1 takes this into account).
All translation systems were trained on lower-
cased data, and predictions were recased using a
standard (LM-based) truecasing approach.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.994969">
4.1 Results on Trial and Simulated Data
</subsectionHeader>
<bodyText confidence="0.996111666666667">
Our first evaluation was performed on the trial data
provided by the Task 5 organizers. Each example
was translated in context by two systems:
</bodyText>
<listItem confidence="0.82818475">
run1: Baseline, non-adapted system (marked 1
below);
run2: Linear LM mixture adaptation, using a
context LM (marked 2 below).
</listItem>
<bodyText confidence="0.9597615">
Table 2 shows that our run] system already
yields high performance on the trial data, while
en-de
en-es
fr-en
nl-en
</bodyText>
<page confidence="0.997721">
193
</page>
<table confidence="0.9994284375">
W@1 F@1 W@5 F@5 +BLEU
78.1 77.0 95.6 94.8 12.4
79.8 79.0 95.8 95.0 12.6
81.8 80.2 97.7 97.2 12.1
84.3 83.2 97.7 97.2 12.5
84.4 83.6 97.1 96.4 11.8
85.9 85.0 97.4 96.6 12.0
83.3 82.0 97.0 96.4 11.8
86.7 86.2 97.5 97.0 12.1
W@1 F@1 W@5 F@5 +BLEU
48.0 46.4 70.8 68.7 4.26
52.3 50.6 71.0 68.9 4.63
47.6 45.2 68.0 65.8 4.12
50.0 47.9 67.8 65.5 4.34
50.1 49.2 73.6 71.8 5.18
51.1 49.5 73.1 71.2 5.19
</table>
<tableCaption confidence="0.83326">
Table 3: News data performance (cf Tab. 2).
</tableCaption>
<figure confidence="0.684221571428571">
en-de1
en-de2
en-es1
en-es2
fr-en1
fr-en2
nl-en1
nl-en2
en-de1
en-de2
en-es1
en-es2
fr-en1
fr-en2
</figure>
<figureCaption confidence="0.396012">
Table 2: Trial data performance, from official eval-
uation script: (W)ord and (F)ragment accuracy at
(1) and (5)-best and BLEU score gain.
</figureCaption>
<bodyText confidence="0.98647868">
adapting the language model on the L2 contexts
in run2 provides a clear gain in the top-1 results.
That improvement all but disappears when taking
into account the best out of five translations (ex-
cept maybe for nl-en). The BLEU scores1 are
very high (97-98) and the word error rates (not
reported) are around 1%, suggesting that the sys-
tem output almost matches the references. This
is no doubt due to the proximity between the trial
data and the MT training corpus. Both are fully or
mainly drawn from Europarl material.
In order to get a less optimistic estimate of per-
formance, we automatically constructed a num-
ber of test examples from the WMT News Com-
mentary development test sets. The L1 source
segments and their L2 reference translations were
word aligned in both directions using the GIZA++
implementation of IBM4 (Och and Ney, 2003)
and the grow-diag-final-and combination heuris-
tic (Koehn et al., 2005). Test instances were cre-
ated by substituting some L2 fragments with their
word-aligned L1 source within L2 reference seg-
ments. Since the goal was to select examples that
were more ambiguous and harder to translate than
the trial data, a subset of interesting L1 phrases
was randomly selected among phrases that oc-
cured at least 4 times in the training corpus and
have a high entropy in the baseline phrase-table.
We selected roughly 1000 L1 phrases per language
pair. For each occurrence p1 of these L1 phrases in
the news development sets, we identify the short-
est L2 phrase p2 that is consistently aligned with
1+BLEU in Tables 2-4 is the difference between our sys-
tem’s output and the sentence with untranslated L1 fragment.
p1.2 A new mixed language test example is con-
structed by replacing p2 with p1 in L2 context.
Results on that simulated data are given in Ta-
ble 3. Performance is markedly lower than on the
trial data. This is due in part to the fact that the
News data is not as close to the training material
as the official trial set, and in part to the fact that
this automatically extracted data contains imper-
fect alignments with an unknown (but sizeable)
amount of “noise”. However, it still appears run2
consistently provides several points of increase in
performance for the top-1 results, over the base-
line run]. Performance on the 5-best is either un-
affected or lower, and the gain in BLEU is much
lower than in Table 2 although the resulting BLEU
is around 96%.
</bodyText>
<subsectionHeader confidence="0.99961">
4.2 Test Results
</subsectionHeader>
<bodyText confidence="0.999655647058824">
Official test results provided by the organizers
are presented in Table 4. While these results
are clearly above what we obtained on the syn-
thetic news data, they fall well below the perfor-
mance observed on the trial data. This is not un-
expected as the trial data is unrealistically close
to the training material, while the automatically
extracted news data is noisy. What we did not
expect, however, is the mediocre performance of
LM adaptation (run2): while consistently better
than run] on both trial and news, it is consistently
worse on the official test data. This may be due to
the fact that test sentences were drawn from differ-
ent sources3 such that it does not constitute a ho-
mogeneous domain on which we can easily adapt
a language model.
For German and Spanish, and to a lesser extent
</bodyText>
<footnote confidence="0.9904975">
2As usual in phrase-based MT, two phrases are said to be
consistently aligned, if there is at least one link between their
words and no external links.
3According to the task description, the test set is based
on ”language learning exercises with gaps and cloze-tests, as
well as learner corpora with annotated errors”.
</footnote>
<page confidence="0.993195">
194
</page>
<table confidence="0.999489666666667">
W@1 F@1 W@5 F@5 +BLEU
71.7 65.7 86.8 83.4 16.6
70.2 64.5 86.5 82.8 16.4
74.5 66.7 88.7 84.3 17.0
73.5 65.1 88.4 83.7 17.5
69.4 55.6 83.9 73.9 10.2
68.6 53.3 83.4 73.1 9.9
61.0 45.0 72.3 60.6 5.03
60.9 44.4 72.1 60.2 5.02
</table>
<tableCaption confidence="0.746662">
Table 4: Test data performance, from official eval-
uation results (cf. Table 2).
</tableCaption>
<table confidence="0.9984432">
OOV’s failed align
0.002 0.058
0.010 0.068
0.026 0.139
0.123 0.261
</table>
<tableCaption confidence="0.993243">
Table 5: Test data error analysis: OOV’s is the
</tableCaption>
<bodyText confidence="0.931959166666667">
proportion of all test fragments containing out-of-
vocabulary tokens; failed align is the proportion of
fragments which our system cannot align to any of
the reference translations by forced decoding.
for French and Dutch, the BLEU and Word Error
Rate (WER) gains are much higher on the test than
on the trial data, although the resulting BLEU are
around 86-92%. This results from the fact that the
amount of L1 material to translate relative to the
L2 context was significantly higher on the test data
than it was on the trial data (e.g. 17% of words on
en-es test versus 7% on trial).
</bodyText>
<subsectionHeader confidence="0.987407">
4.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999837357142857">
On the French and, especially, on the Dutch data,
our systems suffer from a high rate of out-of-
vocabulary (OOV) source words in the L1 frag-
ments, i.e. words that simply did not appear in our
training data (see Table 5). In the case of Dutch,
OOV’s impose a hard ceiling of 88% on fragment-
level accuracy. These problems could possibly be
alleviated by using more training data, and incor-
porating language-specific mechanisms to handle
morphology and compounding into the systems.
We also evaluate the proportion of reference tar-
get fragments that can not be reached by forced
decoding (Table 5). Note that to produce trial
and test translations, we use standard decoding to
</bodyText>
<figure confidence="0.564967555555555">
Freq Type
77 Incorrect L2 sense chosen
75 Incorrect or mangled syntax
26 Incomplete reference
20 Non-idiomatic translation
13 Out-of-vocab. word in fragment
6 Problematic source fragment
3 Casing error
220 Total
</figure>
<tableCaption confidence="0.829658">
Table 6: Analysis of the types of error on 220
French-English test sentences.
</tableCaption>
<bodyText confidence="0.998527486486486">
predict a translation that maximizes model score
given the input. Once we have the reference
translation, we use forced decoding to try to pro-
duce the exact reference given the source frag-
ment and our translation model. In some situa-
tions, the correct translations are simply not reach-
able by our systems, either because some target
word has not been observed in training, some part
of the correspondence between source and target
fragments has not been observed, or the system’s
word alignment mechanism is unable to account
for this correspondence, in whole or in part. Ta-
ble 5 shows that this happens between 6% ans
26% of cases, which gives a better upper bound on
the fragment-level accuracy that our system may
achieve. Again, many of these problems could be
solved by using more training data.
To better understand the behavior of our sys-
tems, we manually reviewed 220 sentences where
our baseline French-English system did not ex-
actly match any of the references. We annotated
several types of errors (Table 6). The most fre-
quent source of errors is incorrect sense (35%), i.e.
the system produced a translation of the fragment
that may be correct in some setting, but is not the
correct sense in that context. Those are presum-
ably the errors of interest in a sense disambigua-
tion setting. A close second (34%) were errors
involving incorrect syntax in the fragment transla-
tion, which points to limitations of the Statistical
MT approach, or to a limited language model.
The last third combines several sources of er-
rors. Most notable in this category are non-
idiomatic translations, where the system’s output
was both syntactically correct and understandable,
but clearly not fluent (e.g. “take a siesta” for “have
a nap”); We also identified a number of cases
</bodyText>
<figure confidence="0.98649725">
en-de1
en-de2
en-es1
en-es2
fr-en1
fr-en2
nl-en1
nl-en2
en-de
en-es
fr-en
nl-en
</figure>
<page confidence="0.995532">
195
</page>
<bodyText confidence="0.997603">
where we felt that either the source segment was
incorrect (eg “je vais ´evanouir” instead of “je vais
m’´evanouir”), or the references were incomplete.
Table 7 gives a few examples.
</bodyText>
<sectionHeader confidence="0.997659" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999998">
We described the systems used for the submissions
of the National Research Council Canada to the
L2 writing assistant task. We framed the problem
as a machine translation task, and used standard
statistical machine translation systems trained on
publicly available corpora for translating L1 frag-
ments in their L2 context. This approach lever-
ages the strengths of phrase-based statistical ma-
chine translation, and therefore performs particu-
larly well when the test examples are close to the
training domain. Conversely, it suffers from the
inherent weaknesses of phrase-based models, in-
cluding their inability to generalize beyond seen
vocabulary, as well as sense and syntax errors.
Overall, we showed that machine translation sys-
tems can be used to address the L2 writing assis-
tant task with a high level of accuracy, reaching
out-of-five word-based accuracy above 80 percent
for 3 out of 4 language pairs.
</bodyText>
<sectionHeader confidence="0.998116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999587289473684">
Ergun Bic¸ici and Marc Dymetman. 2008. Dynamic
Translation Memory: Using Statistical Machine
Translation to Improve Translation Memory Fuzzy
Matches. In Computational Linguistics and Intelli-
gent Text Processing, pages 454–465. Springer.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436, Montr´eal, Canada, June.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128–135, Prague, Czech Republic, June.
Pascale Fung, Xiaohu Liu, and Chi Shun Cheung.
1999. Mixed Language Query Disambiguation. In
Proceedings of ACL’99, pages 333–340, Maryland,
June.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144–151, Prague, Czech Republic, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-gram Language Modeling. In
Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Philipp Koehn and Jean Senellart. 2010. Conver-
gence of Translation Memory and Statistical Ma-
chine Translation. In Proceedings of AMTA Work-
shop on MT Research and the Translation Industry,
pages 21–31.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT-2005, pages 68–75, Pitts-
burgh, PA.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79–86, Phuket, Thailand,
September.
Samuel Larkin, Boxing Chen, George Foster, Ull-
rich Germann, ´Eric Joanis, J. Howard Johnson, and
Roland Kuhn. 2010. Lessons from NRC’s Portage
System at WMT 2010. In 5th Workshop on Statisti-
cal Machine Translation, pages 127–132.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–52.
Michel Simard and Pierre Isabelle. 2009. Phrase-
based Machine Translation in a Computer-assisted
Translation Environment. Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120–127.
Christoph Tillmann. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101–
104, Boston, Massachusetts, USA, May 2 - May 7.
Richard Zens and Hermann Ney. 2004. Improve-
ments in Phrase-Based Statistical Machine Trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 257–264, Boston, Massachusetts, USA,
May 2 - May 7.
</reference>
<page confidence="0.988719">
196
</page>
<reference confidence="0.887171105263158">
Incorrect L2 sense:
In: My dog usually barks au facteur - but look at that, for once, he is being friendly ...
Out: My dog usually barks to the factor - but look at that, for once, he is being friendly ...
Ref: My dog usually barks at the postman - but look at that, for once, he is being friendly ...
In: Grapes ne poussent pas in northern climates , unless one keeps them in a hot-house .
Out: Grapes do not push in northern climates , unless one keeps them in a hot-house .
Ref: Grapes do not grow in northern climates, unless one keeps them in a hot-house .
Missing reference?
In: Twenty-two other people ont ete blessees in the explosion.
Out: Twenty-two other people were injured in the explosion.
Ref: Twenty-two other people have been wounded in the explosion.
Non-idiomatic translation:
In: After patiently stalking its prey , the lion makes a rapide comme l’ eclair charge for the kill .
Out: After patiently stalking its prey , the lion makes a rapid as flash charge for the kill .
Ref: After patiently stalking its prey , the lion makes a lightning-fast charge for the kill .
Problem with input:
In: every time I do n’t eat for a while and my blood sugar gets low I feel like je vais evanouir .
Out: every time I do n’t eat for a while and my blood sugar gets low I feel like I will evaporate .
Ref: every time I do n’t eat for a while and my blood sugar gets low I feel like I ’m going to faint.
</reference>
<tableCaption confidence="0.891888">
Table 7: Examples errors on French-English.
</tableCaption>
<page confidence="0.996441">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840747">
<title confidence="0.996023">CNRC-TMT: Second Language Writing Assistant System Description</title>
<author confidence="0.981883">Cyril Goutte Michel Simard Marine Carpuat</author>
<affiliation confidence="0.947569">National Research Council Multilingual Text</affiliation>
<address confidence="0.994378">1200 Montreal Road, Ottawa, Ontario K1A 0R6,</address>
<email confidence="0.971841">FirstName.LastName@nrc.ca</email>
<abstract confidence="0.998976466666667">We describe the system entered by the National Research Council Canada in the SemEval-2014 L2 writing assistant task. Our system relies on a standard Phrase-Based Statistical Machine Translation trained on generic, publicly available data. Translations are produced by taking the already translated part of the sentence as fixed context. We show that translation systems can address the L2 writing assistant task, reaching out-of-five word-based accuracy above 80 percent for 3 out of 4 language pairs. We also present a brief analysis of remaining errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Marc Dymetman</author>
</authors>
<title>Dynamic Translation Memory: Using Statistical Machine Translation to Improve Translation Memory Fuzzy Matches.</title>
<date>2008</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>454--465</pages>
<publisher>Springer.</publisher>
<marker>Bic¸ici, Dymetman, 2008</marker>
<rawString>Ergun Bic¸ici and Marc Dymetman. 2008. Dynamic Translation Memory: Using Statistical Machine Translation to Improve Translation Memory Fuzzy Matches. In Computational Linguistics and Intelligent Text Processing, pages 454–465. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6674" citStr="Bojar et al., 2013" startWordPosition="1090" endWordPosition="1093">959k 174k 2133k dev - 2000 2000 train 2002k 157k 2158k dev - 2000 2000 train 1974k - 1974k dev 1984 - 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for all 4 language pairs of the evaluation. As Europarl covers parliamentary proceedings, we added some news and commentary (henceforth ”News”) data provided for the 2013 workshop on Machine Translation shared task (Bojar et al., 2013) for language pairs other than nl-en. In all cases, we extracted from the corpus a tuning (“dev”) set of around 2000 sentence pairs. Statistics for the training data are given in Table 1. The trial and test data each consist of 500 sentences with L1 fragments in L2 context provided by the organizers. As the trial data came from Europarl, we filtered our training corpora in order to remove close matches and avoid training on the trial data (Table 1 takes this into account). All translation systems were trained on lowercased data, and predictions were recased using a standard (LM-based) truecasi</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5843" citStr="Cherry and Foster, 2012" startWordPosition="942" endWordPosition="945">the translation models. In the second configuration (runt), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Data SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inEuroparl News Total train 1904k 177k 2081k dev - 2000 2000 train 1959k 174k 2133k dev - 2000 2000 train 2002k 157k 2158k dev - 2000 2000 train 1974k - 1974k dev 1984 - 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5486" citStr="Foster and Kuhn, 2007" startWordPosition="889" endWordPosition="892"> only component of the SMT system that scores how well the translation of the L1 fragment fits in the existing L2 context. We test two different LM configurations. The first of these (run]) uses a single static LM: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (runt), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Data SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inEuroparl News Total train 1904k 177k 2081k dev - 2000 2000 train 1959k 174k 2133k dev - 2000 2000</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Xiaohu Liu</author>
<author>Chi Shun Cheung</author>
</authors>
<title>Mixed Language Query Disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL’99,</booktitle>
<pages>333--340</pages>
<location>Maryland,</location>
<contexts>
<context position="2310" citStr="Fung et al., 1999" startWordPosition="358" endWordPosition="361">techniques can be used to locate the parts of the source segment that do not match the sentence to translate, and from c�2014, The Crown in Right of Canada. there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train the SMT engine (Section 3), and our results on the trial and test data, as well as a short error analysis (Section 4). section 2 System Description The core Machine Translation engine used for all our submissions is Portage (Larkin et al., 2010), the NRC’s phrase-based SMT system. Given a suitably trained SMT syste</context>
</contexts>
<marker>Fung, Liu, Cheung, 1999</marker>
<rawString>Pascale Fung, Xiaohu Liu, and Chi Shun Cheung. 1999. Mixed Language Query Disambiguation. In Proceedings of ACL’99, pages 333–340, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5700" citStr="Huang and Chiang, 2007" startWordPosition="921" endWordPosition="924">: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (runt), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Data SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inEuroparl News Total train 1904k 177k 2081k dev - 2000 2000 train 1959k 174k 2133k dev - 2000 2000 train 2002k 157k 2158k dev - 2000 2000 train 1974k - 1974k dev 1984 - 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test d</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-off for M-gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5156" citStr="Kneser and Ney, 1995" startWordPosition="829" endWordPosition="832">us and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. Language Models When translating L1 fragments in L2 context, the L2 language model (LM) is particularly important as it is the only component of the SMT system that scores how well the translation of the L1 fragment fits in the existing L2 context. We test two different LM configurations. The first of these (run]) uses a single static LM: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (runt), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tu</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-off for M-gram Language Modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Jean Senellart</author>
</authors>
<title>Convergence of Translation Memory and Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA Workshop on MT Research and the Translation Industry,</booktitle>
<pages>21--31</pages>
<contexts>
<context position="1994" citStr="Koehn and Senellart, 2010" startWordPosition="306" endWordPosition="309"> little or no post-editing. However, when only approximate matches are found, post-editing will typically be required to adapt the target side of the partially matching source segment to the source sentence under consideration. It is possible to automate this process: standard string matching algorithms and word alignment techniques can be used to locate the parts of the source segment that do not match the sentence to translate, and from c�2014, The Crown in Right of Canada. there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train </context>
</contexts>
<marker>Koehn, Senellart, 2010</marker>
<rawString>Philipp Koehn and Jean Senellart. 2010. Convergence of Translation Memory and Statistical Machine Translation. In Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<pages>68--75</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4264" citStr="Koehn et al., 2005" startWordPosition="681" endWordPosition="684">valuation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. Language Models When translating L1 fragments in L2 context, the L2 language model (LM) is particularly important as it is the</context>
<context position="9328" citStr="Koehn et al., 2005" startWordPosition="1549" endWordPosition="1552">d) are around 1%, suggesting that the system output almost matches the references. This is no doubt due to the proximity between the trial data and the MT training corpus. Both are fully or mainly drawn from Europarl material. In order to get a less optimistic estimate of performance, we automatically constructed a number of test examples from the WMT News Commentary development test sets. The L1 source segments and their L2 reference translations were word aligned in both directions using the GIZA++ implementation of IBM4 (Och and Ney, 2003) and the grow-diag-final-and combination heuristic (Koehn et al., 2005). Test instances were created by substituting some L2 fragments with their word-aligned L1 source within L2 reference segments. Since the goal was to select examples that were more ambiguous and harder to translate than the trial data, a subset of interesting L1 phrases was randomly selected among phrases that occured at least 4 times in the training corpus and have a high entropy in the baseline phrase-table. We selected roughly 1000 L1 phrases per language pair. For each occurrence p1 of these L1 phrases in the news development sets, we identify the shortest L2 phrase p2 that is consistently</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of IWSLT-2005, pages 68–75, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="6418" citStr="Koehn, 2005" startWordPosition="1051" endWordPosition="1052">ion of MIRA (Cherry and Foster, 2012). 3 Data SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inEuroparl News Total train 1904k 177k 2081k dev - 2000 2000 train 1959k 174k 2133k dev - 2000 2000 train 2002k 157k 2158k dev - 2000 2000 train 1974k - 1974k dev 1984 - 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for all 4 language pairs of the evaluation. As Europarl covers parliamentary proceedings, we added some news and commentary (henceforth ”News”) data provided for the 2013 workshop on Machine Translation shared task (Bojar et al., 2013) for language pairs other than nl-en. In all cases, we extracted from the corpus a tuning (“dev”) set of around 2000 sentence pairs. Statistics for the training data are given in Table 1. The trial and test data each consist of 500 sentences with L1 fragments in L2 context provided by the organizers. As the trial data came from Europarl, we f</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Machine Translation Summit X, pages 79–86, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Larkin</author>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Ullrich Germann</author>
<author>´Eric Joanis</author>
<author>J Howard Johnson</author>
<author>Roland Kuhn</author>
</authors>
<date>2010</date>
<booktitle>Lessons from NRC’s Portage System at WMT 2010. In 5th Workshop on Statistical Machine Translation,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="2839" citStr="Larkin et al., 2010" startWordPosition="451" endWordPosition="454">onnections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train the SMT engine (Section 3), and our results on the trial and test data, as well as a short error analysis (Section 4). section 2 System Description The core Machine Translation engine used for all our submissions is Portage (Larkin et al., 2010), the NRC’s phrase-based SMT system. Given a suitably trained SMT system, the Task 5 input is processed as follows. For each sentence with an L1 fragment to translate, the already translated parts are set as left and right context. The L1 fragment in L2 context is sent to the decoder. The output is a full sentence translation that ensures 1) that the context is left untouched, and 2) that the L1 fragment is translated in a way that fits with the L2 context. We now describe the key components of the MT system (language, translation and reordering models), as well as the decoding and parameter t</context>
</contexts>
<marker>Larkin, Chen, Foster, Germann, Joanis, Johnson, Kuhn, 2010</marker>
<rawString>Samuel Larkin, Boxing Chen, George Foster, Ullrich Germann, ´Eric Joanis, J. Howard Johnson, and Roland Kuhn. 2010. Lessons from NRC’s Portage System at WMT 2010. In 5th Workshop on Statistical Machine Translation, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9257" citStr="Och and Ney, 2003" startWordPosition="1539" endWordPosition="1542">EU scores1 are very high (97-98) and the word error rates (not reported) are around 1%, suggesting that the system output almost matches the references. This is no doubt due to the proximity between the trial data and the MT training corpus. Both are fully or mainly drawn from Europarl material. In order to get a less optimistic estimate of performance, we automatically constructed a number of test examples from the WMT News Commentary development test sets. The L1 source segments and their L2 reference translations were word aligned in both directions using the GIZA++ implementation of IBM4 (Och and Ney, 2003) and the grow-diag-final-and combination heuristic (Koehn et al., 2005). Test instances were created by substituting some L2 fragments with their word-aligned L1 source within L2 reference segments. Since the goal was to select examples that were more ambiguous and harder to translate than the trial data, a subset of interesting L1 phrases was randomly selected among phrases that occured at least 4 times in the training corpus and have a high entropy in the baseline phrase-table. We selected roughly 1000 L1 phrases per language pair. For each occurrence p1 of these L1 phrases in the news devel</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
</authors>
<title>Phrasebased Machine Translation in a Computer-assisted Translation Environment.</title>
<date>2009</date>
<booktitle>Proceedings of the Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>120--127</pages>
<contexts>
<context position="1966" citStr="Simard and Isabelle, 2009" startWordPosition="302" endWordPosition="305">xpected to be suitable with little or no post-editing. However, when only approximate matches are found, post-editing will typically be required to adapt the target side of the partially matching source segment to the source sentence under consideration. It is possible to automate this process: standard string matching algorithms and word alignment techniques can be used to locate the parts of the source segment that do not match the sentence to translate, and from c�2014, The Crown in Right of Canada. there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describ</context>
</contexts>
<marker>Simard, Isabelle, 2009</marker>
<rawString>Michel Simard and Pierre Isabelle. 2009. Phrasebased Machine Translation in a Computer-assisted Translation Environment. Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>101--104</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="4243" citStr="Tillmann, 2004" startWordPosition="679" endWordPosition="680">op on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. Language Models When translating L1 fragments in L2 context, the L2 language model (LM) is particularly i</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 101– 104, Boston, Massachusetts, USA, May 2 - May 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in Phrase-Based Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>257--264</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="3951" citStr="Zens and Ney (2004)" startWordPosition="635" endWordPosition="638">nts of the MT system (language, translation and reordering models), as well as the decoding and parameter tuning. Translation Models We use a single static phrase table including phrase pairs extracted from the symmetrized HMM word-alignment learned 192 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine Translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 257–264, Boston, Massachusetts, USA, May 2 - May 7.</rawString>
</citation>
<citation valid="false">
<title>Incorrect L2 sense: In: My dog usually barks au facteur - but look at that, for once, he is being friendly ... Out: My dog usually barks to the factor - but look at that, for once, he is being friendly ... Ref: My dog usually barks at the postman - but look at that, for once, he is being friendly ...</title>
<marker></marker>
<rawString>Incorrect L2 sense: In: My dog usually barks au facteur - but look at that, for once, he is being friendly ... Out: My dog usually barks to the factor - but look at that, for once, he is being friendly ... Ref: My dog usually barks at the postman - but look at that, for once, he is being friendly ...</rawString>
</citation>
<citation valid="false">
<title>In: Grapes ne poussent pas in northern climates , unless one keeps them in a hot-house . Out: Grapes do not push in northern climates , unless one keeps them in a hot-house . Ref: Grapes do not grow in northern climates, unless one keeps them in a hot-house . Missing reference? In: Twenty-two other people ont ete blessees in the explosion. Out: Twenty-two other people were injured in the explosion. Ref: Twenty-two other people have been wounded in the explosion. Non-idiomatic translation: In: After patiently stalking its prey , the lion makes a rapide comme l’ eclair charge for the kill . Out: After patiently stalking its prey , the lion makes a rapid as flash charge for the kill . Ref: After patiently stalking its prey , the lion makes a lightning-fast charge for the kill . Problem with input: In: every time I do n’t eat for a while and my blood sugar gets low I feel like je vais evanouir . Out: every time I do n’t eat for a while and my blood sugar gets low I feel like I will evaporate . Ref: every time I do n’t eat for a while and my blood sugar gets low I feel like I ’m going to faint.</title>
<marker></marker>
<rawString>In: Grapes ne poussent pas in northern climates , unless one keeps them in a hot-house . Out: Grapes do not push in northern climates , unless one keeps them in a hot-house . Ref: Grapes do not grow in northern climates, unless one keeps them in a hot-house . Missing reference? In: Twenty-two other people ont ete blessees in the explosion. Out: Twenty-two other people were injured in the explosion. Ref: Twenty-two other people have been wounded in the explosion. Non-idiomatic translation: In: After patiently stalking its prey , the lion makes a rapide comme l’ eclair charge for the kill . Out: After patiently stalking its prey , the lion makes a rapid as flash charge for the kill . Ref: After patiently stalking its prey , the lion makes a lightning-fast charge for the kill . Problem with input: In: every time I do n’t eat for a while and my blood sugar gets low I feel like je vais evanouir . Out: every time I do n’t eat for a while and my blood sugar gets low I feel like I will evaporate . Ref: every time I do n’t eat for a while and my blood sugar gets low I feel like I ’m going to faint.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>