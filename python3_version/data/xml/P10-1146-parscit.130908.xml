<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000420">
<title confidence="0.980297">
Learning to Translate with Source and Target Syntax
</title>
<author confidence="0.994374">
David Chiang
</author>
<affiliation confidence="0.8436395">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.673518">
Marina del Rey, CA 90292 USA
</address>
<email confidence="0.990655">
chiang@isi.edu
</email>
<sectionHeader confidence="0.994849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994975">
Statistical translation models that try to
capture the recursive structure of language
have been widely adopted over the last few
years. These models make use of vary-
ing amounts of information from linguis-
tic theory: some use none at all, some use
information about the grammar of the tar-
get language, some use information about
the grammar of the source language. But
progress has been slower on translation
models that are able to learn the rela-
tionship between the grammars of both
the source and target language. We dis-
cuss the reasons why this has been a chal-
lenge, review existing attempts to meet this
challenge, and show how some old and
new ideas can be combined into a sim-
ple approach that uses both source and tar-
get syntax for significant improvements in
translation accuracy.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997823255814">
Statistical translation models that use synchronous
context-free grammars (SCFGs) or related for-
malisms to try to capture the recursive structure of
language have been widely adopted over the last
few years. The simplest of these (Chiang, 2005)
make no use of information from syntactic theo-
ries or syntactic annotations, whereas others have
successfully incorporated syntactic information on
the target side (Galley et al., 2004; Galley et al.,
2006) or the source side (Liu et al., 2006; Huang
et al., 2006). The next obvious step is toward mod-
els that make full use of syntactic information on
both sides. But the natural generalization to this
setting has been found to underperform phrase-
based models (Liu et al., 2009; Ambati and Lavie,
2008), and researchers have begun to explore so-
lutions (Zhang et al., 2008; Liu et al., 2009).
In this paper, we explore the reasons why tree-
to-tree translation has been challenging, and how
source syntax and target syntax might be used to-
gether. Drawing on previous successful attempts to
relax syntactic constraints during grammar extrac-
tion in various ways (Zhang et al., 2008; Liu et al.,
2009; Zollmann and Venugopal, 2006), we com-
pare several methods for extracting a synchronous
grammar from tree-to-tree data. One confounding
factor in such a comparison is that some methods
generate many new syntactic categories, making it
more difficult to satisfy syntactic constraints at de-
coding time. We therefore propose to move these
constraints from the formalism into the model, im-
plemented as features in the hierarchical phrase-
based model Hiero (Chiang, 2005). This aug-
mented model is able to learn from data whether
to rely on syntax or not, or to revert back to mono-
tone phrase-based translation.
In experiments on Chinese-English and Arabic-
English translation, we find that when both source
and target syntax are made available to the model
in an unobtrusive way, the model chooses to build
structures that are more syntactically well-formed
and yield significantly better translations than a
nonsyntactic hierarchical phrase-based model.
</bodyText>
<sectionHeader confidence="0.97389" genericHeader="method">
2 Grammar extraction
</sectionHeader>
<bodyText confidence="0.980660333333333">
A synchronous tree-substitution grammar (STSG)
is a set of rules or elementary tree pairs (&apos;y, α),
where:
</bodyText>
<listItem confidence="0.924489125">
• -y is a tree whose interior labels are source-
language nonterminal symbols and whose
frontier labels are source-language nontermi-
nal symbols or terminal symbols (words). The
nonterminal-labeled frontier nodes are called
substitution nodes, conventionally marked
with an arrow (1).
• α is a tree of the same form except with
</listItem>
<figure confidence="0.617032875">
1443
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
PP
LCP
C
中
�
����
1444
IP NP P
一r
hs ù V 十七 亿hi iy
四 yībǎisìshíqī mě uánNPNN
n chā
1445
</figure>
<bodyText confidence="0.999955481481481">
of packed forests instead of pairs of trees. Since a
packed forest is much more likely to include the
correct tree, it is less likely that parser errors will
cause good rules to be filtered out.
However, even on human-annotated data, tree-
to-tree extraction misses many rules, and many
such rules would seem to be useful. For ex-
ample, in Figure 2, the whole English phrase
“Taiwan’s...shores” is an NP, but its Chinese
counterpart is not a constituent. Furthermore, nei-
ther “surplus...shores” nor its Chinese counterpart
are constituents. But both rules are arguably use-
ful for translation. Wellington et al. therefore ar-
gue that in order to extract as many rules as possi-
ble, a more powerful formalism than synchronous
CFG/TSG is required: for example, generalized
multitext grammar (Melamed et al., 2004), which
is equivalent to synchronous set-local multicom-
ponent CFG/TSG (Weir, 1988).
But the problem illustrated in Figure 2 does
not reflect a very deep fact about syntax or cross-
lingual divergences, but rather choices in annota-
tion style that interact badly with the exact tree-
to-tree extraction heuristic. On the Chinese side,
the IP is too flat (because 台湾/Táiwān has been
analyzed as a topic), whereas the more articulated
structure
</bodyText>
<listItem confidence="0.920134">
(1) [NP Táiwān [NP [PP zaì ...] shùnchā]]
</listItem>
<bodyText confidence="0.9359255">
would also be quite reasonable. On the English
side, the high attachment of the PP disagrees with
the corresponding Chinese structure, but low at-
tachment also seems reasonable:
</bodyText>
<listItem confidence="0.81134">
(2) [NP [NP Taiwan’s] [NP surplus in trade...]]
</listItem>
<bodyText confidence="0.9998271875">
Thus even in the gold-standard parse trees, phrase
structure can be underspecified (like the flat IP
above) or uncertain (like the PP attachment above).
For this reason, some approaches work with a
more flexible notion of constituency. Synchronous
tree-sequence–substitution grammar (STSSG) al-
lows either side of a rule to comprise a sequence of
trees instead of a single tree (Zhang et al., 2008). In
the substitution operation, a sequence of sister sub-
stitution nodes is rewritten with a tree sequence of
equal length (see Figure 3a). This extra flexibility
effectively makes the analysis (1) available to us.
Any STSSG can be converted into an equivalent
STSG via the creation of virtual nodes (see Fig-
ure 3b): for every elementary tree sequence with
roots X1, ... , Xn, create a new root node with a
</bodyText>
<equation confidence="0.95334175">
NP
�P NN NNP�NN
ni i M
s
</equation>
<bodyText confidence="0.999454333333333">
that differ only in their nonterminal labels, only the
most-frequent rule is kept, and its count is the to-
tal count of all the rules. This means that there is a
one-to-one correspondence between the rules ex-
tracted by fuzzy tree-to-tree extraction and hierar-
chical string-to-string extraction.
</bodyText>
<subsectionHeader confidence="0.999792">
2.3 Nesting phrases
</subsectionHeader>
<bodyText confidence="0.997999">
Fuzzy tree-to-tree extraction (like string-to-string
extraction) generates many times more rules than
exact tree-to-tree extraction does. In Figure 2, we
observed that the flat structure of the Chinese IP
prevented exact tree-to-tree extraction from ex-
tracting a rule containing just part of the IP, for
example:
</bodyText>
<listItem confidence="0.999248333333333">
(3) [PP zaì ...] [NP shùnchā]
(4) [NP Táiwān] [PP zaì ...] [NP shùnchā]
(5) [PP zaì ...] [NP shùnchā] [VP ... měiyuán]
</listItem>
<bodyText confidence="0.9977768">
Fuzzy tree-to-tree extraction allows any of these
to be the source side of a rule. We might think of
it as effectively restructuring the trees by insert-
ing nodes with complex labels. However, it is not
possible to represent this restructuring with a sin-
gle tree (see Figure 4). More formally, let us say
that two phrases wi · · · wj−1 and wi′ · · · wj′−1 nest
if i &lt; i′ &lt; j′ &lt; j or i′ &lt; i &lt; j &lt; j′; otherwise,
they cross. The two Chinese phrases (4) and (5)
cross, and therefore cannot both be constituents in
the same tree. In other words, exact tree-to-tree ex-
traction commits to a single structural analysis but
fuzzy tree-to-tree extraction pursues many restruc-
tured analyses at once.
We can strike a compromise by continuing to al-
low SAMT-style complex categories, but commit-
ting to a single analysis by requiring all phrases to
nest. To do this, we use a simple heuristic. Iterate
through all the phrase pairs (f, e) in the following
order:
</bodyText>
<listItem confidence="0.9942583">
1. sort by whether f and e� can be assigned a sim-
ple syntactic category (both, then one, then
neither); if there is a tie,
2. sort by how many syntactic constituents f and
e� cross (low to high); if there is a tie,
3. give priority to ( f, e) if neither f nor e� be-
gins or ends with punctuation; if there is a tie,
finally
4. sort by the position of f in the source-side
string (right to left).
</listItem>
<bodyText confidence="0.9997205">
For each phrase pair, accept it if it does not cross
any previously accepted phrase pair; otherwise, re-
ject it.
Because this heuristic produces a set of nesting
phrases, we can represent them all in a single re-
structured tree. In Figure 4, this heuristic chooses
structure (a) because the English-side counterpart
of IP/VP has the simple category NP.
</bodyText>
<sectionHeader confidence="0.992256" genericHeader="method">
3 Decoding
</sectionHeader>
<bodyText confidence="0.998599897435897">
In decoding, the rules extracted during training
must be reassembled to form a derivation whose
source side matches the input sentence. In the ex-
act tree-to-tree approach, whenever substitution
is performed, the root labels of the substituted
trees must match the labels of the substitution
nodes—call this the matching constraint. Because
this constraint must be satisfied on both the source
and target side, it can become difficult to general-
ize well from training examples to new input sen-
tences.
Venugopal et al. (2009), in the string-to-tree set-
ting, attempt to soften the data-fragmentation ef-
fect of the matching constraint: instead of trying
to find the single derivation with the highest prob-
ability, they sum over derivations that differ only
in their nonterminal labels and try to find the sin-
gle derivation-class with the highest probability.
Still, only derivations that satisfy the matching
constraint are included in the summation.
But in some cases we may want to soften the
matching constraint itself. Some syntactic cate-
gories are similar enough to be considered com-
patible: for example, if a rule rooted in VBD (past-
tense verb) could substitute into a site labeled VBZ
(present-tense verb), it might still generate correct
output. This is all the more true with the addition
of SAMT-style categories: for example, if a rule
rooted in ADVP * VP could substitute into a site
labeled VP, it would very likely generate correct
output.
Since we want syntactic information to help the
model make good translation choices, not to rule
out potentially correct choices, we can change the
way the information is used during decoding: we
allow any rule to substitute into any site, but let
the model learn which substitutions are better than
others. To do this, we add the following features to
the model:
</bodyText>
<table confidence="0.974849157894737">
1447
IP
P P
PPVN 一百
P V 十七 亿hi i y
NP NN 四 yībǎisìshíqī mě uánIP$V
顺
Chi-Eng Ara-Eng
Core training words 32+38M 28+34M
initial phrase size 10 15
final rule size 6 6
nonterminals 2 2
loose source 0 00
loose target 0 2
Full training words 240+260M 190+220M
final rule size 6 6
nonterminals 0 0
loose source 00 00
loose target 1 2
</table>
<tableCaption confidence="0.980906">
Table 2: Rule extraction settings used for exper-
</tableCaption>
<bodyText confidence="0.98901036">
iments. “Loose source/target” is the maximum
number of unaligned source/target words at the
endpoints of a phrase.
limit, above which the glue rule must be used.
We trained two 5-gram language models: one
on the combined English halves of the bitexts, and
one on two billion words of English. These were
smoothed using modified Kneser-Ney (Chen and
Goodman, 1998) and stored using randomized data
structures similar to those of Talbot and Brants
(2008).
The base feature set for all systems was similar
to the expanded set recently used for Hiero (Chiang
et al., 2009), but with bigram features (source and
target word) instead of trigram features (source and
target word and neighboring source word). For all
systems but the baselines, the features described
in Section 3 were added. The systems were trained
using MIRA (Crammer and Singer, 2003; Chiang
et al., 2009) on a tuning set of about 3000 sentences
of newswire from NIST MT evaluation data and
GALE development data, disjoint from the train-
ing data. We optimized feature weights on 90% of
this and held out the other 10% to determine when
to stop.
</bodyText>
<sectionHeader confidence="0.601347" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.99999235483871">
Table 3 shows the scores on our development sets
and test sets, which are about 3000 and 2000
sentences, respectively, of newswire drawn from
NIST MT evaluation data and GALE development
data and disjoint from the tuning data.
For Chinese, we first tried increasing the distor-
tion limit from 10 words to 20. This limit controls
how deeply nested the tree structures built by the
decoder are, and we want to see whether adding
syntactic information leads to more complex struc-
tures. This change by itself led to an increase in
the BLEU score. We then compared against two
systems using tree-to-tree grammars. Using ex-
act tree-to-tree extraction, we got a much smaller
grammar, but decreased accuracy on all but the
Chinese-English test set, where there was no sig-
nificant change. But with fuzzy tree-to-tree extrac-
tion, we obtained an improvement of +0.6 on both
Chinese-English sets, and +0.7/+0.8 on the Arabic-
English sets.
Applying the heuristic for nesting phrases re-
duced the grammar sizes dramatically (by a factor
of 2.4 for Chinese and 4.2 for Arabic) but, interest-
ingly, had almost no effect on translation quality: a
slight decrease in BLEU on the Arabic-English de-
velopment set and no significant difference on the
other sets. This suggests that the strength of fuzzy
tree-to-tree extraction lies in its ability to break up
flat structures and to reconcile the source and target
trees with each other, rather than multiple restruc-
turings of the training trees.
</bodyText>
<subsectionHeader confidence="0.998714">
4.3 Rule usage
</subsectionHeader>
<bodyText confidence="0.996950464285714">
We then took a closer look at the behavior of
the string-to-string and fuzzy tree-to-tree gram-
mars (without the nesting heuristic). Because the
rules of these grammars are in one-to-one corre-
spondence, we can analyze the string-to-string sys-
tem’s derivations as though they had syntactic cat-
egories. First, Table 4 shows that the system using
the tree-to-tree grammar used the glue rule much
less and performed more matching substitutions.
That is, in order to minimize errors on the tuning
set, the model learned to build syntactically richer
and more well-formed derivations.
Tables 5 and 6 show how the new syntax fea-
tures affected particular substitutions. In general
we see a shift towards more matching substitu-
tions; correct placement of punctuation is particu-
larly emphasized. Several changes appear to have
to do with definiteness of NPs: on the English
side, adding the syntax features encourages match-
ing substitutions of type DT \ NP-C (anarthrous
NP), but discourages DT \ NP-C and NN from
substituting into NP-C and vice versa. For ex-
ample, a translation with the rewriting NP-C —*
DT \ NP-C begins with “24th meeting of the
Standing Committee...,” but the system using the
fuzzy tree-to-tree grammar changes this to “The
24th meeting of the Standing Committee....”
The root features had a less noticeable effect on
</bodyText>
<table confidence="0.970888090909091">
1449
BLEU
dist.lim. rules features dev test
10 440M 1k 32.7 23.4
20 440M 1k 33.3 23.7
20 50M 5k 32.8 23.9 1
20 440M 160k 33.9 24.3
20 180M 79k 33.91 24&apos;31
10 790M 1k 48.7 48.9
10 38M 5k 46.6 47.5
10 790M 130k 49.4 49.7
10 190M 66k 49.2 49.8 1
task extraction
Chi-Eng string-to-string
string-to-string
tree-to-tree exact
tree-to-tree fuzzy
+ nesting
Ara-Eng string-to-string
tree-to-tree exact
tree-to-tree fuzzy
+ nesting
</table>
<tableCaption confidence="0.704809333333333">
Table 3: On both the Chinese-English and Arabic-English translation tasks, fuzzy tree-to-tree extraction
outperforms exact tree-to-tree extraction and string-to-string extraction. Brackets indicate statistically
insignificant differences (P &gt; 0.05).
</tableCaption>
<bodyText confidence="0.998455333333333">
rule choice; one interesting change was that the fre-
quency of rules with Chinese root VP / IP and En-
glish root VP / S-C increased from 0.2% to 0.7%:
apparently the model learned that it is good to use
rules that pair Chinese and English verbs that sub-
categorize for sentential complements.
</bodyText>
<sectionHeader confidence="0.993931" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972461538462">
Though exact tree-to-tree translation tends to ham-
per translation quality by imposing too many con-
straints during both grammar extraction and de-
coding, we have shown that using both source and
target syntax improves translation accuracy when
the model is given the opportunity to learn from
data how strongly to apply syntactic constraints.
Indeed, we have found that the model learns on its
own to choose syntactically richer and more well-
formed structures, demonstrating that source- and
target-side syntax can be used together profitably
as long as they are not allowed to overconstrain the
translation model.
</bodyText>
<sectionHeader confidence="0.972332" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.986756571428572">
Thanks to Steve DeNeefe, Adam Lopez, Jonathan
May, Miles Osborne, Adam Pauls, Richard
Schwartz, and the anonymous reviewers for their
valuable help. This research was supported in part
by DARPA contract HR0011-06-C-0022 under
subcontract to BBN Technologies and DARPA
contract HR0011-09-1-0028. S. D. G.
</bodyText>
<table confidence="0.999730571428571">
task side kind frequency (%)
s-to-s t-to-t
Chi-Eng source glue 25 18
match 17 30
mismatch 58 52
target glue 25 18
match 9 23
mismatch 66 58
Ara-Eng source glue 36 19
match 17 34
mismatch 48 47
target glue 36 19
match 11 29
mismatch 53 52
</table>
<tableCaption confidence="0.897663">
Table 4: Moving from string-to-string (s-to-s) ex-
traction to fuzzy tree-to-tree (t-to-t) extraction de-
creases glue rule usage and increases the frequency
of matching substitutions.
</tableCaption>
<table confidence="0.998435733333333">
1450
kind frequency (%)
s-to-s t-to-t
NP NP 16.0 20.7
VP VP 3.3 5.9
NN NP 3.1 1.3
NP VP 2.5 0.8
NP NN 2.0 1.4
NP entity 1.4 1.6
NN NN 1.1 1.0
QP entity 1.0 1.3
VV VP 1.0 0.7
PU NP 0.8 1.1
VV VP*PU 0.2 1.2
PU PU 0.1 3.8
</table>
<tableCaption confidence="0.993377">
Table 5: Comparison of frequency of source-side
</tableCaption>
<bodyText confidence="0.9385475">
rewrites in Chinese-English translation between
string-to-string (s-to-s) and fuzzy tree-to-tree (t-to-
t) grammars. All rewrites occurring more than 1%
of the time in either system are shown. The label
“entity” stands for handwritten rules for named en-
tities and numbers.
</bodyText>
<table confidence="0.999301647058824">
kind frequency (%)
s-to-s t-to-t
NP-C NP-C 5.3 8.7
NN NN 1.7 3.0
NP-C entity 1.1 1.4
DT \ NP-C DT \ NP-C 1.1 2.6
NN NP-C 0.8 0.4
NP-C VP 0.8 1.1
DT \ NP-C NP-C 0.8 0.5
NP-C DT \ NP-C 0.6 0.4
JJ JJ 0.5 1.8
NP-C NN 0.5 0.3
PP PP 0.4 1.7
VP-C VP-C 0.4 1.2
VP VP 0.4 1.4
IN IN 0.1 1.8
, � , 0.1 1.7
</table>
<tableCaption confidence="0.988048">
Table 6: Comparison of frequency of target-side
</tableCaption>
<bodyText confidence="0.942940333333333">
rewrites in Chinese-English translation between
string-to-string (s-to-s) and fuzzy tree-to-tree (t-
to-t) grammars. All rewrites occurring more than
1% of the time in either system are shown, plus a
few more of interest. The label “entity” stands for
handwritten rules for named entities and numbers.
</bodyText>
<sectionHeader confidence="0.993093" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994395835051546">
Vamshi Ambati and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proc. AMTA-2008 Student Research Workshop,
pages 235–244.
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical
notation for syntactic description. Language,
29(1):47–58.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. NAACL HLT 2009, pages 218–226.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proc. ACL 2005, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proc. ACL-EACL,
pages 16–23.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. ACL
2003 Companion Volume, pages 205–208.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment
for syntax-based statistical machine translation. In
Proc. Third Workshop on Statistical Machine Trans-
lation, pages 44–52.
Alexander Fraser and Daniel Marcu. 2007. Getting
the structure right for word alignment: LEAF. In
Proc. EMNLP 2007, pages 51–60.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proc. COLING-ACL 2006, pages 961–968.
Mary Hearne and Andy Way. 2003. Seeing the wood
for the trees: Data-Oriented Translation. In Proc. MT
Summit IX, pages 165–172.
1451
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. AMTA 2006, pages
65–73.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. SSST-2, pages 87–95.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLING-ACL 2006, pages
609–616.
Yang Liu, Yajuan La, and Qun Liu. 2009. Improv-
ing tree-to-tree translation with packed forests. In
Proc. ACL 2009, pages 558–566.
I. Dan Melamed, Giorgio Satta, and Ben Welling-
ton. 2004. Generalized multitext grammars. In
Proc. ACL 2004, pages 661–668.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. COLING-ACL
2006, pages 433–440.
Arjen Poutsma. 2000. Data-Oriented Translation. In
Proc. COLING 2000, pages 635–641.
David Talbot and Thorsten Brants. 2008. Random-
ized language models via perfect hash functions. In
Proc. ACL-08: HLT, pages 505–513.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statisti-
cal machine translation. In Proc. NAACL HLT 2009,
pages 236–244.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on
the complexity of translational equivalence. In
Proc. COLING-ACL 2006, pages 977–984.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proc. ACL-08: HLT, pages 559–567.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Transla-
tion, pages 138–141.
</reference>
<page confidence="0.833795">
1452
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956859">
<title confidence="0.999938">Learning to Translate with Source and Target Syntax</title>
<author confidence="0.999983">David Chiang</author>
<affiliation confidence="0.999976">USC Information Sciences Institute</affiliation>
<address confidence="0.990482">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA</address>
<email confidence="0.99973">chiang@isi.edu</email>
<abstract confidence="0.99882680952381">Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Alon Lavie</author>
</authors>
<title>Improving syntax driven translation models by re-structuring divergent and non-isomorphic parse tree structures.</title>
<date>2008</date>
<booktitle>In Proc. AMTA-2008 Student Research Workshop,</booktitle>
<pages>235--244</pages>
<contexts>
<context position="1736" citStr="Ambati and Lavie, 2008" startWordPosition="282" endWordPosition="285"> recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new synt</context>
</contexts>
<marker>Ambati, Lavie, 2008</marker>
<rawString>Vamshi Ambati and Alon Lavie. 2008. Improving syntax driven translation models by re-structuring divergent and non-isomorphic parse tree structures. In Proc. AMTA-2008 Student Research Workshop, pages 235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
</authors>
<title>A quasi-arithmetical notation for syntactic description.</title>
<date>1953</date>
<journal>Language,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Bar-Hillel, 1953</marker>
<rawString>Yehoshua Bar-Hillel. 1953. A quasi-arithmetical notation for syntactic description. Language, 29(1):47–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="11152" citStr="Chen and Goodman, 1998" startWordPosition="1872" endWordPosition="1875">4M initial phrase size 10 15 final rule size 6 6 nonterminals 2 2 loose source 0 00 loose target 0 2 Full training words 240+260M 190+220M final rule size 6 6 nonterminals 0 0 loose source 00 00 loose target 1 2 Table 2: Rule extraction settings used for experiments. “Loose source/target” is the maximum number of unaligned source/target words at the endpoints of a phrase. limit, above which the glue rule must be used. We trained two 5-gram language models: one on the combined English halves of the bitexts, and one on two billion words of English. These were smoothed using modified Kneser-Ney (Chen and Goodman, 1998) and stored using randomized data structures similar to those of Talbot and Brants (2008). The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al., 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). For all systems but the baselines, the features described in Section 3 were added. The systems were trained using MIRA (Crammer and Singer, 2003; Chiang et al., 2009) on a tuning set of about 3000 sentences of newswire from NIST MT evaluation data and GALE deve</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Wei Wang</author>
<author>Kevin Knight</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<pages>218--226</pages>
<contexts>
<context position="11357" citStr="Chiang et al., 2009" startWordPosition="1907" endWordPosition="1910">1 2 Table 2: Rule extraction settings used for experiments. “Loose source/target” is the maximum number of unaligned source/target words at the endpoints of a phrase. limit, above which the glue rule must be used. We trained two 5-gram language models: one on the combined English halves of the bitexts, and one on two billion words of English. These were smoothed using modified Kneser-Ney (Chen and Goodman, 1998) and stored using randomized data structures similar to those of Talbot and Brants (2008). The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al., 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). For all systems but the baselines, the features described in Section 3 were added. The systems were trained using MIRA (Crammer and Singer, 2003; Chiang et al., 2009) on a tuning set of about 3000 sentences of newswire from NIST MT evaluation data and GALE development data, disjoint from the training data. We optimized feature weights on 90% of this and held out the other 10% to determine when to stop. 4.2 Results Table 3 shows the scores on our development sets</context>
</contexts>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>David Chiang, Wei Wang, and Kevin Knight. 2009. 11,001 new features for statistical machine translation. In Proc. NAACL HLT 2009, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrasebased model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1232" citStr="Chiang, 2005" startWordPosition="200" endWordPosition="201">arn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In thi</context>
<context position="2594" citStr="Chiang, 2005" startWordPosition="421" endWordPosition="422">wing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difficult to satisfy syntactic constraints at decoding time. We therefore propose to move these constraints from the formalism into the model, implemented as features in the hierarchical phrasebased model Hiero (Chiang, 2005). This augmented model is able to learn from data whether to rely on syntax or not, or to revert back to monotone phrase-based translation. In experiments on Chinese-English and ArabicEnglish translation, we find that when both source and target syntax are made available to the model in an unobtrusive way, the model chooses to build structures that are more syntactically well-formed and yield significantly better translations than a nonsyntactic hierarchical phrase-based model. 2 Grammar extraction A synchronous tree-substitution grammar (STSG) is a set of rules or elementary tree pairs (&apos;y, α</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrasebased model for statistical machine translation. In Proc. ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ACL-EACL,</booktitle>
<pages>16--23</pages>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative lexicalised models for statistical parsing. In Proc. ACL-EACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="11635" citStr="Crammer and Singer, 2003" startWordPosition="1951" endWordPosition="1954">nglish halves of the bitexts, and one on two billion words of English. These were smoothed using modified Kneser-Ney (Chen and Goodman, 1998) and stored using randomized data structures similar to those of Talbot and Brants (2008). The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al., 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). For all systems but the baselines, the features described in Section 3 were added. The systems were trained using MIRA (Crammer and Singer, 2003; Chiang et al., 2009) on a tuning set of about 3000 sentences of newswire from NIST MT evaluation data and GALE development data, disjoint from the training data. We optimized feature weights on 90% of this and held out the other 10% to determine when to stop. 4.2 Results Table 3 shows the scores on our development sets and test sets, which are about 3000 and 2000 sentences, respectively, of newswire drawn from NIST MT evaluation data and GALE development data and disjoint from the tuning data. For Chinese, we first tried increasing the distortion limit from 10 words to 20. This limit control</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL 2003 Companion Volume,</booktitle>
<pages>205--208</pages>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. ACL 2003 Companion Volume, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. Third Workshop on Statistical Machine Translation,</booktitle>
<pages>44--52</pages>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment for syntax-based statistical machine translation. In Proc. Third Workshop on Statistical Machine Translation, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>51--60</pages>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Getting the structure right for word alignment: LEAF. In Proc. EMNLP 2007, pages 51–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1417" citStr="Galley et al., 2004" startWordPosition="225" endWordPosition="228">enge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL 2004, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1439" citStr="Galley et al., 2006" startWordPosition="229" endWordPosition="232">me old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic co</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. COLING-ACL 2006, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Seeing the wood for the trees: Data-Oriented Translation.</title>
<date>2003</date>
<booktitle>In Proc. MT Summit IX,</booktitle>
<pages>165--172</pages>
<marker>Hearne, Way, 2003</marker>
<rawString>Mary Hearne and Andy Way. 2003. Seeing the wood for the trees: Data-Oriented Translation. In Proc. MT Summit IX, pages 165–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. AMTA</booktitle>
<pages>65--73</pages>
<contexts>
<context position="1497" citStr="Huang et al., 2006" startWordPosition="241" endWordPosition="244"> that uses both source and target syntax for significant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. AMTA 2006, pages 65–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proc. SSST-2,</booktitle>
<pages>87--95</pages>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. SSST-2, pages 87–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1476" citStr="Liu et al., 2006" startWordPosition="237" endWordPosition="240"> a simple approach that uses both source and target syntax for significant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction i</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. COLING-ACL 2006, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan La</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proc. ACL</booktitle>
<pages>558--566</pages>
<contexts>
<context position="1711" citStr="Liu et al., 2009" startWordPosition="278" endWordPosition="281">try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some metho</context>
</contexts>
<marker>Liu, La, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan La, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. ACL 2009, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Ben Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In Proc. ACL</booktitle>
<pages>661--668</pages>
<contexts>
<context position="4616" citStr="Melamed et al., 2004" startWordPosition="746" endWordPosition="749">tered out. However, even on human-annotated data, treeto-tree extraction misses many rules, and many such rules would seem to be useful. For example, in Figure 2, the whole English phrase “Taiwan’s...shores” is an NP, but its Chinese counterpart is not a constituent. Furthermore, neither “surplus...shores” nor its Chinese counterpart are constituents. But both rules are arguably useful for translation. Wellington et al. therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar (Melamed et al., 2004), which is equivalent to synchronous set-local multicomponent CFG/TSG (Weir, 1988). But the problem illustrated in Figure 2 does not reflect a very deep fact about syntax or crosslingual divergences, but rather choices in annotation style that interact badly with the exact treeto-tree extraction heuristic. On the Chinese side, the IP is too flat (because 台湾/Táiwān has been analyzed as a topic), whereas the more articulated structure (1) [NP Táiwān [NP [PP zaì ...] shùnchā]] would also be quite reasonable. On the English side, the high attachment of the PP disagrees with the corresponding Chine</context>
</contexts>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004. Generalized multitext grammars. In Proc. ACL 2004, pages 661–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>433--440</pages>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING-ACL 2006, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Data-Oriented Translation.</title>
<date>2000</date>
<booktitle>In Proc. COLING</booktitle>
<pages>635--641</pages>
<marker>Poutsma, 2000</marker>
<rawString>Arjen Poutsma. 2000. Data-Oriented Translation. In Proc. COLING 2000, pages 635–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>505--513</pages>
<contexts>
<context position="11241" citStr="Talbot and Brants (2008)" startWordPosition="1886" endWordPosition="1889">e target 0 2 Full training words 240+260M 190+220M final rule size 6 6 nonterminals 0 0 loose source 00 00 loose target 1 2 Table 2: Rule extraction settings used for experiments. “Loose source/target” is the maximum number of unaligned source/target words at the endpoints of a phrase. limit, above which the glue rule must be used. We trained two 5-gram language models: one on the combined English halves of the bitexts, and one on two billion words of English. These were smoothed using modified Kneser-Ney (Chen and Goodman, 1998) and stored using randomized data structures similar to those of Talbot and Brants (2008). The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al., 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). For all systems but the baselines, the features described in Section 3 were added. The systems were trained using MIRA (Crammer and Singer, 2003; Chiang et al., 2009) on a tuning set of about 3000 sentences of newswire from NIST MT evaluation data and GALE development data, disjoint from the training data. We optimized feature weights on 90% of thi</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proc. ACL-08: HLT, pages 505–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<pages>236--244</pages>
<contexts>
<context position="9122" citStr="Venugopal et al. (2009)" startWordPosition="1523" endWordPosition="1526"> structure (a) because the English-side counterpart of IP/VP has the simple category NP. 3 Decoding In decoding, the rules extracted during training must be reassembled to form a derivation whose source side matches the input sentence. In the exact tree-to-tree approach, whenever substitution is performed, the root labels of the substituted trees must match the labels of the substitution nodes—call this the matching constraint. Because this constraint must be satisfied on both the source and target side, it can become difficult to generalize well from training examples to new input sentences. Venugopal et al. (2009), in the string-to-tree setting, attempt to soften the data-fragmentation effect of the matching constraint: instead of trying to find the single derivation with the highest probability, they sum over derivations that differ only in their nonterminal labels and try to find the single derivation-class with the highest probability. Still, only derivations that satisfy the matching constraint are included in the summation. But in some cases we may want to soften the matching constraint itself. Some syntactic categories are similar enough to be considered compatible: for example, if a rule rooted </context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Proc. NAACL HLT 2009, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly ContextSensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4698" citStr="Weir, 1988" startWordPosition="759" endWordPosition="760">and many such rules would seem to be useful. For example, in Figure 2, the whole English phrase “Taiwan’s...shores” is an NP, but its Chinese counterpart is not a constituent. Furthermore, neither “surplus...shores” nor its Chinese counterpart are constituents. But both rules are arguably useful for translation. Wellington et al. therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar (Melamed et al., 2004), which is equivalent to synchronous set-local multicomponent CFG/TSG (Weir, 1988). But the problem illustrated in Figure 2 does not reflect a very deep fact about syntax or crosslingual divergences, but rather choices in annotation style that interact badly with the exact treeto-tree extraction heuristic. On the Chinese side, the IP is too flat (because 台湾/Táiwān has been analyzed as a topic), whereas the more articulated structure (1) [NP Táiwān [NP [PP zaì ...] shùnchā]] would also be quite reasonable. On the English side, the high attachment of the PP disagrees with the corresponding Chinese structure, but low attachment also seems reasonable: (2) [NP [NP Taiwan’s] [NP </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing Mildly ContextSensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>977--984</pages>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proc. COLING-ACL 2006, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="1805" citStr="Zhang et al., 2008" startWordPosition="294" endWordPosition="297">w years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difficult to satisfy syntactic const</context>
<context position="5717" citStr="Zhang et al., 2008" startWordPosition="923" endWordPosition="926">ld also be quite reasonable. On the English side, the high attachment of the PP disagrees with the corresponding Chinese structure, but low attachment also seems reasonable: (2) [NP [NP Taiwan’s] [NP surplus in trade...]] Thus even in the gold-standard parse trees, phrase structure can be underspecified (like the flat IP above) or uncertain (like the PP attachment above). For this reason, some approaches work with a more flexible notion of constituency. Synchronous tree-sequence–substitution grammar (STSSG) allows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al., 2008). In the substitution operation, a sequence of sister substitution nodes is rewritten with a tree sequence of equal length (see Figure 3a). This extra flexibility effectively makes the analysis (1) available to us. Any STSSG can be converted into an equivalent STSG via the creation of virtual nodes (see Figure 3b): for every elementary tree sequence with roots X1, ... , Xn, create a new root node with a NP �P NN NNP�NN ni i M s that differ only in their nonterminal labels, only the most-frequent rule is kept, and its count is the total count of all the rules. This means that there is a one-to-</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. ACL-08: HLT, pages 559–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="2159" citStr="Zollmann and Venugopal, 2006" startWordPosition="352" endWordPosition="355"> models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difficult to satisfy syntactic constraints at decoding time. We therefore propose to move these constraints from the formalism into the model, implemented as features in the hierarchical phrasebased model Hiero (Chiang, 2005). This augmented model is able to learn from data whether to rely on syntax or not, or to revert back to monotone phrase-based translation. In experiments on Chinese</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>