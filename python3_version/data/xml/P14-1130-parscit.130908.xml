<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.995985">
Low-Rank Tensors for Scoring Dependency Structures
</title>
<author confidence="0.997079">
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola
</author>
<affiliation confidence="0.997145">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.949067">
{taolei, yuxin, yuanzh, regina, tommi}@csail.mit.edu
</email>
<sectionHeader confidence="0.994141" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999434190476191">
Accurate scoring of syntactic structures
such as head-modifier arcs in dependency
parsing typically requires rich, high-
dimensional feature representations. A
small subset of such features is often se-
lected manually. This is problematic when
features lack clear linguistic meaning as
in embeddings or when the information is
blended across features. In this paper, we
use tensors to map high-dimensional fea-
ture vectors into low dimensional repre-
sentations. We explicitly maintain the pa-
rameters as a low-rank tensor to obtain low
dimensional representations of words in
their syntactic roles, and to leverage mod-
ularity in the tensor for easy training with
online algorithms. Our parser consistently
outperforms the Turbo and MST parsers
across 14 different languages. We also ob-
tain the best published UAS results on 5
languages.1
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999844714285714">
Finding an expressive representation of input sen-
tences is crucial for accurate parsing. Syntac-
tic relations manifest themselves in a broad range
of surface indicators, ranging from morphological
to lexical, including positional and part-of-speech
(POS) tagging features. Traditionally, parsing re-
search has focused on modeling the direct connec-
tion between the features and the predicted syntac-
tic relations such as head-modifier (arc) relations
in dependency parsing. Even in the case of first-
order parsers, this results in a high-dimensional
vector representation of each arc. Discrete fea-
tures, and their cross products, can be further com-
plemented with auxiliary information about words
</bodyText>
<footnote confidence="0.9924325">
1Our code is available at https://github.com/
taolei87/RBGParser.
</footnote>
<bodyText confidence="0.999938731707317">
participating in an arc, such as continuous vector
representations of words. The exploding dimen-
sionality of rich feature vectors must then be bal-
anced with the difficulty of effectively learning the
associated parameters from limited training data.
A predominant way to counter the high dimen-
sionality of features is to manually design or select
a meaningful set of feature templates, which are
used to generate different types of features (Mc-
Donald et al., 2005a; Koo and Collins, 2010; Mar-
tins et al., 2013). Direct manual selection may be
problematic for two reasons. First, features may
lack clear linguistic interpretation as in distribu-
tional features or continuous vector embeddings of
words. Second, designing a small subset of tem-
plates (and features) is challenging when the rel-
evant linguistic information is distributed across
the features. For instance, morphological proper-
ties are closely tied to part-of-speech tags, which
in turn relate to positional features. These features
are not redundant. Therefore, we may suffer a per-
formance loss if we select only a small subset of
the features. On the other hand, by including all
the rich features, we face over-fitting problems.
We depart from this view and leverage high-
dimensional feature vectors by mapping them into
low dimensional representations. We begin by
representing high-dimensional feature vectors as
multi-way cross-products of smaller feature vec-
tors that represent words and their syntactic rela-
tions (arcs). The associated parameters are viewed
as a tensor (multi-way array) of low rank, and opti-
mized for parsing performance. By explicitly rep-
resenting the tensor in a low-rank form, we have
direct control over the effective dimensionality of
the set of parameters. We obtain role-dependent
low-dimensional representations for words (head,
modifier) that are specifically tailored for parsing
accuracy, and use standard online algorithms for
optimizing the low-rank tensor components.
The overall approach has clear linguistic and
</bodyText>
<page confidence="0.935377">
1381
</page>
<note confidence="0.8177785">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381–1391,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.905913142857143">
computational advantages:
• Our low dimensional embeddings are tailored
to the syntactic context of words (head, modi-
fier). This low dimensional syntactic abstrac-
tion can be thought of as a proxy to manually
constructed POS tags.
• By automatically selecting a small number of
</listItem>
<bodyText confidence="0.921342833333333">
dimensions useful for parsing, we can lever-
age a wide array of (correlated) features. Un-
like parsers such as MST, we can easily bene-
fit from auxiliary information (e.g., word vec-
tors) appended as features.
We implement the low-rank factorization model
in the context of first- and third-order depen-
dency parsing. The model was evaluated on 14
languages, using dependency data from CoNLL
2008 and CoNLL 2006. We compare our results
against the MST (McDonald et al., 2005a) and
Turbo (Martins et al., 2013) parsers. The low-rank
parser achieves average performance of 89.08%
across 14 languages, compared to 88.73% for the
Turbo parser, and 87.19% for MST. The power of
the low-rank model becomes evident in the ab-
sence of any part-of-speech tags. For instance,
on the English dataset, the low-rank model trained
without POS tags achieves 90.49% on first-order
parsing, while the baseline gets 86.70% if trained
under the same conditions, and 90.58% if trained
with 12 core POS tags. Finally, we demonstrate
that the model can successfully leverage word vec-
tor representations, in contrast to the baselines.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997688671641791">
Selecting Features for Dependency Parsing A
great deal of parsing research has been dedicated
to feature engineering (Lazaridou et al., 2013;
Marton et al., 2010; Marton et al., 2011). While
in most state-of-the-art parsers, features are se-
lected manually (McDonald et al., 2005a; McDon-
ald et al., 2005b; Koo and Collins, 2010; Mar-
tins et al., 2013; Zhang and McDonald, 2012a;
Rush and Petrov, 2012a), automatic feature selec-
tion methods are gaining popularity (Martins et al.,
2011b; Ballesteros and Nivre, 2012; Nilsson and
Nugues, 2010; Ballesteros, 2013). Following stan-
dard machine learning practices, these algorithms
iteratively select a subset of features by optimizing
parsing performance on a development set. These
feature selection methods are particularly promis-
ing in parsing scenarios where the optimal feature
set is likely to be a small subset of the original set
of candidate features. Our technique, in contrast,
is suitable for cases where the relevant information
is distributed across a larger set of related features.
Embedding for Dependency Parsing A lot of
recent work has been done on mapping words into
vector spaces (Collobert and Weston, 2008; Turian
et al., 2010; Dhillon et al., 2011; Mikolov et al.,
2013). Traditionally, these vector representations
have been derived primarily from co-occurrences
of words within sentences, ignoring syntactic roles
of the co-occurring words. Nevertheless, any such
word-level representation can be used to offset in-
herent sparsity problems associated with full lexi-
calization (Cirik and S¸ensoy, 2013). In this sense
they perform a role similar to POS tags.
Word-level vector space embeddings have so
far had limited impact on parsing performance.
From a computational perspective, adding non-
sparse vectors directly as features, including their
combinations, can significantly increase the num-
ber of active features for scoring syntactic struc-
tures (e.g., dependency arc). Because of this is-
sue, Cirik and S¸ensoy (2013) used word vectors
only as unigram features (without combinations)
as part of a shift reduce parser (Nivre et al., 2007).
The improvement on the overall parsing perfor-
mance was marginal. Another application of word
vectors is compositional vector grammar (Socher
et al., 2013). While this method learns to map
word combinations into vectors, it builds on ex-
isting word-level vector representations. In con-
trast, we represent words as vectors in a manner
that is directly optimized for parsing. This frame-
work enables us to learn new syntactically guided
embeddings while also leveraging separately esti-
mated word vectors as starting features, leading to
improved parsing performance.
Dimensionality Reduction Many machine
learning problems can be cast as matrix problems
where the matrix represents a set of co-varying
parameters. Such problems include, for example,
multi-task learning and collaborative filtering.
Rather than assuming that each parameter can be
set independently of others, it is helpful to assume
that the parameters vary in a low dimensional
subspace that has to be estimated together with the
parameters. In terms of the parameter matrix, this
corresponds to a low-rank assumption. Low-rank
constraints are commonly used for improving
</bodyText>
<page confidence="0.991055">
1382
</page>
<bodyText confidence="0.999884741935484">
generalization (Lee and Seung, 1999; Srebro
et al., 2003; Srebro et al., 2004; Evgeniou and
Pontil, 2007)
A strict low-rank assumption can be restrictive.
Indeed, recent approaches to matrix problems de-
compose the parameter matrix as a sum of low-
rank and sparse matrices (Tao and Yuan, 2011;
Zhou and Tao, 2011). The sparse matrix is used to
highlight a small number of parameters that should
vary independently even if most of them lie on
a low-dimensional subspace (Waters et al., 2011;
Chandrasekaran et al., 2011). We follow this de-
composition while extending the parameter matrix
into a tensor.
Tensors are multi-way generalizations of ma-
trices and possess an analogous notion of rank.
Tensors are increasingly used as tools in spec-
tral estimation (Hsu and Kakade, 2013), includ-
ing in parsing (Cohen et al., 2012) and other NLP
problems (de Cruys et al., 2013), where the goal
is to avoid local optima in maximum likelihood
estimation. In contrast, we expand features for
parsing into a multi-way tensor, and operate with
an explicit low-rank representation of the associ-
ated parameter tensor. The explicit representa-
tion sidesteps inherent complexity problems asso-
ciated with the tensor rank (Hillar and Lim, 2009).
Our parameters are divided into a sparse set corre-
sponding to manually chosen MST or Turbo parser
features and a larger set governed by a low-rank
tensor.
</bodyText>
<sectionHeader confidence="0.981571" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.9999884">
We will commence here by casting first-order de-
pendency parsing as a tensor estimation problem.
We will start by introducing the notation used in
the paper, followed by a more formal description
of our dependency parsing task.
</bodyText>
<subsectionHeader confidence="0.999279">
3.1 Basic Notations
</subsectionHeader>
<bodyText confidence="0.998454357142857">
Let A E Rnxnxd be a 3-dimensional tensor (a 3-
way array). We denote each element of the tensor
as Ai,j,k where i E [n], j E [n], k E [d] and [n]
is a shorthand for the set of integers 11, 2, · · · , n}.
Similarly, we use Mi,j and ui to represent the ele-
ments of matrix M and vector u, respectively.
We define the inner product of two tensors (or
matrices) as (A, B) = vec(A)Tvec(B), where
vec(·) concatenates the tensor (or matrix) ele-
ments into a column vector. The squared norm
of a tensor/matrix is denoted by 11A112 = (A, A).
The Kronecker product of three vectors is de-
noted by u®v ®w and forms a rank-1 tensor such
that
</bodyText>
<equation confidence="0.835289">
(u ® v ® w)i,j,k = uivjwk.
</equation>
<bodyText confidence="0.9989645">
Note that the vectors u, v, and w may be column
or row vectors. Their orientation is defined based
on usage. For example, u ® v is a rank-1 matrix
uvT when u and v are column vectors (uT v if they
are row vectors).
We say that tensor A is in Kruskal form if
</bodyText>
<equation confidence="0.995297666666667">
r
A = U(i, :) ® V (i, :) ® W(i, :) (1)
i=1
</equation>
<bodyText confidence="0.99997475">
where U, V E Rrxn, W E Rrxd and U(i, :) is the
ith row of matrix U. We will directly learn a low-
rank tensor A (because r is small) in this form as
one of our model parameters.
</bodyText>
<subsectionHeader confidence="0.998478">
3.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9999352">
Let x be a sentence and Y(x) the set of possible
dependency trees over the words in x. We assume
that the score S(x, y) of each candidate depen-
dency tree y E Y(x) decomposes into a sum of
“local” scores for arcs. Specifically:
</bodyText>
<equation confidence="0.945854">
S(x, y) = 1: s(h —* m) by E Y(x)
h,m E y
</equation>
<bodyText confidence="0.978403">
where h —* m is the head-modifier dependency
arc in the tree y. Each y is understood as a col-
lection of arcs h —* m where h and m index
words in x.2 For example, x(h) is the word cor-
responding to h. We suppress the dependence on
x whenever it is clear from context. For exam-
ple, s(h —* m) can depend on x in complicated
ways as discussed below. The predicted parse is
obtained as yˆ = arg maxyEY(x) S(x, y).
A key problem is how we parameterize the
arc scores s(h —* m). Following the MST
parser (McDonald et al., 2005a) we can define
rich features characterizing each head-modifier
arc, compiled into a sparse binary vector Oh,m E
RL that depends on the sentence x as well as the
chosen arc h —* m (again, we suppress the depen-
dence on x). Based on this feature representation,
we define the score of each arc as sθ(h —* m) =
2Note that in the case of high-order parsing, the sum
S(x, y) may also include local scores for other syntactic
structures, such as grandhead-head-modifier score s(g →
h → m). See (Martins et al., 2013) for a complete list of
these structures.
</bodyText>
<page confidence="0.907991">
1383
</page>
<table confidence="0.998445083333333">
Unigram features:
form form-p form-n
lemma lemma-p lemma-n
pos pos-p pos-n
morph bias
Bigram features:
pos-p, pos
pos, pos-n
pos, lemma
morph, lemma
Trigram features:
pos-p, pos, pos-n
</table>
<tableCaption confidence="0.997452">
Table 1: Word feature templates used by our
</tableCaption>
<bodyText confidence="0.9961858">
model. pos, form, lemma and morph stand for
the fine POS tag, word form, word lemma and the
morphology feature (provided in CoNLL format
file) of the current word. There is a bias term that
is always active for any word. The suffixes -p and
-n refer to the left and right of the current word re-
spectively. For example, pos-p means the POS tag
to the left of the current word in the sentence.
(θ, Oh→m) where θ E RL represent adjustable pa-
rameters to be learned, and L is the number of pa-
rameters (and possible features in Oh→m).
We can alternatively specify arc features in
terms of rank-1 tensors by taking the Kronecker
product of simpler feature vectors associated with
the head (vector Oh E Rn), and modifier (vector
Om E Rn), as well as the arc itself (vector Oh,m E
Rd). Here Oh,m is much lower dimensional than
the MST arc feature vector Oh→m discussed ear-
lier. For example, Oh,m may be composed of only
indicators for binned arc lengths3. Oh and Om, on
the other hand, are built from features shown in
Table 1. By taking the cross-product of all these
component feature vectors, we obtain the full fea-
ture representation for arc h —* m as a rank-1 ten-
sor
</bodyText>
<equation confidence="0.568017">
Oh ® Om ® Oh,m E Rn×n×d
</equation>
<bodyText confidence="0.999194">
Note that elements of this rank-1 tensor include
feature combinations that are not part of the fea-
ture crossings in Oh→m. In this sense, the rank-1
tensor represents a substantial feature expansion.
The arc score stensor(h —* m) associated with the
3In our current version, φh,m only contains the binned
arc length. Other possible features include, for example, the
label of the arc h → m, the POS tags between the head and
the modifier, boolean flags which indicate the occurence of
in-between punctutations or conjunctions, etc.
</bodyText>
<equation confidence="0.83865">
tensor representation is defined analogously as
stensor(h —* m) = (A, Oh ® Om ® Oh,m)
</equation>
<bodyText confidence="0.999535222222222">
where the adjustable parameters A also form a ten-
sor. Given the typical dimensions of the compo-
nent feature vectors, Oh, Om, Oh,m, it is not even
possible to store all the parameters in A. Indeed,
in the full English training set of CoNLL-2008, the
tensor involves around 8 x 1011 entries while the
MST feature vector has approximately 1.5 x 107
features. To counter this feature explosion, we re-
strict the parameters A to have low rank.
Low-Rank Dependency Scoring We can repre-
sent a rank-r tensor A explicitly in terms of pa-
rameter matrices U, V , and W as shown in Eq. 1.
As a result, the arc score for the tensor reduces to
evaluating UOh, V Om, and W Oh,m which are all
r dimensional vectors and can be computed effi-
ciently based on any sparse vectors Oh, Om, and
Oh,m. The resulting arc score stensor(h —* m) is
then
</bodyText>
<equation confidence="0.997623333333333">
r
[UOh]i[V Om]i[WOh,m]i (2)
i=1
</equation>
<bodyText confidence="0.998558423076923">
By learning parameters U, V , and W that function
well in dependency parsing, we also learn context-
dependent embeddings for words and arcs. Specif-
ically, UOh (for a given sentence, suppressed) is an
r dimensional vector representation of the word
corresponding to h as a head word. Similarly,
V Om provides an analogous representation for a
modifier m. Finally, W Oh,m is a vector embed-
ding of the supplemental arc-dependent informa-
tion. The resulting embedding is therefore tied
to the syntactic roles of the words (and arcs), and
learned in order to perform well in parsing.
We expect a dependency parsing model to ben-
efit from several aspects of the low-rank tensor
scoring. For example, we can easily incorpo-
rate additional useful features in the feature vec-
tors Oh, Om and Oh,m, since the low-rank assump-
tion (for small enough r) effectively counters the
otherwise uncontrolled feature expansion. More-
over, by controlling the amount of information
we can extract from each of the component fea-
ture vectors (via rank r), the statistical estimation
problem does not scale dramatically with the di-
mensions of Oh, Om and Oh,m. In particular, the
low-rank constraint can help generalize to unseen
arcs. Consider a feature S(x(h) = a) · S(x(m) =
</bodyText>
<page confidence="0.956137">
1384
</page>
<bodyText confidence="0.993157705882353">
b) · δ(dis(x, h, m) = c) which is non-zero only
for an arc a → b with distance c in sentence x.
If the arc has not been seen in the available train-
ing data, it does not contribute to the traditional
arc score sθ(·). In contrast, with the low-rank con-
straint, the arc score in Eq. 2 would typically be
non-zero.
Combined Scoring Our parsing model aims to
combine the strengths of both traditional features
from the MST/Turbo parser as well as the new
low-rank tensor features. In this way, our model
is able to capture a wide range of information in-
cluding the auxiliary features without having un-
controlled feature explosion, while still having the
full accessibility to the manually engineered fea-
tures that are proven useful. Specifically, we de-
fine the arc score sγ(h → m) as the combination
</bodyText>
<equation confidence="0.997896">
(1−γ)stensor(h → m) + γsθ(h → m)
r
= (1 − γ) [Uφh]i[V φm]i[W φh,m]i
i=1
+ γ hθ, φh,mi (3)
</equation>
<bodyText confidence="0.99982725">
where θ ∈ RL, U ∈ Rrxn, V ∈ Rrxn, and W ∈
Rrxd are the model parameters to be learned. The
rank r and γ ∈ [0, 1] (balancing the two scores)
represent hyper-parameters in our model.
</bodyText>
<sectionHeader confidence="0.995883" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.9981814">
The training set D = {(ˆxi, ˆyi)}Ni=1 consists of N
pairs, where each pair consists of a sentence xi
and the corresponding gold (target) parse yi. The
goal is to learn values for the parameters θ, U, V
and W that optimize the combined scoring func-
tion Sγ(x, y) = Eh,mEy sγ(h → m), defined
in Eq. 3, for parsing performance. We adopt a
maximum soft-margin framework for this learning
problem. Specifically, we find parameters θ, U, V ,
W, and {ξi} that minimize
</bodyText>
<equation confidence="0.9960108">
C
� ξi + kθk2 + kUk2 + kV k2 + kWk2
i
s.t. Sγ(ˆxi, ˆyi) ≥ Sγ(ˆxi, yi) + kˆyi − yik1 − ξi
∀yi ∈ Y(ˆxi), ∀i. (4)
</equation>
<bodyText confidence="0.999943625">
where kˆyi−yik1 is the number of mismatched arcs
between the two trees, and ξi is a non-negative
slack variable. The constraints serve to separate
the gold tree from other alternatives in Y(ˆxi) with
a margin that increases with distance.
The objective as stated is not jointly convex
with respect to U, V and W due to our explicit
representation of the low-rank tensor. However, if
we fix any two sets of parameters, for example, if
we fix V and W, then the combined score Sγ(x, y)
will be a linear function of both θ and U. As a re-
sult, the objective will be jointly convex with re-
spect to θ and U and could be optimized using
standard tools. However, to accelerate learning,
we adopt an online learning setup. Specifically,
we use the passive-aggressive learning algorithm
(Crammer et al., 2006) tailored to our setting, up-
dating pairs of parameter sets, (θ, U), (θ, V ) and
(θ, W) in an alternating manner. This method is
described below.
Online Learning In an online learning setup,
we update parameters successively based on each
sentence. In order to apply the passive-aggressive
algorithm, we fix two of U, V and W (say, for ex-
ample, V and W) in an alternating manner, and
apply a closed-form update to the remaining pa-
rameters (here U and θ). This is possible since
the objective function with respect to (θ, U) has a
similar form as in the original passive-aggressive
algorithm. To illustrate this, consider a training
sentence xi. The update involves finding first the
best competing tree,
</bodyText>
<equation confidence="0.986146">
˜yi = arg max Sγ(ˆxi, yi) + kˆyi − yik1 (5)
yiEY(ˆxi)
</equation>
<bodyText confidence="0.99915875">
which is the tree that violates the constraint in
Eq. 4 most (i.e. maximizes the loss ξi). We then
obtain parameter increments Δθ and ΔU by solv-
ing
</bodyText>
<equation confidence="0.9488705">
Δθ, oin� ≥0 1 kΔθk2 + 1 kΔUk2 + Cξ
s.t. Sγ(ˆxi, ˆyi) ≥ Sγ(ˆxi, ˜yi) + kˆyi − ˜yik1 − ξ
</equation>
<bodyText confidence="0.997687">
In this way, the optimization problem attempts to
keep the parameter change as small as possible,
while forcing it to achieve mostly zero loss on this
single instance. This problem has a closed form
solution
</bodyText>
<table confidence="0.847769833333333">
IDθ = min C �
loss ,γdθ
γ2kdθk2 + (1 − γ)2kduk2
IDU = min C �
loss ,(1 − γ)du
γ2kdθk2 + (1 − γ)2kduk2
</table>
<page confidence="0.952213">
1385
</page>
<bodyText confidence="0.999950679245283">
where
where (u (D v)i = uivi is the Hadamard (element-
wise) product. The magnitude of change of θ and
U is controlled by the parameter C. By varying C,
we can determine an appropriate step size for the
online updates. The updates also illustrate how γ
balances the effect of the MST component of the
score relative to the low-rank tensor score. When
γ = 0, the arc scores are entirely based on the low-
rank tensor and Aθ = 0. Note that φh, φm, φh,m,
and φh→m are typically very sparse for each word
or arc. Therefore du and dθ are also sparse and
can be computed efficiently.
Initialization The alternating online algorithm
relies on how we initialize U, V , and W since each
update is carried out in the context of the other
two. A random initialization of these parameters is
unlikely to work well, both due to the dimensions
involved, and the nature of the alternating updates.
We consider here instead a reasonable determinis-
tic “guess” as the initialization method.
We begin by training our model without any
low-rank parameters, and obtain parameters θ.
The majority of features in this MST component
can be expressed as elements of the feature ten-
sor, i.e., as [φh ® φm ® φh,m]i,j,k. We can there-
fore create a tensor representation of θ such that
Bi,j,k equals the corresponding parameter value
in θ. We use a low-rank version of B as the ini-
tialization. Specifically, we unfold the tensor B
into a matrix B(h) of dimensions n and nd, where
n = dim(φh) = dim(φm) and d = dim(φh,m).
For instance, a rank-1 tensor can be unfolded as
u ® v ® w = u ® vec(v ® w). We compute the
top-r SVD of the resulting unfolded matrix such
that B(h) = PT SQ. U is initialized as P. Each
right singular vector SiQ(i, :) is also a matrix in
Rnxd. The leading left and right singular vectors
of this matrix are assigned to V (i, :) and W (i, :)
respectively. In our implementation, we run one
epoch of our model without low-rank parameters
and initialize the tensor A.
Parameter Averaging The passive-aggressive
algorithm regularizes the increments (e.g. Aθ and
AU) during each update but does not include any
overall regularization. In other words, keeping up-
dating the model may lead to large parameter val-
ues and over-fitting. To counter this effect, we use
parameter averaging as used in the MST and Turbo
parsers. The final parameters are those averaged
across all the iterations (cf. (Collins, 2002)). For
simplicity, in our algorithm we average U, V , W
and θ separately, which works well empirically.
</bodyText>
<sectionHeader confidence="0.998147" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999901857142857">
Datasets We test our dependency model on 14
languages, including the English dataset from
CoNLL 2008 shared tasks and all 13 datasets from
CoNLL 2006 shared tasks (Buchholz and Marsi,
2006; Surdeanu et al., 2008). These datasets in-
clude manually annotated dependency trees, POS
tags and morphological information. Following
standard practices, we encode this information as
features.
Methods We compare our model to MST and
Turbo parsers on non-projective dependency pars-
ing. For our parser, we train both a first-order
parsing model (as described in Section 3 and 4)
as well as a third-order model. The third order
parser simply adds high-order features, those typ-
ically used in MST and Turbo parsers, into our
sθ(x, y) = (θ, φ(x, y)) scoring component. The
decoding algorithm for the third-order parsing is
based on (Zhang et al., 2014). For the Turbo
parser, we directly compare with the recent pub-
lished results in (Martins et al., 2013). For the
MST parser, we train and test using the most re-
cent version of the code.4 In addition, we im-
plemented two additional baselines, NT-1st (first
order) and NT-3rd (third order), corresponding to
our model without the tensor component.
Features For the arc feature vector φh→m, we
use the same set of feature templates as MST
v0.5.1. For head/modifier vector φh and φm, we
show the complete set of feature templates used
by our model in Table 1. Finally, we use a similar
set of feature templates as Turbo v2.1 for 3rd order
parsing.
To add auxiliary word vector representations,
we use the publicly available word vectors (Cirik
</bodyText>
<equation confidence="0.968357636363636">
4http://sourceforge.net/projects/mstparser/
loss = Sγ(ˆxi, ˜yi) + llˆyi − ˜yill1 − Sγ(ˆxi, ˆyi)
dθ = �
du = h→m E ˆyz
�
h→m E ˆyz
�φh→m − φh→m
h→m E ˜yz
[(V φm) (D (W φh,m)] ® φh
�− [(V φm) (D (W φh,m)] ® φh
h→m E ˜yz
</equation>
<page confidence="0.974125">
1386
</page>
<table confidence="0.999555823529412">
First-order only High-order
Ours NT-1st MST Turbo Ours-3rd NT-3rd MST-2nd Turbo-3rd Best Published
Arabic 79.60 78.71 78.3 77.23 79.95 79.53 78.75 79.64 81.12 (Ma11)
Bulgarian 92.30 91.14 90.98 91.76 93.50 92.79 91.56 93.1 94.02 (Zh13)
Chinese 91.43 90.85 90.40 88.49 92.68 92.39 91.77 89.98 91.89 (Ma10)
Czech 87.90 86.62 86.18 87.66 90.50 89.43 87.3 90.32 90.32 (Ma13)
Danish 90.64 89.80 89.84 89.42 91.39 90.82 90.5 91.48 92.00 (Zh13)
Dutch 84.81 83.77 82.89 83.61 86.41 86.08 84.11 86.19 86.19 (Ma13)
English 91.84 91.40 90.59 91.21 93.02 92.82 91.54 93.22 93.22 (Ma13)
German 90.24 89.70 89.54 90.52 91.97 92.26 90.14 92.41 92.41 (Ma13)
Japanese 93.74 93.36 93.38 92.78 93.71 93.23 92.92 93.52 93.72 (Ma11)
Portuguese 90.94 90.67 89.92 91.14 91.92 91.63 91.08 92.69 93.03 (Ko10)
Slovene 84.25 83.15 82.09 82.81 86.24 86.07 83.25 86.01 86.95 (Ma11)
Spanish 85.27 84.95 83.79 83.61 88.00 87.47 84.33 85.59 87.96 (Zh13)
Swedish 89.86 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13)
Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10)
Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43
</table>
<tableCaption confidence="0.992394">
Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the
</tableCaption>
<bodyText confidence="0.992455">
English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyper-
parameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding
to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et
al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013),
Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013).
and S¸ensoy, 2013), learned from raw data (Glober-
son et al., 2007; Maron et al., 2010). Three
languages in our dataset – English, German and
Swedish – have corresponding word vectors in this
collection.5 The dimensionality of this representa-
tion varies by language: English has 50 dimen-
sional word vectors, while German and Swedish
have 25 dimensional word vectors. Each entry of
the word vector is added as a feature value into
feature vectors Oh and Om. For each word in the
sentence, we add its own word vector as well as
the vectors of its left and right words.
We should note that since our model parameter
A is represented and learned in the low-rank form,
we only have to store and maintain the low-rank
projections UOh, V Om and W Oh,m rather than ex-
plicitly calculate the feature tensor Oh⊗Om⊗Oh,m.
Therefore updating parameters and decoding a
sentence is still efficient, i.e., linear in the num-
ber of values of the feature vector. In contrast,
assume we take the cross-product of the auxiliary
word vector values, POS tags and lexical items of
a word and its context, and add the crossed val-
ues into a normal model (in Oh→m). The number
of features for each arc would be at least quadratic,
growing into thousands, and would be a significant
impediment to parsing efficiency.
Evaluation Following standard practices, we
train our full model and the baselines for 10
</bodyText>
<footnote confidence="0.668713">
5https://github.com/wolet/sprml13-word-embeddings
</footnote>
<bodyText confidence="0.999124333333333">
epochs. As the evaluation measure, we use un-
labeled attachment scores (UAS) excluding punc-
tuation. In all the reported experiments, the hyper-
parameters are set as follows: r = 50 (rank of the
tensor), C = 1 for first-order model and C = 0.01
for third-order model.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999962272727273">
Overall Performance Table 2 shows the per-
formance of our model and the baselines on 14
CoNLL datasets. Our model outperforms Turbo
parser, MST parser, as well as its own variants
without the tensor component. The improvements
of our low-rank model are consistent across lan-
guages: results for the first order parser are better
on 11 out of 14 languages. By comparing NT-1st
and NT-3rd (models without low-rank) with our
full model (with low-rank), we obtain 0.7% abso-
lute improvement on first-order parsing, and 0.3%
improvement on third-order parsing. Our model
also achieves the best UAS on 5 languages.
We next focus on the first-order model and
gauge the impact of the tensor component. First,
we test our model by varying the hyper-parameter
-y which balances the tensor score and the tradi-
tional MST/Turbo score components. Figure 1
shows the average UAS on CoNLL test datasets
after each training epoch. We can see that the im-
provement of adding the low-rank tensor is con-
sistent across various choices of hyper parame-
</bodyText>
<page confidence="0.985021">
1387
</page>
<figure confidence="0.835821">
# Epochs
</figure>
<figureCaption confidence="0.6838614">
Figure 1: Average UAS on CoNLL testsets af-
ter different epochs. Our full model consistently
performs better than NT-1st (its variation without
tensor component) under different choices of the
hyper-parameter &apos;y.
</figureCaption>
<table confidence="0.80321375">
no word vector with word vector
English 91.84 92.07
German 90.24 90.48
Swedish 89.86 90.38
</table>
<tableCaption confidence="0.816193333333333">
Table 3: Results of adding unsupervised word vec-
tors to the tensor. Adding this information yields
consistent improvement for all languages.
</tableCaption>
<bodyText confidence="0.9993985">
ter &apos;y. When training with the tensor component
alone (&apos;y = 0), the model converges more slowly.
Learning of the tensor is harder because the scor-
ing function is not linear (nor convex) with respect
to parameters U, V and W. However, the tensor
scoring component achieves better generalization
on the test data, resulting in better UAS than NT-
1st after 8 training epochs.
To assess the ability of our model to incorpo-
rate a range of features, we add unsupervised word
vectors to our model. As described in previous
section, we do so by appending the values of dif-
ferent coordinates in the word vector into oh and
om. As Table 3 shows, adding this information in-
creases the parsing performance for all the three
languages. For instance, we obtain more than
0.5% absolute improvement on Swedish.
Syntactic Abstraction without POS Since our
model learns a compressed representation of fea-
ture vectors, we are interested to measure its per-
formance when part-of-speech tags are not pro-
vided (See Table 4). The rationale is that given all
other features, the model would induce representa-
tions that play a similar role to POS tags. Note that
</bodyText>
<table confidence="0.9988218">
Our model NT-1st
-POS +wv. -POS +POS
English 88.89 90.49 86.70 90.58
German 82.63 85.80 78.71 88.50
Swedish 81.84 85.90 79.65 88.75
</table>
<tableCaption confidence="0.986871">
Table 4: The first three columns show parsing re-
</tableCaption>
<bodyText confidence="0.994503642857143">
sults when models are trained without POS tags.
The last column gives the upper-bound, i.e. the
performance of a parser trained with 12 Core POS
tags. The low-rank model outperforms NT-1st by
a large margin. Adding word vector features fur-
ther improves performance.
the performance of traditional parsers drops when
tags are not provided. For example, the perfor-
mance gap is 10% on German. Our experiments
show that low-rank parser operates effectively in
the absence of tags. In fact, it nearly reaches the
performance of the original parser that used the
tags on English.
Examples of Derived Projections We manu-
ally analyze low-dimensional projections to assess
whether they capture syntactic abstraction. For
this purpose, we train a model with only a ten-
sor component (such that it has to learn an accu-
rate tensor) on the English dataset and obtain low
dimensional embeddings Uow and V ow for each
word. The two r-dimension vectors are concate-
nated as an “averaged” vector. We use this vector
to calculate the cosine similarity between words.
Table 5 shows examples of five closest neighbors
of queried words. While these lists include some
noise, we can clearly see that the neighbors ex-
hibit similar syntactic behavior. For example, “on”
is close to other prepositions. More interestingly,
we can consider the impact of syntactic context
on the derived projections. The bottom part of
Table 5 shows that the neighbors change substan-
tially depending on the syntactic role of the word.
For example, the closest words to the word “in-
crease” are verbs in the context phrase “will in-
crease again”, while the closest words become
nouns given a different phrase “an increase of”.
Running Time Table 6 illustrates the impact of
estimating low-rank tensor parameters on the run-
ning time of the algorithm. For comparison, we
also show the NT-1st times across three typical
languages. The Arabic dataset has the longest av-
erage sentence length, while the Chinese dataset
</bodyText>
<figure confidence="0.996837866666667">
84.0%
2 4 6 8 10
88.0%
87.5%
87.0%
86.5%
86.0%
85.5%
85.0%
84.5%
y=0.0
y=0.2
y=0.3
y=0.4
NT−1st
</figure>
<page confidence="0.96527">
1388
</page>
<table confidence="0.761887833333333">
greatly profit says on when
actively earnings adds with where
openly franchisees predicts into what
significantly shares noted at why
outright revenue wrote during which
substantially members contends over who
increase will increase again an increase of
rise arguing gain
advance be prices
contest charging payment
halt gone members
Exchequer making subsidiary
hit attacks hit the hardest hit is
shed distributes monopolies
rallied stayed pills
triggered sang sophistication
appeared removed ventures
understate eased factors
</table>
<tableCaption confidence="0.793520666666667">
Table 5: Five closest neighbors of the queried
words (shown in bold). The upper part shows our
learned embeddings group words with similar syn-
</tableCaption>
<bodyText confidence="0.809949333333333">
tactic behavior. The two bottom parts of the table
demonstrate that how the projections change de-
pending on the syntactic context of the word.
</bodyText>
<table confidence="0.9954742">
#Tok. Len. Train. Time (hour)
NT-1st Ours
Arabic 42K 32 0.13 0.22
Chinese 337K 6 0.37 0.65
English 958K 24 1.88 2.83
</table>
<tableCaption confidence="0.7600635">
Table 6: Comparison of training times across three
typical datasets. The second column is the number
</tableCaption>
<bodyText confidence="0.993197125">
of tokens in each data set. The third column shows
the average sentence length. Both first-order mod-
els are implemented in Java and run as a single
process.
has the shortest sentence length in CoNLL 2006.
Based on these results, estimating a rank-50 tensor
together with MST parameters only increases the
running time by a factor of 1.7.
</bodyText>
<sectionHeader confidence="0.991747" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999042272727273">
Accurate scoring of syntactic structures such as
head-modifier arcs in dependency parsing typi-
cally requires rich, high-dimensional feature rep-
resentations. We introduce a low-rank factoriza-
tion method that enables to map high dimensional
feature vectors into low dimensional representa-
tions. Our method maintains the parameters as a
low-rank tensor to obtain low dimensional repre-
sentations of words in their syntactic roles, and to
leverage modularity in the tensor for easy train-
ing with online algorithms. We implement the
approach on first-order to third-order dependency
parsing. Our parser outperforms the Turbo and
MST parsers across 14 languages.
Future work involves extending the tensor com-
ponent to capture higher-order structures. In par-
ticular, we would consider second-order structures
such as grandparent-head-modifier by increasing
the dimensionality of the tensor. This tensor will
accordingly be a four or five-way array. The online
update algorithm remains applicable since each di-
mension is optimized in an alternating fashion.
</bodyText>
<sectionHeader confidence="0.996907" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999992785714286">
The authors acknowledge the support of the MURI
program (W911NF-10-1-0533) and the DARPA
BOLT program. This research is developed in col-
laboration with the Arabic Language Technoligies
(ALT) group at Qatar Computing Research Insti-
tute (QCRI) within the LYAS project. We thank
Volkan Cirik for sharing the unsupervised word
vector data. Thanks to Amir Globerson, Andreea
Gane, the members of the MIT NLP group and
the ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
</bodyText>
<sectionHeader confidence="0.997512" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.880317666666667">
Miguel Ballesteros and Joakim Nivre. 2012. Mal-
tOptimizer: An optimization tool for MaltParser. In
EACL. The Association for Computer Linguistics.
Miguel Ballesteros. 2013. Effective morpholog-
ical feature selection with MaltOptimizer at the
SPMRL 2013 shared task. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06.
Association for Computational Linguistics.
Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Par-
rilo, and Alan S Willsky. 2011. Rank-sparsity in-
coherence for matrix decomposition. SIAM Journal
on Optimization.
Volkan Cirik and H¨usn¨u S¸ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task : Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
</reference>
<page confidence="0.97031">
1389
</page>
<reference confidence="0.999558530973451">
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1. Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - Volume 10, EMNLP ’02. As-
sociation for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of
semantic compositionality. In HLT-NAACL. The As-
sociation for Computational Linguistics.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multiview learning of word embeddings via
CCA. In Advances in Neural Information Process-
ing Systems.
A Evgeniou and Massimiliano Pontil. 2007. Multi-
task feature learning. In Advances in neural infor-
mation processing systems: Proceedings of the 2006
conference. The MIT Press.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search.
Christopher Hillar and Lek-Heng Lim. 2009. Most
tensor problems are NP-hard. arXiv preprint
arXiv:0911.1393.
Daniel Hsu and Sham M Kakade. 2013. Learning mix-
tures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th
Conference on Innovations in Theoretical Computer
Science. ACM.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10. Association for Com-
putational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-
of-speech induction. In Advances in Neural Infor-
mation Processing Systems.
Andr´e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M´ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M´ario A. T. Figueiredo. 2011a. Dual
decomposition with many overlapping components.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11. Association for Computational Linguistics.
Andr´e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M´ario AT Figueiredo. 2011b. Structured spar-
sity in structured prediction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr´e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving arabic dependency parsing with
lexical and inflectional morphological features. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, SPMRL ’10. Association for Computa-
tional Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2011. Improving arabic dependency parsing with
form-based and functional morphological features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05).
</reference>
<page confidence="0.760426">
1390
</page>
<reference confidence="0.99987194059406">
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR.
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010). Coling
2010 Organizing Committee.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering.
Alexander Rush and Slav Petrov. 2012a. Vine pruning
for efficient multi-pass dependency parsing. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL ’12).
Alexander M Rush and Slav Petrov. 2012b. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics.
Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted
low-rank approximations. In ICML.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.
2004. Maximum-margin matrix factorization. In
Advances in neural information processing systems.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ’08. Association for
Computational Linguistics.
Min Tao and Xiaoming Yuan. 2011. Recovering low-
rank and sparse components of matrices from in-
complete and noisy observations. SIAM Journal on
Optimization.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10. Association for Com-
putational Linguistics.
Andrew E Waters, Aswin C Sankaranarayanan, and
Richard Baraniuk. 2011. SpaRCS: Recovering low-
rank and sparse matrices from compressive mea-
surements. In Advances in Neural Information Pro-
cessing Systems.
Hao Zhang and Ryan McDonald. 2012a. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ’12. Association for Compu-
tational Linguistics.
Hao Zhang and Ryan McDonald. 2012b. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Association for Computational Linguistics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
Tianyi Zhou and Dacheng Tao. 2011. Godec: Ran-
domized low-rank &amp; sparse matrix decomposition in
noisy case. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11).
</reference>
<page confidence="0.992883">
1391
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382035">
<title confidence="0.99984">Low-Rank Tensors for Scoring Dependency Structures</title>
<author confidence="0.998379">Tao Lei</author>
<author confidence="0.998379">Yu Xin</author>
<author confidence="0.998379">Yuan Zhang</author>
<author confidence="0.998379">Regina Barzilay</author>
<author confidence="0.998379">Tommi</author>
<affiliation confidence="0.9867545">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.593527">yuxin,yuanzh,regina,</email>
<abstract confidence="0.983831380952381">Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, highdimensional feature representations. A small subset of such features is often selected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations. We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also obtain the best published UAS results on 5</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Joakim Nivre</author>
</authors>
<title>MaltOptimizer: An optimization tool for MaltParser. In EACL. The Association for Computer Linguistics.</title>
<date>2012</date>
<contexts>
<context position="6004" citStr="Ballesteros and Nivre, 2012" startWordPosition="905" endWordPosition="908">that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on </context>
</contexts>
<marker>Ballesteros, Nivre, 2012</marker>
<rawString>Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: An optimization tool for MaltParser. In EACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
</authors>
<title>Effective morphological feature selection with MaltOptimizer at the SPMRL 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6050" citStr="Ballesteros, 2013" startWordPosition="913" endWordPosition="914">presentations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert an</context>
</contexts>
<marker>Ballesteros, 2013</marker>
<rawString>Miguel Ballesteros. 2013. Effective morphological feature selection with MaltOptimizer at the SPMRL 2013 shared task. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23481" citStr="Buchholz and Marsi, 2006" startWordPosition="4009" endWordPosition="4012">not include any overall regularization. In other words, keeping updating the model may lead to large parameter values and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. (Collins, 2002)). For simplicity, in our algorithm we average U, V , W and θ separately, which works well empirically. 5 Experimental Setup Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our sθ(x, y) = (θ, φ(x, y)) scoring component. The decoding algorithm for the th</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Venkat Chandrasekaran</author>
<author>Sujay Sanghavi</author>
<author>Pablo A Parrilo</author>
<author>Alan S Willsky</author>
</authors>
<title>Rank-sparsity incoherence for matrix decomposition.</title>
<date>2011</date>
<journal>SIAM Journal on Optimization.</journal>
<contexts>
<context position="9260" citStr="Chandrasekaran et al., 2011" startWordPosition="1403" endWordPosition="1406">r matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit repre</context>
</contexts>
<marker>Chandrasekaran, Sanghavi, Parrilo, Willsky, 2011</marker>
<rawString>Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. 2011. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volkan Cirik</author>
<author>H¨usn¨u S¸ensoy</author>
</authors>
<title>The AI-KU system at the SPMRL 2013 shared task : Unsupervised features for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages. Association for Computational Linguistics.</booktitle>
<marker>Cirik, S¸ensoy, 2013</marker>
<rawString>Volkan Cirik and H¨usn¨u S¸ensoy. 2013. The AI-KU system at the SPMRL 2013 shared task : Unsupervised features for dependency parsing. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9561" citStr="Cohen et al., 2012" startWordPosition="1452" endWordPosition="1455"> decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit representation sidesteps inherent complexity problems associated with the tensor rank (Hillar and Lim, 2009). Our parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor. 3 Problem Formulation We will commence he</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23168" citStr="Collins, 2002" startWordPosition="3959" endWordPosition="3960"> of this matrix are assigned to V (i, :) and W (i, :) respectively. In our implementation, we run one epoch of our model without low-rank parameters and initialize the tensor A. Parameter Averaging The passive-aggressive algorithm regularizes the increments (e.g. Aθ and AU) during each update but does not include any overall regularization. In other words, keeping updating the model may lead to large parameter values and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. (Collins, 2002)). For simplicity, in our algorithm we average U, V , W and θ separately, which works well empirically. 5 Experimental Setup Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML.</booktitle>
<contexts>
<context position="6664" citStr="Collobert and Weston, 2008" startWordPosition="1006" endWordPosition="1009">teros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008; Turian et al., 2010; Dhillon et al., 2011; Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vect</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="19472" citStr="Crammer et al., 2006" startWordPosition="3282" endWordPosition="3285">natives in Y(ˆxi) with a margin that increases with distance. The objective as stated is not jointly convex with respect to U, V and W due to our explicit representation of the low-rank tensor. However, if we fix any two sets of parameters, for example, if we fix V and W, then the combined score Sγ(x, y) will be a linear function of both θ and U. As a result, the objective will be jointly convex with respect to θ and U and could be optimized using standard tools. However, to accelerate learning, we adopt an online learning setup. Specifically, we use the passive-aggressive learning algorithm (Crammer et al., 2006) tailored to our setting, updating pairs of parameter sets, (θ, U), (θ, V ) and (θ, W) in an alternating manner. This method is described below. Online Learning In an online learning setup, we update parameters successively based on each sentence. In order to apply the passive-aggressive algorithm, we fix two of U, V and W (say, for example, V and W) in an alternating manner, and apply a closed-form update to the remaining parameters (here U and θ). This is possible since the objective function with respect to (θ, U) has a similar form as in the original passive-aggressive algorithm. To illust</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>A tensor-based factorization model of semantic compositionality.</title>
<date>2013</date>
<booktitle>In HLT-NAACL. The Association for Computational Linguistics.</booktitle>
<marker>Van de Cruys, Poibeau, Korhonen, 2013</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic compositionality. In HLT-NAACL. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Multiview learning of word embeddings via CCA.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6707" citStr="Dhillon et al., 2011" startWordPosition="1014" endWordPosition="1017"> practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008; Turian et al., 2010; Dhillon et al., 2011; Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their c</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. 2011. Multiview learning of word embeddings via CCA. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Evgeniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Multitask feature learning.</title>
<date>2007</date>
<booktitle>In Advances in neural information processing systems: Proceedings of the</booktitle>
<pages>conference.</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8848" citStr="Evgeniou and Pontil, 2007" startWordPosition="1334" endWordPosition="1337">an be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. Rather than assuming that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are in</context>
</contexts>
<marker>Evgeniou, Pontil, 2007</marker>
<rawString>A Evgeniou and Massimiliano Pontil. 2007. Multitask feature learning. In Advances in neural information processing systems: Proceedings of the 2006 conference. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Gal Chechik</author>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
</authors>
<title>Euclidean embedding of cooccurrence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="26883" citStr="Globerson et al., 2007" startWordPosition="4582" endWordPosition="4586">er parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our model parameter A is represented and learned in the low-rank form, we</context>
</contexts>
<marker>Globerson, Chechik, Pereira, Tishby, 2007</marker>
<rawString>Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. 2007. Euclidean embedding of cooccurrence data. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Hillar</author>
<author>Lek-Heng Lim</author>
</authors>
<title>Most tensor problems are NP-hard. arXiv preprint arXiv:0911.1393.</title>
<date>2009</date>
<contexts>
<context position="9963" citStr="Hillar and Lim, 2009" startWordPosition="1515" endWordPosition="1518"> Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit representation sidesteps inherent complexity problems associated with the tensor rank (Hillar and Lim, 2009). Our parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor. 3 Problem Formulation We will commence here by casting first-order dependency parsing as a tensor estimation problem. We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task. 3.1 Basic Notations Let A E Rnxnxd be a 3-dimensional tensor (a 3- way array). We denote each element of the tensor as Ai,j,k where i E [n], j E [n], k E [d] and [n] is a shorthand for the set o</context>
</contexts>
<marker>Hillar, Lim, 2009</marker>
<rawString>Christopher Hillar and Lek-Heng Lim. 2009. Most tensor problems are NP-hard. arXiv preprint arXiv:0911.1393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
</authors>
<title>Learning mixtures of spherical gaussians: moment methods and spectral decompositions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="9518" citStr="Hsu and Kakade, 2013" startWordPosition="1444" endWordPosition="1447"> Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit representation sidesteps inherent complexity problems associated with the tensor rank (Hillar and Lim, 2009). Our parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor</context>
</contexts>
<marker>Hsu, Kakade, 2013</marker>
<rawString>Daniel Hsu and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2364" citStr="Koo and Collins, 2010" startWordPosition="344" endWordPosition="347"> can be further complemented with auxiliary information about words 1Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subs</context>
<context position="5818" citStr="Koo and Collins, 2010" startWordPosition="876" endWordPosition="879">s achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in cont</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26739" citStr="Koo et al. (2010)" startWordPosition="4558" endWordPosition="4561">75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
</authors>
<title>Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5632" citStr="Lazaridou et al., 2013" startWordPosition="845" endWordPosition="848">87.19% for MST. The power of the low-rank model becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature select</context>
</contexts>
<marker>Lazaridou, Vecchi, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni. 2013. Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature.</journal>
<contexts>
<context position="8778" citStr="Lee and Seung, 1999" startWordPosition="1322" endWordPosition="1325">ance. Dimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. Rather than assuming that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizatio</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D Lee and H Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yariv Maron</author>
<author>Michael Lamar</author>
<author>Elie Bienenstock</author>
</authors>
<title>Sphere embedding: An application to partof-speech induction.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="26904" citStr="Maron et al., 2010" startWordPosition="4587" endWordPosition="4590">ts on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store a</context>
</contexts>
<marker>Maron, Lamar, Bienenstock, 2010</marker>
<rawString>Yariv Maron, Michael Lamar, and Elie Bienenstock. 2010. Sphere embedding: An application to partof-speech induction. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro MQ Aguiar</author>
<author>M´ario AT Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26673" citStr="Martins et al. (2010)" startWordPosition="4546" endWordPosition="4549"> 90.83 89.05 91.14 91.62 (Zh13) Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om.</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and M´ario AT Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5974" citStr="Martins et al., 2011" startWordPosition="901" endWordPosition="904">inally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of</context>
<context position="26695" citStr="Martins et al. (2011" startWordPosition="4550" endWordPosition="4553">2 (Zh13) Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the </context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011a. Dual decomposition with many overlapping components. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Pedro MQ Aguiar</author>
<author>M´ario AT Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5974" citStr="Martins et al., 2011" startWordPosition="901" endWordPosition="904">inally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of</context>
<context position="26695" citStr="Martins et al. (2011" startWordPosition="4550" endWordPosition="4553">2 (Zh13) Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the </context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e FT Martins, Noah A Smith, Pedro MQ Aguiar, and M´ario AT Figueiredo. 2011b. Structured sparsity in structured prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2387" citStr="Martins et al., 2013" startWordPosition="348" endWordPosition="352">ented with auxiliary information about words 1Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subset of the features. On </context>
<context position="4875" citStr="Martins et al., 2013" startWordPosition="726" endWordPosition="729">action can be thought of as a proxy to manually constructed POS tags. • By automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features. Unlike parsers such as MST, we can easily benefit from auxiliary information (e.g., word vectors) appended as features. We implement the low-rank factorization model in the context of first- and third-order dependency parsing. The model was evaluated on 14 languages, using dependency data from CoNLL 2008 and CoNLL 2006. We compare our results against the MST (McDonald et al., 2005a) and Turbo (Martins et al., 2013) parsers. The low-rank parser achieves average performance of 89.08% across 14 languages, compared to 88.73% for the Turbo parser, and 87.19% for MST. The power of the low-rank model becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines</context>
<context position="12840" citStr="Martins et al., 2013" startWordPosition="2085" endWordPosition="2088">A key problem is how we parameterize the arc scores s(h —* m). Following the MST parser (McDonald et al., 2005a) we can define rich features characterizing each head-modifier arc, compiled into a sparse binary vector Oh,m E RL that depends on the sentence x as well as the chosen arc h —* m (again, we suppress the dependence on x). Based on this feature representation, we define the score of each arc as sθ(h —* m) = 2Note that in the case of high-order parsing, the sum S(x, y) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s(g → h → m). See (Martins et al., 2013) for a complete list of these structures. 1383 Unigram features: form form-p form-n lemma lemma-p lemma-n pos pos-p pos-n morph bias Bigram features: pos-p, pos pos, pos-n pos, lemma morph, lemma Trigram features: pos-p, pos, pos-n Table 1: Word feature templates used by our model. pos, form, lemma and morph stand for the fine POS tag, word form, word lemma and the morphology feature (provided in CoNLL format file) of the current word. There is a bias term that is always active for any word. The suffixes -p and -n refer to the left and right of the current word respectively. For example, pos-p</context>
<context position="24234" citStr="Martins et al., 2013" startWordPosition="4132" endWordPosition="4135">ing standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our sθ(x, y) = (θ, φ(x, y)) scoring component. The decoding algorithm for the third-order parsing is based on (Zhang et al., 2014). For the Turbo parser, we directly compare with the recent published results in (Martins et al., 2013). For the MST parser, we train and test using the most recent version of the code.4 In addition, we implemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component. Features For the arc feature vector φh→m, we use the same set of feature templates as MST v0.5.1. For head/modifier vector φh and φm, we show the complete set of feature templates used by our model in Table 1. Finally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing. To add auxiliary word vector representations, we use the pub</context>
<context position="26720" citStr="Martins et al. (2013)" startWordPosition="4554" endWordPosition="4557">4.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own </context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e FT Martins, Miguel B Almeida, and Noah A Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Improving arabic dependency parsing with lexical and inflectional morphological features.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, SPMRL ’10. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5653" citStr="Marton et al., 2010" startWordPosition="849" endWordPosition="852">r of the low-rank model becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are parti</context>
</contexts>
<marker>Marton, Habash, Rambow, 2010</marker>
<rawString>Yuval Marton, Nizar Habash, and Owen Rambow. 2010. Improving arabic dependency parsing with lexical and inflectional morphological features. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, SPMRL ’10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Improving arabic dependency parsing with form-based and functional morphological features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="5675" citStr="Marton et al., 2011" startWordPosition="853" endWordPosition="856">el becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in p</context>
</contexts>
<marker>Marton, Habash, Rambow, 2011</marker>
<rawString>Yuval Marton, Nizar Habash, and Owen Rambow. 2011. Improving arabic dependency parsing with form-based and functional morphological features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="2340" citStr="McDonald et al., 2005" startWordPosition="339" endWordPosition="343">nd their cross products, can be further complemented with auxiliary information about words 1Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we </context>
<context position="4840" citStr="McDonald et al., 2005" startWordPosition="720" endWordPosition="723">his low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags. • By automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features. Unlike parsers such as MST, we can easily benefit from auxiliary information (e.g., word vectors) appended as features. We implement the low-rank factorization model in the context of first- and third-order dependency parsing. The model was evaluated on 14 languages, using dependency data from CoNLL 2008 and CoNLL 2006. We compare our results against the MST (McDonald et al., 2005a) and Turbo (Martins et al., 2013) parsers. The low-rank parser achieves average performance of 89.08% across 14 languages, compared to 88.73% for the Turbo parser, and 87.19% for MST. The power of the low-rank model becomes evident in the absence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representa</context>
<context position="12329" citStr="McDonald et al., 2005" startWordPosition="1992" endWordPosition="1995">composes into a sum of “local” scores for arcs. Specifically: S(x, y) = 1: s(h —* m) by E Y(x) h,m E y where h —* m is the head-modifier dependency arc in the tree y. Each y is understood as a collection of arcs h —* m where h and m index words in x.2 For example, x(h) is the word corresponding to h. We suppress the dependence on x whenever it is clear from context. For example, s(h —* m) can depend on x in complicated ways as discussed below. The predicted parse is obtained as yˆ = arg maxyEY(x) S(x, y). A key problem is how we parameterize the arc scores s(h —* m). Following the MST parser (McDonald et al., 2005a) we can define rich features characterizing each head-modifier arc, compiled into a sparse binary vector Oh,m E RL that depends on the sentence x as well as the chosen arc h —* m (again, we suppress the dependence on x). Based on this feature representation, we define the score of each arc as sθ(h —* m) = 2Note that in the case of high-order parsing, the sum S(x, y) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s(g → h → m). See (Martins et al., 2013) for a complete list of these structures. 1383 Unigram features: form form-p form-n lemma</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26650" citStr="McDonald et al. (2006)" startWordPosition="4542" endWordPosition="4545"> 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13) Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into fea</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="6730" citStr="Mikolov et al., 2013" startWordPosition="1018" endWordPosition="1021">rithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008; Turian et al., 2010; Dhillon et al., 2011; Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their combinations, can signif</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Nilsson</author>
<author>Pierre Nugues</author>
</authors>
<title>Automatic discovery of feature sets for dependency parsing.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling</booktitle>
<contexts>
<context position="6030" citStr="Nilsson and Nugues, 2010" startWordPosition="909" endWordPosition="912">ly leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector </context>
</contexts>
<marker>Nilsson, Nugues, 2010</marker>
<rawString>Peter Nilsson and Pierre Nugues. 2010. Automatic discovery of feature sets for dependency parsing. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨uls¸en Eryiit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26626" citStr="Nivre et al. (2006)" startWordPosition="4538" endWordPosition="4541"> (Zh13) Swedish 89.86 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13) Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as </context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryiit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<contexts>
<context position="7601" citStr="Nivre et al., 2007" startWordPosition="1146" endWordPosition="1149"> sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g., dependency arc). Because of this issue, Cirik and S¸ensoy (2013) used word vectors only as unigram features (without combinations) as part of a shift reduce parser (Nivre et al., 2007). The improvement on the overall parsing performance was marginal. Another application of word vectors is compositional vector grammar (Socher et al., 2013). While this method learns to map word combinations into vectors, it builds on existing word-level vector representations. In contrast, we represent words as vectors in a manner that is directly optimized for parsing. This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. Dimensionality Reduction Many machin</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine pruning for efficient multi-pass dependency parsing.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL ’12).</booktitle>
<contexts>
<context position="5890" citStr="Rush and Petrov, 2012" startWordPosition="889" endWordPosition="892"> if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distribute</context>
<context position="26762" citStr="Rush and Petrov (2012" startWordPosition="4562" endWordPosition="4565">.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its </context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander Rush and Slav Petrov. 2012a. Vine pruning for efficient multi-pass dependency parsing. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL ’12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine pruning for efficient multi-pass dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="5890" citStr="Rush and Petrov, 2012" startWordPosition="889" endWordPosition="892"> if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distribute</context>
<context position="26762" citStr="Rush and Petrov (2012" startWordPosition="4562" endWordPosition="4565">.55 (Ko10) Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its </context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander M Rush and Slav Petrov. 2012b. Vine pruning for efficient multi-pass dependency parsing. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7757" citStr="Socher et al., 2013" startWordPosition="1169" endWordPosition="1172">space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g., dependency arc). Because of this issue, Cirik and S¸ensoy (2013) used word vectors only as unigram features (without combinations) as part of a shift reduce parser (Nivre et al., 2007). The improvement on the overall parsing performance was marginal. Another application of word vectors is compositional vector grammar (Socher et al., 2013). While this method learns to map word combinations into vectors, it builds on existing word-level vector representations. In contrast, we represent words as vectors in a manner that is directly optimized for parsing. This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. Dimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-tas</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In ICML.</booktitle>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted low-rank approximations. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Jason Rennie</author>
<author>Tommi S Jaakkola</author>
</authors>
<title>Maximum-margin matrix factorization. In Advances in neural information processing systems.</title>
<date>2004</date>
<contexts>
<context position="8820" citStr="Srebro et al., 2004" startWordPosition="1330" endWordPosition="1333">e learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. Rather than assuming that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous no</context>
</contexts>
<marker>Srebro, Rennie, Jaakkola, 2004</marker>
<rawString>Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. 2004. Maximum-margin matrix factorization. In Advances in neural information processing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08. Association for Computational Linguistics.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tao</author>
<author>Xiaoming Yuan</author>
</authors>
<title>Recovering lowrank and sparse components of matrices from incomplete and noisy observations.</title>
<date>2011</date>
<journal>SIAM Journal on Optimization.</journal>
<contexts>
<context position="9033" citStr="Tao and Yuan, 2011" startWordPosition="1365" endWordPosition="1368">ng that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to av</context>
</contexts>
<marker>Tao, Yuan, 2011</marker>
<rawString>Min Tao and Xiaoming Yuan. 2011. Recovering lowrank and sparse components of matrices from incomplete and noisy observations. SIAM Journal on Optimization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6685" citStr="Turian et al., 2010" startWordPosition="1010" endWordPosition="1013">dard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008; Turian et al., 2010; Dhillon et al., 2011; Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as featu</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew E Waters</author>
<author>Aswin C Sankaranarayanan</author>
<author>Richard Baraniuk</author>
</authors>
<title>SpaRCS: Recovering lowrank and sparse matrices from compressive measurements.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="9230" citStr="Waters et al., 2011" startWordPosition="1399" endWordPosition="1402">terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parame</context>
</contexts>
<marker>Waters, Sankaranarayanan, Baraniuk, 2011</marker>
<rawString>Andrew E Waters, Aswin C Sankaranarayanan, and Richard Baraniuk. 2011. SpaRCS: Recovering lowrank and sparse matrices from compressive measurements. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5866" citStr="Zhang and McDonald, 2012" startWordPosition="885" endWordPosition="888">le the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant i</context>
<context position="26790" citStr="Zhang and McDonald (2012" startWordPosition="4566" endWordPosition="4569">87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We sho</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012a. Generalized higher-order dependency parsing with cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5866" citStr="Zhang and McDonald, 2012" startWordPosition="885" endWordPosition="888">le the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant i</context>
<context position="26790" citStr="Zhang and McDonald (2012" startWordPosition="4566" endWordPosition="4569">87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We sho</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012b. Generalized higher-order dependency parsing with cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang Kai Zhao</author>
<author>Ryan McDonald</author>
</authors>
<title>Online learning for inexact hypergraph search.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26816" citStr="Zhang et al. (2013)" startWordPosition="4571" endWordPosition="4574">.19 88.73 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter -y = 0.3. To remove the tensor in our model, we ran experiments with -y = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors Oh and Om. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our mo</context>
</contexts>
<marker>Zhang, Zhao, McDonald, 2013</marker>
<rawString>Hao Zhang, Liang Huang Kai Zhao, and Ryan McDonald. 2013. Online learning for inexact hypergraph search. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Zhang</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
<author>Amir Globerson</author>
</authors>
<title>Steps to excellence: Simple inference with refined scoring of dependency trees.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24131" citStr="Zhang et al., 2014" startWordPosition="4114" endWordPosition="4117"> datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our sθ(x, y) = (θ, φ(x, y)) scoring component. The decoding algorithm for the third-order parsing is based on (Zhang et al., 2014). For the Turbo parser, we directly compare with the recent published results in (Martins et al., 2013). For the MST parser, we train and test using the most recent version of the code.4 In addition, we implemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component. Features For the arc feature vector φh→m, we use the same set of feature templates as MST v0.5.1. For head/modifier vector φh and φm, we show the complete set of feature templates used by our model in Table 1. Finally, we use a similar set of feature temp</context>
</contexts>
<marker>Zhang, Lei, Barzilay, Jaakkola, Globerson, 2014</marker>
<rawString>Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola, and Amir Globerson. 2014. Steps to excellence: Simple inference with refined scoring of dependency trees. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianyi Zhou</author>
<author>Dacheng Tao</author>
</authors>
<title>Godec: Randomized low-rank &amp; sparse matrix decomposition in noisy case.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11).</booktitle>
<contexts>
<context position="9054" citStr="Zhou and Tao, 2011" startWordPosition="1369" endWordPosition="1372">er can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving 1382 generalization (Lee and Seung, 1999; Srebro et al., 2003; Srebro et al., 2004; Evgeniou and Pontil, 2007) A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in m</context>
</contexts>
<marker>Zhou, Tao, 2011</marker>
<rawString>Tianyi Zhou and Dacheng Tao. 2011. Godec: Randomized low-rank &amp; sparse matrix decomposition in noisy case. In Proceedings of the 28th International Conference on Machine Learning (ICML-11).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>