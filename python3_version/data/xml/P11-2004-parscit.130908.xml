<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.983331">
Efficient Online Locality Sensitive Hashing via Reservoir Counting
</title>
<author confidence="0.975457">
Benjamin Van Durme Ashwin Lall
</author>
<affiliation confidence="0.9608825">
HLTCOE Mathematics and Computer Science
Johns Hopkins University Denison University
</affiliation>
<figure confidence="0.797087">
Abstract cos(ii,v) = ~u·~v Cos(
|~u||~v |cos ~v* ),
b
</figure>
<bodyText confidence="0.999427571428571">
We describe a novel mechanism called Reser-
voir Counting for application in online Local-
ity Sensitive Hashing. This technique allows
for significant savings in the streaming setting,
allowing for maintaining a larger number of
signatures, or an increased level of approxima-
tion accuracy at a similar memory footprint.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939388888889">
Feature vectors based on lexical co-occurrence are
often of a high dimension, d. This leads to O(d) op-
erations to calculate cosine similarity, a fundamental
tool in distributional semantics. This is improved in
practice through the use of data structures that ex-
ploit feature sparsity, leading to an expected O(f)
operations, where f is the number of unique features
we expect to have non-zero entries in a given vector.
Ravichandran et al. (2005) showed that the Lo-
cality Sensitive Hash (LSH) procedure of Charikar
(2002), following from Indyk and Motwani (1998)
and Goemans and Williamson (1995), could be suc-
cessfully used to compress textually derived fea-
ture vectors in order to achieve speed efficiencies
in large-scale noun clustering. Such LSH bit signa-
tures are constructed using the following hash func-
tion, where v E Rd is a vector in the original feature
space, and r is randomly drawn from N(0, 1)d:
</bodyText>
<equation confidence="0.915789">
� 1 if �� · �� &gt; 0,
h(��) =
0 otherwise.
</equation>
<bodyText confidence="0.987123">
If hb(v) is the b-bit signature resulting from b such
hash functions, then the cosine similarity between
vectors i and v is approximated by:
</bodyText>
<page confidence="0.987156">
18
</page>
<bodyText confidence="0.9998815">
where D(·, ·) is Hamming distance, the number of
bits that disagree. This technique is used when
b « d, which leads to faster pair-wise comparisons
between vectors, and a lower memory footprint.
Van Durme and Lall (2010) observed1 that if
the feature values are additive over a dataset (e.g.,
when collecting word co-occurrence frequencies),
then these signatures may be constructed online by
unrolling the dot-product into a series of local oper-
ations: v ·ri = Σtvt ·fi, where vt represents features
observed locally at time t in a data-stream.
Since updates may be done locally, feature vec-
tors do not need to be stored explicitly. This di-
rectly leads to significant space savings, as only one
counter is needed for each of the b running sums.
In this work we focus on the following observa-
tion: the counters used to store the running sums
may themselves be an inefficient use of space, in
that they may be amenable to compression through
approximation.2 Since the accuracy of this LSH rou-
tine is a function of b, then if we were able to reduce
the online requirements of each counter, we might
afford a larger number of projections. Even if a
chance of approximation error were introduced for
each hash function, this may be justified in greater
overall fidelity from the resultant increase in b.
</bodyText>
<footnote confidence="0.937635142857143">
1A related point was made by Li et al. (2008) when dis-
cussing stable random projections.
2A b bit signature requires the online storage of b*32 bits of
memory when assuming a 32-bit floating point representation
per counter, but since here the only thing one cares about these
sums are their sign (positive or negative) then an approximation
to the true sum may be sufficient.
</footnote>
<note confidence="0.83352">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 18–23,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958571428571">
Thus, we propose to approximate the online hash
function, using a novel technique we call Reservoir
Counting, in order to create a space trade-off be-
tween the number of projections and the amount of
memory each projection requires. We show experi-
mentally that this leads to greater accuracy approx-
imations at the same memory cost, or similar accu-
racy approximations at a significantly reduced cost.
This result is relevant to work in large-scale distribu-
tional semantics (Bhagat and Ravichandran, 2008;
Van Durme and Lall, 2009; Pantel et al., 2009; Lin
et al., 2010; Goyal et al., 2010; Bergsma and Van
Durme, 2011), as well as large-scale processing of
social media (Petrovic et al., 2010).
</bodyText>
<sectionHeader confidence="0.988821" genericHeader="keywords">
2 Approach
</sectionHeader>
<bodyText confidence="0.987207954545455">
While not strictly required, we assume here to be
dealing exclusively with integer-valued features. We
then employ an integer-valued projection matrix in
order to work with an integer-valued stream of on-
line updates, which is reduced (implicitly) to a
stream of positive and negative unit updates. The
sign of the sum of these updates is approximated
through a novel twist on Reservoir Sampling. When
computed explicitly this leads to an impractical
mechanism linear in each feature value update. To
ensure our counter can (approximately) add and sub-
tract in constant time, we then derive expressions for
the expected value of each step of the update. The
full algorithms are provided at the close.
Unit Projection Rather than construct a projec-
tion matrix from N(0,1), a matrix randomly pop-
ulated with entries from the set {−1, 0,1} will suf-
fice, with quality dependent on the relative propor-
tion of these elements. If we let p be the percent
probability mass allocated to zeros, then we create
a discrete projection matrix by sampling from the
multinomial: (1�P
</bodyText>
<equation confidence="0.951121">
2 : −1,p : 0, 1�P
2 : +1). An
</equation>
<bodyText confidence="0.9993428">
experiment displaying the resultant quality is dis-
played in Fig. 1, for varied p. Henceforth we assume
this discrete projection matrix, with p = 0.5.3 The
use of such sparse projections was first proposed by
Achlioptas (2003), then extended by Li et al. (2006).
</bodyText>
<footnote confidence="0.98828">
3Note that if using the pooling trick of Van Durme and Lall
(2010), this equates to a pool of the form: (-1,0,0,1).
</footnote>
<figure confidence="0.9760912">
0.5
0.4
Mean.Absolute.Error 0.3
0.2
0.1
</figure>
<figureCaption confidence="0.936165333333333">
Figure 1: With b = 256, mean absolute error in cosine
approximation when using a projection based on N(0,1),
compared to {−1, 0, 1}.
</figureCaption>
<bodyText confidence="0.99922292">
Unit Stream Based on a unit projection, we can
view an online counter as summing over a stream
drawn from {−1,1}: each projected feature value
unrolled into its (positive or negative) unary repre-
sentation. For example, the stream: (3,-2,1), can be
viewed as the updates: (1,1,1,-1,-1,1).
Reservoir Sampling We can maintain a uniform
sample of size k over a stream of unknown length
as follows. Accept the first k elements into an reser-
voir (array) of size k. Each following element at po-
sition n is accepted with probability n, whereupon
an element currently in the reservoir is evicted, and
replaced with the just accepted item. This scheme
is guaranteed to provide a uniform sample, where
early items are more likely to be accepted, but also at
greater risk of eviction. Reservoir sampling is a folk-
lore algorithm that was extended by Vitter (1985) to
allow for multiple updates.
Reservoir Counting If we are sampling over a
stream drawn from just two values, we can implic-
itly represent the reservoir by counting only the fre-
quency of one or the other elements.4 We can there-
fore sample the proportion of positive and negative
unit values by tracking the current position in the
stream, n, and keeping a lo92(k + 1)-bit integer
</bodyText>
<footnote confidence="0.8675395">
4For example, if we have a reservoir of size 5, containing
three values of −1, and two values of 1, then the exchangeabil-
ity of the elements means the reservoir is fully characterized by
knowing k, and that there are two 1’s.
</footnote>
<figure confidence="0.991348">
0.2 0.4 0.6 0.8 1.0
Method
Discrete
Normal
Percent.Zeros
</figure>
<page confidence="0.990615">
19
</page>
<bodyText confidence="0.9998375">
counter, s, for tracking the number of 1 values cur-
rently in the reservoir.5 When a negative value is
accepted, we decrement the counter with probability
k. When a positive update is accepted, we increment
</bodyText>
<subsectionHeader confidence="0.524612">
s
</subsectionHeader>
<bodyText confidence="0.999892525">
the counter with probability (1− sk). This reflects an
update evicting either an element of the same sign,
which has no effect on the makeup of the reservoir,
or decreasing/increasing the number of 1’s currently
sampled. An approximate sum of all values seen up
to position n is then simply: n(�sk − 1). While this
value is potentially interesting in future applications,
here we are only concerned with its sign.
Parallel Reservoir Counting On its own this
counting mechanism hardly appears useful: as it is
dependent on knowing n, then we might just as well
sum the elements of the stream directly, counting in
whatever space we would otherwise use in maintain-
ing the value of n. However, if we have a set of tied
streams that we process in parallel,6 then we only
need to track n once, across b different streams, each
with their own reservoir.
When dealing with parallel streams resulting from
different random projections of the same vector, we
cannot assume these will be strictly tied. Some pro-
jections will cancel out heavier elements than oth-
ers, leading to update streams of different lengths
once elements are unrolled into their (positive or
negative) unary representation. In practice we have
found that tracking the mean value of n across b
streams is sufficient. When using a p = 0.5 zeroed
matrix, we can update n by one half the magnitude
of each observed value, as on average half the pro-
jections will cancel out any given element. This step
can be found in Algorithm 2, lines 8 and 9.
Example To make concrete what we have cov-
ered to this point, consider a given feature vec-
tor of dimensionality d = 3, say: [3, 2, 1]. This
might be projected into b = 4, vectors: [3, 0, 0],
[0, -2, 1], [0, 0, 1], and [-3, 2, 0]. When viewed as
positive/negative, loosely-tied unit streams, they re-
spectively have length n: 3, 3, 1, and 5, with mean
length 3. The goal of reservoir counting is to effi-
ciently keep track of an approximation of their sums
(here: 3, -1, 1, and -1), while the underlying feature
</bodyText>
<footnote confidence="0.990827333333333">
5E.g., a reservoir of size k = 255 requires an 8-bit integer.
6Tied in the sense that each stream is of the same length,
e.g., (-1,1,1) is the same length as (1,-1,-1).
</footnote>
<table confidence="0.999819166666667">
k n m mean(A) mean(A&apos;)
10 20 10 3.80 4.02
10 20 1000 37.96 39.31
50 150 1000 101.30 101.83
100 1100 100 8.88 8.72
100 10100 10 0.13 0.10
</table>
<tableCaption confidence="0.999921">
Table 1: Average over repeated calls to A and A&apos;.
</tableCaption>
<bodyText confidence="0.9991085">
vector is being updated online. A k = 3 reservoir
used for the last projected vector, [-3, 2, 0], might
reasonably contain two values of -1, and one value
of 1.7 Represented explicitly as a vector, the reser-
voir would thus be in the arrangement:
[1, -1, -1], [-1, 1, -1], or [-1, -1, 1].
These are functionally equivalent: we only need to
know that one of the k = 3 elements is positive.
Expected Number of Samples Traversing m con-
secutive values of either 1 or −1 in the unit stream
should be thought of as seeing positive or negative
m as a feature update. For a reservoir of size k, let
A(m, n, k) be the number of samples accepted when
traversing the stream from position n + 1 to n + m.
A is non-deterministic: it represents the results of
flipping m consecutive coins, where each coin is in-
creasingly biased towards rejection.
Rather than computing A explicitly, which is lin-
ear in m, we will instead use the expected number of
updates, A&apos;(m, n, k) = E[A(m, n, k)], which can
be computed in constant time. Where H(x) is the
harmonic number of x:8
</bodyText>
<equation confidence="0.9936912">
A&apos;(m, n, k) =
= k(H(n + m) − H(n))
^ k loge(n + m
).
n
</equation>
<bodyText confidence="0.981661666666667">
For example, consider m = 30, encountered at
position n = 100, with a reservoir of k = 10. We
will then accept 10 loge (130) ^ 3.79 samples of 1.
</bodyText>
<page confidence="0.354484">
100
</page>
<bodyText confidence="0.97591975">
As the reservoir is a discrete set of bins, fractional
portions of a sample are resolved by a coin flip: if
a = k loge(n+mn ), then accept u = ra] samples
with probability (a − La]), and u = La] samples
</bodyText>
<footnote confidence="0.800531666666667">
7Other options are: three -1’s, or one -1 and two 1’s.
8With x a positive integer, H(x) = EL, 1/x Pz� loge(x)+
-y, where -y is Euler’s constant.
</footnote>
<equation confidence="0.920700333333333">
n+m k
i
i=n+1
</equation>
<page confidence="0.967921">
20
</page>
<bodyText confidence="0.999326857142857">
otherwise. These steps are found in lines 3 and 4
of Algorithm 1. See Table 1 for simulation results
using a variety of parameters.
Expected Reservoir Change We now discuss
how to simulate many independent updates of the
same type to the reservoir counter, e.g.: five updates
of 1, or three updates of -1, using a single estimate.
Consider a situation in which we have a reservoir of
size k with some current value of s, 0 &lt; s &lt; k, and
we wish to perform u independent updates. We de-
note by U0�(s, u) the expected value of the reservoir
after these u updates have taken place. Since a sin-
gle update leads to no change with probability k, we
can write the following recurrence for U0�:
</bodyText>
<equation confidence="0.99974">
U0�(s,u) = s k U0�(s,u−1)+k −s
k U0 �(s+1,u − 1),
</equation>
<bodyText confidence="0.999440666666667">
with the boundary condition: for all s, U0�(s, 0) = s.
Solving the above recurrence, we get that the ex-
pected value of the reservoir after these updates is:
</bodyText>
<equation confidence="0.9883">
Uk (s, u) = k + (s − k) (1 − ,
</equation>
<bodyText confidence="0.973655875">
which can be mechanically checked via induction.
The case for negative updates follows similarly (see
lines 7 and 8 of Algorithm 1).
Hence, instead of simulating u independent up-
dates of the same type to the reservoir, we simply
update it to this expected value, where fractional up-
dates are handled similarly as when estimating the
number of accepts. These steps are found in lines 5
through 9 of Algorithm 1, and as seen in Fig. 2, this
can give a tight estimate.
Comparison Simulation results over Zipfian dis-
tributed data can be seen in Fig. 3, which shows the
use of reservoir counting in Online Locality Sensi-
tive Hashing (as made explicit in Algorithm 2), as
compared to the method described by Van Durme
and Lall (2010).
The total amount of space required when using
this counting scheme is b log2(k + 1) + 32: b reser-
voirs, and a 32 bit integer to track n. This is com-
pared to b 32 bit floating point values, as is standard.
Note that our scheme comes away with similar lev-
els of accuracy, often at half the memory cost, while
requiring larger b to account for the chance of ap-
proximation errors in individual reservoir counters.
</bodyText>
<figure confidence="0.977095571428572">
250
200
150
100
50
50 100 150 200 250
Expected
</figure>
<figureCaption confidence="0.9520235">
Figure 2: Results of simulating many iterations of U&apos;,
for k = 255, and various values of s and u.
</figureCaption>
<equation confidence="0.909987875">
Algorithm 1 RESERVOIRUPDATE(n, k, m, σ, s)
Parameters:
n : size of stream so far
k : size of reservoir, also maximum value of s
m : magnitude of update
σ : sign of update
s : current value of reservoir
1: if m = 0 or σ = 0 then
</equation>
<listItem confidence="0.99341675">
2: Return without doing anything
)3: a := A&apos;(m, n, k) = k log, �n+M n
4: u := ra] with probability a − La], La] otherwise
5: if σ = 1 then
6: s&apos; := U&apos;(s, a) = k + (s − k) (1 − 1/k)&apos;
7: else
8: s&apos; := U&apos;(s, a) = s (1 − 1/k)&apos;
9: Return rs&apos;] with probability s&apos; −Ls&apos;], Ls&apos;] otherwise
</listItem>
<table confidence="0.974995428571429">
● ●● ● ● log2.k 8
● ● 32
● ●
● b 128
64
● ● ●
● 192
</table>
<page confidence="0.409598">
256
512
</page>
<figure confidence="0.880626">
1000 2000 3000 4000 5000 6000 7000 8000
Bits.Required
</figure>
<figureCaption confidence="0.994107">
Figure 3: Online LSH using reservoir counting (red) vs.
standard counting mechanisms (blue), as measured by the
amount of total memory required to the resultant error.
</figureCaption>
<figure confidence="0.998578">
11&apos;�
k /I
True
0.12
1
0.1
Mean.Absolute.Error
0.10
0.09
0.08
0.07
0.06
</figure>
<page confidence="0.731818">
21
</page>
<figure confidence="0.807678833333333">
Algorithm 2 COMPUTESIGNATURE(S,k,b,p)
Parameters:
S : bit array of size b
k : size of each reservoir
b : number of projections
p : percentage of zeros in projection, p E [0, 1]
</figure>
<listItem confidence="0.633679473684211">
1: Initialize b reservoirs R[1, ... , b], each represented
by a loge(k + 1)-bit unsigned integer
2: Initialize b hash functions hi(w) that map features w
to elements in a vector made up of −1 and 1 each
with proportion 1�p
e , and 0 at proportion p.
3: n := 0
4: {Processing the stream}
5: for each feature value pair (w, m) in stream do
6: for i := 1 to b do
7: R[i] := ReservoirUpdate(n, k, m, hi(w), R[i])
8: n := n + Lm(1 − p)j
9: n := n+1 with probability m(1−p)−Lm(1−p)j
10: {Post-processing to compute signature}
11: for i := 1 ... b do
12: if R[i] &gt; 2 then
13: S[i] := 1
14: else
15: S[i] := 0
</listItem>
<sectionHeader confidence="0.962123" genericHeader="introduction">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999665142857143">
Time and Space While we have provided a con-
stant time, approximate update mechanism, the con-
stants involved will practically remain larger than
the cost of performing single hardware addition
or subtraction operations on a traditional 32-bit
counter. This leads to a tradeoff in space vs. time,
where a high-throughput streaming application that
is not concerned with online memory requirements
will not have reason to consider the developments in
this article. The approach given here is motivated
by cases where data is not flooding in at breakneck
speed, and resource considerations are dominated by
a large number of unique elements for which we
are maintaining signatures. Empirically investigat-
ing this tradeoff is a matter of future work.
Random Walks As we here only care for the sign
of the online sum, rather than an approximation of
its actual value, then it is reasonable to consider in-
stead modeling the problem directly as a random
walk on a linear Markov chain, with unit updates
directly corresponding to forward or backward state
</bodyText>
<figureCaption confidence="0.9531554">
Figure 4: A simple 8-state Markov chain, requiring
lg(8) = 3 bits. Dark or light states correspond to a
prediction of a running sum being positive or negative.
States are numerically labeled to reflect the similarity to
a small bit integer data type, one that never overflows.
</figureCaption>
<bodyText confidence="0.999943476190476">
transitions. Assuming a fixed probability of a posi-
tive versus negative update, then in expectation the
state of the chain should correspond to the sign.
However if we are concerned with the global statis-
tic, as we are here, then the assumption of a fixed
probability update precludes the analysis of stream-
ing sources that contain local irregularities.9
In distributional semantics, consider a feature
stream formed by sequentially reading the n-gram
resource of Brants and Franz (2006). The pair: (the
dog : 3,502,485), can be viewed as a feature value
pair: (leftWord=’the’ : 3,502,485), with respect to
online signature generation for the word dog. Rather
than viewing this feature repeatedly, spread over a
large corpus, the update happens just once, with
large magnitude. A simple chain such as seen in
Fig. 4 will be “pushed” completely to the right or
the left, based on the polarity of the projection, irre-
spective of previously observed updates. Reservoir
Counting, representing an online uniform sample, is
agnostic to the ordering of elements in the stream.
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999992142857143">
We have presented a novel approximation scheme
we call Reservoir Counting, motivated here by a de-
sire for greater space efficiency in Online Locality
Sensitive Hashing. Going beyond our results pro-
vided for synthetic data, future work will explore ap-
plications of this technique, such as in experiments
with streaming social media like Twitter.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.973876">
This work benefited from conversations with Daniel
ˇStefonkoviˇc and Damianos Karakos.
</bodyText>
<footnote confidence="0.8636155">
9For instance: (1,1,...,1,1,-1,-1,-1), is overall positive, but
locally negative at the end.
</footnote>
<note confidence="0.280391">
-4 -3 -2 -1 0 1 2 3
</note>
<page confidence="0.996308">
22
</page>
<sectionHeader confidence="0.982953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878194029851">
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-lindenstrauss with binary coins.
J. Comput. Syst. Sci., 66:671–687, June.
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing Bilingual Lexicons using the Visual Similarity of
Labeled Web Images. In Proc. of the International
Joint Conference on Artificial Intelligence (IJCAI).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Surface
Patterns. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite pro-
gramming. JACM, 42:1115–1145.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and
Suresh Venkatasubramanian. 2010. Sketch Tech-
niques for Scaling Distributional Similarity to the
Web. In Proceedings of the ACL Workshop on GEo-
metrical Models of Natural Language Semantics.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections. In Proceedings of
the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’06,
pages 287–296, New York, NY, USA. ACM.
Ping Li, Kenneth W. Church, and Trevor J. Hastie. 2008.
One Sketch For All: Theory and Application of Con-
ditional Random Sampling. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-Scale
Distributional Similarity and Entity Set Expansion. In
Proc. of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with applica-
tion to Twitter. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Benjamin Van Durme and Ashwin Lall. 2009. Streaming
Pointwise Mutual Information. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Benjamin Van Durme and Ashwin Lall. 2010. Online
Generation of Locality Sensitive Hash Signatures. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37–57, March.
</reference>
<page confidence="0.998937">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803565">
<title confidence="0.999964">Efficient Online Locality Sensitive Hashing via Reservoir Counting</title>
<author confidence="0.999661">Benjamin Van_Durme Ashwin Lall</author>
<affiliation confidence="0.9952255">HLTCOE Mathematics and Computer Science Johns Hopkins University Denison University</affiliation>
<abstract confidence="0.974121125">Cos( describe a novel mechanism called Reser- Counting application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections: Johnson-lindenstrauss with binary coins.</title>
<date>2003</date>
<journal>J. Comput. Syst. Sci.,</journal>
<pages>66--671</pages>
<contexts>
<context position="5565" citStr="Achlioptas (2003)" startWordPosition="920" endWordPosition="921">t Projection Rather than construct a projection matrix from N(0,1), a matrix randomly populated with entries from the set {−1, 0,1} will suffice, with quality dependent on the relative proportion of these elements. If we let p be the percent probability mass allocated to zeros, then we create a discrete projection matrix by sampling from the multinomial: (1�P 2 : −1,p : 0, 1�P 2 : +1). An experiment displaying the resultant quality is displayed in Fig. 1, for varied p. Henceforth we assume this discrete projection matrix, with p = 0.5.3 The use of such sparse projections was first proposed by Achlioptas (2003), then extended by Li et al. (2006). 3Note that if using the pooling trick of Van Durme and Lall (2010), this equates to a pool of the form: (-1,0,0,1). 0.5 0.4 Mean.Absolute.Error 0.3 0.2 0.1 Figure 1: With b = 256, mean absolute error in cosine approximation when using a projection based on N(0,1), compared to {−1, 0, 1}. Unit Stream Based on a unit projection, we can view an online counter as summing over a stream drawn from {−1,1}: each projected feature value unrolled into its (positive or negative) unary representation. For example, the stream: (3,-2,1), can be viewed as the updates: (1,</context>
</contexts>
<marker>Achlioptas, 2003</marker>
<rawString>Dimitris Achlioptas. 2003. Database-friendly random projections: Johnson-lindenstrauss with binary coins. J. Comput. Syst. Sci., 66:671–687, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning Bilingual Lexicons using the Visual Similarity of Labeled Web Images.</title>
<date>2011</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<marker>Bergsma, Van Durme, 2011</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2011. Learning Bilingual Lexicons using the Visual Similarity of Labeled Web Images. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large Scale Acquisition of Paraphrases for Learning Surface Patterns.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4043" citStr="Bhagat and Ravichandran, 2008" startWordPosition="660" endWordPosition="663">onal Linguistics:shortpapers, pages 18–23, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). 2 Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of positive and negative unit updates. The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampling. W</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large Scale Acquisition of Paraphrases for Learning Surface Patterns. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1.</title>
<date>2006</date>
<contexts>
<context position="17282" citStr="Brants and Franz (2006)" startWordPosition="3094" endWordPosition="3097">itive or negative. States are numerically labeled to reflect the similarity to a small bit integer data type, one that never overflows. transitions. Assuming a fixed probability of a positive versus negative update, then in expectation the state of the chain should correspond to the sign. However if we are concerned with the global statistic, as we are here, then the assumption of a fixed probability update precludes the analysis of streaming sources that contain local irregularities.9 In distributional semantics, consider a feature stream formed by sequentially reading the n-gram resource of Brants and Franz (2006). The pair: (the dog : 3,502,485), can be viewed as a feature value pair: (leftWord=’the’ : 3,502,485), with respect to online signature generation for the word dog. Rather than viewing this feature repeatedly, spread over a large corpus, the update happens just once, with large magnitude. A simple chain such as seen in Fig. 4 will be “pushed” completely to the right or the left, based on the polarity of the projection, irrespective of previously observed updates. Reservoir Counting, representing an online uniform sample, is agnostic to the ordering of elements in the stream. 4 Conclusion We h</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="1090" citStr="Charikar (2002)" startWordPosition="166" endWordPosition="167">r an increased level of approximation accuracy at a similar memory footprint. 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f) operations, where f is the number of unique features we expect to have non-zero entries in a given vector. Ravichandran et al. (2005) showed that the Locality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signatures are constructed using the following hash function, where v E Rd is a vector in the original feature space, and r is randomly drawn from N(0, 1)d: � 1 if �� · �� &gt; 0, h(��) = 0 otherwise. If hb(v) is the b-bit signature resulting from b such hash functions, then the cosine similarity between vectors i and v is approximated by: 18 where D(·, ·) is Hamming</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel X Goemans</author>
<author>David P Williamson</author>
</authors>
<title>Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming.</title>
<date>1995</date>
<pages>42--1115</pages>
<publisher>JACM,</publisher>
<contexts>
<context position="1165" citStr="Goemans and Williamson (1995)" startWordPosition="175" endWordPosition="178">emory footprint. 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f) operations, where f is the number of unique features we expect to have non-zero entries in a given vector. Ravichandran et al. (2005) showed that the Locality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signatures are constructed using the following hash function, where v E Rd is a vector in the original feature space, and r is randomly drawn from N(0, 1)d: � 1 if �� · �� &gt; 0, h(��) = 0 otherwise. If hb(v) is the b-bit signature resulting from b such hash functions, then the cosine similarity between vectors i and v is approximated by: 18 where D(·, ·) is Hamming distance, the number of bits that disagree. This technique is used when b </context>
</contexts>
<marker>Goemans, Williamson, 1995</marker>
<rawString>Michel X. Goemans and David P. Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. JACM, 42:1115–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Sketch Techniques for Scaling Distributional Similarity to the Web.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL Workshop on GEometrical Models of Natural Language Semantics.</booktitle>
<marker>Goyal, Jagarlamudi, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and Suresh Venkatasubramanian. 2010. Sketch Techniques for Scaling Distributional Similarity to the Web. In Proceedings of the ACL Workshop on GEometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="1131" citStr="Indyk and Motwani (1998)" startWordPosition="170" endWordPosition="173">ation accuracy at a similar memory footprint. 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f) operations, where f is the number of unique features we expect to have non-zero entries in a given vector. Ravichandran et al. (2005) showed that the Locality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signatures are constructed using the following hash function, where v E Rd is a vector in the original feature space, and r is randomly drawn from N(0, 1)d: � 1 if �� · �� &gt; 0, h(��) = 0 otherwise. If hb(v) is the b-bit signature resulting from b such hash functions, then the cosine similarity between vectors i and v is approximated by: 18 where D(·, ·) is Hamming distance, the number of bits that disagr</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Trevor J Hastie</author>
<author>Kenneth W Church</author>
</authors>
<title>Very sparse random projections.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>287--296</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5600" citStr="Li et al. (2006)" startWordPosition="925" endWordPosition="928"> projection matrix from N(0,1), a matrix randomly populated with entries from the set {−1, 0,1} will suffice, with quality dependent on the relative proportion of these elements. If we let p be the percent probability mass allocated to zeros, then we create a discrete projection matrix by sampling from the multinomial: (1�P 2 : −1,p : 0, 1�P 2 : +1). An experiment displaying the resultant quality is displayed in Fig. 1, for varied p. Henceforth we assume this discrete projection matrix, with p = 0.5.3 The use of such sparse projections was first proposed by Achlioptas (2003), then extended by Li et al. (2006). 3Note that if using the pooling trick of Van Durme and Lall (2010), this equates to a pool of the form: (-1,0,0,1). 0.5 0.4 Mean.Absolute.Error 0.3 0.2 0.1 Figure 1: With b = 256, mean absolute error in cosine approximation when using a projection based on N(0,1), compared to {−1, 0, 1}. Unit Stream Based on a unit projection, we can view an online counter as summing over a stream drawn from {−1,1}: each projected feature value unrolled into its (positive or negative) unary representation. For example, the stream: (3,-2,1), can be viewed as the updates: (1,1,1,-1,-1,1). Reservoir Sampling We</context>
</contexts>
<marker>Li, Hastie, Church, 2006</marker>
<rawString>Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 287–296, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth W Church</author>
<author>Trevor J Hastie</author>
</authors>
<title>One Sketch For All: Theory and Application of Conditional Random Sampling.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="3011" citStr="Li et al. (2008)" startWordPosition="501" endWordPosition="504"> b running sums. In this work we focus on the following observation: the counters used to store the running sums may themselves be an inefficient use of space, in that they may be amenable to compression through approximation.2 Since the accuracy of this LSH routine is a function of b, then if we were able to reduce the online requirements of each counter, we might afford a larger number of projections. Even if a chance of approximation error were introduced for each hash function, this may be justified in greater overall fidelity from the resultant increase in b. 1A related point was made by Li et al. (2008) when discussing stable random projections. 2A b bit signature requires the online storage of b*32 bits of memory when assuming a 32-bit floating point representation per counter, but since here the only thing one cares about these sums are their sign (positive or negative) then an approximation to the true sum may be sufficient. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 18–23, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Thus, we propose to approximate the online hash function, using a nov</context>
</contexts>
<marker>Li, Church, Hastie, 2008</marker>
<rawString>Ping Li, Kenneth W. Church, and Trevor J. Hastie. 2008. One Sketch For All: Theory and Application of Conditional Random Sampling. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New Tools for Web-Scale N-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="4108" citStr="Lin et al., 2010" startWordPosition="673" endWordPosition="676"> c�2011 Association for Computational Linguistics Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). 2 Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of positive and negative unit updates. The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampling. When computed explicitly this leads to an impractical mechanism li</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New Tools for Web-Scale N-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-Scale Distributional Similarity and Entity Set Expansion. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming First Story Detection with application to Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="4233" citStr="Petrovic et al., 2010" startWordPosition="694" endWordPosition="697">el technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). 2 Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of positive and negative unit updates. The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampling. When computed explicitly this leads to an impractical mechanism linear in each feature value update. To ensure our counter can (approximately) add and subtract in constant time, we then deriv</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming First Story Detection with application to Twitter. In Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1015" citStr="Ravichandran et al. (2005)" startWordPosition="152" endWordPosition="155">gs in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint. 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f) operations, where f is the number of unique features we expect to have non-zero entries in a given vector. Ravichandran et al. (2005) showed that the Locality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signatures are constructed using the following hash function, where v E Rd is a vector in the original feature space, and r is randomly drawn from N(0, 1)d: � 1 if �� · �� &gt; 0, h(��) = 0 otherwise. If hb(v) is the b-bit signature resulting from b such hash functions, then the cosine similar</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming Pointwise Mutual Information.</title>
<date>2009</date>
<booktitle>In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009. Streaming Pointwise Mutual Information. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online Generation of Locality Sensitive Hash Signatures.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online Generation of Locality Sensitive Hash Signatures. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey S Vitter</author>
</authors>
<title>Random sampling with a reservoir.</title>
<date>1985</date>
<journal>ACM Trans. Math. Softw.,</journal>
<pages>11--37</pages>
<contexts>
<context position="6741" citStr="Vitter (1985)" startWordPosition="1123" endWordPosition="1124">1), can be viewed as the updates: (1,1,1,-1,-1,1). Reservoir Sampling We can maintain a uniform sample of size k over a stream of unknown length as follows. Accept the first k elements into an reservoir (array) of size k. Each following element at position n is accepted with probability n, whereupon an element currently in the reservoir is evicted, and replaced with the just accepted item. This scheme is guaranteed to provide a uniform sample, where early items are more likely to be accepted, but also at greater risk of eviction. Reservoir sampling is a folklore algorithm that was extended by Vitter (1985) to allow for multiple updates. Reservoir Counting If we are sampling over a stream drawn from just two values, we can implicitly represent the reservoir by counting only the frequency of one or the other elements.4 We can therefore sample the proportion of positive and negative unit values by tracking the current position in the stream, n, and keeping a lo92(k + 1)-bit integer 4For example, if we have a reservoir of size 5, containing three values of −1, and two values of 1, then the exchangeability of the elements means the reservoir is fully characterized by knowing k, and that there are tw</context>
</contexts>
<marker>Vitter, 1985</marker>
<rawString>Jeffrey S. Vitter. 1985. Random sampling with a reservoir. ACM Trans. Math. Softw., 11:37–57, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>