<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003159">
<title confidence="0.996962">
Hierarchical Reinforcement Learning and Hidden Markov Models for
Task-Oriented Natural Language Generation
</title>
<author confidence="0.986437">
Nina Dethlefs Heriberto Cuay´ahuitl
</author>
<affiliation confidence="0.9718605">
Department of Linguistics, German Research Centre for Artificial Intelligence
University of Bremen (DFKI), Saarbr¨ucken
</affiliation>
<email confidence="0.994259">
dethlefs@uni-bremen.de heriberto.cuayahuitl@dfki.de
</email>
<sectionHeader confidence="0.993721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997557428571429">
Surface realisation decisions in language gen-
eration can be sensitive to a language model,
but also to decisions of content selection. We
therefore propose the joint optimisation of
content selection and surface realisation using
Hierarchical Reinforcement Learning (HRL).
To this end, we suggest a novel reward func-
tion that is induced from human data and is
especially suited for surface realisation. It is
based on a generation space in the form of
a Hidden Markov Model (HMM). Results in
terms of task success and human-likeness sug-
gest that our unified approach performs better
than greedy or random baselines.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908290909091">
Surface realisation decisions in a Natural Language
Generation (NLG) system are often made accord-
ing to a language model of the domain (Langkilde
and Knight, 1998; Bangalore and Rambow, 2000;
Oh and Rudnicky, 2000; White, 2004; Belz, 2008).
However, there are other linguistic phenomena, such
as alignment (Pickering and Garrod, 2004), consis-
tency (Halliday and Hasan, 1976), and variation,
which influence people’s assessment of discourse
(Levelt and Kelter, 1982) and generated output (Belz
and Reiter, 2006; Foster and Oberlander, 2006).
Also, in dialogue the most likely surface form may
not always be appropriate, because it does not cor-
respond to the user’s information need, the user is
confused, or the most likely sequence is infelicitous
with respect to the dialogue history. In such cases, it
is important to optimise surface realisation in a uni-
fied fashion with content selection. We suggest to
use Hierarchical Reinforcement Learning (HRL) to
achieve this. Reinforcement Learning (RL) is an at-
tractive framework for optimising a sequence of de-
cisions given incomplete knowledge of the environ-
ment or best strategy to follow (Rieser et al., 2010;
Janarthanam and Lemon, 2010). HRL has the ad-
ditional advantage of scaling to large and complex
problems (Dethlefs and Cuay´ahuitl, 2010). Since
an HRL agent will ultimately learn the behaviour
it is rewarded for, the reward function is arguably
the agent’s most crucial component. Previous work
has therefore suggested to learn a reward function
from human data as in the PARADISE framework
(Walker et al., 1997). Since PARADISE-based re-
ward functions typically rely on objective metrics,
they are not ideally suited for surface realisation,
which is more dependend on linguistic phenomena,
e.g. frequency, consistency, and variation. However,
linguistic and psychological studies (cited above)
show that such phenomena are indeed worth mod-
elling in an NLG system. The contribution of this
paper is therefore to induce a reward function from
human data, specifically suited for surface genera-
tion. To this end, we train HMMs (Rabiner, 1989)
on a corpus of grammatical word sequences and use
them to inform the agent’s learning process. In addi-
tion, we suggest to optimise surface realisation and
content selection decisions in a joint, rather than iso-
lated, fashion. Results show that our combined ap-
proach generates more successful and human-like
utterances than a greedy or random baseline. This is
related to Angeli et al. (2010), who also address in-
terdependent decision making, but do not use an opt-
misation framework. Since language models in our
approach can be obtained for any domain for which
corpus data is available, it generalises to new do-
mains with limited effort and reduced development
</bodyText>
<page confidence="0.987228">
654
</page>
<note confidence="0.768146">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 654–659,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.811312777777778">
Utterance
string=“turn around and go out”, time=‘20:54:55’
Utterance type
content=‘orientation,destination’ [straight, path, direction]
navigation level=‘low’ [high]
User
user reaction=‘perform desired action’
[perform undesired action, wait, request help]
user position=‘on track’ [off track]
</figure>
<figureCaption confidence="0.998892">
Figure 1: Example annotation: alternative values for at-
tributes are given in square brackets.
</figureCaption>
<bodyText confidence="0.99988725">
time. For related work on using graphical models
for language generation, see e.g., Barzilay and Lee
(2002), who use lattices, or Mairesse et al. (2010),
who use dynamic Bayesian networks.
</bodyText>
<sectionHeader confidence="0.980042" genericHeader="method">
2 Generation Spaces
</sectionHeader>
<bodyText confidence="0.999699586206897">
We are concerned with the generation of navigation
instructions in a virtual 3D world as in the GIVE
scenario (Koller et al., 2010). In this task, two peo-
ple engage in a ‘treasure hunt’, where one partici-
pant navigates the other through the world, pressing
a sequence of buttons and completing the task by
obtaining a trophy. The GIVE-2 corpus (Gargett et
al., 2010) provides transcripts of such dialogues in
English and German. For this paper, we comple-
mented the English dialogues of the corpus with a
set of semantic annotations,1 an example of which
is given in Figure 1. This example also exempli-
fies the type of utterances we generate. The input to
the system consists of semantic variables compara-
ble to the annotated values, the output corresponds
to strings of words. We use HRL to optimise deci-
sions of content selection (‘what to say’) and HMMs
for decisions of surface realisation (‘how to say it’).
Content selection involves whether to use a low-, or
high-level navigation strategy. The former consists
of a sequence of primitive instructions (‘go straight’,
‘turn left’), the latter represents contractions of se-
quences of low-level instructions (‘head to the next
room’). Content selection also involves choosing a
level of detail for the instruction corresponding to
the user’s information need. We evaluate the learnt
content selection decisions in terms of task success.
For surface realisation, we use HMMs to inform
the HRL agent’s learning process. Here we address
</bodyText>
<footnote confidence="0.517316">
1The annotations are available on request.
</footnote>
<bodyText confidence="0.999948387096774">
the one-to-many relationship arising between a se-
mantic form (from the content selection stage) and
its possible realisations. Semantic forms of instruc-
tions have an average of 650 surface realisations,
including syntactic and lexical variation, and deci-
sions of granularity. We refer to the set of alterna-
tive realisations of a semantic form as its ‘generation
space’. In surface realisation, we aim to optimise the
tradeoff between alignment and consistency (Picker-
ing and Garrod, 2004; Halliday and Hasan, 1976) on
the one hand, and variation (to improve text quality
and readability) on the other hand (Belz and Reiter,
2006; Foster and Oberlander, 2006) in a 50/50 dis-
tribution. We evaluate the learnt surface realisation
decisions in terms of similarity with human data.
Note that while we treat content selection and
surface realisation as separate NLG tasks, their op-
timisation is achieved jointly. This is due to a
tradeoff arising between the two tasks. For exam-
ple, while surface realisation decisions that are opti-
mised solely with respect to a language model tend
to favour frequent and short sequences, these can
be inappropriate according to the user’s information
need (because they are unfamiliar with the naviga-
tion task, or are confused or lost). In such situa-
tions, it is important to treat content selection and
surface realisation as a unified whole. Decisions of
both tasks are inextricably linked and we will show
in Section 5.2 that their joint optimisation leads to
better results than an isolated optimisation as in, for
example, a two-stage model.
</bodyText>
<sectionHeader confidence="0.992877" genericHeader="method">
3 NLG Using HRL and HMMs
</sectionHeader>
<subsectionHeader confidence="0.99992">
3.1 Hierarchical Reinforcement Learning
</subsectionHeader>
<bodyText confidence="0.999949384615385">
The idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system’s knowledge about
the generation task (e.g. content selection, naviga-
tion strategy, surface realisation). The action set
describes the system’s capabilities (e.g. ‘use high
level navigation strategy’, ‘use imperative mood’,
etc.). The reward function assigns a numeric value
for each action taken. In this way, language gen-
</bodyText>
<page confidence="0.998123">
655
</page>
<figureCaption confidence="0.9929874">
Figure 2: Hierarchy of learning agents (left), where shaded agents use an HMM-based reward function. The top three
layers are responsible for content selection (CS) decisions and use HRL. The shaded agents in the bottom use HRL
with an HMM-based reward function and joint optimisation of content selection and surface realisation (SR). They
provide the observation sequence to the HMMs. The HMMs represent generation spaces for surface realisation. An
example HMM, representing the generation space of ‘destination’ instructions, is shown on the right.
</figureCaption>
<bodyText confidence="0.989273291666667">
eration can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt−1, st},
where the goal is to find an optimal strategy auto-
matically. To do this we use RL with a divide-and-
conquer approach to optimise a hierarchy of genera-
tion policies rather than a single policy. The hierar-
chy of RL agents consists of L levels and N models
per level, denoted as Mij, where j ∈ {0,..., N − 1}
and i ∈ {0,..., L − 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple &lt; Sij, Aij, Tji , Rij &gt;.
Sij is a set of states, Aij is a set of actions, Tji is
a transition function that determines the next state
s′ from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting T time steps. The random
variable T represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
</bodyText>
<equation confidence="0.6757195">
7r*ij(s) = arg maxaEA� Q*ij(s, a), where Q*i
j (s, a)
</equation>
<bodyText confidence="0.999679">
specifies the expected cumulative reward for execut-
ing action a in state s and then following policy 7r*ij.
We use HSMQ-Learning (Dietterich, 1999) to learn
a hierarchy of generation policies.
</bodyText>
<subsectionHeader confidence="0.997347">
3.2 Hidden Markov Models for NLG
</subsectionHeader>
<bodyText confidence="0.999938653846154">
The idea of representing the generation space of
a surface realiser as an HMM can be roughly de-
fined as the converse of POS tagging, where an in-
put string of words is mapped onto a hidden se-
quence of POS tags. Our scenario is as follows:
given a set of (specialised) semantic symbols (e.g.,
‘actor’, ‘process’, ‘destination’),2 what is the most
likely sequence of words corresponding to the sym-
bols? Figure 2 provides a graphic illustration of this
idea. We treat states as representing words, and se-
quences of states i0...in as representing phrases or
sentences. An observation sequence o0...on consists
of a finite set of semantic symbols specific to the in-
struction type (i.e., ‘destination’, ‘direction’, ‘orien-
tation’, ‘path’, ‘straight’). Each symbol has an ob-
servation likelihood bi(ot), which gives the proba-
bility of observing o in state i at time t. The tran-
sition and emission probabilities are learnt during
training using the Baum-Welch algorithm. To de-
sign an HMM from the corpus data, we used the
ABL algorithm (van Zaanen, 2000), which aligns
strings based on Minimum Edit Distance, and in-
duces a context-free grammar from the aligned ex-
amples. Subsequently, we constructed the HMMs
from the CFGs, one for each instruction type, and
trained them on the annotated data.
</bodyText>
<footnote confidence="0.838236">
2Utterances typically contain five to ten semantic categories.
</footnote>
<page confidence="0.992253">
656
</page>
<subsectionHeader confidence="0.924607">
3.3 An HMM-based Reward Function Induced
from Human Data
</subsectionHeader>
<bodyText confidence="0.99400875">
Due to its unique function in an RL framework, we
suggest to induce a reward function for surface re-
alisation from human data. To this end, we create
and train HMMs to represent the generation space
of a particular surface realisation task. We then use
the forward probability, derived from the Forward
algorithm, of an observation sequence to inform the
agent’s learning process.
</bodyText>
<tableCaption confidence="0.560464777777778">
0 for reaching the goal state
+1 for a desired semantic choice or
maintaining an equal distribution
of alignment and variation
-2 for executing action a and remain-
ing in the same state s = s′
P(w0...w.) for for reaching a goal state corres-
ponding to word sequence w0...w.
-1 otherwise.
</tableCaption>
<bodyText confidence="0.996088666666667">
Whenever the agent has generated a word sequence
w0...wr,,, the HMM assigns a reward corresponding
to the likelihood of observing the sequence in the
data. In addition, the agent is rewarded for short
interactions at maximal task success3 and optimal
content selection (cf. Section 2). Note that while re-
ward P(w0...wr,,) applies only to surface realisation
agents M30���4, the other rewards apply to all agents
of the hierarchy.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999986714285715">
We test our approach using the (hand-crafted) hierar-
chy of generation subtasks in Figure 2. It consists of
a root agent (M00 ), and subtasks for low-level (M20 )
and high-level (Mi) navigation strategies (M1), and
for instruction types ‘orientation’ (M30 ), ‘straight’
(M31), ‘direction’ (M23), ‘path’ (M33) and destina-
tion’ (M43). Models M30���4 are responsible for sur-
face generation. They will be trained using HRL
with an HMM-based reward function induced from
human data. All other agents use hand-crafted re-
wards. Finally, subtask Mo can repair a previous
system utterance. The states of the agent contain
all situational and linguistic information relevant to
its decision making, e.g., the spatial environment,
</bodyText>
<footnote confidence="0.9064045">
3Task success is addressed by that each utterance needs to
be ‘accepted’ by the user (cf. Section 5.1).
</footnote>
<bodyText confidence="0.998974">
discourse history, and status of grounding.4 Due to
space constraints, please see Dethlefs et al. (2011)
for the full state-action space. We distinguish prim-
itive actions (corresponding to single generation de-
cisions) and composite actions (corresponding to
generation subtasks (Fig. 2)).
</bodyText>
<sectionHeader confidence="0.996484" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.956977">
5.1 The Simulated Environment
</subsectionHeader>
<bodyText confidence="0.999979">
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user’s reaction to a system utterance. The first as-
pect is represented by a set of contextual variables
describing the environment, 5 and user behaviour.6
Altogether, this leads to 115 thousand different con-
textual configurations, which are estimated from
data (cf. Section 2). The uncertainty regarding
the user’s reaction to an utterance is represented by
a Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.7 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.8 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
cross-corpus validation using a 60%-40% split.
</bodyText>
<subsectionHeader confidence="0.999842">
5.2 Comparison of Generation Policies
</subsectionHeader>
<bodyText confidence="0.999815666666667">
We trained three different generation policies. The
learnt policy optimises content selection and sur-
face realisation decisions in a unified fashion, and
is informed by an HMM-based generation space
reward function. The greedy policy is informed
only by the HMM and always chooses the most
</bodyText>
<footnote confidence="0.979061461538461">
4An example for the state variables of model M1 are the
annotation values in Fig. 1 which are used as the agent’s
knowledge base. Actions are ‘choose easy route’, ‘choose short
route’, ‘choose low level strategy’, ‘choose high level strategy’.
5previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
6previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
7navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
8User reactions measure the system’s task success.
</footnote>
<equation confidence="0.992">
r = {
</equation>
<page confidence="0.990364">
657
</page>
<bodyText confidence="0.999988942857143">
likely sequence independent of content selection.
The valid sequence policy generates any grammat-
ical sequence. All policies were trained for 20000
episodes.9 Figure 3, which plots the average re-
wards of all three policies (averaged over ten runs),
shows that the ‘learnt’ policy performs best in terms
of task success by reaching the highest overall re-
wards over time. An absolute comparison of the av-
erage rewards (rescaled from 0 to 1) of the last 1000
training episodes of each policy shows that greedy
improves ‘any valid sequence’ by 71%, and learnt
improves greedy by 29% (these differences are sig-
nificant at p &lt; 0.01). This is due to the learnt policy
showing more adaptation to contextual features than
the greedy or ‘valid sequence’ policies. To evaluate
human-likeness, we compare instructions (i.e. word
sequences) using Precision-Recall based on the F-
Measure score, and dialogue similarity based on the
Kulback-Leibler (KL) divergence (Cuay´ahuitl et al.,
2005). The former shows how the texts generated by
each of our generation policies compare to human-
authored texts in terms of precision and recall. The
latter shows how similar they are to human-authored
texts. Table 1 shows results of the comparison of
two human data sets ‘Real1’ vs ‘Real2’ and both of
them together against our different policies. While
the greedy policy receives higher F-Measure scores,
the learnt policy is most similar to the human data.
This is due to variation: in contrast to greedy be-
haviour, which always exploits the most likely vari-
ant, the learnt policy varies surface forms. This leads
to lower F-Measure scores, but achieves higher sim-
ilarity with human authors. This ultimately is a de-
sirable property, since it enhances the quality and
naturalness of our instructions.
</bodyText>
<sectionHeader confidence="0.995036" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99920675">
We have presented a novel approach to optimising
surface realisation using HRL. We suggested to
inform an HRL agent’s learning process by an
HMM-based reward function, which was induced
</bodyText>
<footnote confidence="0.913321125">
9For training, the step-size parameter α (one for each
SMDP) , which indicates the learning rate, was initiated with
1 and then reduced over time by α = 1
1�� , where t is the time
step. The discount rate γ, which indicates the relevance of fu-
ture rewards in relation to immediate rewards, was set to 0.99,
and the probability of a random action ǫ was 0.01. See Sutton
and Barto (1998) for details on these parameters.
</footnote>
<figureCaption confidence="0.985224">
Figure 3: Performance of ‘learnt’, ‘greedy’, and ‘any
valid sequence’ generation behaviours (average rewards).
</figureCaption>
<figure confidence="0.6746408">
F-Measure KL-Divergence
0.58 1.77
0.40 2.80
0.49 4.34
0.0 10.06
</figure>
<tableCaption confidence="0.9772325">
Table 1: Evaluation of generation behaviours with
Precision-Recall and KL-divergence.
</tableCaption>
<bodyText confidence="0.999809">
from data and in which the HMM represents the
generation space of a surface realiser. We also
proposed to jointly optimise surface realisation
and content selection to balance the tradeoffs of
(a) frequency in terms of a language model, (b)
alignment/consistency vs variation, (c) properties
of the user and environment. Results showed that
our hybrid approach outperforms two baselines in
terms of task success and human-likeness: a greedy
baseline acting independent of content selection,
and a random ‘valid sequence’ baseline. Future
work can transfer our approach to different domains
to confirm its benefits. Also, a detailed human
evaluation study is needed to assess the effects
of different surface form variants. Finally, other
graphical models besides HMMs, such as Bayesian
Networks, can be explored for informing the surface
realisation process of a generation system.
</bodyText>
<sectionHeader confidence="0.998318" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995706">
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ‘Spatial Cognition’ and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work.
</bodyText>
<figure confidence="0.992391647058824">
50
0
Average Rewards i0
−100
−150
−20
−25
Valid Sequence
Gmedy
Leamt
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
Episodes x 104
Compared Policies
Real1 - Real2
Real - ‘Learnt’
Real - ‘Greedy’
Real - ‘Valid Seq.’
</figure>
<page confidence="0.988488">
658
</page>
<sectionHeader confidence="0.988738" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999643428571429">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 502–512.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th Conference on Computa-
tional Linguistics (ACL) - Volume 1, pages 42–48.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164–171.
Anja Belz and Ehud Reiter. 2006. Comparing Automatic
and Human Evaluation of NLG Systems. In Proc. of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 313–320.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1–26.
Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-Computer Dia-
logue Simulation Using Hidden Markov Models. In
Proc. ofASRU, pages 290–295.
Nina Dethlefs and Heriberto Cuay´ahuitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceeding of the 6th International Con-
ference on Natural Language Generation (INLG).
Nina Dethlefs, Heriberto Cuay´ahuitl, and Jette Viethen.
2011. Optimising Natural Language Generation De-
cision Making for Situated Dialogue. In Proc. of the
12th Annual SIGdial Meeting on Discourse and Dia-
logue.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227–303.
Mary Ellen Foster and Jon Oberlander. 2006. Data-
driven generation of emphatic facial displays. In Proc.
of the European Chapter of the Association for Com-
putational Linguistic (EACL), pages 353–360.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus of
giving instructions in virtual environments. In LREC.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 69–78.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337–361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
704–710.
W J M Levelt and S Kelter. 1982. Surface form and
memory in question answering. Cognitive Psychol-
ogy, 14.
Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇciˇcek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552–1561.
Alice H. Oh and Alexander I. Rudnicky. 2000. Stochas-
tic language generation for spoken dialogue systems.
In Proceedings of the 2000 ANLP/NAACL Workshop
on Conversational systems - Volume 3, pages 27–32.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
L R Rabiner. 1989. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Proceedings ofIEEE, pages 257–286.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. of the Annual Meeting of
the Association for Computational Lingustics (ACL),
pages 1009–1018.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Menno van Zaanen. 2000. Bootstrapping syntax and
recursion using alginment-based learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning (ICML), pages 1063–1070, San
Francisco, CA, USA.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 271–280.
Michael White. 2004. Reining in CCG chart realization.
In Proc. of the International Conference on Natural
Language Generation (INLG), pages 182–191.
</reference>
<page confidence="0.998928">
659
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645641">
<title confidence="0.997549">Hierarchical Reinforcement Learning and Hidden Markov Models Task-Oriented Natural Language Generation</title>
<author confidence="0.999836">Nina Dethlefs Heriberto Cuay´ahuitl</author>
<affiliation confidence="0.9571735">Department of Linguistics, German Research Centre for Artificial Intelligence University of Bremen (DFKI), Saarbr¨ucken</affiliation>
<email confidence="0.726122">dethlefs@uni-bremen.deheriberto.cuayahuitl@dfki.de</email>
<abstract confidence="0.998168">Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>502--512</pages>
<contexts>
<context position="3447" citStr="Angeli et al. (2010)" startWordPosition="519" endWordPosition="522">enomena are indeed worth modelling in an NLG system. The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation. To this end, we train HMMs (Rabiner, 1989) on a corpus of grammatical word sequences and use them to inform the agent’s learning process. In addition, we suggest to optimise surface realisation and content selection decisions in a joint, rather than isolated, fashion. Results show that our combined approach generates more successful and human-like utterances than a greedy or random baseline. This is related to Angeli et al. (2010), who also address interdependent decision making, but do not use an optmisation framework. Since language models in our approach can be obtained for any domain for which corpus data is available, it generalises to new domains with limited effort and reduced development 654 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 654–659, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Utterance string=“turn around and go out”, time=‘20:54:55’ Utterance type content=‘orientation,destination’ [straight, path,</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 502–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics (ACL) -</booktitle>
<volume>1</volume>
<pages>42--48</pages>
<contexts>
<context position="1145" citStr="Bangalore and Rambow, 2000" startWordPosition="159" endWordPosition="162">tion and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In s</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th Conference on Computational Linguistics (ACL) - Volume 1, pages 42–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>164--171</pages>
<contexts>
<context position="4418" citStr="Barzilay and Lee (2002)" startWordPosition="652" endWordPosition="655">al Linguistics:shortpapers, pages 654–659, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Utterance string=“turn around and go out”, time=‘20:54:55’ Utterance type content=‘orientation,destination’ [straight, path, direction] navigation level=‘low’ [high] User user reaction=‘perform desired action’ [perform undesired action, wait, request help] user position=‘on track’ [off track] Figure 1: Example annotation: alternative values for attributes are given in square brackets. time. For related work on using graphical models for language generation, see e.g., Barzilay and Lee (2002), who use lattices, or Mairesse et al. (2010), who use dynamic Bayesian networks. 2 Generation Spaces We are concerned with the generation of navigation instructions in a virtual 3D world as in the GIVE scenario (Koller et al., 2010). In this task, two people engage in a ‘treasure hunt’, where one participant navigates the other through the world, pressing a sequence of buttons and completing the task by obtaining a trophy. The GIVE-2 corpus (Gargett et al., 2010) provides transcripts of such dialogues in English and German. For this paper, we complemented the English dialogues of the corpus w</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing Automatic and Human Evaluation of NLG Systems.</title>
<date>2006</date>
<booktitle>In Proc. of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>313--320</pages>
<contexts>
<context position="1464" citStr="Belz and Reiter, 2006" startWordPosition="205" endWordPosition="208">uman-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environmen</context>
<context position="6678" citStr="Belz and Reiter, 2006" startWordPosition="1016" endWordPosition="1019">-to-many relationship arising between a semantic form (from the content selection stage) and its possible realisations. Semantic forms of instructions have an average of 650 surface realisations, including syntactic and lexical variation, and decisions of granularity. We refer to the set of alternative realisations of a semantic form as its ‘generation space’. In surface realisation, we aim to optimise the tradeoff between alignment and consistency (Pickering and Garrod, 2004; Halliday and Hasan, 1976) on the one hand, and variation (to improve text quality and readability) on the other hand (Belz and Reiter, 2006; Foster and Oberlander, 2006) in a 50/50 distribution. We evaluate the learnt surface realisation decisions in terms of similarity with human data. Note that while we treat content selection and surface realisation as separate NLG tasks, their optimisation is achieved jointly. This is due to a tradeoff arising between the two tasks. For example, while surface realisation decisions that are optimised solely with respect to a language model tend to favour frequent and short sequences, these can be inappropriate according to the user’s information need (because they are unfamiliar with the navig</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing Automatic and Human Evaluation of NLG Systems. In Proc. of the European Chapter of the Association for Computational Linguistics (EACL), pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<pages>1--1</pages>
<contexts>
<context position="1194" citStr="Belz, 2008" startWordPosition="169" endWordPosition="170">arning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface re</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 1:1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Steve Renals</author>
<author>Oliver Lemon</author>
<author>Hiroshi Shimodaira</author>
</authors>
<title>Human-Computer Dialogue Simulation Using Hidden Markov Models.</title>
<date>2005</date>
<booktitle>In Proc. ofASRU,</booktitle>
<pages>290--295</pages>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2005</marker>
<rawString>Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and Hiroshi Shimodaira. 2005. Human-Computer Dialogue Simulation Using Hidden Markov Models. In Proc. ofASRU, pages 290–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Heriberto Cuay´ahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning for Adaptive Text Generation.</title>
<date>2010</date>
<booktitle>Proceeding of the 6th International Conference on Natural Language Generation (INLG).</booktitle>
<marker>Dethlefs, Cuay´ahuitl, 2010</marker>
<rawString>Nina Dethlefs and Heriberto Cuay´ahuitl. 2010. Hierarchical Reinforcement Learning for Adaptive Text Generation. Proceeding of the 6th International Conference on Natural Language Generation (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Heriberto Cuay´ahuitl</author>
<author>Jette Viethen</author>
</authors>
<title>Optimising Natural Language Generation Decision Making for Situated Dialogue.</title>
<date>2011</date>
<booktitle>In Proc. of the 12th Annual SIGdial Meeting on Discourse and Dialogue.</booktitle>
<marker>Dethlefs, Cuay´ahuitl, Viethen, 2011</marker>
<rawString>Nina Dethlefs, Heriberto Cuay´ahuitl, and Jette Viethen. 2011. Optimising Natural Language Generation Decision Making for Situated Dialogue. In Proc. of the 12th Annual SIGdial Meeting on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>13--227</pages>
<contexts>
<context position="10335" citStr="Dietterich, 1999" startWordPosition="1638" endWordPosition="1639">r taking an action a in state s lasting T time steps. The random variable T represents the number of time steps the agent takes to complete a subtask. Actions can be either primitive or composite. The former yield single rewards, the latter correspond to SMDPs and yield cumulative discounted rewards. The goal of each SMDP is to find an optimal policy that maximises the reward for each visited state, according to 7r*ij(s) = arg maxaEA� Q*ij(s, a), where Q*i j (s, a) specifies the expected cumulative reward for executing action a in state s and then following policy 7r*ij. We use HSMQ-Learning (Dietterich, 1999) to learn a hierarchy of generation policies. 3.2 Hidden Markov Models for NLG The idea of representing the generation space of a surface realiser as an HMM can be roughly defined as the converse of POS tagging, where an input string of words is mapped onto a hidden sequence of POS tags. Our scenario is as follows: given a set of (specialised) semantic symbols (e.g., ‘actor’, ‘process’, ‘destination’),2 what is the most likely sequence of words corresponding to the symbols? Figure 2 provides a graphic illustration of this idea. We treat states as representing words, and sequences of states i0.</context>
</contexts>
<marker>Dietterich, 1999</marker>
<rawString>Thomas G. Dietterich. 1999. Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. Journal of Artificial Intelligence Research, 13:227–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Jon Oberlander</author>
</authors>
<title>Datadriven generation of emphatic facial displays.</title>
<date>2006</date>
<booktitle>In Proc. of the European Chapter of the Association for Computational Linguistic (EACL),</booktitle>
<pages>353--360</pages>
<contexts>
<context position="1494" citStr="Foster and Oberlander, 2006" startWordPosition="209" endWordPosition="212">hat our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (</context>
<context position="6708" citStr="Foster and Oberlander, 2006" startWordPosition="1020" endWordPosition="1023">rising between a semantic form (from the content selection stage) and its possible realisations. Semantic forms of instructions have an average of 650 surface realisations, including syntactic and lexical variation, and decisions of granularity. We refer to the set of alternative realisations of a semantic form as its ‘generation space’. In surface realisation, we aim to optimise the tradeoff between alignment and consistency (Pickering and Garrod, 2004; Halliday and Hasan, 1976) on the one hand, and variation (to improve text quality and readability) on the other hand (Belz and Reiter, 2006; Foster and Oberlander, 2006) in a 50/50 distribution. We evaluate the learnt surface realisation decisions in terms of similarity with human data. Note that while we treat content selection and surface realisation as separate NLG tasks, their optimisation is achieved jointly. This is due to a tradeoff arising between the two tasks. For example, while surface realisation decisions that are optimised solely with respect to a language model tend to favour frequent and short sequences, these can be inappropriate according to the user’s information need (because they are unfamiliar with the navigation task, or are confused or</context>
</contexts>
<marker>Foster, Oberlander, 2006</marker>
<rawString>Mary Ellen Foster and Jon Oberlander. 2006. Datadriven generation of emphatic facial displays. In Proc. of the European Chapter of the Association for Computational Linguistic (EACL), pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gargett</author>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>The GIVE-2 corpus of giving instructions in virtual environments.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="4886" citStr="Gargett et al., 2010" startWordPosition="732" endWordPosition="735"> for attributes are given in square brackets. time. For related work on using graphical models for language generation, see e.g., Barzilay and Lee (2002), who use lattices, or Mairesse et al. (2010), who use dynamic Bayesian networks. 2 Generation Spaces We are concerned with the generation of navigation instructions in a virtual 3D world as in the GIVE scenario (Koller et al., 2010). In this task, two people engage in a ‘treasure hunt’, where one participant navigates the other through the world, pressing a sequence of buttons and completing the task by obtaining a trophy. The GIVE-2 corpus (Gargett et al., 2010) provides transcripts of such dialogues in English and German. For this paper, we complemented the English dialogues of the corpus with a set of semantic annotations,1 an example of which is given in Figure 1. This example also exemplifies the type of utterances we generate. The input to the system consists of semantic variables comparable to the annotated values, the output corresponds to strings of words. We use HRL to optimise decisions of content selection (‘what to say’) and HMMs for decisions of surface realisation (‘how to say it’). Content selection involves whether to use a low-, or h</context>
</contexts>
<marker>Gargett, Garoufi, Koller, Striegnitz, 2010</marker>
<rawString>Andrew Gargett, Konstantina Garoufi, Alexander Koller, and Kristina Striegnitz. 2010. The GIVE-2 corpus of giving instructions in virtual environments. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="1329" citStr="Halliday and Hasan, 1976" startWordPosition="186" endWordPosition="189">r surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Rei</context>
<context position="6564" citStr="Halliday and Hasan, 1976" startWordPosition="996" endWordPosition="999">se HMMs to inform the HRL agent’s learning process. Here we address 1The annotations are available on request. the one-to-many relationship arising between a semantic form (from the content selection stage) and its possible realisations. Semantic forms of instructions have an average of 650 surface realisations, including syntactic and lexical variation, and decisions of granularity. We refer to the set of alternative realisations of a semantic form as its ‘generation space’. In surface realisation, we aim to optimise the tradeoff between alignment and consistency (Pickering and Garrod, 2004; Halliday and Hasan, 1976) on the one hand, and variation (to improve text quality and readability) on the other hand (Belz and Reiter, 2006; Foster and Oberlander, 2006) in a 50/50 distribution. We evaluate the learnt surface realisation decisions in terms of similarity with human data. Note that while we treat content selection and surface realisation as separate NLG tasks, their optimisation is achieved jointly. This is due to a tradeoff arising between the two tasks. For example, while surface realisation decisions that are optimised solely with respect to a language model tend to favour frequent and short sequence</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning to adapt to unknown users: referring expression generation in spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>69--78</pages>
<contexts>
<context position="2143" citStr="Janarthanam and Lemon, 2010" startWordPosition="314" endWordPosition="317">he most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (Rieser et al., 2010; Janarthanam and Lemon, 2010). HRL has the additional advantage of scaling to large and complex problems (Dethlefs and Cuay´ahuitl, 2010). Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent’s most crucial component. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al., 1997). Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. frequency, consistency, and vari</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2010. Learning to adapt to unknown users: referring expression generation in spoken dialogue systems. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<title>The first challenge on generating instructions in virtual environments.</title>
<date>2010</date>
<booktitle>Empirical Methods on Natural Language Generation,</booktitle>
<pages>337--361</pages>
<editor>In M. Theune and E. Krahmer, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin/Heidelberg, Germany.</location>
<contexts>
<context position="4651" citStr="Koller et al., 2010" startWordPosition="691" endWordPosition="694">[straight, path, direction] navigation level=‘low’ [high] User user reaction=‘perform desired action’ [perform undesired action, wait, request help] user position=‘on track’ [off track] Figure 1: Example annotation: alternative values for attributes are given in square brackets. time. For related work on using graphical models for language generation, see e.g., Barzilay and Lee (2002), who use lattices, or Mairesse et al. (2010), who use dynamic Bayesian networks. 2 Generation Spaces We are concerned with the generation of navigation instructions in a virtual 3D world as in the GIVE scenario (Koller et al., 2010). In this task, two people engage in a ‘treasure hunt’, where one participant navigates the other through the world, pressing a sequence of buttons and completing the task by obtaining a trophy. The GIVE-2 corpus (Gargett et al., 2010) provides transcripts of such dialogues in English and German. For this paper, we complemented the English dialogues of the corpus with a set of semantic annotations,1 an example of which is given in Figure 1. This example also exemplifies the type of utterances we generate. The input to the system consists of semantic variables comparable to the annotated values</context>
</contexts>
<marker>Koller, Striegnitz, Byron, Cassell, Dale, Moore, Oberlander, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010. The first challenge on generating instructions in virtual environments. In M. Theune and E. Krahmer, editors, Empirical Methods on Natural Language Generation, pages 337–361, Berlin/Heidelberg, Germany. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1117" citStr="Langkilde and Knight, 1998" startWordPosition="155" endWordPosition="158">ptimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect t</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL), pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
<author>S Kelter</author>
</authors>
<title>Surface form and memory in question answering.</title>
<date>1982</date>
<journal>Cognitive Psychology,</journal>
<volume>14</volume>
<contexts>
<context position="1420" citStr="Levelt and Kelter, 1982" startWordPosition="198" endWordPosition="201">l (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions </context>
</contexts>
<marker>Levelt, Kelter, 1982</marker>
<rawString>W J M Levelt and S Kelter. 1982. Surface form and memory in question answering. Cognitive Psychology, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Milica Gaˇsi´c</author>
<author>Filip Jurˇciˇcek</author>
<author>Simon Keizer</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Phrase-based statistical language generation using graphical models and active learning.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1552--1561</pages>
<marker>Mairesse, Gaˇsi´c, Jurˇciˇcek, Keizer, Thomson, Yu, Young, 2010</marker>
<rawString>Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇciˇcek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1552–1561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice H Oh</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Stochastic language generation for spoken dialogue systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems -</booktitle>
<volume>3</volume>
<pages>27--32</pages>
<contexts>
<context position="1168" citStr="Oh and Rudnicky, 2000" startWordPosition="163" endWordPosition="166"> using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is import</context>
</contexts>
<marker>Oh, Rudnicky, 2000</marker>
<rawString>Alice H. Oh and Alexander I. Rudnicky. 2000. Stochastic language generation for spoken dialogue systems. In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems - Volume 3, pages 27–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Pickering</author>
<author>Simon Garrod</author>
</authors>
<title>Toward a mechanistc psychology of dialog.</title>
<date>2004</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>27</volume>
<contexts>
<context position="1289" citStr="Pickering and Garrod, 2004" startWordPosition="180" endWordPosition="183">rom human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforce</context>
<context position="6537" citStr="Pickering and Garrod, 2004" startWordPosition="991" endWordPosition="995">or surface realisation, we use HMMs to inform the HRL agent’s learning process. Here we address 1The annotations are available on request. the one-to-many relationship arising between a semantic form (from the content selection stage) and its possible realisations. Semantic forms of instructions have an average of 650 surface realisations, including syntactic and lexical variation, and decisions of granularity. We refer to the set of alternative realisations of a semantic form as its ‘generation space’. In surface realisation, we aim to optimise the tradeoff between alignment and consistency (Pickering and Garrod, 2004; Halliday and Hasan, 1976) on the one hand, and variation (to improve text quality and readability) on the other hand (Belz and Reiter, 2006; Foster and Oberlander, 2006) in a 50/50 distribution. We evaluate the learnt surface realisation decisions in terms of similarity with human data. Note that while we treat content selection and surface realisation as separate NLG tasks, their optimisation is achieved jointly. This is due to a tradeoff arising between the two tasks. For example, while surface realisation decisions that are optimised solely with respect to a language model tend to favour </context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>Martin J. Pickering and Simon Garrod. 2004. Toward a mechanistc psychology of dialog. Behavioral and Brain Sciences, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.</title>
<date>1989</date>
<booktitle>In Proceedings ofIEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="3055" citStr="Rabiner, 1989" startWordPosition="457" endWordPosition="458">n a reward function from human data as in the PARADISE framework (Walker et al., 1997). Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. frequency, consistency, and variation. However, linguistic and psychological studies (cited above) show that such phenomena are indeed worth modelling in an NLG system. The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation. To this end, we train HMMs (Rabiner, 1989) on a corpus of grammatical word sequences and use them to inform the agent’s learning process. In addition, we suggest to optimise surface realisation and content selection decisions in a joint, rather than isolated, fashion. Results show that our combined approach generates more successful and human-like utterances than a greedy or random baseline. This is related to Angeli et al. (2010), who also address interdependent decision making, but do not use an optmisation framework. Since language models in our approach can be obtained for any domain for which corpus data is available, it generali</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L R Rabiner. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Proceedings ofIEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>Optimising information presentation for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Lingustics (ACL),</booktitle>
<pages>1009--1018</pages>
<contexts>
<context position="2113" citStr="Rieser et al., 2010" startWordPosition="310" endWordPosition="313">. Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a unified fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (Rieser et al., 2010; Janarthanam and Lemon, 2010). HRL has the additional advantage of scaling to large and complex problems (Dethlefs and Cuay´ahuitl, 2010). Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent’s most crucial component. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al., 1997). Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. fr</context>
</contexts>
<marker>Rieser, Lemon, Liu, 2010</marker>
<rawString>Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising information presentation for spoken dialogue systems. In Proc. of the Annual Meeting of the Association for Computational Lingustics (ACL), pages 1009–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="18514" citStr="Sutton and Barto (1998)" startWordPosition="2936" endWordPosition="2939">y and naturalness of our instructions. 6 Conclusion We have presented a novel approach to optimising surface realisation using HRL. We suggested to inform an HRL agent’s learning process by an HMM-based reward function, which was induced 9For training, the step-size parameter α (one for each SMDP) , which indicates the learning rate, was initiated with 1 and then reduced over time by α = 1 1�� , where t is the time step. The discount rate γ, which indicates the relevance of future rewards in relation to immediate rewards, was set to 0.99, and the probability of a random action ǫ was 0.01. See Sutton and Barto (1998) for details on these parameters. Figure 3: Performance of ‘learnt’, ‘greedy’, and ‘any valid sequence’ generation behaviours (average rewards). F-Measure KL-Divergence 0.58 1.77 0.40 2.80 0.49 4.34 0.0 10.06 Table 1: Evaluation of generation behaviours with Precision-Recall and KL-divergence. from data and in which the HMM represents the generation space of a surface realiser. We also proposed to jointly optimise surface realisation and content selection to balance the tradeoffs of (a) frequency in terms of a language model, (b) alignment/consistency vs variation, (c) properties of the user a</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S Sutton and Andrew G Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>Bootstrapping syntax and recursion using alginment-based learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),</booktitle>
<pages>1063--1070</pages>
<location>San Francisco, CA, USA.</location>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. Bootstrapping syntax and recursion using alginment-based learning. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML), pages 1063–1070, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--280</pages>
<contexts>
<context position="2527" citStr="Walker et al., 1997" startWordPosition="376" endWordPosition="379">) to achieve this. Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (Rieser et al., 2010; Janarthanam and Lemon, 2010). HRL has the additional advantage of scaling to large and complex problems (Dethlefs and Cuay´ahuitl, 2010). Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent’s most crucial component. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al., 1997). Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. frequency, consistency, and variation. However, linguistic and psychological studies (cited above) show that such phenomena are indeed worth modelling in an NLG system. The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation. To this end, we train HMMs (Rabiner, 1989) on a corpus of grammatical word sequences and use them to inform the ag</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proc. of the International Conference on Natural Language Generation (INLG),</booktitle>
<pages>182--191</pages>
<contexts>
<context position="1181" citStr="White, 2004" startWordPosition="167" endWordPosition="168">nforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made according to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consistency (Halliday and Hasan, 1976), and variation, which influence people’s assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimi</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG chart realization. In Proc. of the International Conference on Natural Language Generation (INLG), pages 182–191.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>