<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003407">
<title confidence="0.998895333333333">
A Combination of Active Learning and Semi-supervised Learning
Starting with Positive and Unlabeled Examples for Word Sense
Disambiguation: An Empirical Study on Japanese Web Search Query
</title>
<author confidence="0.855824">
Makoto Imamura
and Yasuhiro Takayama
</author>
<affiliation confidence="0.833243">
Information Technology R&amp;D Center,
Mitsubishi Electric Corporation
</affiliation>
<address confidence="0.772866">
5-1-1 Ofuna, Kamakura, Kanagawa, Japan
</address>
<email confidence="0.62809">
{Imamura.Makoto@bx,Takayama.Yasu
hiro@ea).MitsubishiElectric.co.jp
</email>
<sectionHeader confidence="0.996083" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999440105263158">
This paper proposes to solve the bottle-
neck of finding training data for word
sense disambiguation (WSD) in the do-
main of web queries, where a complete set
of ambiguous word senses are unknown.
In this paper, we present a combination of
active learning and semi-supervised learn-
ing method to treat the case when positive
examples, which have an expected word
sense in web search result, are only given.
The novelty of our approach is to use
“pseudo negative examples” with reliable
confidence score estimated by a classifier
trained with positive and unlabeled exam-
ples. We show experimentally that our
proposed method achieves close enough
WSD accuracy to the method with the
manually prepared negative examples in
several Japanese Web search data.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999577066666667">
In Web mining for sentiment or reputation
analysis, it is important for reliable analysis to
extract large amount of texts about certain prod-
ucts, shops, or persons with high accuracy. When
retrieving texts from Web archive, we often suf-
fer from word sense ambiguity and WSD system
is indispensable. For instance, when we try to
analyze reputation of &amp;quot;Loft&amp;quot;, a name of variety
store chain in Japan, we found that simple text
search retrieved many unrelated texts which con-
tain &amp;quot;Loft&amp;quot; with different senses such as an attic
room, an angle of golf club face, a movie title, a
name of a club with live music and so on. The
words in Web search queries are often proper
nouns. Then it is not trivial to discriminate these
</bodyText>
<note confidence="0.8043585">
Nobuhiro Kaji, Masashi Toyoda
and Masaru Kitsuregawa
</note>
<affiliation confidence="0.9966775">
Institute of Industrial Science,
The University of Tokyo
</affiliation>
<address confidence="0.839131">
4-6-1 Komaba, Meguro-ku Tokyo, Japan
</address>
<email confidence="0.9495725">
{kaji,toyoda,kitsure)
@tkl.iis.u-tokyo.ac.jp
</email>
<bodyText confidence="0.999734025">
senses especially for the language like Japanese
whose proper nouns are not capitalized.
To train WSD systems we need a large
amount of positive and negative examples. In the
real Web mining application, how to acquire
training data for a various target of analysis has
become a major hurdle to use supervised WSD.
Fortunately, it is not so difficult to create posi-
tive examples. We can retrieve positive examples
from Web archive with high precision (but low
recall) by manually augmenting queries with hy-
pernyms or semantically related words (e.g.,
&amp;quot;Loft AND shop&amp;quot; or &amp;quot;Loft AND stationary&amp;quot;).
On the other hand, it is often costly to create
negative examples. In principle, we can create
negative examples in the same way as we did to
create positive ones. The problem is, however,
that we are not sure of most of the senses of a
target word. Because target words are often
proper nouns, their word senses are rarely listed
in hand-crafted lexicon. In addition, since the
Web is huge and contains heterogeneous do-
mains, we often find a large number of unex-
pected senses. For example, all the authors did
not know the music club meaning of Loft. As the
result, we often had to spend much time to find
such unexpected meaning of target words.
This situation motivated us to study active
learning for WSD starting with only positive ex-
amples. The previous techniques (Chan and Ng,
2007; Chen et al. 2006) require balanced positive
and negative examples to estimate the score. In
our problem setting, however, we have no nega-
tive examples at the initial stage. To tackle this
problem, we propose a method of active learning
for WSD with pseudo negative examples, which
are selected from unlabeled data by a classifier
trained with positive and unlabeled examples.
McCallum and Nigam (1998) combined active
learning and semi-supervised learning technique
</bodyText>
<page confidence="0.993027">
61
</page>
<note confidence="0.9257725">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61–64,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999657333333333">
by using EM with unlabeled data integrated into
active learning, but it did not treat our problem
setting where only positive examples are given.
The construction of this paper is as follows;
Section 2 describes a proposed learning algo-
rithm. Section 3 shows the experimental results.
</bodyText>
<sectionHeader confidence="0.939963" genericHeader="method">
2 Learning Starting with Positive and
Unlabeled Examples for WSD
</sectionHeader>
<bodyText confidence="0.9999835">
We treat WSD problem as binary classification
where desired texts are positive examples and
other texts are negative examples. This setting is
practical, because ambiguous senses other than
the expected sense are difficult to know and are
no concern in most Web mining applications.
</bodyText>
<subsectionHeader confidence="0.865741">
2.1 Classifier
</subsectionHeader>
<bodyText confidence="0.9995746">
For our experiment, we use naive Bayes classifi-
ers as learning algorithm. In performing WSD,
the sense “s” is assigned to an example charac-
terized with the probability of linguistic features
f1,...,fn so as to maximize:
</bodyText>
<equation confidence="0.993644666666667">
n
p(sTI p(fj  |s) (1)
i=1
</equation>
<bodyText confidence="0.999930384615385">
The sense s is positive when it is the target
meaning in Web mining application, otherwise s
is negative. We use the following typical linguis-
tic features for Japanese sentence analysis, (a)
Word feature within sentences, (b) Preceding
word feature within bunsetsu (Japanese base
phrase), (c) Backward word feature within bun-
setsu, (d) Modifier bunsetsu feature and (e)
Modifiee bunsetsu feature.
Using naive Bayes classifier, we can estimate
the confidence score c(d, s) that the sense of a
data instance “d”, whose features are f1, f2, ..., fn,
is predicted sense “s”.
</bodyText>
<equation confidence="0.991545666666667">
n
c(d, s) = log p(s) + ∑ log p(fj  |s) (2)
i=1
</equation>
<subsectionHeader confidence="0.996683">
2.2 Proposed Algorithm
</subsectionHeader>
<bodyText confidence="0.993855529411765">
At the beginning of our algorithm, the system is
provided with positive examples and unlabeled
examples. The positive examples are collected
by full text queries with hypernyms or semanti-
cally related words.
First we select positive dataset P from initial
dataset by manually augmenting full text query.
At each iteration of active learning, we select
pseudo negative dataset Np (Figure 1 line 15). In
selecting pseudo negative dataset, we predict
word sense of each unlabeled example using the
naive Bayes classifier with all the unlabeled ex-
amples as negative examples (Figure 2). In detail,
if the prediction score (equation(3)) is more than
, which means the example is very likely to be
negative, it is considered as the pseudo negative
example (Figure 2 line 10-12).
</bodyText>
<figure confidence="0.984919411764706">
c(d, psdNeg) = c(d, neg) − c(d, pos) (3)
01 # Definition
02 (P, N): WSD system trained on P as Positive
03 examples, N as Negative examples.
04 EM(P, N, U): WSD system trained on P as
05 Positive examples, N as Negative examples,
06 U as Unlabeled examples by using EM
07 (Nigam et. all 2000)
08 # Input
09 T Initial unlabeled dataset which contain
10 ambiguous words
11 # Initialization
12 P positive training dataset by full text search on T
13 N (initial negative training dataset)
14 repeat
15 # selecting pseudo negative examples Np
16 by the score of (P, T-P) (see figure 2)
17 # building a classifier with Np
18 new EM (P, N+Np, T-N-P)
19 # sampling data by using the score of new
20 cmin
21 foreach d (T – P – N )
22 classify d by WSD system new
23 s(d) word sense prediction for d using new
24 c(d, s(d)) the confidence of prediction of d
25 if c(d, s(d)) cmin then
26 cmin c(d), d min d
27 end
28 end
29 provide correct sense s for d min by human
30 if s is positive then add d min to P
31 else add d min to N
32 until Training dataset reaches desirable size
33 new is the output classifier
</figure>
<figureCaption confidence="0.891721">
Figure 1: A combination of active learning and
semi-supervised learning starting with positive
and unlabeled examples
</figureCaption>
<bodyText confidence="0.996287727272727">
Next we use Nigam’s semi-supervised learning
method using EM and a naive Bayes classifier
(Nigam et. all, 2000) with pseudo negative data-
set Np as negative training dataset to build the
refined classifier ΓEM (Figure 1 line 17).
In building training dataset by active learning,
we use uncertainty sampling like (Chan and Ng,
2007) (Figure 1 line 30-31). This step selects the
most uncertain example that is predicted with the
lowest confidence in the refined classifier ΓEM.
Then, the correct sense for the most uncertain
</bodyText>
<page confidence="0.996302">
62
</page>
<bodyText confidence="0.9946126">
example is provided by human and added to the
positive dataset P or the negative dataset N ac-
cording to the sense of d.
The above steps are repeated until dataset
reaches the predefined desirable size.
</bodyText>
<figure confidence="0.988674615384615">
01 foreach d ( T – P – N )
02 classify d by WSD system (P, T-P)
03 c(d, pos) the confidence score that d is
04 predicted as positive defined in equation (2)
05 c(d, neg) the confidence score that d is
06 predicted as negative defined in equation (2)
07 c(d, psdNeg) = c(d, neg) - c(d, pos)
08 (the confidence score that d is
09 predicted as pseudo negative)
10 PN d ( T – P – N )  |s(d) = neg
11 c(d, psdNeg) }
12 (PN is pseudo negative dataset )
13 end
</figure>
<figureCaption confidence="0.999915">
Figure 2: Selection of pseudo negative examples
</figureCaption>
<sectionHeader confidence="0.99953" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999662">
3.1 Data and Condition of Experiments
</subsectionHeader>
<bodyText confidence="0.966716666666667">
We select several example data sets from Japa-
nese blog data crawled from Web. Table 1 shows
the ambiguous words and each ambiguous senses.
Word Positive sense Other ambiguous senses
Wega product name Las Vegas, football team
(TV) name, nickname, star, horse
race, Baccarat glass, atelier,
wine, game, music
Loft store name attic room, angle of golf
club face, club with live
music, movie
Honda personal name Personal names (actress,
(football player) artists, other football play-
ers, etc.) hardware store, car
company name
Tsubaki product name flower name, kimono, horse
(shampoo) race, camellia ingredient,
shop name
</bodyText>
<tableCaption confidence="0.995888">
Table 1: Selected examples for evaluation
</tableCaption>
<bodyText confidence="0.9987742">
Table 2 shows the ambiguous words, the num-
ber of its senses, the number of its data instances,
the number of feature, and the percentage of
positive sense instances for each data set.
Assigning the correct labels of data instances is
done by one person and 48.5% of all the labels
are checked by another person. The percentage
of agreement between 2 persons for the assigned
labels is 99.0%. The average time of assigning
labels is 35 minutes per 100 instances.
Selected instances for evaluation are randomly
divided 10% test set and 90% training set. Table
3 shows the each full text search query and the
number of initial positive examples and the per-
centage of it in the training data set.
</bodyText>
<note confidence="0.495751">
word No. of No. of No. of Percentage of
</note>
<table confidence="0.9879914">
senses instances features positive sense
Wega 11 5,372 164,617 31.1%
Loft 5 1,582 38,491 39.4%
Honda 25 2,100 65,687 21.2%
Tsubaki 6 2,022 47,629 40.2%
</table>
<tableCaption confidence="0.966943">
Table 2: Selected examples for evaluation
</tableCaption>
<table confidence="0.999689875">
word Full text query for initial No. of positive
positive examples examples (percent-
age in trainig set)
Wega Wega AND TV 316 (6.5%)
Loft Loft AND (Grocery OR- 64 (4.5%)
Stationery)
Honda Honda AND Keisuke 86 (4.6%)
Tsubaki Tsubaki AND Shiseido 380 (20.9%)
</table>
<tableCaption confidence="0.999571">
Table 3: Initial positive examples
</tableCaption>
<bodyText confidence="0.999011">
The threshold value in figure 2 is set to em-
pirically optimized value 50. Dependency on
threshold value will be discussed in 3.3.
</bodyText>
<subsectionHeader confidence="0.999567">
3.2 Comparison Results
</subsectionHeader>
<bodyText confidence="0.843324">
Figure 3 shows the average WSD accuracy of
the following 6 approaches.
</bodyText>
<figure confidence="0.716893">
0 10 20 30 40 50 60 70 80 90 100
</figure>
<figureCaption confidence="0.999556">
Figure 3: Average active learning process
</figureCaption>
<bodyText confidence="0.999178545454546">
B-clustering is a standard unsupervised WSD, a
clustering using naive Bayes classifier learned
with two cluster numbers via EM algorithm. The
given number of the clusters are two, negative
and positive datasets.
M-clustering is a variant of b-clustering where
the given number of clusters are each number of
ambiguous word senses in table 2.
Human labeling, abbreviated as human, is an
active learning approach starting with human
labeled negative examples. The number of hu-
</bodyText>
<figure confidence="0.938385631578947">
91
89
87
human
with-EM
without-EM
random
m-clustering
b-clustering
62
77
58
56
85
83
66
81
64
79
</figure>
<page confidence="0.998082">
63
</page>
<bodyText confidence="0.99855787037037">
man labeled negative examples in initial training
data is the same as that of positive examples in
figure 3. Human labeling is considered to be the
upper accuracy in the variants of selecting
pseudo negative examples.
Random sampling with EM, abbreviated as
with-EM, is the variant approach where dmin in
line 26 of figure 1 is randomly selected without
using confidence score.
Uncertainty sampling without EM (Takayama
et al. 2009), abbreviated as without-EM, is a vari-
ant approach where EM (P, N+Np, T-N-P) in
line 18 of figure 1 is replaced by (P, N+Np).
Uncertainty Sampling with EM, abbreviated as un-
certain, is a proposed method described in figure 1.
The accuracy of the proposed approach with-
EM is gradually increasing according to the per-
centage of added hand labeled examples.
The initial accuracy of with-EM, which means
the accuracy with no hand labeled negative ex-
amples, is the best score 81.4% except for that of
human. The initial WSD accuracy of with-EM is
23.4 and 4.2 percentage points higher than those
of b-clustering (58.0%) and m-clustering
(77.2%), respectively. This result shows that the
proposed selecting method of pseudo negative
examples is effective.
The initial WSD accuracy of with-EM is 1.3
percentage points higher than that of without-EM
(80.1%). This result suggests semi-supervised
learning using unlabeled examples is effective.
The accuracies of with-EM, random and with-
out-EM are gradually increasing according to the
percentage of added hand labeled examples and
catch up that of human and converge at 30 per-
centage added points. This result suggests that
our proposed approach can reduce the labor cost
of assigning correct labels.
The curve with-EM are slightly upper than the
curve random at the initial stage of active learn-
ing. At 20 percentage added point, the accuracy
with-EM is 87.0 %, 1.1 percentage points higher
than that of random (85.9%). This result suggests
that the effectiveness of proposed uncertainty
sampling method is not remarkable depending on
the word distribution of target data.
There is really not much difference between the
curve with-EM and without-EM. As a classifies
to use the score for sampling examples in adapta-
tion iterations, it is indifferent whether with-EM
or without-EM.
Larger evaluation is the future issue to confirm
if the above results could be generalized beyond
the above four examples used as proper nouns.
</bodyText>
<subsectionHeader confidence="0.994533">
3.3 Dependency on Threshold Value τ
</subsectionHeader>
<bodyText confidence="0.999946333333333">
Figure 4 shows the average WSD accuracies of
with-EM at 0, 25, 50 and 75 as the values of .
The each curve represents our proposed algorithm
with threshold value in the parenthesis. The
accuracy in the case of = 75 is higher than that
of = 50 over 20 percentage data added point.
This result suggests that as the number of hand
labeled negative examples increasing, should
be gradually decreasing, that is, the number of
pseudo negative examples should be decreasing.
Because, if sufficient number of hand labeled
negative examples exist, a classifier does not need
pseudo negative examples. The control of
depending on the number of hand labeled examples
during active learning iterations is a future issue.
</bodyText>
<figure confidence="0.827564">
0 10 20 30 40 50 60 70 80 90 100
</figure>
<figureCaption confidence="0.999715">
Figure 4: Dependency of threshold value
</figureCaption>
<sectionHeader confidence="0.997204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999317045454545">
Chan, Y. S. and Ng, H. T. 2007. Domain Adaptation
with Active Learning for Word Sense Disambigua-
tion. Proc. of ACL 2007, 49-56.
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006.
An Empirical Study of the Behavior of Active
Learning for Word Sense Disambiguation, Proc. of
the main conference on Human Language Tech-
nology Conference of the North American Chapter
of ACL, pp. 120-127.
McCallum, A. and Nigam, K. 1998. Employing EM
and Pool-Based Active Learning for Text Classifi-
cation. Proceedings of the Fifteenth international
Conference on Machine Learning, 350-358.
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39,
103-134.
Takayama, Y., Imamura, M., Kaji N., Toyoda, M. and
Kitsuregawa, M. 2009. Active Learning with
Pseudo Negative Examples for Word Sense Dis-
ambiguation in Web Mining (in Japanese), Journal
of IPSJ (in printing).
</reference>
<figure confidence="0.995244076923077">
92
90
88
86
84
82
80
78
76
z= 0.0
z= 25.0
z= 50.0
z= 75.0
</figure>
<page confidence="0.966516">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434289">
<title confidence="0.978008333333333">A Combination of Active Learning and Semi-supervised Learning Starting with Positive and Unlabeled Examples for Word Sense Disambiguation: An Empirical Study on Japanese Web Search Query</title>
<author confidence="0.991121">Makoto Imamura</author>
<affiliation confidence="0.906833666666667">and Yasuhiro Takayama Information Technology R&amp;D Center, Mitsubishi Electric Corporation</affiliation>
<address confidence="0.973183">5-1-1 Ofuna, Kamakura, Kanagawa, Japan</address>
<email confidence="0.813429">{Imamura.Makoto@bx,Takayama.Yasuhiro@ea).MitsubishiElectric.co.jp</email>
<abstract confidence="0.99896285">This paper proposes to solve the bottleneck of finding training data for word sense disambiguation (WSD) in the domain of web queries, where a complete set of ambiguous word senses are unknown. In this paper, we present a combination of active learning and semi-supervised learning method to treat the case when positive examples, which have an expected word sense in web search result, are only given. The novelty of our approach is to use “pseudo negative examples” with reliable confidence score estimated by a classifier trained with positive and unlabeled examples. We show experimentally that our proposed method achieves close enough WSD accuracy to the method with the manually prepared negative examples in several Japanese Web search data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Domain Adaptation with Active Learning for Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>Proc. of ACL</booktitle>
<pages>49--56</pages>
<contexts>
<context position="3463" citStr="Chan and Ng, 2007" startWordPosition="553" endWordPosition="556">lem is, however, that we are not sure of most of the senses of a target word. Because target words are often proper nouns, their word senses are rarely listed in hand-crafted lexicon. In addition, since the Web is huge and contains heterogeneous domains, we often find a large number of unexpected senses. For example, all the authors did not know the music club meaning of Loft. As the result, we often had to spend much time to find such unexpected meaning of target words. This situation motivated us to study active learning for WSD starting with only positive examples. The previous techniques (Chan and Ng, 2007; Chen et al. 2006) require balanced positive and negative examples to estimate the score. In our problem setting, however, we have no negative examples at the initial stage. To tackle this problem, we propose a method of active learning for WSD with pseudo negative examples, which are selected from unlabeled data by a classifier trained with positive and unlabeled examples. McCallum and Nigam (1998) combined active learning and semi-supervised learning technique 61 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61–64, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP b</context>
<context position="7917" citStr="Chan and Ng, 2007" startWordPosition="1318" endWordPosition="1321">ense s for d min by human 30 if s is positive then add d min to P 31 else add d min to N 32 until Training dataset reaches desirable size 33 new is the output classifier Figure 1: A combination of active learning and semi-supervised learning starting with positive and unlabeled examples Next we use Nigam’s semi-supervised learning method using EM and a naive Bayes classifier (Nigam et. all, 2000) with pseudo negative dataset Np as negative training dataset to build the refined classifier ΓEM (Figure 1 line 17). In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). This step selects the most uncertain example that is predicted with the lowest confidence in the refined classifier ΓEM. Then, the correct sense for the most uncertain 62 example is provided by human and added to the positive dataset P or the negative dataset N according to the sense of d. The above steps are repeated until dataset reaches the predefined desirable size. 01 foreach d ( T – P – N ) 02 classify d by WSD system (P, T-P) 03 c(d, pos) the confidence score that d is 04 predicted as positive defined in equation (2) 05 c(d, neg) the confidence score that d is 06</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Chan, Y. S. and Ng, H. T. 2007. Domain Adaptation with Active Learning for Word Sense Disambiguation. Proc. of ACL 2007, 49-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>A Schein</author>
<author>L Ungar</author>
<author>M Palmer</author>
</authors>
<title>An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation,</title>
<date>2006</date>
<booktitle>Proc. of the main conference on Human Language Technology Conference of the North American Chapter of ACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="3482" citStr="Chen et al. 2006" startWordPosition="557" endWordPosition="560">at we are not sure of most of the senses of a target word. Because target words are often proper nouns, their word senses are rarely listed in hand-crafted lexicon. In addition, since the Web is huge and contains heterogeneous domains, we often find a large number of unexpected senses. For example, all the authors did not know the music club meaning of Loft. As the result, we often had to spend much time to find such unexpected meaning of target words. This situation motivated us to study active learning for WSD starting with only positive examples. The previous techniques (Chan and Ng, 2007; Chen et al. 2006) require balanced positive and negative examples to estimate the score. In our problem setting, however, we have no negative examples at the initial stage. To tackle this problem, we propose a method of active learning for WSD with pseudo negative examples, which are selected from unlabeled data by a classifier trained with positive and unlabeled examples. McCallum and Nigam (1998) combined active learning and semi-supervised learning technique 61 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61–64, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP by using EM with unl</context>
</contexts>
<marker>Chen, Schein, Ungar, Palmer, 2006</marker>
<rawString>Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation, Proc. of the main conference on Human Language Technology Conference of the North American Chapter of ACL, pp. 120-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM and Pool-Based Active Learning for Text Classification.</title>
<date>1998</date>
<booktitle>Proceedings of the Fifteenth international Conference on Machine Learning,</booktitle>
<pages>350--358</pages>
<contexts>
<context position="3866" citStr="McCallum and Nigam (1998)" startWordPosition="618" endWordPosition="621">we often had to spend much time to find such unexpected meaning of target words. This situation motivated us to study active learning for WSD starting with only positive examples. The previous techniques (Chan and Ng, 2007; Chen et al. 2006) require balanced positive and negative examples to estimate the score. In our problem setting, however, we have no negative examples at the initial stage. To tackle this problem, we propose a method of active learning for WSD with pseudo negative examples, which are selected from unlabeled data by a classifier trained with positive and unlabeled examples. McCallum and Nigam (1998) combined active learning and semi-supervised learning technique 61 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61–64, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP by using EM with unlabeled data integrated into active learning, but it did not treat our problem setting where only positive examples are given. The construction of this paper is as follows; Section 2 describes a proposed learning algorithm. Section 3 shows the experimental results. 2 Learning Starting with Positive and Unlabeled Examples for WSD We treat WSD problem as binary classification where de</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>McCallum, A. and Nigam, K. 1998. Employing EM and Pool-Based Active Learning for Text Classification. Proceedings of the Fifteenth international Conference on Machine Learning, 350-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM,</title>
<date>2000</date>
<journal>Machine Learning,</journal>
<volume>39</volume>
<pages>103--134</pages>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Nigam, K., McCallum, A., Thrun, S., and Mitchell, T. 2000. Text Classification from Labeled and Unlabeled Documents using EM, Machine Learning, 39, 103-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Takayama</author>
<author>M Imamura</author>
<author>N Kaji</author>
<author>M Toyoda</author>
<author>M Kitsuregawa</author>
</authors>
<title>Active Learning with Pseudo Negative Examples for Word Sense Disambiguation in Web Mining (in Japanese),</title>
<date>2009</date>
<journal>Journal of IPSJ</journal>
<note>(in printing).</note>
<contexts>
<context position="12059" citStr="Takayama et al. 2009" startWordPosition="2032" endWordPosition="2035"> active learning approach starting with human labeled negative examples. The number of hu91 89 87 human with-EM without-EM random m-clustering b-clustering 62 77 58 56 85 83 66 81 64 79 63 man labeled negative examples in initial training data is the same as that of positive examples in figure 3. Human labeling is considered to be the upper accuracy in the variants of selecting pseudo negative examples. Random sampling with EM, abbreviated as with-EM, is the variant approach where dmin in line 26 of figure 1 is randomly selected without using confidence score. Uncertainty sampling without EM (Takayama et al. 2009), abbreviated as without-EM, is a variant approach where EM (P, N+Np, T-N-P) in line 18 of figure 1 is replaced by (P, N+Np). Uncertainty Sampling with EM, abbreviated as uncertain, is a proposed method described in figure 1. The accuracy of the proposed approach withEM is gradually increasing according to the percentage of added hand labeled examples. The initial accuracy of with-EM, which means the accuracy with no hand labeled negative examples, is the best score 81.4% except for that of human. The initial WSD accuracy of with-EM is 23.4 and 4.2 percentage points higher than those of b-clus</context>
</contexts>
<marker>Takayama, Imamura, Kaji, Toyoda, Kitsuregawa, 2009</marker>
<rawString>Takayama, Y., Imamura, M., Kaji N., Toyoda, M. and Kitsuregawa, M. 2009. Active Learning with Pseudo Negative Examples for Word Sense Disambiguation in Web Mining (in Japanese), Journal of IPSJ (in printing).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>