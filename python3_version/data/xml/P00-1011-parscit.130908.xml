<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000928">
<title confidence="0.911308">
Tagging Unknown Proper Names Using Decision Trees
</title>
<author confidence="0.549107">
Frederic Bechet and Alexis Nasr and Franck Genee
t LIA Universite d&apos;Avignon (f rederic . bechet@lia.univ-avignon.fr)
t LIM Universite Aix-Marseille 2 (alexis nasr@lim univ-mrs .f r)
</author>
<sectionHeader confidence="0.890124" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951785714286">
This paper describes a supervised learn-
ing method to automatically select from
a set of noun phrases, embedding proper
names of different semantic classes, their
most distinctive features. The result of
the learning process is a decision tree
which classifies an unknown proper name
on the basis of its context of occur-
rence. This classifier is used to esti-
mate the probability distribution of an
out of vocabulary proper name over a
tagset. This probability distribution is
itself used to estimate the parameters of
a stochastic part of speech tagger.
</bodyText>
<sectionHeader confidence="0.996301" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9956215">
The work described in this paper aims at enrich-
ing lexica with new proper names. In such lexica,
every word w is assigned a count distribution over
the different tags of the tagger tagset (the number
of times w was labelled with tag t). This distribu-
tion is called the count distribution of the word.
The produced lexica are used to estimate the pa-
rameters of a POS stochastic tagger.
We will concentrate on proper names in a news-
paper corpus (Le Monde 1987-1992), although the
techniques described can be used for any category
of words. The decision to concentrate on proper
names follows from the fact that although proper
names represent only a moderate proportion of
the words occurrences in such corpora (3.67%),
the probability of an out of vocabulary (00V)
word being a proper name is high. (Bechet and
Yvon, 2000) report experiments concerning 00V
proper names on a very close corpus (Le monde
diplomatique 1987-1995). They showed that 72%
of 00V words with respect to a newspaper 265 K
words lexicon are potentially proper names. Fur-
thermore, the same experiments showed that 30%
of the sentences contain at least one 00V word.
Besides, proper names are important for many
tasks as information retrieval or named entities
extraction.
The technique described has two stages. During
the first stage, a training corpus is used to grow
decision trees of a special kind called Semantic
Classification Trees (SCT). Such trees model the
salient features of the contexts in which words of
a given semantic class occur. In a second stage,
SCTs are used to update the lexical entries of
00V words appearing in a test corpus, based on
their different context of occurrence in the test
corpus. The updated lexicon is then used to esti-
mate the parameters of a POS tagger.
The paper is structured as follows. In section 2
the tagger and the tagset used are described, sec-
tion 3 introduces Semantic Classification Trees,
section 4 describes how an SCT is built and sec-
tion 5 how it is used to estimate the parameters of
a POS tagger. The performances of the method
are given in section 5.1, section 6 briefly describes
other approaches to deal with 00V words and
section 7 concludes the paper with some future
work.
2 The tagger and the tagset
The POS tagger we use (Spriet and El-beze, 1995)
is based on the standard trigram model(Charniak
et al., 1993):
</bodyText>
<equation confidence="0.9994025">
T(W1,n) = arg max TT P(ti I ti_2ti_i)P(wi Iti)
t1,, &amp;quot;I&amp;quot; 1-
i=1
(1)
</equation>
<bodyText confidence="0.999815">
where T(wi,n) is a sequence of n POS tags
corresponding to the sequence of words wi,n•
The second term of the product of equation 1
(P(wilti)) is estimated using the count distribu-
tions stored in the lexicon.
The tagger was trained on the newspaper Le
Monde between years 87 and 91. It uses a 265K
</bodyText>
<figure confidence="0.953117352941176">
&lt;+président+&gt;?
yes
no
&lt;+président+groupe&gt; ?
&lt;+secrétaire+&gt; ?
yes no
yes no
ORG : 0.8
FAMILY:0.2
&lt;+ville+&gt;?
&lt;+secrétaire général+&gt;?
yes no
&lt;+ville de+&gt;?
Then the finite-state parser will isolate the fol-
lowing NPs :
[(the,DETS)(president,NS)(of,PREP)(SONY,ORG)]
[(an,DETS)(interview,NS)]
</figure>
<bodyText confidence="0.9672784">
among which the NP the president of SONY will
be kept since it contains a proper name (SONY)
recognized as an organization name (oRG). The
NP is stored in the sample corpus under the for-
mat :
</bodyText>
<equation confidence="0.760136">
(the,DET)(president,N)(of,PREP) (XXXX,PN)=ORG
</equation>
<bodyText confidence="0.9998544">
Where the proper name is replaced by the sym-
bol XXXX and its tag becomes PN, for proper
name. This is to indicate that a proper name la-
belled ORG has occurred in this context.
At the end of this process a set of samples for
each class of proper names has been built and the
NPs whose number of occurrences exceeds a given
threshold, are kept. This set of samples consti-
tutes the training corpus on which the decision
tree will be built.
</bodyText>
<subsectionHeader confidence="0.997735">
4.2 Set of questions
</subsectionHeader>
<bodyText confidence="0.9993576">
The original aspect of SCT is the way in which the
set of possible questions is generated. These ques-
tions ask whether a sequence of words and POS
tags matches a certain regular expression involv-
ing words, POS and gaps.
During the growing process of the SCT, each
node of the tree is associated with a regular ex-
pression called the Known Structure (KS) and a
set of samples containing all the NPs from the
sample corpus which satisfy this regular expres-
sion. At the beginning of the growing process, the
root of the tree is associated to the KS &lt; +&gt; and
to the entire training corpus. A KS also records
the position of the last item that was introduced
in it.
The KS of a node and the set L composed of
the lexical entries of the lexicon and the tagset
will give rise to several new regular expressions by
replacing in KS a gap with elements of L. More
precisely:
</bodyText>
<listItem confidence="0.975018333333333">
• each element i of L produces four different
patterns: {i},{±i},{i±},{+i+}
• each of the generated patterns replaces in KS
the gap situated respectively at the right and
at the left of the element of the last item in-
troduced.
</listItem>
<bodyText confidence="0.996269142857143">
With this method, a given KS generates a max-
imum of 4x ILI x 2 regular expressions. A 2K word
lexicon and a tagset containing 100 POS tags,
will produce for each KS, 4 x (2000 + 100) x 2 =
16.8K different new regular expression. The KS:
&lt; +president+ &gt;, and the lexical item of will
produce the eight following regular expressions:
</bodyText>
<listItem confidence="0.95539725">
&lt; of president+ &gt; &lt; +o f president+ &gt;
&lt; o f + president+ &gt; &lt; +o f + president+ &gt;
&lt; +president of &gt; &lt; +president + o f &gt;
&lt; +president of +&gt; &lt; +president + of +&gt;
</listItem>
<bodyText confidence="0.9962596">
Each regular expression splits the set of samples
associated to the node in two: the set of the sam-
ples that match the regular expression and the set
of those that don&apos;t. A regular expression is there-
fore seen as a yes-no question.
</bodyText>
<subsectionHeader confidence="0.99546">
4.3 Split criteria
</subsectionHeader>
<bodyText confidence="0.999947625">
The choice of the regular expression that will be
associated to a node, is made in accordance with
the Gini impurity criteria (Breiman et al., 1984).
The best question (here regular expression) is the
one which brings the maximum drop in impurity
between the node and its children. If the two chil-
dren of a node T are called Tyes and Tno, the drop
of impurity A/ is defined as:
</bodyText>
<equation confidence="0.969356666666667">
ITyesi ITnol
= 1(T)
ITI ITI 1(Tyes) I(T0) (2)
</equation>
<bodyText confidence="0.940008">
Impurity of a node t with i and j ranging over
the tagset is computed with following formula:
</bodyText>
<equation confidence="0.775173">
/co =Ep(iit)p(ilt) (3)
</equation>
<bodyText confidence="0.999975">
where Ai It) is estimated with the relative fre-
quency of samples labelled with tag i in t.
The question associated to node t will be the
question which maximises M.
</bodyText>
<subsectionHeader confidence="0.99308">
4.4 Stop condition
</subsectionHeader>
<bodyText confidence="0.999906785714286">
A node of the tree is not further split if either the
impurity of the node is below a threshold or the
number of samples left in a node equals 1.
At the end of this training process a tree has
been built. Each node is associated to a regular
expression made with words, POS and gaps. Each
leaf contains a set of samples from the training
corpus : those that match the regular expression
represented by the leaf. From this set, a proba-
bility distribution over the different proper names
semantic classes is computed. For example, if a
leaf contains 100 samples, 90 of which are labelled
with the tag TOWN and 10 labelled with ORG, the
class TOWN will receive the probability 0.9 and
class ORG the probability 0.1. The more uniform
this distribution is, the less representative of a se-
mantic class the leaf regular expression will be.
The shape of a node distribution allows us to
distinguish the leaves with respect to their abil-
ity to discriminate one semantic class, such leaves
will be called discriminant. A leaf will be consid-
ered discriminant when the probability of its top
category exceeds a certain threshold. The proba-
bility of the top category will be called discrimi-
nance of the leaf and the threshold will be called
the minimum discriminance threshold. When the
minimum discriminance threshold is set to 0, all
leaves are considered discriminant.
</bodyText>
<subsectionHeader confidence="0.881212">
4.5 Estimating the quality of an SCT
</subsectionHeader>
<bodyText confidence="0.999765764705882">
The quality of an SCT has been evaluated by test-
ing its ability to correctly tag a proper name oc-
currence according to the NP in which it appears.
A test corpus of labelled NPs has been gathered
and each NP has been given a tag by the SCT.
The tagging process is straightforward, it consists
on traversing the tree, starting at the root un-
til a leaf is reached. For each node N visited, if
the NP matches the regular expression of N, the
next node to visit is the daughter of N labelled
yes, otherwise, it is the daughter labelled no. If
the leaf reached is discriminant (its discriminance
is above the minimum discriminance threshold),
the embedded proper name is labelled with the
top category. If the leaf is not discriminant, the
proper name is not given any tag. Three figures
have been computed:
</bodyText>
<listItem confidence="0.989669083333333">
• the precision, which is the number of proper
name occurrences correctly tagged divided
by the number of proper name occurrences
tagged;
• the syntactic coverage, which is the propor-
tion of NPs occurrences in the test corpus
that match a discriminant leaf&apos;s regular ex-
pression;
• the lexical coverage, which is the number of
different proper names tagged at least one
time by the SCT divided by the total number
of different proper names in the test corpus
</listItem>
<bodyText confidence="0.991234666666667">
The training corpus is composed of NPs ex-
tracted from the newspaper Le Monde, between
years 1987 and 1991 which constitute a corpus of
almost 98 M words and a vocabulary of almost 400
Kwords. This corpus has been processed following
the different steps described in section 4.1. In or-
der to limit spurious NPs due to wrong PP attach-
ments, we have limited the coverage of the gram-
mar to NPs containing, at most, one PP attach-
ment. The length of the NPs actually detected
range from 1 to 12 words.
We kept, in the sample corpus, all the NPs oc-
curring at least four times. This represents a set of
107K different NPs. The regular expressions were
built on the vocabulary made of the 105 tags of the
tagset and the 11K words of the sample corpus. At
the end of the growing process, a tree containing
21K nodes is obtained 1. Here are some examples
of the regular expressions attached to the leaves
of the tree (the proper name is represented by the
letter X) :
</bodyText>
<table confidence="0.64275475">
+president++administration de DET X+ =XSOC
+president PREP gouvernement de DET X+ =XPAY
+president PREP directoire de DET X + =XSOC
+le+ +president PREP + + de DET X+ =XSOC
</table>
<bodyText confidence="0.9944735">
All these examples correspond to nodes where
the discriminance value is maximal (equal to 1).
Experiments have been conducted on two dif-
ferent test corpora :
</bodyText>
<listItem confidence="0.878036470588235">
• Co is made of 1.2M NPs from the newspa-
per Le Monde for the years 1991 and 1992.
Each NP contains a proper name appear-
ing in the lexicon, representing 4.6K different
proper names. This test corpus is therefore
close to the training corpus (although they
correspond to different years), the aim of the
experiment is to test the ability of the SCT
to model the training data.
• C1 is made of 695 NPs containing 282 differ-
ent proper names appearing no more than 4
times in the years 91 and 92 of the newspaper
Le Monde. The aim of this experiment is to
test the ability of the SCT to correctly tag
low frequency proper names (We make the
assumption that real 00V proper names are
sparse).
</listItem>
<bodyText confidence="0.999900933333333">
The results of the experiment on corpus Co are
reported in figure 2, this figure shows coverage
and precision with respect to the minimum dis-
criminance threshold. These results show that a
good precision can be reached by raising the min-
imum discriminance threshold. Raising the mini-
mum discriminance threshold tends on the other
hand to lower coverage.
The three different measures allow to draw dif-
ferent conclusions on this experiment.
The shape of the precision curve indicates that
contexts that have showed to be discriminant on
the training corpus led to a correct tagging on the
test corpus. The shape of the syntactic coverage
curve shows on another hand that an important
</bodyText>
<footnote confidence="0.750094666666667">
1-The SCT was built using a toolkit developped in
the framework of the SMADA project funded by the
European Commission (Boves et al., 2000).
</footnote>
<figure confidence="0.992093428571429">
100
90
80
70
60
❼
score
o
❼
50
40
30
precision
syntactic coverage
lexical coverage
20
10
0
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
discriminance threshold
</figure>
<bodyText confidence="0.999944090909091">
This method relies on the implicit assumption
that a proper name appearing in an ambiguous
context tends to be ambiguous and that its lexical
distribution reflects the probability distribution
(as estimated by the SCT) of the context in which
it appears. This assumption is of course too strong
and non ambiguous words can appear in an am-
biguous context. The proper name Lebanon, for
example is not very ambiguous while the context
the president of X, in which Lebanon can appear,
is. Estimating the ambiguity of Lebanon with the
ambiguity associated to the context the president
of X is not very convincing and the assumption
introduces a bias in the method. The importance
of this bias is nevertheless attenuated by the fact
that an 00V word can appear in several different
contexts, in which case, its lexical distribution, as
estimated by the SCT, will be less dependent of
one context. The bias can be further attenuated
by raising the parameter Sd, and therefore take
into account low ambiguity contexts only in esti-
mating the lexical distribution of a proper name.
</bodyText>
<subsectionHeader confidence="0.931619">
5.1 Experiments
</subsectionHeader>
<bodyText confidence="0.9999724375">
The approach described in section 5 has been com-
pared to a standard technique for handling 00V
words in POS tagging, which constitutes a base-
line model. Experiments conducted using both
techniques are reported in the following two sec-
tions. They were done on the 47.6 Kwords test
corpus T made of the sentences from which were
extracted the NPs of corpus Ci., presented in 4.5.
The experiments consisted on considering that
the elements of set E, made of the 282 proper
names of C1, were unknown to the lexicon and on
estimating their lexical entries using two differ-
ent techniques. After the new lexical distribution
were computed, T was tagged again, yielding the
tagged corpora Tbase and TscT and the accuracy
of the tagging computed.
</bodyText>
<subsubsectionHeader confidence="0.937072">
5.1.1 Baseline model
</subsubsectionHeader>
<bodyText confidence="0.999995461538462">
In the baseline model, an assumption is made
that all elements of E share the same uniform lex-
ical distribution. This is a standard technique for
handling 00V words in POS tagging (Weischedel
et al., 1993). When the tagger is faced with an
00V word in a sentence, its most probable POS
tag is chosen with respect to the sole trigram prob-
ability. The accuracy on Tbase reached 67.3%,
meaning that 67.30% of the occurrences of the
words of E were given the right tag. This re-
sult is consistent (although a bit higher, due the
difference of the tagsets size) with the equivalent
experiments in (Weischedel et al., 1993).
</bodyText>
<subsubsectionHeader confidence="0.936989">
5.1.2 SCT model
</subsubsectionHeader>
<bodyText confidence="0.9996895">
In this experiment, the lexical distribution of
the elements of E are estimated following the
process described in 5, using corpus T. The re-
sults obtained strongly rely on the discriminance
threshold Sd chosen. For a given Sd, only a sub-
set E&apos; of E received new lexical distribution, the
other items remained with their uniform distribu-
tions. Of course, if Sd is set to 0, then E&apos; = E.
Tagging accuracy of TscT for different values of
Sd are reported in table 1 which also shows the rel-
ative gain compared to the baseline model. The
best result (73%) is obtained with Sd set to 0.4.
</bodyText>
<table confidence="0.989197333333333">
Sd 0.0 0.2 0.4 0.6 0.8 1
Tscy 71.3 71.3 73.0 72.31 70.8 67.5
%gain 5.9 5.9 8.5 7.5 5.2 0.3
</table>
<tableCaption confidence="0.999866">
Table 1: Accuracy of TscT limited to set E
</tableCaption>
<bodyText confidence="0.999910227272727">
The drop of accuracy observed for values of Sd
higher than 0.5 can be explained by the decrease of
the size of E&apos; when Sd increases, as shown by the
IIEE&apos;ll curve in figure 3. Figure 3 displays also accu-
racy of Tbase and TscT limited to the items of E&apos;.
These two curves allow to compare more finely the
performances of the two tagging techniques (uni-
form distribution v/s SCT estimated distribution)
since the accuracy is computed only on items of
E&apos; which, by definition of E&apos;, are tagged using
different distributions in Tbase and in TscT.
The shapes of these two curves call for two com-
ments. The curve TscT(E&apos;) is always higher than
Tbase(E&apos;), meaning that SCT estimated lexical
distributions are always better than uniform dis-
tributions. The average absolute gain of TscT(E&apos;)
over Tbase(E&apos;) reaches 9%. The curve Tbase(E&apos;)
shows an improvement of accuracy (from 67.3%
up to 88%) for the items of E&apos;. This fact indicates
that when a context has been judged discriminant
according to the SCT, it is also more discriminant
for the trigram model alone.
</bodyText>
<sectionHeader confidence="0.999858" genericHeader="introduction">
6 Related work
</sectionHeader>
<bodyText confidence="0.998433727272727">
Within the framework of Named Entity Extrac-
tion, popularized by the MUG conferences, sev-
eral methods have been proposed to handle 00V
proper names. In most of these works it is difficult
to distinguish the process of unknown words tag-
ging from the process of named entity detection.
These methods can be classified as follows: hand-
coded rules (which can also be associated with sta-
tistical methods), co-occurrences between words,
Maximum-Entropy, Decision Trees and Hidden
Markov models.
</bodyText>
<figure confidence="0.9955125">
100
90
80
70
�
score
o
�
60
50
Tbase
TSCT
|E’|/|E|
40
30
20
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
discriminance threshold
</figure>
<bodyText confidence="0.96175178125">
Such processing involves the following steps :
1. Sending a query to a search engine for each
unknown proper name we want to process.
The two parameters of the query are the
proper name as a keyword and the language
of the text.
2. From all the answers sent back by the engine,
only textual data is kept (HTML files). The
HTML tags are then removed and the text is
tokenised, tagged and parsed like the train-
ing corpus used for the building of our tree.
Eventually, the NPs containing the proper
name are kept.
3. All these NPs are processed through the tree
and only those ending up in discriminant
leaves are kept.
At the end of this process these samples are
added to the samples already appearing in the test
corpus to reestimate the POS tagger model.
We carried out a first experiment on the set
E presented in 5.1. Each item of E, processed by
the method previously described, generates on av-
erage a 3Mo HTML text corpus. After the cleaning
process, only 110Ko of HTML text is kept for each
item. Then, a corpus C2 containing the 695 NPs
of C1 in addition to the 5K NPs extracted from
the HTML corpus is built. After updating the lex-
icon E following the method described in 5 with
C2, the test corpus is tagged, yielding T2. The
tagging accuracy is reported in table 2: even if
a slight improvement is observed, with respect to
corpus Ti., the result is quite disapointing.
</bodyText>
<table confidence="0.949665">
Sd 0.0 0.2 0.4 0.6 0.8 1
Ti. 71.3 71.3 73.0 72.31 70.8 67.5
T2 72.6 72.6 73.5 73.7 73.9 70.9
</table>
<tableCaption confidence="0.998544">
Table 2: Results T1 and T2 on the set E
</tableCaption>
<bodyText confidence="0.999807">
A manual check on the data collected on the
www gave us some clues in order to explain this
poor improvement: first, even with the cleaning
process, the data collected remain very noisy (lack
of punctuation, HTML tag errors, wrong sentence
and word tokenization, etc.); second, the differ-
ences between the data used to train the tree
(newspaper articles) and those found on the www
(litterature, chat, etc.) leads the SCT to misclas-
sify proper names, even with a high discriminance
threshold.
Nevertheless, this first experiment encourage us
to carry on this way. Probing the www in order to
automatically update semantic lexicon for proper
names is a promising approach since proper names
appear and disapear every day and we believe that
an automatic searching and learning process can
help producing relevant resources which can be
directly used in any statistical NLP application.
</bodyText>
<sectionHeader confidence="0.997774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996142704545455">
Frederic Bechet and Francois Yvon. 2000. Les
noms propres en traitement automatique de la
parole. to appear in Traitement Automatigue
des Langues.
Daniel Bikel, Richard Schwartz, and Ralph
Weischedel. 1999. An algorithm that learns
what&apos;s in a name. Machine Learning - Special
Issue on NL Learning, 34,1-3.
Lou Boves, Denis Jouvet, Juergen Sienel, Renato
de Mori, Frederic Bechet, Luciano Fissore, and
Pietro Laface. 2000. Asr for automatic direc-
tory assistance : the smada project. In to ap-
pear in ISCA workshop : ASR2000, Paris.
L Breiman, J Friedman, R Ohlsen, and C Stone.
1984. Classification and Regression Trees.
Wadsworth.
Eugene Charniak, Curtis Hendrickson, Neil Ja-
cobson, and Mike Perkowitz. 1993. Equa-
tions for part-of-speech tagging. In 11th
National Conference on Artificial Intelligence,
pages 784-789.
Michael Collins and Yoram Singer. 1999. Unsu-
pervised models for named entity classification.
In Empirical Methods in NLP processing and
Very Large Corpora - EMNLP-VLC&apos;99, Univer-
sity of Maryland.
Roland Kuhn and Renato de Mori. 1996. The ap-
plication of semantic classification trees to nat-
ural language understanding. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 17(5):449-460, May.
Thierry Spriet and Marc El-beze. 1995. Eti-
quetage probabiliste et contraintes syntaxiques.
Traitement Automatigue des Langues, Vol 36,
n1-2.
Nina Wascholder, Yael Ravin, and Misook Choi.
1997. Disambiguation of proper names in
text. In Applied Natural Language Processing,
A NL P &apos;97.
Ralph Weischedel, Richard Schwartz, Jeff Pal-
mucci, Marie Meteer, and Lance Ramshaw.
1993. Coping with ambiguity and unknown
words through probabilistic models. Computa-
tional Linguistics, 19(2):359-382.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314475">
<title confidence="0.999906">Tagging Unknown Proper Names Using Decision Trees</title>
<author confidence="0.992159">Frederic Bechet</author>
<author confidence="0.992159">Alexis Nasr</author>
<author confidence="0.992159">Franck Genee</author>
<affiliation confidence="0.555555">LIA Universite d&apos;Avignon (f rederic .</affiliation>
<note confidence="0.433319">t LIM Universite Aix-Marseille 2 (alexis nasr@lim univ-mrs .f r)</note>
<abstract confidence="0.999563333333333">This paper describes a supervised learning method to automatically select from a set of noun phrases, embedding proper names of different semantic classes, their most distinctive features. The result of the learning process is a decision tree which classifies an unknown proper name on the basis of its context of occurrence. This classifier is used to estimate the probability distribution of an out of vocabulary proper name over a tagset. This probability distribution is itself used to estimate the parameters of a stochastic part of speech tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Frederic Bechet</author>
<author>Francois Yvon</author>
</authors>
<title>Les noms propres en traitement automatique de la parole. to appear in Traitement Automatigue des Langues.</title>
<date>2000</date>
<contexts>
<context position="1630" citStr="Bechet and Yvon, 2000" startWordPosition="265" endWordPosition="268"> of times w was labelled with tag t). This distribution is called the count distribution of the word. The produced lexica are used to estimate the parameters of a POS stochastic tagger. We will concentrate on proper names in a newspaper corpus (Le Monde 1987-1992), although the techniques described can be used for any category of words. The decision to concentrate on proper names follows from the fact that although proper names represent only a moderate proportion of the words occurrences in such corpora (3.67%), the probability of an out of vocabulary (00V) word being a proper name is high. (Bechet and Yvon, 2000) report experiments concerning 00V proper names on a very close corpus (Le monde diplomatique 1987-1995). They showed that 72% of 00V words with respect to a newspaper 265 K words lexicon are potentially proper names. Furthermore, the same experiments showed that 30% of the sentences contain at least one 00V word. Besides, proper names are important for many tasks as information retrieval or named entities extraction. The technique described has two stages. During the first stage, a training corpus is used to grow decision trees of a special kind called Semantic Classification Trees (SCT). Suc</context>
</contexts>
<marker>Bechet, Yvon, 2000</marker>
<rawString>Frederic Bechet and Francois Yvon. 2000. Les noms propres en traitement automatique de la parole. to appear in Traitement Automatigue des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>An algorithm that learns what&apos;s in a name.</title>
<date>1999</date>
<journal>Machine Learning - Special Issue on NL Learning,</journal>
<pages>34--1</pages>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel Bikel, Richard Schwartz, and Ralph Weischedel. 1999. An algorithm that learns what&apos;s in a name. Machine Learning - Special Issue on NL Learning, 34,1-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Boves</author>
<author>Denis Jouvet</author>
<author>Juergen Sienel</author>
<author>Renato de Mori</author>
<author>Frederic Bechet</author>
<author>Luciano Fissore</author>
<author>Pietro Laface</author>
</authors>
<title>Asr for automatic directory assistance : the smada project.</title>
<date>2000</date>
<booktitle>In to appear in ISCA workshop : ASR2000,</booktitle>
<location>Paris.</location>
<marker>Boves, Jouvet, Sienel, de Mori, Bechet, Fissore, Laface, 2000</marker>
<rawString>Lou Boves, Denis Jouvet, Juergen Sienel, Renato de Mori, Frederic Bechet, Luciano Fissore, and Pietro Laface. 2000. Asr for automatic directory assistance : the smada project. In to appear in ISCA workshop : ASR2000, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J Friedman</author>
<author>R Ohlsen</author>
<author>C Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth.</publisher>
<contexts>
<context position="6488" citStr="Breiman et al., 1984" startWordPosition="1137" endWordPosition="1140">item of will produce the eight following regular expressions: &lt; of president+ &gt; &lt; +o f president+ &gt; &lt; o f + president+ &gt; &lt; +o f + president+ &gt; &lt; +president of &gt; &lt; +president + o f &gt; &lt; +president of +&gt; &lt; +president + of +&gt; Each regular expression splits the set of samples associated to the node in two: the set of the samples that match the regular expression and the set of those that don&apos;t. A regular expression is therefore seen as a yes-no question. 4.3 Split criteria The choice of the regular expression that will be associated to a node, is made in accordance with the Gini impurity criteria (Breiman et al., 1984). The best question (here regular expression) is the one which brings the maximum drop in impurity between the node and its children. If the two children of a node T are called Tyes and Tno, the drop of impurity A/ is defined as: ITyesi ITnol = 1(T) ITI ITI 1(Tyes) I(T0) (2) Impurity of a node t with i and j ranging over the tagset is computed with following formula: /co =Ep(iit)p(ilt) (3) where Ai It) is estimated with the relative frequency of samples labelled with tag i in t. The question associated to node t will be the question which maximises M. 4.4 Stop condition A node of the tree is n</context>
</contexts>
<marker>Breiman, Friedman, Ohlsen, Stone, 1984</marker>
<rawString>L Breiman, J Friedman, R Ohlsen, and C Stone. 1984. Classification and Regression Trees. Wadsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Curtis Hendrickson</author>
<author>Neil Jacobson</author>
<author>Mike Perkowitz</author>
</authors>
<title>Equations for part-of-speech tagging.</title>
<date>1993</date>
<booktitle>In 11th National Conference on Artificial Intelligence,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="3157" citStr="Charniak et al., 1993" startWordPosition="528" endWordPosition="531"> estimate the parameters of a POS tagger. The paper is structured as follows. In section 2 the tagger and the tagset used are described, section 3 introduces Semantic Classification Trees, section 4 describes how an SCT is built and section 5 how it is used to estimate the parameters of a POS tagger. The performances of the method are given in section 5.1, section 6 briefly describes other approaches to deal with 00V words and section 7 concludes the paper with some future work. 2 The tagger and the tagset The POS tagger we use (Spriet and El-beze, 1995) is based on the standard trigram model(Charniak et al., 1993): T(W1,n) = arg max TT P(ti I ti_2ti_i)P(wi Iti) t1,, &amp;quot;I&amp;quot; 1- i=1 (1) where T(wi,n) is a sequence of n POS tags corresponding to the sequence of words wi,n• The second term of the product of equation 1 (P(wilti)) is estimated using the count distributions stored in the lexicon. The tagger was trained on the newspaper Le Monde between years 87 and 91. It uses a 265K &lt;+président+&gt;? yes no &lt;+président+groupe&gt; ? &lt;+secrétaire+&gt; ? yes no yes no ORG : 0.8 FAMILY:0.2 &lt;+ville+&gt;? &lt;+secrétaire général+&gt;? yes no &lt;+ville de+&gt;? Then the finite-state parser will isolate the following NPs : [(the,DETS)(preside</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. Equations for part-of-speech tagging. In 11th National Conference on Artificial Intelligence, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Empirical Methods in NLP processing and Very Large Corpora - EMNLP-VLC&apos;99,</booktitle>
<institution>University of Maryland.</institution>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Empirical Methods in NLP processing and Very Large Corpora - EMNLP-VLC&apos;99, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato de Mori</author>
</authors>
<title>The application of semantic classification trees to natural language understanding.</title>
<date>1996</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>17--5</pages>
<marker>Kuhn, de Mori, 1996</marker>
<rawString>Roland Kuhn and Renato de Mori. 1996. The application of semantic classification trees to natural language understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5):449-460, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Spriet</author>
<author>Marc El-beze</author>
</authors>
<title>Etiquetage probabiliste et contraintes syntaxiques.</title>
<date>1995</date>
<journal>Traitement Automatigue des Langues, Vol</journal>
<volume>36</volume>
<pages>1--2</pages>
<contexts>
<context position="3095" citStr="Spriet and El-beze, 1995" startWordPosition="518" endWordPosition="521">ccurrence in the test corpus. The updated lexicon is then used to estimate the parameters of a POS tagger. The paper is structured as follows. In section 2 the tagger and the tagset used are described, section 3 introduces Semantic Classification Trees, section 4 describes how an SCT is built and section 5 how it is used to estimate the parameters of a POS tagger. The performances of the method are given in section 5.1, section 6 briefly describes other approaches to deal with 00V words and section 7 concludes the paper with some future work. 2 The tagger and the tagset The POS tagger we use (Spriet and El-beze, 1995) is based on the standard trigram model(Charniak et al., 1993): T(W1,n) = arg max TT P(ti I ti_2ti_i)P(wi Iti) t1,, &amp;quot;I&amp;quot; 1- i=1 (1) where T(wi,n) is a sequence of n POS tags corresponding to the sequence of words wi,n• The second term of the product of equation 1 (P(wilti)) is estimated using the count distributions stored in the lexicon. The tagger was trained on the newspaper Le Monde between years 87 and 91. It uses a 265K &lt;+président+&gt;? yes no &lt;+président+groupe&gt; ? &lt;+secrétaire+&gt; ? yes no yes no ORG : 0.8 FAMILY:0.2 &lt;+ville+&gt;? &lt;+secrétaire général+&gt;? yes no &lt;+ville de+&gt;? Then the finite-sta</context>
</contexts>
<marker>Spriet, El-beze, 1995</marker>
<rawString>Thierry Spriet and Marc El-beze. 1995. Etiquetage probabiliste et contraintes syntaxiques. Traitement Automatigue des Langues, Vol 36, n1-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Wascholder</author>
<author>Yael Ravin</author>
<author>Misook Choi</author>
</authors>
<title>Disambiguation of proper names in text.</title>
<date>1997</date>
<booktitle>In Applied Natural Language Processing, A NL P &apos;97.</booktitle>
<marker>Wascholder, Ravin, Choi, 1997</marker>
<rawString>Nina Wascholder, Yael Ravin, and Misook Choi. 1997. Disambiguation of proper names in text. In Applied Natural Language Processing, A NL P &apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Richard Schwartz</author>
<author>Jeff Palmucci</author>
<author>Marie Meteer</author>
<author>Lance Ramshaw</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="14720" citStr="Weischedel et al., 1993" startWordPosition="2593" endWordPosition="2596"> NPs of corpus Ci., presented in 4.5. The experiments consisted on considering that the elements of set E, made of the 282 proper names of C1, were unknown to the lexicon and on estimating their lexical entries using two different techniques. After the new lexical distribution were computed, T was tagged again, yielding the tagged corpora Tbase and TscT and the accuracy of the tagging computed. 5.1.1 Baseline model In the baseline model, an assumption is made that all elements of E share the same uniform lexical distribution. This is a standard technique for handling 00V words in POS tagging (Weischedel et al., 1993). When the tagger is faced with an 00V word in a sentence, its most probable POS tag is chosen with respect to the sole trigram probability. The accuracy on Tbase reached 67.3%, meaning that 67.30% of the occurrences of the words of E were given the right tag. This result is consistent (although a bit higher, due the difference of the tagsets size) with the equivalent experiments in (Weischedel et al., 1993). 5.1.2 SCT model In this experiment, the lexical distribution of the elements of E are estimated following the process described in 5, using corpus T. The results obtained strongly rely on</context>
</contexts>
<marker>Weischedel, Schwartz, Palmucci, Meteer, Ramshaw, 1993</marker>
<rawString>Ralph Weischedel, Richard Schwartz, Jeff Palmucci, Marie Meteer, and Lance Ramshaw. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359-382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>