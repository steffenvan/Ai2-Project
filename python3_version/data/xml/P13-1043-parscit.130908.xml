<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.674927">
Fast and Accurate Shift-Reduce Constituent Parsing
</note>
<author confidence="0.704953">
Muhua Zhut, Yue Zhangt, Wenliang Chen*, Min Zhang* and Jingbo Zhut
</author>
<affiliation confidence="0.685839">
tNatural Language Processing Lab., Northeastern University, China
tSingapore University of Technology and Design, Singapore
* Soochow University, China and Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.936494666666667">
zhumuhua@gmail.com yue zhang@sutd.edu.sg
chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg
zhujingbo@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.994516" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997585">
Shift-reduce dependency parsers give
comparable accuracies to their chart-
based counterparts, yet the best shift-
reduce constituent parsers still lag behind
the state-of-the-art. One important reason
is the existence of unary nodes in phrase
structure trees, which leads to different
numbers of shift-reduce actions between
different outputs for the same input. This
turns out to have a large empirical impact
on the framework of global training and
beam search. We propose a simple yet
effective extension to the shift-reduce
process, which eliminates size differences
between action sequences in beam-search.
Our parser gives comparable accuracies
to the state-of-the-art chart parsers. With
linear run-time complexity, our parser is
over an order of magnitude faster than the
fastest chart parser.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958411764706">
Transition-based parsers employ a set of shift-
reduce actions and perform parsing using a se-
quence of state transitions. The pioneering mod-
els rely on a classifier to make local decisions, and
search greedily for a transition sequence to build a
parse tree. Greedy, classifier-based parsers have
been developed for both dependency grammars
(Yamada and Matsumoto, 2003; Nivre et al., 2006)
and phrase-structure grammars (Sagae and Lavie,
2005). With linear run-time complexity, they were
commonly regarded as a faster but less accurate
alternative to graph-based chart parsers (Collins,
1997; Charniak, 2000; McDonald et al., 2005).
Various methods have been proposed to address
the disadvantages of greedy local parsing, among
which a framework of beam-search and global
discriminative training have been shown effective
for dependency parsing (Zhang and Clark, 2008;
Huang and Sagae, 2010). While beam-search
reduces error propagation compared with greedy
search, a discriminative model that is globally op-
timized for whole sequences of transition actions
can avoid local score biases (Lafferty et al., 2001).
This framework preserves the most important ad-
vantage of greedy local parsers, including linear
run-time complexity and the freedom to define ar-
bitrary features. With the use of rich non-local fea-
tures, transition-based dependency parsers achieve
state-of-the-art accuracies that are comparable to
the best-graph-based parsers (Zhang and Nivre,
2011; Bohnet and Nivre, 2012). In addition, pro-
cessing tens of sentences per second (Zhang and
Nivre, 2011), these transition-based parsers can be
a favorable choice for dependency parsing.
The above global-learning and beam-search
framework can be applied to transition-based
phrase-structure (constituent) parsing also (Zhang
and Clark, 2009), maintaining all the afore-
mentioned benefits. However, the effects were
not as significant as for transition-based depen-
dency parsing. The best reported accuracies of
transition-based constituent parsers still lag behind
the state-of-the-art (Sagae and Lavie, 2006; Zhang
and Clark, 2009). One difference between phrase-
structure parsing and dependency parsing is that
for the former, parse trees with different numbers
of unary rules require different numbers of actions
to build. Hence the scoring model needs to disam-
biguate between transitions sequences with differ-
ent sizes. For the same sentence, the largest output
can take twice as many as actions to build as the
</bodyText>
<page confidence="0.986784">
434
</page>
<note confidence="0.9141135">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999867677419355">
smallest one. This turns out to have a significant
empirical impact on parsing with beam-search.
We propose an extension to the shift-reduce pro-
cess to address this problem, which gives signifi-
cant improvements to the parsing accuracies. Our
method is conceptually simple, requiring only one
additional transition action to eliminate size dif-
ferences between different candidate outputs. On
standard evaluations using both the Penn Tree-
bank and the Penn Chinese Treebank, our parser
gave higher accuracies than the Berkeley parser
(Petrov and Klein, 2007), a state-of-the-art chart
parser. In addition, our parser runs with over 89
sentences per second, which is 14 times faster than
the Berkeley parser, and is the fastest that we are
aware of for phrase-structure parsing. An open
source release of our parser (version 0.6) is freely
available on the Web. 1
In addition to the above contributions, we apply
a variety of semi-supervised learning techniques to
our transition-based parser. These techniques have
been shown useful to improve chart-based pars-
ing (Koo et al., 2008; Chen et al., 2012), but little
work has been done for transition-based parsers.
We therefore fill a gap in the literature by report-
ing empirical results using these methods. Experi-
mental results show that semi-supervised methods
give a further improvement of 0.9% in F-score on
the English data and 2.4% on the Chinese data.
Our Chinese results are the best that we are aware
of on the standard CTB data.
</bodyText>
<sectionHeader confidence="0.959032" genericHeader="method">
2 Baseline parser
</sectionHeader>
<bodyText confidence="0.9998675">
We adopt the parser of Zhang and Clark (2009) for
our baseline, which is based on the shift-reduce
process of Sagae and Lavie (2005), and employs
global perceptron training and beam search.
</bodyText>
<subsectionHeader confidence="0.982787">
2.1 Vanilla Shift-Reduce
</subsectionHeader>
<bodyText confidence="0.999868428571429">
Shift-reduce parsing is based on a left-to-right
scan of the input sentence. At each step, a tran-
sition action is applied to consume an input word
or construct a new phrase-structure. A stack
is used to maintain partially constructed phrase-
structures, while the input words are stored in a
buffer. The set of transition actions are
</bodyText>
<listItem confidence="0.996535">
• SHIFT: pop the front word from the buffer,
and push it onto the stack.
</listItem>
<footnote confidence="0.638637">
1http://sourceforge.net/projects/zpar/
</footnote>
<equation confidence="0.985695142857143">
Axioms [φ, 0, false,0]
Goal [S, n, true, C]
UNARY-X
[SIX, i, false, c + cu]
[S, n, false, c]
FINISH [S,
n, true, c + cf]
</equation>
<figureCaption confidence="0.911253">
Figure 1: Deduction system of the baseline shift-
reduce parsing process.
</figureCaption>
<figure confidence="0.295795">
•
</figure>
<bodyText confidence="0.933848">
REDUCE-L/R-X: pop the top two con-
stituents off the stack, combine them into a
new constituent with label X, and push the
new constituent onto the stack.
</bodyText>
<listItem confidence="0.999145666666667">
• UNARY-X: pop the top constituent off the
stack, raise it to a new constituent with la-
bel X, and push the new constituent onto the
stack.
• FINISH: pop the root node off the stack and
ends parsing.
</listItem>
<bodyText confidence="0.99991675">
The deduction system for the process is shown
in Figure 1, where the item is formed as (stack,
buffer front index, completion mark, score), and
cs, cr, and cu represent the incremental score of
the SHIFT, REDUCE, and UNARY parsing steps,
respectively; these scores are calculated according
to the context features of the parser state item. n
is the number of words in the input.
</bodyText>
<subsectionHeader confidence="0.9968175">
2.2 Global Discriminative Training and
Beam-Search
</subsectionHeader>
<bodyText confidence="0.999944636363636">
For a given input sentence, the initial state has an
empty stack and a buffer that contains all the input
words. An agenda is used to keep the k best state
items at each step. At initialization, the agenda
contains only the initial state. At each step, every
state item in the agenda is popped and expanded
by applying a valid transition action, and the top
k from the newly constructed state items are put
back onto the agenda. The process repeats until
the agenda is empty, and the best completed state
item (recorded as candidate output) is taken for
</bodyText>
<equation confidence="0.803311">
REDUCE-L/R-X [SIX, i, false, c + cr]
Inference Rules:
[S, i, false, c]
SHIFT [SIw, i + 1, false, c + cs]
[S|s1s0, i, false, c]
[S|s0, i, false, c]
435
Description Templates
unigrams s0tc, s0wc, s1tc, s1wc, s2tc
s2wc, s3tc, s3wc, q0wt, q1wt
q2wt, q3wt, s0lwc, s0rwc
s0uwc, s1lwc, s1rwc, s1uwc
bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,
s0wq0w, s0wq0t, s0cq0w, s0cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t,
s1wq0w, s1wq0t, s1cq0w, s1cq0t
trigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0t
s0cs1cs2w, s0cs1cq0t, s0ws1cq0t
s0cs1wq0t, s0cs1cq0w
</equation>
<tableCaption confidence="0.971883">
Table 1: A summary of baseline feature templates,
</tableCaption>
<bodyText confidence="0.992714142857143">
where si represents the ith item on the stack S and
qi denotes the ith item in the queue Q. w refers to
the head lexicon, t refers to the head POS, and c
refers to the constituent label.
the output.
The score of a state item is the total score of the
transition actions that have been applied to build
</bodyText>
<equation confidence="0.97830725">
the item:
N
C(α) = Φ(ai) · θ�
i=1
</equation>
<bodyText confidence="0.9981896">
Here Φ(ai) represents the feature vector for the ith
action ai in state item α. It is computed by apply-
ing the feature templates in Table 1 to the context
of α. N is the total number of actions in α.
The model parameter θ� is trained with the aver-
aged perceptron algorithm, applied to state items
(sequence of actions) globally. We apply the early
update strategy (Collins and Roark, 2004), stop-
ping parsing for parameter updates when the gold-
standard state item falls off the agenda.
</bodyText>
<subsectionHeader confidence="0.99789">
2.3 Baseline Features
</subsectionHeader>
<bodyText confidence="0.999449272727273">
Our baseline features are adopted from Zhang and
Clark (2009), and are shown in Table 1 Here si
represents the ith item on the top of the stack S
and qi denotes the ith item in the front end of the
queue Q. The symbol w denotes the lexical head
of an item; the symbol c denotes the constituent
label of an item; the symbol t is the POS of a lex-
ical head. These features are adapted from Zhang
and Clark (2009). We remove Chinese specific
features and make the baseline parser language-
independent.
</bodyText>
<sectionHeader confidence="0.989074" genericHeader="method">
3 Improved hypotheses comparison
</sectionHeader>
<bodyText confidence="0.99846">
Unlike dependency parsing, constituent parse
trees for the same sentence can have different
numbers of nodes, mainly due to the existence
of unary nodes. As a result, completed state
</bodyText>
<figure confidence="0.870877666666667">
NP
NN NNS
address issues
</figure>
<figureCaption confidence="0.9367995">
Figure 2: Example parse trees of the same sen-
tence with different numbers of actions.
</figureCaption>
<bodyText confidence="0.999988365853659">
items for the same sentence can have different
numbers of unary actions. Take the phrase “ad-
dress issues” for example, two possible parses
are shown in Figure 2 (a) and (b), respectively.
The first parse corresponds to the action sequence
[SHIFT, SHIFT, REDUCE-R-NP, FINISH], while
the second parse corresponds to the action se-
quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-
VP, FINISH], which consists of one more action
than the first case. In practice, variances between
state items can be much larger than the chosen ex-
ample. In the extreme case where a state item does
not contain any unary action, the number of ac-
tions is 2n, where n is the number of words in
the sentence. On the other hand, if the maximum
number of consequent unary actions is 2 (Sagae
and Lavie, 2005; Zhang and Clark, 2009), then the
maximum number of actions a state item can have
is 4n.
The significant variance in the number of ac-
tions N can have an impact on the linear sepa-
rability of state items, for which the feature vec-
tors are �Ni=1 Φ (ai). This turns out to have a sig-
nificant empirical influence on perceptron training
with early-update, where the training of the model
interacts with search (Daume III, 2006).
One way of improving the comparability of
state items is to reduce the differences in their
sizes, and we use a padding method to achieve
this. The idea is to extend the set of actions by
adding an IDLE action, so that completed state
items can be further expanded using the IDLE ac-
tion. The action does not change the state itself,
but simply adds to the number of actions in the
sequence. A feature vector is extracted for the
IDLE action according to the final state context,
in the same way as other actions. Using the IDLE
action, the transition sequence for the two parses
in Figure 2 can be [SHIFT, SHIFT, REDUCE-
NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-
NP, REDUCE-L-VP, FINISH], respectively. Their
</bodyText>
<figure confidence="0.978259">
VP
VB
address
NP
NNS
issues
</figure>
<page confidence="0.985282">
436
</page>
<bodyText confidence="0.694928">
Axioms [φ, 0, false, 0, 0]
</bodyText>
<equation confidence="0.972198461538462">
Goal [S, n, true, m : 2n &lt; m &lt; 4n, C]
Inference Rules:
[S, i, false, k,c]
SHIFT [S|w, i + 1, false, k + 1, c + cs]
REDUCE-L/R-X
[S|s1s0, i, false, k, c]
[S|X, i, false, k + 1, c + cr]
UNARY-X
[S|X, i, false, k + 1, c + cu]
[S, n, false, k, c]
FINISH [S, n, true, k + 1, c + cf]
[S, n, true, k, c]
IDLE [S, n, true, k + 1, c + ci]
</equation>
<figureCaption confidence="0.9740305">
Figure 3: Deductive system of the extended tran-
sition system.
</figureCaption>
<bodyText confidence="0.999745791666667">
corresponding feature vectors have about the same
sizes, and are more linearly separable. Note that
there can be more than one action that are padded
to a sequence of actions, and the number of IDLE
actions depends on the size difference between the
current action sequence and the largest action se-
quence without IDLE actions.
Given this extension, the deduction system is
shown in Figure 3. We add the number of actions
k to an item. The initial item (Axioms) has k = 0,
while the goal item has 2n &lt; k &lt; 4n. Given this
process, beam-search decoding can be made sim-
pler than that of Zhang and Clark (2009). While
they used a candidate output to record the best
completed state item, and finish decoding when
the agenda contains no more items, we can sim-
ply finish decoding when all items in the agenda
are completed, and output the best state item in
the agenda. With this new transition process, we
experimented with several extended features,and
found that the templates in Table 2 are useful to
improve the accuracies further. Here sill denotes
the left child of si’s left child. Other notations can
be explained in a similar way.
</bodyText>
<sectionHeader confidence="0.858202" genericHeader="method">
4 Semi-supervised Parsing with Large
Data
</sectionHeader>
<bodyText confidence="0.99997025">
This section discusses how to extract informa-
tion from unlabeled data or auto-parsed data to
further improve shift-reduce parsing accuracies.
We consider three types of information, including
</bodyText>
<construct confidence="0.992256">
s0llwc, s0lrwc, s0luwc
s0rlwc, s0rrwc, s0ruwc
s0ulwc, s0urwc, s0uuwc
s1llwc, s1lrwc, s1luwc
s1rlwc, s1rrwc, s1ruwc
</construct>
<tableCaption confidence="0.775839">
Table 2: New features for the extended parser.
</tableCaption>
<bodyText confidence="0.999960142857143">
paradigmatic relations, dependency relations, and
structural relations. These relations are captured
by word clustering, lexical dependencies, and a
dependency language model, respectively. Based
on the information, we propose a set of novel fea-
tures specifically designed for shift-reduce con-
stituent parsing.
</bodyText>
<subsectionHeader confidence="0.961243">
4.1 Paradigmatic Relations: Word
Clustering
</subsectionHeader>
<bodyText confidence="0.9999822">
Word clusters are regarded as lexical intermedi-
aries for dependency parsing (Koo et al., 2008)
and POS tagging (Sun and Uszkoreit, 2012). We
employ the Brown clustering algorithm (Liang,
2005) on unannotated data (word segmentation is
performed if necessary). In the initial state of clus-
tering, each word in the input corpus is regarded
as a cluster, then the algorithm repeatedly merges
pairs of clusters that cause the least decrease in
the likelihood of the input corpus. The clustering
results are a binary tree with words appearing as
leaves. Each cluster is represented as a bit-string
from the root to the tree node that represents the
cluster. We define a function CLU(w) to return the
cluster ID (a bit string) of an input word w.
</bodyText>
<subsectionHeader confidence="0.7450385">
4.2 Dependency Relations: Lexical
Dependencies
</subsectionHeader>
<bodyText confidence="0.999559428571429">
Lexical dependencies represent linguistic relations
between words: whether a word modifies another
word. The idea of exploiting lexical dependency
information from auto-parsed data has been ex-
plored before for dependency parsing (Chen et al.,
2009) and constituent parsing (Zhu et al., 2012).
To extract lexical dependencies, we first run the
baseline parser on unlabeled data. To simplify
the extraction process, we can convert auto-parsed
constituency trees into dependency trees by using
Penn2Malt. 2 From the dependency trees, we ex-
tract bigram lexical dependencies (w1, w2, L/R)
where the symbol L (R) means that w1 (w2) is the
head of w2 (w1). We also extract trigram lexical
</bodyText>
<footnote confidence="0.6256985">
2http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html
[S|s0, i, false, k, c]
</footnote>
<page confidence="0.964614">
437
</page>
<bodyText confidence="0.976875230769231">
dependencies (w1, w2, w3, L/R), where L means
that w1 is the head of w2 and w3, meanwhile w2
and w3 are required to be siblings.
Following the strategy of Chen et al. (2009),
we assign categories to bigram and trigram items
separately according to their frequency counts.
Specifically, top-10% most frequent items are as-
signed to the category of High Frequency (HF);
otherwise if an item is among top 20%, we assign
it to the category of Middle Frequency (MF); oth-
erwise the category of Low Frequency (LF). Here-
after, we refer to the bigram and trigram lexical
dependency lists as BLD and TLD, respectively.
</bodyText>
<subsectionHeader confidence="0.987479">
4.3 Structural Relations: Dependency
Language Model
</subsectionHeader>
<bodyText confidence="0.999874875">
The dependency language model is proposed by
Shen et al. (2008) and is used as additional in-
formation for graph-based dependency parsing in
Chen et al. (2012). Formally, given a depen-
dency tree y of an input sentence x, we can
denote by H(y) the set of words that have at
least one dependent. For each xh E H(y), we
have a corresponding dependency structure Dh =
</bodyText>
<equation confidence="0.678694666666667">
(xLk, ... xL1, xh, xR1, ... , xRm). The probability
P(Dh) is defined to be
P(Dh) = PL(Dh) X PR(Dh)
</equation>
<bodyText confidence="0.73775">
where PL(Dh) can be in turn defined as:
</bodyText>
<equation confidence="0.9784536">
PL(Dh) Pz P(xL1|xh)
XP(xL2|xL1, xh)
X ...
XP(xLk|xLk−1, . . . , xLk−N+1, xh)
PR(Dh) can be defined in a similar way.
</equation>
<bodyText confidence="0.999808125">
We build dependency language models on auto-
parsed data. Again, we convert constituency trees
into dependency trees for the purpose of simplic-
ity. From the dependency trees, we build a bigram
and a trigram language model, which are denoted
by BLM and TLM, respectively. The following
are the templates of the records of the dependency
language models.
</bodyText>
<listItem confidence="0.9980615">
(1) (xLi, xh, P(xLi|xh))
(2) (xRi, xh, P(xRi|xh))
(3) (xLi, xLi−1, xh, P(xLi|xLi−1, xh))
(4) (xRi, xRi−1, xh, P(xRi|xRi−1, xh))
</listItem>
<bodyText confidence="0.967997">
Here the templates (1) and (2) belong to BLM
and the templates (3) and (4) belong to TLM. To
</bodyText>
<note confidence="0.445348">
Stat Train Dev Test Unlabeled
</note>
<table confidence="0.9987045">
EN # sent 39.8k 1.7k 2.4k 3,139.1k
# word 950.0k 40.1k 56.7k 76,041.4k
CH # sent 18.1k 350 348 11,810.7k
# word 493.8k 8.0k 6.8k 269,057.2k
</table>
<tableCaption confidence="0.420728">
Table 4: Statistics on sentence and word numbers
of the experimental data.
use the dependency language models, we employ
a map function Φ(r) to assign a category to each
record r according to its probability, as in Chen et
al. (2012). The following is the map function.
</tableCaption>
<table confidence="0.228645666666667">
HP if P(r) E top−10%
MP else if P(r) E top−30%
LP otherwise
</table>
<subsectionHeader confidence="0.933868">
4.4 Semi-supervised Features
</subsectionHeader>
<bodyText confidence="0.9999447">
We design a set of features based on the infor-
mation extracted from auto-parsed data or unan-
notated data. The features are summarized in Ta-
ble 3. Here CLU returns a cluster ID for a word.
The functions BLDl/r(·), TLDl/r(·), BLMl/r(·),
and TLMl/r(·) check whether a given word com-
bination can be found in the corresponding lists.
For example, BLDl(s1w, s0w) returns a category
tag (HF, MF, or LF) if (s1w, s0w, L) exits in the
list BLD, else it returns NONE.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987418">
5.1 Set-up
</subsectionHeader>
<bodyText confidence="0.999881777777778">
Labeled English data employed in this paper were
derived from the Wall Street Journal (WSJ) corpus
of the Penn Treebank (Marcus et al., 1993). We
used sections 2-21 as labeled training data, section
24 for system development, and section 23 for fi-
nal performance evaluation. For labeled Chinese
data, we used the version 5.1 of the Penn Chinese
Treebank (CTB) (Xue et al., 2005). Articles 001-
270 and 440-1151 were used for training, articles
301-325 were used as development data, and arti-
cles 271-300 were used for evaluation.
For both English and Chinese data, we used ten-
fold jackknifing (Collins, 2000) to automatically
assign POS tags to the training data. We found that
this simple technique could achieve an improve-
ment of 0.4% on English and an improvement of
2.0% on Chinese. For English POS tagging, we
adopted SVMTool, 3 and for Chinese POS tagging
</bodyText>
<equation confidence="0.790926772727273">
3http://www.lsi.upc.edu/∼nlp/SVMTool/
⎧
⎨⎪
⎪⎩
Φ(r) =
438
Word Cluster Features
CLU(s1w) CLU(s0w) CLU(q0w)
CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w
Lexical Dependency Features
BLDl(s1w, s0w) BLDl(s1w, s0w)os1tos0t BLDr(s1w, s0w)
BLDr(s1w, s0w)os1tos0t BLDl(s1w, q0w)os1toq0t BLDl(s1w, q0w)
BLDr(s1w, q0w) BLDr(s1w, q0w)os1toq0t BLDl(s0w, q0w)
BLDl(s0w, q0w)os0toq0t BLDr(s0w, q0w)os0toq0t BLDr(s0w, q0w)
TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)os1tos0t TLDr(s1w, s0ldw, s0w)
TLDr(s1w, s0ldw, s0w)os1tos0t TLDl(s0w, s0rdw, q0w)os0toq0t TLDl(s0w, s0rdw, q0w)
TLDr(s0w, NONE, q0w) TLDr(s0w, NONE, q0w)os0toq0t
Dependency Language Model Features
BLMl(s1w, s0w) BLMl(s1w, s0w)os1tos0t BLMr(s1w, s0w)
BLMr(s1w, s0w)os1tos0t BLMl(s0w, q0w) BLMl(s0w, q0w)os0toq0t
BLMr(s0w, q0w)os0toq0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)
TLMl(s1w, s1rdw, s0w)os1tos0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)os1tos0t
</equation>
<tableCaption confidence="0.880705142857143">
Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and
dependency language models. Here the symbol si denotes a stack item, qi denotes a queue item, w
represents a word, and t represents a POS tag.
Table 5: Experimental results on the English and
Chinese development sets with the padding tech-
nique and new supervised features added incre-
mentally.
</tableCaption>
<table confidence="0.99987">
Features LR LP F1
+word cluster 89.3 90.0 89.7
+lexical dependencies 89.7 90.3 90.0
+dependency LM 90.0 90.6 90.3
+word cluster 85.7 87.5 86.6
+lexical dependencies 87.2 88.6 87.9
+dependency LM 87.2 88.7 88.0
</table>
<tableCaption confidence="0.89791175">
Table 6: Experimental results on the English and
Chinese development sets with different types of
semi-supervised features added incrementally to
the extended parser.
</tableCaption>
<figure confidence="0.998630421052631">
Lan. System
Baseline
+padding
+features
Baseline
+padding
+features
LR LP F1
88.4 88.7 88.6
88.8 89.5 89.1
89.0 89.7 89.3
85.6 86.3 86.0
85.5 87.2 86.4
85.5 87.6 86.5
ENG
CHN
Lan.
ENG
CHN
</figure>
<bodyText confidence="0.9979231">
we employed the Stanford POS tagger. 4
We took the WSJ articles from the TIPSTER
corpus (LDC93T3A) as unlabeled English data. In
addition, we removed from the unlabeled English
data the sentences that appear in the WSJ corpus
of the Penn Treebank. For unlabeled Chinese data,
we used Chinese Gigaword (LDC2003T09), on
which we conducted Chinese word segmentation
by using a CRF-based segmenter. Table 4 summa-
rizes data statistics on sentence and word numbers
of the data sets listed above.
We used EVALB to evaluate parser perfor-
mances, including labeled precision (LP), labeled
recall (LR), and bracketing F1. 5 For significance
tests, we employed the randomized permutation-
based tool provided by Daniel Bikel. 6
In both training and decoding, we set the beam
size to 16, which achieves a good tradeoff be-
tween efficiency and accuracy. The optimal iter-
ation number of perceptron learning is determined
</bodyText>
<footnote confidence="0.997975">
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.cs.nyu.edu/evalb
6http://www.cis.upenn.edu/∼dbikel/software.html#comparator
</footnote>
<bodyText confidence="0.996795333333333">
on the development sets. For word clustering, we
set the cluster number to 50 for both the English
and Chinese experiments.
</bodyText>
<subsectionHeader confidence="0.862394">
5.2 Results on Development Sets
</subsectionHeader>
<bodyText confidence="0.9999755">
Table 5 reports the results of the extended parser
(baseline + padding + supervised features) on the
English and Chinese development sets. We inte-
grated the padding method into the baseline parser,
based on which we further incorporated the super-
vised features in Table 2. From the results we find
that the padding method improves the parser accu-
racies by 0.5% and 0.4% on English and Chinese,
respectively. Incorporating the supervised features
in Table 2 gives further improvements of 0.2% on
English and 0.1% on Chinese.
Based on the extended parser, we experimented
different types of semi-supervised features by
adding the features incrementally. The results are
shown in Table 6. By comparing the results in Ta-
ble 5 and the results in Table 6 we can see that the
semi-supervised features achieve an overall im-
provement of 1.0% on the English data and an im-
</bodyText>
<page confidence="0.998362">
439
</page>
<table confidence="0.996789647058823">
Type Parser LR LP F1
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0
Sagae &amp; Lavie (2006)* 87.8 88.1 87.9
Baseline 90.0 89.9 89.9
Petrov &amp; Klein (2007) 90.1 90.2 90.1
Baseline+Padding 90.2 90.7 90.4
Carreras et al. (2008) 90.7 91.4 91.1
RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5
Huang (2008) 92.2 91.2 91.7
Zhu et al. (2012)* 90.4 90.5 90.4
Baseline+Padding+Semi 91.1 91.5 91.3
SE Huang &amp; Harper (2009) 91.1 91.6 91.3
Huang et al. (2010)† 91.4 91.8 91.6
McClosky et al. (2006) 92.1 92.5 92.3
</table>
<tableCaption confidence="0.935321">
Table 7: Comparison of our parsers and related
work on the English test set. * Shift-reduce
parsers. † The results of self-training with a sin-
gle latent annotation grammar.
</tableCaption>
<table confidence="0.999873">
Type Parser LR LP F1
Charniak (2000)* 79.6 82.1 80.8
Bikel (2004)† 79.3 82.0 80.6
SI Baseline 82.1 83.1 82.6
Baseline+Padding 82.1 84.3 83.2
Petrov &amp; Klein (2007) 81.9 84.8 83.3
RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3
SE Zhu et al. (2012) 80.6 81.9 81.2
Baseline+Padding+Semi 84.4 86.8 85.6
</table>
<tableCaption confidence="0.996363">
Table 8: Comparison of our parsers and related
</tableCaption>
<bodyText confidence="0.8827345">
work on the test set of CTB5.1.* Huang (2009)
adapted the parsers to Chinese parsing on CTB5.1.
† We run the parser on CTB5.1 to get the results.
provement of 1.5% on the Chinese data.
</bodyText>
<subsectionHeader confidence="0.99666">
5.3 Final Results
</subsectionHeader>
<bodyText confidence="0.999977352941176">
Here we report the final results on the English and
Chinese test sets. We compared the final results
with a large body of related work. We grouped the
parsers into three categories: single parsers (SI),
discriminative reranking parsers (RE), and semi-
supervised parsers (SE). Table 7 shows the com-
parative results on the English test set and Table 8
reports the comparison on the Chinese test set.
From the results we can see that our extended
parser (baseline + padding + supervised features)
outperforms the Berkeley parser by 0.3% on En-
glish, and is comparable with the Berkeley parser
on Chinese (−0.1% less). Here +padding means
the padding technique and the features in Table 2.
After integrating semi-supervised features, the
parsing accuracy on English is improved to 91.3%.
We note that the performance is on the same level
</bodyText>
<table confidence="0.947828909090909">
Parser #Sent/Second
Ratnaparkhi (1997) Unk
Collins (1999) 3.5
Charniak (2000) 5.7
Sagae &amp; Lavie (2005)* 3.7$
Sagae &amp; Lavie (2006)† 2.2$
Petrov &amp; Klein (2007) 6.2
Carreras et al. (2008) Unk
Baseline 100.7
This Paper Baseline+Padding 89.5
Baseline+Padding+Semi 46.8
</table>
<tableCaption confidence="0.997044">
Table 9: Comparison of running times on the En-
</tableCaption>
<bodyText confidence="0.991961263157895">
glish test set, where the time for loading models
is excluded. * The results of SVM-based shift-
reduce parsing with greedy search. † The results of
MaxEnt-based shift-reduce parser with best-first
search. $ Times reported by authors running on
different hardware.
as the performance of self-trained parsers, except
for McClosky et al. (2006), which is based on the
combination of reranking and self-training. On
Chinese, the final parsing accuracy is 85.6%. To
our knowledge, this is by far the best reported per-
formance on this data set.
The padding technique, supervised features,
and semi-supervised features achieve an overall
improvement of 1.4% over the baseline on En-
glish, which is significant on the level of p &lt;
10−5. The overall improvement on Chinese is
3.0%, which is also significant on the level of
p &lt; 10−5.
</bodyText>
<subsectionHeader confidence="0.999647">
5.4 Comparison of Running Time
</subsectionHeader>
<bodyText confidence="0.999974133333333">
We also compared the running times of our parsers
with the related single parsers. We ran timing tests
on an Intel 2.3GHz processor with 8GB mem-
ory. The comparison is shown in Table 9. From
the table, we can see that incorporating semi-
supervised features decreases parsing speed, but
the semi-supervised parser still has the advantage
of efficiency over other parsers. Specifically, the
semi-supervised parser is 7 times faster than the
Berkeley parser. Note that Sagae &amp; Lavie (2005)
and Sagae &amp; Lavie (2006) are also shift-reduce
parsers, and their running times were evaluated on
different hardwares. In practice, the running times
of the shift-reduce parsers should be much shorter
than the reported times in the table.
</bodyText>
<subsectionHeader confidence="0.86199">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9995895">
We conducted error analysis for the three sys-
tems: the baseline parser, the extended parser with
</bodyText>
<page confidence="0.993969">
440
</page>
<figure confidence="0.974254">
1 2 3 4 5 6 7 8
Span Length
</figure>
<figureCaption confidence="0.999162333333333">
Figure 5: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parsers on spans of different lengths.
</figureCaption>
<figure confidence="0.995441">
10 20 30 40 50 60 70
Sentence Length
</figure>
<figureCaption confidence="0.996634666666667">
Figure 6: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parser on sentences of different lengths.
</figureCaption>
<figure confidence="0.9958656">
94
92
90
88
86
Baseline
Extended
Semi-supervised
94
92
90
88
86
84
82
Baseline
Extended
Semi-supervised
F Score
F Score
</figure>
<bodyText confidence="0.999511142857143">
the padding technique, and the semi-supervised
parser, focusing on the English test set. The analy-
sis was performed in four dimensions: parsing ac-
curacies on different phrase types, on constituents
of different span lengths, on different sentence
lengths, and on sentences with different numbers
of unknown words.
</bodyText>
<subsectionHeader confidence="0.598239">
5.5.1 Different Phrase Types
</subsectionHeader>
<bodyText confidence="0.999992055555556">
Table 10 shows the parsing accuracies of the base-
line, extended parser, and semi-supervised parser
on different phrase types. Here we only consider
the nine most frequent phrase types in the English
test set. In the table, the phrase types are ordered
from left to right in the descending order of their
frequencies. We also show the improvements of
the semi-supervised parser over the baseline parser
(the last row in the table). As the results show, the
extended parser achieves improvements on most
of the phrase types with two exceptions: Preposi-
tion Prase (PP) and Quantifier Phrase (QP). Semi-
supervised features further improve parsing accu-
racies over the extended parser (QP is an excep-
tion). From the last row, we can see that improve-
ments of the semi-supervised parser over the base-
line on VP, S, SBAR, ADVP, and ADJP are above
the average improvement (1.4%).
</bodyText>
<subsectionHeader confidence="0.679836">
5.5.2 Different Span Lengths
</subsectionHeader>
<bodyText confidence="0.999984">
Figure 5 shows a comparison of the three parsers
on spans of different lengths. Here we consider
span lengths up to 8. As the results show, both
the padding extension and semi-supervised fea-
tures are more helpful on relatively large spans:
the performance gaps between the three parsers
are enlarged with increasing span lengths.
</bodyText>
<subsectionHeader confidence="0.50597">
5.5.3 Different Sentence Lengths
</subsectionHeader>
<bodyText confidence="0.999938923076923">
Figure 6 shows a comparison of parsing accura-
cies of the three parsers on sentences of different
lengths. Each number on the horizontal axis repre-
sents the sentences whose lengths are between the
number and its previous number. For example, the
number 30 refers to the sentences whose lengths
are between 20 and 30. From the results we can
see that semi-supervised features improve parsing
accuracy on both short and long sentences. The
points at 70 are exceptions. In fact, sentences with
lengths between 60 and 70 have only 8 instances,
and the statistics on such a small number of sen-
tences are not reliable.
</bodyText>
<subsubsectionHeader confidence="0.4219">
5.5.4 Different Numbers of Unknown Words
</subsubsectionHeader>
<bodyText confidence="0.9999417">
Figure 4 shows a comparison of parsing accura-
cies of the baseline, extended parser, and semi-
supervised parser on sentences with different num-
bers of unknown words. As the results show,
the padding method is not very helpful on sen-
tences with large numbers of unknown words,
while semi-supervised features help significantly
on this aspect. This conforms to the intuition that
semi-supervised methods reduce data sparseness
and improve the performance on unknown words.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99991275">
In this paper, we addressed the problem of dif-
ferent action-sequence lengths for shift-reduce
phrase-structure parsing, and designed a set of
novel non-local features to further improve pars-
ing. The resulting supervised parser outperforms
the Berkeley parser, a state-of-the-art chart parser,
in both accuracies and speeds. In addition, we in-
corporated a set of semi-supervised features. The
</bodyText>
<page confidence="0.995093">
441
</page>
<table confidence="0.9958656">
System IP VP S PP SEAR ADVP ADJP WHIP QP
Baseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3
Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6
Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1
Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2
</table>
<tableCaption confidence="0.9793">
Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers
on different phrase types.
</tableCaption>
<figure confidence="0.652148">
Baseline Extended Semi-supervised
</figure>
<figureCaption confidence="0.9942325">
Figure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser
on sentences of different unknown words.
</figureCaption>
<figure confidence="0.962032">
0 1 2 3 4 5 6 7
F-score (%)
100
90
80
70
</figure>
<bodyText confidence="0.996249666666667">
final parser reaches an accuracy of 91.3% on En-
glish and 85.6% on Chinese, by far the best re-
ported accuracies on the CTB data.
</bodyText>
<sectionHeader confidence="0.994582" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999975769230769">
We thank the anonymous reviewers for their valu-
able comments. Yue Zhang and Muhua Zhu
were supported partially by SRG-ISTD-2012-038
from Singapore University of Technology and De-
sign. Muhua Zhu and Jingbo Zhu were funded
in part by the National Science Foundation of
China (61073140; 61272376), Specialized Re-
search Fund for the Doctoral Program of Higher
Education (20100042110031), and the Fundamen-
tal Research Funds for the Central Universities
(N100204002). Wenliang Chen was funded par-
tially by the National Science Foundation of China
(61203314).
</bodyText>
<sectionHeader confidence="0.997764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998590295454545">
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 12–14, Jeju Island, Ko-
rea.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9–16, Manchester, England.
Eugune Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings ofACL, pages 173–180.
Eugune Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132–139, Seattle, Washington, USA.
Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of EMNLP, pages 570–579, Singa-
pore.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency. In Proceedings of ACL, pages
213–222, Jeju, Republic of Korea.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings ofACL, Stroudsburg, PA, USA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, Madrid, Spain.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking
for natural language processing. In Proceedings of
ICML, pages 175–182, Stanford, CA, USA.
Hal Daume III. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
USC.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
</reference>
<page confidence="0.979862">
442
</page>
<reference confidence="0.999740055555556">
across languages. In Proceedings of EMNLP, pages
832–841, Singapore.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077–1086, Uppsala,
Sweden.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent variable
grammars. In Proceedings ofEMNLP, pages 12–22,
Massachusetts, USA.
Liang Huang. 2008. Forest reranking: discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586–594, Ohio, USA.
Liang-Ya Huang. 2009. Improve Chinese parsing with
Max-Ent reranking parser. In Master Project Re-
port, Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings ofICML, pages 282–289, Massachusetts, USA,
June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts Insti-
tute of Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewiz. 1993. Building a large anno-
tated corpus of English. Computational Linguistics,
19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT/NAACL, Main Conference,
pages 152–159, New York City, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings ofACL, pages 91–
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: a data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216–2219.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404–411, Rochester, New York,
April.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP, Rhode Island, USA.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings ofIWPT, pages 125–132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT/NAACL,
Companion Volume: Short Papers, pages 129–132,
New York, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL, pages 577–585, Ohio, USA.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: to-
wards accurate Chinese part-of-speech tagging. In
Proceedings ofACL, Jeju, Republic of Korea.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195–206,
Nancy, France.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL/HLT, pages 888–896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings ofACL, pages 188–193, Portland, Ore-
gon, USA.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING, pages 3171–3186, Mum-
bai, India.
</reference>
<page confidence="0.999343">
443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425820">
<title confidence="0.999989">Fast and Accurate Shift-Reduce Constituent Parsing</title>
<author confidence="0.962035">Yue Wenliang Min</author>
<author confidence="0.962035">Jingbo</author>
<affiliation confidence="0.9996755">Language Processing Lab., Northeastern University, University of Technology and Design,</affiliation>
<address confidence="0.612682">University, China and Institute for Infocomm Research, Singapore</address>
<abstract confidence="0.986269791666667">zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>On the parameter space of generative lexicalized statistical parsing models.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24091" citStr="Bikel (2004)" startWordPosition="3930" endWordPosition="3931">89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 Baseline+Padding+Semi 84.4 86.8 85.6 Table 8: Comparison of our parsers and related work on the test set of CTB5.1.* Huang (2009) adapted the parsers to Chinese parsing on CTB5.1. † We run the parser on CTB5.1 to get the results. provement of 1.5% on the Chinese data. 5.3 Final Results Here we report the final results on the English and Chinese test sets. We compared the final results with a large body </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. On the parameter space of generative lexicalized statistical parsing models. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>12--14</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2739" citStr="Bohnet and Nivre, 2012" startWordPosition="381" endWordPosition="384">2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of EMNLP, pages 12–14, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<location>Manchester, England.</location>
<contexts>
<context position="23580" citStr="Carreras et al. (2008)" startWordPosition="3838" endWordPosition="3841">, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klei</context>
<context position="25588" citStr="Carreras et al. (2008)" startWordPosition="4181" endWordPosition="4184">t. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. On Chinese, the final parsing accuracy is 85.6%. To our knowledge,</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL, pages 9–16, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugune Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="23624" citStr="Charniak &amp; Johnson (2005)" startWordPosition="3846" endWordPosition="3849">-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnso</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugune Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugune Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1860" citStr="Charniak, 2000" startWordPosition="256" endWordPosition="257">oduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear</context>
<context position="23372" citStr="Charniak (2000)" startWordPosition="3803" endWordPosition="3804">s by 0.5% and 0.4% on English and Chinese, respectively. Incorporating the supervised features in Table 2 gives further improvements of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of s</context>
<context position="25481" citStr="Charniak (2000)" startWordPosition="4163" endWordPosition="4164">omparative results on the English test set and Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the com</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugune Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL, pages 132–139, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Junichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="15121" citStr="Chen et al., 2009" startWordPosition="2482" endWordPosition="2485">the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input word w. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies (w1, w2, L/R) where the symbol L (R) means that w1 (w2) is the head of w2 (w1). We also extract trigram lexical 2http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html [S|s0, i, false, k, c] 437 dependencies (w1, w2, w3, L/R), where L means that w1 is the head of w2 and w3, meanwh</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings of EMNLP, pages 570–579, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Utilizing dependency language models for graphbased dependency.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>213--222</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="5015" citStr="Chen et al., 2012" startWordPosition="724" endWordPosition="727">er gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. In addition, our parser runs with over 89 sentences per second, which is 14 times faster than the Berkeley parser, and is the fastest that we are aware of for phrase-structure parsing. An open source release of our parser (version 0.6) is freely available on the Web. 1 In addition to the above contributions, we apply a variety of semi-supervised learning techniques to our transition-based parser. These techniques have been shown useful to improve chart-based parsing (Koo et al., 2008; Chen et al., 2012), but little work has been done for transition-based parsers. We therefore fill a gap in the literature by reporting empirical results using these methods. Experimental results show that semi-supervised methods give a further improvement of 0.9% in F-score on the English data and 2.4% on the Chinese data. Our Chinese results are the best that we are aware of on the standard CTB data. 2 Baseline parser We adopt the parser of Zhang and Clark (2009) for our baseline, which is based on the shift-reduce process of Sagae and Lavie (2005), and employs global perceptron training and beam search. 2.1 V</context>
<context position="16453" citStr="Chen et al. (2012)" startWordPosition="2699" endWordPosition="2702">igram and trigram items separately according to their frequency counts. Specifically, top-10% most frequent items are assigned to the category of High Frequency (HF); otherwise if an item is among top 20%, we assign it to the category of Middle Frequency (MF); otherwise the category of Low Frequency (LF). Hereafter, we refer to the bigram and trigram lexical dependency lists as BLD and TLD, respectively. 4.3 Structural Relations: Dependency Language Model The dependency language model is proposed by Shen et al. (2008) and is used as additional information for graph-based dependency parsing in Chen et al. (2012). Formally, given a dependency tree y of an input sentence x, we can denote by H(y) the set of words that have at least one dependent. For each xh E H(y), we have a corresponding dependency structure Dh = (xLk, ... xL1, xh, xR1, ... , xRm). The probability P(Dh) is defined to be P(Dh) = PL(Dh) X PR(Dh) where PL(Dh) can be in turn defined as: PL(Dh) Pz P(xL1|xh) XP(xL2|xL1, xh) X ... XP(xLk|xLk−1, . . . , xLk−N+1, xh) PR(Dh) can be defined in a similar way. We build dependency language models on autoparsed data. Again, we convert constituency trees into dependency trees for the purpose of simpl</context>
<context position="17889" citStr="Chen et al. (2012)" startWordPosition="2957" endWordPosition="2960">Li, xh, P(xLi|xh)) (2) (xRi, xh, P(xRi|xh)) (3) (xLi, xLi−1, xh, P(xLi|xLi−1, xh)) (4) (xRi, xRi−1, xh, P(xRi|xRi−1, xh)) Here the templates (1) and (2) belong to BLM and the templates (3) and (4) belong to TLM. To Stat Train Dev Test Unlabeled EN # sent 39.8k 1.7k 2.4k 3,139.1k # word 950.0k 40.1k 56.7k 76,041.4k CH # sent 18.1k 350 348 11,810.7k # word 493.8k 8.0k 6.8k 269,057.2k Table 4: Statistics on sentence and word numbers of the experimental data. use the dependency language models, we employ a map function Φ(r) to assign a category to each record r according to its probability, as in Chen et al. (2012). The following is the map function. HP if P(r) E top−10% MP else if P(r) E top−30% LP otherwise 4.4 Semi-supervised Features We design a set of features based on the information extracted from auto-parsed data or unannotated data. The features are summarized in Table 3. Here CLU returns a cluster ID for a word. The functions BLDl/r(·), TLDl/r(·), BLMl/r(·), and TLMl/r(·) check whether a given word combination can be found in the corresponding lists. For example, BLDl(s1w, s0w) returns a category tag (HF, MF, or LF) if (s1w, s0w, L) exits in the list BLD, else it returns NONE. 5 Experiments 5.</context>
</contexts>
<marker>Chen, Zhang, Li, 2012</marker>
<rawString>Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Utilizing dependency language models for graphbased dependency. In Proceedings of ACL, pages 213–222, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8906" citStr="Collins and Roark, 2004" startWordPosition="1403" endWordPosition="1406">to the head lexicon, t refers to the head POS, and c refers to the constituent label. the output. The score of a state item is the total score of the transition actions that have been applied to build the item: N C(α) = Φ(ai) · θ� i=1 Here Φ(ai) represents the feature vector for the ith action ai in state item α. It is computed by applying the feature templates in Table 1 to the context of α. N is the total number of actions in α. The model parameter θ� is trained with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. 2.3 Baseline Features Our baseline features are adopted from Zhang and Clark (2009), and are shown in Table 1 Here si represents the ith item on the top of the stack S and qi denotes the ith item in the front end of the queue Q. The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head. These features are adapted from Zhang and Clark (2009). We remove Chinese specific features and make the baseline parser langua</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1844" citStr="Collins, 1997" startWordPosition="254" endWordPosition="255"> parser. 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="23341" citStr="Collins (1999)" startWordPosition="3798" endWordPosition="3799"> improves the parser accuracies by 0.5% and 0.4% on English and Chinese, respectively. Incorporating the supervised features in Table 2 gives further improvements of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-red</context>
<context position="25461" citStr="Collins (1999)" startWordPosition="4160" endWordPosition="4161">Table 7 shows the comparative results on the English test set and Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language processing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>175--182</pages>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="19106" citStr="Collins, 2000" startWordPosition="3168" endWordPosition="3169">et-up Labeled English data employed in this paper were derived from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). We used sections 2-21 as labeled training data, section 24 for system development, and section 23 for final performance evaluation. For labeled Chinese data, we used the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). Articles 001- 270 and 440-1151 were used for training, articles 301-325 were used as development data, and articles 271-300 were used for evaluation. For both English and Chinese data, we used tenfold jackknifing (Collins, 2000) to automatically assign POS tags to the training data. We found that this simple technique could achieve an improvement of 0.4% on English and an improvement of 2.0% on Chinese. For English POS tagging, we adopted SVMTool, 3 and for Chinese POS tagging 3http://www.lsi.upc.edu/∼nlp/SVMTool/ ⎧ ⎨⎪ ⎪⎩ Φ(r) = 438 Word Cluster Features CLU(s1w) CLU(s0w) CLU(q0w) CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w Lexical Dependency Features BLDl(s1w, s0w) BLDl(s1w, s0w)os1tos0t BLDr(s1w, s0w) BLDr(s1w, s0w)os1tos0t BLDl(s1w, q0w)os1toq0t BLDl(s1w, q0w) BLDr(s1w, q0w) BLDr(s1w, q0w)os1toq0t BLDl(s0w, q0w) BLDl(s0w,</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language processing. In Proceedings of ICML, pages 175–182, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Practical Structured Learning for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis, USC.</tech>
<marker>Daume, 2006</marker>
<rawString>Hal Daume III. 2006. Practical Structured Learning for Natural Language Processing. Ph.D. thesis, USC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>832--841</pages>
<contexts>
<context position="23763" citStr="Huang &amp; Harper (2009)" startWordPosition="3870" endWordPosition="3873">s in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 Baseline+Padding+Semi 84.4 86.8 85.6 Table 8: Comparison of our parsers and re</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In Proceedings of EMNLP, pages 832–841, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala,</location>
<contexts>
<context position="2144" citStr="Huang and Sagae, 2010" startWordPosition="296" endWordPosition="299">assifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In </context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL, pages 1077–1086, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>12--22</pages>
<location>Massachusetts, USA.</location>
<contexts>
<context position="23798" citStr="Huang et al. (2010)" startWordPosition="3877" endWordPosition="3880">-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 Baseline+Padding+Semi 84.4 86.8 85.6 Table 8: Comparison of our parsers and related work on the test set of CTB5.</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In Proceedings ofEMNLP, pages 12–22, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>586--594</pages>
<location>Ohio, USA.</location>
<contexts>
<context position="23652" citStr="Huang (2008)" startWordPosition="3853" endWordPosition="3854">es incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: discriminative parsing with non-local features. In Proceedings of ACL, pages 586–594, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang-Ya Huang</author>
</authors>
<title>Improve Chinese parsing with Max-Ent reranking parser.</title>
<date>2009</date>
<booktitle>In Master Project Report,</booktitle>
<institution>Brown University.</institution>
<contexts>
<context position="24414" citStr="Huang (2009)" startWordPosition="3986" endWordPosition="3987">.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 Baseline+Padding+Semi 84.4 86.8 85.6 Table 8: Comparison of our parsers and related work on the test set of CTB5.1.* Huang (2009) adapted the parsers to Chinese parsing on CTB5.1. † We run the parser on CTB5.1 to get the results. provement of 1.5% on the Chinese data. 5.3 Final Results Here we report the final results on the English and Chinese test sets. We compared the final results with a large body of related work. We grouped the parsers into three categories: single parsers (SI), discriminative reranking parsers (RE), and semisupervised parsers (SE). Table 7 shows the comparative results on the English test set and Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Liang-Ya Huang. 2009. Improve Chinese parsing with Max-Ent reranking parser. In Master Project Report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4995" citStr="Koo et al., 2008" startWordPosition="720" endWordPosition="723">Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. In addition, our parser runs with over 89 sentences per second, which is 14 times faster than the Berkeley parser, and is the fastest that we are aware of for phrase-structure parsing. An open source release of our parser (version 0.6) is freely available on the Web. 1 In addition to the above contributions, we apply a variety of semi-supervised learning techniques to our transition-based parser. These techniques have been shown useful to improve chart-based parsing (Koo et al., 2008; Chen et al., 2012), but little work has been done for transition-based parsers. We therefore fill a gap in the literature by reporting empirical results using these methods. Experimental results show that semi-supervised methods give a further improvement of 0.9% in F-score on the English data and 2.4% on the Chinese data. Our Chinese results are the best that we are aware of on the standard CTB data. 2 Baseline parser We adopt the parser of Zhang and Clark (2009) for our baseline, which is based on the shift-reduce process of Sagae and Lavie (2005), and employs global perceptron training an</context>
<context position="14179" citStr="Koo et al., 2008" startWordPosition="2332" endWordPosition="2335">uding s0llwc, s0lrwc, s0luwc s0rlwc, s0rrwc, s0ruwc s0ulwc, s0urwc, s0uuwc s1llwc, s1lrwc, s1luwc s1rlwc, s1rrwc, s1ruwc Table 2: New features for the extended parser. paradigmatic relations, dependency relations, and structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. 4.1 Paradigmatic Relations: Word Clustering Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>282--289</pages>
<location>Massachusetts, USA,</location>
<contexts>
<context position="2363" citStr="Lafferty et al., 2001" startWordPosition="328" endWordPosition="331">commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. The above global-learning and beam-search framework can be appl</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML, pages 282–289, Massachusetts, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="14277" citStr="Liang, 2005" startWordPosition="2349" endWordPosition="2350">c, s1rrwc, s1ruwc Table 2: New features for the extended parser. paradigmatic relations, dependency relations, and structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. 4.1 Paradigmatic Relations: Word Clustering Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input word w. 4.2 Dependency Relations: Lexical Dependencies Lexi</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewiz</author>
</authors>
<title>Building a large annotated corpus of English.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="18639" citStr="Marcus et al., 1993" startWordPosition="3089" endWordPosition="3092">esign a set of features based on the information extracted from auto-parsed data or unannotated data. The features are summarized in Table 3. Here CLU returns a cluster ID for a word. The functions BLDl/r(·), TLDl/r(·), BLMl/r(·), and TLMl/r(·) check whether a given word combination can be found in the corresponding lists. For example, BLDl(s1w, s0w) returns a category tag (HF, MF, or LF) if (s1w, s0w, L) exits in the list BLD, else it returns NONE. 5 Experiments 5.1 Set-up Labeled English data employed in this paper were derived from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). We used sections 2-21 as labeled training data, section 24 for system development, and section 23 for final performance evaluation. For labeled Chinese data, we used the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). Articles 001- 270 and 440-1151 were used for training, articles 301-325 were used as development data, and articles 271-300 were used for evaluation. For both English and Chinese data, we used tenfold jackknifing (Collins, 2000) to automatically assign POS tags to the training data. We found that this simple technique could achieve an improvement of 0.4% on E</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewiz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewiz. 1993. Building a large annotated corpus of English. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<location>New York City, USA,</location>
<contexts>
<context position="23837" citStr="McClosky et al. (2006)" startWordPosition="3884" endWordPosition="3887">all improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 Baseline+Padding+Semi 84.4 86.8 85.6 Table 8: Comparison of our parsers and related work on the test set of CTB5.1.* Huang (2009) adapted the parsers to</context>
<context position="26054" citStr="McClosky et al. (2006)" startWordPosition="4252" endWordPosition="4255">khi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. On Chinese, the final parsing accuracy is 85.6%. To our knowledge, this is by far the best reported performance on this data set. The padding technique, supervised features, and semi-supervised features achieve an overall improvement of 1.4% over the baseline on English, which is significant on the level of p &lt; 10−5. The overall improvement on Chinese is 3.0%, which is also significant on the level of p &lt; 10−5. 5.4 Comparison of Running Time We also compared the running times of our parsers with the related single parsers. We </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the HLT/NAACL, Main Conference, pages 152–159, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL, pages 91– 98,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1884" citStr="McDonald et al., 2005" startWordPosition="258" endWordPosition="261">ion-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91– 98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: a data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="1642" citStr="Nivre et al., 2006" startWordPosition="224" endWordPosition="227"> sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: a data-driven parser-generator for dependency parsing. In Proceedings of LREC, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="4472" citStr="Petrov and Klein, 2007" startWordPosition="635" endWordPosition="638">, August 4-9 2013. c�2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs. On standard evaluations using both the Penn Treebank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. In addition, our parser runs with over 89 sentences per second, which is 14 times faster than the Berkeley parser, and is the fastest that we are aware of for phrase-structure parsing. An open source release of our parser (version 0.6) is freely available on the Web. 1 In addition to the above contributions, we apply a variety of semi-supervised learning techniques to our transition-based parser. These techniques have been shown useful to improve chart-based parsing (Koo et al., 2008; Chen et al., 2012), but little work has been done for transition-based pars</context>
<context position="23510" citStr="Petrov &amp; Klein (2007)" startWordPosition="3827" endWordPosition="3830"> of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI</context>
<context position="25561" citStr="Petrov &amp; Klein (2007)" startWordPosition="4176" endWordPosition="4179">son on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. On Chinese, the final parsing accuracy </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT/NAACL, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Rhode Island, USA.</location>
<contexts>
<context position="23311" citStr="Ratnaparkhi (1997)" startWordPosition="3793" endWordPosition="3794">ts we find that the padding method improves the parser accuracies by 0.5% and 0.4% on English and Chinese, respectively. Incorporating the supervised features in Table 2 gives further improvements of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the</context>
<context position="25442" citStr="Ratnaparkhi (1997)" startWordPosition="4157" endWordPosition="4158">pervised parsers (SE). Table 7 shows the comparative results on the English test set and Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky e</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of EMNLP, Rhode Island, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT,</booktitle>
<pages>125--132</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1696" citStr="Sagae and Lavie, 2005" startWordPosition="231" endWordPosition="234">ble accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of tran</context>
<context position="5552" citStr="Sagae and Lavie (2005)" startWordPosition="817" endWordPosition="820">een shown useful to improve chart-based parsing (Koo et al., 2008; Chen et al., 2012), but little work has been done for transition-based parsers. We therefore fill a gap in the literature by reporting empirical results using these methods. Experimental results show that semi-supervised methods give a further improvement of 0.9% in F-score on the English data and 2.4% on the Chinese data. Our Chinese results are the best that we are aware of on the standard CTB data. 2 Baseline parser We adopt the parser of Zhang and Clark (2009) for our baseline, which is based on the shift-reduce process of Sagae and Lavie (2005), and employs global perceptron training and beam search. 2.1 Vanilla Shift-Reduce Shift-reduce parsing is based on a left-to-right scan of the input sentence. At each step, a transition action is applied to consume an input word or construct a new phrase-structure. A stack is used to maintain partially constructed phrasestructures, while the input words are stored in a buffer. The set of transition actions are • SHIFT: pop the front word from the buffer, and push it onto the stack. 1http://sourceforge.net/projects/zpar/ Axioms [φ, 0, false,0] Goal [S, n, true, C] UNARY-X [SIX, i, false, c + c</context>
<context position="10617" citStr="Sagae and Lavie, 2005" startWordPosition="1704" endWordPosition="1707"> in Figure 2 (a) and (b), respectively. The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action sequence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. In practice, variances between state items can be much larger than the chosen example. In the extreme case where a state item does not contain any unary action, the number of actions is 2n, where n is the number of words in the sentence. On the other hand, if the maximum number of consequent unary actions is 2 (Sagae and Lavie, 2005; Zhang and Clark, 2009), then the maximum number of actions a state item can have is 4n. The significant variance in the number of actions N can have an impact on the linear separability of state items, for which the feature vectors are �Ni=1 Φ (ai). This turns out to have a significant empirical influence on perceptron training with early-update, where the training of the model interacts with search (Daume III, 2006). One way of improving the comparability of state items is to reduce the differences in their sizes, and we use a padding method to achieve this. The idea is to extend the set of</context>
<context position="23411" citStr="Sagae &amp; Lavie (2005)" startWordPosition="3809" endWordPosition="3812">Chinese, respectively. Incorporating the supervised features in Table 2 gives further improvements of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annot</context>
<context position="25506" citStr="Sagae &amp; Lavie (2005)" startWordPosition="4166" endWordPosition="4169">n the English test set and Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and</context>
<context position="27052" citStr="Sagae &amp; Lavie (2005)" startWordPosition="4418" endWordPosition="4421"> &lt; 10−5. The overall improvement on Chinese is 3.0%, which is also significant on the level of p &lt; 10−5. 5.4 Comparison of Running Time We also compared the running times of our parsers with the related single parsers. We ran timing tests on an Intel 2.3GHz processor with 8GB memory. The comparison is shown in Table 9. From the table, we can see that incorporating semisupervised features decreases parsing speed, but the semi-supervised parser still has the advantage of efficiency over other parsers. Specifically, the semi-supervised parser is 7 times faster than the Berkeley parser. Note that Sagae &amp; Lavie (2005) and Sagae &amp; Lavie (2006) are also shift-reduce parsers, and their running times were evaluated on different hardwares. In practice, the running times of the shift-reduce parsers should be much shorter than the reported times in the table. 5.5 Error Analysis We conducted error analysis for the three systems: the baseline parser, the extended parser with 440 1 2 3 4 5 6 7 8 Span Length Figure 5: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers on spans of different lengths. 10 20 30 40 50 60 70 Sentence Length Figure 6: Comparison of parsing accurac</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings ofIWPT, pages 125–132, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL, Companion Volume: Short Papers,</booktitle>
<pages>129--132</pages>
<location>New York, USA.</location>
<contexts>
<context position="3319" citStr="Sagae and Lavie, 2006" startWordPosition="460" endWordPosition="463"> and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics sm</context>
<context position="23448" citStr="Sagae &amp; Lavie (2006)" startWordPosition="3816" endWordPosition="3819">the supervised features in Table 2 gives further improvements of 0.2% on English and 0.1% on Chinese. Based on the extended parser, we experimented different types of semi-supervised features by adding the features incrementally. The results are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 C</context>
<context position="25533" citStr="Sagae &amp; Lavie (2006)" startWordPosition="4171" endWordPosition="4174">Table 8 reports the comparison on the Chinese test set. From the results we can see that our extended parser (baseline + padding + supervised features) outperforms the Berkeley parser by 0.3% on English, and is comparable with the Berkeley parser on Chinese (−0.1% less). Here +padding means the padding technique and the features in Table 2. After integrating semi-supervised features, the parsing accuracy on English is improved to 91.3%. We note that the performance is on the same level Parser #Sent/Second Ratnaparkhi (1997) Unk Collins (1999) 3.5 Charniak (2000) 5.7 Sagae &amp; Lavie (2005)* 3.7$ Sagae &amp; Lavie (2006)† 2.2$ Petrov &amp; Klein (2007) 6.2 Carreras et al. (2008) Unk Baseline 100.7 This Paper Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading models is excluded. * The results of SVM-based shiftreduce parsing with greedy search. † The results of MaxEnt-based shift-reduce parser with best-first search. $ Times reported by authors running on different hardware. as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. On Chinese,</context>
<context position="27077" citStr="Sagae &amp; Lavie (2006)" startWordPosition="4423" endWordPosition="4426">ovement on Chinese is 3.0%, which is also significant on the level of p &lt; 10−5. 5.4 Comparison of Running Time We also compared the running times of our parsers with the related single parsers. We ran timing tests on an Intel 2.3GHz processor with 8GB memory. The comparison is shown in Table 9. From the table, we can see that incorporating semisupervised features decreases parsing speed, but the semi-supervised parser still has the advantage of efficiency over other parsers. Specifically, the semi-supervised parser is 7 times faster than the Berkeley parser. Note that Sagae &amp; Lavie (2005) and Sagae &amp; Lavie (2006) are also shift-reduce parsers, and their running times were evaluated on different hardwares. In practice, the running times of the shift-reduce parsers should be much shorter than the reported times in the table. 5.5 Error Analysis We conducted error analysis for the three systems: the baseline parser, the extended parser with 440 1 2 3 4 5 6 7 8 Span Length Figure 5: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers on spans of different lengths. 10 20 30 40 50 60 70 Sentence Length Figure 6: Comparison of parsing accuracies of the baseline, exte</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of HLT/NAACL, Companion Volume: Short Papers, pages 129–132, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>577--585</pages>
<location>Ohio, USA.</location>
<contexts>
<context position="16358" citStr="Shen et al. (2008)" startWordPosition="2683" endWordPosition="2686">equired to be siblings. Following the strategy of Chen et al. (2009), we assign categories to bigram and trigram items separately according to their frequency counts. Specifically, top-10% most frequent items are assigned to the category of High Frequency (HF); otherwise if an item is among top 20%, we assign it to the category of Middle Frequency (MF); otherwise the category of Low Frequency (LF). Hereafter, we refer to the bigram and trigram lexical dependency lists as BLD and TLD, respectively. 4.3 Structural Relations: Dependency Language Model The dependency language model is proposed by Shen et al. (2008) and is used as additional information for graph-based dependency parsing in Chen et al. (2012). Formally, given a dependency tree y of an input sentence x, we can denote by H(y) the set of words that have at least one dependent. For each xh E H(y), we have a corresponding dependency structure Dh = (xLk, ... xL1, xh, xR1, ... , xRm). The probability P(Dh) is defined to be P(Dh) = PL(Dh) X PR(Dh) where PL(Dh) can be in turn defined as: PL(Dh) Pz P(xL1|xh) XP(xL2|xL1, xh) X ... XP(xLk|xLk−1, . . . , xLk−N+1, xh) PR(Dh) can be defined in a similar way. We build dependency language models on autop</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL, pages 577–585, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Capturing paradigmatic and syntagmatic lexical relations: towards accurate Chinese part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL, Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="14221" citStr="Sun and Uszkoreit, 2012" startWordPosition="2339" endWordPosition="2342">, s0rrwc, s0ruwc s0ulwc, s0urwc, s0uuwc s1llwc, s1lrwc, s1luwc s1rlwc, s1rrwc, s1ruwc Table 2: New features for the extended parser. paradigmatic relations, dependency relations, and structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. 4.1 Paradigmatic Relations: Word Clustering Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input wor</context>
</contexts>
<marker>Sun, Uszkoreit, 2012</marker>
<rawString>Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradigmatic and syntagmatic lexical relations: towards accurate Chinese part-of-speech tagging. In Proceedings ofACL, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="18876" citStr="Xue et al., 2005" startWordPosition="3129" endWordPosition="3132">r(·) check whether a given word combination can be found in the corresponding lists. For example, BLDl(s1w, s0w) returns a category tag (HF, MF, or LF) if (s1w, s0w, L) exits in the list BLD, else it returns NONE. 5 Experiments 5.1 Set-up Labeled English data employed in this paper were derived from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). We used sections 2-21 as labeled training data, section 24 for system development, and section 23 for final performance evaluation. For labeled Chinese data, we used the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). Articles 001- 270 and 440-1151 were used for training, articles 301-325 were used as development data, and articles 271-300 were used for evaluation. For both English and Chinese data, we used tenfold jackknifing (Collins, 2000) to automatically assign POS tags to the training data. We found that this simple technique could achieve an improvement of 0.4% on English and an improvement of 2.0% on Chinese. For English POS tagging, we adopted SVMTool, 3 and for Chinese POS tagging 3http://www.lsi.upc.edu/∼nlp/SVMTool/ ⎧ ⎨⎪ ⎪⎩ Φ(r) = 438 Word Cluster Features CLU(s1w) CLU(s0w) CLU(q0w) CLU(s1w)s1</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>195--206</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1621" citStr="Yamada and Matsumoto, 2003" startWordPosition="220" endWordPosition="223">e differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, pages 195–206, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>888--896</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2120" citStr="Zhang and Clark, 2008" startWordPosition="292" endWordPosition="295"> parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohn</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL/HLT, pages 888–896, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the Chinese Treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="3054" citStr="Zhang and Clark, 2009" startWordPosition="424" endWordPosition="427">al parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentenc</context>
<context position="5465" citStr="Zhang and Clark (2009)" startWordPosition="802" endWordPosition="805">-supervised learning techniques to our transition-based parser. These techniques have been shown useful to improve chart-based parsing (Koo et al., 2008; Chen et al., 2012), but little work has been done for transition-based parsers. We therefore fill a gap in the literature by reporting empirical results using these methods. Experimental results show that semi-supervised methods give a further improvement of 0.9% in F-score on the English data and 2.4% on the Chinese data. Our Chinese results are the best that we are aware of on the standard CTB data. 2 Baseline parser We adopt the parser of Zhang and Clark (2009) for our baseline, which is based on the shift-reduce process of Sagae and Lavie (2005), and employs global perceptron training and beam search. 2.1 Vanilla Shift-Reduce Shift-reduce parsing is based on a left-to-right scan of the input sentence. At each step, a transition action is applied to consume an input word or construct a new phrase-structure. A stack is used to maintain partially constructed phrasestructures, while the input words are stored in a buffer. The set of transition actions are • SHIFT: pop the front word from the buffer, and push it onto the stack. 1http://sourceforge.net/p</context>
<context position="9085" citStr="Zhang and Clark (2009)" startWordPosition="1432" endWordPosition="1435">en applied to build the item: N C(α) = Φ(ai) · θ� i=1 Here Φ(ai) represents the feature vector for the ith action ai in state item α. It is computed by applying the feature templates in Table 1 to the context of α. N is the total number of actions in α. The model parameter θ� is trained with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. 2.3 Baseline Features Our baseline features are adopted from Zhang and Clark (2009), and are shown in Table 1 Here si represents the ith item on the top of the stack S and qi denotes the ith item in the front end of the queue Q. The symbol w denotes the lexical head of an item; the symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head. These features are adapted from Zhang and Clark (2009). We remove Chinese specific features and make the baseline parser languageindependent. 3 Improved hypotheses comparison Unlike dependency parsing, constituent parse trees for the same sentence can have different numbers of nodes, mainly due to the exi</context>
<context position="10641" citStr="Zhang and Clark, 2009" startWordPosition="1708" endWordPosition="1711">), respectively. The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action sequence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. In practice, variances between state items can be much larger than the chosen example. In the extreme case where a state item does not contain any unary action, the number of actions is 2n, where n is the number of words in the sentence. On the other hand, if the maximum number of consequent unary actions is 2 (Sagae and Lavie, 2005; Zhang and Clark, 2009), then the maximum number of actions a state item can have is 4n. The significant variance in the number of actions N can have an impact on the linear separability of state items, for which the feature vectors are �Ni=1 Φ (ai). This turns out to have a significant empirical influence on perceptron training with early-update, where the training of the model interacts with search (Daume III, 2006). One way of improving the comparability of state items is to reduce the differences in their sizes, and we use a padding method to achieve this. The idea is to extend the set of actions by adding an ID</context>
<context position="12805" citStr="Zhang and Clark (2009)" startWordPosition="2125" endWordPosition="2128">nsition system. corresponding feature vectors have about the same sizes, and are more linearly separable. Note that there can be more than one action that are padded to a sequence of actions, and the number of IDLE actions depends on the size difference between the current action sequence and the largest action sequence without IDLE actions. Given this extension, the deduction system is shown in Figure 3. We add the number of actions k to an item. The initial item (Axioms) has k = 0, while the goal item has 2n &lt; k &lt; 4n. Given this process, beam-search decoding can be made simpler than that of Zhang and Clark (2009). While they used a candidate output to record the best completed state item, and finish decoding when the agenda contains no more items, we can simply finish decoding when all items in the agenda are completed, and output the best state item in the agenda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here sill denotes the left child of si’s left child. Other notations can be explained in a similar way. 4 Semi-supervised Parsing with Large Data This section discusses how to</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the Chinese Treebank using a global discriminative model. In Proceedings of IWPT, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2714" citStr="Zhang and Nivre, 2011" startWordPosition="377" endWordPosition="380">sing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie,</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings ofACL, pages 188–193, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
</authors>
<title>Exploiting lexical dependencies from large-scale data for better shift-reduce constituency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>3171--3186</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="15164" citStr="Zhu et al., 2012" startWordPosition="2489" endWordPosition="2492">input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input word w. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies (w1, w2, L/R) where the symbol L (R) means that w1 (w2) is the head of w2 (w1). We also extract trigram lexical 2http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html [S|s0, i, false, k, c] 437 dependencies (w1, w2, w3, L/R), where L means that w1 is the head of w2 and w3, meanwhile w2 and w3 are required to be siblings. </context>
<context position="23685" citStr="Zhu et al. (2012)" startWordPosition="3858" endWordPosition="3861">s are shown in Table 6. By comparing the results in Table 5 and the results in Table 6 we can see that the semi-supervised features achieve an overall improvement of 1.0% on the English data and an im439 Type Parser LR LP F1 Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 SI Sagae &amp; Lavie (2005)* 86.1 86.0 86.0 Sagae &amp; Lavie (2006)* 87.8 88.1 87.9 Baseline 90.0 89.9 89.9 Petrov &amp; Klein (2007) 90.1 90.2 90.1 Baseline+Padding 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 RE Charniak &amp; Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.2 91.2 91.7 Zhu et al. (2012)* 90.4 90.5 90.4 Baseline+Padding+Semi 91.1 91.5 91.3 SE Huang &amp; Harper (2009) 91.1 91.6 91.3 Huang et al. (2010)† 91.4 91.8 91.6 McClosky et al. (2006) 92.1 92.5 92.3 Table 7: Comparison of our parsers and related work on the English test set. * Shift-reduce parsers. † The results of self-training with a single latent annotation grammar. Type Parser LR LP F1 Charniak (2000)* 79.6 82.1 80.8 Bikel (2004)† 79.3 82.0 80.6 SI Baseline 82.1 83.1 82.6 Baseline+Padding 82.1 84.3 83.2 Petrov &amp; Klein (2007) 81.9 84.8 83.3 RE Charniak &amp; Johnson (2005)* 80.8 83.8 82.3 SE Zhu et al. (2012) 80.6 81.9 81.2 </context>
</contexts>
<marker>Zhu, Zhu, Wang, 2012</marker>
<rawString>Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012. Exploiting lexical dependencies from large-scale data for better shift-reduce constituency parsing. In Proceedings of COLING, pages 3171–3186, Mumbai, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>