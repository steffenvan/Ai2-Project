<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000742">
<title confidence="0.9925145">
Low-cost Enrichment of Spanish WordNet with Automatically Translated
Glosses: Combining General and Specialized Models
</title>
<author confidence="0.979757">
Jes´us Gim´enez and Lluis M`arquez
</author>
<affiliation confidence="0.948989">
TALP Research Center, LSI Department
</affiliation>
<address confidence="0.740378">
Universitat Polit`ecnica de Catalunya
Jordi Girona Salgado 1–3, E-08034, Barcelona
</address>
<email confidence="0.999692">
{jgimenez,lluism}@lsi.upc.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941454545454">
This paper studies the enrichment of Span-
ish WordNet with synset glosses automat-
ically obtained from the English Word-
Net glosses using a phrase-based Statisti-
cal Machine Translation system. We con-
struct the English-Spanish translation sys-
tem from a parallel corpus of proceed-
ings of the European Parliament, and study
how to adapt statistical models to the do-
main of dictionary definitions. We build
specialized language and translation mod-
els from a small set of parallel definitions
and experiment with robust manners to
combine them. A statistically significant
increase in performance is obtained. The
best system is finally used to generate a
definition for all Spanish synsets, which
are currently ready for a manual revision.
As a complementary issue, we analyze the
impact of the amount of in-domain data
needed to improve a system trained en-
tirely on out-of-domain data.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986213">
Statistical Machine Translation (SMT) is today a
very promising approach. It allows to build very
quickly and fully automatically Machine Trans-
lation (MT) systems, exhibiting very competitive
results, only from a parallel corpus aligning sen-
tences from the two languages involved.
In this work we approach the task of enriching
Spanish WordNet with automatically translated
glosses1. The source glosses for these translations
are taken from the English WordNet (Fellbaum,
</bodyText>
<note confidence="0.498666">
1Glosses are short dictionary definitions that accompany
WordNet synsets. See examples in Tables 5 and 6.
</note>
<bodyText confidence="0.998922264705882">
1998), which is linked, at the synset level, to Span-
ish WordNet. This resource is available, among
other sources, through the Multilingual Central
Repository (MCR) developed by the MEANING
project (Atserias et al., 2004).
We start by empirically testing the performance
of a previously developed English–Spanish SMT
system, built from the large Europarl corpus2
(Koehn, 2003). The first observation is that this
system completely fails to translate the specific
WordNet glosses, due to the large language varia-
tions in both domains (vocabulary, style, grammar,
etc.). Actually, this is confirming one of the main
criticisms against SMT, which is its strong domain
dependence. Since parameters are estimated from
a corpus in a concrete domain, the performance
of the system on a different domain is often much
worse. This flaw of statistical and machine learn-
ing approaches is well known and has been largely
described in the NLP literature, for a variety of
tasks (e.g., parsing, word sense disambiguation,
and semantic role labeling).
Fortunately, we count on a small set of Spanish
hand-developed glosses in MCR3. Thus, we move
to a working scenario in which we introduce a
small corpus of aligned translations from the con-
crete domain of WordNet glosses. This in-domain
corpus could be itself used as a source for con-
structing a specialized SMT system. Again, ex-
periments show that this small corpus alone does
not suffice, since it does not allow to estimate
good translation parameters. However, it is well
suited for combination with the Europarl corpus,
to generate combined Language and Translation
</bodyText>
<footnote confidence="0.995044">
2The Europarl Corpus is available at: http://-
people.csail.mit.edu/people/koehn/publications/europarl
3About 10% of the 68,000 Spanish synsets contain a defi-
nition, generated without considering its English counterpart.
</footnote>
<page confidence="0.903594">
287
</page>
<note confidence="0.7349505">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287–294,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929315789474">
Models. A substantial increase in performance is
achieved, according to several standard MT eval-
uation metrics. Although moderate, this boost
in performance is statistically significant accord-
ing to the bootstrap resampling test described by
Koehn (2004b) and applied to the BLEU metric.
The main reason behind this improvement is
that the large out-of-domain corpus contributes
mainly with coverage and recall and the in-domain
corpus provides more precise translations. We
present a qualitative error analysis to support these
claims. Finally, we also address the important
question of how much in-domain data is needed
to be able to improve the baseline results.
Apart from the experimental findings, our study
has generated a very valuable resource. Currently,
we have the complete Spanish WordNet enriched
with one gloss per synset, which, far from being
perfect, constitutes an axcellent starting point for
a posterior manual revision.
Finally, we note that the construction of a
SMT system when few domain-specific data are
available has been also investigated by other au-
thors. For instance, Vogel and Tribble (2002) stud-
ied whether an SMT system for speech-to-speech
translation built on top of a small parallel corpus
can be improved by adding knowledge sources
which are not domain specific. In this work, we
look at the same problem the other way around.
We study how to adapt an out-of-domain SMT
system using in-domain data.
The rest of the paper is organized as follows.
In Section 2 the fundamentals of SMT and the
components of our MT architecture are described.
The experimental setting is described in Section 3.
Evaluation is carried out in Section 4. Finally, Sec-
tion 5 contains error analysis and Section 6 con-
cludes and outlines future work.
</bodyText>
<sectionHeader confidence="0.987273" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999900483870968">
Current state-of-the-art SMT systems are based on
ideas borrowed from the Communication Theory
field. Brown et al. (1988) suggested that MT can
be statistically approximated to the transmission
of information through a noisy channel. Given a
sentence f = f1..fn (distorted signal), it is possi-
ble to approximate the sentence e = e1..em (origi-
nal signal) which produced f. We need to estimate
P(e|f), the probability that a translator produces
f as a translation of e. By applying Bayes’ rule it
is decomposed into: P(e|f) = P(fje)~P(e)
To obtain the string e which maximizes the
translation probability for f, a search in the prob-
ability space must be performed. Because the de-
nominator is independent of e, we can ignore it for
the purpose of the search: e = argmaxeP(f|e) ~
P(e). This last equation devises three compo-
nents in a SMT system. First, a language model
that estimates P(e). Second, a translation model
representing P(f|e). Last, a decoder responsi-
ble for performing the arg-max search. Language
models are typically estimated from large mono-
lingual corpora, translation models are built out
from parallel corpora, and decoders usually per-
form approximate search, e.g., by using dynamic
programming and beam search.
However, in word-based models the modeling
of the context in which the words occur is very
weak. This problem is significantly alleviated by
phrase-based models (Och, 2002), which repre-
sent nowadays the state-of-the-art in SMT.
</bodyText>
<subsectionHeader confidence="0.988603">
2.1 System Construction
</subsectionHeader>
<bodyText confidence="0.999953571428571">
Fortunately, there is a number of freely available
tools to build a phrase-based SMT system. We
used only standard components and techniques for
our basic system, which are all described below.
The SRI Language Modeling Toolkit (SRILM)
(Stolcke, 2002) supports creation and evaluation
of a variety of language models. We build trigram
language models applying linear interpolation and
Kneser-Ney discounting for smoothing.
In order to build phrase-based translation mod-
els, a phrase extraction must be performed on
a word-aligned parallel corpus. We used the
GIZA++ SMT Toolkit4 (Och and Ney, 2003) to
generate word alignments We applied the phrase-
extract algorithm, as described by Och (2002), on
the Viterbi alignments output by GIZA++. We
work with the union of source-to-target and target-
to-source alignments, with no heuristic refine-
ment. Phrases up to length five are considered.
Also, phrase pairs appearing only once are dis-
carded, and phrase pairs in which the source/target
phrase was more than three times longer than the
target/source phrase are ignored. Finally, phrase
pairs are scored by relative frequency. Note that
no smoothing is performed.
Regarding the arg-max search, we used the
Pharaoh beam search decoder (Koehn, 2004a),
which naturally fits with the previous tools.
</bodyText>
<equation confidence="0.6569675">
4http://www.fjoch.com/GIZA++.html
P(f) .
</equation>
<page confidence="0.99477">
288
</page>
<sectionHeader confidence="0.905808" genericHeader="method">
3 Data Sets and Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.997566955555556">
As a general source of English–Spanish parallel
text, we used a collection of 730,740 parallel sen-
tences extracted from the Europarl corpus. These
correspond exactly to the training data from the
Shared Task 2: Exploiting Parallel Texts for Sta-
tistical Machine Translation from the ACL-2005
Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond5.
To be used as specialized source, we extracted,
from the MCR, the set of 6,519 English–Spanish
parallel glosses corresponding to the already de-
fined synsets in Spanish WordNet. These defini-
tions corresponded to 5,698 nouns, 87 verbs, and
734 adjectives. Examples and parenthesized texts
were removed. Parallel glosses were tokenized
and case lowered. We discarded some of these
parallel glosses based on the difference in length
between the source and the target. The gloss av-
erage length for the resulting 5,843 glosses was
8.25 words for English and 8.13 for Spanish. Fi-
nally, gloss pairs were randomly split into training
(4,843), development (500) and test (500) sets.
Additionally, we counted on two large mono-
lingual Spanish electronic dictionaries, consisting
of 142,892 definitions (2,112,592 tokens) (‘D1’)
(Marti, 1996) and 168,779 definitons (1,553,674
tokens) (‘D2’) (Vox, 1990), respectively.
Regarding evaluation, we used up to four dif-
ferent metrics with the aim of showing whether
the improvements attained are consistent or not.
We have computed the BLEU score (accumu-
lated up to 4-grams) (Papineni et al., 2001), the
NIST score (accumulated up to 5-grams) (Dod-
dington, 2002), the General Text Matching (GTM)
F-measure (e = 1, 2) (Melamed et al., 2003),
and the METEOR measure (Banerjee and Lavie,
2005). These metrics work at the lexical level by
rewarding n-gram matches between the candidate
translation and a set of human references. Addi-
tionally, METEOR considers stemming, and al-
lows for WordNet synonymy lookup.
The discussion of the significance of the results
will be based on the BLEU score, for which we
computed a bootstrap resampling test of signifi-
cance (Koehn, 2004b).
</bodyText>
<footnote confidence="0.779785">
5http://www.statmt.org/wpt05/.
</footnote>
<sectionHeader confidence="0.991291" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995126">
4.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999982767441861">
As explained in the introduction we built two indi-
vidual baseline systems. The first baseline (‘EU’)
system is entirely based on the training data from
the Europarl corpus. The second baseline system
(‘WNG’) is entirely based on the training set from
of the in-domain corpus of parallel glosses. In the
second case phrase pairs occurring only once in
the training corpus are not discarded due to the ex-
tremely small size of the corpus.
Table 1 shows results of the two baseline sys-
tems, both for the development and test sets. We
compare the performance of the ‘EU’ baseline on
these data sets with respect to the (in-domain) Eu-
roparl test set provided by the organizers of the
ACL-2005 MT workshop. As expected, there is
a very significant decrease in performance (e.g.,
from 0.24 to 0.08 according to BLEU) when the
‘EU’ baseline system is applied to the new do-
main. Some of this decrement is also due to a cer-
tain degree of free translation exhibited by the set
of available ‘quasi-parallel’ glosses. We further
discuss this issue in Section 5.
The results obtained by ‘WNG’ are also very
low, though slightly better than those of ‘EU’. This
is a very interesting fact. Although the amount of
data utilized to construct the ‘WNG’ baseline is
150 times smaller than the amount utilized to con-
struct the ‘EU’ baseline, its performance is higher
consistently according to all metrics. We interpret
this result as an indicator that models estimated
from in-domain data provide higher precision.
We also compare the results to those of a com-
mercial system such as the on-line version 5.0 of
SYSTRAN6, a general-purpose MT system based
on manually-defined lexical and syntactic trans-
ferrules. The performance of the baseline sys-
tems is significantly worse than SYSTRAN’s on
both development and test sets. This means that
a rule-based system like SYSTRAN is more ro-
bust than the SMT-based systems. The difference
against the specialized ‘WNG’ also suggests that
the amount of data used to train the ‘WNG’ base-
line is clearly insufficient.
</bodyText>
<subsectionHeader confidence="0.991725">
4.2 Combining Sources: Language Models
</subsectionHeader>
<bodyText confidence="0.9988155">
In order to improve results, in first place we turned
our eyes to language modeling. In addition to
</bodyText>
<footnote confidence="0.973371">
6http://www.systransoft.com/.
</footnote>
<page confidence="0.972358">
289
</page>
<table confidence="0.999964181818182">
system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881
WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288
SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394
test
EU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920
WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219
SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295
acl05-test
EU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153
</table>
<tableCaption confidence="0.9916975">
Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the ‘EU’
baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus. BLEU.n4 shows the accumulated
BLEU score for 4-grams. NIST.n5 shows the accumulated NIST score for 5-grams. GTM.e1 and GTM.e2 show the GTM F1-
measure for different values of the e parameter (e = 1, e = 2, respectively). METEOR reflects the METEOR score.
</tableCaption>
<bodyText confidence="0.99997078125">
the language model built from the Europarl cor-
pus (‘EU’) and the specialized language model
based on the small training set of parallel glosses
(‘WNG’), two specialized language models, based
on the two large monolingual Spanish electronic
dictionaries (‘D1’ and ‘D2’) were used. We tried
several configurations. In all cases, language mod-
els are combined with equal probability. See re-
sults, for the development set, in Table 2.
As expected, the closer the language model is
to the target domain, the better results. Observe
how results using language models ‘D1’ and ‘D2’
outperform results using ‘EU’. Note also that best
results are in all cases consistently attained by us-
ing the ‘WNG’ language model. This means that
language models estimated from small sets of in-
domain data are helpful. A second conclusion is
that a significant gain is obtained by incrementally
adding (in-domain) specialized language models
to the baselines, according to all metrics but BLEU
for which no combination seems to significantly
outperform the ‘WNG’ baseline alone. Observe
that best results are obtained, except in the case
of BLEU, by the system using ‘EU’ as translation
model and ‘WNG’ as language model. We inter-
pret this result as an indicator that translation mod-
els estimated from out-of-domain data are help-
ful because they provide recall. A third interest-
ing point is that adding an out-of-domain language
model (‘EU’) does not seem to help, at least com-
bined with equal probability than in-domain mod-
els. Same conclusions hold for the test set, too.
</bodyText>
<subsectionHeader confidence="0.999786">
4.3 Tuning the System
</subsectionHeader>
<bodyText confidence="0.999925">
Adjusting the Pharaoh parameters that control
the importance of the different probabilities that
govern the search may yield significant improve-
ments. In our case, it is specially important to
properly adjust the contribution of the language
models. We adjusted parameters by means of a
software based on the Downhill Simplex Method
in Multidimensions (William H. Press and Flan-
nery, 2002). The tuning was based on the improve-
ment attained in BLEU score over the develop-
ment set. We tuned 6 parameters: 4 language mod-
els (Al.EU, Al.D1, Al.D2, Al.WNG), the transla-
tion model (Ao), and the word penalty (Aw)7.
Results improve substantially. See Table 3. Best
results are still attained using the ‘EU’ translation
model. Interestingly, as suggested by Table 2, the
weight of language models is concentrated on the
‘WNG’ language model (Al.WNG = 0.95).
</bodyText>
<subsectionHeader confidence="0.995334">
4.4 Combining Sources: Translation Models
</subsectionHeader>
<bodyText confidence="0.967056238095238">
In this section we study the possibility of combin-
ing out-of-domain and in-domain translation mod-
els aiming at achieving a good balance between
precision and recall that yields better MT results.
Two different strategies have been tried. In
a first stragegy we simply concatenate the out-
of-domain corpus (‘EU’) and the in-domain cor-
pus (‘WNG’). Then, we construct the translatation
model (‘EUWNG’) as detailed in Section 2.1. A
second manner to proceed is to linearly combine
the two different translation models into a single
translation model (‘EU+WNG’). In this case, we
can assign different weights (w) to the contribution
of the different models to the search. We can also
determine a certain threshold θ which allows us
7Final values when using the ‘EU’ translation model are
λlmEU = 0.22, λlmD1 = 0, λlmD2 = 0.01, λlmWNG =
0.95, λφ = 1, and λw = −2.97, while when using the
‘WNG’ translation model final values are λlmEU = 0.17,
λlmD1 = 0.07, λlmD2 = 0.13, λlmWNG = 1, λφ = 0.95,
and λw = −2.64.
</bodyText>
<page confidence="0.991243">
290
</page>
<table confidence="0.999933529411765">
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
EU EU 0.0737 2.8832 0.3131 0.2216 0.2881
EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377
EU D1 0.0959 3.2570 0.3461 0.2503 0.3158
EU D2 0.0896 3.2518 0.3497 0.2482 0.3163
EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244
EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160
EU D1 + D2 +WNG 0.1094 3.4954 0.3690 0.2662 0.3372
EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321
WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689
WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288
WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050
WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883
WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995
WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847
WNG D1 + D2 +WNG 0.0980 3.2238 0.3462 0.2479 0.3117
WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941
</table>
<tableCaption confidence="0.992213666666667">
Table 2: MT Results on development set, for several translation/language model configurations. ‘EU’ and ‘WNG’ refer to
the models estimated from the Europarl corpus and the training set of parallel WordNet glosses, respectively. ‘D1’, and ‘D2’
denote the specialized language models estimated from the two dictionaries.
</tableCaption>
<table confidence="0.999769">
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695
WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452
test
EU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644
WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343
</table>
<tableCaption confidence="0.8674105">
Table 3: MT Results on development and test sets after tuning for the ‘EU + D1 + D2 + WNG’ language model configuration
for the two translation models, ‘EU’ and ‘WNG’.
</tableCaption>
<bodyText confidence="0.999966409090909">
to discard phrase pairs under a certain probability.
These weights and thresholds were adjusted8 as
detailed in Subsection 4.3. Interestingly, at combi-
nation time the importance of the ‘WNG’ transla-
tion model (WtmWNG = 0.9) is much higher than
that of the ‘EU’ translation model (WtmEU = 0.1).
Table 4 shows results for the two strategies.
As expected, the ‘EU+WNG’ strategy consistently
obtains the best results according to all metrics
both on the development and test sets, since it
allows to better adjust the relative importance of
each translation model. However, both techniques
achieve a very competitive performance. Results
improve, according to BLEU, from 0.13 to 0.16,
and from 0.11 to 0.14, for the development and
test sets, respectively.
We measured the statistical signficance of
the overall improvement in BLEU.n4 attained
with respect to the baseline results by ap-
plying the bootstrap resampling technique de-
scribed by Koehn (2004b). The 95% confi-
dence intervals extracted from the test set after
</bodyText>
<footnote confidence="0.7137585">
8We used values wtmEU = 0.1, wtmWNG = 0.9,
BtmEU = 0.1, and BtmWNG = 0.01
</footnote>
<bodyText confidence="0.987966142857143">
10,000 samples are the following: IEU−base =
[0.0642, 0.0939], IWNG−base = [0.0788, 0.1112],
IEU+WNG−best = [0.1221, 0.1572]. Since the in-
tervals are not ovelapping, we can conclude that
the performance of the best combined method is
statistically higher than the ones of the two base-
line systems.
</bodyText>
<subsectionHeader confidence="0.991328">
4.5 How much in-domain data is needed?
</subsectionHeader>
<bodyText confidence="0.9999734">
In principle, the more in-domain data we have the
better, but these may be difficult or expensive to
collect. Thus, a very interesting issue in the con-
text of our work is how much in-domain data is
needed in order to improve results attained using
out-of-domain data alone. To answer this question
we focus on the ‘EU+WNG’ strategy and analyze
the impact on performance (BLEU.n4) of special-
ized models extracted from an incrementally big-
ger number of example glosses. The results are
presented in the plot of Figure 1. We compute
three variants separately, by considering the use of
the in-domain data: only for the translation model
(TM), only for the language model (LM), and si-
multaneously in both models (TM+LM). In order
</bodyText>
<page confidence="0.992252">
291
</page>
<table confidence="0.9999236">
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711
EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998
EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804
EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130
test
EUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595
EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885
EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723
EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963
</table>
<tableCaption confidence="0.999597">
Table 4: MT Results on development and test sets for the two strategies for combining translations models.
</tableCaption>
<figure confidence="0.998117888888889">
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
</figure>
<figureCaption confidence="0.948513">
Figure 1: Impact of the size of in-domain data on
MT system performance for the test set.
</figureCaption>
<bodyText confidence="0.999947882352941">
to avoid the possible effect of over-fitting we focus
on the behavior on the test set. Note that the opti-
mization of parameters is performed at each point
in the x-axis using only the development set.
A significant initial gain of around 0.3 BLEU
points is observed when adding as few as 100
glosses. In all cases, it is not until around 1,000
glosses are added that the ‘EU+WNG’ system sta-
bilizes. After that, results continue improving as
more in-domain data are added. We observe a
very significant increase by just adding around
3,000 glosses. Another interesting observation is
the boosting effect of the combination of TM and
LM specialized models. While individual curves
for TM and LM tend to be more stable with more
than 4,000 added examples, the TM+LM curve
still shows a steep increase in this last part.
</bodyText>
<sectionHeader confidence="0.998695" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999939105263158">
We inspected results at the sentence level based on
the GTM F-measure (e = 1) for the best config-
uration of the ‘EU+WNG’ system. 196 sentences
out from the 500 obtain an F-measure equal to or
higher than 0.5 on the development set (181 sen-
tences in the case of test set), whereas only 54
sentences obtain a score lower than 0.1. These
numbers give a first idea of the relative useful-
ness of our system. Table 5 shows some trans-
lation cases selected for discussion. For instance,
Case 1 is a clear example of unfair low score. The
problem is that source and reference are not par-
allel but ‘quasi-parallel’. Both glosses define the
same concept but in a different way. Thus, metrics
based on rewarding lexical similarities are not well
suited for these cases. Cases 2, 3, 4 are examples
of proper cooperation between ‘EU’ and ‘WNG’
models. ‘EU’ models provides recall, for instance
by suggesting translation candidates for ‘bombs’
or ‘price below’. ‘WNG’ models provide preci-
sion, for instance by choosing the right translation
for ‘an attack’ or ‘the act of’.
We also compared the ‘EU+WNG’ system to
SYSTRAN. In the case of SYSTRAN 167 sen-
tences obtain a score equal to or higher than 0.5
whereas 79 sentences obtain a score lower than
0.1. These numbers are slightly under the per-
formance of the ‘EU+WNG’ system. Table 6
shows some translation cases selected for discus-
sion. Case 1 is again an example of both sys-
tems obtaining very low scores because of ‘quasi-
parallelism’. Cases 2 and 3 are examples of SYS-
TRAN outperforming our system. In case 2 SYS-
TRAN exhibits higher precision in the translation
of ‘accompanying’ and ‘illustration’, whereas in
case 3 it shows higher recall by suggesting ap-
propriate translation candidates for ‘fibers’, ‘silk-
worm’, ‘cocoon’, ‘threads’, and ‘knitting’. Cases
</bodyText>
<figure confidence="0.995738857142857">
BLEU.n4
baseline
TM + LM impact
TM impact
LM impact
0 500 1000 1500 2000 2500 3000 3500 4000 4500
# glosses
</figure>
<page confidence="0.978284">
292
</page>
<bodyText confidence="0.962083925925926">
FE FW FEW Source OutE OutW OutEW Reference
0.0000 0.1333 0.1111 of the younger de acuerdo con de la younger de acuerdo con que tiene
of two boys el mas joven de dos boys el mas joven de menos edad
with the same de dos boys tiene el mismo dos muchachos
family name con la misma nombre familia tiene el mismo
familia fama nombre familia
0.2857 0.2500 0.5000 an attack atacar por ataque ataque ataque con
by dropping cayendo realizado por realizado por bombas
bombs bombas dropping bombs cayendo bombas
0.1250 0.7059 0.5882 the act of acto de la acci´on y efecto acci´on y efecto acci´on y efecto
informing by informaci´on de informing de informaba de informar
verbal report por verbales por verbal por verbales con una expli-
ponencia explicaci´on explicaci´on caci´on verbal
0.5000 0.0000 0.5000 a price below un precio por una price un precio por precio que esta
the standard debajo de la below n´umbero debajo de la por debajo de
price norma precio estandar price estandar precio lo normal
Table 5: MT output analysis of the ‘EU’, ‘WNG’ and ‘EU+WNG’ systems. FE, FW and FEW refer to the GTM (e = 1)
F-measure attained by the ‘EU’, ‘WNG’ and ‘EU+WNG’ systems, respectively. ‘Source’, OutE, OutW and OutEW refer to
the input and the output of the systems. ‘Reference’ corresponds to the expected output.
4 and 5 are examples where our system outper-
forms SYSTRAN. In case 4, our system provides
higher recall by suggesting an adequate transla-
tion for ‘top of something’. In case 5, our system
shows higher precision by selecting a better trans-
lation for ‘rate’. However, we observed that SYS-
TRAN tends in most cases to construct sentences
exhibiting a higher degree of grammaticality.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999971891891892">
In this work, we have enriched every synset in
Spanish WordNet with a preliminary gloss, which
can be later updated in a lighter process of manual
revision. Though imperfect, this material consti-
tutes a very valuable resource. For instance, Word-
Net glosses have been used in the past to generate
sense tagged corpora (Mihalcea and Moldovan,
1999), or as external knowledge for Question An-
swering systems (Hovy et al., 2001).
We have also shown the importance of using a
small set of in-domain parallel sentences in or-
der to adapt a phrase-based general SMT sys-
tem to a new domain. In particular, we have
worked on specialized language and translation
models and on their combination with general
models in order to achieve a proper balance be-
tween precision (specialized in-domain models)
and recall (general out-of-domain models). A sub-
stantial increase is consistently obtained according
to standard MT evaluation metrics, which has been
shown to be statistically significant in the case
of BLEU. Broadly speaking, we have shown that
around 3,000 glosses (very short sentence frag-
ments) suffice in this domain to obtain a signifi-
cant improvement. Besides, all the methods used
are language independent, assumed the availabil-
ity of the required in-domain additional resources.
In the future we plan to work on domain inde-
pendent translation models built from WordNet it-
self. We may use the WordNet topology to pro-
vide translation candidates weighted according to
the given domain. Moreover, we are experiment-
ing the applicability of current Word Sense Dis-
ambiguation (WSD) technology to MT. We could
favor those translation candidates showing a closer
semantic relation to the source. We believe that
coarse-grained is sufficient for the purpose of MT.
</bodyText>
<sectionHeader confidence="0.997366" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999894083333333">
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02) and the Spanish Ministry of
Education and Science (TRANGRAM, TIN2004-
07925-C03-02). Our research group, TALP Re-
search Center, is recognized as a Quality Research
Group (2001 SGR 00254) by DURSI, the Re-
search Department of the Catalan Government.
Authors are grateful to Patrik Lambert for pro-
viding us with the implementation of the Simplex
Method, and specially to German Rigau for moti-
vating in its origin all this work.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9301935">
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
</reference>
<page confidence="0.996611">
293
</page>
<bodyText confidence="0.966424">
FEW FS Source OutEW OutS Reference
0.0000 0.0000 a newspaper that peri´odico que un peri´odico publicaci´on
is published se publica diario que se publica peri´odica
every day cada dia monotem´atica
0.1818 0.8333 brief description breve descripci´on breve descripci´on peque˜na descripci´on
accompanying an adjuntas un aclaraci´on que acompafna que acompa˜na
illustration una ilustraci´on una ilustraci´on
0.1905 0.7333 fibers from silkworm fibers desde silkworm las fibras de los fibras de los capullos
cocoons provide cocoons proporcionan capullos del gusano de gusano de seda
threads for knitting threads para knitting de seda proporcionan que proporcionan
los hilos de rosca hilos para tejer
para hacer punto
1.0000 0.0000 the top of something parte superior de la tapa algo parte superior de
una cosa una cosa
0.6667 0.3077 a rate at which un ritmo al que una tarifa en la ritmo al que
something happens sucede algo cual algo sucede sucede una cosa
</bodyText>
<tableCaption confidence="0.7354395">
Table 6: MT output analysis of the ‘EU+WNG’ and SYSTRAN systems. FEW and FS refer to the GTM (e = 1) F-measure
attained by the ‘EU+WNG’ and SYSTRAN systems, respectively. ‘Source’, OutEW and OutS refer to the input and the output
</tableCaption>
<reference confidence="0.995263910447761">
of the systems. ‘Reference’ corresponds to the expected output.
Vossen. 2004. The MEANING Multilingual Cen-
tral Repository. In Proceedings of 2nd GWC.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings ofACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, Robert L.
Mercer, , and Paul S. Roossin. 1988. A statistical
approach to language translation. In Proceedings of
COLING’88.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138–145.
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.
2001. The Use of External Knowledge of Factoid
QA. In Proceedings of TREC.
Philipp Koehn. 2003. Europarl: A Multilin-
gual Corpus for Evaluation of Machine Transla-
tion. Technical report, http://people.csail.mit.edu/-
people/koehn/publications/europarl/.
Philipp Koehn. 2004a. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings ofAMTA’04.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP’04.
Maria Antonia Marti, editor. 1996. Gran dic-
cionario de la Lengua Espa˜nola. Larousse Planeta,
Barcelona.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings ofHLT/NAACL’03.
Rada Mihalcea and Dan Moldovan. 1999. An Au-
tomatic Method for Generating Sense Tagged Cor-
pora. In Proceedings ofAAAI.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, IBM Research Re-
port, RC22176. Technical report, IBM T.J. Watson
Research Center.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of IC-
SLP’02.
Stephan Vogel and Alicia Tribble. 2002. Improv-
ing Statistical Machine Translation for a Speech-to-
Speech Translation Task. In Proceedings ofICSLP-
2002 Workshop on Speech-to-Speech Translation.
Vox, editor. 1990. Diccionario Actual de la Lengua
Espa˜nola. Bibliograf, Barcelona.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art ofScientific Computing. Cambridge
University Press.
</reference>
<page confidence="0.998362">
294
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929296">
<title confidence="0.9984795">Low-cost Enrichment of Spanish WordNet with Automatically Translated Glosses: Combining General and Specialized Models</title>
<author confidence="0.999053">Gim´enez M`arquez</author>
<affiliation confidence="0.9991915">TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya</affiliation>
<address confidence="0.961024">Jordi Girona Salgado 1–3, E-08034, Barcelona</address>
<abstract confidence="0.998258391304348">This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English Word- Net glosses using a phrase-based Statistical Machine Translation system. We construct the English-Spanish translation system from a parallel corpus of proceedings of the European Parliament, and study how to adapt statistical models to the domain of dictionary definitions. We build specialized language and translation models from a small set of parallel definitions and experiment with robust manners to combine them. A statistically significant increase in performance is obtained. The best system is finally used to generate a definition for all Spanish synsets, which are currently ready for a manual revision. As a complementary issue, we analyze the impact of the amount of in-domain data needed to improve a system trained entirely on out-of-domain data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jordi Atserias</author>
</authors>
<title>Luis Villarejo, German Rigau, Eneko Agirre,</title>
<location>John Carroll, Bernardo</location>
<marker>Atserias, </marker>
<rawString>Jordi Atserias, Luis Villarejo, German Rigau, Eneko Agirre, John Carroll, Bernardo Magnini, and Piek of the systems. ‘Reference’ corresponds to the expected output.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vossen</author>
</authors>
<title>The MEANING Multilingual Central Repository.</title>
<date>2004</date>
<booktitle>In Proceedings of 2nd GWC.</booktitle>
<marker>Vossen, 2004</marker>
<rawString>Vossen. 2004. The MEANING Multilingual Central Repository. In Proceedings of 2nd GWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="10103" citStr="Banerjee and Lavie, 2005" startWordPosition="1551" endWordPosition="1554">ed on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Marti, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05/. 4 Experimental Evaluation 4.1 Baseline Systems As explained in the introduction we built two individual baseline systems. The first baseline (‘EU’) system is entirely based on the training</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING’88.</booktitle>
<contexts>
<context position="5668" citStr="Brown et al. (1988)" startWordPosition="860" endWordPosition="863">e not domain specific. In this work, we look at the same problem the other way around. We study how to adapt an out-of-domain SMT system using in-domain data. The rest of the paper is organized as follows. In Section 2 the fundamentals of SMT and the components of our MT architecture are described. The experimental setting is described in Section 3. Evaluation is carried out in Section 4. Finally, Section 5 contains error analysis and Section 6 concludes and outlines future work. 2 Background Current state-of-the-art SMT systems are based on ideas borrowed from the Communication Theory field. Brown et al. (1988) suggested that MT can be statistically approximated to the transmission of information through a noisy channel. Given a sentence f = f1..fn (distorted signal), it is possible to approximate the sentence e = e1..em (original signal) which produced f. We need to estimate P(e|f), the probability that a translator produces f as a translation of e. By applying Bayes’ rule it is decomposed into: P(e|f) = P(fje)~P(e) To obtain the string e which maximizes the translation probability for f, a search in the probability space must be performed. Because the denominator is independent of e, we can ignore</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, 1988</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, Robert L. Mercer, , and Paul S. Roossin. 1988. A statistical approach to language translation. In Proceedings of COLING’88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd Internation Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="9975" citStr="Doddington, 2002" startWordPosition="1530" endWordPosition="1532">ly, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Marti, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05/. 4 Experimental Evaluation 4.1 Baseline Systems As explained </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd Internation Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Ulf Hermjakob</author>
<author>Chin-Yew Lin</author>
</authors>
<title>The Use of External Knowledge of Factoid QA.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="26764" citStr="Hovy et al., 2001" startWordPosition="4360" endWordPosition="4363">r precision by selecting a better translation for ‘rate’. However, we observed that SYSTRAN tends in most cases to construct sentences exhibiting a higher degree of grammaticality. 6 Conclusions In this work, we have enriched every synset in Spanish WordNet with a preliminary gloss, which can be later updated in a lighter process of manual revision. Though imperfect, this material constitutes a very valuable resource. For instance, WordNet glosses have been used in the past to generate sense tagged corpora (Mihalcea and Moldovan, 1999), or as external knowledge for Question Answering systems (Hovy et al., 2001). We have also shown the importance of using a small set of in-domain parallel sentences in order to adapt a phrase-based general SMT system to a new domain. In particular, we have worked on specialized language and translation models and on their combination with general models in order to achieve a proper balance between precision (specialized in-domain models) and recall (general out-of-domain models). A substantial increase is consistently obtained according to standard MT evaluation metrics, which has been shown to be statistically significant in the case of BLEU. Broadly speaking, we hav</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2001</marker>
<rawString>Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2001. The Use of External Knowledge of Factoid QA. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<date>2003</date>
<tech>Technical report, http://people.csail.mit.edu/-people/koehn/publications/europarl/.</tech>
<contexts>
<context position="2161" citStr="Koehn, 2003" startWordPosition="317" endWordPosition="318">et with automatically translated glosses1. The source glosses for these translations are taken from the English WordNet (Fellbaum, 1Glosses are short dictionary definitions that accompany WordNet synsets. See examples in Tables 5 and 6. 1998), which is linked, at the synset level, to Spanish WordNet. This resource is available, among other sources, through the Multilingual Central Repository (MCR) developed by the MEANING project (Atserias et al., 2004). We start by empirically testing the performance of a previously developed English–Spanish SMT system, built from the large Europarl corpus2 (Koehn, 2003). The first observation is that this system completely fails to translate the specific WordNet glosses, due to the large language variations in both domains (vocabulary, style, grammar, etc.). Actually, this is confirming one of the main criticisms against SMT, which is its strong domain dependence. Since parameters are estimated from a corpus in a concrete domain, the performance of the system on a different domain is often much worse. This flaw of statistical and machine learning approaches is well known and has been largely described in the NLP literature, for a variety of tasks (e.g., pars</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Philipp Koehn. 2003. Europarl: A Multilingual Corpus for Evaluation of Machine Translation. Technical report, http://people.csail.mit.edu/-people/koehn/publications/europarl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA’04.</booktitle>
<contexts>
<context position="4022" citStr="Koehn (2004" startWordPosition="597" endWordPosition="598">us is available at: http://- people.csail.mit.edu/people/koehn/publications/europarl 3About 10% of the 68,000 Spanish synsets contain a definition, generated without considering its English counterpart. 287 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287–294, Sydney, July 2006. c�2006 Association for Computational Linguistics Models. A substantial increase in performance is achieved, according to several standard MT evaluation metrics. Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by Koehn (2004b) and applied to the BLEU metric. The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations. We present a qualitative error analysis to support these claims. Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results. Apart from the experimental findings, our study has generated a very valuable resource. Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from</context>
<context position="8270" citStr="Koehn, 2004" startWordPosition="1273" endWordPosition="1274">pplied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++. We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase was more than three times longer than the target/source phrase are ignored. Finally, phrase pairs are scored by relative frequency. Note that no smoothing is performed. Regarding the arg-max search, we used the Pharaoh beam search decoder (Koehn, 2004a), which naturally fits with the previous tools. 4http://www.fjoch.com/GIZA++.html P(f) . 288 3 Data Sets and Evaluation Metrics As a general source of English–Spanish parallel text, we used a collection of 730,740 parallel sentences extracted from the Europarl corpus. These correspond exactly to the training data from the Shared Task 2: Exploiting Parallel Texts for Statistical Machine Translation from the ACL-2005 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond5. To be used as specialized source, we extracted, from the MCR, the set of 6,519 English–</context>
<context position="10480" citStr="Koehn, 2004" startWordPosition="1614" endWordPosition="1615">ated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05/. 4 Experimental Evaluation 4.1 Baseline Systems As explained in the introduction we built two individual baseline systems. The first baseline (‘EU’) system is entirely based on the training data from the Europarl corpus. The second baseline system (‘WNG’) is entirely based on the training set from of the in-domain corpus of parallel glosses. In the second case phrase pairs occurring only once in the training corpus are not discarded due to the extremely small size of the corpus. Table 1 shows results of the two baseline systems, both for the development and te</context>
<context position="19667" citStr="Koehn (2004" startWordPosition="3130" endWordPosition="3131">s. As expected, the ‘EU+WNG’ strategy consistently obtains the best results according to all metrics both on the development and test sets, since it allows to better adjust the relative importance of each translation model. However, both techniques achieve a very competitive performance. Results improve, according to BLEU, from 0.13 to 0.16, and from 0.11 to 0.14, for the development and test sets, respectively. We measured the statistical signficance of the overall improvement in BLEU.n4 attained with respect to the baseline results by applying the bootstrap resampling technique described by Koehn (2004b). The 95% confidence intervals extracted from the test set after 8We used values wtmEU = 0.1, wtmWNG = 0.9, BtmEU = 0.1, and BtmWNG = 0.01 10,000 samples are the following: IEU−base = [0.0642, 0.0939], IWNG−base = [0.0788, 0.1112], IEU+WNG−best = [0.1221, 0.1572]. Since the intervals are not ovelapping, we can conclude that the performance of the best combined method is statistically higher than the ones of the two baseline systems. 4.5 How much in-domain data is needed? In principle, the more in-domain data we have the better, but these may be difficult or expensive to collect. Thus, a very</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004a. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings ofAMTA’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<contexts>
<context position="4022" citStr="Koehn (2004" startWordPosition="597" endWordPosition="598">us is available at: http://- people.csail.mit.edu/people/koehn/publications/europarl 3About 10% of the 68,000 Spanish synsets contain a definition, generated without considering its English counterpart. 287 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287–294, Sydney, July 2006. c�2006 Association for Computational Linguistics Models. A substantial increase in performance is achieved, according to several standard MT evaluation metrics. Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by Koehn (2004b) and applied to the BLEU metric. The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations. We present a qualitative error analysis to support these claims. Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results. Apart from the experimental findings, our study has generated a very valuable resource. Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from</context>
<context position="8270" citStr="Koehn, 2004" startWordPosition="1273" endWordPosition="1274">pplied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++. We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase was more than three times longer than the target/source phrase are ignored. Finally, phrase pairs are scored by relative frequency. Note that no smoothing is performed. Regarding the arg-max search, we used the Pharaoh beam search decoder (Koehn, 2004a), which naturally fits with the previous tools. 4http://www.fjoch.com/GIZA++.html P(f) . 288 3 Data Sets and Evaluation Metrics As a general source of English–Spanish parallel text, we used a collection of 730,740 parallel sentences extracted from the Europarl corpus. These correspond exactly to the training data from the Shared Task 2: Exploiting Parallel Texts for Statistical Machine Translation from the ACL-2005 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond5. To be used as specialized source, we extracted, from the MCR, the set of 6,519 English–</context>
<context position="10480" citStr="Koehn, 2004" startWordPosition="1614" endWordPosition="1615">ated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05/. 4 Experimental Evaluation 4.1 Baseline Systems As explained in the introduction we built two individual baseline systems. The first baseline (‘EU’) system is entirely based on the training data from the Europarl corpus. The second baseline system (‘WNG’) is entirely based on the training set from of the in-domain corpus of parallel glosses. In the second case phrase pairs occurring only once in the training corpus are not discarded due to the extremely small size of the corpus. Table 1 shows results of the two baseline systems, both for the development and te</context>
<context position="19667" citStr="Koehn (2004" startWordPosition="3130" endWordPosition="3131">s. As expected, the ‘EU+WNG’ strategy consistently obtains the best results according to all metrics both on the development and test sets, since it allows to better adjust the relative importance of each translation model. However, both techniques achieve a very competitive performance. Results improve, according to BLEU, from 0.13 to 0.16, and from 0.11 to 0.14, for the development and test sets, respectively. We measured the statistical signficance of the overall improvement in BLEU.n4 attained with respect to the baseline results by applying the bootstrap resampling technique described by Koehn (2004b). The 95% confidence intervals extracted from the test set after 8We used values wtmEU = 0.1, wtmWNG = 0.9, BtmEU = 0.1, and BtmWNG = 0.01 10,000 samples are the following: IEU−base = [0.0642, 0.0939], IWNG−base = [0.0788, 0.1112], IEU+WNG−best = [0.1221, 0.1572]. Since the intervals are not ovelapping, we can conclude that the performance of the best combined method is statistically higher than the ones of the two baseline systems. 4.5 How much in-domain data is needed? In principle, the more in-domain data we have the better, but these may be difficult or expensive to collect. Thus, a very</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Gran diccionario de la Lengua Espa˜nola. Larousse Planeta,</booktitle>
<editor>Maria Antonia Marti, editor.</editor>
<location>Barcelona.</location>
<marker>1996</marker>
<rawString>Maria Antonia Marti, editor. 1996. Gran diccionario de la Lengua Espa˜nola. Larousse Planeta, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL’03.</booktitle>
<contexts>
<context position="10052" citStr="Melamed et al., 2003" startWordPosition="1543" endWordPosition="1546">00) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Marti, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05/. 4 Experimental Evaluation 4.1 Baseline Systems As explained in the introduction we built two individual baseline systems. The first basel</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings ofHLT/NAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>An Automatic Method for Generating Sense Tagged Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context position="26687" citStr="Mihalcea and Moldovan, 1999" startWordPosition="4347" endWordPosition="4350">sting an adequate translation for ‘top of something’. In case 5, our system shows higher precision by selecting a better translation for ‘rate’. However, we observed that SYSTRAN tends in most cases to construct sentences exhibiting a higher degree of grammaticality. 6 Conclusions In this work, we have enriched every synset in Spanish WordNet with a preliminary gloss, which can be later updated in a lighter process of manual revision. Though imperfect, this material constitutes a very valuable resource. For instance, WordNet glosses have been used in the past to generate sense tagged corpora (Mihalcea and Moldovan, 1999), or as external knowledge for Question Answering systems (Hovy et al., 2001). We have also shown the importance of using a small set of in-domain parallel sentences in order to adapt a phrase-based general SMT system to a new domain. In particular, we have worked on specialized language and translation models and on their combination with general models in order to achieve a proper balance between precision (specialized in-domain models) and recall (general out-of-domain models). A substantial increase is consistently obtained according to standard MT evaluation metrics, which has been shown </context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. An Automatic Method for Generating Sense Tagged Corpora. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7626" citStr="Och and Ney, 2003" startWordPosition="1170" endWordPosition="1173">System Construction Fortunately, there is a number of freely available tools to build a phrase-based SMT system. We used only standard components and techniques for our basic system, which are all described below. The SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) supports creation and evaluation of a variety of language models. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. In order to build phrase-based translation models, a phrase extraction must be performed on a word-aligned parallel corpus. We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) to generate word alignments We applied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++. We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase was more than three times longer than the target/source phrase are ignored. Finally, phrase pairs are scored by relative frequency. Note that no smoothing is performed. Regarding the arg-max search, we used </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen.</institution>
<contexts>
<context position="6948" citStr="Och, 2002" startWordPosition="1070" endWordPosition="1071"> equation devises three components in a SMT system. First, a language model that estimates P(e). Second, a translation model representing P(f|e). Last, a decoder responsible for performing the arg-max search. Language models are typically estimated from large monolingual corpora, translation models are built out from parallel corpora, and decoders usually perform approximate search, e.g., by using dynamic programming and beam search. However, in word-based models the modeling of the context in which the words occur is very weak. This problem is significantly alleviated by phrase-based models (Och, 2002), which represent nowadays the state-of-the-art in SMT. 2.1 System Construction Fortunately, there is a number of freely available tools to build a phrase-based SMT system. We used only standard components and techniques for our basic system, which are all described below. The SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) supports creation and evaluation of a variety of language models. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. In order to build phrase-based translation models, a phrase extraction must be performed on a wor</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Josef Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176. Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="9912" citStr="Papineni et al., 2001" startWordPosition="1519" endWordPosition="1522">5,843 glosses was 8.25 words for English and 8.13 for Spanish. Finally, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Marti, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5http://www.statmt.org/wpt05</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, IBM Research Report, RC22176. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP’02.</booktitle>
<contexts>
<context position="7279" citStr="Stolcke, 2002" startWordPosition="1120" endWordPosition="1121"> corpora, and decoders usually perform approximate search, e.g., by using dynamic programming and beam search. However, in word-based models the modeling of the context in which the words occur is very weak. This problem is significantly alleviated by phrase-based models (Och, 2002), which represent nowadays the state-of-the-art in SMT. 2.1 System Construction Fortunately, there is a number of freely available tools to build a phrase-based SMT system. We used only standard components and techniques for our basic system, which are all described below. The SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) supports creation and evaluation of a variety of language models. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. In order to build phrase-based translation models, a phrase extraction must be performed on a word-aligned parallel corpus. We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) to generate word alignments We applied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++. We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement. Phrases</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of ICSLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Alicia Tribble</author>
</authors>
<title>Improving Statistical Machine Translation for a Speech-toSpeech Translation Task.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP2002 Workshop on Speech-to-Speech Translation.</booktitle>
<contexts>
<context position="4893" citStr="Vogel and Tribble (2002)" startWordPosition="730" endWordPosition="733">or analysis to support these claims. Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results. Apart from the experimental findings, our study has generated a very valuable resource. Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from being perfect, constitutes an axcellent starting point for a posterior manual revision. Finally, we note that the construction of a SMT system when few domain-specific data are available has been also investigated by other authors. For instance, Vogel and Tribble (2002) studied whether an SMT system for speech-to-speech translation built on top of a small parallel corpus can be improved by adding knowledge sources which are not domain specific. In this work, we look at the same problem the other way around. We study how to adapt an out-of-domain SMT system using in-domain data. The rest of the paper is organized as follows. In Section 2 the fundamentals of SMT and the components of our MT architecture are described. The experimental setting is described in Section 3. Evaluation is carried out in Section 4. Finally, Section 5 contains error analysis and Secti</context>
</contexts>
<marker>Vogel, Tribble, 2002</marker>
<rawString>Stephan Vogel and Alicia Tribble. 2002. Improving Statistical Machine Translation for a Speech-toSpeech Translation Task. In Proceedings ofICSLP2002 Workshop on Speech-to-Speech Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>editor Vox</author>
</authors>
<date>1990</date>
<booktitle>Diccionario Actual de la Lengua Espa˜nola. Bibliograf,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="9673" citStr="Vox, 1990" startWordPosition="1482" endWordPosition="1483"> texts were removed. Parallel glosses were tokenized and case lowered. We discarded some of these parallel glosses based on the difference in length between the source and the target. The gloss average length for the resulting 5,843 glosses was 8.25 words for English and 8.13 for Spanish. Finally, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Marti, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stem</context>
</contexts>
<marker>Vox, 1990</marker>
<rawString>Vox, editor. 1990. Diccionario Actual de la Lengua Espa˜nola. Bibliograf, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William T Vetterling William H Press</author>
<author>Saul A Teukolsky</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C++: the Art ofScientific Computing.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<marker>Press, Teukolsky, Flannery, 2002</marker>
<rawString>William T. Vetterling William H. Press, Saul A. Teukolsky and Brian P. Flannery. 2002. Numerical Recipes in C++: the Art ofScientific Computing. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>