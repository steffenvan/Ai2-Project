<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995855">
A Mixture Model with Sharing for Lexical Semantics
</title>
<author confidence="0.997301">
Joseph Reisinger
</author>
<affiliation confidence="0.998464">
Department of Computer Science
University of Texas at Austin
</affiliation>
<address confidence="0.983817">
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
</address>
<email confidence="0.998899">
joeraii@cs.utexas.edu
</email>
<author confidence="0.990619">
Raymond Mooney
</author>
<affiliation confidence="0.9983665">
Department of Computer Science
University of Texas at Austin
</affiliation>
<address confidence="0.9751425">
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
</address>
<email confidence="0.999191">
mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.994754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99887619047619">
We introduce tiered clustering, a mixture
model capable of accounting for varying de-
grees of shared (context-independent) fea-
ture structure, and demonstrate its applicabil-
ity to inferring distributed representations of
word meaning. Common tasks in lexical se-
mantics such as word relatedness or selec-
tional preference can benefit from modeling
such structure: Polysemous word usage is of-
ten governed by some common background
metaphoric usage (e.g. the senses of line or
run), and likewise modeling the selectional
preference of verbs relies on identifying com-
monalities shared by their typical arguments.
Tiered clustering can also be viewed as a form
of soft feature selection, where features that do
not contribute meaningfully to the clustering
can be excluded. We demonstrate the applica-
bility of tiered clustering, highlighting partic-
ular cases where modeling shared structure is
beneficial and where it can be detrimental.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985152">
Word meaning can be represented as high-
dimensional vectors inhabiting a common space
whose dimensions capture semantic or syntactic
properties of interest (e.g. Erk and Pado, 2008;
Lowe, 2001). Such vector-space representations of
meaning induce measures of word similarity that can
be tuned to correlate well with judgements made
by humans. Previous work has focused on de-
signing feature representations and semantic spaces
that capture salient properties of word meaning (e.g.
Curran, 2004; Gabrilovich and Markovitch, 2007;
Landauer and Dumais, 1997), often leveraging the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Miller and Charles, 1991;
Pereira et al., 1993).
Since vector-space representations are con-
structed at the lexical level, they conflate multiple
word meanings into the same vector, e.g. collaps-
ing occurrences of bankinstitution and bankriver. Meth-
ods such as Clustering by Committee (Pantel, 2003)
and multi-prototype representations (Reisinger and
Mooney, 2010) address this issue by perform-
ing word-sense disambiguation across word occur-
rences, and then building meaning vectors from
the disambiguated words. Such approaches can
readily capture the structure of homonymous words
with several unrelated meanings (e.g. bat and club),
but are not suitable for representing the common
metaphor structure found in highly polysemous
words such as line or run.
In this paper, we introduce tiered clustering, a
novel probabilistic model of the shared structure
often neglected in clustering problems. Tiered
clustering performs soft feature selection, allocat-
ing features between a Dirichlet Process cluster-
ing model and a background model consisting of
a single component. The background model ac-
counts for features commonly shared by all occur-
rences (i.e. context-independent feature variation),
while the clustering model accounts for variation
in word usage (i.e. context-dependent variation, or
word senses; Table 1).
Using the tiered clustering model, we derive a
multi-prototype representation capable of capturing
varying degrees of sharing between word senses,
and demonstrate its effectiveness in lexical seman-
tic tasks where such sharing is desirable. In partic-
ular we show that tiered clustering outperforms the
multi-prototype approach for (1) selectional prefer-
ence (Resnik, 1997; Pantel et al., 2007), i.e. predict-
</bodyText>
<page confidence="0.906083">
1173
</page>
<note confidence="0.815712">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999861941176471">
ing the typical filler of an argument slot of a verb,
and (2) word-relatedness in the presence of highly
polysemous words. The former case exhibits a high
degree of explicit structure, especially for more se-
lectionally restrictive verbs (e.g. the set of things that
can be eaten or can shoot).
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on the
methods compared, Section 3 outlines the multi-
prototype model based on the Dirichlet Process mix-
ture model, Section 4 derives the tiered cluster-
ing model, Section 5 discusses similarity metrics,
Section 6 details the experimental setup and in-
cludes a micro-analysis of feature selection, Section
7 presents results applying tiered clustering to word
relatedness and selectional preference, Section 8 dis-
cusses future work, and Section 9 concludes.
</bodyText>
<sectionHeader confidence="0.992108" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999955966666667">
Models of the attributional similarity of concepts,
i.e. the degree to which concepts overlap based on
their attributes (Turney, 2006), are commonly imple-
mented using vector-spaces derived from (1) word
collocations (Sch¨utze, 1998), directly leveraging the
distributional hypothesis (Miller and Charles, 1991),
(2) syntactic relations (Pad´o and Lapata, 2007), (3)
structured corpora (e.g. Gabrilovich and Markovitch
(2007)) or (4) latent semantic spaces (Finkelstein
et al., 2001; Landauer and Dumais, 1997). Such
models can be evaluated based on their correlation
with human-reported lexical similarity judgements
using e.g. the WordSim-353 collection (Finkelstein
et al., 2001). Distributional methods exhibit a high
degree of scalability (Gorman and Curran, 2006) and
have been applied broadly in information retrieval
(Manning et al., 2008), large-scale taxonomy induc-
tion (Snow et al., 2006), and knowledge acquisition
(Van Durme and Pas¸ca, 2008).
Reisinger and Mooney (2010) introduced a multi-
prototype approach to vector-space lexical seman-
tics where individual words are represented as col-
lections of “prototype” vectors. This representation
is capable of accounting for homonymy and poly-
semy, as well as other forms of variation in word
usage, like similar context-dependent methods (Erk
and Pado, 2008). The set of vectors for a word
is determined by unsupervised word sense discov-
ery (Sch¨utze, 1998), which clusters the contexts in
which a word appears. Average prototype vectors
</bodyText>
<equation confidence="0.89340725">
LIFE
all, about, life, would, death
my, you, real, your, about
spent, years, rest, lived, last
</equation>
<bodyText confidence="0.607383">
sentenced, imprisonment, sentence, prison
insurance, peer, Baron, member, company
Guru, Rabbi, Baba, la, teachings
</bodyText>
<table confidence="0.92482025">
RADIO
station, radio, stations, television
amateur, frequency, waves, system
show, host, personality, American
song, single, released, airplay
operator, contact, communications, message
WIZARD
evil, powerful, magic, wizard
Merlin, King, Arthur, Arthurian
fairy, wicked, scene, tale
Harry, Potter, Voldemort, Dumbledore
STOCK
</table>
<tableCaption confidence="0.700717875">
stock, all, other, company, new
market, crash, markets, price, prices
housing, breeding, fish, water, horses
car, racing, cars, NASCAR, race, engine
card, cards, player, pile, game, paper
rolling, locomotives, line, new, railway
Table 1: Example tiered clustering representation of
words with varying degrees of polysemy. Each boxed
</tableCaption>
<bodyText confidence="0.9975797">
set shows the most common background (shared) fea-
tures, and each prototype captures one thematic usage
of the word. For example, wizard is broken up into a
background cluster describing features common to all us-
ages of the word (e.g., magic and evil) and several genre-
specific usages (e.g. Merlin, fairy tales and Harry Potter).
are then computed separately for each cluster, pro-
ducing a distributed representation for each word.
Distributional methods have also proven to be a
powerful approach to modeling selectional prefer-
ence (Pad´o et al., 2007; Pantel et al., 2007), rivaling
methods based on existing semantic resources such
as WordNet (Clark and Weir, 2002; Resnik, 1997)
and FrameNet (Pad´o, 2007) and performing nearly
as well as supervised methods (Herdaˇgdelen and Ba-
roni, 2009). Selectional preference has been shown
to be useful for, e.g., resolving ambiguous attach-
ments (Hindle and Rooth, 1991), word sense disam-
biguation (McCarthy and Carroll, 2003) and seman-
tic role labeling (Gildea and Jurafsky, 2002).
</bodyText>
<sectionHeader confidence="0.999096" genericHeader="method">
3 Multi-Prototype Models
</sectionHeader>
<bodyText confidence="0.9873815">
Representing words as mixtures over several pro-
totypes has proven to be a powerful approach to
</bodyText>
<page confidence="0.992248">
1174
</page>
<bodyText confidence="0.999941425">
vector-space lexical semantics (Pantel, 2003; Pantel
et al., 2007; Reisinger and Mooney, 2010). In this
section we briefly introduce a version of the multi-
prototype model based on the Dirichlet Process Mix-
ture Model (DPMM), capable of inferring automat-
ically the number of prototypes necessary for each
word (Rasmussen, 2000). Similarity between two
DPMM word-representations is then computed as a
function of their cluster centroids (§5), instead of the
centroid of all the word’s occurrences.
Multiple prototypes for each word w are gener-
ated by clustering feature vectors v(c) derived from
each occurrence c E C(w) in a large textual cor-
pus and collecting the resulting cluster centroids
7rk(w), k E [1, Kw]. This approach is commonly
employed in unsupervised word sense discovery;
however, we do not assume that clusters correspond
to word senses. Rather, we only rely on clusters to
capture meaningful variation in word usage.
Instead of assuming all words can be repre-
sented by the same number of clusters, we allocate
representational flexibility dynamically using the
DPMM. The DPMM is an infinite capacity model
capable of assigning data to a variable, but finite
number of clusters Kw, with probability of assign-
ment to cluster k proportional to the number of data
points previously assigned to k. A single parameter
q controls the degree of smoothing, producing more
uniform clusterings as q ---&gt; oc. Using this model,
the number of clusters no longer needs to be fixed
a priori, allowing the model to allocate expressivity
dynamically to concepts with richer structure. Such
a model naturally allows the word representation to
allocate additional capacity for highly polysemous
words, with the number of clusters growing loga-
rithmically with the number of occurrences. The
DPMM has been used for rational models of con-
cept organization (Sanborn et al., 2006), but to our
knowledge has not yet been applied directly to lexi-
cal semantics.
</bodyText>
<sectionHeader confidence="0.994719" genericHeader="method">
4 Tiered Clustering
</sectionHeader>
<bodyText confidence="0.999601714285714">
Tiered clustering allocates features between two
submodels: a (context-dependent) DPMM and a sin-
gle (context-independent) background component.
This model is similar structurally to the feature se-
lective clustering model proposed by Law et al.
(2002). However, instead of allocating entire feature
dimensions between model and background compo-
</bodyText>
<figure confidence="0.542747">
background
R
Roo
clusters
</figure>
<figureCaption confidence="0.998211">
Figure 1: Plate diagram for the tiered clustering model
with cluster indicators drawn from the Chinese Restau-
rant Process.
</figureCaption>
<bodyText confidence="0.965759277777778">
nents, assignment is done at the level of individual
feature occurrences, much like topic assignment in
Latent Dirichlet Allocation (LDA; Griffiths et al.,
2007). At a high level, the tiered model can be
viewed as a combination of a multi-prototype model
and a single-prototype back-off model. However,
by leveraging both representations in a joint frame-
work, uninformative features can be removed from
the clustering, resulting in more semantically tight
clusters.
Concretely, each word occurrence Wd first selects
a cluster φd from the DPMM; then each feature wi,d
is generated from either the background model φback
or the selected cluster φd, determined by the tier
indicator zi,d. The full generative model for tiered
clustering is given by
Bdlα Beta(α) d E D,
,,//,, φd l β, G0 DP(β, G0) d E D,
</bodyText>
<equation confidence="0.986969285714286">
Ob.klβback Dirichlet(βback)
zi,dlBd Bernoulli(Bd) i E lWdl,
☆ ✫✬✬
Mult(φback)
Mult(φd)
(zi,d = 1) i E lWdl,
(otherwise)
</equation>
<bodyText confidence="0.999832533333333">
where α controls the per-data tier distribution
smoothing and β controls the uniformity of the DP
cluster allocation. The DP is parameterized by a
base measure Go, controlling the per-cluster term
distribution smoothing; which use a Dirichlet with
hyperparameter q, as is common (Figure 1).
Since the background topic is shared across all oc-
currences, it can account for features with context-
independent variance, such as stop words and other
high-frequency noise, as well as the central tendency
of the collection (Table 1). Furthermore, it is possi-
ble to put an asymmetric prior on q, yielding more
fine-grained control over the assumed uniformity of
the occurrence of noisy features, unlike in the model
proposed by Law et al. (2002).
</bodyText>
<figure confidence="0.9900905">
D w
w
z e
c T1
d
T1
α
wi,dlφd, zi,d ✬✬✪
</figure>
<page confidence="0.970179">
1175
</page>
<bodyText confidence="0.999796">
Although exact posterior inference is intractable
in this model, we derive an efficient collapsed Gibbs
sampler via analogy to LDA (Appendix 1).
</bodyText>
<sectionHeader confidence="0.875912" genericHeader="method">
5 Measuring Semantic Similarity
</sectionHeader>
<bodyText confidence="0.976060434782609">
Due to its richer representational structure, comput-
ing similarity in the multi-prototype model is less
straightforward than in the single prototype case.
Reisinger and Mooney (2010) found that simply av-
eraging all similarity scores over all pairs of proto-
types (sampled from the cluster distributions) per-
forms reasonably well and is robust to noise. Given
two words w and w&apos;, this AvgSim metric is
1k✏1
Kw and Kw✶ are the number of clusters for w and w&apos;
respectively, and d♣•, •) is a standard distributional
similarity measure (e.g. cosine distance). As cluster
sizes become more uniform, AvgSim tends towards
the single prototype similarity,1 hence the effective-
ness of AvgSim stems from boosting the influence
of small clusters.
Tiered clustering representations offer more pos-
sibilities for computing semantic similarity than
multi-prototype, as the background prototype can be
treated separately from the other prototypes. We
make use of a simple sum of the distance between
the two background components, and the AvgSim
of the two sets of clustering components.
</bodyText>
<sectionHeader confidence="0.994306" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.990477">
6.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999938333333333">
Word occurrence statistics are collected from a snap-
shot of English Wikipedia taken on Sept. 29th, 2009.
Wikitext markup is removed, as are articles with
fewer than 100 words, leaving 2.8M articles with a
total of 2.05B words. Wikipedia was chosen due to
its semantic breadth.
</bodyText>
<subsectionHeader confidence="0.998425">
6.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.955313714285714">
We evaluate the tiered clustering model on two prob-
lems from lexical semantics: word relatedness and
selectional preference. For the word relatedness
1This can be problematic for certain clustering methods
that specify uniform priors over cluster sizes; however the
DPMM naturally exhibits a linear decay in cluster sizes with
the lE[# clusters of size M] ✏ 77④M.
</bodyText>
<figure confidence="0.967936">
Rating distribution
Sense count distribution
80
10
3
0
WS-353 Evocation Pado
</figure>
<figureCaption confidence="0.99463925">
Figure 2: (top) The distribution of ratings (scaled [0,1])
on WS-353, WN-Evocation and Pad´o datasets. (bottom)
The distribution of sense counts for each data set (log-
domain), collected from WordNet 3.0.
</figureCaption>
<bodyText confidence="0.998741206896552">
evaluation, we compared the predicted similarity of
word pairs from each model to two collections of hu-
man similarity judgements: WordSim-353 (Finkel-
stein et al., 2001) and the Princeton Evocation rela-
tions (WN-Evocation, Ma et al., 2009).
WS-353 contains between 13 and 16 human sim-
ilarity judgements for each of 353 word pairs, rated
on a 1–10 integer scale. WN-Evocation is signif-
icantly larger than WS-353, containing over 100k
similarity comparisons collected from trained hu-
man raters. Comparisons are assigned to only 3-
5 human raters on average and contain a signifi-
cantly higher fraction of zero- and low-similarity
items than WS-353 (Figure 2), reflecting more ac-
curately real-world lexical semantics settings. In our
experiments we discard all comparisons with fewer
than 5 ratings and then sample 10% of the remain-
ing pairs uniformly at random, resulting in a test set
with 1317 comparisons.
For selectional preference, we employ the Pad´o
dataset, which contains 211 verb-noun pairs with
human similarity judgements for how plausible the
noun is for each argument of the verb (2 arguments
per verb, corresponding roughly to subject and ob-
ject). Results are averaged across 20 raters; typical
inter-rater agreement is p ✏ 0.7 (Pad´o et al., 2007).
In all cases correlation with human judgements
is computed using Spearman’s nonparametric rank
correlation (p) with average human judgements
</bodyText>
<figure confidence="0.997355777777778">
1.0
0.5
0.0
Evocation Pado
WS-353
��➳ ��✶ ➳
AvgSim(w, w✶) def 1K d(�k(w), �7 (w ))
w Kw
7✏
</figure>
<page confidence="0.914276">
1176
</page>
<note confidence="0.424134">
(Agirre et al., 2009).
</note>
<subsectionHeader confidence="0.992741">
6.3 Feature Representation
</subsectionHeader>
<bodyText confidence="0.9999673">
In the following analyses we confine ourselves to
representing word occurrences using unordered un-
igrams collected from a window of size T=10 cen-
tered around the occurrence, represented using tf-idf
weighting. Feature vectors are pruned to a fixed
length f, discarding all but the highest-weight fea-
tures (f is selected via empirical validation, as de-
scribed in the next section). Finally, semantic simi-
larity between word pairs is computed using cosine
distance (`2-normalized dot-product).2
</bodyText>
<subsectionHeader confidence="0.968055">
6.4 Feature Pruning
</subsectionHeader>
<bodyText confidence="0.96751303030303">
Feature pruning is one of the most significant factors
in obtaining high correlation with human similarity
judgements using vector-space models, and has been
suggested as one way to improve sense disambigua-
tion for polysemous verbs (Xue et al., 2006). In this
section, we calibrate the single prototype and multi-
prototype methods on WS-353, reaching the limit
of human and oracle performance and demonstrat-
ing robust performance gains even with semanti-
cally impoverished features. In particular we obtain
p=0.75 correlation on WS-353 using only unigram
collocations and p=0.77 using a fixed-K multi-
prototype representation (Figure 3; Reisinger and
Mooney, 2010). This result rivals average human
performance, obtaining correlation near that of the
supervised oracle approach of Agirre et al. (2009).
The optimal pruning cutoff depends on the fea-
ture weighting and number of prototypes as well as
the feature representation. t-test and X2 features are
most robust to feature noise and perform well even
with no pruning; tf-idf yields the best results but is
most sensitive to the pruning parameter (Figure 3).
As the number of features increases, more pruning
is required to combat feature noise.
Figure 4 breaks down the similarity pairs into four
quantiles for each data set and then shows corre-
lation separately for each quantile. In general the
more polarized data quantiles (1 and 4) have higher
correlation, indicating that fine-grained distinctions
2(Parameter robustness) We observe lower correlations on
average for T✏25 and T✏5 and therefore observe T✏10 to
be near-optimal. Substituting weighted Jaccard similarity for
cosine does not significantly affect the results in this paper.
</bodyText>
<figure confidence="0.429064">
Q1 Q2 Q9 Q4 Q1 Q2 Q9 Q4
</figure>
<figureCaption confidence="0.959005125">
Figure 4: Correlation results on WS-353 broken down
over quantiles in the human ratings. Quantile ranges are
shown in Figure 2. In general ratings for highly sim-
ilar (dissimilar) pairs are more predictable (quantiles 1
and 4) than middle similarity pairs (quantiles 2, 3). ESA
shows results for a more semantically rich feature set de-
rived using Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007).
</figureCaption>
<bodyText confidence="0.999682666666667">
in semantic distance are easier for those sets.3 Fea-
ture pruning improves correlations in quantiles 2–4
while reducing correlation in quantile 1 (lowest sim-
ilarity). This result is to be expected as more fea-
tures are necessary to make fine-grained distinctions
between dissimilar pairs.
</bodyText>
<sectionHeader confidence="0.999897" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.999966636363636">
We evaluate four models: (1) the standard single-
prototype approach, (2) the DPMM multi-prototype
approach outlined in §3, (3) a simple combina-
tion of the multi-prototype and single-prototype ap-
proaches (MP+SP)4 and (4) the tiered clustering ap-
proach (§4). Each data set is divided into 5 quan-
tiles based on per-pair average sense counts,5 col-
lected from WordNet 3.0 (Fellbaum, 1998); ex-
amples of pairs in the high-polysemy quantile are
shown in Table 2. Unless otherwise specified,
both DPMM multi-prototype and tiered clustering
</bodyText>
<footnote confidence="0.991472923076923">
3The fact that the per-quantile correlation is significantly
lower than the full correlation e.g. in the human case indicates
that fine-grained ordering (within quantile) is more difficult than
coarse-grained (between quantile).
4(MP+SP) Tiered clustering’s ability to model both shared
and idiosyncratic structure can be easily approximated by us-
ing the single prototype model as the shared component and
multi-prototype model as the clustering. However, unlike in the
tiered model, all features are assigned to both components. We
demonstrate that this simplification actually hurts performance.
5Despite many skewed pairs (e.g. line has 36 senses while
insurance has 3), we found that arithmetic average and geomet-
ric average perform the same.
</footnote>
<figure confidence="0.99544658974359">
unpruned pruned (best)
human
Single-p
Multi-p
ESA
0.7
Spearman&apos;s ρ
0.0
-0.2
1177
10 20 100200 5001k 2k 5k 10k all
tf-idf cosine, K=1,10,50
# of features
K=50
K=10
K=1
Spearman&apos;s ρ
0
K=1 K=10 K=50
0.80.8
0.00
10 20 100200 5001k 2k 5k 10k all
# of features
10 20 100200 500 1k 2k 5k 10k all
# of features
10 20 100200 500 1k 2k 5k 10k all
# of features
tf-idf
ttest
χ2
tf
tf-idf
ttest
χ2
tf
tf-idf
ttest
χ2
tf
</figure>
<figureCaption confidence="0.9608502">
Figure 3: Effects of feature pruning and representation on WS-353 correlation broken down across multi-prototype
representation size. In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation for
moderate levels of pruning and significantly lower correlation than other representations without pruning. The optimal
amount of pruning varies with the number of prototypes used, with fewer features being optimal for more clusters.
Bars show 95% confidence intervals.
</figureCaption>
<table confidence="0.995171764705883">
WordSim-353 Method p • 100 E[C] background
stock-live, start-match, line-insurance, game-
round, street-place, company-stock
Evocation
break-fire, clear-pass, take-call, break-tin,
charge-charge, run-heat, social-play
Pad´o
see-drop, see-return, hit-stock, raise-bank, see-
face, raise-firm, raise-question
Single prototype 73.4±0.5 1.0 -
high polysemy 76.0±0.9 1.0 -
Multi-prototype 76.8±0.4 14.8 -
high polysemy 79.3±1.3 12.5 -
MP+SP 75.4±0.5 14.8 -
high polysemy 80.1±1.0 12.5 -
Tiered 76.9±0.5 27.2 43.0%
high polysemy 83.1±1.0 24.2 43.0%
</table>
<tableCaption confidence="0.7677895">
Table 2: Examples of highly polysemous pairs from each
data set using sense counts from WordNet.
</tableCaption>
<bodyText confidence="0.992486666666667">
use symmetric Dirichlet hyperparameters, 0=0.1,
=0.1, and tiered clustering uses α=10 for the back-
ground/clustering allocation smoother.
</bodyText>
<subsectionHeader confidence="0.997487">
7.1 WordSim-353
</subsectionHeader>
<bodyText confidence="0.997413066666667">
Correlation results for WS-353 are shown in Table
3. In general the approaches incorporating multiple
prototypes outperform single prototype (p = 0.768
vs. p = 0.734). The tiered clustering model does not
significantly outperform either the multi-prototype
or MP+SP models on the full set, but yields signifi-
cantly higher correlation on the high-polysemy set.
The tiered model generates more clusters than
DPMM multi-prototype (27.2 vs. 14.8), despite us-
ing the same hyperparameter settings: Since words
commonly shared across clusters have been allo-
cated to the background component, the cluster
components have less overlap and hence the model
naturally allocates more clusters.
Examples of the tiered clusterings for several
</bodyText>
<tableCaption confidence="0.795525">
Table 3: Spearman’s correlation on the WS-353 data set.
</tableCaption>
<figureCaption confidence="0.5799105">
All refers to the full set of pairs, high polysemy refers to
the top 20% of pairs, ranked by sense count. E[C] is the
average number of clusters employed by each method and
background is the average percentage of features allo-
cated by the tiered model to the background cluster. 95%
confidence intervals are computed via bootstrapping.
</figureCaption>
<bodyText confidence="0.995287">
words from WS-353 are shown in Table 1 and corre-
sponding clusters from the multi-prototype approach
are shown in Table 4. In general the background
component does indeed capture commonalities be-
tween all the sense clusters (e.g. all wizards use
magic) and hence the tiered clusters are more se-
mantically pure. This effect is most visible in the-
matically polysemous words, e.g. radio and wizard.
</bodyText>
<subsectionHeader confidence="0.987301">
7.2 Evocation
</subsectionHeader>
<bodyText confidence="0.99986075">
Compared to WS-353, the WN-Evocation pair set
is sampled more uniformly from English word pairs
and hence contains a significantly larger fraction of
unrelated words, reflecting the fact that word sim-
</bodyText>
<page confidence="0.96219">
1178
</page>
<note confidence="0.343281">
LIFE
</note>
<bodyText confidence="0.861331">
my, you, real, about, your, would
years, spent, rest, lived, last
sentenced, imprisonment, sentence, prison
years, cycle, life, all, expectancy, other
all, life, way, people, human, social, many
</bodyText>
<sectionHeader confidence="0.33863" genericHeader="method">
RADIO
</sectionHeader>
<bodyText confidence="0.9226428">
station, FM, broadcasting, format, AM
radio, station, stations, amateur,
show, station, host, program, radio
stations, song, single, released, airplay
station, operator, radio, equipment, contact
</bodyText>
<sectionHeader confidence="0.588615" genericHeader="method">
WIZARD
</sectionHeader>
<keyword confidence="0.4430185">
evil, magic, powerful, named, world
Merlin, King, Arthur, powerful, court
spells, magic, cast, wizard, spell, witch
Harry, Dresden, series, Potter, character
</keyword>
<sectionHeader confidence="0.548087" genericHeader="method">
STOCK
</sectionHeader>
<bodyText confidence="0.933214888888889">
market, price, stock, company, value, crash
housing, breeding, all, large, stock, many
car, racing, company, cars, summer, NASCAR
stock, extended, folded, card, barrel, cards
rolling, locomotives, new, character, line
Table 4: Example DPMM multi-prototype representation
of words with varying degrees of polysemy. Compared to
the tiered clustering results in Table 1 the multi-prototype
clusters are significantly less pure for thematically poly-
semous words such as radio and wizard.
ilarity is a sparse relation (Figure 2 top). Further-
more, it contains proportionally more highly polyse-
mous words relative to WS-353 (Figure 2 bottom).
On WN-Evocation, the single prototype and
multi-prototype do not differ significantly in terms
of correlation (p=0.198 and p=0.201 respectively;
Table 5), while SP+MP yields significantly lower
correlation (p=0.176), and the tiered model yields
significantly higher correlation (p=0.224). Restrict-
ing to the top 20% of pairs with highest human
similarity judgements yields similar outcomes, with
single prototype, multi-prototype and SP+MP sta-
tistically indistinguishable (p=0.239, p=0.227 and
p=0.235), and tiered clustering yielding signifi-
cantly higher correlation (p=0.277). Likewise tiered
clustering achieves the most significant gains on the
high polysemy subset.
</bodyText>
<subsectionHeader confidence="0.997935">
7.3 Selectional Preference
</subsectionHeader>
<bodyText confidence="0.87125825">
Tiered clustering is a natural model for verb selec-
tional preference, especially for more selectionally
restrictive verbs: the set of words that appear in a
particular argument slot naturally have some kind of
</bodyText>
<table confidence="0.999779076923077">
Method p • 100 ]E[C] background
Single prototype 19.8±0.6 1.0 -
high similarity 23.9±1.1 1.0 -
high polysemy 11.5±1.2 1.0 -
Multi-prototype 20.1±0.5 14.8 -
high similarity 22.7±1.2 14.1 -
high polysemy 13.0±1.3 13.2 -
MP+SP 17.6±0.5 14.8 -
high similarity 23.5±1.2 14.1 -
high polysemy 11.4±1.0 13.2 -
Tiered 22.4±0.6 29.7 46.6%
high similarity 27.7±1.3 29.9 47.2%
high polysemy 15.4±1.1 27.4 46.6%
</table>
<tableCaption confidence="0.992329666666667">
Table 5: Spearman’s correlation on the Evocation data
set. The high similarity subset contains the top 20% of
pairs sorted by average rater score.
</tableCaption>
<table confidence="0.999915888888889">
Method p • 100 ]E[C] background
Single prototype 25.8±0.8 1.0 -
high polysemy 17.3±1.7 1.0 -
Multi-prototype 20.2±1.0 18.5 -
high polysemy 14.1±2.4 17.4 -
MP+SP 19.7±1.0 18.5 -
high polysemy 10.5±2.5 17.4 -
Tiered 29.4±1.0 37.9 41.7%
high polysemy 28.5±2.4 37.4 43.2%
</table>
<tableCaption confidence="0.999264">
Table 6: Spearman’s correlation on the Pad´o data set.
</tableCaption>
<bodyText confidence="0.999874473684211">
commonality (i.e. they can be eaten or can promise).
The background component of the tiered clustering
model can capture such general argument structure.
We model each verb argument slot in the Pad´o set
with a separate tiered clustering model, separating
terms co-occurring with the target verb according to
which slot they fill.
On the Pad´o set, the performance of the DPMM
multi-prototype approach breaks down and it yields
significantly lower correlation with human norms
than the single prototype (p=0.202 vs. p=0.258;
Table 6), due to its inability to capture the shared
structure among verb arguments. Furthermore com-
bining with the single prototype does not signif-
icantly change its performance (p=0.197). Mov-
ing to the tiered model, however, yields significant
improvements in correlation over the other models
(p=0.294), primarily improving correlation in the
case of highly polysemous verbs and arguments.
</bodyText>
<page confidence="0.996967">
1179
</page>
<sectionHeader confidence="0.939251" genericHeader="method">
8 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999988260869565">
We have demonstrated a novel model for dis-
tributional lexical semantics capable of capturing
both shared (context-independent) and idiosyncratic
(context-dependent) structure in a set of word occur-
rences. The benefits of this tiered model were most
pronounced on a selectional preference task, where
there is significant shared structure imposed by con-
ditioning on the verb. Although our results on the
Pad´o are not state of the art,6 we believe this to be
due to the impoverished vector-space design; tiered
clustering can be applied to more expressive vec-
tor spaces, such as those incorporating dependency
parse and FrameNet features.
One potential explanation for the superior perfor-
mance of the tiered model vs. the DPMM multi-
prototype model is simply that it allocates more
clusters to represent each word (Reisinger and
Mooney, 2010). However, we find that decreas-
ing the hyperparameter Q (decreasing vocabulary
smoothing and hence increasing the effective num-
ber of clusters) beyond Q = 0.1 actually hurts multi-
prototype performance. The additional clusters do
not provide more semantic content due to significant
background similarity.
Finally, the DPMM multi-prototype and tiered
clustering models allocate clusters based on the vari-
ance of the underlying data set. We observe a neg-
ative correlation (p=✁0.33) between the number of
clusters allocated by the DPMM and the number of
word senses found in WordNet. This result is most
likely due to our use of unigram context window
features, which induce clustering based on thematic
rather than syntactic differences. Investigating this
issue is future work.
(Future Work) The word similarity experiments
can be expanded by breaking pairs down further into
highly homonymous and highly polysemous pairs,
using e.g. WordNet to determine how closely related
the senses are. With this data it would be interest-
ing to validate the hypothesis that the percentage of
features allocated to the background cluster is corre-
lated with the degree of homonymy.
The basic tiered clustering can be extended with
additional background tiers, allocating more expres-
sivity to model background feature variation. This
class of models covers the spectrum between a pure
</bodyText>
<note confidence="0.80041">
6E.g., Pad´o et al. (2007) report p✏0.515 on the same data.
</note>
<bodyText confidence="0.999675294117647">
topic model (all background tiers) and a pure clus-
tering model and may be reasonable when there is
believed to be more background structure (e.g. when
jointly modeling all verb arguments). Furthermore,
it is straightforward to extend the model to a two-
tier, two-clustering structure capable of additionally
accounting for commonalities between arguments.
Applying more principled feature selection ap-
proaches to vector-space lexical semantics may
yield more significant performance gains. Towards
this end we are currently evaluating two classes of
approaches for setting pruning parameters per-word
instead of globally: (1) subspace clustering, i.e.
unsupervised feature selection (e.g., Parsons et al.,
2004) and (2) multiple clustering, i.e. finding fea-
ture partitions that lead to disparate clusterings (e.g.,
Shafto et al., 2006).
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="method">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999975416666667">
This paper introduced a simple probabilistic model
of tiered clustering inspired by feature selective
clustering that leverages feature exchangeability to
allocate data features between a clustering model
and shared component. The ability to model back-
ground variation, or shared structure, is shown to be
beneficial for modeling words with high polysemy,
yielding increased correlation with human similarity
judgements modeling word relatedness and selec-
tional preference. Furthermore, the tiered clustering
model is shown to significantly outperform related
models, yielding qualitatively more precise clusters.
</bodyText>
<sectionHeader confidence="0.997563" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99938925">
Thanks to Yinon Bentor and Bryan Silverthorn for
many illuminating discussions. This work was sup-
ported by an NSF Graduate Research Fellowship to
the first author, and a Google Research Award.
</bodyText>
<sectionHeader confidence="0.693999" genericHeader="conclusions">
A Collapsed Gibbs Sampler
</sectionHeader>
<bodyText confidence="0.999859">
In order to sample efficiently from this model, we
leverage the Chinese Restaurant Process represen-
tation of the DP (cf., Aldous, 1985), introducing a
per-word-occurrence cluster indicator cd. Word oc-
currence features are then drawn from a combination
of a single cluster component indicated by cd and the
background topic.
By exploiting conjugacy, the latent variables 9, �
and 71d can be integrated out, yielding an efficient
</bodyText>
<page confidence="0.972741">
1180
</page>
<bodyText confidence="0.983144">
collapsed Gibbs sampler. The likelihood of word
occurrence d is given by
</bodyText>
<equation confidence="0.970862">
P ♣wd⑤z, ➵cd,φq ✏
P♣wi,d⑤φcdqδ♣zd,i✏0qP♣wi,d⑤φnoiseqδ♣zd,i✏1q.
i
</equation>
<bodyText confidence="0.999864909090909">
Hence, this model can be viewed as a two-topic
variant of LDA with the addition of a per-word-
occurrence (i.e. document) cluster indicator.7 The
update rule for the latent tier indicator z is similar
to the update rule for 2-topic LDA, with the back-
ground component as the first topic and the second
topic being determined by the per-word-occurrence
cluster indicator c.
We can efficiently approximate p♣z⑤wq via Gibbs
sampling, which requires the complete conditional
posteriors for all zi,d. These are
</bodyText>
<equation confidence="0.9974424">
P♣zi,d ✏ t⑤z✁♣i,dq, w, α, βq ✏
n♣wi,dq +β
t
➦w ♣n ♣wq
t + βq
</equation>
<bodyText confidence="0.957604333333333">
where z✁♣i,dq is shorthand for the set z✁tzi,d✉, n♣wq
t
is the number of occurrences of word w in topic t not
counting wi,d and n♣dq
t is the number of features in
occurrence d assigned to topic t, not counting wi,d.
Likewise sampling the cluster indicators condi-
tioned on the data p♣cd⑤w, c✁d, α,ηq decomposes
into the DP posterior over cluster assignments
and the cluster-conditional Multinomial-Dirichlet
word-occurrence likelihood p♣cd⑤w, c✁d, α, ηq ✏
p♣cd⑤c✁d, ηqp♣wd⑤w✁d, c, z, αq given by
</bodyText>
<equation confidence="0.890051666666667">
P♣cd ✏ knew⑤c✁d,α, ηq✾ η
m♣✁dq ✌+
η
</equation>
<bodyText confidence="0.992948333333333">
where m♣✁dq
k is the number of occurrences as-
signed to k not including d, ÝÑn ♣dq
k is the vector of
counts of words from occurrence wd assigned to
7Effectively, the tiered clustering model is a special case of
the nested Chinese Restaurant Process with the tree depth fixed
to two (Blei et al., 2003).
cluster k (i.e. words with zi,d ✏ 0) and C♣☎q is
the normalizing constant for the Dirichlet C♣aq ✏
&amp;quot;♣➦mj✏1 ajq✁1Hm j✏1 F♣ajq operating over vectors
of counts a.
</bodyText>
<sectionHeader confidence="0.973235" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.965972794871795">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and Wordnet-based approaches. In Proc.
of NAACL-HLT-09, pages 19–27.
David J. Aldous. 1985. Exchangeability and related
topics. In ´Ecole d’´et´e de probabilit´es de Saint-
Flour, XIII—1983, volume 1117, pages 1–198.
Springer, Berlin.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187–206.
James Richard Curran. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis, University
of Edinburgh. College of Science.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context.
In Proceedings of EMNLP 2008.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database and Some of its Ap-
plications. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
Evgeniy Gabrilovich and Shaul Markovitch.
2007. Computing semantic relatedness using
Wikipedia-based explicit semantic analysis. In
Proc. of IJCAI-07, pages 1606–1611.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Lin-
guistics, 28(3):245–288.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
</reference>
<figure confidence="0.538136473684211">
n♣dq � α
t
♣n ♣dq + αq .
P♣cd ✏ kold⑤c✁d, α,ηq✾
mkdq
✄m♣✁dq
✌
+ η
p♣Cd⑤C-d,ηq
C♣α + ÝÑ n ♣✁dq
k + ÝÑ n ♣dq
✌ qq
❧♦♠♦♥
C♣α + ÝÑ n ♣✁dq
k q
p♣wd⑤w-d,C,z,αq
C♣α + ÝÑ n ♣dq
✌ q
C♣αq
</figure>
<page confidence="0.973111">
1181
</page>
<reference confidence="0.999883483516484">
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Amac¸ Herdaˇgdelen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proc. of GEMS 2009.
Donald Hindle and Mats Rooth. 1991. Structural
ambiguity and lexical relations. In Proc. of ACL
1991.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211–240.
Martin H. C. Law, Anil K. Jain, and M´ario A. T.
Figueiredo. 2002. Feature selection in mixture-
based clustering. In Proc. of NIPS 2002.
Will Lowe. 2001. Towards a theory of semantic
space. In Proceedings of the 23rd Annual Meeting
of the Cognitive Science Society, pages 576–581.
Xiaojuan Ma, Jordan Boyd-Graber, Sonya S.
Nikolova, and Perry Cook. 2009. Speaking
through pictures: Images vs. icons. In ACM Con-
ference on Computers and Accessibility.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Com-
putational Linguistics, 29(4):639–654.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1–28.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161–199.
Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proc. of EMNLP 2007.
Ulrike Pad´o. 2007. The Integration of Syntax and Se-
mantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Ph.D. thesis, Saarland Uni-
versity, Saarbr¨ucken.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski,
and Eduard Hovy. 2007. ISP: Learning inferen-
tial selectional preferences. In In Proceedings of
NAACL 2007.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, Edmonton, Alta., Canada.
Lance Parsons, Ehtesham Haque, and Huan Liu.
2004. Subspace clustering for high dimensional
data: A review. SIGKDD Explor. Newsl., 6(1).
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proc. of ACL 1993.
Carl E. Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Informa-
tion Processing Systems. MIT Press.
Joseph Reisinger and Raymond Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Proc. of NAACL 2010.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52–57. ACL.
Adam N. Sanborn, Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th An-
nual Conference of the Cognitive Science Society.
Hinrich Sch¨utze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97–123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Benjamin Van Durme and Marius Pas¸ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
Nianwen Xue, Jinying Chen, and Martha Palmer.
2006. Aligning features with sense distinction di-
mensions. In Proc. of COLING/ACL 2006.
</reference>
<page confidence="0.995748">
1182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369439">
<title confidence="0.998834">A Mixture Model with Sharing for Lexical Semantics</title>
<author confidence="0.994251">Joseph</author>
<affiliation confidence="0.995013">Department of Computer University of Texas at</affiliation>
<address confidence="0.974759">1616 Guadalupe, Suite</address>
<author confidence="0.493785">TX Austin</author>
<email confidence="0.999391">joeraii@cs.utexas.edu</email>
<author confidence="0.999797">Raymond Mooney</author>
<affiliation confidence="0.995948">Department of Computer University of Texas at</affiliation>
<address confidence="0.994582">1616 Guadalupe, Suite 2.408</address>
<author confidence="0.811913">TX Austin</author>
<email confidence="0.999648">mooney@cs.utexas.edu</email>
<abstract confidence="0.998672">introduce a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background usage (e.g. the senses of and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and Wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT-09,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and Wordnet-based approaches. In Proc. of NAACL-HLT-09, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>In ´Ecole d’´et´e de probabilit´es de SaintFlour, XIII—1983,</booktitle>
<volume>1117</volume>
<pages>1--198</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="31871" citStr="Aldous, 1985" startWordPosition="4838" endWordPosition="4839">sed correlation with human similarity judgements modeling word relatedness and selectional preference. Furthermore, the tiered clustering model is shown to significantly outperform related models, yielding qualitatively more precise clusters. Acknowledgments Thanks to Yinon Bentor and Bryan Silverthorn for many illuminating discussions. This work was supported by an NSF Graduate Research Fellowship to the first author, and a Google Research Award. A Collapsed Gibbs Sampler In order to sample efficiently from this model, we leverage the Chinese Restaurant Process representation of the DP (cf., Aldous, 1985), introducing a per-word-occurrence cluster indicator cd. Word occurrence features are then drawn from a combination of a single cluster component indicated by cd and the background topic. By exploiting conjugacy, the latent variables 9, � and 71d can be integrated out, yielding an efficient 1180 collapsed Gibbs sampler. The likelihood of word occurrence d is given by P ♣wdz, ➵cd,φq ✏ P♣wi,dφcdqδ♣zd,i✏0qP♣wi,dφnoiseqδ♣zd,i✏1q. i Hence, this model can be viewed as a two-topic variant of LDA with the addition of a per-wordoccurrence (i.e. document) cluster indicator.7 The update rule for the </context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>David J. Aldous. 1985. Exchangeability and related topics. In ´Ecole d’´et´e de probabilit´es de SaintFlour, XIII—1983, volume 1117, pages 1–198. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Thomas Griffiths</author>
<author>Michael Jordan</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese restaurant process.</title>
<date>2003</date>
<booktitle>In Proc. NIPS-2003.</booktitle>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>David Blei, Thomas Griffiths, Michael Jordan, and Joshua Tenenbaum. 2003. Hierarchical topic models and the nested Chinese restaurant process. In Proc. NIPS-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="7753" citStr="Clark and Weir, 2002" startWordPosition="1130" endWordPosition="1133"> features, and each prototype captures one thematic usage of the word. For example, wizard is broken up into a background cluster describing features common to all usages of the word (e.g., magic and evil) and several genrespecific usages (e.g. Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we b</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Richard Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh. College of Science.</institution>
<contexts>
<context position="1785" citStr="Curran, 2004" startWordPosition="260" endWordPosition="261">hlighting particular cases where modeling shared structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word oc</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James Richard Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh. College of Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1473" citStr="Erk and Pado, 2008" startWordPosition="212" endWordPosition="215">ectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, th</context>
<context position="6040" citStr="Erk and Pado, 2008" startWordPosition="878" endWordPosition="881">l methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsupervised word sense discovery (Sch¨utze, 1998), which clusters the contexts in which a word appears. Average prototype vectors LIFE all, about, life, would, death my, you, real, your, about spent, years, rest, lived, last sentenced, imprisonment, sentence, prison insurance, peer, Baron, member, company Guru, Rabbi, Baba, la, teachings RADIO station, radio, stations, television amateur, frequency, waves, system show, host, personality, American song, single, released, airplay operator, contact, communications, message WIZARD evil, powerful, m</context>
</contexts>
<marker>Erk, Pado, 2008</marker>
<rawString>Katrin Erk and Sebastian Pado. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW</booktitle>
<contexts>
<context position="5206" citStr="Finkelstein et al., 2001" startWordPosition="757" endWordPosition="760">lying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are</context>
<context position="14798" citStr="Finkelstein et al., 2001" startWordPosition="2243" endWordPosition="2247">lustering methods that specify uniform priors over cluster sizes; however the DPMM naturally exhibits a linear decay in cluster sizes with the lE[# clusters of size M] ✏ 77M. Rating distribution Sense count distribution 80 10 3 0 WS-353 Evocation Pado Figure 2: (top) The distribution of ratings (scaled [0,1]) on WS-353, WN-Evocation and Pad´o datasets. (bottom) The distribution of sense counts for each data set (logdomain), collected from WordNet 3.0. evaluation, we compared the predicted similarity of word pairs from each model to two collections of human similarity judgements: WordSim-353 (Finkelstein et al., 2001) and the Princeton Evocation relations (WN-Evocation, Ma et al., 2009). WS-353 contains between 13 and 16 human similarity judgements for each of 353 word pairs, rated on a 1–10 integer scale. WN-Evocation is significantly larger than WS-353, containing over 100k similarity comparisons collected from trained human raters. Comparisons are assigned to only 3- 5 human raters on average and contain a significantly higher fraction of zero- and low-similarity items than WS-353 (Figure 2), reflecting more accurately real-world lexical semantics settings. In our experiments we discard all comparisons </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI-07,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="1819" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="262" endWordPosition="265">icular cases where modeling shared structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meani</context>
<context position="5149" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="748" endWordPosition="751">icro-analysis of feature selection, Section 7 presents results applying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to </context>
<context position="18815" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2874" endWordPosition="2877">ower correlations on average for T✏25 and T✏5 and therefore observe T✏10 to be near-optimal. Substituting weighted Jaccard similarity for cosine does not significantly affect the results in this paper. Q1 Q2 Q9 Q4 Q1 Q2 Q9 Q4 Figure 4: Correlation results on WS-353 broken down over quantiles in the human ratings. Quantile ranges are shown in Figure 2. In general ratings for highly similar (dissimilar) pairs are more predictable (quantiles 1 and 4) than middle similarity pairs (quantiles 2, 3). ESA shows results for a more semantically rich feature set derived using Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). in semantic distance are easier for those sets.3 Feature pruning improves correlations in quantiles 2–4 while reducing correlation in quantile 1 (lowest similarity). This result is to be expected as more features are necessary to make fine-grained distinctions between dissimilar pairs. 7 Results We evaluate four models: (1) the standard singleprototype approach, (2) the DPMM multi-prototype approach outlined in §3, (3) a simple combination of the multi-prototype and single-prototype approaches (MP+SP)4 and (4) the tiered clustering approach (§4). Each data set is divided into 5 quantiles bas</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proc. of IJCAI-07, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="8110" citStr="Gildea and Jurafsky, 2002" startWordPosition="1184" endWordPosition="1187">ted representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster centroids (§5), instead of the centroid of all th</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5493" citStr="Gorman and Curran, 2006" startWordPosition="796" endWordPosition="799">only implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsu</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>James Gorman and James R. Curran. 2006. Scaling distributional similarity to large corpora. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<pages>114--2007</pages>
<contexts>
<context position="10859" citStr="Griffiths et al., 2007" startWordPosition="1613" endWordPosition="1616">atures between two submodels: a (context-dependent) DPMM and a single (context-independent) background component. This model is similar structurally to the feature selective clustering model proposed by Law et al. (2002). However, instead of allocating entire feature dimensions between model and background compobackground R Roo clusters Figure 1: Plate diagram for the tiered clustering model with cluster indicators drawn from the Chinese Restaurant Process. nents, assignment is done at the level of individual feature occurrences, much like topic assignment in Latent Dirichlet Allocation (LDA; Griffiths et al., 2007). At a high level, the tiered model can be viewed as a combination of a multi-prototype model and a single-prototype back-off model. However, by leveraging both representations in a joint framework, uninformative features can be removed from the clustering, resulting in more semantically tight clusters. Concretely, each word occurrence Wd first selects a cluster φd from the DPMM; then each feature wi,d is generated from either the background model φback or the selected cluster φd, determined by the tier indicator zi,d. The full generative model for tiered clustering is given by Bdlα Beta(α) d </context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114:2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amac¸ Herdaˇgdelen</author>
<author>Marco Baroni</author>
</authors>
<title>Bagpack: A general framework to represent semantic relations.</title>
<date>2009</date>
<booktitle>In Proc. of GEMS</booktitle>
<marker>Herdaˇgdelen, Baroni, 2009</marker>
<rawString>Amac¸ Herdaˇgdelen and Marco Baroni. 2009. Bagpack: A general framework to represent semantic relations. In Proc. of GEMS 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1991</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="7999" citStr="Hindle and Rooth, 1991" startWordPosition="1167" endWordPosition="1170">. Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-re</context>
</contexts>
<marker>Hindle, Rooth, 1991</marker>
<rawString>Donald Hindle and Mats Rooth. 1991. Structural ambiguity and lexical relations. In Proc. of ACL 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1847" citStr="Landauer and Dumais, 1997" startWordPosition="266" endWordPosition="269"> structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambig</context>
<context position="5234" citStr="Landauer and Dumais, 1997" startWordPosition="761" endWordPosition="764"> word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin H C Law</author>
<author>Anil K Jain</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Feature selection in mixturebased clustering.</title>
<date>2002</date>
<booktitle>In Proc. of NIPS</booktitle>
<contexts>
<context position="10456" citStr="Law et al. (2002)" startWordPosition="1554" endWordPosition="1557">ly allows the word representation to allocate additional capacity for highly polysemous words, with the number of clusters growing logarithmically with the number of occurrences. The DPMM has been used for rational models of concept organization (Sanborn et al., 2006), but to our knowledge has not yet been applied directly to lexical semantics. 4 Tiered Clustering Tiered clustering allocates features between two submodels: a (context-dependent) DPMM and a single (context-independent) background component. This model is similar structurally to the feature selective clustering model proposed by Law et al. (2002). However, instead of allocating entire feature dimensions between model and background compobackground R Roo clusters Figure 1: Plate diagram for the tiered clustering model with cluster indicators drawn from the Chinese Restaurant Process. nents, assignment is done at the level of individual feature occurrences, much like topic assignment in Latent Dirichlet Allocation (LDA; Griffiths et al., 2007). At a high level, the tiered model can be viewed as a combination of a multi-prototype model and a single-prototype back-off model. However, by leveraging both representations in a joint framework</context>
<context position="12355" citStr="Law et al. (2002)" startWordPosition="1857" endWordPosition="1860">he DP is parameterized by a base measure Go, controlling the per-cluster term distribution smoothing; which use a Dirichlet with hyperparameter q, as is common (Figure 1). Since the background topic is shared across all occurrences, it can account for features with contextindependent variance, such as stop words and other high-frequency noise, as well as the central tendency of the collection (Table 1). Furthermore, it is possible to put an asymmetric prior on q, yielding more fine-grained control over the assumed uniformity of the occurrence of noisy features, unlike in the model proposed by Law et al. (2002). D w w z e c T1 d T1 α wi,dlφd, zi,d ✬✬✪ 1175 Although exact posterior inference is intractable in this model, we derive an efficient collapsed Gibbs sampler via analogy to LDA (Appendix 1). 5 Measuring Semantic Similarity Due to its richer representational structure, computing similarity in the multi-prototype model is less straightforward than in the single prototype case. Reisinger and Mooney (2010) found that simply averaging all similarity scores over all pairs of prototypes (sampled from the cluster distributions) performs reasonably well and is robust to noise. Given two words w and w&apos;</context>
</contexts>
<marker>Law, Jain, Figueiredo, 2002</marker>
<rawString>Martin H. C. Law, Anil K. Jain, and M´ario A. T. Figueiredo. 2002. Feature selection in mixturebased clustering. In Proc. of NIPS 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
</authors>
<title>Towards a theory of semantic space.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>576--581</pages>
<contexts>
<context position="1486" citStr="Lowe, 2001" startWordPosition="216" endWordPosition="217">of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate m</context>
</contexts>
<marker>Lowe, 2001</marker>
<rawString>Will Lowe. 2001. Towards a theory of semantic space. In Proceedings of the 23rd Annual Meeting of the Cognitive Science Society, pages 576–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojuan Ma</author>
<author>Jordan Boyd-Graber</author>
<author>Sonya S Nikolova</author>
<author>Perry Cook</author>
</authors>
<title>Speaking through pictures: Images vs. icons.</title>
<date>2009</date>
<booktitle>In ACM Conference on Computers and Accessibility.</booktitle>
<contexts>
<context position="14868" citStr="Ma et al., 2009" startWordPosition="2255" endWordPosition="2258">PMM naturally exhibits a linear decay in cluster sizes with the lE[# clusters of size M] ✏ 77M. Rating distribution Sense count distribution 80 10 3 0 WS-353 Evocation Pado Figure 2: (top) The distribution of ratings (scaled [0,1]) on WS-353, WN-Evocation and Pad´o datasets. (bottom) The distribution of sense counts for each data set (logdomain), collected from WordNet 3.0. evaluation, we compared the predicted similarity of word pairs from each model to two collections of human similarity judgements: WordSim-353 (Finkelstein et al., 2001) and the Princeton Evocation relations (WN-Evocation, Ma et al., 2009). WS-353 contains between 13 and 16 human similarity judgements for each of 353 word pairs, rated on a 1–10 integer scale. WN-Evocation is significantly larger than WS-353, containing over 100k similarity comparisons collected from trained human raters. Comparisons are assigned to only 3- 5 human raters on average and contain a significantly higher fraction of zero- and low-similarity items than WS-353 (Figure 2), reflecting more accurately real-world lexical semantics settings. In our experiments we discard all comparisons with fewer than 5 ratings and then sample 10% of the remaining pairs u</context>
</contexts>
<marker>Ma, Boyd-Graber, Nikolova, Cook, 2009</marker>
<rawString>Xiaojuan Ma, Jordan Boyd-Graber, Sonya S. Nikolova, and Perry Cook. 2009. Speaking through pictures: Images vs. icons. In ACM Conference on Computers and Accessibility.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="8055" citStr="McCarthy and Carroll, 2003" startWordPosition="1175" endWordPosition="1178">mputed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cl</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="1973" citStr="Miller and Charles, 1991" startWordPosition="284" endWordPosition="287">ors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words. Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat a</context>
<context position="5035" citStr="Miller and Charles, 1991" startWordPosition="733" endWordPosition="736">g model, Section 5 discusses similarity metrics, Section 6 details the experimental setup and includes a micro-analysis of feature selection, Section 7 presents results applying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowle</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<marker>Pad´o, Pad´o, Erk, 2007</marker>
<rawString>Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proc. of EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Pad´o</author>
</authors>
<title>The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Sentence Processing.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University, Saarbr¨ucken.</institution>
<marker>Pad´o, 2007</marker>
<rawString>Ulrike Pad´o. 2007. The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Sentence Processing. Ph.D. thesis, Saarland University, Saarbr¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences. In</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="3658" citStr="Pantel et al., 2007" startWordPosition="527" endWordPosition="530">accounts for features commonly shared by all occurrences (i.e. context-independent feature variation), while the clustering model accounts for variation in word usage (i.e. context-dependent variation, or word senses; Table 1). Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable. In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference (Resnik, 1997; Pantel et al., 2007), i.e. predict1173 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words. The former case exhibits a high degree of explicit structure, especially for more selectionally restrictive verbs (e.g. the set of things that can be eaten or can shoot). The remainder of the paper is organized as follows: Section 2 gives relevant background o</context>
<context position="7660" citStr="Pantel et al., 2007" startWordPosition="1116" endWordPosition="1119">ds with varying degrees of polysemy. Each boxed set shows the most common background (shared) features, and each prototype captures one thematic usage of the word. For example, wizard is broken up into a background cluster describing features common to all usages of the word (e.g., magic and evil) and several genrespecific usages (e.g. Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical se</context>
</contexts>
<marker>Pantel, Bhagat, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In In Proceedings of NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Andre Pantel</author>
</authors>
<title>Clustering by committee.</title>
<date>2003</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Edmonton, Alta., Canada.</location>
<contexts>
<context position="2246" citStr="Pantel, 2003" startWordPosition="326" endWordPosition="327">s. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words. Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat and club), but are not suitable for representing the common metaphor structure found in highly polysemous words such as line or run. In this paper, we introduce tiered clustering, a novel probabilistic model of the shared structure often neglected in clustering problems. Ti</context>
<context position="8281" citStr="Pantel, 2003" startWordPosition="1211" endWordPosition="1212">g methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster centroids (§5), instead of the centroid of all the word’s occurrences. Multiple prototypes for each word w are generated by clustering feature vectors v(c) derived from each occurrence c E C(w) in a large textual corpus </context>
</contexts>
<marker>Pantel, 2003</marker>
<rawString>Patrick Andre Pantel. 2003. Clustering by committee. Ph.D. thesis, Edmonton, Alta., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Parsons</author>
<author>Ehtesham Haque</author>
<author>Huan Liu</author>
</authors>
<title>Subspace clustering for high dimensional data: A review.</title>
<date>2004</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="30747" citStr="Parsons et al., 2004" startWordPosition="4675" endWordPosition="4678"> there is believed to be more background structure (e.g. when jointly modeling all verb arguments). Furthermore, it is straightforward to extend the model to a twotier, two-clustering structure capable of additionally accounting for commonalities between arguments. Applying more principled feature selection approaches to vector-space lexical semantics may yield more significant performance gains. Towards this end we are currently evaluating two classes of approaches for setting pruning parameters per-word instead of globally: (1) subspace clustering, i.e. unsupervised feature selection (e.g., Parsons et al., 2004) and (2) multiple clustering, i.e. finding feature partitions that lead to disparate clusterings (e.g., Shafto et al., 2006). 9 Conclusions This paper introduced a simple probabilistic model of tiered clustering inspired by feature selective clustering that leverages feature exchangeability to allocate data features between a clustering model and shared component. The ability to model background variation, or shared structure, is shown to be beneficial for modeling words with high polysemy, yielding increased correlation with human similarity judgements modeling word relatedness and selectiona</context>
</contexts>
<marker>Parsons, Haque, Liu, 2004</marker>
<rawString>Lance Parsons, Ehtesham Haque, and Huan Liu. 2004. Subspace clustering for high dimensional data: A review. SIGKDD Explor. Newsl., 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="1996" citStr="Pereira et al., 1993" startWordPosition="288" endWordPosition="291">ace whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words. Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat and club), but are not s</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proc. of ACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl E Rasmussen</author>
</authors>
<title>The infinite Gaussian mixture model.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8562" citStr="Rasmussen, 2000" startWordPosition="1256" endWordPosition="1257">olving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster centroids (§5), instead of the centroid of all the word’s occurrences. Multiple prototypes for each word w are generated by clustering feature vectors v(c) derived from each occurrence c E C(w) in a large textual corpus and collecting the resulting cluster centroids 7rk(w), k E [1, Kw]. This approach is commonly employed in unsupervised word sense discovery; however, we do not assume that clusters correspond to word senses. Rather, we only rely on clusters to capture meaningful variation in word </context>
</contexts>
<marker>Rasmussen, 2000</marker>
<rawString>Carl E. Rasmussen. 2000. The infinite Gaussian mixture model. In Advances in Neural Information Processing Systems. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="2311" citStr="Reisinger and Mooney, 2010" startWordPosition="331" endWordPosition="334">epresentations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver. Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words. Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat and club), but are not suitable for representing the common metaphor structure found in highly polysemous words such as line or run. In this paper, we introduce tiered clustering, a novel probabilistic model of the shared structure often neglected in clustering problems. Tiered clustering performs soft feature selection, allocating featu</context>
<context position="5708" citStr="Reisinger and Mooney (2010)" startWordPosition="828" endWordPosition="831">007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsupervised word sense discovery (Sch¨utze, 1998), which clusters the contexts in which a word appears. Average prototype vectors LIFE all, about, life, would, death my, you, real, your, about spent, years, rest, lived</context>
<context position="8331" citStr="Reisinger and Mooney, 2010" startWordPosition="1217" endWordPosition="1220">c resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster centroids (§5), instead of the centroid of all the word’s occurrences. Multiple prototypes for each word w are generated by clustering feature vectors v(c) derived from each occurrence c E C(w) in a large textual corpus and collecting the resulting cluster centroids 7rk</context>
<context position="12761" citStr="Reisinger and Mooney (2010)" startWordPosition="1923" endWordPosition="1926">(Table 1). Furthermore, it is possible to put an asymmetric prior on q, yielding more fine-grained control over the assumed uniformity of the occurrence of noisy features, unlike in the model proposed by Law et al. (2002). D w w z e c T1 d T1 α wi,dlφd, zi,d ✬✬✪ 1175 Although exact posterior inference is intractable in this model, we derive an efficient collapsed Gibbs sampler via analogy to LDA (Appendix 1). 5 Measuring Semantic Similarity Due to its richer representational structure, computing similarity in the multi-prototype model is less straightforward than in the single prototype case. Reisinger and Mooney (2010) found that simply averaging all similarity scores over all pairs of prototypes (sampled from the cluster distributions) performs reasonably well and is robust to noise. Given two words w and w&apos;, this AvgSim metric is 1k✏1 Kw and Kw✶ are the number of clusters for w and w&apos; respectively, and d♣•, •) is a standard distributional similarity measure (e.g. cosine distance). As cluster sizes become more uniform, AvgSim tends towards the single prototype similarity,1 hence the effectiveness of AvgSim stems from boosting the influence of small clusters. Tiered clustering representations offer more pos</context>
<context position="17352" citStr="Reisinger and Mooney, 2010" startWordPosition="2642" endWordPosition="2645"> significant factors in obtaining high correlation with human similarity judgements using vector-space models, and has been suggested as one way to improve sense disambiguation for polysemous verbs (Xue et al., 2006). In this section, we calibrate the single prototype and multiprototype methods on WS-353, reaching the limit of human and oracle performance and demonstrating robust performance gains even with semantically impoverished features. In particular we obtain p=0.75 correlation on WS-353 using only unigram collocations and p=0.77 using a fixed-K multiprototype representation (Figure 3; Reisinger and Mooney, 2010). This result rivals average human performance, obtaining correlation near that of the supervised oracle approach of Agirre et al. (2009). The optimal pruning cutoff depends on the feature weighting and number of prototypes as well as the feature representation. t-test and X2 features are most robust to feature noise and perform well even with no pruning; tf-idf yields the best results but is most sensitive to the pruning parameter (Figure 3). As the number of features increases, more pruning is required to combat feature noise. Figure 4 breaks down the similarity pairs into four quantiles for</context>
<context position="28608" citStr="Reisinger and Mooney, 2010" startWordPosition="4352" endWordPosition="4355">fits of this tiered model were most pronounced on a selectional preference task, where there is significant shared structure imposed by conditioning on the verb. Although our results on the Pad´o are not state of the art,6 we believe this to be due to the impoverished vector-space design; tiered clustering can be applied to more expressive vector spaces, such as those incorporating dependency parse and FrameNet features. One potential explanation for the superior performance of the tiered model vs. the DPMM multiprototype model is simply that it allocates more clusters to represent each word (Reisinger and Mooney, 2010). However, we find that decreasing the hyperparameter Q (decreasing vocabulary smoothing and hence increasing the effective number of clusters) beyond Q = 0.1 actually hurts multiprototype performance. The additional clusters do not provide more semantic content due to significant background similarity. Finally, the DPMM multi-prototype and tiered clustering models allocate clusters based on the variance of the underlying data set. We observe a negative correlation (p=✁0.33) between the number of clusters allocated by the DPMM and the number of word senses found in WordNet. This result is most</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proc. of NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics,</booktitle>
<pages>52--57</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3636" citStr="Resnik, 1997" startWordPosition="525" endWordPosition="526">kground model accounts for features commonly shared by all occurrences (i.e. context-independent feature variation), while the clustering model accounts for variation in word usage (i.e. context-dependent variation, or word senses; Table 1). Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable. In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference (Resnik, 1997; Pantel et al., 2007), i.e. predict1173 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words. The former case exhibits a high degree of explicit structure, especially for more selectionally restrictive verbs (e.g. the set of things that can be eaten or can shoot). The remainder of the paper is organized as follows: Section 2 gives</context>
<context position="7768" citStr="Resnik, 1997" startWordPosition="1134" endWordPosition="1135">ototype captures one thematic usage of the word. For example, wizard is broken up into a background cluster describing features common to all usages of the word (e.g., magic and evil) and several genrespecific usages (e.g. Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to 1174 vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduc</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 52–57. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam N Sanborn</author>
<author>Thomas L Griffiths</author>
<author>Daniel J Navarro</author>
</authors>
<title>A more rational model of categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="10107" citStr="Sanborn et al., 2006" startWordPosition="1502" endWordPosition="1505">nal to the number of data points previously assigned to k. A single parameter q controls the degree of smoothing, producing more uniform clusterings as q ---&gt; oc. Using this model, the number of clusters no longer needs to be fixed a priori, allowing the model to allocate expressivity dynamically to concepts with richer structure. Such a model naturally allows the word representation to allocate additional capacity for highly polysemous words, with the number of clusters growing logarithmically with the number of occurrences. The DPMM has been used for rational models of concept organization (Sanborn et al., 2006), but to our knowledge has not yet been applied directly to lexical semantics. 4 Tiered Clustering Tiered clustering allocates features between two submodels: a (context-dependent) DPMM and a single (context-independent) background component. This model is similar structurally to the feature selective clustering model proposed by Law et al. (2002). However, instead of allocating entire feature dimensions between model and background compobackground R Roo clusters Figure 1: Plate diagram for the tiered clustering model with cluster indicators drawn from the Chinese Restaurant Process. nents, as</context>
</contexts>
<marker>Sanborn, Griffiths, Navarro, 2006</marker>
<rawString>Adam N. Sanborn, Thomas L. Griffiths, and Daniel J. Navarro. 2006. A more rational model of categorization. In Proceedings of the 28th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Shafto</author>
<author>Charles Kemp</author>
<author>Vikash Mansinghka</author>
<author>Matthew Gordon</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Learning cross-cutting systems of categories.</title>
<date>2006</date>
<booktitle>In Proc. CogSci</booktitle>
<contexts>
<context position="30871" citStr="Shafto et al., 2006" startWordPosition="4694" endWordPosition="4697">htforward to extend the model to a twotier, two-clustering structure capable of additionally accounting for commonalities between arguments. Applying more principled feature selection approaches to vector-space lexical semantics may yield more significant performance gains. Towards this end we are currently evaluating two classes of approaches for setting pruning parameters per-word instead of globally: (1) subspace clustering, i.e. unsupervised feature selection (e.g., Parsons et al., 2004) and (2) multiple clustering, i.e. finding feature partitions that lead to disparate clusterings (e.g., Shafto et al., 2006). 9 Conclusions This paper introduced a simple probabilistic model of tiered clustering inspired by feature selective clustering that leverages feature exchangeability to allocate data features between a clustering model and shared component. The ability to model background variation, or shared structure, is shown to be beneficial for modeling words with high polysemy, yielding increased correlation with human similarity judgements modeling word relatedness and selectional preference. Furthermore, the tiered clustering model is shown to significantly outperform related models, yielding qualita</context>
</contexts>
<marker>Shafto, Kemp, Mansinghka, Gordon, Tenenbaum, 2006</marker>
<rawString>Patrick Shafto, Charles Kemp, Vikash Mansinghka, Matthew Gordon, and Joshua B. Tenenbaum. 2006. Learning cross-cutting systems of categories. In Proc. CogSci 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5623" citStr="Snow et al., 2006" startWordPosition="816" endWordPosition="819">sis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsupervised word sense discovery (Sch¨utze, 1998), which clusters the contexts in which a word appears. Average prototype vectors LIF</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="4859" citStr="Turney, 2006" startWordPosition="713" endWordPosition="714">background on the methods compared, Section 3 outlines the multiprototype model based on the Dirichlet Process mixture model, Section 4 derives the tiered clustering model, Section 5 discusses similarity metrics, Section 6 details the experimental setup and includes a micro-analysis of feature selection, Section 7 presents results applying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of sca</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI</booktitle>
<marker>Van Durme, Pas¸ca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pas¸ca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proc. of AAAI 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Aligning features with sense distinction dimensions.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<contexts>
<context position="16941" citStr="Xue et al., 2006" startWordPosition="2582" endWordPosition="2585">ered around the occurrence, represented using tf-idf weighting. Feature vectors are pruned to a fixed length f, discarding all but the highest-weight features (f is selected via empirical validation, as described in the next section). Finally, semantic similarity between word pairs is computed using cosine distance (`2-normalized dot-product).2 6.4 Feature Pruning Feature pruning is one of the most significant factors in obtaining high correlation with human similarity judgements using vector-space models, and has been suggested as one way to improve sense disambiguation for polysemous verbs (Xue et al., 2006). In this section, we calibrate the single prototype and multiprototype methods on WS-353, reaching the limit of human and oracle performance and demonstrating robust performance gains even with semantically impoverished features. In particular we obtain p=0.75 correlation on WS-353 using only unigram collocations and p=0.77 using a fixed-K multiprototype representation (Figure 3; Reisinger and Mooney, 2010). This result rivals average human performance, obtaining correlation near that of the supervised oracle approach of Agirre et al. (2009). The optimal pruning cutoff depends on the feature </context>
</contexts>
<marker>Xue, Chen, Palmer, 2006</marker>
<rawString>Nianwen Xue, Jinying Chen, and Martha Palmer. 2006. Aligning features with sense distinction dimensions. In Proc. of COLING/ACL 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>