<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.4858714">
Learning the Structure of Task-driven Human-Human Dialogs
Srinivas Bangalore
AT&amp;T Labs-Research
180 Park Ave
Florham Park, NJ 07932
</title>
<email confidence="0.717377">
srini@research.att.com
</email>
<note confidence="0.7872405">
Giuseppe Di Fabbrizio
AT&amp;T Labs-Research
180 Park Ave
Florham Park, NJ 07932
</note>
<email confidence="0.78807">
pino@research.att.com
</email>
<author confidence="0.79782">
Amanda Stent
</author>
<affiliation confidence="0.8104605">
Dept of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.621646">
Stony Brook, NY
</address>
<email confidence="0.98996">
stent@cs.sunysb.edu
</email>
<sectionHeader confidence="0.99728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997665">
Data-driven techniques have been used
for many computational linguistics tasks.
Models derived from data are generally
more robust than hand-crafted systems
since they better reflect the distribution
of the phenomena being modeled. With
the availability of large corpora of spo-
ken dialog, dialog management is now
reaping the benefits of data-driven tech-
niques. In this paper, we compare two ap-
proaches to modeling subtask structure in
dialog: a chunk-based model of subdialog
sequences, and a parse-based, or hierarchi-
cal, model. We evaluate these models us-
ing customer agent dialogs from a catalog
service domain.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969727272727">
As large amounts of language data have become
available, approaches to sentence-level process-
ing tasks such as parsing, language modeling,
named-entity detection and machine translation
have become increasingly data-driven and empiri-
cal. Models for these tasks can be trained to cap-
ture the distributions of phenomena in the data
resulting in improved robustness and adaptabil-
ity. However, this trend has yet to significantly
impact approaches to dialog management in dia-
log systems. Dialog managers (both plan-based
and call-flow based, for example (Di Fabbrizio and
Lewis, 2004; Larsson et al., 1999)) have tradition-
ally been hand-crafted and consequently some-
what brittle and rigid. With the ability to record,
store and process large numbers of human-human
dialogs (e.g. from call centers), we anticipate
that data-driven methods will increasingly influ-
ence approaches to dialog management.
A successful dialog system relies on the syn-
ergistic working of several components: speech
recognition (ASR), spoken language understand-
ing (SLU), dialog management (DM), language
generation (LG) and text-to-speech synthesis
(TTS). While data-driven approaches to ASR and
SLU are prevalent, such approaches to DM, LG
and TTS are much less well-developed. In on-
going work, we are investigating data-driven ap-
proaches for building all components of spoken
dialog systems.
In this paper, we address one aspect of this prob-
lem – inferring predictive models to structure task-
oriented dialogs. We view this problem as a first
step in predicting the system state of a dialog man-
ager and in predicting the system utterance during
an incremental execution of a dialog. In particular,
we learn models for predicting dialog acts of ut-
terances, and models for predicting subtask struc-
tures of dialogs. We use three different dialog act
tag sets for three different human-human dialog
corpora. We compare a flat chunk-based model
to a hierarchical parse-based model as models for
predicting the task structure of dialogs.
The outline of this paper is as follows: In Sec-
tion 2, we review current approaches to building
dialog systems. In Section 3, we review related
work in data-driven dialog modeling. In Section 4,
we present our view of analyzing the structure of
task-oriented human-human dialogs. In Section 5,
we discuss the problem of segmenting and label-
ing dialog structure and building models for pre-
dicting these labels. In Section 6, we report ex-
perimental results on Maptask, Switchboard and a
dialog data collection from a catalog ordering ser-
vice domain.
</bodyText>
<sectionHeader confidence="0.964976" genericHeader="introduction">
2 Current Methodology for Building
Dialog systems
</sectionHeader>
<bodyText confidence="0.999925416666667">
Current approaches to building dialog systems
involve several manual steps and careful craft-
ing of different modules for a particular domain
or application. The process starts with a small
scale “Wizard-of-Oz” data collection where sub-
jects talk to a machine driven by a human ‘behind
the curtains’. A user experience (UE) engineer an-
alyzes the collected dialogs, subject matter expert
interviews, user testimonials and other evidences
(e.g. customer care history records). This hetero-
geneous set of information helps the UE engineer
to design some system functionalities, mainly: the
</bodyText>
<page confidence="0.983532">
201
</page>
<bodyText confidence="0.998202730769231">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 201–208,
Sydney, July 2006. c�2006 Association for Computational Linguistics
semantic scope (e.g. call-types in the case of call
routing systems), the LG model, and the DM strat-
egy. A larger automated data collection follows,
and the collected data is transcribed and labeled by
expert labelers following the UE engineer recom-
mendations. Finally, the transcribed and labeled
data is used to train both the ASR and the SLU.
This approach has proven itself in many com-
mercial dialog systems. However, the initial UE
requirements phase is an expensive and error-
prone process because it involves non-trivial de-
sign decisions that can only be evaluated after sys-
tem deployment. Moreover, scalability is compro-
mised by the time, cost and high level of UE know-
how needed to reach a consistent design.
The process of building speech-enabled auto-
mated contact center services has been formalized
and cast into a scalable commercial environment
in which dialog components developed for differ-
ent applications are reused and adapted (Gilbert
et al., 2005). However, we still believe that ex-
ploiting dialog data to train/adapt or complement
hand-crafted components will be vital for robust
and adaptable spoken dialog systems.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999840629032258">
In this paper, we discuss methods for automati-
cally creating models of dialog structure using di-
alog act and task/subtask information. Relevant
related work includes research on automatic dia-
log act tagging and stochastic dialog management,
and on building hierarchical models of plans using
task/subtask information.
There has been considerable research on statis-
tical dialog act tagging (Core, 1998; Jurafsky et
al., 1998; Poesio and Mikheev, 1998; Samuel et
al., 1998; Stolcke et al., 2000; Hastie et al., 2002).
Several disambiguation methods (n-gram models,
hidden Markov models, maximum entropy mod-
els) that include a variety of features (cue phrases,
speaker ID, word n-grams, prosodic features, syn-
tactic features, dialog history) have been used. In
this paper, we show that use of extended context
gives improved results for this task.
Approaches to dialog management include
AI-style plan recognition-based approaches (e.g.
(Sidner, 1985; Litman and Allen, 1987; Rich
and Sidner, 1997; Carberry, 2001; Bohus and
Rudnicky, 2003)) and information state-based ap-
proaches (e.g. (Larsson et al., 1999; Bos et al.,
2003; Lemon and Gruenstein, 2004)). In recent
years, there has been considerable research on
how to automatically learn models of both types
from data. Researchers who treat dialog as a se-
quence of information states have used reinforce-
ment learning and/or Markov decision processes
to build stochastic models for dialog management
that are evaluated by means of dialog simulations
(Levin and Pieraccini, 1997; Scheffler and Young,
2002; Singh et al., 2002; Williams et al., 2005;
Henderson et al., 2005; Frampton and Lemon,
2005). Most recently, Henderson et al. showed
that it is possible to automatically learn good dia-
log management strategies from automatically la-
beled data over a large potential space of dialog
states (Henderson et al., 2005); and Frampton and
Lemon showed that the use of context informa-
tion (the user’s last dialog act) can improve the
performance of learned strategies (Frampton and
Lemon, 2005). In this paper, we combine the use
of automatically labeled data and extended context
for automatic dialog modeling.
Other researchers have looked at probabilistic
models for plan recognition such as extensions of
Hidden Markov Models (Bui, 2003) and proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this paper, we compare hierarchical grammar-
style and flat chunking-style models of dialog.
In recent research, Hardy (2004) used a large
corpus of transcribed and annotated telephone
conversations to develop the Amities dialog sys-
tem. For their dialog manager, they trained sepa-
rate task and dialog act classifiers on this corpus.
For task identification they report an accuracy of
85% (true task is one of the top 2 results returned
by the classifier); for dialog act tagging they report
86% accuracy.
</bodyText>
<sectionHeader confidence="0.972196" genericHeader="method">
4 Structural Analysis of a Dialog
</sectionHeader>
<bodyText confidence="0.999965833333333">
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree that encap-
sulates the task structure (dominance and prece-
dence relations among tasks), dialog act structure
(sequences of dialog acts), and linguistic structure
of utterances (inter-clausal relations and predicate-
argument relations within a clause), as illustrated
in Figure 1. As the dialog proceeds, an utterance
from a participant is accommodated into the tree in
an incremental manner, much like an incremental
syntactic parser accommodates the next word into
a partial parse tree (Alexandersson and Reithinger,
1997). With this model, we can tightly couple
language understanding and dialog management
using a shared representation, which leads to im-
proved accuracy (Taylor et al., 1998).
In order to infer models for predicting the struc-
ture of task-oriented dialogs, we label human-
human dialogs with the hierarchical information
shown in Figure 1 in several stages: utterance
segmentation (Section 4.1), syntactic annotation
(Section 4.2), dialog act tagging (Section 4.3) and
</bodyText>
<page confidence="0.98719">
202
</page>
<figure confidence="0.790919333333333">
subtask labeling (Section 5).
Dialog
Clause
</figure>
<figureCaption confidence="0.998104">
Figure 1: Structural analysis of a dialog
</figureCaption>
<subsectionHeader confidence="0.808469">
4.1 Utterance Segmentation
</subsectionHeader>
<bodyText confidence="0.999928714285714">
The task of ”cleaning up” spoken language utter-
ances by detecting and removing speech repairs
and dysfluencies and identifying sentence bound-
aries has been a focus of spoken language parsing
research for several years (e.g. (Bear et al., 1992;
Seneff, 1992; Shriberg et al., 2000; Charniak and
Johnson, 2001)). We use a system that segments
the ASR output of a user’s utterance into clauses.
The system annotates an utterance for sentence
boundaries, restarts and repairs, and identifies
coordinating conjunctions, filled pauses and dis-
course markers. These annotations are done using
a cascade of classifiers, details of which are de-
scribed in (Bangalore and Gupta, 2004).
</bodyText>
<subsectionHeader confidence="0.991228">
4.2 Syntactic Annotation
</subsectionHeader>
<bodyText confidence="0.999947125">
We automatically annotate a user’s utterance with
supertags (Bangalore and Joshi, 1999). Supertags
encapsulate predicate-argument information in a
local structure. They are composed with each
other using the substitution and adjunction oper-
ations of Tree-Adjoining Grammars (Joshi, 1987)
to derive a dependency analysis of an utterance
and its predicate-argument structure.
</bodyText>
<subsectionHeader confidence="0.998827">
4.3 Dialog Act Tagging
</subsectionHeader>
<bodyText confidence="0.999975444444445">
We use a domain-specific dialog act tag-
ging scheme based on an adapted version of
DAMSL (Core, 1998). The DAMSL scheme is
quite comprehensive, but as others have also found
(Jurafsky et al., 1998), the multi-dimensionality
of the scheme makes the building of models from
DAMSL-tagged data complex. Furthermore, the
generality of the DAMSL tags reduces their util-
ity for natural language generation. Other tagging
schemes, such as the Maptask scheme (Carletta et
al., 1997), are also too general for our purposes.
We were particularly concerned with obtaining
sufficient discriminatory power between different
types of statement (for generation), and to include
an out-of-domain tag (for interpretation). We pro-
vide a sample list of our dialog act tags in Table 2.
Our experiments in automatic dialog act tagging
are described in Section 6.3.
</bodyText>
<sectionHeader confidence="0.960115" genericHeader="method">
5 Modeling Subtask Structure
</sectionHeader>
<bodyText confidence="0.999555875">
Figure 2 shows the task structure for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting structure can be as deep as
five levels. Most often the nesting is at the left or
right frontier of the subtask tree.
</bodyText>
<figure confidence="0.58472">
Order Placement
</figure>
<figureCaption confidence="0.9822255">
Figure 2: A sample task structure in our applica-
tion domain.
</figureCaption>
<figure confidence="0.944868">
Opening Contact Info Order Item Payment Info Summary Closing
Shipping Info Delivery Info
</figure>
<figureCaption confidence="0.971974">
Figure 3: An example output of the chunk model’s
task structure
</figureCaption>
<bodyText confidence="0.999908214285714">
The goal of subtask segmentation is to predict if
the current utterance in the dialog is part of the cur-
rent subtask or starts a new subtask. We compare
two models for recovering the subtask structure
– a chunk-based model and a parse-based model.
In the chunk-based model, we recover the prece-
dence relations (sequence) of the subtasks but not
dominance relations (subtask structure) among the
subtasks. Figure 3 shows a sample output from the
chunk model. In the parse model, we recover the
complete task structure from the sequence of ut-
terances as shown in Figure 2. Here, we describe
our two models. We present our experiments on
subtask segmentation and labeling in Section 6.4.
</bodyText>
<subsectionHeader confidence="0.861605">
5.1 Chunk-based model
</subsectionHeader>
<bodyText confidence="0.992070166666667">
This model is similar to the second one described
in (Poesio and Mikheev, 1998), except that we
use tasks and subtasks rather than dialog games.
We model the prediction problem as a classifica-
tion task as follows: given a sequence of utter-
ances in a dialog and a
</bodyText>
<figure confidence="0.998619066666667">
Task
Task Task
Utterance
Utterance
Utterance
Topic/Subtask
Topic/Subtask
Topic/Subtask
DialogAct,Pred−Args DialogAct,Pred−Args DialogAct,Pred−Args
Shipping Info
Delivery Info
Opening
Contact Info
Order Item Payment Info Summary
Closing
</figure>
<page confidence="0.995204">
203
</page>
<bodyText confidence="0.982115333333333">
subtask label vocabulary , we need
to predict the best subtask label sequence
as shown in equation 1.
</bodyText>
<equation confidence="0.855499">
(1)
</equation>
<bodyText confidence="0.996380225">
Each subtask has begin, middle (possibly ab-
sent) and end utterances. If we incorporate this
information, the refined vocabulary of subtask la-
bels is . In
our experiments, we use a classifier to assign to
each utterance a refined subtask label conditioned
on a vector of local contextual features ( ). In
the interest of using an incremental left-to-right
decoder, we restrict the contextual features to be
from the preceding context only. Furthermore, the
search is limited to the label sequences that re-
spect precedence among the refined labels (begin
middle end). This constraint is expressed
in a grammar G encoded as a regular expression
( ). However, in order
to cope with the prediction errors of the classifier,
we approximate with an -gram language
model on sequences of the refined tag labels:
In order to estimate the conditional distribution
we use the general technique of choos-
ing the maximum entropy (maxent) distribution
that properly estimates the average of each feature
over the training data (Berger et al., 1996). This
can be written as a Gibbs distribution parameter-
ized with weights, where is the size of the
label set. Thus,
We use the machine learning toolkit
LLAMA (Haffner, 2006) to estimate the con-
ditional distribution using maxent. LLAMA
encodes multiclass maxent as binary maxent, in
order to increase the speed of training and to scale
this method to large data sets. Each of the
classes in the set is encoded as a bit vector
such that, in the vector for class, the bit is one
and all other bits are zero. Then, one-vs-other
binary classifiers are used as follows.
where is the parameter vector for the anti-
label and . In order to compute
, we use class independence assumption
and require that and for all
</bodyText>
<subsectionHeader confidence="0.84052">
5.2 Parse-based Model
</subsectionHeader>
<bodyText confidence="0.99984865">
As seen in Figure 3, the chunk model does
not capture dominance relations among subtasks,
which are important for resolving anaphoric refer-
ences (Grosz and Sidner, 1986). Also, the chunk
model is representationally inadequate for center-
embedded nestings of subtasks, which do occur
in our domain, although less frequently than the
more prevalent “tail-recursive” structures.
In this model, we are interested in finding the
most likely plan tree ( ) given the sequence of
utterances:
For real-time dialog management we use a top-
down incremental parser that incorporates bottom-
up information (Roark, 2001).
We rewrite equation (6) to exploit the subtask
sequence provided by the chunk model as shown
in Equation 7. For the purpose of this paper, we
approximate Equation 7 using one-best (or k-best)
chunk output.1
where
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998244">
In this section, we present the results of our exper-
iments for modeling subtask structure.
</bodyText>
<subsectionHeader confidence="0.97372">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.9461612">
As our primary data set, we used 915 telephone-
based customer-agent dialogs related to the task
of ordering products from a catalog. Each dia-
log was transcribed by hand; all numbers (tele-
phone, credit card, etc.) were removed for pri-
vacy reasons. The average dialog lasted for 3.71
&apos;However, it is conceivable to parse the multiple hypothe-
ses of chunks (encoded as a weighted lattice) produced by the
chunk model.
.
</bodyText>
<page confidence="0.992279">
204
</page>
<bodyText confidence="0.999829785714286">
minutes and included 61.45 changes of speaker. A
single customer-service representative might par-
ticipate in several dialogs, but customers are rep-
resented by only one dialog each. Although the
majority of the dialogs were on-topic, some were
idiosyncratic, including: requests for order cor-
rections, transfers to customer service, incorrectly
dialed numbers, and long friendly out-of-domain
asides. Annotations applied to these dialogs in-
clude: utterance segmentation (Section 4.1), syn-
tactic annotation (Section 4.2), dialog act tag-
ging (Section 4.3) and subtask segmentation (Sec-
tion 5). The former two annotations are domain-
independent while the latter are domain-specific.
</bodyText>
<subsectionHeader confidence="0.953915">
6.2 Features
</subsectionHeader>
<bodyText confidence="0.9998725">
Offline natural language processing systems, such
as part-of-speech taggers and chunkers, rely on
both static and dynamic features. Static features
are derived from the local context of the text be-
ing tagged. Dynamic features are computed based
on previous predictions. The use of dynamic fea-
tures usually requires a search for the globally op-
timal sequence, which is not possible when doing
incremental processing. For dialog act tagging and
subtask segmentation during dialog management,
we need to predict incrementally since it would
be unrealistic to wait for the entire dialog before
decoding. Thus, in order to train the dialog act
(DA) and subtask segmentation classifiers, we use
only static features from the current and left con-
text as shown in Table 1.2 This obviates the need
for constructing a search network and performing
a dynamic programming search during decoding.
In lieu of the dynamic context, we use larger static
context to compute features – word trigrams and
trigrams of words annotated with supertags com-
puted from up to three previous utterances.
</bodyText>
<table confidence="0.999272833333333">
Label Type Features
Dialog Speaker, word trigrams from
Acts current/previous utterance(s)
supertagged utterance
Subtask Speaker, word trigrams from current
utterance, previous utterance(s)/turn
</table>
<tableCaption confidence="0.999195">
Table 1: Features used for the classifiers.
</tableCaption>
<subsectionHeader confidence="0.980068">
6.3 Dialog Act Labeling
</subsectionHeader>
<bodyText confidence="0.997330564102564">
For dialog act labeling, we built models from
our corpus and from the Maptask (Carletta et al.,
1997) and Switchboard-DAMSL (Jurafsky et al.,
1998) corpora. From the files for the Maptask cor-
pus, we extracted the moves, words and speaker
information (follower/giver). Instead of using the
2We could use dynamic contexts as well and adopt a
greedy decoding algorithm instead of a viterbi search. We
have not explored this approach in this paper.
raw move information, we augmented each move
with speaker information, so that for example,
the instruct move was split into instruct-giver and
instruct-follower. For the Switchboard corpus, we
clustered the original labels, removing most of
the multidimensional tags and combining together
tags with minimum training data as described in
(Jurafsky et al., 1998). For all three corpora, non-
sentence elements (e.g., dysfluencies, discourse
markers, etc.) and restarts (with and without re-
pairs) were kept; non-verbal content (e.g., laughs,
background noise, etc.) was removed.
As mentioned in Section 4, we use a domain-
specific tag set containing 67 dialog act tags for
the catalog corpus. In Table 2, we give examples
of our tags. We manually annotated 1864 clauses
from 20 dialogs selected at random from our cor-
pus and used a ten-fold cross-validation scheme
for testing. In our annotation, a single utterance
may have multiple dialog act labels. For our ex-
periments with the Switchboard-DAMSL corpus,
we used 42 dialog act tags obtained by clustering
over the 375 unique tags in the data. This cor-
pus has 1155 dialogs and 218,898 utterances; 173
dialogs, selected at random, were used for testing.
The Maptask tagging scheme has 12 unique dialog
act tags; augmented with speaker information, we
get 24 tags. This corpus has 128 dialogs and 26181
utterances; ten-fold cross validation was used for
testing.
</bodyText>
<table confidence="0.9997249375">
Type Subtype
Ask Info
Explain Catalog, CC Related, Discount, Order Info
Order Problem, Payment Rel, Product Info
Promotions, Related Offer, Shipping
Convers- Ack, Goodbye, Hello, Help, Hold,
-ational YoureWelcome, Thanks, Yes, No, Ack,
Repeat, Not(Information)
Request Code, Order Problem, Address, Catalog,
CC Related, Change Order, Conf, Credit,
Customer Info, Info, Make Order, Name,
Order Info, Order Status, Payment Rel,
Phone Number, Product Info, Promotions,
Shipping, Store Info
YNQ Address, Email, Info, Order Info,
Order Status,Promotions, Related Offer
</table>
<tableCaption confidence="0.999661">
Table 2: Sample set of dialog act labels
</tableCaption>
<bodyText confidence="0.9981055">
Table 3 shows the error rates for automatic dia-
log act labeling using word trigram features from
the current and previous utterance. We compare
error rates for our tag set to those of Switchboard-
DAMSL and Maptask using the same features and
the same classifier learner. The error rates for the
catalog and the Maptask corpus are an average
of ten-fold cross-validation. We suspect that the
larger error rate for our domain compared to Map-
task and Switchboard might be due to the small
size of our annotated corpus (about 2K utterances
for our domain as against about 20K utterances for
</bodyText>
<page confidence="0.997182">
205
</page>
<bodyText confidence="0.997008555555556">
Maptask and 200K utterances for DAMSL).
The error rates for the Switchboard-DAMSL
data are significantly better than previously pub-
lished results (28% error rate) (Jurafsky et al.,
1998) with the same tag set. This improvement
is attributable to the richer feature set we use and a
discriminative modeling framework that supports
a large number of features, in contrast to the gener-
ative model used in (Jurafsky et al., 1998). A sim-
ilar obeservation applies to the results on Maptask
dialog act tagging. Our model outperforms previ-
ously published results (42.8% error rate) (Poesio
and Mikheev, 1998).
In labeling the Switchboard data, long utter-
ances were split into slash units (Meteer et.al.,
1995). A speaker’s turn can be divided in one or
more slash units and a slash unit can extend over
multiple turns, for example:
</bodyText>
<construct confidence="0.8249214">
sv B.64 utt3: C but, F uh –
b A.65 utt1: Uh-huh. /
+ B.66 utt1: – people want all of that/
sv B.66 utt2: C and not all of those are necessities. /
b A.67 utt1: Right. /
</construct>
<bodyText confidence="0.988956428571428">
The labelers were instructed to label on the ba-
sis of the whole slash unit. This makes, for ex-
ample, the dysfluency turn B.64 a Statement opin-
ion (sv) rather than a non-verbal. For the pur-
pose of discriminative learning, this could intro-
duce noisy data since the context associated to the
labeling decision shows later in the dialog. To ad-
dress this issue, we compare 2 classifiers: the first
(non-merged), simply propagates the same label
to each continuation, cross turn slash unit; the sec-
ond (merged) combines the units in one single ut-
terance. Although the merged classifier breaks the
regular structure of the dialog, the results in Table
3 show better overall performance.
</bodyText>
<table confidence="0.9996005">
Tagset current + stagged + 3 previous
utterance utterance (stagged)
utterance
Catalog 46.3 46.1 42.2
Domain
DAMSL 24.7 23.8 19.1
(non-merged)
DAMSL 22.0 20.6 16.5
(merged)
Maptask 34.3 33.9 30.3
</table>
<tableCaption confidence="0.999192">
Table 3: Error rates in dialog act tagging
</tableCaption>
<subsectionHeader confidence="0.995877">
6.4 Subtask Segmentation and Labeling
</subsectionHeader>
<bodyText confidence="0.999381833333333">
For subtask labeling, we used a random partition
of 864 dialogs from our catalog domain as the
training set and 51 dialogs as the test set. All
the dialogs were annotated with subtask labels by
hand. We used a set of 18 labels grouped as shown
in Figure 4.
</bodyText>
<table confidence="0.998466">
Type Subtask Labels
1 opening, closing
2 contact-information, delivery-information,
payment-information, shipping-address,summary
3 order-item, related-offer, order-problem
discount, order-change, check-availability
4 call-forward, out-of-domain, misc-other, sub-call
</table>
<tableCaption confidence="0.8248205">
Table 4: Subtask label set
6.4.1 Chunk-based Model
</tableCaption>
<bodyText confidence="0.975203823529412">
Table 5 shows error rates on the test set when
predicting refined subtask labels using word -
gram features computed on different dialog con-
texts. The well-formedness constraint on the re-
fined subtask labels significantly improves predic-
tion accuracy. Utterance context is also very help-
ful; just one utterance of left-hand context leads to
a 10% absolute reduction in error rate, with fur-
ther reductions for additional context. While the
use of trigram features helps, it is not as helpful as
other contextual information. We used the dialog
act tagger trained from Switchboard-DAMSL cor-
pus to automatically annotate the catalog domain
utterances. We included these tags as features for
the classifier, however, we did not see an improve-
ment in the error rates, probably due to the high
error rate of the dialog act tagger.
</bodyText>
<table confidence="0.9995715">
Feature Utterance Context
Context
Current +prev +three prev
utt/with DA utt/with DA utt/with DA
Unigram 42.9/42.4 33.6/34.1 30.0/30.3
(53.4/52.8) (43.0/43.0) (37.6/37.6)
Trigram 41.7/41.7 31.6/31.4 30.0/29.1
(52.5/52.0) (42.9/42.7) (37.6/37.4)
</table>
<tableCaption confidence="0.974924">
Table 5: Error rate for predicting the refined sub-
</tableCaption>
<bodyText confidence="0.7969055">
task labels. The error rates without the well-
formedness constraint is shown in parenthesis.
The error rates with dialog acts as features are sep-
arated by a slash.
</bodyText>
<sectionHeader confidence="0.511635" genericHeader="method">
6.4.2 Parsing-based Model
</sectionHeader>
<bodyText confidence="0.989650533333334">
We retrained a top-down incremental
parser (Roark, 2001) on the plan trees in the
training dialogs. For the test dialogs, we used
the-best (k=50) refined subtask labels for each
utterance as predicted by the chunk-based classi-
fier to create a lattice of subtask label sequences.
For each dialog we then created -best sequences
(100-best for these experiments) of subtask labels;
these were parsed and (re-)ranked by the parser.3
We combine the weights of the subtask label
sequences assigned by the classifier with the parse
score assigned by the parser and select the top
3Ideally, we would have parsed the subtask label lattice
directly, however, the parser has to be reimplemented to parse
such lattice inputs.
</bodyText>
<page confidence="0.995639">
206
</page>
<table confidence="0.999211625">
Features Constraints
No Constraint Sequence Constraint Parser Constraint
Current Utt 54.4 42.0 41.5
+ DA 53.8 40.5 40.2
Current+Prev Utt 41.6 27.7 27.7
+DA 40.0 28.8 28.1
Current+3 Prev Utt 37.5 24.7 24.7
+DA 39.7 29.6 28.9
</table>
<tableCaption confidence="0.923551">
Table 6: Error rates for task structure prediction, with no constraints, sequence constraints and parser
constraints
</tableCaption>
<bodyText confidence="0.999798666666667">
scoring sequence from the list for each dialog.
The results are shown in Table 6. It can be seen
that using the parsing constraint does not help the
subtask label sequence prediction significantly.
The chunk-based model gives almost the same
accuracy, and is incremental and more efficient.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9998754">
The experiments reported in this section have been
performed on transcribed speech. The audio for
these dialogs, collected at a call center, were stored
in a compressed format, so the speech recognition
error rate is high. In future work, we will assess
the performance of dialog structure prediction on
recognized speech.
The research presented in this paper is but one
step, albeit a crucial one, towards achieving the
goal of inducing human-machine dialog systems
using human-human dialogs. Dialog structure in-
formation is necessary for language generation
(predicting the agents’ response) and dialog state
specific text-to-speech synthesis. However, there
are several challenging problems that remain to be
addressed.
The structuring of dialogs has another applica-
tion in call center analytics. It is routine practice to
monitor, analyze and mine call center data based
on indicators such as the average length of dialogs,
the task completion rate in order to estimate the ef-
ficiency of a call center. By incorporating structure
to the dialogs, as presented in this paper, the anal-
ysis of dialogs can be performed at a more fine-
grained (task and subtask) level.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999965533333333">
In order to build a dialog manager using a data-
driven approach, the following are necessary: a
model for labeling/interpreting the user’s current
action; a model for identifying the current sub-
task/topic; and a model for predicting what the
system’s next action should be. Prior research in
plan identification and in dialog act labeling has
identified possible features for use in such models,
but has not looked at the performance of different
feature sets (reflecting different amounts of con-
text and different views of dialog) across different
domains (label sets). In this paper, we compared
the performance of a dialog act labeler/predictor
across three different tag sets: one using very de-
tailed, domain-specific dialog acts usable for inter-
pretation and generation; and two using general-
purpose dialog acts and corpora available to the
larger research community. We then compared
two models for subtask labeling: a flat, chunk-
based model and a hierarchical, parsing-based
model. Findings include that simpler chunk-based
models perform as well as hierarchical models for
subtask labeling and that a dialog act feature is not
helpful for subtask labeling.
In on-going work, we are using our best per-
forming models for both DM and LG components
(to predict the next dialog move(s), and to select
the next system utterance). In future work, we will
address the use of data-driven dialog management
to improve SLU.
</bodyText>
<sectionHeader confidence="0.996597" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972666666667">
We thank Barbara Hollister and her team for their
effort in annotating the dialogs for dialog acts and
subtask structure. We thank Patrick Haffner for
providing us with the LLAMA machine learning
toolkit and Brian Roark for providing us with his
top-down parser used in our experiments. We also
thank Alistair Conkie, Mazin Gilbert, Narendra
Gupta, and Benjamin Stern for discussions during
the course of this work.
</bodyText>
<sectionHeader confidence="0.998441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986913307692308">
J. Alexandersson and N. Reithinger. 1997. Learning dia-
logue structures from a corpus. In Proceedings of Eu-
rospeech’97.
S. Bangalore and N. Gupta. 2004. Extracting clauses in di-
alogue corpora : Application to spoken language under-
standing. Journal Traitement Automatique des Langues
(TAL), 45(2).
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
approach to almost parsing. Computational Linguistics,
25(2).
J. Bear et al. 1992. Integrating multiple knowledge sources
for detection and correction of repairs in human-computer
dialog. In Proceedings of ACL’92.
</reference>
<page confidence="0.99777">
207
</page>
<note confidence="0.6885524">
S. Larsson et al. 1999. TrindiKit manual. Technical report,
TRINDI Deliverable D2.2.
A. Berger, S.D. Pietra, and V.D. Pietra. 1996. A Maximum
Entropy Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39–71.
</note>
<reference confidence="0.999874855769231">
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dialog man-
agement using hierarchical task decomposition and an ex-
pectation agenda. In Proceedings of Eurospeech’03.
J. Bos et al. 2003. DIPPER: Description and formalisation of
an information-state update dialogue system architecture.
In Proceedings of SIGdial.
H.H. Bui. 2003. A general model for online probabalistic
plan recognition. In Proceedings of IJCAI’03.
S. Carberry. 2001. Techniques for plan recognition. User
Modeling and User-Adapted Interaction, 11(1–2).
J. Carletta et al. 1997. The reliability of a dialog structure
coding scheme. Computational Linguistics, 23(1).
E. Charniak and M. Johnson. 2001. Edit detection and pars-
ing for transcribed speech. In Proceedings of NAACL’01.
M. Core. 1998. Analyzing and predicting patterns of
DAMSL utterance tags. In Proceedings of the AAAI
spring symposium on Applying machine learning to dis-
course processing.
M. Meteer et.al. 1995. Dysfluency annotation stylebook for
the switchboard corpus. Distributed by LDC.
G. Di Fabbrizio and C. Lewis. 2004. Florence: a dialogue
manager framework for spoken dialogue systems. In IC-
SLP 2004, 8th International Conference on Spoken Lan-
guage Processing, Jeju, Jeju Island, Korea, October 4-8.
M. Frampton and O. Lemon. 2005. Reinforcement learning
of dialogue strategies using the user’s last dialogue act. In
Proceedings of the 4th IJCAI workshop on knowledge and
reasoning in practical dialogue systems.
M. Gilbert et al. 2005. Intelligent virtual agents for con-
tact center automation. IEEE Signal Processing Maga-
zine, 22(5), September.
B.J. Grosz and C.L. Sidner. 1986. Attention, intentions and
the structure of discoursep. Computational Linguistics,
12(3).
P. Haffner. 2006. Scaling large margin classifiers for spoken
language understanding. Speech Communication, 48(4).
H. Hardy et al. 2004. Data-driven strategies for an automated
dialogue system. In Proceedings of ACL’04.
H. Wright Hastie et al. 2002. Automatically predicting dia-
logue structure using prosodic features. Speech Commu-
nication, 36(1–2).
J. Henderson et al. 2005. Hybrid reinforcement/supervised
learning for dialogue policies from COMMUNICATOR
data. In Proceedings of the 4th IJCAI workshop on knowl-
edge and reasoning in practical dialogue systems.
A. K. Joshi. 1987. An introduction to tree adjoining gram-
mars. In A. Manaster-Ramer, editor, Mathematics of Lan-
guage. John Benjamins, Amsterdam.
D. Jurafsky et al. 1998. Switchboard discourse language
modeling project report. Technical Report Research Note
30, Center for Speech and Language Processing, Johns
Hopkins University, Baltimore, MD.
O. Lemon and A. Gruenstein. 2004. Multithreaded con-
text for robust conversational interfaces: Context-sensitive
speech recognition and interpretation of corrective frag-
ments. ACM Transactions on Computer-Human Interac-
tion, 11(3).
E. Levin and R. Pieraccini. 1997. A stochastic model of
computer-human interaction for learning dialogue strate-
gies. In Proceedings of Eurospeech’97.
D. Litman and J. Allen. 1987. A plan recognition model for
subdialogs in conversations. Cognitive Science, 11(2).
K. Lochbaum. 1998. A collaborative planning model of in-
tentional structure. Computational Linguistics, 24(4).
M. Poesio and A. Mikheev. 1998. The predictive power of
game structure in dialogue act recognition: experimental
results using maximum entropy estimation. In Proceed-
ings of ICSLP’98.
D.V. Pynadath and M.P. Wellman. 2000. Probabilistic state-
dependent grammars for plan recognition. In In Proceed-
ings of the 16th Conference on Uncertainty in Artificial
Intelligence (UAI-2000).
C. Rich and C.L. Sidner. 1997. COLLAGEN: When agents
collaborate with people. In Proceedings of the First Inter-
national Conference on Autonomous Agents (Agents’97).
B. Roark. 2001. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2).
K. Samuel et al. 1998. Computing dialogue acts from fea-
tures with transformation-based learning. In Proceedings
of the AAAI spring symposium on Applying machine learn-
ing to discourse processing.
K. Scheffler and S. Young. 2002. Automatic learning of di-
alogue strategy using dialogue simulation and reinforce-
ment learning. In Proceedings of HLT’02.
S. Seneff. 1992. A relaxation method for understanding
spontaneous speech utterances. In Proceedings of the
Speech and Natural Language Workshop, San Mateo, CA.
E. Shriberg et al. 2000. Prosody-based automatic segmenta-
tion of speech into sentences and topics. Speech Commu-
nication, 32, September.
C.L. Sidner. 1985. Plan parsing for intended response recog-
nition in discourse. Computational Intelligence, 1(1).
S. Singh et al. 2002. Optimizing dialogue management with
reinforcement learning: Experiments with the NJFun sys-
tem. Journal of Artificial Intelligence Research, 16.
A. Stolcke et al. 2000. Dialogue act modeling for automatic
tagging and recognition of conversational speech. Com-
putational Linguistics, 26(3).
P. Taylor et al. 1998. Intonation and dialogue context as
constraints for speech recognition. Language and Speech,
41(3).
J. Williams et al. 2005. Partially observable Markov deci-
sion processes with continuous observations for dialogue
management. In Proceedings of SIGdial.
</reference>
<page confidence="0.997784">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726240">
<title confidence="0.999971">Learning the Structure of Task-driven Human-Human Dialogs</title>
<author confidence="0.978715">Srinivas Bangalore</author>
<affiliation confidence="0.999758">AT&amp;T Labs-Research</affiliation>
<address confidence="0.9977995">180 Park Ave Florham Park, NJ 07932</address>
<email confidence="0.999855">srini@research.att.com</email>
<author confidence="0.999547">Giuseppe Di_Fabbrizio</author>
<affiliation confidence="0.99992">AT&amp;T Labs-Research</affiliation>
<address confidence="0.997814">180 Park Ave Florham Park, NJ 07932</address>
<email confidence="0.999414">pino@research.att.com</email>
<author confidence="0.99973">Amanda Stent</author>
<affiliation confidence="0.955826">Dept of Computer Science Stony Brook University</affiliation>
<author confidence="0.849606">Stony Brook</author>
<author confidence="0.849606">NY</author>
<email confidence="0.999517">stent@cs.sunysb.edu</email>
<abstract confidence="0.999071470588235">Data-driven techniques have been used for many computational linguistics tasks. Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled. With the availability of large corpora of spoken dialog, dialog management is now reaping the benefits of data-driven techniques. In this paper, we compare two approaches to modeling subtask structure in dialog: a chunk-based model of subdialog sequences, and a parse-based, or hierarchical, model. We evaluate these models using customer agent dialogs from a catalog service domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Alexandersson</author>
<author>N Reithinger</author>
</authors>
<title>Learning dialogue structures from a corpus.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech’97.</booktitle>
<contexts>
<context position="7897" citStr="Alexandersson and Reithinger, 1997" startWordPosition="1203" endWordPosition="1206">n good dialog management strategies from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog modeling. Other researchers have looked at probabilistic models for plan recognition such as extensions of Hidden Markov Models (Bui, 2003) and probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). In this paper, we compare hierarchical grammarstyle and flat chunking-style models of dialog. In recent research, Hardy (2004) used a large corpus of transcribed and annotated telephone conversations to develop the Amities dialog system. For their dialog manager, they trained separate task and dialog act classifiers on this corpus. For task identification they report an accuracy of 85% (true task is one of the top 2 results returned by the classifier); for dialog act tagging they report 86% accuracy. 4 Structural Analysis of a Dialog We consider a task-oriented d</context>
<context position="9168" citStr="Alexandersson and Reithinger, 1997" startWordPosition="1402" endWordPosition="1405">l creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree that encapsulates the task structure (dominance and precedence relations among tasks), dialog act structure (sequences of dialog acts), and linguistic structure of utterances (inter-clausal relations and predicateargument relations within a clause), as illustrated in Figure 1. As the dialog proceeds, an utterance from a participant is accommodated into the tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Reithinger, 1997). With this model, we can tightly couple language understanding and dialog management using a shared representation, which leads to improved accuracy (Taylor et al., 1998). In order to infer models for predicting the structure of task-oriented dialogs, we label humanhuman dialogs with the hierarchical information shown in Figure 1 in several stages: utterance segmentation (Section 4.1), syntactic annotation (Section 4.2), dialog act tagging (Section 4.3) and 202 subtask labeling (Section 5). Dialog Clause Figure 1: Structural analysis of a dialog 4.1 Utterance Segmentation The task of ”cleanin</context>
</contexts>
<marker>Alexandersson, Reithinger, 1997</marker>
<rawString>J. Alexandersson and N. Reithinger. 1997. Learning dialogue structures from a corpus. In Proceedings of Eurospeech’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>N Gupta</author>
</authors>
<title>Extracting clauses in dialogue corpora : Application to spoken language understanding.</title>
<date>2004</date>
<journal>Journal Traitement Automatique des Langues (TAL),</journal>
<volume>45</volume>
<issue>2</issue>
<contexts>
<context position="10420" citStr="Bangalore and Gupta, 2004" startWordPosition="1593" endWordPosition="1596">rances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Ju</context>
</contexts>
<marker>Bangalore, Gupta, 2004</marker>
<rawString>S. Bangalore and N. Gupta. 2004. Extracting clauses in dialogue corpora : Application to spoken language understanding. Journal Traitement Automatique des Langues (TAL), 45(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="10534" citStr="Bangalore and Joshi, 1999" startWordPosition="1608" endWordPosition="1611">cus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Jurafsky et al., 1998), the multi-dimensionality of the scheme makes the building of models from DAMSL-tagged data c</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
</authors>
<title>Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog.</title>
<date>1992</date>
<booktitle>In Proceedings of ACL’92.</booktitle>
<marker>Bear, 1992</marker>
<rawString>J. Bear et al. 1992. Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. In Proceedings of ACL’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>RavenClaw: Dialog management using hierarchical task decomposition and an expectation agenda.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech’03.</booktitle>
<contexts>
<context position="6569" citStr="Bohus and Rudnicky, 2003" startWordPosition="998" endWordPosition="1001">t al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson et al., 2005; Frampton and </context>
</contexts>
<marker>Bohus, Rudnicky, 2003</marker>
<rawString>D. Bohus and A. Rudnicky. 2003. RavenClaw: Dialog management using hierarchical task decomposition and an expectation agenda. In Proceedings of Eurospeech’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>DIPPER: Description and formalisation of an information-state update dialogue system architecture.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGdial.</booktitle>
<marker>Bos, 2003</marker>
<rawString>J. Bos et al. 2003. DIPPER: Description and formalisation of an information-state update dialogue system architecture. In Proceedings of SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Bui</author>
</authors>
<title>A general model for online probabalistic plan recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI’03.</booktitle>
<contexts>
<context position="7821" citStr="Bui, 2003" startWordPosition="1196" endWordPosition="1197">al. showed that it is possible to automatically learn good dialog management strategies from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog modeling. Other researchers have looked at probabilistic models for plan recognition such as extensions of Hidden Markov Models (Bui, 2003) and probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). In this paper, we compare hierarchical grammarstyle and flat chunking-style models of dialog. In recent research, Hardy (2004) used a large corpus of transcribed and annotated telephone conversations to develop the Amities dialog system. For their dialog manager, they trained separate task and dialog act classifiers on this corpus. For task identification they report an accuracy of 85% (true task is one of the top 2 results returned by the classifier); for dialog act tagging they report 8</context>
</contexts>
<marker>Bui, 2003</marker>
<rawString>H.H. Bui. 2003. A general model for online probabalistic plan recognition. In Proceedings of IJCAI’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carberry</author>
</authors>
<title>Techniques for plan recognition. User Modeling and User-Adapted Interaction,</title>
<date>2001</date>
<pages>11--1</pages>
<contexts>
<context position="6542" citStr="Carberry, 2001" startWordPosition="996" endWordPosition="997">1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson </context>
</contexts>
<marker>Carberry, 2001</marker>
<rawString>S. Carberry. 2001. Techniques for plan recognition. User Modeling and User-Adapted Interaction, 11(1–2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>The reliability of a dialog structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>Carletta, 1997</marker>
<rawString>J. Carletta et al. 1997. The reliability of a dialog structure coding scheme. Computational Linguistics, 23(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL’01.</booktitle>
<contexts>
<context position="10056" citStr="Charniak and Johnson, 2001" startWordPosition="1537" endWordPosition="1540"> humanhuman dialogs with the hierarchical information shown in Figure 1 in several stages: utterance segmentation (Section 4.1), syntactic annotation (Section 4.2), dialog act tagging (Section 4.3) and 202 subtask labeling (Section 5). Dialog Clause Figure 1: Structural analysis of a dialog 4.1 Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the s</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>E. Charniak and M. Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of NAACL’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
</authors>
<title>Analyzing and predicting patterns of DAMSL utterance tags.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI</booktitle>
<contexts>
<context position="5932" citStr="Core, 1998" startWordPosition="904" endWordPosition="905">d (Gilbert et al., 2005). However, we still believe that exploiting dialog data to train/adapt or complement hand-crafted components will be vital for robust and adaptable spoken dialog systems. 3 Related Work In this paper, we discuss methods for automatically creating models of dialog structure using dialog act and task/subtask information. Relevant related work includes research on automatic dialog act tagging and stochastic dialog management, and on building hierarchical models of plans using task/subtask information. There has been considerable research on statistical dialog act tagging (Core, 1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carb</context>
<context position="10944" citStr="Core, 1998" startWordPosition="1670" endWordPosition="1671">a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Jurafsky et al., 1998), the multi-dimensionality of the scheme makes the building of models from DAMSL-tagged data complex. Furthermore, the generality of the DAMSL tags reduces their utility for natural language generation. Other tagging schemes, such as the Maptask scheme (Carletta et al., 1997), are also too general for our purposes. We were particularly concerned with obtaining sufficient discriminatory power between different types of statement (for generation), and to include an out-of-domain tag (for interpretatio</context>
</contexts>
<marker>Core, 1998</marker>
<rawString>M. Core. 1998. Analyzing and predicting patterns of DAMSL utterance tags. In Proceedings of the AAAI spring symposium on Applying machine learning to discourse processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer et al</author>
</authors>
<title>Dysfluency annotation stylebook for the switchboard corpus. Distributed by LDC.</title>
<date>1995</date>
<marker>al, 1995</marker>
<rawString>M. Meteer et.al. 1995. Dysfluency annotation stylebook for the switchboard corpus. Distributed by LDC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Di Fabbrizio</author>
<author>C Lewis</author>
</authors>
<title>Florence: a dialogue manager framework for spoken dialogue systems.</title>
<date>2004</date>
<booktitle>In ICSLP 2004, 8th International Conference on Spoken Language Processing,</booktitle>
<pages>4--8</pages>
<location>Jeju, Jeju Island,</location>
<marker>Di Fabbrizio, Lewis, 2004</marker>
<rawString>G. Di Fabbrizio and C. Lewis. 2004. Florence: a dialogue manager framework for spoken dialogue systems. In ICSLP 2004, 8th International Conference on Spoken Language Processing, Jeju, Jeju Island, Korea, October 4-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frampton</author>
<author>O Lemon</author>
</authors>
<title>Reinforcement learning of dialogue strategies using the user’s last dialogue act.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th IJCAI workshop on</booktitle>
<contexts>
<context position="7181" citStr="Frampton and Lemon, 2005" startWordPosition="1094" endWordPosition="1097">dnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson et al., 2005; Frampton and Lemon, 2005). Most recently, Henderson et al. showed that it is possible to automatically learn good dialog management strategies from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog modeling. Other researchers have looked at probabilistic models for plan recognition such as extens</context>
</contexts>
<marker>Frampton, Lemon, 2005</marker>
<rawString>M. Frampton and O. Lemon. 2005. Reinforcement learning of dialogue strategies using the user’s last dialogue act. In Proceedings of the 4th IJCAI workshop on knowledge and reasoning in practical dialogue systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gilbert</author>
</authors>
<title>Intelligent virtual agents for contact center automation.</title>
<date>2005</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>22</volume>
<issue>5</issue>
<marker>Gilbert, 2005</marker>
<rawString>M. Gilbert et al. 2005. Intelligent virtual agents for contact center automation. IEEE Signal Processing Magazine, 22(5), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions and the structure of discoursep.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="15573" citStr="Grosz and Sidner, 1986" startWordPosition="2424" endWordPosition="2427">ent, in order to increase the speed of training and to scale this method to large data sets. Each of the classes in the set is encoded as a bit vector such that, in the vector for class, the bit is one and all other bits are zero. Then, one-vs-other binary classifiers are used as follows. where is the parameter vector for the antilabel and . In order to compute , we use class independence assumption and require that and for all 5.2 Parse-based Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures. In this model, we are interested in finding the most likely plan tree ( ) given the sequence of utterances: For real-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B.J. Grosz and C.L. Sidner. 1986. Attention, intentions and the structure of discoursep. Computational Linguistics, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<volume>48</volume>
<issue>4</issue>
<contexts>
<context position="14849" citStr="Haffner, 2006" startWordPosition="2299" endWordPosition="2300">sed in a grammar G encoded as a regular expression ( ). However, in order to cope with the prediction errors of the classifier, we approximate with an -gram language model on sequences of the refined tag labels: In order to estimate the conditional distribution we use the general technique of choosing the maximum entropy (maxent) distribution that properly estimates the average of each feature over the training data (Berger et al., 1996). This can be written as a Gibbs distribution parameterized with weights, where is the size of the label set. Thus, We use the machine learning toolkit LLAMA (Haffner, 2006) to estimate the conditional distribution using maxent. LLAMA encodes multiclass maxent as binary maxent, in order to increase the speed of training and to scale this method to large data sets. Each of the classes in the set is encoded as a bit vector such that, in the vector for class, the bit is one and all other bits are zero. Then, one-vs-other binary classifiers are used as follows. where is the parameter vector for the antilabel and . In order to compute , we use class independence assumption and require that and for all 5.2 Parse-based Model As seen in Figure 3, the chunk model does not</context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
</authors>
<title>Data-driven strategies for an automated dialogue system.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’04.</booktitle>
<contexts>
<context position="8054" citStr="Hardy (2004)" startWordPosition="1228" endWordPosition="1229">use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog modeling. Other researchers have looked at probabilistic models for plan recognition such as extensions of Hidden Markov Models (Bui, 2003) and probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). In this paper, we compare hierarchical grammarstyle and flat chunking-style models of dialog. In recent research, Hardy (2004) used a large corpus of transcribed and annotated telephone conversations to develop the Amities dialog system. For their dialog manager, they trained separate task and dialog act classifiers on this corpus. For task identification they report an accuracy of 85% (true task is one of the top 2 results returned by the classifier); for dialog act tagging they report 86% accuracy. 4 Structural Analysis of a Dialog We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree that en</context>
</contexts>
<marker>Hardy, 2004</marker>
<rawString>H. Hardy et al. 2004. Data-driven strategies for an automated dialogue system. In Proceedings of ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wright Hastie</author>
</authors>
<title>Automatically predicting dialogue structure using prosodic features.</title>
<date>2002</date>
<journal>Speech Communication,</journal>
<pages>36--1</pages>
<marker>Hastie, 2002</marker>
<rawString>H. Wright Hastie et al. 2002. Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1–2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Hybrid reinforcement/supervised learning for dialogue policies from COMMUNICATOR data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th IJCAI workshop on</booktitle>
<marker>Henderson, 2005</marker>
<rawString>J. Henderson et al. 2005. Hybrid reinforcement/supervised learning for dialogue policies from COMMUNICATOR data. In Proceedings of the 4th IJCAI workshop on knowledge and reasoning in practical dialogue systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In A. Manaster-Ramer, editor,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="10734" citStr="Joshi, 1987" startWordPosition="1636" endWordPosition="1637">nce into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Jurafsky et al., 1998), the multi-dimensionality of the scheme makes the building of models from DAMSL-tagged data complex. Furthermore, the generality of the DAMSL tags reduces their utility for natural language generation. Other tagging schemes, such as the Maptask scheme (Carletta et al., 1997), are also too gen</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>A. K. Joshi. 1987. An introduction to tree adjoining grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>Switchboard discourse language modeling project report.</title>
<date>1998</date>
<tech>Technical Report Research Note 30,</tech>
<institution>Center for Speech and Language Processing, Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<marker>Jurafsky, 1998</marker>
<rawString>D. Jurafsky et al. 1998. Switchboard discourse language modeling project report. Technical Report Research Note 30, Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>A Gruenstein</author>
</authors>
<title>Multithreaded context for robust conversational interfaces: Context-sensitive speech recognition and interpretation of corrective fragments.</title>
<date>2004</date>
<journal>ACM Transactions on Computer-Human Interaction,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="6684" citStr="Lemon and Gruenstein, 2004" startWordPosition="1016" endWordPosition="1019">isambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson et al., 2005; Frampton and Lemon, 2005). Most recently, Henderson et al. showed that it is possible to automatically learn good dialog managem</context>
</contexts>
<marker>Lemon, Gruenstein, 2004</marker>
<rawString>O. Lemon and A. Gruenstein. 2004. Multithreaded context for robust conversational interfaces: Context-sensitive speech recognition and interpretation of corrective fragments. ACM Transactions on Computer-Human Interaction, 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogue strategies.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech’97.</booktitle>
<contexts>
<context position="7060" citStr="Levin and Pieraccini, 1997" startWordPosition="1074" endWordPosition="1077">ognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson et al., 2005; Frampton and Lemon, 2005). Most recently, Henderson et al. showed that it is possible to automatically learn good dialog management strategies from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context</context>
</contexts>
<marker>Levin, Pieraccini, 1997</marker>
<rawString>E. Levin and R. Pieraccini. 1997. A stochastic model of computer-human interaction for learning dialogue strategies. In Proceedings of Eurospeech’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Allen</author>
</authors>
<title>A plan recognition model for subdialogs in conversations.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="6503" citStr="Litman and Allen, 1987" startWordPosition="988" endWordPosition="991">earch on statistical dialog act tagging (Core, 1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., </context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>D. Litman and J. Allen. 1987. A plan recognition model for subdialogs in conversations. Cognitive Science, 11(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lochbaum</author>
</authors>
<title>A collaborative planning model of intentional structure.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="8597" citStr="Lochbaum, 1998" startWordPosition="1319" endWordPosition="1320">d flat chunking-style models of dialog. In recent research, Hardy (2004) used a large corpus of transcribed and annotated telephone conversations to develop the Amities dialog system. For their dialog manager, they trained separate task and dialog act classifiers on this corpus. For task identification they report an accuracy of 85% (true task is one of the top 2 results returned by the classifier); for dialog act tagging they report 86% accuracy. 4 Structural Analysis of a Dialog We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree that encapsulates the task structure (dominance and precedence relations among tasks), dialog act structure (sequences of dialog acts), and linguistic structure of utterances (inter-clausal relations and predicateargument relations within a clause), as illustrated in Figure 1. As the dialog proceeds, an utterance from a participant is accommodated into the tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Reithinger, 1997). With this model, we can tig</context>
</contexts>
<marker>Lochbaum, 1998</marker>
<rawString>K. Lochbaum. 1998. A collaborative planning model of intentional structure. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>A Mikheev</author>
</authors>
<title>The predictive power of game structure in dialogue act recognition: experimental results using maximum entropy estimation.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP’98.</booktitle>
<contexts>
<context position="5981" citStr="Poesio and Mikheev, 1998" startWordPosition="910" endWordPosition="913">we still believe that exploiting dialog data to train/adapt or complement hand-crafted components will be vital for robust and adaptable spoken dialog systems. 3 Related Work In this paper, we discuss methods for automatically creating models of dialog structure using dialog act and task/subtask information. Relevant related work includes research on automatic dialog act tagging and stochastic dialog management, and on building hierarchical models of plans using task/subtask information. There has been considerable research on statistical dialog act tagging (Core, 1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and inform</context>
<context position="13114" citStr="Poesio and Mikheev, 1998" startWordPosition="2019" endWordPosition="2022">ls for recovering the subtask structure – a chunk-based model and a parse-based model. In the chunk-based model, we recover the precedence relations (sequence) of the subtasks but not dominance relations (subtask structure) among the subtasks. Figure 3 shows a sample output from the chunk model. In the parse model, we recover the complete task structure from the sequence of utterances as shown in Figure 2. Here, we describe our two models. We present our experiments on subtask segmentation and labeling in Section 6.4. 5.1 Chunk-based model This model is similar to the second one described in (Poesio and Mikheev, 1998), except that we use tasks and subtasks rather than dialog games. We model the prediction problem as a classification task as follows: given a sequence of utterances in a dialog and a Task Task Task Utterance Utterance Utterance Topic/Subtask Topic/Subtask Topic/Subtask DialogAct,Pred−Args DialogAct,Pred−Args DialogAct,Pred−Args Shipping Info Delivery Info Opening Contact Info Order Item Payment Info Summary Closing 203 subtask label vocabulary , we need to predict the best subtask label sequence as shown in equation 1. (1) Each subtask has begin, middle (possibly absent) and end utterances. I</context>
<context position="22444" citStr="Poesio and Mikheev, 1998" startWordPosition="3501" endWordPosition="3504">t 20K utterances for 205 Maptask and 200K utterances for DAMSL). The error rates for the Switchboard-DAMSL data are significantly better than previously published results (28% error rate) (Jurafsky et al., 1998) with the same tag set. This improvement is attributable to the richer feature set we use and a discriminative modeling framework that supports a large number of features, in contrast to the generative model used in (Jurafsky et al., 1998). A similar obeservation applies to the results on Maptask dialog act tagging. Our model outperforms previously published results (42.8% error rate) (Poesio and Mikheev, 1998). In labeling the Switchboard data, long utterances were split into slash units (Meteer et.al., 1995). A speaker’s turn can be divided in one or more slash units and a slash unit can extend over multiple turns, for example: sv B.64 utt3: C but, F uh – b A.65 utt1: Uh-huh. / + B.66 utt1: – people want all of that/ sv B.66 utt2: C and not all of those are necessities. / b A.67 utt1: Right. / The labelers were instructed to label on the basis of the whole slash unit. This makes, for example, the dysfluency turn B.64 a Statement opinion (sv) rather than a non-verbal. For the purpose of discriminat</context>
</contexts>
<marker>Poesio, Mikheev, 1998</marker>
<rawString>M. Poesio and A. Mikheev. 1998. The predictive power of game structure in dialogue act recognition: experimental results using maximum entropy estimation. In Proceedings of ICSLP’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Pynadath</author>
<author>M P Wellman</author>
</authors>
<title>Probabilistic statedependent grammars for plan recognition. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI-2000).</booktitle>
<contexts>
<context position="7926" citStr="Pynadath and Wellman, 2000" startWordPosition="1207" endWordPosition="1210">from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog modeling. Other researchers have looked at probabilistic models for plan recognition such as extensions of Hidden Markov Models (Bui, 2003) and probabilistic context-free grammars (Alexandersson and Reithinger, 1997; Pynadath and Wellman, 2000). In this paper, we compare hierarchical grammarstyle and flat chunking-style models of dialog. In recent research, Hardy (2004) used a large corpus of transcribed and annotated telephone conversations to develop the Amities dialog system. For their dialog manager, they trained separate task and dialog act classifiers on this corpus. For task identification they report an accuracy of 85% (true task is one of the top 2 results returned by the classifier); for dialog act tagging they report 86% accuracy. 4 Structural Analysis of a Dialog We consider a task-oriented dialog to be the result of inc</context>
</contexts>
<marker>Pynadath, Wellman, 2000</marker>
<rawString>D.V. Pynadath and M.P. Wellman. 2000. Probabilistic statedependent grammars for plan recognition. In In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rich</author>
<author>C L Sidner</author>
</authors>
<title>COLLAGEN: When agents collaborate with people.</title>
<date>1997</date>
<booktitle>In Proceedings of the First International Conference on Autonomous Agents (Agents’97).</booktitle>
<contexts>
<context position="6526" citStr="Rich and Sidner, 1997" startWordPosition="992" endWordPosition="995">log act tagging (Core, 1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., </context>
</contexts>
<marker>Rich, Sidner, 1997</marker>
<rawString>C. Rich and C.L. Sidner. 1997. COLLAGEN: When agents collaborate with people. In Proceedings of the First International Conference on Autonomous Agents (Agents’97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="16007" citStr="Roark, 2001" startWordPosition="2492" endWordPosition="2493">ed Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures. In this model, we are interested in finding the most likely plan tree ( ) given the sequence of utterances: For real-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7 using one-best (or k-best) chunk output.1 where 6 Experiments and Results In this section, we present the results of our experiments for modeling subtask structure. 6.1 Data As our primary data set, we used 915 telephonebased customer-agent dialogs related to the task of ordering products from a catalog. Each dialog was transcribed by hand; all numbers (telephone, credit card, etc.) were removed for privacy reasons. The average d</context>
<context position="25733" citStr="Roark, 2001" startWordPosition="4030" endWordPosition="4031">ror rates, probably due to the high error rate of the dialog act tagger. Feature Utterance Context Context Current +prev +three prev utt/with DA utt/with DA utt/with DA Unigram 42.9/42.4 33.6/34.1 30.0/30.3 (53.4/52.8) (43.0/43.0) (37.6/37.6) Trigram 41.7/41.7 31.6/31.4 30.0/29.1 (52.5/52.0) (42.9/42.7) (37.6/37.4) Table 5: Error rate for predicting the refined subtask labels. The error rates without the wellformedness constraint is shown in parenthesis. The error rates with dialog acts as features are separated by a slash. 6.4.2 Parsing-based Model We retrained a top-down incremental parser (Roark, 2001) on the plan trees in the training dialogs. For the test dialogs, we used the-best (k=50) refined subtask labels for each utterance as predicted by the chunk-based classifier to create a lattice of subtask label sequences. For each dialog we then created -best sequences (100-best for these experiments) of subtask labels; these were parsed and (re-)ranked by the parser.3 We combine the weights of the subtask label sequences assigned by the classifier with the parse score assigned by the parser and select the top 3Ideally, we would have parsed the subtask label lattice directly, however, the par</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Samuel</author>
</authors>
<title>Computing dialogue acts from features with transformation-based learning.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI</booktitle>
<marker>Samuel, 1998</marker>
<rawString>K. Samuel et al. 1998. Computing dialogue acts from features with transformation-based learning. In Proceedings of the AAAI spring symposium on Applying machine learning to discourse processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scheffler</author>
<author>S Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT’02.</booktitle>
<contexts>
<context position="7087" citStr="Scheffler and Young, 2002" startWordPosition="1078" endWordPosition="1081">.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and Young, 2002; Singh et al., 2002; Williams et al., 2005; Henderson et al., 2005; Frampton and Lemon, 2005). Most recently, Henderson et al. showed that it is possible to automatically learn good dialog management strategies from automatically labeled data over a large potential space of dialog states (Henderson et al., 2005); and Frampton and Lemon showed that the use of context information (the user’s last dialog act) can improve the performance of learned strategies (Frampton and Lemon, 2005). In this paper, we combine the use of automatically labeled data and extended context for automatic dialog model</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>K. Scheffler and S. Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>A relaxation method for understanding spontaneous speech utterances.</title>
<date>1992</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<location>San Mateo, CA.</location>
<contexts>
<context position="10004" citStr="Seneff, 1992" startWordPosition="1531" endWordPosition="1532">re of task-oriented dialogs, we label humanhuman dialogs with the hierarchical information shown in Figure 1 in several stages: utterance segmentation (Section 4.1), syntactic annotation (Section 4.2), dialog act tagging (Section 4.3) and 202 subtask labeling (Section 5). Dialog Clause Figure 1: Structural analysis of a dialog 4.1 Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Syntactic Annotation We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local stru</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>S. Seneff. 1992. A relaxation method for understanding spontaneous speech utterances. In Proceedings of the Speech and Natural Language Workshop, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>32</volume>
<marker>Shriberg, 2000</marker>
<rawString>E. Shriberg et al. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Plan parsing for intended response recognition in discourse.</title>
<date>1985</date>
<journal>Computational Intelligence,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="6479" citStr="Sidner, 1985" startWordPosition="986" endWordPosition="987">nsiderable research on statistical dialog act tagging (Core, 1998; Jurafsky et al., 1998; Poesio and Mikheev, 1998; Samuel et al., 1998; Stolcke et al., 2000; Hastie et al., 2002). Several disambiguation methods (n-gram models, hidden Markov models, maximum entropy models) that include a variety of features (cue phrases, speaker ID, word n-grams, prosodic features, syntactic features, dialog history) have been used. In this paper, we show that use of extended context gives improved results for this task. Approaches to dialog management include AI-style plan recognition-based approaches (e.g. (Sidner, 1985; Litman and Allen, 1987; Rich and Sidner, 1997; Carberry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management that are evaluated by means of dialog simulations (Levin and Pieraccini, 1997; Scheffler and You</context>
</contexts>
<marker>Sidner, 1985</marker>
<rawString>C.L. Sidner. 1985. Plan parsing for intended response recognition in discourse. Computational Intelligence, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<marker>Singh, 2002</marker>
<rawString>S. Singh et al. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, 2000</marker>
<rawString>A. Stolcke et al. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
</authors>
<title>Intonation and dialogue context as constraints for speech recognition.</title>
<date>1998</date>
<journal>Language and Speech,</journal>
<volume>41</volume>
<issue>3</issue>
<marker>Taylor, 1998</marker>
<rawString>P. Taylor et al. 1998. Intonation and dialogue context as constraints for speech recognition. Language and Speech, 41(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
</authors>
<title>Partially observable Markov decision processes with continuous observations for dialogue management.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGdial.</booktitle>
<marker>Williams, 2005</marker>
<rawString>J. Williams et al. 2005. Partially observable Markov decision processes with continuous observations for dialogue management. In Proceedings of SIGdial.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>