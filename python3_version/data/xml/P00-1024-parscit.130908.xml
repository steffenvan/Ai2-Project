<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000330">
<title confidence="0.967341">
Learning Attribute Selections for Non-Pronominal Expressions
</title>
<author confidence="0.895414">
Pamela Jordan
</author>
<affiliation confidence="0.8600085">
Intelligent Systems Program
University of Pittsburgh
</affiliation>
<address confidence="0.565188">
Pittsburgh, PA
</address>
<email confidence="0.995864">
jordan@isp.pitt.edu
</email>
<author confidence="0.72315">
Marilyn Walker
</author>
<affiliation confidence="0.628313">
AT&amp;T Labs—Research
</affiliation>
<address confidence="0.92359">
180 Park Avenue
Florham Park, NJ 07932-0971 USA
</address>
<email confidence="0.998862">
walker@research.att.com
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999623863636364">
A fundamental function of any task-
oriented dialogue system is the abil-
ity to generate nominal expressions
that describe objects in the task
domain. In this paper, we report
results from using machine learn-
ing to train and test a nominal-
expression generator on a set of 393
nominal descriptions from the CO-
CONUT corpus of task-oriented de-
sign dialogues. Results show that
we can achieve a 50% match to hu-
man performance as opposed to a
16% baseline for just guessing the
most frequent type of nominal ex-
pression in the COCONUT corpus. To
our surprise our results indicate that
many of the central features of previ-
ously proposed selection models did
not improve the performance of the
learned nominal-expression genera-
tor.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986821033333333">
A fundamental function of any task-oriented
dialogue system is the ability to generate
nominal expressions that describe objects in
the task domain. For example, consider the
excerpt of a task-oriented dialogue from the
COCONUT corpus in Figure 1 (Di Eugenio et
al., 2000) The conversants in this dialogue are
attempting to collaboratively construct a so-
lution for furnishing a two room house. Each
conversant starts the task with a set of furni-
ture items that can be used in the solution. In
the process of negotiating the solution, they
(Partial solution to problem already agreed upon in
prior dialogue)
G: That leaves us with 250 dollars. I have a yellow
rug for 150 dollars. Do you have any other furniture
left that matches for 100 dollars?&amp;quot;
S: No, I have no furniture left that costs $100. I guess
you can buy the yellow rug for $150.
G: Okay. I&apos;ll buy the rug for 150 dollars. I have a
green chair that I can buy for 100 dollars that should
leave us with no money.
S: That sounds good. Go ahead and buy the yellow
rug and the green chair.
G: I&apos;ll buy the green 100 dollar chair. Design Com-
plete?
S: Sounds good, do you want the green chair in the
dining room with the other chairs? I put the yellow
rug in the living room. Then the design is complete.
G: Sounds good. Hit the design complete
</bodyText>
<figureCaption confidence="0.934217666666667">
Figure 1: Excerpt of a COCONUT dialogue il-
lustrating variable selection of attributes for
nominal descriptions
</figureCaption>
<bodyText confidence="0.999960285714286">
generate nominal expressions (shown in ital-
ics) describing the items of furniture.
Each furniture type in the COCONUT task
domain has four associated attributes: color,
price, owner and quantity. A nominal expres-
sion generator must decide which of these four
attributes to include in the generated expres-
sion. For example, the task domain objects
under discussion in the dialogue in Figure 1
are a $150 yellow rug owned by Garrett (CI)
and a $100 dollar green chair owned by Steve
(5). In the dialogue excerpt in Figure 1 the
yellow rug is described first as a yellow rug
for 150 dollars and then subsequently as the
yellow rug for 150 dollars, the rug for 150
dollars, the yellow rug. It could also have
been described by any of the following non-
pronominal expressions: the rug, my rug, my
yellow rug, my $150 yellow rug, the $150 yel-
low rug, the $150 rug. The content of these
descriptions varies depending on which at-
tributes are included in the description. How
does the speaker decide which attributes to
include?
The problem of content selection for nom-
inal expressions has been the focus of much
previous work and a large number of models
have been proposed (Clark and Wilkes-Gibbs,
1986; Brennan and Clark, 1996; Dale and Re-
iter, 1995; Passonneau, 1995; Jordan, 2000)
inter alia. The factors that these models uti-
lize include the discourse structure, the at-
tributes used in the last mention, the recency
of last mention, the frequency of mention, the
task structure, the inferential complexity of
the task, and ways of determining salient ob-
jects and the salient attributes of an object.
In this paper we utilize a set of factors consid-
ered as important for three of these models,
and empirically compare the utility of these
factors as predictors in a machine learning ex-
periment. The factor sets we utilize are:
</bodyText>
<listItem confidence="0.992026333333333">
• CONTRAST SET factors, inspired
by the INCREMENTAL MODEL of
Dale and Reiter (1995);
• CONCEPTUAL PACT factors, inspired by
the models of Clark and colleagues
(Clark and Wilkes-Gibbs, 1986; Brennan
and Clark, 1996);
• INTENTIONAL INFLUENCES factors, in-
spired by the model of Jordan (2000).
</listItem>
<bodyText confidence="0.992296626666667">
Dale and Reiter&apos;s INCREMENTAL MODEL fo-
cuses on the production of near-minimal de-
scriptions that allow the hearer to reliably
distinguish the task object from similar task
objects. Following Grosz and Sidner (1986),
Dale and Reiter&apos;s algorithm utilizes discourse
structure as an important factor in determin-
ing which objects the current object must
be distinguished from. The model of Clark,
Brennan and Wilkes-Gibbs is based on the
notion of CONCEPTUAL PACTS, i.e. the con-
versants attempt to coordinate with one an-
other by establishing a conceptual pact for
describing an object. Jordan&apos;s INTENTIONAL
INFLUENCES model is based on the assump-
tion that the underlying task-related infer-
ences required to achieve the task goals are
an important factor in content selection for
non-minimal descriptions. We describe these
models in more detail below.
We compare the predictive power of the
factors utilized in these models by using ma-
chine learning to train and test a nominal-
expression generator on a set of 393 nomi-
nal descriptions from the corpus of COCONUT
dialogues. We provide the machine learner
with distinct sets of features motivated by the
models above, in addition to discourse fea-
tures representing given-new distinctions, and
dialogue specific features such as the speaker
of the nominal expression, its absolute loca-
tion in the discourse, and the problem that
the conversants are currently trying to solve.
We evaluate the nominal-expression gen-
erator by comparing its predictions against
what humans said at the same point in
the dialogue. We provide a rigorous test
of the nominal-expression generator by only
counting as correct those nominal expressions
which exactly match the content of the hu-
man generated nominal expressions.&apos; We also
quantify the contributions of each feature set
to the performance of the nominal-expression
generator. Our results show that nominal-
expression generators based on a combination
of the given-new, and dialogue specific and IN-
TENTIONAL INFLUENCES features can achieve
50% accuracy at matching human perfor-
mance, a significant improvement over the
majority class baseline of 16% in which the
generator simply guesses the most frequent
property combination. In addition, to our
surprise, the results indicate that the CON-
CEPTUAL PACT features and the CONTRAST
SET features make no significant contribution
to performance.
&apos;While this approach is controversial, we believe
that human performance is currently the only reason-
able standard against which we can evaluate natural
language generators (Oberlander, 1998). Note that
the more attributes a discourse entity has, the harder
it is to achieve an exact match to a human descrip-
tion, i.e. for our problem the nominal-expression gen-
erator must correctly choose among 16 possibilities
represented by the power set of the four attributes.
Section 2 describes the COCONUT corpus,
the encoding of the corpus and the features
used in machine learning in more detail. Sec-
tion 3 presents the quantitative results of test-
ing the learned rules against the corpus, dis-
cusses the features that the machine learner
identifies as important, and provides exam-
ples of the rules that are learned. Section 4
summarizes our results and discusses future
work.
</bodyText>
<sectionHeader confidence="0.84098" genericHeader="method">
2 Corpus, Data, Methods
</sectionHeader>
<bodyText confidence="0.999792447368421">
Our experiments utilize the rule learning
program RIPPER (Cohen, 1996) to learn a
nominal-expression generator from the nomi-
nal expressions in the COCONUT corpus. Al-
though we had several learners available to
us, we chose RIPPER primarily because the if-
then rules that are used to express the learned
nominal generator model are easy for peo-
ple to understand and thus facilitate compar-
ison with the theoretical models we are try-
ing to evaluate. Like other learning programs,
RIPPER takes as input the names of a set of
classes to be learned, the names and ranges of
values of a fixed set of features, and training
data specifying the class and feature values for
each example in a training set. Its output is
a classification model for predicting the class
of future examples. In RIPPER, the classifi-
cation model is learned using greedy search
guided by an information gain metric, and is
expressed as an ordered set of if-then rules.
Thus to apply RIPPER, the nominal expres-
sions in the corpus must be encoded in terms
of a set of classes (the output classification)
and a set of input features that are used
as predictors for the classes. As mentioned
above, we are trying to learn which of a set
of content attributes should be included in a
nominal expression. The features we encode
for each nominal expression are motivated by
factors claimed in the literature to be impor-
tant predictors of the content of a nominal
expression.
Below we describe our corpus of nominal
expressions, the assignment of classes to each
nominal expression, the extraction of features
from the dialogue in which each expression
occurs, and our learning experiments.
</bodyText>
<subsectionHeader confidence="0.948746">
2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99998556626506">
The COCONUT corpus is a set of 24 computer-
mediated dialogues consisting of a total of
1102 utterances. The dialogues were collected
in an experiment where two human subjects
collaborated on a simple design task, that of
buying furniture for two rooms of a house (Di
Eugenio et al., 1998). An excerpt of a CO-
CONUT dialogue was given in Figure 1. The
participants&apos; main goal is to negotiate the
purchases; the items of highest priority are
a sofa for the living room and a table and
four chairs for the dining room. The partici-
pants also have specific secondary goals which
further constrain the problem solving task.
Participants are instructed to try to meet as
many of these goals as possible, and are mo-
tivated to do so by rewards associated with
satisfied goals. The secondary goals are: 1)
match colors within a room, 2) buy as much
furniture as you can, 3) spend all your money.
The participants are told which rewards are
associated with achieving each goal.
Each participant is given a separate bud-
get and inventory of furniture. Neither par-
ticipant knows what is in the other&apos;s inven-
tory or how much money the other has. By
sharing information during the conversation,
they can combine their budgets and select
furniture from each other&apos;s inventories. The
participants are equals and purchasing deci-
sions are joint. In the experiment, each set
of participants solved one to three scenarios
with varying inventories and budgets. The
problem scenarios varied task complexity by
ranging from tasks where items are inexpen-
sive and the budget is relatively large to tasks
where the items are expensive and the budget
relatively small.
After the corpus was collected it was anno-
tated by human coders for two types of fea-
tures. The DISCOURSE ENTITY LEVEL anno-
tations provide discourse reference informa-
tion from which initial representations of dis-
course entities and updates to them can be
derived, and explicit attribute usage informa-
tion that reflects how each discourse entity
was evoked. For example, the initial repre-
sentation for &amp;quot;I have a yellow rug. It costs
$150.&amp;quot; would include type, quantity, color
and owner following the first utterance. Only
the quantity attribute is inferred. After the
second utterance the entity would be updated
to include price. The UTTERANCE LEVEL AN-
NOTATIONS capture the problem solving state
in terms of goals, constraint changes and the
size of the solution set for the current con-
straint equations as well as current variable
assignments. The utterance level discourse
features encode when an offer is made and
the level of a speaker&apos;s commitment to a pro-
posal under consideration, i.e. conditional or
unconditional.
In order to derive some of the discourse in-
formation the task structure must be identi-
fied. The COCONUT corpus was encoded via
a set of instructions to coders to record all
domain goals. Changes to a different domain
goal or action were used as a cue to derive the
non-linguistic task structure (Terken, 1985;
Grosz and Sidner, 1986). Each domain ac-
tion provides a discourse segment purpose so
that each utterance that relates to a different
domain action or set of domain actions defines
a new segment. The encoded features all have
good intercoder reliability (Di Eugenio et al.,
1998; Jordan, 2000).
Our experimental data is 393 non-
pronominal nominal descriptions from 13 dia-
logues of the COCONUT corpus as well as fea-
tures constructed from the annotations de-
scribed above. We explain how we use the
annotations to construct the features in more
detail below.
</bodyText>
<subsectionHeader confidence="0.99602">
2.2 Class Assignment
</subsectionHeader>
<bodyText confidence="0.999933333333333">
The corpus of nominal expressions is used to
construct the machine learning classes as fol-
lows.
We are trying to learn which subset of the
four attributes, color, price, owner, quantity,
should be included in a nominal expression.
We encode each nominal expression in the
corpus as a member of the category repre-
sented by the set of properties expressed by
the nominal expression. This results in 16
classes representing the power set of the four
attributes.
</bodyText>
<subsectionHeader confidence="0.997416">
2.3 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.998904272727273">
The corpus is used to construct the machine
learning features as follows. In RIPPER, fea-
ture values are continuous (numeric), set-
valued, or symbolic. We encoded each non-
pronominal description in terms of a set of 58
features that were either directly annotated
by humans as described above, derived from
annotated features or inherent to the dialogue
(Di Eugenio et al., 1998; Jordan, 2000). The
dialogue context in which each description oc-
curs is represented in the encodings.
</bodyText>
<listItem confidence="0.99150125">
• what is mutually known: type-mk, color-mk,
owner-mk, price-mk, quantity-mk
• reference-relation: one of initial, coref, , in-
ference
</listItem>
<figureCaption confidence="0.99572">
Figure 2: Given-New Feature Set.
</figureCaption>
<bodyText confidence="0.999754296296296">
The GIVEN-NEW features in Figure 2 en-
code fundamental attributes of the entity that
is to be described by the nominal expression
(Clark and Marshall, 1981; Prince, 1981). We
encode what is mutually known about the dis-
course entity at the point at which it is to
be described (type-ink, color-ink, owner-ink,
price-ink, quantity-ink). We utilize a refer-
ence-relation feature to encode whether the
entity is new (initial), given (coref) or dis-
course inferred (inference) relative to the
discourse history. The types of inferences sup-
ported by the annotation are set, subset, class
and common noun anaphora (e.g. one and
null anaphora) (Jordan, 2000).
The INHERENT FEATURES in Figure 3 are
a specific encoding of particulars about the
discourse situation, such as the speaker, the
task, and the entity&apos;s known attributes (type,
color, owner, price, quantity). While we don&apos;t
expect this feature set to generalize to other
dialogue situations, it allows us to examine
whether there are individual differences in at-
tribute selection algorithms (speaker, speaker-
pair), or whether specifics about the prop-
erties of the object, the location within the
dialogue (utterance-number), and the prob-
</bodyText>
<listItem confidence="0.994863666666667">
• utterance-number, speaker-pair, speaker, prob-
lem-number
• attribute values:
</listItem>
<tableCaption confidence="0.8972575">
type: one of sofa, chair, table, rug,
lamp
color: one of red, blue, green, yellow
owner: one of self, other
price: ranged from $50 to $600
quantity: ranged from 0 to 4.
</tableCaption>
<figureCaption confidence="0.931600666666667">
Figure 3: INHERENT Feature Set: Task,
Speaker and Discourse Entity Specific fea-
tures.
</figureCaption>
<listItem confidence="0.9821155">
• interactions with other discourse entities: dis-
tance-last-ref, distance-last-ref-in-turns, num-
ber-prev-mentions, speaker-of-last-ref
• previous description: color-in-last-exp, type-
</listItem>
<bodyText confidence="0.54842725">
in-last-exp, owner-in-last-exp, price-in-last-exp,
quantity-in-last-exp, type-in-last-turn, color-in-
last-turn, owner-in-last-turn, price-in-last-turn,
quantity-in-last-turn, initial-in-last-turn
</bodyText>
<figureCaption confidence="0.996428">
Figure 4: CONCEPTUAL PACT Feature Set.
</figureCaption>
<bodyText confidence="0.999130648648649">
lem difficulty (problem-number) play signifi-
cant roles in attribute selection.
The CONCEPTUAL PACT model suggests
that dialogue participants negotiate a descrip-
tion that both find adequate for describing an
object (Clark and Wilkes-Gibbs, 1986; Bren-
nan and Clark, 1996). The speaker gener-
ates trial descriptions that the hearer modifies
based on which object he thinks he is sup-
pose to identify. The negotiation continues
until the participants are confident that the
hearer has correctly identified the intended
object. The additional features suggested by
this model include the previous description
since that is the description that will be mod-
ified, and how long ago the description was
made. If the description were made further
back in the dialogue, that would indicate that
the negotiation process had been completed.
Furthermore, the model suggests that, once a
pact has been reached, that the dialogue par-
ticipants will continue to use the description
that they previously negotiated. This aspect
of the model is also similar to Passonneau&apos;s
LEXICAL FOCUS model (Passonneau, 1995).
The CONCEPTUAL PACT features in Fig-
ure 4 encode how the current description re-
lates to previous descriptions of the same
entity. We encode when the entity was
last described in terms of number of ut-
terances and turns (distance-last-ref, dis-
tance-last-in-turns), how frequently it was de-
scribed (number-prey-mentions), who last de-
scribed it (speaker-of-last-ref), and how it
was last described in terms of turn and
expression since the description may have
been broken into several utterances (color-in-
</bodyText>
<construct confidence="0.7263185">
last- exp, type-in-last- exp, owner-in-last- exp,
price-in-last-exp, quantity-in-last-exp, type-
in-last-turn, color-in-last-turn, owner-in-last-
turn, price-in-last-turn, quantity-in-last-turn,
</construct>
<listItem confidence="0.980200666666667">
• ONE UTTERANCE Distractors: type-distractors,
color-distractors, owner-distractors, price-dis-
tractors, quantity-distractors
• SEGMENT Distractors: type-distractors, color-
distractors, owner-distractors, price-distractors,
quantity-distractors
</listItem>
<figureCaption confidence="0.997244">
Figure 5: CONTRAST SET Feature Sets
</figureCaption>
<bodyText confidence="0.929649531914894">
The INCREMENTAL MODEL builds a descrip-
tion incrementally by considering the other
objects that are currently expected to be in
focus for the hearer (Dale and Reiter, 1995).
These other objects are called distractors.
The basic idea is to add attributes as nec-
essary until any distractors are ruled out as
competing co-specifiers. Based on these ideas,
we developed a set of features we call CON-
TRAST SET features, as in Figure 5. The
goal of our encoding is to represent whether
there are distractors present in the focus space
which might motivate the inclusion of a par-
ticular attribute.2
2This representatation only approximates the
model since the INCREMENTAL model utilizes a pre-
ferred salience ordering of attributes and eliminates
distractors as attributes are added to a description.
For example, adding the attribute type when the ob-
ject is a chair, eliminates any distractors that aren&apos;t
chairs. Our encoding treats attributes instead of ob-
jects as distractors. This view has the advantage that
the preferred ordering of attributes could adjust ac-
cording to the focus space and this interpretation of
Dale and Reiter&apos;s model was shown in (Jordan, 2000)
to perform similarly to the strict model. However, the
feature representation is still impoverished with re-
spect to (Jordan, 2000) since it doesn&apos;t capture what
the most salient attribute values for the focus space
are.
An open issue with deriving the distrac-
tors is how to define a focus space (Walker,
1996). We use two focus space definitions,
one based on recency, and the other on in-
tentional structure. See Figure 5. For in-
tentional structure we utilize the task goal
segmentation encoded in the COCONUT cor-
pus as discussed above (SEGMENT). For re-
cency, we simply consider the entities from
the previous utterance as possible distractors
(ONE UTTERANCE). For each focus space
definition, we encode whether the attribute
value of the item to be described is the
same as at least one other item in the fo-
cus space (type- distractors , color- distractors ,
owner-distractors, price-distractors, quantity-
distractors).
</bodyText>
<listItem confidence="0.680298888888889">
• task situation: goal, colormatch, colormatch-
constraintpresence, pricelimit, pricelimit-con-
straintpresence, priceevaluator, priceevaluator-
constraintpresence, colorlimit, colorlimit-con-
straintpresence, priceupperlimit, priceupperlim-
it-constraintpresence
• agreement state: influence-on-listener, com-
mit-speaker, solution-size, prev-influence-on-lis-
tener, prey-commit-speaker, prey-solution-size,
</listItem>
<figureCaption confidence="0.6914575">
distance-of-last-state-in-utterances, dist ance-
of-last-stat e- in-t urns, ref-made-in-prey- action-
st at e, speaker-of-last-state
• solution interactions: color-contrast, price-con-
trast
Figure 6: Intentional Influences Feature Set.
</figureCaption>
<bodyText confidence="0.99977440625">
Jordan (Jordan, 2000) proposed a model to
select attributes for nominals called the IN-
TENTIONAL INFLUENCES model. This model
posits that the task-related inferences and
the agreement process for task negotiation
are important factors in selecting attributes.
The features used to approximate Jordan&apos;s
model are in Figure 6. The task situation
features encode inferrable changes in the task
situation that are related to item attributes.
The agreement state features encode criti-
cal points of agreement during problem solv-
ing. For example, if a dialogue participant
is accepting a proposal, she may want to
verify that she has the same item and the
same entity description as her partner. These
are features that (Di Eugenio et al., 2000)
found to be indicative of agreement states
and include DAMSL features (influence-
on-listener, commit-speaker, prey-influence-
on-listener, prey-commit-speaker) (Allen and
Core, 1997), progress towards a solution
(solution-size, prey-solution-size, ref-made-
in-prey- action-state), and features inherent
to an agreement state (speaker-of-last-state,
distance-of-last-state-in-utterances, distance-
of-last-state-in-turns). The solution interac-
tions features represent situations where mul-
tiple proposals are under consideration which
may contrast with one another in terms of
solving color-matching goals (color-contrast)
or price related goals (price-contrast).
</bodyText>
<subsectionHeader confidence="0.995343">
2.4 Learning Experiments
</subsectionHeader>
<bodyText confidence="0.99999647826087">
The final input for learning is training data,
i.e., a representation of a set of nominal ex-
pressions in terms of feature and class values.
In order to induce rules from a variety of fea-
ture representations, our training data is rep-
resented differently in different experiments.
First, examples are represented using only the
GIVEN-NEW features in Figure 2 to establish a
performance baseline for given-new informa-
tion. Then other feature sets are added in to
examine their individual contribution, culmi-
nating with the full feature set.
The output of each machine learning exper-
iment is a model for nominal expression gen-
eration for this domain and task, learned from
the training data. To evaluate these models,
the error rates of the learned models are esti-
mated using 25-fold cross-validation, i.e. the
total set of examples is randomly divided into
25 disjoint test sets, and 25 runs of the learn-
ing program are performed. Thus, each run
uses the examples not in the test set for train-
ing and the remaining examples for testing.
</bodyText>
<sectionHeader confidence="0.986593" genericHeader="method">
3 Experimental Results
</sectionHeader>
<footnote confidence="0.800504">
Table 1 summarizes our experimental results.
For each feature set, we report accuracy
rates and standard errors resulting from cross-
validation.3 It is clear that performance de-
3Accuracy rates are statistically significantly dif-
ferent when the accuracies plus or minus twice the
standard error do not overlap (Cohen, 1995), p. 134.
</footnote>
<bodyText confidence="0.9996053">
pends on the features that the learner has
available. The 16.3% MAJORITY CLASS BASE-
LINE accuracy rate in the first row is a stan-
dard baseline that corresponds to the accu-
racy one would achieve from simply choos-
ing the description type that occurs most
frequently in the corpus, which in this case
means that the nominal-expression generator
would always use the color, price and quantity
to describe a domain entity.
</bodyText>
<table confidence="0.999673333333333">
Feature Sets Used Accuracy (SE)
MAJORITY CLASS BASELINE 16.3 %
GIVEN-NEW 18.2% (2.3)
GIVEN-NEw,sEG 18.5% (2.5)
GIVEN-NEw,cP 20.3% (2.5)
GIVEN-NEW,IINF 33.1% (2.7)
GIVEN-NEW,IINF ,CP , SEG 33.6 % (2.2)
GIVEN-NEW, INH 42.6% (2.7)
GIVEN-NEW,IINF,INH 47.9% (2.0)
GIVEN-NEW,IINF,INH,CP 50.1% (2.2)
GIVEN-NEW,IINF ,INH, CP ,SEG 48.2% (2.9)
GIVEN-NEW,IINF ,INH, CP , iUTT 46.0% (1.9)
</table>
<tableCaption confidence="0.999424">
Table 1: Accuracy rates for the Nominal Gen-
</tableCaption>
<bodyText confidence="0.991175772727273">
erator using different feature sets, SE = Stan-
dard Error. CP = the CONCEPTUAL PACT
features. IINF = the INTENTIONAL INFLU-
ENCES features. INH = the INHERENT fea-
tures. SEG = the CONTRAST-SET, SEGMENT
features. 1uTT = the CONTRAST SET, ONE
UTTERANCE features.
The row of Table 1 labelled GIVEN-NEW
shows that providing the learner with infor-
mation about whether the values of the at-
tributes for a discourse entity are mutually
known does not, in and of itself, improve per-
formance over the baseline. Similarly, the
rows labelled GIVEN-NEW,SEG and GIVEN-
NEW,CP show that providing the features for
contrast set and conceptual pact does not
statistically improve performance over the
baseline. The GIVEN-NEW,IINF and GIVEN-
NEW,IINF,CP,SEG rows show that adding IN-
TENTIONAL INFLUENCES features does pro-
vide a significant performance improvement,
but allowing the learner to learn rules that
would combine features from the INTEN-
TIONAL INFLUENCES features, the CONTRAST
SET features and the CONCEPTUAL PACT fea-
tures does not significantly improve perfor-
mance over just having the INTENTIONAL IN-
FLUENCES features alone. Figure 7 shows
the rules that are learned for the generation
of nominal expressions given the GIVEN-NEW
and INTENTIONAL INFLUENCES features.
The row labelled GIVEN-NEW,INH in Table
1 shows that, if we were only interested in
doing well in this domain, that adding dis-
course entity and task specific information
does improve the performance of the learned
nominal-expression generator. This is at the
cost of losing generality in the rules that are
learned. Figure 8 shows that the generation
rules learned given access to the INHERENT
feature set make use of many discourse en-
tity, task, and speaker specific features. The
speaker-pair feature alone is used in seven of
the learned rules.
The GIVEN-NEW,IINF,INH row in Table 1
suggests that interesting rules can be learned
by adding the INTENTIONAL INFLUENCES fea-
tures to the INHERENT features, but the per-
formance improvement over the INHERENT
feature set is not significant.
The remainder of the table shows that the
ability to utilize all of the features provides a
slight performance improvement which how-
ever is not statistically significant. The last
two rows suggest that adding in features rep-
resenting various views of discourse segmenta-
tion do not contribute to performance. Figure
8 shows the generation rules learned with the
best performing features set shown in the row
labelled GIVEN-NEW, IINF ,INH, CP. As men-
tioned above, many task, entity and speaker
specific features are used in these rules. How-
ever, this rule set performs at 50% accuracy,
as opposed to 33.6% accuracy for our most
general feature set (shown in the row labelled
GIVEN- NEW IINF CP, SEG) .
</bodyText>
<sectionHeader confidence="0.990228" genericHeader="discussions">
4 Discussion and Future Work.
</sectionHeader>
<bodyText confidence="0.998120142857143">
While previous research in natural language
generation has applied machine learning to
accent placement, cue-word selection, text
planning, and determining the form of a nom-
inal expression (Hirschberg, 1993; Moser and
Moore, 1995; Mellish et al., 1998; Poesio,
2000; Strube and Wolters, 2000), we know of
</bodyText>
<table confidence="0.988901413793103">
Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS)
Say PO if (pricelimit=yes)
Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA)
Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distance-
of-last-st at e-in-utt erances &lt; 4)
Say CO if (colorlimit=yes)
Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A
(solution-size=INDETERMINATE)
Say CO if (distance-of-last-state-in-utterances&gt;9) A (distance-of-last-state-in-turns&lt;l)
Say 0 if (influence-on-listener=info-request) A (distance-of-last-state-in-turns&lt;O)
Say 0 if (prev-influence-on-listener=open-option) A (quantity-mk=yes) A (prey-solution-
size=INDETERMINATE)
Say CP if (solution-size=INDETERMINATE) A (influence-on-listener=na) A (price-contrast=yes) A (distance-
of-last-state-in-turns&gt;2)
Say T if (prev-solution-size=DETERMINATE) A (color-contrast=no) A (distance-of-last-state-in-utterances&gt;3)
A (prev-influence-on-listener=na)
Say T if (prev-solution-size=DETERMINATE) A (colormatch-constraintpresence=EXPLICIT)
Say T if (ref-made-in-prev-action-state=yes) A (distance-of-last-state-in-turns&lt;O) A (color-mk=yes)
Say T if (influence-on-listener=info-request)
Say T if (priceevaluator=yes)
Say CPOQ if (distance-of-last-state-in-utterances&gt;5) A (speaker-of-last-state=0THER) A (color-contrast=no)
Say CPOQ if (goal=SELECTCHAIRS) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prey-
action-state=no) A (distance-of-last-state-in-utterances&lt;3)
Say CPO if (influence-on-listener=action-directive) A (reference-relation=initial)
Say CPO if (goal=SELECTSOFA) A (distance-of-last-state-in-utterances&gt;2)
Say CPO if (ref-made-in-prev-action-state=no) A (goal=SELECTTABLE) A (distance-of-last-state-in-turns&gt;1)
Say CPO if (prev-influence-on-listener=action-directive) A (goal=SELECTTABLE)
Say CPO if (influence-on-listener=open-option) A (distance-of-last-state-in-utterances&lt;O)
default Say CPQ
</table>
<figureCaption confidence="0.984724666666667">
Figure 7: Rules Learned Using GIVEN-NEW and INTENTIONAL INFLUENCES Features. The
classes encode the four attributes, e.g CPOQ = Color,Price,Owner and Quantity, T = Type
only
</figureCaption>
<bodyText confidence="0.999939285714286">
no other work applying machine learning to
determining the content of a nominal expres-
sion. In our experiments, we train a nominal-
expression generator to learn which attributes
of an entity to include in a nominal descrip-
tion from a corpus of dialogues. Our results
show that:
</bodyText>
<listItem confidence="0.996344714285714">
• Our best performing learned nominal-
expression generator can achieve a 50%
match to human performance as opposed
to a 16% baseline;
• The intentional influences features devel-
oped to approximate Jordan&apos;s intentional
influences model do significantly improve
performance;
• Features specific to the task, speaker and
discourse entity also provide significant
performance improvements;
• Surprisingly, the use of given-new, con-
trast set and conceptual pact features do
not improve performance.
</listItem>
<bodyText confidence="0.999990260869565">
We might have expected to achieve a best-
performing accuracy higher than 50% but as
this is the first study of this kind, there are
several issues to consider. First, the nominal
expressions in the corpus may represent just
one way to describe the entity at that point
in the dialogue, so that using human perfor-
mance as a standard against which to evalu-
ate the learned nominal-expression generators
provides an overly rigorous test (Oberlander,
1998). Furthermore, we do not know whether
humans would produce identical nominal ex-
pressions given the same discourse situation.
A previous study of anaphor generation in
Chinese showed that rates of match for human
speakers averaged 74% for that problem (Yeh
and Mellish, 1997), and our results show that
including speaker-specific features improves
performance significantly. Our conclusion is
that it may be important to quantify the best
performance that a human could achieve at
matching the nominal expressions in the cor-
pus, given the complete discourse context and
</bodyText>
<table confidence="0.992976766666667">
Say PO if (color=unk) A (owner=SELF) A (speaker-pair=GARRETT-STEVE)
Say OQ if (color=unk) A (quantity&gt;2)
Say OQ if (distance-of-last-state-in-turns=2) A (distance-last-ref-in-turns&gt;31) A (type=CHAIR)
Say COQ if (quantity&gt;2) A (price=unknown)
Say COQ if (quantity&gt;2) A (type-in-last-exp=no)
Say CQ if (speaker-pair=DAVE-GREG) A (distance-last-ref-in-turns&gt;15) A (distance-last-ref&gt;4)
Say C if (prev-commit-speaker=commit) A (utterance-number&gt;43)
Say C if (price-in-last-turn=no) A (utterance-number&lt;21)
Say CO if (price=unknown) A (utterance-number&gt;16)
Say CO if (quantity-in-last-exp=no) A (distance-last-ref-in-turns&lt;18) A) (utterance-number&gt;18) A (price&gt;325)
Say CO if (price-in-last-exp=yes) A (speaker-pair=JILL-PENNY)
Say CO if (priceupperlimit=yes)
Say 0 if (speaker-pair=GARRETT-STEVE) A (speaker-of-last-state=SELF) A (color-contrast=no)
Say 0 if (color=unk) A (owner=0THER) A (price&lt;300)
Say T if (prev-solution-size=DETERMINATE) A (price&gt;250) A (color-contrast=no)
Say T if (color=unk)
Say T if (distance-last-ref-in-turns&gt;10) A (speaker-pair=KATHY-MARK) A (distance-last-ref&lt;2)
Say CP if (utterance-number&lt;5) A (utterance-number&gt;3) A (quantity&lt;l)
Say CP if (distance-of-last-state-in-utterances=&apos;4&apos;) A (problem&gt;2) A (price-contrast=yes)
Say CP if (quantity&lt;-1)
Say CP if (speaker=KRISTI) A (reference-relation=inference)
Say CPOQ if (goal=SELECTCHAIRS) A (prev-solution-size=INDETERMINATE)
Say CPOQ if (speaker-pair=KATHY-MARK) A (prev-solution-size=INDETERMINATE) A) (reference-
relation=initial)
Say CPOQ if (goal=SELECTCHAIRS) A (problem&lt;l)
Say CPO if (goal=SELECTSOFA)
Say CPO if (problem&lt;l) A (utterance-number&gt;7) A (reference-relation=initial) A (price&gt;150)
Say CPO if (speaker-pair=KATHY-MARK) A (quantity&lt;l)
Say CPO if (utterance-number&gt;77)
default Say CPQ
</table>
<figureCaption confidence="0.995704666666667">
Figure 8: The best performing rule set, learned using the combination of the GIVEN-NEW, IN-
TENTIONAL INFLUENCES, INHERENT, and CONCEPTUAL PACT feature sets. The classes encode
the four attributes, e.g., CPOQ = Color,Price,Owner and Quantity, T = Type only.
</figureCaption>
<bodyText confidence="0.999972575757576">
the identity of the referent. In addition, the
difficulty of this problem depends on the num-
ber of attributes available for describing an
object in the domain; our nominal expression
generator has to correctly make four different
decisions to achieve an exact match to human
performance. Finally, the COCONUT corpus is
publicly available, and other researchers can
now attempt to improve on our results.
One of the most surprising results of our
study is the finding that many of the theoret-
ically motivated features previously proposed
in the literature do not improve performance
on our task. However, in previous work, Jor-
dan (Jordan, 2000) utilized the COCONUT cor-
pus to develop a rule-based model of nomi-
nal expression generation for redescriptions,
i.e. the subset of our data where the ref-
erence relation is coref. Jordan also found
that varying how contrast sets are derived
made no significant difference in performance
and that the CONCEPTUAL PACTS model was
significantly worse than the INTENTIONAL IN-
FLUENCES model. In future work, we plan to
perform similar experiments on different cor-
pora with different communications settings
and problem types (e.g. planning, scheduling,
designing) to determine whether our findings
are specific to the genre of dialogues that we
examine here, or whether they are more gen-
eral. We also intend to develop other feature
sets to provide additional approximations to
these models.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868238636364">
James Allen and Mark Core. 1997. Draft of
DAMSL: Dialog act markup in several layers.
Susan E. Brennan and Herbert H. Clark. 1996.
Lexical choice and conceptual pacts in conver-
sation. Journal of Experimental Psychology:
Learning, Memory And Cognition.
Herbert H. Clark and Catherine R. Marshall.
1981. Definite reference and mutual knowledge.
In Joshi, Webber, and Sag, editors, Elements
of Discourse Understanding, pages 10-63. CUP,
Cambridge.
Herbert H. Clark and Deanna Wilkes-Gibbs.
1986. Referring as a collaborative process. Cog-
nition, 22:1-39.
Paul R. Cohen. 1995. Empirical Methods for Ar-
tificial Intelligence. MIT Press, Boston.
William Cohen. 1996. Learning trees and rules
with set-valued features. In Fourteenth Confer-
ence of the American Association of Artificial
Intelligence.
Robert Dale and Ehud Reiter. 1995. Computa-
tional Interpretations of the Gricean Maxims in
the Generation of Referring Expressions. Cog-
nitive Science, 19(2):233-263, Apr-June.
Barbara Di Eugenio, Pamela W. Jordan, Jo-
hanna D. Moore, and Richmond H. Thomason.
1998. An empirical investigation of collabora-
tive dialogues. In ACL-COLING98, Proceed-
ings of the Thirty-sixth Conference of the As-
sociation for Computational Linguistics, Mon-
treal, Canada, August.
Barbara Di Eugenio, Pamela W. Jordan, Rich-
mond H. Thomason, and Johanna D. Moore.
2000. The agreement process: An empiri-
cal investigation of human-human computer-
mediated collaborative dialogues. To Appear
in International Journal of Human-Computer
Studies.
Barbara J. Grosz and Candace L. Sidner. 1986.
Attention, intentions and the structure of dis-
course. Computational Linguistics, 12:175-204.
Julia B. Hirschberg. 1993. Pitch accent in con-
text: predicting intonational prominence from
text. Artificial Intelligence Journal, 63:305-
340.
Pamela W. Jordan. 2000. Intentional Influences
on Object Redescriptions in Dialogue: Evidence
from an Empirical Study. Ph.D. thesis, Intel-
ligent Systems Program, University of Pitts-
burgh.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O&apos;Donnell. 1998. Experiments us-
ing stochastic search for text planning. In Pro-
ceedings of International Conference on Natural
Language Generation, pages 97-108.
Margaret G. Moser and Johanna Moore. 1995.
Investigating cue selection and placement in tu-
torial discourse. In ACL 95, pages 130-137.
Jon Oberlander. 1998. Do the right thing.. .but
expect the unexpected. Computational Lin-
guistics, 24(3):501-508.
Rebecca J. Passonneau. 1995. Integrating
Gricean and Attentional Constraints. In Pro-
ceedings of IJCAI 95.
Massimo Poesio. 2000. Annotating a corpus to
develop and evaluate discourse entity realiza-
tion algorithms: issues and preliminary results.
In Proc. Language Resources and Evaluation
Conference, LREC-2000.
Ellen F. Prince. 1981. Toward a taxonomy of
given-new information. In Radical Pragmatics,
pages 223-255. Academic Press.
Michael Strube and Maria Wolters. 2000. A prob-
abilistic genre-independent model of pronomi-
nalization. In Proceedings of the North Amer-
ican Meeting of the Association for Computa-
tional Linguistics, pages 18-25.
J. M. B. Terken. 1985. Use and Function of Ac-
centuation: Some Experiments. Ph.D. thesis,
Institute for Perception Research, Eindhoven,
The Netherlands.
Marilyn A. Walker. 1996. Limited attention and
discourse structure. Computational Linguis-
tics, 22-2:255-264.
Ching-Long Yeh and Chris Mellish. 1997. An em-
pirical study on the generation of anaphora in
chinese. Computational Linguistics, 23-1:169-
190.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748558">
<title confidence="0.999976">Learning Attribute Selections for Non-Pronominal Expressions</title>
<author confidence="0.999976">Pamela Jordan</author>
<affiliation confidence="0.997495">Intelligent Systems Program University of Pittsburgh</affiliation>
<address confidence="0.983477">Pittsburgh, PA</address>
<email confidence="0.998942">jordan@isp.pitt.edu</email>
<author confidence="0.999844">Marilyn Walker</author>
<affiliation confidence="0.999874">AT&amp;T Labs—Research</affiliation>
<address confidence="0.9989065">180 Park Avenue Florham Park, NJ 07932-0971 USA</address>
<email confidence="0.999911">walker@research.att.com</email>
<abstract confidence="0.989868956521739">A fundamental function of any taskoriented dialogue system is the ability to generate nominal expressions that describe objects in the task domain. In this paper, we report results from using machine learning to train and test a nominalexpression generator on a set of 393 descriptions from the COof task-oriented design dialogues. Results show that we can achieve a 50% match to human performance as opposed to a 16% baseline for just guessing the most frequent type of nominal exin the To our surprise our results indicate that many of the central features of previously proposed selection models did not improve the performance of the learned nominal-expression generator.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Mark Core</author>
</authors>
<title>Draft of DAMSL: Dialog act markup in several layers.</title>
<date>1997</date>
<contexts>
<context position="21758" citStr="Allen and Core, 1997" startWordPosition="3425" endWordPosition="3428">an&apos;s model are in Figure 6. The task situation features encode inferrable changes in the task situation that are related to item attributes. The agreement state features encode critical points of agreement during problem solving. For example, if a dialogue participant is accepting a proposal, she may want to verify that she has the same item and the same entity description as her partner. These are features that (Di Eugenio et al., 2000) found to be indicative of agreement states and include DAMSL features (influenceon-listener, commit-speaker, prey-influenceon-listener, prey-commit-speaker) (Allen and Core, 1997), progress towards a solution (solution-size, prey-solution-size, ref-madein-prey- action-state), and features inherent to an agreement state (speaker-of-last-state, distance-of-last-state-in-utterances, distanceof-last-state-in-turns). The solution interactions features represent situations where multiple proposals are under consideration which may contrast with one another in terms of solving color-matching goals (color-contrast) or price related goals (price-contrast). 2.4 Learning Experiments The final input for learning is training data, i.e., a representation of a set of nominal expressi</context>
</contexts>
<marker>Allen, Core, 1997</marker>
<rawString>James Allen and Mark Core. 1997. Draft of DAMSL: Dialog act markup in several layers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Herbert H Clark</author>
</authors>
<title>Lexical choice and conceptual pacts in conversation.</title>
<date>1996</date>
<journal>Journal of Experimental Psychology: Learning, Memory And Cognition.</journal>
<contexts>
<context position="3645" citStr="Brennan and Clark, 1996" startWordPosition="617" endWordPosition="620"> subsequently as the yellow rug for 150 dollars, the rug for 150 dollars, the yellow rug. It could also have been described by any of the following nonpronominal expressions: the rug, my rug, my yellow rug, my $150 yellow rug, the $150 yellow rug, the $150 rug. The content of these descriptions varies depending on which attributes are included in the description. How does the speaker decide which attributes to include? The problem of content selection for nominal expressions has been the focus of much previous work and a large number of models have been proposed (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996; Dale and Reiter, 1995; Passonneau, 1995; Jordan, 2000) inter alia. The factors that these models utilize include the discourse structure, the attributes used in the last mention, the recency of last mention, the frequency of mention, the task structure, the inferential complexity of the task, and ways of determining salient objects and the salient attributes of an object. In this paper we utilize a set of factors considered as important for three of these models, and empirically compare the utility of these factors as predictors in a machine learning experiment. The factor sets we utilize ar</context>
<context position="16337" citStr="Brennan and Clark, 1996" startWordPosition="2645" endWordPosition="2649">ef, distance-last-ref-in-turns, number-prev-mentions, speaker-of-last-ref • previous description: color-in-last-exp, typein-last-exp, owner-in-last-exp, price-in-last-exp, quantity-in-last-exp, type-in-last-turn, color-inlast-turn, owner-in-last-turn, price-in-last-turn, quantity-in-last-turn, initial-in-last-turn Figure 4: CONCEPTUAL PACT Feature Set. lem difficulty (problem-number) play significant roles in attribute selection. The CONCEPTUAL PACT model suggests that dialogue participants negotiate a description that both find adequate for describing an object (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996). The speaker generates trial descriptions that the hearer modifies based on which object he thinks he is suppose to identify. The negotiation continues until the participants are confident that the hearer has correctly identified the intended object. The additional features suggested by this model include the previous description since that is the description that will be modified, and how long ago the description was made. If the description were made further back in the dialogue, that would indicate that the negotiation process had been completed. Furthermore, the model suggests that, once </context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>Susan E. Brennan and Herbert H. Clark. 1996. Lexical choice and conceptual pacts in conversation. Journal of Experimental Psychology: Learning, Memory And Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Catherine R Marshall</author>
</authors>
<title>Definite reference and mutual knowledge.</title>
<date>1981</date>
<booktitle>Elements of Discourse Understanding,</booktitle>
<pages>10--63</pages>
<editor>In Joshi, Webber, and Sag, editors,</editor>
<publisher>CUP, Cambridge.</publisher>
<contexts>
<context position="14288" citStr="Clark and Marshall, 1981" startWordPosition="2359" endWordPosition="2362">nal description in terms of a set of 58 features that were either directly annotated by humans as described above, derived from annotated features or inherent to the dialogue (Di Eugenio et al., 1998; Jordan, 2000). The dialogue context in which each description occurs is represented in the encodings. • what is mutually known: type-mk, color-mk, owner-mk, price-mk, quantity-mk • reference-relation: one of initial, coref, , inference Figure 2: Given-New Feature Set. The GIVEN-NEW features in Figure 2 encode fundamental attributes of the entity that is to be described by the nominal expression (Clark and Marshall, 1981; Prince, 1981). We encode what is mutually known about the discourse entity at the point at which it is to be described (type-ink, color-ink, owner-ink, price-ink, quantity-ink). We utilize a reference-relation feature to encode whether the entity is new (initial), given (coref) or discourse inferred (inference) relative to the discourse history. The types of inferences supported by the annotation are set, subset, class and common noun anaphora (e.g. one and null anaphora) (Jordan, 2000). The INHERENT FEATURES in Figure 3 are a specific encoding of particulars about the discourse situation, s</context>
</contexts>
<marker>Clark, Marshall, 1981</marker>
<rawString>Herbert H. Clark and Catherine R. Marshall. 1981. Definite reference and mutual knowledge. In Joshi, Webber, and Sag, editors, Elements of Discourse Understanding, pages 10-63. CUP, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<contexts>
<context position="3620" citStr="Clark and Wilkes-Gibbs, 1986" startWordPosition="613" endWordPosition="616">w rug for 150 dollars and then subsequently as the yellow rug for 150 dollars, the rug for 150 dollars, the yellow rug. It could also have been described by any of the following nonpronominal expressions: the rug, my rug, my yellow rug, my $150 yellow rug, the $150 yellow rug, the $150 rug. The content of these descriptions varies depending on which attributes are included in the description. How does the speaker decide which attributes to include? The problem of content selection for nominal expressions has been the focus of much previous work and a large number of models have been proposed (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996; Dale and Reiter, 1995; Passonneau, 1995; Jordan, 2000) inter alia. The factors that these models utilize include the discourse structure, the attributes used in the last mention, the recency of last mention, the frequency of mention, the task structure, the inferential complexity of the task, and ways of determining salient objects and the salient attributes of an object. In this paper we utilize a set of factors considered as important for three of these models, and empirically compare the utility of these factors as predictors in a machine learning experiment. The </context>
<context position="16311" citStr="Clark and Wilkes-Gibbs, 1986" startWordPosition="2641" endWordPosition="2644">urse entities: distance-last-ref, distance-last-ref-in-turns, number-prev-mentions, speaker-of-last-ref • previous description: color-in-last-exp, typein-last-exp, owner-in-last-exp, price-in-last-exp, quantity-in-last-exp, type-in-last-turn, color-inlast-turn, owner-in-last-turn, price-in-last-turn, quantity-in-last-turn, initial-in-last-turn Figure 4: CONCEPTUAL PACT Feature Set. lem difficulty (problem-number) play significant roles in attribute selection. The CONCEPTUAL PACT model suggests that dialogue participants negotiate a description that both find adequate for describing an object (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996). The speaker generates trial descriptions that the hearer modifies based on which object he thinks he is suppose to identify. The negotiation continues until the participants are confident that the hearer has correctly identified the intended object. The additional features suggested by this model include the previous description since that is the description that will be modified, and how long ago the description was made. If the description were made further back in the dialogue, that would indicate that the negotiation process had been completed. Furthermore, the </context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="23638" citStr="Cohen, 1995" startWordPosition="3708" endWordPosition="3709">using 25-fold cross-validation, i.e. the total set of examples is randomly divided into 25 disjoint test sets, and 25 runs of the learning program are performed. Thus, each run uses the examples not in the test set for training and the remaining examples for testing. 3 Experimental Results Table 1 summarizes our experimental results. For each feature set, we report accuracy rates and standard errors resulting from crossvalidation.3 It is clear that performance de3Accuracy rates are statistically significantly different when the accuracies plus or minus twice the standard error do not overlap (Cohen, 1995), p. 134. pends on the features that the learner has available. The 16.3% MAJORITY CLASS BASELINE accuracy rate in the first row is a standard baseline that corresponds to the accuracy one would achieve from simply choosing the description type that occurs most frequently in the corpus, which in this case means that the nominal-expression generator would always use the color, price and quantity to describe a domain entity. Feature Sets Used Accuracy (SE) MAJORITY CLASS BASELINE 16.3 % GIVEN-NEW 18.2% (2.3) GIVEN-NEw,sEG 18.5% (2.5) GIVEN-NEw,cP 20.3% (2.5) GIVEN-NEW,IINF 33.1% (2.7) GIVEN-NEW,</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Learning trees and rules with set-valued features.</title>
<date>1996</date>
<booktitle>In Fourteenth Conference of the American Association of Artificial Intelligence.</booktitle>
<contexts>
<context position="7872" citStr="Cohen, 1996" startWordPosition="1288" endWordPosition="1289">ssion generator must correctly choose among 16 possibilities represented by the power set of the four attributes. Section 2 describes the COCONUT corpus, the encoding of the corpus and the features used in machine learning in more detail. Section 3 presents the quantitative results of testing the learned rules against the corpus, discusses the features that the machine learner identifies as important, and provides examples of the rules that are learned. Section 4 summarizes our results and discusses future work. 2 Corpus, Data, Methods Our experiments utilize the rule learning program RIPPER (Cohen, 1996) to learn a nominal-expression generator from the nominal expressions in the COCONUT corpus. Although we had several learners available to us, we chose RIPPER primarily because the ifthen rules that are used to express the learned nominal generator model are easy for people to understand and thus facilitate comparison with the theoretical models we are trying to evaluate. Like other learning programs, RIPPER takes as input the names of a set of classes to be learned, the names and ranges of values of a fixed set of features, and training data specifying the class and feature values for each ex</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>William Cohen. 1996. Learning trees and rules with set-valued features. In Fourteenth Conference of the American Association of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<date>1995</date>
<journal>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science,</journal>
<pages>19--2</pages>
<location>Apr-June.</location>
<contexts>
<context position="3668" citStr="Dale and Reiter, 1995" startWordPosition="621" endWordPosition="625">ow rug for 150 dollars, the rug for 150 dollars, the yellow rug. It could also have been described by any of the following nonpronominal expressions: the rug, my rug, my yellow rug, my $150 yellow rug, the $150 yellow rug, the $150 rug. The content of these descriptions varies depending on which attributes are included in the description. How does the speaker decide which attributes to include? The problem of content selection for nominal expressions has been the focus of much previous work and a large number of models have been proposed (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996; Dale and Reiter, 1995; Passonneau, 1995; Jordan, 2000) inter alia. The factors that these models utilize include the discourse structure, the attributes used in the last mention, the recency of last mention, the frequency of mention, the task structure, the inferential complexity of the task, and ways of determining salient objects and the salient attributes of an object. In this paper we utilize a set of factors considered as important for three of these models, and empirically compare the utility of these factors as predictors in a machine learning experiment. The factor sets we utilize are: • CONTRAST SET facto</context>
<context position="18302" citStr="Dale and Reiter, 1995" startWordPosition="2921" endWordPosition="2924">last- exp, owner-in-last- exp, price-in-last-exp, quantity-in-last-exp, typein-last-turn, color-in-last-turn, owner-in-lastturn, price-in-last-turn, quantity-in-last-turn, • ONE UTTERANCE Distractors: type-distractors, color-distractors, owner-distractors, price-distractors, quantity-distractors • SEGMENT Distractors: type-distractors, colordistractors, owner-distractors, price-distractors, quantity-distractors Figure 5: CONTRAST SET Feature Sets The INCREMENTAL MODEL builds a description incrementally by considering the other objects that are currently expected to be in focus for the hearer (Dale and Reiter, 1995). These other objects are called distractors. The basic idea is to add attributes as necessary until any distractors are ruled out as competing co-specifiers. Based on these ideas, we developed a set of features we call CONTRAST SET features, as in Figure 5. The goal of our encoding is to represent whether there are distractors present in the focus space which might motivate the inclusion of a particular attribute.2 2This representatation only approximates the model since the INCREMENTAL model utilizes a preferred salience ordering of attributes and eliminates distractors as attributes are add</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science, 19(2):233-263, Apr-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Johanna D Moore</author>
<author>Richmond H Thomason</author>
</authors>
<title>An empirical investigation of collaborative dialogues.</title>
<date>1998</date>
<booktitle>In ACL-COLING98, Proceedings of the Thirty-sixth Conference of the Association for Computational Linguistics,</booktitle>
<location>Montreal, Canada,</location>
<marker>Di Eugenio, Jordan, Moore, Thomason, 1998</marker>
<rawString>Barbara Di Eugenio, Pamela W. Jordan, Johanna D. Moore, and Richmond H. Thomason. 1998. An empirical investigation of collaborative dialogues. In ACL-COLING98, Proceedings of the Thirty-sixth Conference of the Association for Computational Linguistics, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Richmond H Thomason</author>
<author>Johanna D Moore</author>
</authors>
<title>The agreement process: An empirical investigation of human-human computermediated collaborative dialogues. To Appear in</title>
<date>2000</date>
<journal>International Journal of Human-Computer Studies.</journal>
<marker>Di Eugenio, Jordan, Thomason, Moore, 2000</marker>
<rawString>Barbara Di Eugenio, Pamela W. Jordan, Richmond H. Thomason, and Johanna D. Moore. 2000. The agreement process: An empirical investigation of human-human computermediated collaborative dialogues. To Appear in International Journal of Human-Computer Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--175</pages>
<contexts>
<context position="4751" citStr="Grosz and Sidner (1986)" startWordPosition="798" endWordPosition="801">ly compare the utility of these factors as predictors in a machine learning experiment. The factor sets we utilize are: • CONTRAST SET factors, inspired by the INCREMENTAL MODEL of Dale and Reiter (1995); • CONCEPTUAL PACT factors, inspired by the models of Clark and colleagues (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996); • INTENTIONAL INFLUENCES factors, inspired by the model of Jordan (2000). Dale and Reiter&apos;s INCREMENTAL MODEL focuses on the production of near-minimal descriptions that allow the hearer to reliably distinguish the task object from similar task objects. Following Grosz and Sidner (1986), Dale and Reiter&apos;s algorithm utilizes discourse structure as an important factor in determining which objects the current object must be distinguished from. The model of Clark, Brennan and Wilkes-Gibbs is based on the notion of CONCEPTUAL PACTS, i.e. the conversants attempt to coordinate with one another by establishing a conceptual pact for describing an object. Jordan&apos;s INTENTIONAL INFLUENCES model is based on the assumption that the underlying task-related inferences required to achieve the task goals are an important factor in content selection for non-minimal descriptions. We describe th</context>
<context position="12455" citStr="Grosz and Sidner, 1986" startWordPosition="2060" endWordPosition="2063"> and the size of the solution set for the current constraint equations as well as current variable assignments. The utterance level discourse features encode when an offer is made and the level of a speaker&apos;s commitment to a proposal under consideration, i.e. conditional or unconditional. In order to derive some of the discourse information the task structure must be identified. The COCONUT corpus was encoded via a set of instructions to coders to record all domain goals. Changes to a different domain goal or action were used as a cue to derive the non-linguistic task structure (Terken, 1985; Grosz and Sidner, 1986). Each domain action provides a discourse segment purpose so that each utterance that relates to a different domain action or set of domain actions defines a new segment. The encoded features all have good intercoder reliability (Di Eugenio et al., 1998; Jordan, 2000). Our experimental data is 393 nonpronominal nominal descriptions from 13 dialogues of the COCONUT corpus as well as features constructed from the annotations described above. We explain how we use the annotations to construct the features in more detail below. 2.2 Class Assignment The corpus of nominal expressions is used to cons</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions and the structure of discourse. Computational Linguistics, 12:175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia B Hirschberg</author>
</authors>
<title>Pitch accent in context: predicting intonational prominence from text.</title>
<date>1993</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>63--305</pages>
<contexts>
<context position="27459" citStr="Hirschberg, 1993" startWordPosition="4316" endWordPosition="4317">ws the generation rules learned with the best performing features set shown in the row labelled GIVEN-NEW, IINF ,INH, CP. As mentioned above, many task, entity and speaker specific features are used in these rules. However, this rule set performs at 50% accuracy, as opposed to 33.6% accuracy for our most general feature set (shown in the row labelled GIVEN- NEW IINF CP, SEG) . 4 Discussion and Future Work. While previous research in natural language generation has applied machine learning to accent placement, cue-word selection, text planning, and determining the form of a nominal expression (Hirschberg, 1993; Moser and Moore, 1995; Mellish et al., 1998; Poesio, 2000; Strube and Wolters, 2000), we know of Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS) Say PO if (pricelimit=yes) Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA) Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distanceof-last-st at e-in-utt erances &lt; 4) Say CO if (colorlimit=yes) Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A (solution-size=INDETERMINATE) Say CO if (distan</context>
</contexts>
<marker>Hirschberg, 1993</marker>
<rawString>Julia B. Hirschberg. 1993. Pitch accent in context: predicting intonational prominence from text. Artificial Intelligence Journal, 63:305-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
</authors>
<title>Intentional Influences on Object Redescriptions in Dialogue: Evidence from an Empirical Study.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="3701" citStr="Jordan, 2000" startWordPosition="628" endWordPosition="629">ollars, the yellow rug. It could also have been described by any of the following nonpronominal expressions: the rug, my rug, my yellow rug, my $150 yellow rug, the $150 yellow rug, the $150 rug. The content of these descriptions varies depending on which attributes are included in the description. How does the speaker decide which attributes to include? The problem of content selection for nominal expressions has been the focus of much previous work and a large number of models have been proposed (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996; Dale and Reiter, 1995; Passonneau, 1995; Jordan, 2000) inter alia. The factors that these models utilize include the discourse structure, the attributes used in the last mention, the recency of last mention, the frequency of mention, the task structure, the inferential complexity of the task, and ways of determining salient objects and the salient attributes of an object. In this paper we utilize a set of factors considered as important for three of these models, and empirically compare the utility of these factors as predictors in a machine learning experiment. The factor sets we utilize are: • CONTRAST SET factors, inspired by the INCREMENTAL M</context>
<context position="12723" citStr="Jordan, 2000" startWordPosition="2106" endWordPosition="2107">onditional. In order to derive some of the discourse information the task structure must be identified. The COCONUT corpus was encoded via a set of instructions to coders to record all domain goals. Changes to a different domain goal or action were used as a cue to derive the non-linguistic task structure (Terken, 1985; Grosz and Sidner, 1986). Each domain action provides a discourse segment purpose so that each utterance that relates to a different domain action or set of domain actions defines a new segment. The encoded features all have good intercoder reliability (Di Eugenio et al., 1998; Jordan, 2000). Our experimental data is 393 nonpronominal nominal descriptions from 13 dialogues of the COCONUT corpus as well as features constructed from the annotations described above. We explain how we use the annotations to construct the features in more detail below. 2.2 Class Assignment The corpus of nominal expressions is used to construct the machine learning classes as follows. We are trying to learn which subset of the four attributes, color, price, owner, quantity, should be included in a nominal expression. We encode each nominal expression in the corpus as a member of the category represente</context>
<context position="14781" citStr="Jordan, 2000" startWordPosition="2439" endWordPosition="2440"> encode fundamental attributes of the entity that is to be described by the nominal expression (Clark and Marshall, 1981; Prince, 1981). We encode what is mutually known about the discourse entity at the point at which it is to be described (type-ink, color-ink, owner-ink, price-ink, quantity-ink). We utilize a reference-relation feature to encode whether the entity is new (initial), given (coref) or discourse inferred (inference) relative to the discourse history. The types of inferences supported by the annotation are set, subset, class and common noun anaphora (e.g. one and null anaphora) (Jordan, 2000). The INHERENT FEATURES in Figure 3 are a specific encoding of particulars about the discourse situation, such as the speaker, the task, and the entity&apos;s known attributes (type, color, owner, price, quantity). While we don&apos;t expect this feature set to generalize to other dialogue situations, it allows us to examine whether there are individual differences in attribute selection algorithms (speaker, speakerpair), or whether specifics about the properties of the object, the location within the dialogue (utterance-number), and the prob• utterance-number, speaker-pair, speaker, problem-number • at</context>
<context position="19293" citStr="Jordan, 2000" startWordPosition="3084" endWordPosition="3085">the inclusion of a particular attribute.2 2This representatation only approximates the model since the INCREMENTAL model utilizes a preferred salience ordering of attributes and eliminates distractors as attributes are added to a description. For example, adding the attribute type when the object is a chair, eliminates any distractors that aren&apos;t chairs. Our encoding treats attributes instead of objects as distractors. This view has the advantage that the preferred ordering of attributes could adjust according to the focus space and this interpretation of Dale and Reiter&apos;s model was shown in (Jordan, 2000) to perform similarly to the strict model. However, the feature representation is still impoverished with respect to (Jordan, 2000) since it doesn&apos;t capture what the most salient attribute values for the focus space are. An open issue with deriving the distractors is how to define a focus space (Walker, 1996). We use two focus space definitions, one based on recency, and the other on intentional structure. See Figure 5. For intentional structure we utilize the task goal segmentation encoded in the COCONUT corpus as discussed above (SEGMENT). For recency, we simply consider the entities from th</context>
<context position="20862" citStr="Jordan, 2000" startWordPosition="3292" endWordPosition="3293"> colormatchconstraintpresence, pricelimit, pricelimit-constraintpresence, priceevaluator, priceevaluatorconstraintpresence, colorlimit, colorlimit-constraintpresence, priceupperlimit, priceupperlimit-constraintpresence • agreement state: influence-on-listener, commit-speaker, solution-size, prev-influence-on-listener, prey-commit-speaker, prey-solution-size, distance-of-last-state-in-utterances, dist anceof-last-stat e- in-t urns, ref-made-in-prey- actionst at e, speaker-of-last-state • solution interactions: color-contrast, price-contrast Figure 6: Intentional Influences Feature Set. Jordan (Jordan, 2000) proposed a model to select attributes for nominals called the INTENTIONAL INFLUENCES model. This model posits that the task-related inferences and the agreement process for task negotiation are important factors in selecting attributes. The features used to approximate Jordan&apos;s model are in Figure 6. The task situation features encode inferrable changes in the task situation that are related to item attributes. The agreement state features encode critical points of agreement during problem solving. For example, if a dialogue participant is accepting a proposal, she may want to verify that she</context>
<context position="34187" citStr="Jordan, 2000" startWordPosition="5137" endWordPosition="5138">In addition, the difficulty of this problem depends on the number of attributes available for describing an object in the domain; our nominal expression generator has to correctly make four different decisions to achieve an exact match to human performance. Finally, the COCONUT corpus is publicly available, and other researchers can now attempt to improve on our results. One of the most surprising results of our study is the finding that many of the theoretically motivated features previously proposed in the literature do not improve performance on our task. However, in previous work, Jordan (Jordan, 2000) utilized the COCONUT corpus to develop a rule-based model of nominal expression generation for redescriptions, i.e. the subset of our data where the reference relation is coref. Jordan also found that varying how contrast sets are derived made no significant difference in performance and that the CONCEPTUAL PACTS model was significantly worse than the INTENTIONAL INFLUENCES model. In future work, we plan to perform similar experiments on different corpora with different communications settings and problem types (e.g. planning, scheduling, designing) to determine whether our findings are speci</context>
</contexts>
<marker>Jordan, 2000</marker>
<rawString>Pamela W. Jordan. 2000. Intentional Influences on Object Redescriptions in Dialogue: Evidence from an Empirical Study. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O&apos;Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Natural Language Generation,</booktitle>
<pages>97--108</pages>
<contexts>
<context position="27504" citStr="Mellish et al., 1998" startWordPosition="4322" endWordPosition="4325">best performing features set shown in the row labelled GIVEN-NEW, IINF ,INH, CP. As mentioned above, many task, entity and speaker specific features are used in these rules. However, this rule set performs at 50% accuracy, as opposed to 33.6% accuracy for our most general feature set (shown in the row labelled GIVEN- NEW IINF CP, SEG) . 4 Discussion and Future Work. While previous research in natural language generation has applied machine learning to accent placement, cue-word selection, text planning, and determining the form of a nominal expression (Hirschberg, 1993; Moser and Moore, 1995; Mellish et al., 1998; Poesio, 2000; Strube and Wolters, 2000), we know of Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS) Say PO if (pricelimit=yes) Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA) Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distanceof-last-st at e-in-utt erances &lt; 4) Say CO if (colorlimit=yes) Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A (solution-size=INDETERMINATE) Say CO if (distance-of-last-state-in-utterances&gt;9) A (distance</context>
</contexts>
<marker>Mellish, Knott, Oberlander, O&apos;Donnell, 1998</marker>
<rawString>Chris Mellish, Alistair Knott, Jon Oberlander, and Mick O&apos;Donnell. 1998. Experiments using stochastic search for text planning. In Proceedings of International Conference on Natural Language Generation, pages 97-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret G Moser</author>
<author>Johanna Moore</author>
</authors>
<title>Investigating cue selection and placement in tutorial discourse.</title>
<date>1995</date>
<booktitle>In ACL 95,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="27482" citStr="Moser and Moore, 1995" startWordPosition="4318" endWordPosition="4321">rules learned with the best performing features set shown in the row labelled GIVEN-NEW, IINF ,INH, CP. As mentioned above, many task, entity and speaker specific features are used in these rules. However, this rule set performs at 50% accuracy, as opposed to 33.6% accuracy for our most general feature set (shown in the row labelled GIVEN- NEW IINF CP, SEG) . 4 Discussion and Future Work. While previous research in natural language generation has applied machine learning to accent placement, cue-word selection, text planning, and determining the form of a nominal expression (Hirschberg, 1993; Moser and Moore, 1995; Mellish et al., 1998; Poesio, 2000; Strube and Wolters, 2000), we know of Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS) Say PO if (pricelimit=yes) Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA) Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distanceof-last-st at e-in-utt erances &lt; 4) Say CO if (colorlimit=yes) Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A (solution-size=INDETERMINATE) Say CO if (distance-of-last-state-in-utt</context>
</contexts>
<marker>Moser, Moore, 1995</marker>
<rawString>Margaret G. Moser and Johanna Moore. 1995. Investigating cue selection and placement in tutorial discourse. In ACL 95, pages 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
</authors>
<title>Do the right thing.. .but expect the unexpected.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--3</pages>
<contexts>
<context position="7099" citStr="Oberlander, 1998" startWordPosition="1161" endWordPosition="1162">ew, and dialogue specific and INTENTIONAL INFLUENCES features can achieve 50% accuracy at matching human performance, a significant improvement over the majority class baseline of 16% in which the generator simply guesses the most frequent property combination. In addition, to our surprise, the results indicate that the CONCEPTUAL PACT features and the CONTRAST SET features make no significant contribution to performance. &apos;While this approach is controversial, we believe that human performance is currently the only reasonable standard against which we can evaluate natural language generators (Oberlander, 1998). Note that the more attributes a discourse entity has, the harder it is to achieve an exact match to a human description, i.e. for our problem the nominal-expression generator must correctly choose among 16 possibilities represented by the power set of the four attributes. Section 2 describes the COCONUT corpus, the encoding of the corpus and the features used in machine learning in more detail. Section 3 presents the quantitative results of testing the learned rules against the corpus, discusses the features that the machine learner identifies as important, and provides examples of the rules</context>
<context position="30945" citStr="Oberlander, 1998" startWordPosition="4717" endWordPosition="4718"> entity also provide significant performance improvements; • Surprisingly, the use of given-new, contrast set and conceptual pact features do not improve performance. We might have expected to achieve a bestperforming accuracy higher than 50% but as this is the first study of this kind, there are several issues to consider. First, the nominal expressions in the corpus may represent just one way to describe the entity at that point in the dialogue, so that using human performance as a standard against which to evaluate the learned nominal-expression generators provides an overly rigorous test (Oberlander, 1998). Furthermore, we do not know whether humans would produce identical nominal expressions given the same discourse situation. A previous study of anaphor generation in Chinese showed that rates of match for human speakers averaged 74% for that problem (Yeh and Mellish, 1997), and our results show that including speaker-specific features improves performance significantly. Our conclusion is that it may be important to quantify the best performance that a human could achieve at matching the nominal expressions in the corpus, given the complete discourse context and Say PO if (color=unk) A (owner=</context>
</contexts>
<marker>Oberlander, 1998</marker>
<rawString>Jon Oberlander. 1998. Do the right thing.. .but expect the unexpected. Computational Linguistics, 24(3):501-508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Integrating Gricean and Attentional Constraints.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI 95.</booktitle>
<contexts>
<context position="3686" citStr="Passonneau, 1995" startWordPosition="626" endWordPosition="627"> the rug for 150 dollars, the yellow rug. It could also have been described by any of the following nonpronominal expressions: the rug, my rug, my yellow rug, my $150 yellow rug, the $150 yellow rug, the $150 rug. The content of these descriptions varies depending on which attributes are included in the description. How does the speaker decide which attributes to include? The problem of content selection for nominal expressions has been the focus of much previous work and a large number of models have been proposed (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996; Dale and Reiter, 1995; Passonneau, 1995; Jordan, 2000) inter alia. The factors that these models utilize include the discourse structure, the attributes used in the last mention, the recency of last mention, the frequency of mention, the task structure, the inferential complexity of the task, and ways of determining salient objects and the salient attributes of an object. In this paper we utilize a set of factors considered as important for three of these models, and empirically compare the utility of these factors as predictors in a machine learning experiment. The factor sets we utilize are: • CONTRAST SET factors, inspired by th</context>
<context position="17158" citStr="Passonneau, 1995" startWordPosition="2777" endWordPosition="2778">r has correctly identified the intended object. The additional features suggested by this model include the previous description since that is the description that will be modified, and how long ago the description was made. If the description were made further back in the dialogue, that would indicate that the negotiation process had been completed. Furthermore, the model suggests that, once a pact has been reached, that the dialogue participants will continue to use the description that they previously negotiated. This aspect of the model is also similar to Passonneau&apos;s LEXICAL FOCUS model (Passonneau, 1995). The CONCEPTUAL PACT features in Figure 4 encode how the current description relates to previous descriptions of the same entity. We encode when the entity was last described in terms of number of utterances and turns (distance-last-ref, distance-last-in-turns), how frequently it was described (number-prey-mentions), who last described it (speaker-of-last-ref), and how it was last described in terms of turn and expression since the description may have been broken into several utterances (color-inlast- exp, type-in-last- exp, owner-in-last- exp, price-in-last-exp, quantity-in-last-exp, typein</context>
</contexts>
<marker>Passonneau, 1995</marker>
<rawString>Rebecca J. Passonneau. 1995. Integrating Gricean and Attentional Constraints. In Proceedings of IJCAI 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Annotating a corpus to develop and evaluate discourse entity realization algorithms: issues and preliminary results.</title>
<date>2000</date>
<booktitle>In Proc. Language Resources and Evaluation Conference, LREC-2000.</booktitle>
<contexts>
<context position="27518" citStr="Poesio, 2000" startWordPosition="4326" endWordPosition="4327">es set shown in the row labelled GIVEN-NEW, IINF ,INH, CP. As mentioned above, many task, entity and speaker specific features are used in these rules. However, this rule set performs at 50% accuracy, as opposed to 33.6% accuracy for our most general feature set (shown in the row labelled GIVEN- NEW IINF CP, SEG) . 4 Discussion and Future Work. While previous research in natural language generation has applied machine learning to accent placement, cue-word selection, text planning, and determining the form of a nominal expression (Hirschberg, 1993; Moser and Moore, 1995; Mellish et al., 1998; Poesio, 2000; Strube and Wolters, 2000), we know of Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS) Say PO if (pricelimit=yes) Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA) Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distanceof-last-st at e-in-utt erances &lt; 4) Say CO if (colorlimit=yes) Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A (solution-size=INDETERMINATE) Say CO if (distance-of-last-state-in-utterances&gt;9) A (distance-of-last-state</context>
</contexts>
<marker>Poesio, 2000</marker>
<rawString>Massimo Poesio. 2000. Annotating a corpus to develop and evaluate discourse entity realization algorithms: issues and preliminary results. In Proc. Language Resources and Evaluation Conference, LREC-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen F Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<booktitle>In Radical Pragmatics,</booktitle>
<pages>223--255</pages>
<publisher>Academic Press.</publisher>
<contexts>
<context position="14303" citStr="Prince, 1981" startWordPosition="2363" endWordPosition="2364">f a set of 58 features that were either directly annotated by humans as described above, derived from annotated features or inherent to the dialogue (Di Eugenio et al., 1998; Jordan, 2000). The dialogue context in which each description occurs is represented in the encodings. • what is mutually known: type-mk, color-mk, owner-mk, price-mk, quantity-mk • reference-relation: one of initial, coref, , inference Figure 2: Given-New Feature Set. The GIVEN-NEW features in Figure 2 encode fundamental attributes of the entity that is to be described by the nominal expression (Clark and Marshall, 1981; Prince, 1981). We encode what is mutually known about the discourse entity at the point at which it is to be described (type-ink, color-ink, owner-ink, price-ink, quantity-ink). We utilize a reference-relation feature to encode whether the entity is new (initial), given (coref) or discourse inferred (inference) relative to the discourse history. The types of inferences supported by the annotation are set, subset, class and common noun anaphora (e.g. one and null anaphora) (Jordan, 2000). The INHERENT FEATURES in Figure 3 are a specific encoding of particulars about the discourse situation, such as the spea</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Ellen F. Prince. 1981. Toward a taxonomy of given-new information. In Radical Pragmatics, pages 223-255. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Maria Wolters</author>
</authors>
<title>A probabilistic genre-independent model of pronominalization.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Meeting of the Association for Computational Linguistics,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="27545" citStr="Strube and Wolters, 2000" startWordPosition="4328" endWordPosition="4331">n the row labelled GIVEN-NEW, IINF ,INH, CP. As mentioned above, many task, entity and speaker specific features are used in these rules. However, this rule set performs at 50% accuracy, as opposed to 33.6% accuracy for our most general feature set (shown in the row labelled GIVEN- NEW IINF CP, SEG) . 4 Discussion and Future Work. While previous research in natural language generation has applied machine learning to accent placement, cue-word selection, text planning, and determining the form of a nominal expression (Hirschberg, 1993; Moser and Moore, 1995; Mellish et al., 1998; Poesio, 2000; Strube and Wolters, 2000), we know of Say POQ if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTCHAIRS) Say PO if (pricelimit=yes) Say PO if (priceupperlimit-constraintpresence=IMPLICIT) A (goal=SELECTSOFA) Say CQ if (color-mk=yes) A (influence-on-listener=na) A (distance-of-last-state-in-utterances&gt;4) A (distanceof-last-st at e-in-utt erances &lt; 4) Say CO if (colorlimit=yes) Say CO if (price-mk=yes) A (prev-solution-size=INDETERMINATE) A (ref-made-in-prev-action-state=no) A (solution-size=INDETERMINATE) Say CO if (distance-of-last-state-in-utterances&gt;9) A (distance-of-last-state-in-turns&lt;l) Say 0 if (infl</context>
</contexts>
<marker>Strube, Wolters, 2000</marker>
<rawString>Michael Strube and Maria Wolters. 2000. A probabilistic genre-independent model of pronominalization. In Proceedings of the North American Meeting of the Association for Computational Linguistics, pages 18-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M B Terken</author>
</authors>
<title>Use and Function of Accentuation: Some Experiments.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>Institute for Perception Research,</institution>
<location>Eindhoven, The Netherlands.</location>
<contexts>
<context position="12430" citStr="Terken, 1985" startWordPosition="2058" endWordPosition="2059">traint changes and the size of the solution set for the current constraint equations as well as current variable assignments. The utterance level discourse features encode when an offer is made and the level of a speaker&apos;s commitment to a proposal under consideration, i.e. conditional or unconditional. In order to derive some of the discourse information the task structure must be identified. The COCONUT corpus was encoded via a set of instructions to coders to record all domain goals. Changes to a different domain goal or action were used as a cue to derive the non-linguistic task structure (Terken, 1985; Grosz and Sidner, 1986). Each domain action provides a discourse segment purpose so that each utterance that relates to a different domain action or set of domain actions defines a new segment. The encoded features all have good intercoder reliability (Di Eugenio et al., 1998; Jordan, 2000). Our experimental data is 393 nonpronominal nominal descriptions from 13 dialogues of the COCONUT corpus as well as features constructed from the annotations described above. We explain how we use the annotations to construct the features in more detail below. 2.2 Class Assignment The corpus of nominal ex</context>
</contexts>
<marker>Terken, 1985</marker>
<rawString>J. M. B. Terken. 1985. Use and Function of Accentuation: Some Experiments. Ph.D. thesis, Institute for Perception Research, Eindhoven, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Limited attention and discourse structure.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="19603" citStr="Walker, 1996" startWordPosition="3136" endWordPosition="3137">eliminates any distractors that aren&apos;t chairs. Our encoding treats attributes instead of objects as distractors. This view has the advantage that the preferred ordering of attributes could adjust according to the focus space and this interpretation of Dale and Reiter&apos;s model was shown in (Jordan, 2000) to perform similarly to the strict model. However, the feature representation is still impoverished with respect to (Jordan, 2000) since it doesn&apos;t capture what the most salient attribute values for the focus space are. An open issue with deriving the distractors is how to define a focus space (Walker, 1996). We use two focus space definitions, one based on recency, and the other on intentional structure. See Figure 5. For intentional structure we utilize the task goal segmentation encoded in the COCONUT corpus as discussed above (SEGMENT). For recency, we simply consider the entities from the previous utterance as possible distractors (ONE UTTERANCE). For each focus space definition, we encode whether the attribute value of the item to be described is the same as at least one other item in the focus space (type- distractors , color- distractors , owner-distractors, price-distractors, quantitydis</context>
</contexts>
<marker>Walker, 1996</marker>
<rawString>Marilyn A. Walker. 1996. Limited attention and discourse structure. Computational Linguistics, 22-2:255-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Chris Mellish</author>
</authors>
<title>An empirical study on the generation of anaphora in chinese.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="31219" citStr="Yeh and Mellish, 1997" startWordPosition="4758" endWordPosition="4761">tudy of this kind, there are several issues to consider. First, the nominal expressions in the corpus may represent just one way to describe the entity at that point in the dialogue, so that using human performance as a standard against which to evaluate the learned nominal-expression generators provides an overly rigorous test (Oberlander, 1998). Furthermore, we do not know whether humans would produce identical nominal expressions given the same discourse situation. A previous study of anaphor generation in Chinese showed that rates of match for human speakers averaged 74% for that problem (Yeh and Mellish, 1997), and our results show that including speaker-specific features improves performance significantly. Our conclusion is that it may be important to quantify the best performance that a human could achieve at matching the nominal expressions in the corpus, given the complete discourse context and Say PO if (color=unk) A (owner=SELF) A (speaker-pair=GARRETT-STEVE) Say OQ if (color=unk) A (quantity&gt;2) Say OQ if (distance-of-last-state-in-turns=2) A (distance-last-ref-in-turns&gt;31) A (type=CHAIR) Say COQ if (quantity&gt;2) A (price=unknown) Say COQ if (quantity&gt;2) A (type-in-last-exp=no) Say CQ if (spea</context>
</contexts>
<marker>Yeh, Mellish, 1997</marker>
<rawString>Ching-Long Yeh and Chris Mellish. 1997. An empirical study on the generation of anaphora in chinese. Computational Linguistics, 23-1:169-190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>