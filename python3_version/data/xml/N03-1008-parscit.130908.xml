<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004918">
<note confidence="0.950487333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 56-63
Edmonton, May-June 2003
</note>
<bodyText confidence="0.999932510638298">
By performing singular value decomposition of this
matrix, a short vector representation is derived for
each word and document. One advantage of the re-
sulting word and document representations is that
they all live in the same low-dimensional continu-
ous vector space, enabling one to quantitatively mea-
sure closeness or similarity between words and docu-
ments. The cosine of the angle between two vectors
is a standard measure of similarity in this framework.
For language modeling, a pseudo-document is con-
structed from (possibly all) the words preceding a
particular position in an utterance and the result-
ing vector is projected into the abovementioned low-
dimensional vector space, sometimes referred to as
the LSA-space. Intuition suggests that words with
vectors close to the pseudo-document vector are more
likely to follow than those far away from it. This is
used to construct a conditional probability on the
task-vocabulary. This probability, which depends on
a long span of &amp;quot;history&amp;quot; is then suitably combined
with an N-gram probability.
An alternative to first constructing a conditional
probability on the task-vocabulary independently of
the N-gram model and then seeking ways to com-
bine the two probabilities, is directly modeling the
pseudo-document as yet another conditioning event
— on par with the preceding N-1 words — and find-
ing a single probability distribution conditioned on
the entire &amp;quot;history.&amp;quot; Note that the co-occurrence
of the predicted word with, say, the immediately
preceding word in the history is a discrete event
and amenable to simple counting. By contrast, the
pseudo-document is a continuous-valued vector and
simply counting how often a word follows a partic-
ular vector in a training corpus is meaningless; we
must employ a parametric model for word-history
co-occurrence, possibly together with discretization
of the pseudo-document vector.
The remainder of the paper explores these main
themes as follows. For completeness, we briefly de-
scribe in Section 2 the standard LSA language mod-
eling techniques we implemented. We then describe
the maximum entropy alternative for combining N-
gram and latent semantic information in Section 3.
We present experimental results on the Switchboard
corpus of conversational speech in Section 4 and con-
clude in Section 5.
</bodyText>
<sectionHeader confidence="0.629912" genericHeader="method">
2 LSA-Based Language Models
</sectionHeader>
<bodyText confidence="0.999706">
LSA requires a corpus separated into semantically
coherent documents, and a vocabulary to cover
words found in these documents. It is assumed that
the co-occurrence of any two words within a docu-
ment at a rate much greater than chance is an indi-
cation of their semantic similarity. This similarity is
then used for language modeling, as explained below.
The notation and exposition in this section closely
follows that of Bellegarda (2000).
</bodyText>
<subsectionHeader confidence="0.992618">
2.1 Word-Document Frequency Matrix W
</subsectionHeader>
<bodyText confidence="0.9986840625">
The first step in LSA is to represent co-occurrence
information by a large spare matrix. Let V,IVI= M,
be the underlying task vocabulary, and T a text cor-
pus, with document boundaries marked, comprising
N documents relevant to some domain of interest.
Typically, M and N are of the order of 104 and 105,
respectively. T, the language model training corpus,
may thus have hundreds of millions of words. Un-
like N-gram models, the construction of the M x N
matrix W of co-occurrences between words and doc-
uments ignores word order within the document; it is
accumulated from T by simply counting how many
times a word appears in a document.
In constructing the word-document co-occurrence
matrix W, the raw count cu of a word tU, E V in a
document c/3 E T is weighted by
</bodyText>
<listItem confidence="0.987584">
• the &amp;quot;relevance&amp;quot; of a word in the vocabulary to
the topic of a document, function words being
given less weight than content words, and
• the size of the document, a word with a given
count in a longer document being given less
weight than in a shorter one.
</listItem>
<bodyText confidence="0.999858714285714">
To accomplish the former, pretend that a unique (un-
known) document in our collection T is relevant for
some task and our goal is to guess which one it is.
Let the a priori probability of a document being rel-
evant be uniform (k) on the collection and, further,
let an oracle draw a single word at random from the
relevant document and reveals it to us. The condi-
tional probability of c/3 being the relevant document,
given that the relevant document contains the word
w.„ is clearlyc=c3, where c = Eljv cu. The ratio of
the average conditional entropy of the relevant doc-
ument&apos;s identity, given w.„ and its a priori entropy
is thus a measure of the (un)informativeness of w,.
Highly informative words w, have small values of
</bodyText>
<equation confidence="0.939704">
1 C,3 C,3
==log — .
log N 3=1 c,
</equation>
<bodyText confidence="0.9697372">
Since 0 &lt; E. &lt; 1, the raw counts in the i-th row of
W are weighted by (1 — e„).
To achieve the latter effect, the counts in the j-
th column of W are weighted by the total length
c3 = E,=1 cu of the document c/3. In summary,
</bodyText>
<equation confidence="0.999545">
[W] = (1 - €i) C3 (2)
(1)
</equation>
<bodyText confidence="0.760083">
is the resulting ij-th matrix entry.
</bodyText>
<subsectionHeader confidence="0.983304">
2.2 Singular Value Decomposition of W
</subsectionHeader>
<bodyText confidence="0.99989">
Each column of the matrix W represents a document
and each row represents a word. Typically, W is very
sparse. To obtain a compact representation, singular
value decomposition (SVD) is employed (cf. Berry
et al (1993)) to yield
</bodyText>
<equation confidence="0.992481">
W (3)
</equation>
<bodyText confidence="0.999944538461539">
where, for some order R &lt; min(M,N) of the de-
composition, U is a MxR left singular matrix with
rows u,, i = 1, ,M, S is a RxR diagonal ma-
trix of singular values si &gt; s2 &gt; &gt; sR &gt;&gt; 0,
and V is a NxR right singular matrix with rows vj,
j = 1,...,N. For each i, the scaled R-vector u,S
may be viewed as representing wi, the i-th word in
the vocabulary, and similarly the scaled R-vector vjS
as representing dj, the j-th document in the corpus.
Note that the u,S&apos;s and vjS&apos;s both belong to RR,
the so called LSA-space.
The following similarity measure between the i-th
and i&apos;-th words w, and we is frequently used:
</bodyText>
<equation confidence="0.9835785">
K(wi,we) ItiS • u.S
Iluisll x sll
uis2uT
HuSH x ilueSil •
</equation>
<bodyText confidence="0.999298333333333">
Note that K(w„ we) is nothing but the cosine of the
angle between the vectors u,S and ue S. Algorithms
such as K-means clustering have been applied to the
vocabulary using (4) as a measure of similarity.
Replacing the u„&apos;s with vj&apos;s in the definition above,
a corresponding measure K(d3,d3.)
</bodyText>
<subsectionHeader confidence="0.888928">
2.3 Calculating Word-Probabilities via LSA
</subsectionHeader>
<bodyText confidence="0.997512">
Given a sequence wi, w2, , WT of words in a sen-
tence, the semantic coherence between wt, the word
in the t-th position, and dt_1 {wi, • • • , }, all
its predecessors, is used to construct a conditional
probability on the vocabulary. Specifically, for a
word w, in a training document dj, it is true by
virtue of (3) that [W] u,SvT. However, since
</bodyText>
<page confidence="0.989572">
3
</page>
<bodyText confidence="0.991174">
the word-document similarity function
by itself is not a bona fide probability mass function,
a Mx 1 pseudo-document vector dt_1 is constructed
by weighting the frequency of the preceding words in
accordance with (2), and its scaled R-vector repre-
sentation fS = dT 1U is used in (6) to obtain
</bodyText>
<equation confidence="0.99830625">
PLSA (Wt I jt-1) (7)
[K(Wt, Cit-1) Kmin(C-it_l)r
Ew [K(w, cit-1) — Kmin(dt—i)
_ r
</equation>
<bodyText confidence="0.999832625">
where K1in(c1) = min w K(w, d) is an offset to make
the resulting probabilities nonnegative. The coef-
ficient 7 &gt;&gt; 1, as noted by Coccaro and Juraf-
sky (1998), is chosen experimentally to increase the
otherwise small dynamic range of K as w varies over
the vocabulary.
As one processes successive words in a sentence,
the pseudo-document dt_1 is updated incrementally:
</bodyText>
<equation confidence="0.9999395">
at — at-1 ± t ewt
1 — Ew (8)
</equation>
<bodyText confidence="0.9995918">
where ewt is a Mx 1 vector with a 1 in the position
corresponding to tut and 0 elsewhere. Consequently,
the vector &apos;tit_iS needed for the similarity computa-
tion of (6) towards the probability calculation of (7)
is also incrementally updated:
</bodyText>
<equation confidence="0.908219">
vS = At —t 5) + 1 te&amp;quot; Wart (9)
</equation>
<bodyText confidence="0.999992">
where a positive &amp;quot;decay&amp;quot; coefficient A &lt; 1 is thrown
in to accommodate dynamic shifts in topic.
</bodyText>
<subsectionHeader confidence="0.995869">
2.4 Combining /Is A with N-grams
</subsectionHeader>
<bodyText confidence="0.901681">
Several strategies have been proposed (Coccaro and
Jurafsky, 1998; Bellegarda, 2000) for combining the
LSA-based probability (7) with standard N-gram
probabilities, and we list those which we have in-
vestigated for conversational speech.
Linear Interpolation: For some experimentally
determined constants a, and a = 1 — a,
</bodyText>
<equation confidence="0.99846">
P(Wt1Wt-1, Wt-2, dt-1) — (10)
aPL,sA (wt ) + aPN-gram (Wt IWt-1, Wt-2)•
</equation>
<bodyText confidence="0.961214">
Similarity Modulated N-gram: With the simi-
larity (6) offset to be nonnegative, as done in (7),
</bodyText>
<equation confidence="0.984165">
P(Wt1Wt-1, Wt-2, (It -1) — (11)
-K(wt,c-it—i)PN-gram(wtlwt-1,wt-2)
Ew K(w, dt-1)PN-gram (WIWt-1, Wt-2) •
K(dj, ) = HvjSH x • (5)
</equation>
<bodyText confidence="0.999849333333333">
of similarity between the j-th and j&apos;-th documents is
obtained and has been used for document clustering,
filtering and topic detection.
</bodyText>
<equation confidence="0.989348714285714">
V 3• S2VT
3&apos;
(4)
K(w, d) =
x
UtSVT
(6)
</equation>
<bodyText confidence="0.9919815">
Information Weighted Arithmetic Mean: Set-
ting Aw = 1-2€- to account for the informative-
ness of a word w about its document, cf (1), and
Aw = 1 - Aw,
</bodyText>
<equation confidence="0.984630666666667">
P(Wt1Wt-1, Wt-2, dt-1) — (12)
Awt PLSA (Wt Awt PN-gram (Wt IWt-1, Wt-2)
Ew AwPL,sA(wicit-i) + AwPN-gram(WIWt-1, Wt-2)
</equation>
<bodyText confidence="0.996604">
Information Weighted Geometric Mean: With
the same Aw and Aw as above,
</bodyText>
<equation confidence="0.9958312">
P(Wt IWt-1, Wt-2, dt-1) (13)
rts.AS Opt Idt-1) • PN-gram(Wt IWt-1, Wt-2)
\ •
p („„1,4
LSA lu-&apos;1u4-1) N-gram (w IWt-1 Wt-2)
</equation>
<bodyText confidence="0.999975">
We compute language model perplexities for the
Switchboard corpus using each of these methods and
discuss the results in Section 4.1.
</bodyText>
<sectionHeader confidence="0.931243" genericHeader="method">
3 Exponential Models with Latent
Semantic Features
</sectionHeader>
<bodyText confidence="0.999941">
The ad hoc construction of PLsA(wl,cit_i) to some-
how capture K(w, dt_1), and its combination with
N-gram statistics described above are a somewhat
unsatisfactory aspect of the LSA-based models. We
propose, following Khudanpur (2000), an alternative
family of exponential models
</bodyText>
<equation confidence="0.993558571428571">
Pce (W t Wt-2, Wt-1) (14)
fi(wt) f2(wt_i,wt) f3(wt_2,wt_i,wt)
awt a_1 ,Wt aw
t -2 ,Wt-1,Wt
Wt-2, Wt -1)
X afLSA (cit-i,wt)
-
</equation>
<bodyText confidence="0.999858875">
where fi (wt ) f2(wt-i, tut ) and f3(wt-2, tut- wt )
are usually, but not necessarily, {0, 1}-valued indica-
tor functions of N-gram features and awt awt_i,wt
and awwwt are their corresponding feature
weights, and where the semantic coherence between
a word tut and its long-span history dt_1 has been
thrown in as a feature, on par with the standard N-
gram features. E.g., one could have
</bodyText>
<equation confidence="0.623987">
fLSA (Cit - 1 , Wt ) — K (Wt Cit -1 ) • (15)
</equation>
<bodyText confidence="0.999939208333333">
We then find the maximum likelihood estimate of the
model parameters a given the training data. Recall
that the resulting model is also the maximum entropy
(ME) model among models which satisfy constraints
on the marginal probabilities or expected values of
these features (Rosenfeld, 1996).
An important decision that needs to be made in a
model such as (14) is the parameterization a. In
a traditional ME language model, in the absence
of LSA-based features, each N-gram feature func-
tion is a {0, 1}-valued indicator function, and there
is a parameter associated with each feature: an aw
for each unigram constraint, an aw,,w for each bi-
gram constraint, etc. In extending this methodol-
ogy to the LSA features, we note that K(wt,dt-i)
is continuous-valued. That in itself is not a prob-
lem; the ME framework does not require the f (.)&apos;s
to be binary. What is problematic, however, is the
fact that, almost surely, no two pseudo-documents dt
and dt, will ever be identical. Therefore, assigning a
distinct parameter aj,w for each pseudo-document -
word pair (d, w) is counterproductive, and some ty-
ing of parameters for similarly valued d is necessary.
If we tie all the LSA parameters together, i.e., set
</bodyText>
<equation confidence="0.896115">
ad-,w = aLSA Vw E V and E itR , (16)
</equation>
<bodyText confidence="0.972216266666666">
then (14) becomes directly comparable to the sim-
ilarity modulated N-gram model (11), except that
the choice of aLsA here is made jointly with the N-
gram a&apos;s to maximize training data likelihood. If we
let each vocabulary item to have its own a, i.e.
= aLSA,w vJE RR , (17)
then (14) becomes directly comparable to the geo-
metric interpolation method (13), again except that
unlike Aw , the aLsA,w parameters are determined
jointly with the N-gram a&apos;s to maximize a likelihood
criterion.
Since the goal of parameter tying, however, is
to deal with the continuous nature of the pseudo-
document d, another alternative, as suggested by
Khudanpur (2000), is
</bodyText>
<equation confidence="0.873317">
a - =a -
d,w d,w V jE (D(ci) C IRR , (18)
</equation>
<bodyText confidence="0.980530222222222">
where 4.(d) represents a finite partition of IRR in-
dexed by d. We choose to pursue this alternative.
We use a standard K-means clustering of the rep-
resentations vjS of the training documents dj, with
(5) in the role of distance, to obtain a modest number
of clusters. We then pool documents in each cluster
together to form topic-centroids d, and the partition
(DO of RR is defined by the Voronoi regions around
the topic-centroids:
</bodyText>
<equation confidence="0.928454">
(ci) = : K ci) &lt; K(d, d&apos;) V centroids .
</equation>
<bodyText confidence="0.999798909090909">
We also make two approximations to the feature
function of_(15). First, we approximate the pseudo-
document dt_1 in K(.) with its nearest topic-centroid
= ci whenever cit_1 E 4,(d). This is motivated by
the fact that we often deal with very small pseudo-
documents dt_1 in speech recognition, and d provides
a more robust estimate of semantic coherence with tut
than dt_i. Furthermore, keeping in mind the small
dynamic range of the similarity measure of (6), as
well as the interpretation (1) of ett,, we approximate
the feature function of (15) with
</bodyText>
<equation confidence="0.997485666666667">
1 if K (wt, dt-1) &gt; 77
wt) — and ett, T, (19)
0 otherwise.
</equation>
<bodyText confidence="0.999786428571429">
This pragmatic approximation results in a simpli-
fied implementation, particularly for the computa-
tion of feature-expectations during parameter esti-
mation. More importantly, when there is a free
parameter a for each (d, w) pair, as is the case in
(18), AsA(d, w) = 1 and AsA(d, w) = K(w, d) yield
equivalent model families. Therefore, using
</bodyText>
<equation confidence="0.9434825">
instead of alc- (wt&apos; at-1) (20)
d_1 ,Wt
</equation>
<bodyText confidence="0.986078333333333">
in (14) simply amounts to doing feature selection.
For all pairs (d,w) with jisA(d, w) = 1 in (19),
the model-expectation of f is constrained to be the
relative frequency of w within the cluster of training
documents whose centroid is d. By virtue of their
semantic coherence, it is usually higher than the rel-
ative frequency of w in the entire corpus.
Another interesting way of tying the LSA param-
eters, which we have not investigated here, is
</bodyText>
<equation confidence="0.912907">
a- = a- Vw eT(ia),Vde (DO), (21)
</equation>
<bodyText confidence="0.999938222222222">
where T(th) is a finite, possibly d-dependent, parti-
tion of the vocabulary. This parameterization may
be particularly beneficial when, due to a very large
vocabulary or a small training corpus, we do not have
sufficient counts to constrain the model-expectations
of AsA(d, w) for all words w bearing high semantic
similarity with a topic-centroid d. An automatically
derived or knowledge-based semantic classification of
words, e.g. from WordNet, may be used as
</bodyText>
<subsectionHeader confidence="0.993886">
3.1 A Similar ME Model from the Past
</subsectionHeader>
<bodyText confidence="0.98718">
An interesting consequence of (19) is that it makes
the model of (14) identical in form to the model de-
scribed by Khudanpur and Wu (1999). Two signifi-
cant ways in which (14) is novel are that
</bodyText>
<listItem confidence="0.9949923">
• clustering of documents d3 to obtain topic-
centroids ci during training, and assignment of
pseudo-documents dt_1 to topic-centroids dt-1
during recognition, is based on similarity in
LSA-space RR, not document-space RAI, and
• the set of words with active semantic features
(19) for any particular topic-centroid d is deter-
mined by a threshold 97 on LSA similarity, not
by a difference in within-topic v/s corpus-wide
relative frequency.
</listItem>
<bodyText confidence="0.999957166666667">
The former results in some computational savings
both during clustering and on-line topic assignment.
The latter may result in a different choice of topic-
dependent features. We present a comparison of LM
performance between these two ME models in Sec-
tion 4.5 following our main results.
</bodyText>
<sectionHeader confidence="0.987303" genericHeader="method">
4 Switchboard Experiments
</sectionHeader>
<bodyText confidence="0.977649717948718">
We conducted experiments on the Switchboard cor-
pus of conversational telephone speech (Godfrey et
al, 1992), dividing the corpus into a LM training set
of approximately 1500 conversations (2.2M words)
and a test set of 19 conversations (20K words).
The task vocabulary was fixed to 22K words, with
an out-of-vocabulary rate under 0.5% on the test
set. Acoustic models trained on roughly 60 hours
of Switchboard speech and a bigram LM were used
to generate lattices for the test utterances, and a 100-
best list was generated by rescoring the lattice using
a trigram model. All the results in this paper are
based on rescoring this 100-best list with different
language models.
We treated each conversation-side as a separate
document and created W of (2) with M 22, 000
and N 3000. Guided by the fact that one of
70-odd topics was prescribed to a caller when the
Switchboard corpus was collected, we computed the
SVD of (3) with R=73 singular values. We imple-
mented the LSA model of (7) with 7 = 20, and the
four LSA + N-gram combinations of Section 2.4.
To obtain the document clusters and topic-
centroids d required for creating the partition (DO of
(18), we randomly assigned the training documents
to one of 50 clusters and used a K-means algorithm
to iteratively
(i) compute the topic-centroid ci of each cluster by
pooling together all the documents in the clus-
ter, and
(ii) reassigning each document d3 to a cluster to
whose centroid the document bore the greatest
LSA similarity K (d3, d).
Each cluster was required to have a minimum num-
ber of 10 documents in it, and if the number of docu-
ments in a cluster fell below this threshold following
step (ii), then the cluster was eliminated and each
of its documents reassigned to the nearest of the re-
maining centroids. The iteration stopped when no
</bodyText>
<figure confidence="0.9940754">
1 or 0
a d_11Wt
1
0.5
0
Similarity Difference
−0.5
−1
2
1.5
Similarity
1
true pseudo−document
nearest topic−centroid
0.5
0 50 100 150 200 250 300 350
Input Test Word Sequence
0 50 100 150 200 250 300 350
Input Test Word Sequence
0.5
0
−0.5
0 50 100 150 200 250 300 350
Input Test Word Sequence
Similarity Difference
</figure>
<bodyText confidence="0.974126">
on average, and we observe convergence roughly 110
words into the conversation side.
</bodyText>
<subsectionHeader confidence="0.972549">
4.3 Perplexity: ME Model with LSA
Features
</subsectionHeader>
<bodyText confidence="0.9999402">
In the process of comparing our ME model of
(14) with the one described by Khudanpur and
Wu (1999), we noticed that they built a baseline tri-
gram model using the SRI LM toolkit. Other than
this, our experimental setup — training and test set
definitions, vocabulary, etc. — matches theirs exactly.
We report the perplexity of our ME model against
their baseline in Table 2, where the figures in the
first two lines are quoted directly from Khudanpur
and Wu (1999). A single topic-centroid dT selected
</bodyText>
<table confidence="0.9080298">
Language Model Perplexity
SRI Trigram 78.8
ME Trigram 78.9
ME + LSA Features (Closest dT) 73.6
ME + LSA Features (Oracle dT) 73.0
</table>
<tableCaption confidence="0.991604">
Table 2: Perplexities: Maximum Entropy Models
</tableCaption>
<bodyText confidence="0.999899214285714">
for an entire test conversation-side was used in these
experiments. The last line of Table 2 shows the best
perplexity obtainable by any topic-centroid, suggest-
ing that the automatically chosen, Voronoi region
based topic-centroids are quite adequate.
A comparison of Tables 1 and 2 also shows that
the maximum entropy model is more effective in
capturing semantic information than the information
weighted geometric mean of the LSA-based unigram
model and the trigram model. The correspondence
of information weighted geometric mean with the pa-
rameterization of (17) and the corresponding richer
parameterization of (18) are perhaps adequate to ex-
plain this improvement.
</bodyText>
<subsectionHeader confidence="0.999881">
4.4 Word Error Rates for the ME Model
</subsectionHeader>
<bodyText confidence="0.999814230769231">
We rescored the 100-best hypotheses generated by
the baseline trigram model using the ME model with
LSA features. In order to assign a topic-centroid d
to a test utterance in the absence of its correct tran-
scription, we investigated using a concatenation of
the 1-best, 10-best or 100-best first-pass hypotheses
of utterances in the test set, computed d once per test
utterance, and found the performance of the 10-best
hypotheses to yield a slightly lower word error rate
(WER). This is perhaps the optimal trade-off be-
tween robustness in topic assignment resulting from
considering additional word hypotheses, and noise
introduced by considering erroneous words. We also
</bodyText>
<table confidence="0.9982238">
Language Model (dT Assignment) WER
SRI Trigram 38.47%
ME Trigram 38.32%
ME+LSA (per utterance via 10-best) 37.94%
ME+LSA (per cony-side via 10-best) 37.86%
</table>
<tableCaption confidence="0.999915">
Table 3: Error Rates: Maximum Entropy Models
</tableCaption>
<bodyText confidence="0.999926875">
investigated assigning topic for the entire conversa-
tion side based on the first-pass output and found it
to yield a further reduction in WER. We report the
results in Table 3 where the top two lines are, again,
quoted directly from Khudanpur and Wu (1999).
We performed the standard NIST MAPSSWE sta-
tistical significance test (Pallett et al, 1990) and
found that
</bodyText>
<listItem confidence="0.940849857142857">
• the WER improvement of the ME trigram
model over the baseline SRI trigram model is
not significant (p=0.529),
• that of the ME model with LSA features and
utterance-level topic assignment over the ME
trigram model is significant (p=0.008), and
• that of the ME model with LSA features and
</listItem>
<bodyText confidence="0.979273714285714">
conversation-level topic assignment over the ME
trigram model is also significant (p=0.002).
The difference between the WER obtained by
utterance-level v/s conversation-level topic assign-
ment is not significant (p=0.395); nor are other WER
differences (not reported here) between using the 1-
v/s 10- v/s 100-best hypotheses for topic assignment.
</bodyText>
<subsectionHeader confidence="0.9978">
4.5 Benefits of Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.999321761904762">
It was pointed out in Section 3.1 that the model pro-
posed here differs from the model of Khudanpur and
Wu (1999) mainly in the use of the R-dimensional
LSA-space for similarity comparison rather than di-
rect comparison in M-dimensional document-space.
We present in Table 4 a summary comparison of the
two modeling techniques. While, due to the sparse
nature of the vectors, the 22K-dimensional space
does not entail a proportional growth in similarity
computation relative to the 73-dimensional space,
the LSA similarities are still expected to be faster
to compute. Furthermore, the LSA based model
yields comparable perplexity and WER performance
with considerably fewer topic-centroids, resulting in
fewer comparisons during run time for determining
the nearest centroid. Of lesser note is the observation
that the 77-threshold based topic-feature selection of
(19) results in a content word being an active fea-
ture for fewer topics than it does when topic-features
are selected based on differences in within-topic and
overall relative frequencies.
</bodyText>
<table confidence="0.999160333333334">
Attribute Model A Model B
Similarity measure cosine
Document clustering K-means
Vector-space dimension 22K 73
Num. topic-centroids 67 25
Avg. # topics/topic-word 1.8 1.3
Total # topic-parameters 15500 19000
ME + topic perplexity 73.5 73.6
ME + topic WER 37.9%
</table>
<tableCaption confidence="0.972777">
Table 4: A comparison between the model (A) of
Khudanpur and Wu (1999) and our model (B).
</tableCaption>
<sectionHeader confidence="0.930926" genericHeader="evaluation">
5 Summary and Conclusion
</sectionHeader>
<bodyText confidence="0.999969565217391">
We have presented a framework for incorporating
latent semantic information together with standard
N-gram statistics in a unified exponential model for
statistical language modeling. This framework per-
mits varying degrees of parameter tying depending
on the amount of training data available. We have
drawn parallels between some conventional ways of
combining LSA-based models with N-grams and the
parameter-tying decisions in our exponential models,
and our results suggest that incorporating seman-
tic information using maximum entropy principles is
more effective than the ad hoc techniques.
We have presented perplexity and speech recog-
nition accuracy results on the Switchboard corpus
which suggest that LSA-based features, while not as
effective on conversational speech as on newspaper
text, produce modest but statistically significant im-
provements in speech recognition performance.
Finally, we have shown that the maximum entropy
model presented here performs as well as a previously
proposed maximum entropy model for incorporating
topic-dependencies, but it is computationally more
economical.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999871888888889">
We would like to thank Jun Wu of Google Inc. for as-
sistance in the use of his tools for maximum entropy
model estimation and application, and Woosung Kim
of Johns Hopkins University for assistance in the
use of other software. We also thank the anony-
mous referees for comments that helped improve this
manuscript. This research was partially supported
by the National Science Foundation via MLIAM
Grant No ITS 9982329.
</bodyText>
<sectionHeader confidence="0.99206" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987862604166667">
Bellegarda, Jerome. 2000. Exploiting latent seman-
tic information in statistical language modeling.
Proc. IEEE, 88:1279-1296.
Berry, Michael et al. 1993. SVDPACKC (version
1.0) user&apos;s guide. Tech. Report CS-93-194, Uni-
versity of Tennessee, Knoxville, TN.
Chen, Stanley and Roni Rosenfeld. 1998. Topic
adaptation for language modeling using unnormal-
ized exponential models. Proc. ICASSP, pages
681-684, Seattle, WA.
Clarkson, Philip and Anthony Robinson 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. Proc. ICASSP, pages
799-802, Munich, Germany.
Coccaro, Noah and Daniel Jurafsky. 1998. Towards
better integration of semantic predictors in sta-
tistical language modeling. Proc. ICSLP, pages
2403-2406, Sydney, Australia.
Godfrey, John et al. 1992. Switchboard: telephone
speech corpus for research and development. Proc.
ICASSP, pages 517-520, San Francisco, CA.
Gotoh, Yoshihiko and Steve Renals. 1997. Docu-
ment space models using latent semantic analy-
sis. Proc. of Eurospeech, pages 1443-1446, Patras,
Greece.
Iyer, Rukmini and Man Ostendorf. 1999. Model-
ing Long Distance Dependence in Language: Topic
Mixtures vs. Dynamic Cache Models. IEEE Trans
Speech and Audio Processing, 7:30-39.
Khudanpur, Sanjeev and Jun Wu. 1999. A max-
imum entropy language model to integrate n-
grams and topic dependencies for conversational
speech recognition. Proc. ICASSP, pages 553-556,
Phoenix, USA.
Khudanpur, Sanjeev. 2000. Putting language back
into language modeling. Presented at the DARPA-
Lucent Workshop on Spoken Language Recognition
and Understanding, Summit, NJ, Feb 6-9.
Pallett, David et al. 1990. Tools for the analy-
sis of benchmark speech recognition tests. Proc.
ICASSP, 1:97-100, Alburquerque, NM.
Rosenfeld, Roni. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer Speech and Language, 10:187-228.
Wu, Jun. 2002. Maximum entropy language mod-
eling with nonlocal dependencies. PhD Disserta-
tion, Johns Hopkins University CS Department,
Baltimore, MD.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.862707666666667">Proceedings of HLT-NAACL 2003 Main Papers , pp. 56-63 Edmonton, May-June 2003</note>
<abstract confidence="0.976778233695653">By performing singular value decomposition of this matrix, a short vector representation is derived for each word and document. One advantage of the resulting word and document representations is that they all live in the same low-dimensional continuous vector space, enabling one to quantitatively measure closeness or similarity between words and documents. The cosine of the angle between two vectors is a standard measure of similarity in this framework. For language modeling, a pseudo-document is constructed from (possibly all) the words preceding a particular position in an utterance and the resulting vector is projected into the abovementioned lowdimensional vector space, sometimes referred to as the LSA-space. Intuition suggests that words with vectors close to the pseudo-document vector are more likely to follow than those far away from it. This is used to construct a conditional probability on the task-vocabulary. This probability, which depends on a long span of &amp;quot;history&amp;quot; is then suitably combined with an N-gram probability. An alternative to first constructing a conditional probability on the task-vocabulary independently of the N-gram model and then seeking ways to combine the two probabilities, is directly modeling the pseudo-document as yet another conditioning event on par with the preceding — and finding a single probability distribution conditioned on the entire &amp;quot;history.&amp;quot; Note that the co-occurrence of the predicted word with, say, the immediately preceding word in the history is a discrete event and amenable to simple counting. By contrast, the pseudo-document is a continuous-valued vector and simply counting how often a word follows a particular vector in a training corpus is meaningless; we must employ a parametric model for word-history co-occurrence, possibly together with discretization of the pseudo-document vector. The remainder of the paper explores these main themes as follows. For completeness, we briefly describe in Section 2 the standard LSA language modeling techniques we implemented. We then describe maximum entropy alternative for combining Ngram and latent semantic information in Section 3. We present experimental results on the Switchboard corpus of conversational speech in Section 4 and conclude in Section 5. 2 LSA-Based Language Models LSA requires a corpus separated into semantically coherent documents, and a vocabulary to cover words found in these documents. It is assumed that the co-occurrence of any two words within a docuat a rate much greater than chance is an indication of their semantic similarity. This similarity is then used for language modeling, as explained below. The notation and exposition in this section closely follows that of Bellegarda (2000). Word-Document Frequency Matrix The first step in LSA is to represent co-occurrence by a large spare matrix. Let M, be the underlying task vocabulary, and T a text corpus, with document boundaries marked, comprising relevant to some domain of interest. of the order of and respectively. T, the language model training corpus, may thus have hundreds of millions of words. Un- N-gram models, the construction of the x N matrix W of co-occurrences between words and documents ignores word order within the document; it is accumulated from T by simply counting how many times a word appears in a document. In constructing the word-document co-occurrence W, the raw count of a word E a E is weighted by • the &amp;quot;relevance&amp;quot; of a word in the vocabulary to the topic of a document, function words being given less weight than content words, and • the size of the document, a word with a given count in a longer document being given less weight than in a shorter one. To accomplish the former, pretend that a unique (unknown) document in our collection T is relevant for some task and our goal is to guess which one it is. the a of a document being relevant be uniform (k) on the collection and, further, let an oracle draw a single word at random from the relevant document and reveals it to us. The condiprobability of being the relevant document, given that the relevant document contains the word is where c = The ratio of average entropy the relevant docidentity, given w.„ and its a entropy is thus a measure of the (un)informativeness of w,. Highly informative words w, have small values of — . 0 &lt; &lt; the raw counts in the i-th row of W are weighted by (1 — e„). To achieve the latter effect, the counts in the jth column of W are weighted by the total length = of the document summary, [W] = is the resulting ij-th matrix entry. 2.2 Singular Value Decomposition of W Each column of the matrix W represents a document and each row represents a word. Typically, W is very sparse. To obtain a compact representation, singular value decomposition (SVD) is employed (cf. Berry et al (1993)) to yield for some order &lt; of the dea singular matrix with u,, i = 1, S a maof singular values &gt; s2 &gt; &gt; V is a singular matrix with rows each i, the scaled R-vector u,S may be viewed as representing wi, the i-th word in the vocabulary, and similarly the scaled R-vector vjS representing j-th document in the corpus. that the u,S&apos;s and both belong to the so called LSA-space. The following similarity measure between the i-th and i&apos;-th words w, and we is frequently used: ItiS • u.S ilueSil • that is nothing but the cosine of the between the vectors S. such as K-means clustering have been applied to the vocabulary using (4) as a measure of similarity. the u„&apos;s with in the definition above, corresponding measure 2.3 Calculating Word-Probabilities via LSA a sequence wi, w2, , words in a senthe semantic coherence between the word in the t-th position, and dt_1 {wi, • • • , }, its predecessors, is used to construct a conditional probability on the vocabulary. Specifically, for a w, in a training document is true by of (3) that [W] However, since 3 the word-document similarity function itself is not a fide mass function, a Mx 1 pseudo-document vector dt_1 is constructed by weighting the frequency of the preceding words in accordance with (2), and its scaled R-vector representation fS = dT 1U is used in (6) to obtain (Wt Cit-1) — Kmin(dt—i) r K1in(c1) = min wK(w, d) an offset to make the resulting probabilities nonnegative. The coef- &gt;&gt; as noted by Coccaro and Jurafsky (1998), is chosen experimentally to increase the small dynamic range of w varies over the vocabulary. As one processes successive words in a sentence, the pseudo-document dt_1 is updated incrementally: at ± t — is a 1 vector with a 1 in the position corresponding to tut and 0 elsewhere. Consequently, vector needed for the similarity computation of (6) towards the probability calculation of (7) is also incrementally updated: + Wart where a positive &amp;quot;decay&amp;quot; coefficient A &lt; 1 is thrown in to accommodate dynamic shifts in topic. Combining /Is N-grams Several strategies have been proposed (Coccaro and Jurafsky, 1998; Bellegarda, 2000) for combining the LSA-based probability (7) with standard N-gram probabilities, and we list those which we have investigated for conversational speech. Linear Interpolation: For some experimentally constants a, and = 1 Wt-2, dt-1) — aPL,sA (wt ) + aPN-gram IWt-1, Wt-2)• Similarity Modulated N-gram: With the similarity (6) offset to be nonnegative, as done in (7), Wt-2, -1) K(w, (WIWt-1, Wt-2) = • (5) of similarity between the j-th and j&apos;-th documents is obtained and has been used for document clustering, filtering and topic detection.</abstract>
<note confidence="0.855457764705882">3&apos; (4) d) x UtSVT (6) Information Weighted Arithmetic Mean: Set- = to account for the informativeness of a word w about its document, cf (1), and = 1 - P(Wt1Wt-1, Wt-2, dt-1) — PLSA (Wt (Wt IWt-1, AwPL,sA(wicit-i) + Wt-2) Information Weighted Geometric Mean: With same and as above, P(Wt Wt-2, dt-1) Opt Idt-1) • IWt-1, Wt-2</note>
<abstract confidence="0.975308741116751">p („„1,4 N-gram (w IWt-1 Wt-2) We compute language model perplexities for the Switchboard corpus using each of these methods and discuss the results in Section 4.1. 3 Exponential Models with Latent Semantic Features ad hoc construction of to somecapture and its combination with N-gram statistics described above are a somewhat unsatisfactory aspect of the LSA-based models. We propose, following Khudanpur (2000), an alternative family of exponential models (W t Wt-1) fi(wt) f2(wt_i,wt) f3(wt_2,wt_i,wt) ,Wt t -2 ,Wt-1,Wt Wt-2, Wt -1) where fi (wt ) f2(wt-i, tut ) and f3(wt-2, tutwt ) are usually, but not necessarily, {0, 1}-valued indicafunctions of N-gram features and awt_i,wt awwwtare their corresponding feature weights, and where the semantic coherence between word and its long-span history has been in as a feature, on par with the standard Nfeatures. could have 1 , Wt ) — K (Wt ) • We then find the maximum likelihood estimate of the model parameters a given the training data. Recall the resulting model is also the entropy (ME) model among models which satisfy constraints on the marginal probabilities or expected values of these features (Rosenfeld, 1996). An important decision that needs to be made in a model such as (14) is the parameterization a. In a traditional ME language model, in the absence of LSA-based features, each N-gram feature function is a {0, 1}-valued indicator function, and there a parameter associated with each feature: an each unigram constraint, an for each biconstraint, extending this methodolto the LSA features, we note that is continuous-valued. That in itself is not a probthe ME framework does not require the (.)&apos;s to be binary. What is problematic, however, is the that, almost surely, no two pseudo-documents will ever be identical. Therefore, assigning a parameter for each pseudo-document pair w) counterproductive, and some tyof parameters for similarly valued necessary. If we tie all the LSA parameters together, i.e., set = aLSA , then (14) becomes directly comparable to the similarity modulated N-gram model (11), except that choice of aLsA here is made the Ngram a&apos;s to maximize training data likelihood. If we let each vocabulary item to have its own a, i.e. aLSA,w , then (14) becomes directly comparable to the geometric interpolation method (13), again except that , the aLsA,w parameters are determined the N-gram a&apos;s to maximize a likelihood criterion. Since the goal of parameter tying, however, is to deal with the continuous nature of the pseudoalternative, as suggested by Khudanpur (2000), is - =a d,w jE (D(ci) C , 4.(d) represents a finite partition of inby choose to pursue this alternative. We use a standard K-means clustering of the repvjS of the training documents (5) in the role of distance, to obtain a modest number of clusters. We then pool documents in each cluster to form d, the partition of is defined by the regions the topic-centroids: = : K ci) &lt; K(d, d&apos;) centroids We also make two approximations to the feature of_(15). First, we pseudoin its nearest topic-centroid cit_1 is motivated by the fact that we often deal with very small pseudoin speech recognition, and more robust estimate of semantic coherence with Furthermore, keeping in mind the small dynamic range of the similarity measure of (6), as as the interpretation (1) of we the feature function of (15) with if (wt, &gt; 0 otherwise. This pragmatic approximation results in a simplified implementation, particularly for the computation of feature-expectations during parameter estimation. More importantly, when there is a free a for each w) as is the case in AsA(d, w) = 1 and w) = d) equivalent model families. Therefore, using of (20) d_1 ,Wt in (14) simply amounts to doing feature selection. all pairs jisA(d, w) = 1 in (19), model-expectation of constrained to be the relative frequency of w within the cluster of training whose centroid is virtue of their semantic coherence, it is usually higher than the relative frequency of w in the entire corpus. Another interesting way of tying the LSA paramwhich we have here, is a- = a- Vw eT(ia),Vde (DO), (21) where T(th) is a finite, possibly d-dependent, partition of the vocabulary. This parameterization may be particularly beneficial when, due to a very large vocabulary or a small training corpus, we do not have sufficient counts to constrain the model-expectations of AsA(d, w) for all words w bearing high semantic with a topic-centroid automatically derived or knowledge-based semantic classification of words, e.g. from WordNet, may be used as Similar ME Model from the Past An interesting consequence of (19) is that it makes the model of (14) identical in form to the model described by Khudanpur and Wu (1999). Two significant ways in which (14) is novel are that clustering of documents to obtain topictraining, and assignment of pseudo-documents dt_1 to topic-centroids dt-1 during recognition, is based on similarity in not document-space and • the set of words with active semantic features for any particular topic-centroid determined by a threshold 97 on LSA similarity, not by a difference in within-topic v/s corpus-wide relative frequency. The former results in some computational savings both during clustering and on-line topic assignment. The latter may result in a different choice of topicdependent features. We present a comparison of LM performance between these two ME models in Section 4.5 following our main results. 4 Switchboard Experiments We conducted experiments on the Switchboard corpus of conversational telephone speech (Godfrey et al, 1992), dividing the corpus into a LM training set of approximately 1500 conversations (2.2M words) and a test set of 19 conversations (20K words). The task vocabulary was fixed to 22K words, with an out-of-vocabulary rate under 0.5% on the test set. Acoustic models trained on roughly 60 hours of Switchboard speech and a bigram LM were used to generate lattices for the test utterances, and a 100best list was generated by rescoring the lattice using a trigram model. All the results in this paper are based on rescoring this 100-best list with different language models. We treated each conversation-side as a separate and created W of (2) with 000 Guided by the fact that one 70-odd topics was prescribed to a caller when the Switchboard corpus was collected, we computed the SVD of (3) with R=73 singular values. We implethe LSA model of (7) with = and the four LSA + N-gram combinations of Section 2.4. To obtain the document clusters and topicfor creating the partition (DO of (18), we randomly assigned the training documents to one of 50 clusters and used a K-means algorithm to iteratively compute the topic-centroid each cluster by pooling together all the documents in the cluster, and reassigning each document to a cluster to whose centroid the document bore the greatest similarity d). Each cluster was required to have a minimum number of 10 documents in it, and if the number of documents in a cluster fell below this threshold following step (ii), then the cluster was eliminated and each of its documents reassigned to the nearest of the remaining centroids. The iteration stopped when no 1 or 0 1 0.5 0 Similarity Difference −0.5 −1 2 1.5 Similarity 1 true pseudo−document nearest topic−centroid 0.5</abstract>
<phone confidence="0.837347">0 50 100 150 200 250 300 350</phone>
<note confidence="0.876247333333333">Input Test Word Sequence 0 50 100 150 200 250 300 350 Input Test Word Sequence 0.5 0 −0.5</note>
<phone confidence="0.806269">0 50 100 150 200 250 300 350</phone>
<title confidence="0.712419">Input Test Word Sequence</title>
<author confidence="0.60005">Similarity Difference</author>
<abstract confidence="0.992846867346939">average, and we observe convergence roughly words into the conversation side. 4.3 Perplexity: ME Model with LSA Features In the process of comparing our ME model of (14) with the one described by Khudanpur and Wu (1999), we noticed that they built a baseline trigram model using the SRI LM toolkit. Other than this, our experimental setup — training and test set definitions, vocabulary, etc. — matches theirs exactly. We report the perplexity of our ME model against their baseline in Table 2, where the figures in the first two lines are quoted directly from Khudanpur Wu (1999). A single topic-centroid Language Model Perplexity SRI Trigram 78.8 ME Trigram 78.9 ME + LSA Features (Closest dT) 73.6 + LSA Features (Oracle Table 2: Perplexities: Maximum Entropy Models for an entire test conversation-side was used in these experiments. The last line of Table 2 shows the best perplexity obtainable by any topic-centroid, suggesting that the automatically chosen, Voronoi region based topic-centroids are quite adequate. A comparison of Tables 1 and 2 also shows that the maximum entropy model is more effective in capturing semantic information than the information weighted geometric mean of the LSA-based unigram model and the trigram model. The correspondence of information weighted geometric mean with the parameterization of (17) and the corresponding richer parameterization of (18) are perhaps adequate to explain this improvement. 4.4 Word Error Rates for the ME Model We rescored the 100-best hypotheses generated by the baseline trigram model using the ME model with features. In order to assign a topic-centroid to a test utterance in the absence of its correct transcription, we investigated using a concatenation of the 1-best, 10-best or 100-best first-pass hypotheses utterances in the test set, computed per test utterance, and found the performance of the 10-best hypotheses to yield a slightly lower word error rate (WER). This is perhaps the optimal trade-off between robustness in topic assignment resulting from considering additional word hypotheses, and noise introduced by considering erroneous words. We also Language Model (dT Assignment) WER SRI Trigram 38.47% ME Trigram 38.32% ME+LSA (per utterance via 10-best) 37.94% ME+LSA (per cony-side via 10-best) 37.86% Table 3: Error Rates: Maximum Entropy Models investigated assigning topic for the entire conversation side based on the first-pass output and found it to yield a further reduction in WER. We report the results in Table 3 where the top two lines are, again, quoted directly from Khudanpur and Wu (1999). We performed the standard NIST MAPSSWE statistical significance test (Pallett et al, 1990) and found that • the WER improvement of the ME trigram model over the baseline SRI trigram model is significant • that of the ME model with LSA features and utterance-level topic assignment over the ME trigram model is significant (p=0.008), and • that of the ME model with LSA features and conversation-level topic assignment over the ME trigram model is also significant (p=0.002). The difference between the WER obtained by utterance-level v/s conversation-level topic assignment is not significant (p=0.395); nor are other WER differences (not reported here) between using the 1v/s 10v/s 100-best hypotheses for topic assignment. 4.5 Benefits of Dimensionality Reduction It was pointed out in Section 3.1 that the model proposed here differs from the model of Khudanpur and Wu (1999) mainly in the use of the R-dimensional LSA-space for similarity comparison rather than direct comparison in M-dimensional document-space. We present in Table 4 a summary comparison of the two modeling techniques. While, due to the sparse nature of the vectors, the 22K-dimensional space does not entail a proportional growth in similarity computation relative to the 73-dimensional space, the LSA similarities are still expected to be faster to compute. Furthermore, the LSA based model yields comparable perplexity and WER performance with considerably fewer topic-centroids, resulting in fewer comparisons during run time for determining the nearest centroid. Of lesser note is the observation that the 77-threshold based topic-feature selection of (19) results in a content word being an active feature for fewer topics than it does when topic-features are selected based on differences in within-topic and overall relative frequencies.</abstract>
<title confidence="0.516081666666667">Attribute Model A Model B Similarity measure cosine Document clustering K-means</title>
<note confidence="0.813730111111111">Vector-space dimension 22K 73 Num. topic-centroids 67 25 Avg. # topics/topic-word 1.8 1.3 Total # topic-parameters 15500 19000 ME + topic perplexity 73.5 73.6 ME + topic WER 37.9% Table 4: A comparison between the model (A) of Khudanpur and Wu (1999) and our model (B). 5 Summary and Conclusion</note>
<abstract confidence="0.888513348837209">We have presented a framework for incorporating latent semantic information together with standard N-gram statistics in a unified exponential model for statistical language modeling. This framework permits varying degrees of parameter tying depending on the amount of training data available. We have drawn parallels between some conventional ways of combining LSA-based models with N-grams and the parameter-tying decisions in our exponential models, and our results suggest that incorporating semantic information using maximum entropy principles is more effective than the ad hoc techniques. We have presented perplexity and speech recognition accuracy results on the Switchboard corpus which suggest that LSA-based features, while not as effective on conversational speech as on newspaper text, produce modest but statistically significant improvements in speech recognition performance. Finally, we have shown that the maximum entropy model presented here performs as well as a previously proposed maximum entropy model for incorporating topic-dependencies, but it is computationally more economical. 6 Acknowledgments We would like to thank Jun Wu of Google Inc. for assistance in the use of his tools for maximum entropy model estimation and application, and Woosung Kim of Johns Hopkins University for assistance in the use of other software. We also thank the anonymous referees for comments that helped improve this manuscript. This research was partially supported by the National Science Foundation via MLIAM Grant No ITS 9982329. References Bellegarda, Jerome. 2000. Exploiting latent semantic information in statistical language modeling. IEEE, Berry, Michael et al. 1993. SVDPACKC (version user&apos;s guide. Report University of Tennessee, Knoxville, TN. Chen, Stanley and Roni Rosenfeld. 1998. Topic adaptation for language modeling using unnormalexponential models. ICASSP,</abstract>
<address confidence="0.686956">681-684, Seattle, WA. Clarkson, Philip and Anthony Robinson 1997. Lan-</address>
<abstract confidence="0.8702885">guage model adaptation using mixtures and an exdecaying cache. ICASSP,</abstract>
<address confidence="0.9077565">799-802, Munich, Germany. Coccaro, Noah and Daniel Jurafsky. 1998. Towards</address>
<abstract confidence="0.934188">better integration of semantic predictors in stalanguage modeling. ICSLP,</abstract>
<address confidence="0.8842745">2403-2406, Sydney, Australia. Godfrey, John et al. 1992. Switchboard: telephone</address>
<note confidence="0.963017857142857">corpus for research and development. 517-520, San Francisco, CA. Gotoh, Yoshihiko and Steve Renals. 1997. Document space models using latent semantic analyof Eurospeech, 1443-1446, Patras, Greece. Iyer, Rukmini and Man Ostendorf. 1999. Model-</note>
<title confidence="0.859678">ing Long Distance Dependence in Language: Topic</title>
<author confidence="0.908688">Dynamic Cache Models Trans</author>
<affiliation confidence="0.814994">and Audio Processing,</affiliation>
<address confidence="0.652868">Khudanpur, Sanjeev and Jun Wu. 1999. A max-</address>
<abstract confidence="0.5151721875">imum entropy language model to integrate ngrams and topic dependencies for conversational recognition. ICASSP, 553-556, Phoenix, USA. Khudanpur, Sanjeev. 2000. Putting language back language modeling. Presented at the DARPA- Lucent Workshop on Spoken Language Recognition Understanding, NJ, Feb 6-9. Pallett, David et al. 1990. Tools for the analyof benchmark speech recognition tests. NM. Rosenfeld, Roni. 1996. A maximum entropy approach to adaptive statistical language modeling. Speech and Language, Wu, Jun. 2002. Maximum entropy language modwith nonlocal dependencies. Disserta-</abstract>
<affiliation confidence="0.999474">Hopkins University CS Department,</affiliation>
<address confidence="0.989128">Baltimore, MD.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proc. IEEE,</booktitle>
<pages>88--1279</pages>
<contexts>
<context position="2844" citStr="Bellegarda (2000)" startWordPosition="443" endWordPosition="444"> information in Section 3. We present experimental results on the Switchboard corpus of conversational speech in Section 4 and conclude in Section 5. 2 LSA-Based Language Models LSA requires a corpus separated into semantically coherent documents, and a vocabulary to cover words found in these documents. It is assumed that the co-occurrence of any two words within a document at a rate much greater than chance is an indication of their semantic similarity. This similarity is then used for language modeling, as explained below. The notation and exposition in this section closely follows that of Bellegarda (2000). 2.1 Word-Document Frequency Matrix W The first step in LSA is to represent co-occurrence information by a large spare matrix. Let V,IVI= M, be the underlying task vocabulary, and T a text corpus, with document boundaries marked, comprising N documents relevant to some domain of interest. Typically, M and N are of the order of 104 and 105, respectively. T, the language model training corpus, may thus have hundreds of millions of words. Unlike N-gram models, the construction of the M x N matrix W of co-occurrences between words and documents ignores word order within the document; it is accumu</context>
<context position="7833" citStr="Bellegarda, 2000" startWordPosition="1365" endWordPosition="1366">. As one processes successive words in a sentence, the pseudo-document dt_1 is updated incrementally: at — at-1 ± t ewt 1 — Ew (8) where ewt is a Mx 1 vector with a 1 in the position corresponding to tut and 0 elsewhere. Consequently, the vector &apos;tit_iS needed for the similarity computation of (6) towards the probability calculation of (7) is also incrementally updated: vS = At —t 5) + 1 te&amp;quot; Wart (9) where a positive &amp;quot;decay&amp;quot; coefficient A &lt; 1 is thrown in to accommodate dynamic shifts in topic. 2.4 Combining /Is A with N-grams Several strategies have been proposed (Coccaro and Jurafsky, 1998; Bellegarda, 2000) for combining the LSA-based probability (7) with standard N-gram probabilities, and we list those which we have investigated for conversational speech. Linear Interpolation: For some experimentally determined constants a, and a = 1 — a, P(Wt1Wt-1, Wt-2, dt-1) — (10) aPL,sA (wt ) + aPN-gram (Wt IWt-1, Wt-2)• Similarity Modulated N-gram: With the similarity (6) offset to be nonnegative, as done in (7), P(Wt1Wt-1, Wt-2, (It -1) — (11) -K(wt,c-it—i)PN-gram(wtlwt-1,wt-2) Ew K(w, dt-1)PN-gram (WIWt-1, Wt-2) • K(dj, ) = HvjSH x • (5) of similarity between the j-th and j&apos;-th documents is obtained and</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Bellegarda, Jerome. 2000. Exploiting latent semantic information in statistical language modeling. Proc. IEEE, 88:1279-1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Berry</author>
</authors>
<title>SVDPACKC (version 1.0) user&apos;s guide.</title>
<date>1993</date>
<tech>Tech. Report CS-93-194,</tech>
<institution>University of Tennessee,</institution>
<location>Knoxville, TN.</location>
<marker>Berry, 1993</marker>
<rawString>Berry, Michael et al. 1993. SVDPACKC (version 1.0) user&apos;s guide. Tech. Report CS-93-194, University of Tennessee, Knoxville, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Topic adaptation for language modeling using unnormalized exponential models.</title>
<date>1998</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>681--684</pages>
<location>Seattle, WA.</location>
<marker>Chen, Rosenfeld, 1998</marker>
<rawString>Chen, Stanley and Roni Rosenfeld. 1998. Topic adaptation for language modeling using unnormalized exponential models. Proc. ICASSP, pages 681-684, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Anthony Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>799--802</pages>
<location>Munich, Germany.</location>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>Clarkson, Philip and Anthony Robinson 1997. Language model adaptation using mixtures and an exponentially decaying cache. Proc. ICASSP, pages 799-802, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>Proc. ICSLP,</booktitle>
<pages>2403--2406</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7107" citStr="Coccaro and Jurafsky (1998)" startWordPosition="1235" endWordPosition="1239">word w, in a training document dj, it is true by virtue of (3) that [W] u,SvT. However, since 3 the word-document similarity function by itself is not a bona fide probability mass function, a Mx 1 pseudo-document vector dt_1 is constructed by weighting the frequency of the preceding words in accordance with (2), and its scaled R-vector representation fS = dT 1U is used in (6) to obtain PLSA (Wt I jt-1) (7) [K(Wt, Cit-1) Kmin(C-it_l)r Ew [K(w, cit-1) — Kmin(dt—i) _ r where K1in(c1) = min w K(w, d) is an offset to make the resulting probabilities nonnegative. The coefficient 7 &gt;&gt; 1, as noted by Coccaro and Jurafsky (1998), is chosen experimentally to increase the otherwise small dynamic range of K as w varies over the vocabulary. As one processes successive words in a sentence, the pseudo-document dt_1 is updated incrementally: at — at-1 ± t ewt 1 — Ew (8) where ewt is a Mx 1 vector with a 1 in the position corresponding to tut and 0 elsewhere. Consequently, the vector &apos;tit_iS needed for the similarity computation of (6) towards the probability calculation of (7) is also incrementally updated: vS = At —t 5) + 1 te&amp;quot; Wart (9) where a positive &amp;quot;decay&amp;quot; coefficient A &lt; 1 is thrown in to accommodate dynamic shifts i</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Coccaro, Noah and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. Proc. ICSLP, pages 2403-2406, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Godfrey</author>
</authors>
<title>Switchboard: telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<location>San Francisco, CA.</location>
<marker>Godfrey, 1992</marker>
<rawString>Godfrey, John et al. 1992. Switchboard: telephone speech corpus for research and development. Proc. ICASSP, pages 517-520, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshihiko Gotoh</author>
<author>Steve Renals</author>
</authors>
<title>Document space models using latent semantic analysis.</title>
<date>1997</date>
<booktitle>Proc. of Eurospeech,</booktitle>
<pages>1443--1446</pages>
<location>Patras, Greece.</location>
<marker>Gotoh, Renals, 1997</marker>
<rawString>Gotoh, Yoshihiko and Steve Renals. 1997. Document space models using latent semantic analysis. Proc. of Eurospeech, pages 1443-1446, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Man Ostendorf</author>
</authors>
<title>Modeling Long Distance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models.</title>
<date>1999</date>
<booktitle>IEEE Trans Speech and Audio Processing,</booktitle>
<pages>7--30</pages>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>Iyer, Rukmini and Man Ostendorf. 1999. Modeling Long Distance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models. IEEE Trans Speech and Audio Processing, 7:30-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>A maximum entropy language model to integrate ngrams and topic dependencies for conversational speech recognition.</title>
<date>1999</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>553--556</pages>
<location>Phoenix, USA.</location>
<contexts>
<context position="14532" citStr="Khudanpur and Wu (1999)" startWordPosition="2521" endWordPosition="2524">s a finite, possibly d-dependent, partition of the vocabulary. This parameterization may be particularly beneficial when, due to a very large vocabulary or a small training corpus, we do not have sufficient counts to constrain the model-expectations of AsA(d, w) for all words w bearing high semantic similarity with a topic-centroid d. An automatically derived or knowledge-based semantic classification of words, e.g. from WordNet, may be used as 3.1 A Similar ME Model from the Past An interesting consequence of (19) is that it makes the model of (14) identical in form to the model described by Khudanpur and Wu (1999). Two significant ways in which (14) is novel are that • clustering of documents d3 to obtain topiccentroids ci during training, and assignment of pseudo-documents dt_1 to topic-centroids dt-1 during recognition, is based on similarity in LSA-space RR, not document-space RAI, and • the set of words with active semantic features (19) for any particular topic-centroid d is determined by a threshold 97 on LSA similarity, not by a difference in within-topic v/s corpus-wide relative frequency. The former results in some computational savings both during clustering and on-line topic assignment. The </context>
<context position="17685" citStr="Khudanpur and Wu (1999)" startWordPosition="3065" endWordPosition="3068">ssigned to the nearest of the remaining centroids. The iteration stopped when no 1 or 0 a d_11Wt 1 0.5 0 Similarity Difference −0.5 −1 2 1.5 Similarity 1 true pseudo−document nearest topic−centroid 0.5 0 50 100 150 200 250 300 350 Input Test Word Sequence 0 50 100 150 200 250 300 350 Input Test Word Sequence 0.5 0 −0.5 0 50 100 150 200 250 300 350 Input Test Word Sequence Similarity Difference on average, and we observe convergence roughly 110 words into the conversation side. 4.3 Perplexity: ME Model with LSA Features In the process of comparing our ME model of (14) with the one described by Khudanpur and Wu (1999), we noticed that they built a baseline trigram model using the SRI LM toolkit. Other than this, our experimental setup — training and test set definitions, vocabulary, etc. — matches theirs exactly. We report the perplexity of our ME model against their baseline in Table 2, where the figures in the first two lines are quoted directly from Khudanpur and Wu (1999). A single topic-centroid dT selected Language Model Perplexity SRI Trigram 78.8 ME Trigram 78.9 ME + LSA Features (Closest dT) 73.6 ME + LSA Features (Oracle dT) 73.0 Table 2: Perplexities: Maximum Entropy Models for an entire test co</context>
<context position="20104" citStr="Khudanpur and Wu (1999)" startWordPosition="3452" endWordPosition="3455"> trade-off between robustness in topic assignment resulting from considering additional word hypotheses, and noise introduced by considering erroneous words. We also Language Model (dT Assignment) WER SRI Trigram 38.47% ME Trigram 38.32% ME+LSA (per utterance via 10-best) 37.94% ME+LSA (per cony-side via 10-best) 37.86% Table 3: Error Rates: Maximum Entropy Models investigated assigning topic for the entire conversation side based on the first-pass output and found it to yield a further reduction in WER. We report the results in Table 3 where the top two lines are, again, quoted directly from Khudanpur and Wu (1999). We performed the standard NIST MAPSSWE statistical significance test (Pallett et al, 1990) and found that • the WER improvement of the ME trigram model over the baseline SRI trigram model is not significant (p=0.529), • that of the ME model with LSA features and utterance-level topic assignment over the ME trigram model is significant (p=0.008), and • that of the ME model with LSA features and conversation-level topic assignment over the ME trigram model is also significant (p=0.002). The difference between the WER obtained by utterance-level v/s conversation-level topic assignment is not si</context>
<context position="22274" citStr="Khudanpur and Wu (1999)" startWordPosition="3788" endWordPosition="3791">rest centroid. Of lesser note is the observation that the 77-threshold based topic-feature selection of (19) results in a content word being an active feature for fewer topics than it does when topic-features are selected based on differences in within-topic and overall relative frequencies. Attribute Model A Model B Similarity measure cosine Document clustering K-means Vector-space dimension 22K 73 Num. topic-centroids 67 25 Avg. # topics/topic-word 1.8 1.3 Total # topic-parameters 15500 19000 ME + topic perplexity 73.5 73.6 ME + topic WER 37.9% Table 4: A comparison between the model (A) of Khudanpur and Wu (1999) and our model (B). 5 Summary and Conclusion We have presented a framework for incorporating latent semantic information together with standard N-gram statistics in a unified exponential model for statistical language modeling. This framework permits varying degrees of parameter tying depending on the amount of training data available. We have drawn parallels between some conventional ways of combining LSA-based models with N-grams and the parameter-tying decisions in our exponential models, and our results suggest that incorporating semantic information using maximum entropy principles is mor</context>
</contexts>
<marker>Khudanpur, Wu, 1999</marker>
<rawString>Khudanpur, Sanjeev and Jun Wu. 1999. A maximum entropy language model to integrate ngrams and topic dependencies for conversational speech recognition. Proc. ICASSP, pages 553-556, Phoenix, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Putting language back into language modeling. Presented at the DARPALucent Workshop on Spoken Language Recognition and Understanding,</title>
<date>2000</date>
<pages>6--9</pages>
<location>Summit, NJ,</location>
<contexts>
<context position="9421" citStr="Khudanpur (2000)" startWordPosition="1626" endWordPosition="1627">t-2) Information Weighted Geometric Mean: With the same Aw and Aw as above, P(Wt IWt-1, Wt-2, dt-1) (13) rts.AS Opt Idt-1) • PN-gram(Wt IWt-1, Wt-2) \ • p („„1,4 LSA lu-&apos;1u4-1) N-gram (w IWt-1 Wt-2) We compute language model perplexities for the Switchboard corpus using each of these methods and discuss the results in Section 4.1. 3 Exponential Models with Latent Semantic Features The ad hoc construction of PLsA(wl,cit_i) to somehow capture K(w, dt_1), and its combination with N-gram statistics described above are a somewhat unsatisfactory aspect of the LSA-based models. We propose, following Khudanpur (2000), an alternative family of exponential models Pce (W t Wt-2, Wt-1) (14) fi(wt) f2(wt_i,wt) f3(wt_2,wt_i,wt) awt a_1 ,Wt aw t -2 ,Wt-1,Wt Wt-2, Wt -1) X afLSA (cit-i,wt) - where fi (wt ) f2(wt-i, tut ) and f3(wt-2, tut- wt ) are usually, but not necessarily, {0, 1}-valued indicator functions of N-gram features and awt awt_i,wt and awwwt are their corresponding feature weights, and where the semantic coherence between a word tut and its long-span history dt_1 has been thrown in as a feature, on par with the standard Ngram features. E.g., one could have fLSA (Cit - 1 , Wt ) — K (Wt Cit -1 ) • (15</context>
<context position="11910" citStr="Khudanpur (2000)" startWordPosition="2061" endWordPosition="2062">mparable to the similarity modulated N-gram model (11), except that the choice of aLsA here is made jointly with the Ngram a&apos;s to maximize training data likelihood. If we let each vocabulary item to have its own a, i.e. = aLSA,w vJE RR , (17) then (14) becomes directly comparable to the geometric interpolation method (13), again except that unlike Aw , the aLsA,w parameters are determined jointly with the N-gram a&apos;s to maximize a likelihood criterion. Since the goal of parameter tying, however, is to deal with the continuous nature of the pseudodocument d, another alternative, as suggested by Khudanpur (2000), is a - =a - d,w d,w V jE (D(ci) C IRR , (18) where 4.(d) represents a finite partition of IRR indexed by d. We choose to pursue this alternative. We use a standard K-means clustering of the representations vjS of the training documents dj, with (5) in the role of distance, to obtain a modest number of clusters. We then pool documents in each cluster together to form topic-centroids d, and the partition (DO of RR is defined by the Voronoi regions around the topic-centroids: (ci) = : K ci) &lt; K(d, d&apos;) V centroids . We also make two approximations to the feature function of_(15). First, we appro</context>
</contexts>
<marker>Khudanpur, 2000</marker>
<rawString>Khudanpur, Sanjeev. 2000. Putting language back into language modeling. Presented at the DARPALucent Workshop on Spoken Language Recognition and Understanding, Summit, NJ, Feb 6-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pallett</author>
</authors>
<title>Tools for the analysis of benchmark speech recognition tests.</title>
<date>1990</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>1--97</pages>
<location>Alburquerque, NM.</location>
<marker>Pallett, 1990</marker>
<rawString>Pallett, David et al. 1990. Tools for the analysis of benchmark speech recognition tests. Proc. ICASSP, 1:97-100, Alburquerque, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="10313" citStr="Rosenfeld, 1996" startWordPosition="1783" endWordPosition="1784">cator functions of N-gram features and awt awt_i,wt and awwwt are their corresponding feature weights, and where the semantic coherence between a word tut and its long-span history dt_1 has been thrown in as a feature, on par with the standard Ngram features. E.g., one could have fLSA (Cit - 1 , Wt ) — K (Wt Cit -1 ) • (15) We then find the maximum likelihood estimate of the model parameters a given the training data. Recall that the resulting model is also the maximum entropy (ME) model among models which satisfy constraints on the marginal probabilities or expected values of these features (Rosenfeld, 1996). An important decision that needs to be made in a model such as (14) is the parameterization a. In a traditional ME language model, in the absence of LSA-based features, each N-gram feature function is a {0, 1}-valued indicator function, and there is a parameter associated with each feature: an aw for each unigram constraint, an aw,,w for each bigram constraint, etc. In extending this methodology to the LSA features, we note that K(wt,dt-i) is continuous-valued. That in itself is not a problem; the ME framework does not require the f (.)&apos;s to be binary. What is problematic, however, is the fa</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, Roni. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
</authors>
<title>Maximum entropy language modeling with nonlocal dependencies.</title>
<date>2002</date>
<institution>PhD Dissertation, Johns Hopkins University CS Department,</institution>
<location>Baltimore, MD.</location>
<marker>Wu, 2002</marker>
<rawString>Wu, Jun. 2002. Maximum entropy language modeling with nonlocal dependencies. PhD Dissertation, Johns Hopkins University CS Department, Baltimore, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>