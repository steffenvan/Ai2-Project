<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001441">
<title confidence="0.974432">
Mind the Gap: Learning to Choose Gaps for Question Generation
</title>
<author confidence="0.997782">
Lee Becker
</author>
<affiliation confidence="0.999767">
Department of Computer Science
University of Colorado Boulder
</affiliation>
<address confidence="0.784458">
Boulder, CO 80309
</address>
<email confidence="0.998577">
lee.becker@colorado.edu
</email>
<author confidence="0.70255">
Sumit Basu and Lucy Vanderwende
</author>
<affiliation confidence="0.707473">
Microsoft Research
</affiliation>
<address confidence="0.878819">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.999013">
{sumitb,lucyv}@microsoft.com
</email>
<sectionHeader confidence="0.996622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99925668">
Not all learning takes place in an educational
setting: more and more self-motivated learners
are turning to on-line text to learn about new
topics. Our goal is to provide such learners
with the well-known benefits of testing by au-
tomatically generating quiz questions for on-
line text. Prior work on question generation
has focused on the grammaticality of generat-
ed questions and generating effective multi-
ple-choice distractors for individual question
targets, both key parts of this problem. Our
work focuses on the complementary aspect of
determining what part of a sentence we should
be asking about in the first place; we call this
“gap selection.” We address this problem by
asking human judges about the quality of
questions generated from a Wikipedia-based
corpus, and then training a model to effective-
ly replicate these judgments. Our data shows
that good gaps are of variable length and span
all semantic roles, i.e., nouns as well as verbs,
and that a majority of good questions do not
focus on named entities. Our resulting system
can generate fill-in-the-blank (cloze) ques-
tions from generic source materials.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985127659575">
Assessment is a fundamental part of teaching, both
to measure a student’s mastery of the material and
to identify areas where she may need reinforce-
ment or additional instruction. Assessment has also
been shown an important part of learning, as test-
ing assists retention and can be used to guide learn-
ing (Anderson &amp; Biddle, 1975). Thus, as learners
move on from an educational setting to unstruc-
tured self-learning settings, they would still benefit
from having the means for assessment available.
Even in traditional educational settings, there is a
need for automated test generation, as teachers
want multiple tests for topics to give to different
students, and students want different tests with
which to study and practice the material.
One possible solution to providing quizzes for
new source material is the automatic generation of
questions. This is a task the NLP community has
already embraced, and significant progress has
been made in recent years with the introduction of
a shared task (Rus et al., 2010). However, thus far
the research community has focused on the prob-
lem of generating grammatical questions (as in
Heilman and Smith (2010a)) or generating effec-
tive distractors for multiple-choice questions
(Agarwal and Mannem, 2011).
While both of these research threads are of crit-
ical importance, there is another key issue that
must be addressed – which questions should we be
asking in the first place? We have highlighted this
aspect of the problem in the past (see
Vanderwende (2008)) and begin to address it in
this work, postulating that we can both collect hu-
man judgments on what makes a good question
and train a machine learning model that can repli-
cate these judgments. The resulting learned model
can then be applied to new material for automated
question generation. We see this effort as comple-
mentary to the earlier progress.
In our approach, we factor the problem of gen-
erating good questions into two parts: first, the se-
lection of sentences to ask about, and second, the
identification of which part of the resulting sen-
tences the question should address. Because we
want to focus on these aspects of the problem and
not the surface form of the questions, we have cho-
sen to generate simple gap-fill (cloze) questions,
</bodyText>
<page confidence="0.645848">
742
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742–751,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999958827586207">
though our results can also be used to trigger Wh-
questions or multiple-choice questions by leverag-
ing prior work. For sentence selection, we turn to
methods in summarization and use the simple but
effective SumBasic (Nenkova et al., 2006) algo-
rithm to prioritize and choose important sentences
from the article. We cast the second part, gap se-
lection, as a learning problem. To do this, we first
select a corpus of sentences from a very general
body of instructional material (a range of popular
topics from Wikipedia). We then generate a con-
strained subset of all possible gaps via NLP heuris-
tics, and pair each gap with a broad variety of
features pertaining to how it was generated. We
then solicit a large number of human judgments via
crowdsourcing to help us rate the quality of various
gaps. With that data in hand, we train a machine
learning model to replicate these judgments. The
results are promising, with one possible operating
point producing a true positive rate of 83% with a
corresponding false positive rate of 19% (83% of
the possible Good gaps are kept, and 19% of the
not-Good gaps are incorrectly marked); see Figure
6 for the full ROC curve and Section 4 for an ex-
planation of the labels. As the final model has only
minimal dependence on Wikipedia-specific fea-
tures, we expect that it can be applied to an even
wider variety of material (blogs, news articles,
health sites, etc.).
</bodyText>
<sectionHeader confidence="0.862419" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999952">
There already exists a large body of work in auto-
matic question generation (QG) for educational
purposes dating back to the Autoquest system
(Wolfe, 1976), which used an entirely syntactic
approach to generate Wh-Questions from individu-
al sentences. In addition to Autoquest, several oth-
ers have created systems for Wh-question
generation using approaches including transfor-
mation rules (Mitkov and Ha, 2003), template-
based generation (Chen et al., 2009; Curto et al.,
2011), and overgenerate-and-rank (Heilman and
Smith, 2010a). The work in this area has largely
focused on the surface form of the questions, with
an emphasis on grammaticality.
Alternatively, generation of gap-fill style ques-
tions (a.k.a. cloze questions) avoids these issues of
grammaticality by blanking out words or spans in a
known good sentence. There is a large body of ex-
isting work that has focused on generation of this
type of question, most of which has focused on
vocabulary and language learning. The recent work
of Agarwal and Mannem (2011) is closer to our
purposes; they generated fill-in-the-blank questions
and distractor answers for reading comprehension
tests using heuristic scoring measures and a small
evaluation set. Our work has similar aims but em-
ploys a data-driven approach.
The Question-Generation Shared Task and
Evaluation Challenge (QG-STEC) (Rus et al.,
2010) marks a first attempt at creating a common
task and corpus for empirical evaluation of ques-
tion generation components. However, evaluation
in this task was manual and the number of instanc-
es in both the development and training set were
small. As there exists no other dataset for question
generation, we created a new corpus using Amazon
Mechanical Turk by soliciting judgments from
non-experts. Snow et al. (2008) have validated
AMT as a valid data source by comparing non-
expert with gold-standard expert judgments. Cor-
pus creation using AMT has numerous precedents
now; see i.e. Callison-Burch and Dredze (2010)
and Heilman and Smith (2010b). We have made
our corpus (see Section 4) available online to ena-
ble others to continue research on the gap-selection
problem we address here.
</bodyText>
<sectionHeader confidence="0.951889" genericHeader="method">
3 Question Generation
</sectionHeader>
<bodyText confidence="0.999981833333333">
To achieve our goal of selecting better gap-fill
questions, we have broken down the task into stag-
es similar to those proposed by Nielsen (2008): 1)
sentence selection, 2) question construction, and 3)
classification/scoring. Specifically, we utilize sum-
marization to identify key sentences from a pas-
sage. We then apply semantic and syntactic
constraints to construct multiple candidate ques-
tion/answer pairs from a given source sentence.
Lastly we extract features from these hypotheses
for use with a question quality classification mod-
el. To train this final question-scoring component,
we made use of crowdsourcing to collect ratings
for a corpus of candidate questions. While this
pipeline currently produces gap-fill questions, we
envision these components can be used as input for
more complex surface generation such as Wh-
forms or distractor selection.
</bodyText>
<page confidence="0.99748">
743
</page>
<bodyText confidence="0.822954428571429">
In 1874 Röntgen a lecturer at the University of Strassburg.
In Röntgen became a lecturer at the University of Strassburg.
In 1874 became a lecturer at the University of Strassburg.
In 1874 Röntgen became a at the University of Strassburg.
In 1874 Röntgen became a lecturer at of Strassburg.
In 1874 Röntgen became a lecturer at the University of .
In 1874 Röntgen became a lecturer at .
</bodyText>
<figureCaption confidence="0.947542">
Figure 1 An example of the question generation process.
</figureCaption>
<subsectionHeader confidence="0.998977">
3.1 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.9999884375">
When learning about a new subject, a student will
most likely want to learn about key concepts be-
fore moving onto more obscure details. As such, it
is necessary to order target sentences in terms of
their importance. This is fortunately very similar to
the goals of automatic summarization, in which the
selected sentences should be ordered by how cen-
tral they are to the article.
As a result, we make use of our own implemen-
tation of SumBasic (Nenkova et al., 2006), a sim-
ple but competitive document summarization al-
gorithm motivated by the assumption that
sentences containing the article’s most frequently
occurring words are the most important. We thus
use the SumBasic score for each sentence to order
them as candidates for question construction.
</bodyText>
<subsectionHeader confidence="0.999537">
3.2 Question Construction
</subsectionHeader>
<bodyText confidence="0.981302818181818">
We seek to empirically determine how to choose
questions instead of relying on heuristics and rules
for evaluating candidate surface forms. To do this,
we cast question construction as a generate-and-
filter problem: we overgenerate potential ques-
tion/answer pairs from each sentence and train a
discriminative classifier on human judgments of
quality for those pairs. With the trained classifier,
we can then apply this approach on unseen sen-
tences to return the highest-scoring ques-
tion/answer, all question/answer pairs scoring
above a threshold, and so on.
Generation
Although it would be possible to select every word
or phrase as a candidate gap, this tactic would pro-
duce a skewed dataset composed mostly of unusa-
ble questions, which would subsequently require
much more annotation to discriminate good ques-
tions from bad ones. Instead we rely on syntactic
and semantic constraints to reduce the number of
questions that need annotation.
To generate questions we first run the source
sentence through a constituency parser and a se-
mantic role labeler (components of a state-of-the-
art natural language toolkit from (Quirk et al.,
2012)), with the rationale that important parts of
the sentence will occur within a semantic role.
Each verb predicate found within the roles then
automatically becomes a candidate gap. From eve-
ry argument to the predicate, we extract all child
noun phrases (NP) and adjectival phrases (ADJP)
as candidate gaps as well. Figure 1 illustrates this
generation process.
</bodyText>
<subsectionHeader confidence="0.457579">
Classification
</subsectionHeader>
<bodyText confidence="0.999721466666667">
To train the classifier for question quality, we ag-
gregated per-question ratings into a single label
(see Section 4 for details). Questions with an aver-
age rating of 0.67 or greater were considered as
positive examples. This outcome was then paired
with a vector of features (see Section 5) extracted
from the source sentence and the generated ques-
tion.
Because our goal is to score each candidate
question in a meaningful way, we use a calibrated
learner, namely L2-regularized logistic regression
(Bishop 2006). This model’s output is
p(class|features); in our case this is the posteri-
or probability of a candidate receiving a positive
label based on its features.
</bodyText>
<sectionHeader confidence="0.982472" genericHeader="method">
4 Corpus Construction
</sectionHeader>
<bodyText confidence="0.999788666666667">
We downloaded 105 articles from Wikipedia’s
listing of vital articles/popular pages.1 These arti-
cles represent a cross section of historical, social,
</bodyText>
<footnote confidence="0.995428">
1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular
_pages
</footnote>
<page confidence="0.988478">
744
</page>
<table confidence="0.659418">
Source Sentence:
The large scale production of chemicals was an important development during the Industrial Revolution.
Question Answer Ratings
</table>
<bodyText confidence="0.9880195">
The of chemicals was an important large scale production ◯ Good ◉ Okay ◯ Bad
development during the Industrial Revolution.
The large scale production of was an important chemicals ◯ Good ◉ Okay ◯ Bad
development during the Industrial Revolution.
The large scale production of chemicals was an important Industrial Revolution ◉ Good ◯ Okay ◯ Bad
development during the .
</bodyText>
<figureCaption confidence="0.996089">
Figure 2: Example question rating HIT
</figureCaption>
<bodyText confidence="0.999799423076923">
and scientific topics. From each article we sampled
10 sentences using the sentence selection algorithm
described in Section 3.1 for 50% of the sentences;
the other 50% were chosen at random to prevent
any possible overfitting to the selection method.
These sentences were then processed using the
candidate generation method from Section 3.2.
To collect training data outcomes for our ques-
tion classifier, we used Amazon’s Mechanical
Turk (AMT) service to obtain human judgments of
quality for each question. We initially considered
asking about the quality of individual ques-
tion/answer pairs in isolation, but in pilot studies
we found poor agreement in this case; we noticed
that the inability to compare with other possible
questions actually made the task seem difficult and
arbitrary. We thus instead presented raters with a
source sentence and a list of up to ten candidate
questions along with their corresponding answers
(see Figure 2). Raters were asked to rate questions’
quality as Good, Okay, or Bad. The task instruc-
tions defined a Good question as “one that tests
key concepts from the sentence and would be rea-
sonable to answer.” An Okay question was defined
as “one that tests key concepts but might be diffi-
cult to answer (the answer is too lengthy, the an-
swer is ambiguous, etc.).” A Bad question was
“one that asks about an unimportant aspect of the
sentence or has an uninteresting answer that can be
figured out from the context of the sentence.” The-
se ratings were binarized into a score of one for
Good and zero for not-Good (Okay or Bad), as our
goal was to find the probability of a question being
truly Good (and not just Okay).2
2 Heilman and Smith (2010a and b) asked raters to identify
question deficiencies, including vague or obvious, but raters
were not asked to differentiate between Good and Okay. Thus
questions considered Good in their study would include Okay.
Thus far we have run 300 HITs with 4 judges
per HIT. Each HIT consisted of up to 10 candidate
questions generated from a single sentence. In total
this yielded 2252 candidate questions with 4 rat-
ings per question from 85 unique judges. We then
wished to eliminate judges who were gaming the
system or otherwise performing poorly on the task.
It is common to do such filtering when using
crowdsourced data by using the majority or median
vote as the final judgment or to calibrate judges
using expert judgments (Snow et al. 2008). Other
approaches to annotator quality control include
using EM-based algorithms for estimating annota-
tor bias (Wiebe et al. 1999, Ipeirotis et al. 2010).
In our case, we computed the distance for each
judge from the median judgment (from all judges)
on each question, then took the mean of this dis-
tance over all questions they rated. We removed
judges with a mean distance two standard devia-
tions above the mean distance, which eliminated
the five judges who disagreed most with others.
In addition to filtering judges, we wanted to fur-
ther constrain the data to those questions on which
the human annotators had reasonable agreement, as
it would not make sense to attempt to train a model
to replicate judgments on which the annotators
themselves could not agree. To do this, we com-
puted the variance of the judgments for each ques-
tion. By limiting the variance to 0.3, we kept ques-
tions on which up to 1 judge (out of 4) disagreed;
this eliminated 431 questions and retained the 1821
with the highest agreement. Of these filtered ques-
tions, 700 were judged to be Good (38%).
To formally assess inter-rater reliability we
computed Krippendorff’s alpha (Krippendorff,
2004), a statistical measure of agreement applica-
ble for situations with multiple raters and incom-
plete data (in our case not all raters provided
ratings for all items). An alpha value of 1.0 indi-
cates perfect agreement, and an alpha value of 0.0
</bodyText>
<page confidence="0.99275">
745
</page>
<bodyText confidence="0.999871692307692">
indicates no agreement. Our original data yielded
an alpha of 0.34, whereas after filtering judges and
questions the alpha was 0.51. It should be noted
that because Krippendorff’s Alpha accounts for
variability due to multiple raters and sample size,
its values tend to be more pessimistic than many
Kappa values commonly used to measure inter-
rater reliability.
For others interested in working with this data,
we have made our corpus of questions and ratings
available for download at the following location:
http://research.microsoft.com/~sumitb/questiongen
eration.
</bodyText>
<sectionHeader confidence="0.99404" genericHeader="method">
5 Model Features
</sectionHeader>
<bodyText confidence="0.99996435">
While intuition would suggest that selecting high-
quality gaps for cloze questions should be a
straightforward task, analysis of our features im-
plies that identifying important knowledge depends
on more complex interactions between syntax, se-
mantics, and other constraints. In designing fea-
tures, we focused on using commonly extracted
NLP information to profile the answer (gap), the
source sentence, and the relation between the two.
To enable extraction of these features, we used
the MSR Statistical Parsing and Linguistic Analy-
sis Toolkit (MSR SPLAT)3, a state-of-the-art, web-
based service for natural language processing
(Quirk et al., 2012). Table 1 shows a breakdown of
our feature categories and their relative proportion
of the feature space. In the subsections below, we
describe the intuitions behind our choice of fea-
tures and highlight example features from each of
these categories. An exhaustive list of the features
can be found at the corpus URL listed in Section 4.
</bodyText>
<subsectionHeader confidence="0.972002">
5.1 Token Count Features
</subsectionHeader>
<bodyText confidence="0.999569642857143">
A good question gives the user sufficient context to
answer correctly without making the answer obvi-
ous. At the same time, gaps with too many words
may be impossible to answer. Figure 3 shows the
distributions of number of tokens in the answer
(i.e., the gap) for Good and not-Good questions. As
intuition would predict, the not-Good class has
higher likelihoods for the longer answer lengths. In
addition to the number and percentage of tokens in
the answer features, we also included other token
count features such as the number of tokens in the
sentence, and the number of overlapping tokens
between the answer and the remainder of the sen-
tence.
</bodyText>
<table confidence="0.999238125">
Feature Category Number of Features
Token Count 5
Lexical 11
Syntactic 112
Semantic 40
Named Entity 11
Wikipedia link 3
Total 182
</table>
<tableCaption confidence="0.99982">
Table 1: Breakdown of features by category
</tableCaption>
<subsectionHeader confidence="0.996274">
5.2 Lexical features
</subsectionHeader>
<bodyText confidence="0.99994352631579">
Although lexical features play an important role in
system performance for several NLP tasks like
parsing, and semantic role labeling, they require a
large number of examples to provide practical ben-
efit. Furthermore, because most sentences in Wik-
ipedia articles feature numerous domain-specific
terms and names, we cannot rely on lexical fea-
tures to generalize across the variety of possible
articles in our corpus. Instead we approximate lex-
icalization by computing densities of word catego-
ries found within the answer. The intuition behind
these features is that an answer composed primari-
ly of pronouns and stopwords will make for a bad
question while an answer consisting of specific
entities may make for a better question. Examples
of our semi-lexical features include answer pro-
noun density, answer abbreviation density, answer
capitalized word density, and answer stopword
density.
</bodyText>
<subsectionHeader confidence="0.996768">
5.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999930545454545">
The answer’s structure relative to the sentence’s
structure provides information as to where better
spans for the gap may exist. Similarly, part-of-
speech (POS) tags give a topic-independent repre-
sentation of the composition and makeup of the
questions and answers. The collection of syntactic
features includes the answer’s depth with the sen-
tence’s constituent parse, the answer’s location
relative to head verb (before/after), the POS tag
before the answer, the POS tag after the answer,
and the answer bag-of-POS tags.
</bodyText>
<footnote confidence="0.93702">
3 http://research.microsoft.com/projects/msrsplat
</footnote>
<page confidence="0.995715">
746
</page>
<figureCaption confidence="0.9980685">
Figure 3: Distribution of number of tokens in the answer
for Good and not-Good questions.
</figureCaption>
<subsectionHeader confidence="0.983357">
5.4 Semantic Role Label Features
</subsectionHeader>
<bodyText confidence="0.998195909090909">
Beyond syntactic constraints, semantics can yield
additional cues in identifying the important spans
for questioning. Shallow-semantic parses like those
found in Propbank (Palmer et al., 2005) provide a
concise representation for linking predicates
(verbs) to their arguments. Because these semantic
role labels (SRLs) often correspond to the “who,
what, where, and when” of a sentence, they natu-
rally lend themselves for use as features for rating
question quality. To compute SRL features, we
used the MSR SPLAT’s semantic role labeler to
find the SRLs whose spans cover the question’s
answer, the SRLs whose spans are contained with-
in the answer, and the answer’s constituent parse
depth within the closest covering SRL node.
To investigate whether judges keyed in on spe-
cific roles or modifiers when rating questions, we
plotted the distribution of the answer-covering
SRLs (Figure 4). This graph indicates that good
answers are not associated with only a single label
but are actually spread across all SRL classes.
While the bulk of questions came from the argu-
ments often corresponding to subjects and objects
(ARG0-2, shown as A0-A2), we see that good and
bad questions have mostly similar distributions
over SRL classes. However, a notable exception
are answers covered by verb predicates (shown as
“predicate”), which were highly skewed with 190
of the 216 (88.0%) question/answer pairs exhibit-
ing this feature labeled as Bad. Together these dis-
tributions may suggest that judges are more likely
to rate gap-fill questions as Good if they corre-
spond to questions of “who, what, where, and
</bodyText>
<figureCaption confidence="0.9917415">
Figure 4: Distribution of semantic role labels for
Good and not-Good questions.
</figureCaption>
<bodyText confidence="0.9141775">
when” over questions pertaining to “why and
how.”
</bodyText>
<subsectionHeader confidence="0.930853">
5.5 Named Entity Features
</subsectionHeader>
<bodyText confidence="0.99998552">
For many topics, especially in the social sciences,
knowing the relevant people and places marks the
first step toward comprehending new material. To
reflect these concerns we use the named-entity
tagger in the toolkit to identify the spans of text
that refer to persons, locations, or organizations,
which are then used to derive additional features
for distinguishing between candidate questions.
Example named-entity features include: answer
named entity density, answer named entity type
frequency (LOC, ORG, PER), and sentence named
entity frequency.
Figure 5 shows the distribution of named entity
types found within the answers for Good and not-
Good questions. From this graph, we see that Good
questions have a higher class-conditional probabil-
ity of containing a named entity. Furthermore, we
see that Good questions are not confined to a sin-
gle named entity type, but are spread across all
types. Together, these distributions indicate that
while named entities can help to identify important
gaps, the majority of questions labeled Good do
not contain any named entity (515/700, i.e. 74%).
This provides substantial evidence for generating
questions for more than only named entities.
</bodyText>
<page confidence="0.996147">
747
</page>
<sectionHeader confidence="0.916896" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<figure confidence="0.740624">
True Positive Rate
</figure>
<figureCaption confidence="0.998856">
Figure 5: Distribution of answer named entity type for
Good and not-Good questions.
</figureCaption>
<subsectionHeader confidence="0.919979">
5.6 Wikipedia Link Features
</subsectionHeader>
<bodyText confidence="0.999974444444444">
Wikipedia’s markup language allows spans of text
to link to other articles. This annotation inherently
indicates a span of text as noteworthy, and can
serve as evidence of an answer’s importance. We
use the presence of this markup to compute fea-
tures such as answer link density, sentence link
density, and the ratio of the number of linked
words in the answer to the ratio of linked words in
the sentence.
</bodyText>
<sectionHeader confidence="0.924229" genericHeader="evaluation">
6 Model and Training
</sectionHeader>
<bodyText confidence="0.999981375">
We chose logistic regression as our classifier be-
cause of its calibrated output of the class posterior;
we combined it with an L2 regularizer to prevent
overfitting. As the data likelihood is convex in the
model parameters, we trained the model to maxim-
ize this quantity along with the regularization term
using the L-BFGS algorithm for Quasi-Newton
optimization (Nocedal, 1980). Evaluation was
conducted with 10-fold cross validation, taking
care to stratify folds so that all questions generated
from the same source sentence are placed in the
same fold. Results are shown in Section 7 below.
To ensure that we were not overly narrow in
this model choice, we tested two other more pow-
erful classifiers that do not have calibrated outputs,
a linear SVM and a boosted mixture of decision
trees (Caruana and Niculescu-Mizil, 2006); both
produced accuracies within a percentage point of
our model at the equal error rate.
Figure 6 shows ROC curves for our question quali-
ty classifier produced by sweeping the threshold on
the output probability, using the raw collected data,
our filtered version as described above, and a fur-
ther filtered version keeping only those questions
where judges agreed perfectly; the benefits of fil-
tering can be seen in the improved performance. In
this context, the true positive rate refers to the frac-
tion of Good questions that were correctly identi-
fied, and the false positive rate refers to the
fraction of not-Good questions that were incorrect-
ly marked. At the equal error rate, the true positive
rate was 0.83 and the false positive rate was 0.19.
</bodyText>
<figureCaption confidence="0.781267333333333">
Figure 6: ROC for our model using unfiltered data
(green dots), our filtered version (red dashes), and fil-
tered for perfect agreement (blue line).
</figureCaption>
<bodyText confidence="0.999890466666667">
Choosing the appropriate operating point depends
on the final application. By tuning the classifier’s
true positive and false positive rates, we can cus-
tomize the system for several uses. For example, in
a relatively structured scenario like compliance
training, it may be better to reduce any possibility
of confusion by eliminating false positives. On the
other hand, a self-motivated learner attempting to
explore a new topic may tolerate a higher false
positive rate in exchange for a broader diversity of
questions. The balance is subtle, though, as ill-
formed and irrelevant questions could leave the
learner bored or frustrated, but alternatively, overly
conservative question classification could poten-
tially eliminate all but the most trivial questions.
</bodyText>
<page confidence="0.972705">
748
</page>
<figure confidence="0.868572">
accuracy
</figure>
<figureCaption confidence="0.999558">
Figure 7: ROC for our model with (red dash) and with-
out (blue line) Wikipedia-specific features.
Figure 8: Classifier learning curve; each point repre-
sents mean accuracy over 40 folds.
</figureCaption>
<bodyText confidence="0.999985684210526">
We next wanted to get a sense of how well the
model would generalize to other text, and as such
ran an analysis of training the classifier without the
benefit of the Wikipedia-specific features (Figure
7). The resulting model performs about the same as
the original on average over the ROC, slightly bet-
ter in some places and slightly worse in others. We
hypothesize the effect is small because these fea-
tures relate only to Wikipedia entities, and the oth-
er named entity features likely make them
redundant.
Finally, to understand the sensitivity of our
model to the amount of training data, we plot a
learning curve of the question classifier’s accuracy
by training it against fractions of the available data
(Figure 8). While the curve starts to level out
around 1200 data points, the accuracy is still rising
slightly, which suggests the system could achieve
some small benefits in accuracy from more data.
</bodyText>
<subsectionHeader confidence="0.981898">
7.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999975318181819">
To explore the nature of our system’s misclassifi-
cations we examine the errors that occur at the
equal error rate operating point. For our system,
false positive errors occur when the system labels a
question as Good when the raters considered it not-
Good. Table 2 lists three examples of this type of
error. The incorrect high score in example 1
(“Greeks declared ___”) suggests that system per-
formance can be improved via language modeling,
as such features would penalize questions with an-
swers that could be predicted mostly by word tran-
sition probabilities. Similarly, when classifying
questions like example 2 (“such as ____ for a
mathematical function”), the system could benefit
from some measure of word frequency or answer
novelty. While our model included a feature for the
number of overlapping words between the question
and the answer, the high classifier score for exam-
ple 3 (“atop ______, the volcano”), suggests that
this can be solved by explicitly filtering out such
questions at generation time.
With false negative errors the judges rated the
question as Good, whereas the system classified it
as Bad. The question and answer pairs listed in
Table 3 demonstrate some of these errors. In ex-
ample 1 (“where Pompey was soon ___”), the sys-
tem was likely incorrect because a majority of
questions with verb-predicate answers had Bad
ratings (only 12% are Good). Conversely, classifi-
cation of example 2 (“Over the course of dec-
ades...”) could be improved with a feature
indicating the novelty of the words in the answers.
Example 3 (“About 7.5% of the...”) appears to
come from rater error or rater confusion as the
question does little to test the understanding of the
material.
While the raters considered the answer to ex-
ample 4 as Good, the low classifier score argues
for different handling of answers derived from
long coordinated phrases. One alternative approach
would be to generate questions that use multiple
gaps. Conversely, one may argue that a learner
may be better off answering any one of the noun
phrases like palm oil or cocoa in isolation.
</bodyText>
<page confidence="0.994305">
749
</page>
<table confidence="0.999787882352941">
Question Answer Confidence
1 In 1821 the Greeks war 0.732
declared ___ on the
sultan.
2 He also introduced the notion 0.527
much of the modern
mathematical terminol-
ogy and notation, par-
ticularly for
mathematical analysis,
such as of a
mathematical function.
3 Not only is there much the volcano 0.790
ice atop , the
volcano is also being
weakened by hydro-
thermal activity.
</table>
<tableCaption confidence="0.9632595">
Table 2: Example false positives (human judges rated
these as not-Good)
</tableCaption>
<table confidence="0.994683541666667">
Question Answer Confidence
1 Caesar then pursued murdered 0.471
Pompey to Egypt,
where Pompey was
soon ____.
2 Over the course of dec- a new 0.306
ades, individual wells equilibrium
draw down local tem-
peratures and water
levels until _______ is
reached with natural
flows.
3 About 7.5% of world today 0.119
sea trade is carried via
the canal ____.
4 Asante and Dahomey the form of 0.029
concentrated on the palm oil,
development of “legiti- cocoa, tim-
mate commerce” in ber, and
forming gold.
,
the bedrock of West
Africa’s modern export
trade,
</table>
<tableCaption confidence="0.9842165">
Table 3: Example false negatives (human judges rated
these Good)
</tableCaption>
<subsectionHeader confidence="0.994989">
7.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.99984975">
To ensure that all of the gain of the classifier was
not coming from only a handful of isolated fea-
tures, we examined the mean values for each fea-
ture’s learned weight in the model over the course
of 10 cross-validation folds, and then sorted the
means for greater clarity (Figure 8). The weights
indeed seem to be well distributed across many
features.
</bodyText>
<figureCaption confidence="0.993317">
Figure 8: Feature weight means and standard deviations.
</figureCaption>
<sectionHeader confidence="0.989489" genericHeader="discussions">
8 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999955384615385">
We have presented a method that determines
which gaps in a sentence to ask questions about by
training a classifier that largely agrees with human
judgments on question quality. We feel this effort
is complementary to the past work on question
generation, and represents another step towards
helping self-motivated learners with automatically
generated tests.
In our future work, we hope to expand the set of
features as described in Section 7. We additionally
intend to cast the sentence selection problem as a
separate learning problem that can also be trained
from human judgments.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98359295">
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic Gap-fill Question Generation from Text
Books. In Proceedings of the 6th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications. Portland, OR, USA. pages 56-64.
Richard C. Anderson and W. Barry Biddle. 1975. On
asking people questions about what they are read-
ing. In G. Bower (Ed.) Psychology of Learning and
Motivation, 9:90-132.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. New York: Springer, 2006.
Jonathan C. Brown, Gwen A. Frishkoff, and Maxine
Eskenazi. 2005. Automatic Question Generation for
Vocabulary Assessment. In Proceedings of
HLT/EMNLP 2005. Vancouver, Canada: Associa-
tion for Computational Linguistics. pages 819-826.
Rich Caruana and Alexandru Niculescu-Mizil. 2006. An
Empirical Comparison of Supervised Learning Al-
gorithms. In Proceedings of ICML 2006.
mean and std. dev. of feature weight
</reference>
<page confidence="0.977193">
750
</page>
<reference confidence="0.999905621052632">
Chris Callison-Burch and Mark Dredze. 2010. Creating
Speech and Language Data with Amazon&apos;s Me-
chanical Turk. In Proceedings of NAACL 2010
Workshop on Creating Speech and Language Data
with Amazon&apos;s Mechanical Turk. Los Angeles, CA.
pages 1-12.
Wei Chen, Gregory Aist, and Jack Mostow. 2009. Gen-
erating Questions Automatically from Information-
al Text. In S. Craig &amp; S. Dicheva (Ed.),
Proceedings of the 2nd Workshop on Question
Generation.
Sérgio Curto, Ana Cristina Mendes, and Luísa Coheur.
2011. Exploring linguistically-rich patterns for
question generation. In Proceedings of the UCNLG
+ eval: Language Generation and Evaluation
Workshop. Edinburgh, Scotland: Association for
Computational Linguistics. Pages 33-38.
Michael Heilman and Noah A. Smith. 2010a. Good
Question! Statistical Ranking for Question Genera-
tion. In Proceedings of NAACL/HLT 2010. pages
609-617.
Michael Heilman and Noah A. Smith. 2010b. Rating
Computer-Generated Questions with Mechanical
Turk. In Proceedings of NAACL 2010 Workshop on
Creating Speech and Language Data with Ama-
zon&apos;s Mechanical Turk. Los Angeles, CA. pages 35-
40.
Ayako Hoshino and Hiroshi Nakagawa. 2005. A real-
time multiple-choice question generation for lan-
guage testing - a preliminary study -. In Proceed-
ings of the 2nd Workshop on Building Educational
Applications Using NLP. Ann Arbor, MI, USA:
Association for Computational Linguistics. pages
17-20.
Panagiotis G. Ipeirotis, Foster Provost, Jing Wang .
2010. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation (HCOMP’10).
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology. Thousand Oaks, CA:
Sage.
Ruslan Mitkov and Le An Ha. 2003. Computer-Aided
Generation of Multiple-Choice Tests. Proceedings
of the HLT-NAACL 2003 Workshop on Building
Educational Applications Using Natural Language
Processing, pages 17-22.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A computer-aided environment for generat-
ing multiple choice test items. Natural Language
Engineering, 12(2): 177-194.
Ani Nenkova, Lucy Vanderwende, and Kathleen
McKeown. 2006. A Compositional Context Sensi-
tive Multidocument Summarizer. In Proceedings of
SIGIR 2006. pages 573-580.
Rodney D. Nielsen. 2008. Question Generation: Pro-
posed Challenge Tasks and Their Evaluation. In V.
Rus, &amp; A. Graesser (Ed.), In Proceedings of the
Workshop on the Question Generation Shared Task
and Evaluation Challenge. Arlington, VA.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computa-
tion, 35:773-782.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of
Semantic Roles. Computational Linguistics, vol.
31, no. 1, pp. 71--106.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, and Lucy Vanderwende. 2012. MSR
SPLAT, a language analysis toolkit. In Proceedings
of NAACL HLT 2012 Demonstration Session.
http://research.microsoft.com/projects/msrsplat .
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,
Svetlana Stoyanchev and Cristian Moldovan. 2010.
Overview of The First Question Generation Shared
Task Evaluation Challenge. In Proceedings of the
Third Workshop on Question Generation. Pitts-
burgh, PA, USA. pages 45-57.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and Fast—but is it
Good? Evaluating non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of
EMNLP’08. pages 254-263.
Lucy Vanderwende. 2008. The Importance of Being
Important: Question Generation. In Proceedings of
the 1st Workshop on the Question Generation
Shared Task Evaluation Challenge, Arlington, VA.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of ACL 1999.
John H. Wolfe. 1976. Automatic question generation
from text - an aid to independent study. In Proceed-
ings of the ACM SIGCSE-SIGCUE technical sym-
posium on Computer science and education. New
York, NY, USA: ACM. pages 104-112.
</reference>
<page confidence="0.998292">
751
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661158">
<title confidence="0.999456">Mind the Gap: Learning to Choose Gaps for Question Generation</title>
<author confidence="0.996573">Lee</author>
<affiliation confidence="0.99997">Department of Computer University of Colorado</affiliation>
<address confidence="0.999859">Boulder, CO 80309</address>
<email confidence="0.999772">lee.becker@colorado.edu</email>
<author confidence="0.921138">Sumit Basu</author>
<author confidence="0.921138">Lucy</author>
<affiliation confidence="0.930041">Microsoft</affiliation>
<address confidence="0.8871335">One Microsoft Redmond, WA 98052</address>
<email confidence="0.999656">sumitb@microsoft.com</email>
<email confidence="0.999656">lucyv@microsoft.com</email>
<abstract confidence="0.999111692307692">Not all learning takes place in an educational setting: more and more self-motivated learners are turning to on-line text to learn about new topics. Our goal is to provide such learners with the well-known benefits of testing by automatically generating quiz questions for online text. Prior work on question generation has focused on the grammaticality of generated questions and generating effective multiple-choice distractors for individual question targets, both key parts of this problem. Our work focuses on the complementary aspect of determining what part of a sentence we should be asking about in the first place; we call this “gap selection.” We address this problem by asking human judges about the quality of questions generated from a Wikipedia-based corpus, and then training a model to effectively replicate these judgments. Our data shows that good gaps are of variable length and span all semantic roles, i.e., nouns as well as verbs, and that a majority of good questions do not focus on named entities. Our resulting system can generate fill-in-the-blank (cloze) questions from generic source materials.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manish Agarwal</author>
<author>Prashanth Mannem</author>
</authors>
<title>Automatic Gap-fill Question Generation from Text Books.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<pages>56--64</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="2693" citStr="Agarwal and Mannem, 2011" startWordPosition="417" endWordPosition="420">pics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to new material for automated question generation. We see this effort as complementary to the earlier</context>
<context position="6386" citStr="Agarwal and Mannem (2011)" startWordPosition="1031" endWordPosition="1034"> Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; they generated fill-in-the-blank questions and distractor answers for reading comprehension tests using heuristic scoring measures and a small evaluation set. Our work has similar aims but employs a data-driven approach. The Question-Generation Shared Task and Evaluation Challenge (QG-STEC) (Rus et al., 2010) marks a first attempt at creating a common task and corpus for empirical evaluation of question generation components. However, evaluation in this task was manual and the number of instances in both the development and training set were small. As there exists n</context>
</contexts>
<marker>Agarwal, Mannem, 2011</marker>
<rawString>Manish Agarwal and Prashanth Mannem. 2011. Automatic Gap-fill Question Generation from Text Books. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications. Portland, OR, USA. pages 56-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Anderson</author>
<author>W Barry Biddle</author>
</authors>
<title>On asking people questions about what they are reading.</title>
<date>1975</date>
<booktitle>In G. Bower (Ed.) Psychology of Learning and Motivation,</booktitle>
<pages>9--90</pages>
<contexts>
<context position="1772" citStr="Anderson &amp; Biddle, 1975" startWordPosition="274" endWordPosition="277">data shows that good gaps are of variable length and span all semantic roles, i.e., nouns as well as verbs, and that a majority of good questions do not focus on named entities. Our resulting system can generate fill-in-the-blank (cloze) questions from generic source materials. 1 Introduction Assessment is a fundamental part of teaching, both to measure a student’s mastery of the material and to identify areas where she may need reinforcement or additional instruction. Assessment has also been shown an important part of learning, as testing assists retention and can be used to guide learning (Anderson &amp; Biddle, 1975). Thus, as learners move on from an educational setting to unstructured self-learning settings, they would still benefit from having the means for assessment available. Even in traditional educational settings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progres</context>
</contexts>
<marker>Anderson, Biddle, 1975</marker>
<rawString>Richard C. Anderson and W. Barry Biddle. 1975. On asking people questions about what they are reading. In G. Bower (Ed.) Psychology of Learning and Motivation, 9:90-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer,</publisher>
<location>New York:</location>
<contexts>
<context position="11673" citStr="Bishop 2006" startWordPosition="1872" endWordPosition="1873">ases (ADJP) as candidate gaps as well. Figure 1 illustrates this generation process. Classification To train the classifier for question quality, we aggregated per-question ratings into a single label (see Section 4 for details). Questions with an average rating of 0.67 or greater were considered as positive examples. This outcome was then paired with a vector of features (see Section 5) extracted from the source sentence and the generated question. Because our goal is to score each candidate question in a meaningful way, we use a calibrated learner, namely L2-regularized logistic regression (Bishop 2006). This model’s output is p(class|features); in our case this is the posterior probability of a candidate receiving a positive label based on its features. 4 Corpus Construction We downloaded 105 articles from Wikipedia’s listing of vital articles/popular pages.1 These articles represent a cross section of historical, social, 1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular _pages 744 Source Sentence: The large scale production of chemicals was an important development during the Industrial Revolution. Question Answer Ratings The of chemicals was an important large scale productio</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. New York: Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan C Brown</author>
<author>Gwen A Frishkoff</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Automatic Question Generation for Vocabulary Assessment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005.</booktitle>
<pages>819--826</pages>
<location>Vancouver, Canada:</location>
<marker>Brown, Frishkoff, Eskenazi, 2005</marker>
<rawString>Jonathan C. Brown, Gwen A. Frishkoff, and Maxine Eskenazi. 2005. Automatic Question Generation for Vocabulary Assessment. In Proceedings of HLT/EMNLP 2005. Vancouver, Canada: Association for Computational Linguistics. pages 819-826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
<author>Alexandru Niculescu-Mizil</author>
</authors>
<title>An Empirical Comparison of Supervised Learning Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="24895" citStr="Caruana and Niculescu-Mizil, 2006" startWordPosition="3986" endWordPosition="3989">vex in the model parameters, we trained the model to maximize this quantity along with the regularization term using the L-BFGS algorithm for Quasi-Newton optimization (Nocedal, 1980). Evaluation was conducted with 10-fold cross validation, taking care to stratify folds so that all questions generated from the same source sentence are placed in the same fold. Results are shown in Section 7 below. To ensure that we were not overly narrow in this model choice, we tested two other more powerful classifiers that do not have calibrated outputs, a linear SVM and a boosted mixture of decision trees (Caruana and Niculescu-Mizil, 2006); both produced accuracies within a percentage point of our model at the equal error rate. Figure 6 shows ROC curves for our question quality classifier produced by sweeping the threshold on the output probability, using the raw collected data, our filtered version as described above, and a further filtered version keeping only those questions where judges agreed perfectly; the benefits of filtering can be seen in the improved performance. In this context, the true positive rate refers to the fraction of Good questions that were correctly identified, and the false positive rate refers to the f</context>
</contexts>
<marker>Caruana, Niculescu-Mizil, 2006</marker>
<rawString>Rich Caruana and Alexandru Niculescu-Mizil. 2006. An Empirical Comparison of Supervised Learning Algorithms. In Proceedings of ICML 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>dev</author>
</authors>
<note>of feature weight</note>
<marker>dev, </marker>
<rawString>mean and std. dev. of feature weight</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating Speech and Language Data with Amazon&apos;s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk.</booktitle>
<pages>1--12</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7339" citStr="Callison-Burch and Dredze (2010)" startWordPosition="1178" endWordPosition="1181">Rus et al., 2010) marks a first attempt at creating a common task and corpus for empirical evaluation of question generation components. However, evaluation in this task was manual and the number of instances in both the development and training set were small. As there exists no other dataset for question generation, we created a new corpus using Amazon Mechanical Turk by soliciting judgments from non-experts. Snow et al. (2008) have validated AMT as a valid data source by comparing nonexpert with gold-standard expert judgments. Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). We have made our corpus (see Section 4) available online to enable others to continue research on the gap-selection problem we address here. 3 Question Generation To achieve our goal of selecting better gap-fill questions, we have broken down the task into stages similar to those proposed by Nielsen (2008): 1) sentence selection, 2) question construction, and 3) classification/scoring. Specifically, we utilize summarization to identify key sentences from a passage. We then apply semantic and syntactic constraints to construct multiple candidate question/answer p</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating Speech and Language Data with Amazon&apos;s Mechanical Turk. In Proceedings of NAACL 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk. Los Angeles, CA. pages 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chen</author>
<author>Gregory Aist</author>
<author>Jack Mostow</author>
</authors>
<title>Generating Questions Automatically from Informational Text.</title>
<date>2009</date>
<booktitle>In S. Craig &amp; S. Dicheva (Ed.), Proceedings of the 2nd Workshop on Question Generation.</booktitle>
<contexts>
<context position="5816" citStr="Chen et al., 2009" startWordPosition="938" endWordPosition="941">-specific features, we expect that it can be applied to an even wider variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; th</context>
</contexts>
<marker>Chen, Aist, Mostow, 2009</marker>
<rawString>Wei Chen, Gregory Aist, and Jack Mostow. 2009. Generating Questions Automatically from Informational Text. In S. Craig &amp; S. Dicheva (Ed.), Proceedings of the 2nd Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sérgio Curto</author>
<author>Ana Cristina Mendes</author>
<author>Luísa Coheur</author>
</authors>
<title>Exploring linguistically-rich patterns for question generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the UCNLG + eval: Language Generation and Evaluation Workshop.</booktitle>
<pages>33--38</pages>
<location>Edinburgh, Scotland:</location>
<contexts>
<context position="5837" citStr="Curto et al., 2011" startWordPosition="942" endWordPosition="945"> we expect that it can be applied to an even wider variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; they generated fill-in-</context>
</contexts>
<marker>Curto, Mendes, Coheur, 2011</marker>
<rawString>Sérgio Curto, Ana Cristina Mendes, and Luísa Coheur. 2011. Exploring linguistically-rich patterns for question generation. In Proceedings of the UCNLG + eval: Language Generation and Evaluation Workshop. Edinburgh, Scotland: Association for Computational Linguistics. Pages 33-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Good Question! Statistical Ranking for Question Generation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT</booktitle>
<pages>609--617</pages>
<contexts>
<context position="2597" citStr="Heilman and Smith (2010" startWordPosition="405" endWordPosition="408">ettings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to ne</context>
<context position="5889" citStr="Heilman and Smith, 2010" startWordPosition="948" endWordPosition="951">er variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; they generated fill-in-the-blank questions and distractor answers for readi</context>
<context position="7367" citStr="Heilman and Smith (2010" startWordPosition="1183" endWordPosition="1186">pt at creating a common task and corpus for empirical evaluation of question generation components. However, evaluation in this task was manual and the number of instances in both the development and training set were small. As there exists no other dataset for question generation, we created a new corpus using Amazon Mechanical Turk by soliciting judgments from non-experts. Snow et al. (2008) have validated AMT as a valid data source by comparing nonexpert with gold-standard expert judgments. Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). We have made our corpus (see Section 4) available online to enable others to continue research on the gap-selection problem we address here. 3 Question Generation To achieve our goal of selecting better gap-fill questions, we have broken down the task into stages similar to those proposed by Nielsen (2008): 1) sentence selection, 2) question construction, and 3) classification/scoring. Specifically, we utilize summarization to identify key sentences from a passage. We then apply semantic and syntactic constraints to construct multiple candidate question/answer pairs from a given source sen</context>
<context position="14309" citStr="Heilman and Smith (2010" startWordPosition="2294" endWordPosition="2297"> as “one that tests key concepts from the sentence and would be reasonable to answer.” An Okay question was defined as “one that tests key concepts but might be difficult to answer (the answer is too lengthy, the answer is ambiguous, etc.).” A Bad question was “one that asks about an unimportant aspect of the sentence or has an uninteresting answer that can be figured out from the context of the sentence.” These ratings were binarized into a score of one for Good and zero for not-Good (Okay or Bad), as our goal was to find the probability of a question being truly Good (and not just Okay).2 2 Heilman and Smith (2010a and b) asked raters to identify question deficiencies, including vague or obvious, but raters were not asked to differentiate between Good and Okay. Thus questions considered Good in their study would include Okay. Thus far we have run 300 HITs with 4 judges per HIT. Each HIT consisted of up to 10 candidate questions generated from a single sentence. In total this yielded 2252 candidate questions with 4 ratings per question from 85 unique judges. We then wished to eliminate judges who were gaming the system or otherwise performing poorly on the task. It is common to do such filtering when us</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010a. Good Question! Statistical Ranking for Question Generation. In Proceedings of NAACL/HLT 2010. pages 609-617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Rating Computer-Generated Questions with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk.</booktitle>
<pages>35--40</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="2597" citStr="Heilman and Smith (2010" startWordPosition="405" endWordPosition="408">ettings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to ne</context>
<context position="5889" citStr="Heilman and Smith, 2010" startWordPosition="948" endWordPosition="951">er variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; they generated fill-in-the-blank questions and distractor answers for readi</context>
<context position="7367" citStr="Heilman and Smith (2010" startWordPosition="1183" endWordPosition="1186">pt at creating a common task and corpus for empirical evaluation of question generation components. However, evaluation in this task was manual and the number of instances in both the development and training set were small. As there exists no other dataset for question generation, we created a new corpus using Amazon Mechanical Turk by soliciting judgments from non-experts. Snow et al. (2008) have validated AMT as a valid data source by comparing nonexpert with gold-standard expert judgments. Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). We have made our corpus (see Section 4) available online to enable others to continue research on the gap-selection problem we address here. 3 Question Generation To achieve our goal of selecting better gap-fill questions, we have broken down the task into stages similar to those proposed by Nielsen (2008): 1) sentence selection, 2) question construction, and 3) classification/scoring. Specifically, we utilize summarization to identify key sentences from a passage. We then apply semantic and syntactic constraints to construct multiple candidate question/answer pairs from a given source sen</context>
<context position="14309" citStr="Heilman and Smith (2010" startWordPosition="2294" endWordPosition="2297"> as “one that tests key concepts from the sentence and would be reasonable to answer.” An Okay question was defined as “one that tests key concepts but might be difficult to answer (the answer is too lengthy, the answer is ambiguous, etc.).” A Bad question was “one that asks about an unimportant aspect of the sentence or has an uninteresting answer that can be figured out from the context of the sentence.” These ratings were binarized into a score of one for Good and zero for not-Good (Okay or Bad), as our goal was to find the probability of a question being truly Good (and not just Okay).2 2 Heilman and Smith (2010a and b) asked raters to identify question deficiencies, including vague or obvious, but raters were not asked to differentiate between Good and Okay. Thus questions considered Good in their study would include Okay. Thus far we have run 300 HITs with 4 judges per HIT. Each HIT consisted of up to 10 candidate questions generated from a single sentence. In total this yielded 2252 candidate questions with 4 ratings per question from 85 unique judges. We then wished to eliminate judges who were gaming the system or otherwise performing poorly on the task. It is common to do such filtering when us</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010b. Rating Computer-Generated Questions with Mechanical Turk. In Proceedings of NAACL 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk. Los Angeles, CA. pages 35-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayako Hoshino</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>A realtime multiple-choice question generation for language testing - a preliminary study -.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP.</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<location>Ann Arbor, MI, USA:</location>
<marker>Hoshino, Nakagawa, 2005</marker>
<rawString>Ayako Hoshino and Hiroshi Nakagawa. 2005. A realtime multiple-choice question generation for language testing - a preliminary study -. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP. Ann Arbor, MI, USA: Association for Computational Linguistics. pages 17-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
<author>Foster Provost</author>
<author>Jing Wang</author>
</authors>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP’10).</booktitle>
<contexts>
<context position="15208" citStr="Ipeirotis et al. 2010" startWordPosition="2444" endWordPosition="2447">T consisted of up to 10 candidate questions generated from a single sentence. In total this yielded 2252 candidate questions with 4 ratings per question from 85 unique judges. We then wished to eliminate judges who were gaming the system or otherwise performing poorly on the task. It is common to do such filtering when using crowdsourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al. 2008). Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al. 1999, Ipeirotis et al. 2010). In our case, we computed the distance for each judge from the median judgment (from all judges) on each question, then took the mean of this distance over all questions they rated. We removed judges with a mean distance two standard deviations above the mean distance, which eliminated the five judges who disagreed most with others. In addition to filtering judges, we wanted to further constrain the data to those questions on which the human annotators had reasonable agreement, as it would not make sense to attempt to train a model to replicate judgments on which the annotators themselves cou</context>
</contexts>
<marker>Ipeirotis, Provost, Wang, 2010</marker>
<rawString>Panagiotis G. Ipeirotis, Foster Provost, Jing Wang . 2010. In Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Thousand Oaks,</title>
<date>2004</date>
<publisher>Sage.</publisher>
<location>CA:</location>
<contexts>
<context position="16230" citStr="Krippendorff, 2004" startWordPosition="2621" endWordPosition="2622"> data to those questions on which the human annotators had reasonable agreement, as it would not make sense to attempt to train a model to replicate judgments on which the annotators themselves could not agree. To do this, we computed the variance of the judgments for each question. By limiting the variance to 0.3, we kept questions on which up to 1 judge (out of 4) disagreed; this eliminated 431 questions and retained the 1821 with the highest agreement. Of these filtered questions, 700 were judged to be Good (38%). To formally assess inter-rater reliability we computed Krippendorff’s alpha (Krippendorff, 2004), a statistical measure of agreement applicable for situations with multiple raters and incomplete data (in our case not all raters provided ratings for all items). An alpha value of 1.0 indicates perfect agreement, and an alpha value of 0.0 745 indicates no agreement. Our original data yielded an alpha of 0.34, whereas after filtering judges and questions the alpha was 0.51. It should be noted that because Krippendorff’s Alpha accounts for variability due to multiple raters and sample size, its values tend to be more pessimistic than many Kappa values commonly used to measure interrater relia</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
</authors>
<title>Computer-Aided Generation of Multiple-Choice Tests.</title>
<date>2003</date>
<booktitle>Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="5771" citStr="Mitkov and Ha, 2003" startWordPosition="931" endWordPosition="934">l model has only minimal dependence on Wikipedia-specific features, we expect that it can be applied to an even wider variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal an</context>
</contexts>
<marker>Mitkov, Ha, 2003</marker>
<rawString>Ruslan Mitkov and Le An Ha. 2003. Computer-Aided Generation of Multiple-Choice Tests. Proceedings of the HLT-NAACL 2003 Workshop on Building Educational Applications Using Natural Language Processing, pages 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Le An Ha, and Nikiforos Karamanis.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>177--194</pages>
<marker>Mitkov, 2006</marker>
<rawString>Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006. A computer-aided environment for generating multiple choice test items. Natural Language Engineering, 12(2): 177-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A Compositional Context Sensitive Multidocument Summarizer.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>573--580</pages>
<contexts>
<context position="4165" citStr="Nenkova et al., 2006" startWordPosition="660" endWordPosition="663"> we want to focus on these aspects of the problem and not the surface form of the questions, we have chosen to generate simple gap-fill (cloze) questions, 742 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742–751, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics though our results can also be used to trigger Whquestions or multiple-choice questions by leveraging prior work. For sentence selection, we turn to methods in summarization and use the simple but effective SumBasic (Nenkova et al., 2006) algorithm to prioritize and choose important sentences from the article. We cast the second part, gap selection, as a learning problem. To do this, we first select a corpus of sentences from a very general body of instructional material (a range of popular topics from Wikipedia). We then generate a constrained subset of all possible gaps via NLP heuristics, and pair each gap with a broad variety of features pertaining to how it was generated. We then solicit a large number of human judgments via crowdsourcing to help us rate the quality of various gaps. With that data in hand, we train a mach</context>
<context position="9332" citStr="Nenkova et al., 2006" startWordPosition="1504" endWordPosition="1507">me a lecturer at the University of . In 1874 Röntgen became a lecturer at . Figure 1 An example of the question generation process. 3.1 Sentence Selection When learning about a new subject, a student will most likely want to learn about key concepts before moving onto more obscure details. As such, it is necessary to order target sentences in terms of their importance. This is fortunately very similar to the goals of automatic summarization, in which the selected sentences should be ordered by how central they are to the article. As a result, we make use of our own implementation of SumBasic (Nenkova et al., 2006), a simple but competitive document summarization algorithm motivated by the assumption that sentences containing the article’s most frequently occurring words are the most important. We thus use the SumBasic score for each sentence to order them as candidates for question construction. 3.2 Question Construction We seek to empirically determine how to choose questions instead of relying on heuristics and rules for evaluating candidate surface forms. To do this, we cast question construction as a generate-andfilter problem: we overgenerate potential question/answer pairs from each sentence and </context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A Compositional Context Sensitive Multidocument Summarizer. In Proceedings of SIGIR 2006. pages 573-580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
</authors>
<title>Question Generation: Proposed Challenge Tasks and Their Evaluation. In</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge.</booktitle>
<location>Arlington, VA.</location>
<contexts>
<context position="7678" citStr="Nielsen (2008)" startWordPosition="1237" endWordPosition="1238">Mechanical Turk by soliciting judgments from non-experts. Snow et al. (2008) have validated AMT as a valid data source by comparing nonexpert with gold-standard expert judgments. Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). We have made our corpus (see Section 4) available online to enable others to continue research on the gap-selection problem we address here. 3 Question Generation To achieve our goal of selecting better gap-fill questions, we have broken down the task into stages similar to those proposed by Nielsen (2008): 1) sentence selection, 2) question construction, and 3) classification/scoring. Specifically, we utilize summarization to identify key sentences from a passage. We then apply semantic and syntactic constraints to construct multiple candidate question/answer pairs from a given source sentence. Lastly we extract features from these hypotheses for use with a question quality classification model. To train this final question-scoring component, we made use of crowdsourcing to collect ratings for a corpus of candidate questions. While this pipeline currently produces gap-fill questions, we envisi</context>
</contexts>
<marker>Nielsen, 2008</marker>
<rawString>Rodney D. Nielsen. 2008. Question Generation: Proposed Challenge Tasks and Their Evaluation. In V. Rus, &amp; A. Graesser (Ed.), In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge. Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating Quasi-Newton Matrices with Limited Storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<pages>35--773</pages>
<contexts>
<context position="24444" citStr="Nocedal, 1980" startWordPosition="3913" endWordPosition="3914">ce. We use the presence of this markup to compute features such as answer link density, sentence link density, and the ratio of the number of linked words in the answer to the ratio of linked words in the sentence. 6 Model and Training We chose logistic regression as our classifier because of its calibrated output of the class posterior; we combined it with an L2 regularizer to prevent overfitting. As the data likelihood is convex in the model parameters, we trained the model to maximize this quantity along with the regularization term using the L-BFGS algorithm for Quasi-Newton optimization (Nocedal, 1980). Evaluation was conducted with 10-fold cross validation, taking care to stratify folds so that all questions generated from the same source sentence are placed in the same fold. Results are shown in Section 7 below. To ensure that we were not overly narrow in this model choice, we tested two other more powerful classifiers that do not have calibrated outputs, a linear SVM and a boosted mixture of decision trees (Caruana and Niculescu-Mizil, 2006); both produced accuracies within a percentage point of our model at the equal error rate. Figure 6 shows ROC curves for our question quality classif</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating Quasi-Newton Matrices with Limited Storage. Mathematics of Computation, 35:773-782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<pages>71--106</pages>
<contexts>
<context position="20720" citStr="Palmer et al., 2005" startWordPosition="3317" endWordPosition="3320">llection of syntactic features includes the answer’s depth with the sentence’s constituent parse, the answer’s location relative to head verb (before/after), the POS tag before the answer, the POS tag after the answer, and the answer bag-of-POS tags. 3 http://research.microsoft.com/projects/msrsplat 746 Figure 3: Distribution of number of tokens in the answer for Good and not-Good questions. 5.4 Semantic Role Label Features Beyond syntactic constraints, semantics can yield additional cues in identifying the important spans for questioning. Shallow-semantic parses like those found in Propbank (Palmer et al., 2005) provide a concise representation for linking predicates (verbs) to their arguments. Because these semantic role labels (SRLs) often correspond to the “who, what, where, and when” of a sentence, they naturally lend themselves for use as features for rating question quality. To compute SRL features, we used the MSR SPLAT’s semantic role labeler to find the SRLs whose spans cover the question’s answer, the SRLs whose spans are contained within the answer, and the answer’s constituent parse depth within the closest covering SRL node. To investigate whether judges keyed in on specific roles or mod</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, vol. 31, no. 1, pp. 71--106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Pallavi Choudhury</author>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
<author>Kristina Toutanova</author>
<author>Michael Gamon</author>
<author>Wentau Yih</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MSR SPLAT, a language analysis toolkit.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<contexts>
<context position="10785" citStr="Quirk et al., 2012" startWordPosition="1729" endWordPosition="1732"> scoring above a threshold, and so on. Generation Although it would be possible to select every word or phrase as a candidate gap, this tactic would produce a skewed dataset composed mostly of unusable questions, which would subsequently require much more annotation to discriminate good questions from bad ones. Instead we rely on syntactic and semantic constraints to reduce the number of questions that need annotation. To generate questions we first run the source sentence through a constituency parser and a semantic role labeler (components of a state-of-theart natural language toolkit from (Quirk et al., 2012)), with the rationale that important parts of the sentence will occur within a semantic role. Each verb predicate found within the roles then automatically becomes a candidate gap. From every argument to the predicate, we extract all child noun phrases (NP) and adjectival phrases (ADJP) as candidate gaps as well. Figure 1 illustrates this generation process. Classification To train the classifier for question quality, we aggregated per-question ratings into a single label (see Section 4 for details). Questions with an average rating of 0.67 or greater were considered as positive examples. This</context>
<context position="17708" citStr="Quirk et al., 2012" startWordPosition="2845" endWordPosition="2848">st that selecting highquality gaps for cloze questions should be a straightforward task, analysis of our features implies that identifying important knowledge depends on more complex interactions between syntax, semantics, and other constraints. In designing features, we focused on using commonly extracted NLP information to profile the answer (gap), the source sentence, and the relation between the two. To enable extraction of these features, we used the MSR Statistical Parsing and Linguistic Analysis Toolkit (MSR SPLAT)3, a state-of-the-art, webbased service for natural language processing (Quirk et al., 2012). Table 1 shows a breakdown of our feature categories and their relative proportion of the feature space. In the subsections below, we describe the intuitions behind our choice of features and highlight example features from each of these categories. An exhaustive list of the features can be found at the corpus URL listed in Section 4. 5.1 Token Count Features A good question gives the user sufficient context to answer correctly without making the answer obvious. At the same time, gaps with too many words may be impossible to answer. Figure 3 shows the distributions of number of tokens in the </context>
</contexts>
<marker>Quirk, Choudhury, Gao, Suzuki, Toutanova, Gamon, Yih, Vanderwende, 2012</marker>
<rawString>Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami Suzuki, Kristina Toutanova, Michael Gamon, Wentau Yih, and Lucy Vanderwende. 2012. MSR SPLAT, a language analysis toolkit. In Proceedings of NAACL HLT 2012 Demonstration Session. http://research.microsoft.com/projects/msrsplat .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Brendan Wyse</author>
<author>Paul Piwek</author>
<author>Mihai Lintean</author>
<author>Svetlana Stoyanchev</author>
<author>Cristian Moldovan</author>
</authors>
<title>Overview of The First Question Generation Shared Task Evaluation Challenge.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Workshop on Question Generation.</booktitle>
<pages>45--57</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="2461" citStr="Rus et al., 2010" startWordPosition="383" endWordPosition="386">self-learning settings, they would still benefit from having the means for assessment available. Even in traditional educational settings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a go</context>
<context position="6724" citStr="Rus et al., 2010" startWordPosition="1079" endWordPosition="1082"> of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this type of question, most of which has focused on vocabulary and language learning. The recent work of Agarwal and Mannem (2011) is closer to our purposes; they generated fill-in-the-blank questions and distractor answers for reading comprehension tests using heuristic scoring measures and a small evaluation set. Our work has similar aims but employs a data-driven approach. The Question-Generation Shared Task and Evaluation Challenge (QG-STEC) (Rus et al., 2010) marks a first attempt at creating a common task and corpus for empirical evaluation of question generation components. However, evaluation in this task was manual and the number of instances in both the development and training set were small. As there exists no other dataset for question generation, we created a new corpus using Amazon Mechanical Turk by soliciting judgments from non-experts. Snow et al. (2008) have validated AMT as a valid data source by comparing nonexpert with gold-standard expert judgments. Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch an</context>
</contexts>
<marker>Rus, Wyse, Piwek, Lintean, Stoyanchev, Moldovan, 2010</marker>
<rawString>Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev and Cristian Moldovan. 2010. Overview of The First Question Generation Shared Task Evaluation Challenge. In Proceedings of the Third Workshop on Question Generation. Pittsburgh, PA, USA. pages 45-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and Fast—but is it Good? Evaluating non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08.</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and Fast—but is it Good? Evaluating non-Expert Annotations for Natural Language Tasks. In Proceedings of EMNLP’08. pages 254-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>The Importance of Being Important: Question Generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge,</booktitle>
<location>Arlington, VA.</location>
<contexts>
<context position="2950" citStr="Vanderwende (2008)" startWordPosition="464" endWordPosition="465">lready embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to new material for automated question generation. We see this effort as complementary to the earlier progress. In our approach, we factor the problem of generating good questions into two parts: first, the selection of sentences to ask about, and second, the identification of which part of the resulting sentences the question should address. Because we wa</context>
</contexts>
<marker>Vanderwende, 2008</marker>
<rawString>Lucy Vanderwende. 2008. The Importance of Being Important: Question Generation. In Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. O’Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proceedings of ACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John H Wolfe</author>
</authors>
<title>Automatic question generation from text - an aid to independent study.</title>
<date>1976</date>
<booktitle>In Proceedings of the ACM SIGCSE-SIGCUE technical symposium on Computer science and education.</booktitle>
<pages>104--112</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="5517" citStr="Wolfe, 1976" startWordPosition="897" endWordPosition="898"> of 83% with a corresponding false positive rate of 19% (83% of the possible Good gaps are kept, and 19% of the not-Good gaps are incorrectly marked); see Figure 6 for the full ROC curve and Section 4 for an explanation of the labels. As the final model has only minimal dependence on Wikipedia-specific features, we expect that it can be applied to an even wider variety of material (blogs, news articles, health sites, etc.). 2 Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of gramma</context>
</contexts>
<marker>Wolfe, 1976</marker>
<rawString>John H. Wolfe. 1976. Automatic question generation from text - an aid to independent study. In Proceedings of the ACM SIGCSE-SIGCUE technical symposium on Computer science and education. New York, NY, USA: ACM. pages 104-112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>