<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001839">
<title confidence="0.9976">
Portable Translator Capable of Recognizing Characters on
Signboard and Menu Captured by Built-in Camera
</title>
<author confidence="0.95192">
Hideharu Nakajima, Yoshihiro Matsuo, Masaaki Nagata, Kuniko Saito
</author>
<affiliation confidence="0.721792">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.8233735">
Yokosuka, 239-0847, Japan
nakajima.hideharu, matsuo.yoshihiro, nagata.masaaki, saito.kuniko
</address>
<email confidence="0.99516">
@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.995574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987475">
We present a portable translator that rec-
ognizes and translates phrases on sign-
boards and menus as captured by a built-
in camera. This system can be used on
PDAs or mobile phones and resolves the
difficulty of inputting some character sets
such as Japanese and Chinese if the user
doesn’t know their readings. Through the
high speed mobile network, small images
of signboards can be quickly sent to the
recognition and translation server. Since
the server runs state of the art recogni-
tion and translation technology and huge
dictionaries, the proposed system offers
more accurate character recognition and
machine translation.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928846153846">
Our world contains many signboards whose phrases
provide useful information. These include destina-
tions and notices in transportation facilities, names
of buildings and shops, explanations at sightseeing
spots, and the names and prices of dishes in restau-
rants. They are often written in just the mother
tongue of the host country and are not always ac-
companied by pictures. Therefore, tourists must be
provided with translations.
Electronic dictionaries might be helpful in trans-
lating words written in European characters, because
key-input is easy. However, some character sets
such as Japanese and Chinese are hard to input if
</bodyText>
<page confidence="0.988877">
61
</page>
<bodyText confidence="0.999706636363637">
the user doesn’t know the readings such as kana and
pinyin. This is a significant barrier to any translation
service. Therefore, it is essential to replace keyword
entry with some other input approach that supports
the user when character readings are not known.
One solution is the use of optical character recog-
nition (OCR) (Watanabe et al., 1998; Haritaoglu,
2001; Yang et al., 2002). The basic idea is the
connection of OCR and machine translation (MT)
(Watanabe et al., 1998) and implementation with
personal data assistant (PDA) has been proposed
(Haritaoglu, 2001; Yang et al., 2002). These are
based on the document OCR which first tries to ex-
tract character regions; performance is weak due to
the variation in lighting conditions. Although the
system we propose also uses OCR, it is character-
ized by the use of a more robust OCR technology
that doesn’t first extract character regions, by lan-
guage processing to offset the OCR shortcomings,
and by the use of the client-server architecture and
the high speed mobile network (the third generation
(3G) network).
</bodyText>
<sectionHeader confidence="0.860988" genericHeader="method">
2 System design
</sectionHeader>
<bodyText confidence="0.9851837">
Figure 1 overviews the system architecture. After
the user takes a picture by the built-in camera of a
PDA, the picture is sent to a controller in a remote
server. At the server side, the picture is sent to the
OCR module which usually outputs many charac-
ter candidates. Next, the word recognizer identifies
word sequences in the candidates up to the number
specified by the user. Recognized words are sent to
the language translator.
The PDA is linked to the server via wireless com-
</bodyText>
<note confidence="0.482243">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
</note>
<page confidence="0.198874">
pages 61–64, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</page>
<figure confidence="0.845491666666667">
PDA with
built-in camera and
mobile phone
</figure>
<figureCaption confidence="0.991826">
Figure 1: System architecture: http protocol is used
between PDAs and the controller.
</figureCaption>
<bodyText confidence="0.999743166666667">
munication. The current OCR software is Windows-
based while the other components are Linux pro-
grams. The PDA uses Windows.
We also implemented the system for mobile
phones using the i-mode and FOMA devices pro-
vided by NTT-DoCoMo.
</bodyText>
<sectionHeader confidence="0.99719" genericHeader="method">
3 Each component
</sectionHeader>
<subsectionHeader confidence="0.999853">
3.1 Appearance-based full search OCR
</subsectionHeader>
<bodyText confidence="0.999816789473684">
Research into the recognition of characters in nat-
ural scenes has only just begun (Watanabe et al.,
1998; Haritaoglu, 2001; Yang et al., 2002; Wu et
al., 2004). Many conventional approaches first ex-
tract character regions and then classify them into
each character category. However, these approaches
often fail at the extraction stage, because many pic-
tures are taken under less than desirable conditions
such as poor lighting, shading, strain, and distortion
in the natural scene. Unless the recognition target is
limited to some specific signboard (Wu et al., 2004),
it is hard for the conventional OCR techniques to
obtain sufficient accuracy to cover a broad range of
recognition targets.
To solve this difficulty, Kusachi et al. proposed
a robust character classifier (Kusachi et al., 2004).
The classifier uses appearance-based character ref-
erence pattern for robust matching even under poor
capture conditions, and searches the most probable
</bodyText>
<figureCaption confidence="0.76871675">
Figure 2: Many character candidates raised by
appearance-based full search OCR: Rectangles de-
note regions of candidates. The picure shows that
candidates are identified in background regions too.
</figureCaption>
<bodyText confidence="0.999631">
region to identify candidates. As full details are
given in their paper (Kusachi et al., 2004), we focus
here on just its characteristic performance.
As this classifier identifies character candidates
from anywhere in the picture, the precision rate is
quite low, i.e. it lists a lot of wrong candidates. Fig-
ure 2 shows a typical result of this OCR. Rectangles
indicate erroneous candidates, even in background
regions. On the other hand , as it identifies multiple
candidates from the same location, it achieves high
recall rates at each character position (over 80%)
(Kusachi et al., 2004). Hence, if character positions
are known, we can expect that true characters will be
ranked above wrong ones, and greater word recog-
nition accuracies would be achieved by connecting
highly ranked characters in each character position.
This means that location estimation becomes impor-
tant.
</bodyText>
<subsectionHeader confidence="0.999166">
3.2 Word recognition
</subsectionHeader>
<bodyText confidence="0.983765384615385">
Modern PDAs are equipped with styluses. The di-
rect approach to obtaining character location is for
the user to indicate them using the stylus. However,
pointing at all the locations is tiresome, so automatic
estimation is needed. Completely automatic recog-
nition leads to extraction errors so we take the mid-
dle approach: the user specifies the beginning and
ending of the character string to be recognized and
translated. In Figure 3, circles on both ends of the
string denote the user specified points. All the lo-
cations of characters along the target string are esti-
mated from these two locations as shown in Figure
3 and all the candidates as shown in Figure 2.
</bodyText>
<figure confidence="0.998373461538462">
character candidates
character candidates
word candidates
word candidates
translation
image translation
image
Controller
Word
Recognizer
Language
Translator
OCR
</figure>
<page confidence="0.666997">
62
</page>
<figureCaption confidence="0.985166">
Figure 3: Two circles at the ends of the string are
specified by the user with stylus. All the charac-
ter locations (four locations) are automatically esti-
mated.
</figureCaption>
<subsubsectionHeader confidence="0.428749">
3.2.1 Character locations
</subsubsectionHeader>
<bodyText confidence="0.999958888888889">
Once the user has input the end points, assumed
to lie close to the centers of the end characters, the
automatic location module determines the size and
position of the characters in the string. Since the
characters have their own regions delineated by rect-
angles and have x,y coordinates (as shown in Fig-
ure 2), the module considers all candidates and rates
the arrangement of rectangles according to the dif-
ferences in size and separation along the sequences
of rectangles between both ends of the string. The
sequences can be identified by any of the search al-
gorithms used in Natural Language Processing like
the forward Dynamic Programming and backward
A* search (adopted in this work). The sequence with
the highest score, least total difference, is selected as
the true rectangle (candidate) sequence. The centers
of the rectangles are taken as the locations of the
characters in the string.
</bodyText>
<subsectionHeader confidence="0.864659">
3.2.2 Word search
</subsectionHeader>
<bodyText confidence="0.9994085">
The character locations output by the automatic
location module are not taken as specifying the cor-
rect characters, because multiple character candi-
dates are possible at the same location. Therefore,
we identify the words in the string by the probabil-
ities of character combinations. To increase the ac-
curacy, we consider all candidates around each es-
timated location and create a character matrix, an
example of which is shown in Figure 4. At each
location, we rank the candidates according to their
OCR scores, the highest scores occupy the top row.
Next, we apply an algorithm that consists of simi-
lar character matching, similar word retrieval, and
word sequence search using language model scores
</bodyText>
<figureCaption confidence="0.985062">
Figure 4: A character matrix: Character candidates
are bound to each estimated location to make the
matrix. Bold characters are true.
</figureCaption>
<bodyText confidence="0.976034111111111">
(Nagata, 1998).
The algorithm is applied from the start to the end
of the string and examines all possible combinations
of the characters in the matrix. At each location, the
algorithm finds all words, listed in a word dictionary,
that are possible given the location; that is, the first
location restricts the word candidates to those that
start with this character. Moreover, to counter the
case in which the true character is not present in the
matrix, the algorithm identifies those words in the
dictionary that contain characters similar to the char-
acters in the matrix and outputs those words as word
candidates. The connectivity of neighboring words
is represented by the probability defined by the lan-
guage model. Finally, forward Dynamic Program-
ming and backward A* search are used to find the
word sequence with highest probability. The string
in the Figure 3 is recognized as “ .”
</bodyText>
<subsectionHeader confidence="0.999729">
3.3 Language translation
</subsectionHeader>
<bodyText confidence="0.999939615384615">
Our system currently uses the ALT-J/E translation
system which is a rule-based system and employs
the multi-level translation method based on con-
structive process theory (Ikehara et al., 1991). The
string in Figure 3 is translated into “Emergency tele-
phones.”
As target language pairs will increased in future,
the translation component will be replaced by sta-
tistical or corpus based translators since they offer
quicker development. By using this client-server ar-
chitecture on the network, we can place many task
specific translation modules on server machines and
flexibly select them task by task.
</bodyText>
<page confidence="0.999687">
63
</page>
<tableCaption confidence="0.997946">
Table 1: Character Recognition Accuracies
</tableCaption>
<table confidence="0.998403">
[%] OCR OCR+manual OCR+auto
recall 91 91 91
precision 12 82 80
</table>
<sectionHeader confidence="0.7251525" genericHeader="method">
4 Preliminary evaluation of character
recognition
</sectionHeader>
<bodyText confidence="0.999962696969697">
Because this camera base system is primarily for in-
putting character sets, we collected 19 pictures of
signboards with a 1.2 mega pixel CCD camera for
a preliminary evaluation of word recognition perfor-
mance. Both ends of a string in each picture were
specified on a desk-top personal computer for quick
performance analysis such as tallying up the accu-
racy. Average string length was five characters. The
language model for word recognition was basically
a word bigram and trained using news paper articles.
The base OCR system returned over one hundred
candidates for every picture. Though the average
character recall rate was high, over 90%, wrong can-
didates were also numerous and the average charac-
ter precision was about 12%.
The same pictures were evaluated using our
method. It improved the precision to around 80%
(from 12%). This almost equals the precision of
about 82% obtained when the locations of all char-
acters were manually indicated (Table1). Also
the accuracy of character location estimation was
around 95%. 11 of 19 strings (phrases) were cor-
rectly recognized.
The successfully recognized strings consisted of
characters whose sizes were almost the same and
they were evenly spaced. Recognition was success-
ful even if character spacing almost equaled charac-
ter size. If a flash is used to capture the image, the
flash can sometimes be seen in the image which can
lead to insertion error; it is recognized as a punc-
tuation mark. However, this error is not significant
since the picture taking skill of the user will improve
with practice.
</bodyText>
<sectionHeader confidence="0.97905" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9988885">
Our system recognizes characters on signboards and
translates them into other languages. Robust charac-
ter recognition is achieved by combining high-recall
and low-precision OCR and language processing.
In future, we are going to study translation qual-
ities, prepare error-handling mechanisms for brittle
OCR, MT and its combination, and explore new ap-
plication areas of language computation.
</bodyText>
<sectionHeader confidence="0.956262" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9995362">
The authors wish to thank Hisashi Ohara and Ak-
ihiro Imamura for their encouragement and Yoshi-
nori Kusachi, Shingo Ando, Akira Suzuki, and
Ken’ichi Arakawa for providing us with the use of
the OCR program.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993075">
Ismail Haritaoglu. 2001. InfoScope: Link from Real
World to Digital Information Space. In Proceedings of
the 3rd International Conference on Ubiquitous Com-
puting, Springer-Verlag, pages 247-255.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo and Hiromi
Nakaiwa. 1991. Toward an MT System without Pre-
Editing - Effects of New Methods in ALT-J/E -. In
Proceedings of the 3rd MT Summit, pages 101-106.
Yoshinori Kusachi, Akira Suzuki, Naoki Ito, Ken’ichi
Arakawa. 2004. Kanji Recognition in Scene Im-
ages without Detection of Text Fields - Robust Against
Variation of Viewpoint, Contrast, and Background
Texture. In Proceedings ofthe 17th International Con-
ference on Pattern Recognition, pages 204-207.
Masaaki Nagata. 1998. Japanese OCR Error Correc-
tion using Character Shape Similarity and Statistical
Language Model. In Proceedings of the 36th Annual
Meeting ofthe Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics, pages 922-928.
Yasuhiko Watanabe, Yoshihiro Okada, Yeun-Bae Kim,
Tetsuya Takeda. 1998. Translation Camera. In Pro-
ceedings of the 14th International Conference on Pat-
tern Recognition, pages 613–617.
Wen Wu, Xilin Chen, Jie Yang. 2004. Incremental De-
tection of Text on Road Signs from Video with Appli-
cation to a Driving Assistant System. In Proceedings
of the ACMMultimedia 2004, pages 852-859.
Jie Yang, Xilin Chen, Jing Zhang, Ying Zhang, Alex
Waibel. 2002. Automatic Detection and Transla-
tion of Text From Natural Scenes. In Proceedings of
ICASSP, pages 2101-2104.
</reference>
<page confidence="0.999419">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663996">
<title confidence="0.9773595">Portable Translator Capable of Recognizing Characters on Signboard and Menu Captured by Built-in Camera</title>
<author confidence="0.980457">Hideharu Nakajima</author>
<author confidence="0.980457">Yoshihiro Matsuo</author>
<author confidence="0.980457">Masaaki Nagata</author>
<author confidence="0.980457">Kuniko Saito</author>
<affiliation confidence="0.999367">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.998251">Yokosuka, 239-0847, Japan</address>
<email confidence="0.8548595">nakajima.hideharu,matsuo.yoshihiro,nagata.masaaki,saito.kuniko@lab.ntt.co.jp</email>
<abstract confidence="0.998118588235294">We present a portable translator that recognizes and translates phrases on signboards and menus as captured by a builtin camera. This system can be used on PDAs or mobile phones and resolves the difficulty of inputting some character sets such as Japanese and Chinese if the user doesn’t know their readings. Through the high speed mobile network, small images of signboards can be quickly sent to the recognition and translation server. Since the server runs state of the art recognition and translation technology and huge dictionaries, the proposed system offers more accurate character recognition and machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ismail Haritaoglu</author>
</authors>
<title>InfoScope: Link from Real World to Digital Information Space.</title>
<date>2001</date>
<booktitle>In Proceedings of the 3rd International Conference on Ubiquitous Computing,</booktitle>
<pages>247--255</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="1974" citStr="Haritaoglu, 2001" startWordPosition="296" endWordPosition="297">, tourists must be provided with translations. Electronic dictionaries might be helpful in translating words written in European characters, because key-input is easy. However, some character sets such as Japanese and Chinese are hard to input if 61 the user doesn’t know the readings such as kana and pinyin. This is a significant barrier to any translation service. Therefore, it is essential to replace keyword entry with some other input approach that supports the user when character readings are not known. One solution is the use of optical character recognition (OCR) (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002). The basic idea is the connection of OCR and machine translation (MT) (Watanabe et al., 1998) and implementation with personal data assistant (PDA) has been proposed (Haritaoglu, 2001; Yang et al., 2002). These are based on the document OCR which first tries to extract character regions; performance is weak due to the variation in lighting conditions. Although the system we propose also uses OCR, it is characterized by the use of a more robust OCR technology that doesn’t first extract character regions, by language processing to offset the OCR shortcomings, and by the use </context>
<context position="3865" citStr="Haritaoglu, 2001" startWordPosition="606" endWordPosition="607">ages 61–64, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics PDA with built-in camera and mobile phone Figure 1: System architecture: http protocol is used between PDAs and the controller. munication. The current OCR software is Windowsbased while the other components are Linux programs. The PDA uses Windows. We also implemented the system for mobile phones using the i-mode and FOMA devices provided by NTT-DoCoMo. 3 Each component 3.1 Appearance-based full search OCR Research into the recognition of characters in natural scenes has only just begun (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002; Wu et al., 2004). Many conventional approaches first extract character regions and then classify them into each character category. However, these approaches often fail at the extraction stage, because many pictures are taken under less than desirable conditions such as poor lighting, shading, strain, and distortion in the natural scene. Unless the recognition target is limited to some specific signboard (Wu et al., 2004), it is hard for the conventional OCR techniques to obtain sufficient accuracy to cover a broad range of recognition targets. To solve this difficulty, Ku</context>
</contexts>
<marker>Haritaoglu, 2001</marker>
<rawString>Ismail Haritaoglu. 2001. InfoScope: Link from Real World to Digital Information Space. In Proceedings of the 3rd International Conference on Ubiquitous Computing, Springer-Verlag, pages 247-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo and Hiromi Nakaiwa.</title>
<date>1991</date>
<booktitle>Toward an MT System without PreEditing - Effects of New Methods in ALT-J/E -. In Proceedings of the 3rd MT Summit,</booktitle>
<pages>101--106</pages>
<marker>Ikehara, Shirai, 1991</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, Akio Yokoo and Hiromi Nakaiwa. 1991. Toward an MT System without PreEditing - Effects of New Methods in ALT-J/E -. In Proceedings of the 3rd MT Summit, pages 101-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshinori Kusachi</author>
<author>Akira Suzuki</author>
<author>Naoki Ito</author>
<author>Ken’ichi Arakawa</author>
</authors>
<title>Kanji Recognition in Scene Images without Detection of Text Fields - Robust Against Variation of Viewpoint, Contrast, and Background Texture.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 17th International Conference on Pattern Recognition,</booktitle>
<pages>204--207</pages>
<contexts>
<context position="4539" citStr="Kusachi et al., 2004" startWordPosition="710" endWordPosition="713">nal approaches first extract character regions and then classify them into each character category. However, these approaches often fail at the extraction stage, because many pictures are taken under less than desirable conditions such as poor lighting, shading, strain, and distortion in the natural scene. Unless the recognition target is limited to some specific signboard (Wu et al., 2004), it is hard for the conventional OCR techniques to obtain sufficient accuracy to cover a broad range of recognition targets. To solve this difficulty, Kusachi et al. proposed a robust character classifier (Kusachi et al., 2004). The classifier uses appearance-based character reference pattern for robust matching even under poor capture conditions, and searches the most probable Figure 2: Many character candidates raised by appearance-based full search OCR: Rectangles denote regions of candidates. The picure shows that candidates are identified in background regions too. region to identify candidates. As full details are given in their paper (Kusachi et al., 2004), we focus here on just its characteristic performance. As this classifier identifies character candidates from anywhere in the picture, the precision rate </context>
</contexts>
<marker>Kusachi, Suzuki, Ito, Arakawa, 2004</marker>
<rawString>Yoshinori Kusachi, Akira Suzuki, Naoki Ito, Ken’ichi Arakawa. 2004. Kanji Recognition in Scene Images without Detection of Text Fields - Robust Against Variation of Viewpoint, Contrast, and Background Texture. In Proceedings ofthe 17th International Conference on Pattern Recognition, pages 204-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>922--928</pages>
<contexts>
<context position="8576" citStr="Nagata, 1998" startWordPosition="1357" endWordPosition="1358">y the probabilities of character combinations. To increase the accuracy, we consider all candidates around each estimated location and create a character matrix, an example of which is shown in Figure 4. At each location, we rank the candidates according to their OCR scores, the highest scores occupy the top row. Next, we apply an algorithm that consists of similar character matching, similar word retrieval, and word sequence search using language model scores Figure 4: A character matrix: Character candidates are bound to each estimated location to make the matrix. Bold characters are true. (Nagata, 1998). The algorithm is applied from the start to the end of the string and examines all possible combinations of the characters in the matrix. At each location, the algorithm finds all words, listed in a word dictionary, that are possible given the location; that is, the first location restricts the word candidates to those that start with this character. Moreover, to counter the case in which the true character is not present in the matrix, the algorithm identifies those words in the dictionary that contain characters similar to the characters in the matrix and outputs those words as word candida</context>
</contexts>
<marker>Nagata, 1998</marker>
<rawString>Masaaki Nagata. 1998. Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model. In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 922-928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhiko Watanabe</author>
<author>Yoshihiro Okada</author>
<author>Yeun-Bae Kim</author>
<author>Tetsuya Takeda</author>
</authors>
<title>Translation Camera.</title>
<date>1998</date>
<booktitle>In Proceedings of the 14th International Conference on Pattern Recognition,</booktitle>
<pages>613--617</pages>
<contexts>
<context position="1956" citStr="Watanabe et al., 1998" startWordPosition="292" endWordPosition="295"> by pictures. Therefore, tourists must be provided with translations. Electronic dictionaries might be helpful in translating words written in European characters, because key-input is easy. However, some character sets such as Japanese and Chinese are hard to input if 61 the user doesn’t know the readings such as kana and pinyin. This is a significant barrier to any translation service. Therefore, it is essential to replace keyword entry with some other input approach that supports the user when character readings are not known. One solution is the use of optical character recognition (OCR) (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002). The basic idea is the connection of OCR and machine translation (MT) (Watanabe et al., 1998) and implementation with personal data assistant (PDA) has been proposed (Haritaoglu, 2001; Yang et al., 2002). These are based on the document OCR which first tries to extract character regions; performance is weak due to the variation in lighting conditions. Although the system we propose also uses OCR, it is characterized by the use of a more robust OCR technology that doesn’t first extract character regions, by language processing to offset the OCR shortcoming</context>
<context position="3847" citStr="Watanabe et al., 1998" startWordPosition="602" endWordPosition="605">monstration Sessions, pages 61–64, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics PDA with built-in camera and mobile phone Figure 1: System architecture: http protocol is used between PDAs and the controller. munication. The current OCR software is Windowsbased while the other components are Linux programs. The PDA uses Windows. We also implemented the system for mobile phones using the i-mode and FOMA devices provided by NTT-DoCoMo. 3 Each component 3.1 Appearance-based full search OCR Research into the recognition of characters in natural scenes has only just begun (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002; Wu et al., 2004). Many conventional approaches first extract character regions and then classify them into each character category. However, these approaches often fail at the extraction stage, because many pictures are taken under less than desirable conditions such as poor lighting, shading, strain, and distortion in the natural scene. Unless the recognition target is limited to some specific signboard (Wu et al., 2004), it is hard for the conventional OCR techniques to obtain sufficient accuracy to cover a broad range of recognition targets. To solve t</context>
</contexts>
<marker>Watanabe, Okada, Kim, Takeda, 1998</marker>
<rawString>Yasuhiko Watanabe, Yoshihiro Okada, Yeun-Bae Kim, Tetsuya Takeda. 1998. Translation Camera. In Proceedings of the 14th International Conference on Pattern Recognition, pages 613–617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wu</author>
<author>Xilin Chen</author>
<author>Jie Yang</author>
</authors>
<title>Incremental Detection of Text on Road Signs from Video with Application to a Driving Assistant System.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACMMultimedia</booktitle>
<pages>852--859</pages>
<contexts>
<context position="3902" citStr="Wu et al., 2004" startWordPosition="612" endWordPosition="615">005 Association for Computational Linguistics PDA with built-in camera and mobile phone Figure 1: System architecture: http protocol is used between PDAs and the controller. munication. The current OCR software is Windowsbased while the other components are Linux programs. The PDA uses Windows. We also implemented the system for mobile phones using the i-mode and FOMA devices provided by NTT-DoCoMo. 3 Each component 3.1 Appearance-based full search OCR Research into the recognition of characters in natural scenes has only just begun (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002; Wu et al., 2004). Many conventional approaches first extract character regions and then classify them into each character category. However, these approaches often fail at the extraction stage, because many pictures are taken under less than desirable conditions such as poor lighting, shading, strain, and distortion in the natural scene. Unless the recognition target is limited to some specific signboard (Wu et al., 2004), it is hard for the conventional OCR techniques to obtain sufficient accuracy to cover a broad range of recognition targets. To solve this difficulty, Kusachi et al. proposed a robust charac</context>
</contexts>
<marker>Wu, Chen, Yang, 2004</marker>
<rawString>Wen Wu, Xilin Chen, Jie Yang. 2004. Incremental Detection of Text on Road Signs from Video with Application to a Driving Assistant System. In Proceedings of the ACMMultimedia 2004, pages 852-859.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Yang</author>
<author>Xilin Chen</author>
<author>Jing Zhang</author>
<author>Ying Zhang</author>
<author>Alex Waibel</author>
</authors>
<title>Automatic Detection and Translation of Text From Natural Scenes.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>2101--2104</pages>
<contexts>
<context position="1994" citStr="Yang et al., 2002" startWordPosition="298" endWordPosition="301"> provided with translations. Electronic dictionaries might be helpful in translating words written in European characters, because key-input is easy. However, some character sets such as Japanese and Chinese are hard to input if 61 the user doesn’t know the readings such as kana and pinyin. This is a significant barrier to any translation service. Therefore, it is essential to replace keyword entry with some other input approach that supports the user when character readings are not known. One solution is the use of optical character recognition (OCR) (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002). The basic idea is the connection of OCR and machine translation (MT) (Watanabe et al., 1998) and implementation with personal data assistant (PDA) has been proposed (Haritaoglu, 2001; Yang et al., 2002). These are based on the document OCR which first tries to extract character regions; performance is weak due to the variation in lighting conditions. Although the system we propose also uses OCR, it is characterized by the use of a more robust OCR technology that doesn’t first extract character regions, by language processing to offset the OCR shortcomings, and by the use of the client-server</context>
<context position="3884" citStr="Yang et al., 2002" startWordPosition="608" endWordPosition="611">bor, June 2005. c�2005 Association for Computational Linguistics PDA with built-in camera and mobile phone Figure 1: System architecture: http protocol is used between PDAs and the controller. munication. The current OCR software is Windowsbased while the other components are Linux programs. The PDA uses Windows. We also implemented the system for mobile phones using the i-mode and FOMA devices provided by NTT-DoCoMo. 3 Each component 3.1 Appearance-based full search OCR Research into the recognition of characters in natural scenes has only just begun (Watanabe et al., 1998; Haritaoglu, 2001; Yang et al., 2002; Wu et al., 2004). Many conventional approaches first extract character regions and then classify them into each character category. However, these approaches often fail at the extraction stage, because many pictures are taken under less than desirable conditions such as poor lighting, shading, strain, and distortion in the natural scene. Unless the recognition target is limited to some specific signboard (Wu et al., 2004), it is hard for the conventional OCR techniques to obtain sufficient accuracy to cover a broad range of recognition targets. To solve this difficulty, Kusachi et al. propos</context>
</contexts>
<marker>Yang, Chen, Zhang, Zhang, Waibel, 2002</marker>
<rawString>Jie Yang, Xilin Chen, Jing Zhang, Ying Zhang, Alex Waibel. 2002. Automatic Detection and Translation of Text From Natural Scenes. In Proceedings of ICASSP, pages 2101-2104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>