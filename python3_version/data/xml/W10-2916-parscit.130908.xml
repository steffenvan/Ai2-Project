<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004832">
<title confidence="0.985337">
A Semi-Supervised Batch-Mode Active Learning Strategy for Improved
Statistical Machine Translation
</title>
<author confidence="0.901929">
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard and Prem Natarajan
</author>
<affiliation confidence="0.610349">
BBN Technologies
</affiliation>
<address confidence="0.985216">
10 Moulton Street
Cambridge, MA, U.S.A.
</address>
<email confidence="0.998995">
{sanantha,rprasad,stallard,prem}@bbn.com
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972875">
The availability of substantial, in-domain
parallel corpora is critical for the develop-
ment of high-performance statistical ma-
chine translation (SMT) systems. Such
corpora, however, are expensive to pro-
duce due to the labor intensive nature of
manual translation. We propose to al-
leviate this problem with a novel, semi-
supervised, batch-mode active learning
strategy that attempts to maximize in-
domain coverage by selecting sentences,
which represent a balance between domain
match, translation difficulty, and batch di-
versity. Simulation experiments on an
English-to-Pashto translation task show
that the proposed strategy not only outper-
forms the random selection baseline, but
also traditional active learning techniques
based on dissimilarity to existing training
data. Our approach achieves a relative im-
provement of 45.9% in BLEU over the
seed baseline, while the closest competitor
gained only 24.8% with the same number
of selected sentences.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99983012962963">
Rapid development of statistical machine transla-
tion (SMT) systems for resource-poor language
pairs is a problem of significant interest to the
research community in academia, industry, and
government. Tight turn-around schedules, bud-
get restrictions, and scarcity of human translators
preclude the production of large parallel corpora,
which form the backbone of SMT systems.
Given these constraints, the focus is on making
the best possible use of available resources. This
usually involves some form of prioritized data col-
lection. In other words, one would like to con-
struct the smallest possible parallel training corpus
that achieves a desired level of performance on un-
seen test data.
Within an active learning framework, this can
be cast as a data selection problem. The goal is
to choose, for manual translation, the most infor-
mative instances from a large pool of source lan-
guage sentences. The resulting sentence pairs, in
combination with any existing in-domain seed par-
allel corpus, are expected to provide a significantly
higher performance gain than a naive random se-
lection strategy. This process is repeated until a
certain level of performance is attained.
Previous work on active learning for SMT has
focused on unsupervised dissimilarity measures
for sentence selection. Eck et al. (2005) describe a
selection strategy that attempts to maximize cov-
erage by choosing sentences with the highest pro-
portion of previously unseen n-grams. However,
if the pool is not completely in-domain, this strat-
egy may select irrelevant sentences, whose trans-
lations are unlikely to improve performance on an
in-domain test set. They also propose a technique,
based on TF-IDF, to de-emphasize sentences sim-
ilar to those that have already been selected. How-
ever, this strategy is bootstrapped by random ini-
tial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they work
exclusively with the source language and do not
use any SMT-derived features to guide selection.
Haffari et al. (2009) propose a number of fea-
tures, such as similarity to the seed corpus, trans-
lation probability, relative frequencies of n-grams
and “phrases” in the seed vs. pool data, etc., for
active learning. While many of their experiments
use the above features independently to compare
their relative efficacy, one of their experiments
attempts to predict a rank, as a linear combina-
tion of these features, for each candidate sentence.
The top-ranked sentences are chosen for manual
translation. The latter strategy is particularly rel-
evant to this paper, because the goal of our active
</bodyText>
<page confidence="0.978001">
126
</page>
<note confidence="0.9554745">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 126–134,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999886571428572">
learning strategy is not to compare features, but to
learn the trade-off between various characteristics
of the candidate sentences that potentially maxi-
mizes translation improvement.
The parameters of the linear ranking model pro-
posed by Haffari et al. (2009) are trained using
two held-out development sets D1 and D2 - the
model attempts to learn the ordering of D1 that
incrementally maximizes translation performance
on D2. Besides the need for multiple parallel
corpora and the computationally intensive nature
of incrementally retraining an SMT system, their
approach suffers from another major deficiency.
It requires that the pool have the same distribu-
tional characteristics as the development sets used
to train the ranking model. Additionally, they se-
lect all sentences that constitute a batch in a single
operation following the ranking procedure. Since
similar or identical sentences in the pool will typ-
ically meet the selection criteria simultaneously,
this can have the undesired effect of choosing re-
dundant batches with low diversity. This results in
under-utilization of human translation resources.
In this paper, we propose a novel batch-mode
active learning strategy that ameliorates the above
issues. Our semi-supervised learning approach
combines a parallel ranking strategy with sev-
eral features, including domain representativeness,
translation confidence, and batch diversity. The
proposed approach includes a greedy, incremental
batch selection strategy, which encourages diver-
sity and reduces redundancy. The following sec-
tions detail our active learning approach, includ-
ing the experimental setup and simulation results
that clearly demonstrate its effectiveness.
</bodyText>
<sectionHeader confidence="0.954107" genericHeader="method">
2 Active Learning Paradigm
</sectionHeader>
<bodyText confidence="0.999647619047619">
Active learning has been studied extensively in the
context of multi-class labeling problems, and the-
oretically optimal selection strategies have been
identified for simple classification tasks with met-
ric features (Freund et al., 1997). However, nat-
ural language applications such as SMT present a
significantly higher level of complexity. For in-
stance, SMT model parameters (translation rules,
language model n-grams, etc.) are not fixed in
number or type, and vary depending on the train-
ing instances. This gives rise to the concept of
domain. Even large quantities of out-of-domain
training data usually do not improve translation
performance. As we will see, this causes simple
active selection techniques based on dissimilarity
or translation difficulty to be ineffective, because
they tend to favor out-of-domain sentences.
Our proposed active learning strategy is moti-
vated by the idea that the chosen sentences should
maximize coverage, and by extension, translation
performance on an unseen test set. It should
pick sentences that represent the target domain,
while simultaneously enriching the training data
with hitherto unseen, difficult-to-translate con-
structs that are likely to improve performance on a
test set. We refer to the former as representative-
ness and to the latter as difficulty.
Since it is computationally prohibitive to re-
train an SMT system for individual translation
pairs, a batch of sentences is usually selected at
each iteration. We desire that each batch be suffi-
ciently diverse; this increases the number of con-
cepts (phrase pairs, translation rules, etc.) that can
be learned from manual translations of a selected
batch. Thus, our active learning strategy attempts,
at each iteration, to select a batch of mutually di-
verse source sentences, which, while introducing
new concepts, shares at least some commonality
with the target domain. This is done in a com-
pletely statistical, data-driven fashion.
In designing this active learning paradigm, we
make the following assumptions.
</bodyText>
<listItem confidence="0.998788533333333">
• A small seed parallel corpus S is available
for training an initial SMT system. This may
range from a few hundred to a few thousand
sentence pairs.
• Sentences must be selected from a large pool
P. This may be an arbitrary collection of in-
and out-of-domain source language sentences.
Some measure of redundancy is permitted and
expected, i.e. some sentences may be identical
or very similar to others.
• A development set D is available to tune the
SMT system and train the selection algorithm.
An unseen test set T is used to evaluate it.
• The seed, development, and test sets are de-
rived from the target domain distribution.
</listItem>
<bodyText confidence="0.999197285714286">
To re-iterate, we do not assume or require the
pool to have the same domain distribution as the
seed, development, and test sets. This reflects a
real-world scenario, where the pool may be drawn
from multiple sources (e.g. targeted collections,
newswire text, web, etc.). This is a key departure
from existing work on active learning for SMT.
</bodyText>
<page confidence="0.952253">
127
</page>
<figure confidence="0.999664083333333">
Seed corpus
SMT system
Monolingual pool
Input features MLP Classifiers Classifier targets
Trans. difficulty
Pool training
Domain match
Diversity
C 1
C 2
Preferred order
Devel. set
</figure>
<figureCaption confidence="0.999971">
Figure 1: Flow-diagram of the active learner.
</figureCaption>
<sectionHeader confidence="0.986805" genericHeader="method">
3 Active Learning Architecture
</sectionHeader>
<bodyText confidence="0.999452142857143">
Figure 1 illustrates the proposed active learning
architecture in the form of a high-level flow-
diagram. We begin by randomly sampling a small
fraction of the large monolingual pool P to cre-
ate a pool training set PT, which is used to train
the learner. The remainder, which we call the pool
evaluation set PE, is set aside for active selection.
We also train an initial phrase-based SMT system
(Koehn et al., 2003) with the available seed cor-
pus. The pool training set PT, in conjunction with
the seed corpus S, initial SMT system, and held-
out development set D, is used to derive a number
of input features as well as target labels for train-
ing two parallel classifiers.
</bodyText>
<subsectionHeader confidence="0.998956">
3.1 Preferred Ordering
</subsectionHeader>
<bodyText confidence="0.999994541666667">
The learner must be able to map input features
to an ordering of the pool sentences that attempts
to maximize coverage on an unseen test set. We
teach it to do this by providing it with an ordering
of PT that incrementally maximizes source cov-
erage on D. This preferred ordering algorithm in-
crementally maps sentences in PT to a ordered set
OT by picking, at each iteration, the sentence with
the highest coverage criterion with respect to D,
and inserting it at the current position within OT.
The coverage criterion is based on content-word
n-gram overlap with D, discounted by constructs
already observed in S and higher-ranked sentences
in OT, as illustrated in Algorithm 1. Our hypoth-
esis is that sentences, which maximally improve
coverage, likely lead to bigger gains in translation
performance as well.
The O(|PT|2) complexity of this algorithm is
one reason we restrict PT to a few thousand sen-
tences. Another reason not to order the entire pool
and simply select the top-ranked sentences, is that
batches thus constructed would overfit the devel-
opment set on which the ordering is based, and
not generalize well to an unseen test set.
</bodyText>
<subsectionHeader confidence="0.998652">
3.2 Ranker Features
</subsectionHeader>
<bodyText confidence="0.99997525">
Each candidate sentence in the pool is represented
by a vector of features, which fall under one of
the three categories, viz. representativeness, dif-
ficulty, and diversity. We refer to the first two
as context-independent, because they can be com-
puted independently for each sentence. Diversity
is a context-dependent feature and must be evalu-
ated in the context of an ordering of sentences.
</bodyText>
<page confidence="0.968385">
128
</page>
<figure confidence="0.486476">
Algorithm 1 Preferred ordering
OT (k) — y∗
Sg — Sg + y∗g Vg E ngr(y∗)
end for
return OT
</figure>
<subsectionHeader confidence="0.569612">
3.2.1 Domain Representativeness
</subsectionHeader>
<bodyText confidence="0.995103333333333">
Domain representativeness features gauge the de-
gree of similarity between a candidate pool sen-
tence and the seed training data. We quantify this
using an n-gram overlap measure between candi-
date sentence x and the seed corpus S defined by
Equation 1.
</bodyText>
<equation confidence="0.9734285">
min(Sng , Cn)
Cn
E xg
g∈ngr(x)
</equation>
<bodyText confidence="0.999982666666667">
xg is the number of times n-gram g occurs in x,
Sg the number of times it occurs in the seed cor-
pus, n its length in words, and Cn the count of
n-grams of length n in S. Longer n-grams that
occur frequently in the seed receive high similar-
ity scores, and vice-versa. In evaluating this fea-
ture, we only consider n-grams up to length five
that contain least one content word.
Another simple domain similarity feature we
use is sentence length. Sentences in conversational
domains are typically short, while those in web
and newswire domains run longer.
</bodyText>
<subsectionHeader confidence="0.545728">
3.2.2 Translation Difficulty
</subsectionHeader>
<bodyText confidence="0.999973326086957">
All else being equal, the selection strategy should
favor sentences that the existing SMT system finds
difficult to translate. To this end, we estimate a
confidence score for each SMT hypothesis, using
a discriminative classification framework reminis-
cent of Blatz et al. (2004). Confidence estima-
tion is treated as a binary classification problem,
where each hypothesized word is labeled “cor-
rect” or “incorrect”. Word-level reference labels
for training the classifier are obtained from Trans-
lation Edit Rate (TER) analysis, which produces
the lowest-cost alignment between the hypothe-
ses and the gold-standard references (Snover et
al., 2006). A hypothesized word is “correct” if
it aligns to itself in this alignment, and “incorrect”
otherwise.
We derive features for confidence estimation
from the phrase derivations used by the decoder in
generating the hypotheses. For each target word,
we look up the corresponding source phrase that
produced it, and use this information to compute
a number of features from the translation phrase
table and target language model (LM). These in-
clude the in-context LM probability of the target
word, the forward and reverse phrase translation
probabilities, the maximum forward and reverse
word-level lexical translation probabilities, num-
ber of competing target phrases in which the tar-
get word occurs, etc. In all, we use 11 word-level
features (independent of the active learning fea-
tures) to train the classifier in conjunction with the
abovementioned binary reference labels.
A logistic regression model is used to directly
estimate the posterior probability of the binary
word label. Thus, our confidence score is es-
sentially the probability of the word being “in-
correct”. Sentence-level confidence is computed
as the geometric average of word-level posteriors.
Confidence estimation models are trained on the
held-out development set.
We employ two additional measures of transla-
tion difficulty for active learning: (a) the number
of “unknown” words in target hypotheses caused
by untranslatable source words, and (b) the aver-
age length of source phrases in the 1-best SMT
decoder derivations.
</bodyText>
<subsectionHeader confidence="0.851986">
3.2.3 Batch Diversity
</subsectionHeader>
<bodyText confidence="0.999760142857143">
Batch diversity is evaluated in the context of an
explicit ordering of the candidate sentences. In
general, sentences that are substantially similar to
those above them in a ranked list have low diver-
sity, and vice-versa. We use content-word n-gram
overlap to measure similarity with previous sen-
tences, per Equation 2.
</bodyText>
<equation confidence="0.990427">
E nxBg
g∈ngr(b)
E n x max(Bg,1.0)
g∈ngr(b)
</equation>
<bodyText confidence="0.993886">
B represents the set of sentences ranked higher
than the candidate b, for which we wish to evalu-
ate diversity. Bg is the number of times n-gram g
</bodyText>
<equation confidence="0.995934133333333">
OT — ()
Sg count(g) Vg E ngr(S)
Dg count(g) Vg E ngr(D)
fork = 1 to |PT |do
PU +— PT — OT
y∗ +— arg max
y∈PU g∈ngr(y)
ygxDgxn
Sg + 1
sim(x, S) =
E xg x
g∈ngr(x)
(1)
d(b  |B) = 1.0 —
(2)
</equation>
<page confidence="0.974429">
129
</page>
<bodyText confidence="0.999971875">
occurs in B. Longer, previously unseen n-grams
serve to boost diversity. The first sentence in a
given ordering is always assigned unit diversity.
The coverage criterion used by the preferred or-
dering algorithm in Section 3.1 ensures good cor-
respondence between the rank of a sentence and its
diversity, i.e. higher-ranked in-domain sentences
have higher diversity, and vice-versa.
</bodyText>
<subsectionHeader confidence="0.998983">
3.3 Training the Learner
</subsectionHeader>
<bodyText confidence="0.999995682926829">
The active learner is trained on the pool training
set PT. The seed training corpus S serves as the
basis for extracting domain similarity features for
each sentence in this set. Translation difficulty fea-
tures are evaluated by decoding sentences in PT
with the seed SMT system. Finally, we compute
diversity for each sentence in PT based on its pre-
ferred order OT according to Equation 2. Learn-
ing is semi-supervised as it does not require trans-
lation references for either PT or D.
Traditional ranking algorithms such as PRank
(Crammer and Singer, 2001) work best when the
number of ranks is much smaller than the sample
size; more than one sample can be assigned the
same rank. In the active learning problem, how-
ever, each sample is associated with a unique rank.
Moreover, the dynamic range of ranks in OT is
significantly smaller than that in PE, to which the
ranking model is applied, resulting in a mismatch
between training and evaluation conditions.
We overcome these issues by re-casting the
ranking problem as a binary classification task.
The top 10% sentences in OT are assigned a “se-
lect” label, while the remaining are assigned a
contrary “do-not-select” label. The input features
are mapped to class posterior probabilities using
multi-layer perceptron (MLP) classifiers. The use
of posteriors allows us to assign a unique rank to
each candidate sentence. The best candidate sen-
tence is the one to which the classifier assigns the
highest posterior probability for the “select” la-
bel. We use one hidden layer with eight sigmoid-
activated nodes in this implementation.
Note that we actually train two MLP classi-
fiers with different sets of input features as shown
in Figure 1. Classifier C1 is trained using only
the context-independent features, whereas C2 is
trained with the full set of features including batch
diversity. These classifiers are used to implement
an incremental, greedy selection algorithm with
parallel ranking, as explained below.
</bodyText>
<equation confidence="0.883820636363636">
Algorithm 2 Incremental greedy selection
B +— ()
fork = 1 to N do
Pci +— {x E PE  |d(x  |B) = 1.0}
Pcd +— {x E PE  |d(x  |B) &lt; 1.0}
C +— C1(�cZ(Pci)) U C2(�c�(Pcd,B))
bk +— arg max C(x)
xEPE
PE+—PE—{bk}
end for
return B
</equation>
<sectionHeader confidence="0.995658" genericHeader="method">
4 Incremental Greedy Selection
</sectionHeader>
<bodyText confidence="0.999928166666667">
Traditional rank-and-select batch construction ap-
proaches choose constituent sentences indepen-
dently, and therefore cannot ensure that the cho-
sen sentences are sufficiently diverse. Our strat-
egy implements a greedy selection algorithm that
constructs each batch iteratively; the decision bk
(the sentence to fill the kth position in a batch)
depends on all previous decisions b1, · · · , bk−1.
This allows de-emphasizing sentences similar to
those that have already been placed in the batch,
while favoring samples containing previously un-
seen constructs.
</bodyText>
<subsectionHeader confidence="0.997449">
4.1 Parallel Ranking
</subsectionHeader>
<bodyText confidence="0.999751375">
We begin with an empty batch B, to which sen-
tences from the pool evaluation set PE must be
added. We then partition the sentences in PE in
two mutually-exclusive groups Pcd and Pci. The
former contains candidates that share at least one
content-word n-gram with any existing sentences
in B, while the latter consists of sentences that
do not share any overlap with them. Note that
B is empty to start with; thus, Pcd is empty and
Pci = PE at the beginning of the first iteration
of selection. The diversity feature is computed for
each sentence in Pcd based on existing selections
in B, while the context-independent features are
evaluated for sentences in both partitions.
Next, we apply C1 to Pci and C2 to Pcd and in-
dependently obtain posterior probabilities for the
“select” label for both partitions. We take the
union of class posteriors from both partitions and
select the sentence with the highest probability of
the “select” label to fill the next slot bk, corre-
sponding to iteration k, in the batch. The selected
sentence is subsequently removed from PE.
The above parallel ranking technique (Algo-
rithm 2) is applied iteratively until the batch
</bodyText>
<page confidence="0.993255">
130
</page>
<bodyText confidence="0.998599166666667">
reaches a pre-determined size N. At itera-
tion k, the remaining sentences in PE are par-
titioned based on overlap with previous selec-
tions b1, · · · , bk_1 and ranked based on the union
of posterior probabilities generated by the corre-
sponding classifiers. This ensures that sentences
substantially similar to those that have already
been selected receive a low diversity score, and are
suitably de-emphasized. Depending on the char-
acteristics of the pool, batches constructed by this
algorithm are likely more diverse than a simple
rank-and-select approach.
</bodyText>
<sectionHeader confidence="0.982229" genericHeader="evaluation">
5 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.999967833333333">
We demonstrate the effectiveness of the proposed
sentence selection algorithm by performing a set
of simulation experiments in the context of an
English-to-Pashto (E2P) translation task. We sim-
ulate a low-resource condition by using a very
small number of training sentence pairs, sampled
from the collection, to bootstrap a phrase-based
SMT system. The remainder of this parallel cor-
pus is set aside as the pool.
At each iteration, the selection algorithm picks a
fixed-size batch of source sentences from the pool.
The seed training data are augmented with the
chosen source sentences and their translations. A
new set of translation models is then estimated and
used to decode the test set. We track SMT perfor-
mance across several iterations and compare the
proposed algorithm to a random selection baseline
as well as other common selection strategies.
</bodyText>
<subsectionHeader confidence="0.990742">
5.1 Data Configuration
</subsectionHeader>
<bodyText confidence="0.999991846153846">
Our English-Pashto data originates from a two-
way collection of spoken dialogues, and thus con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-to-English (P2E)
corpus. Each sub-corpus has its own independent
training, development, and test partitions. The di-
rectional E2P training, development, and test sets
consist of 33.9k, 2.4k, and 1.1k sentence pairs, re-
spectively. The directional P2E training set con-
sists of 76.5k sentence pairs.
We obtain a seed training corpus for the simula-
tion experiments by randomly sampling 1,000 sen-
tence pairs from the directional E2P training par-
tition. The remainder of this set, and the entire re-
versed directional P2E training partition are com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered “out-of-domain” as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collec-
tion, leaving us with 99.4k candidate sentences.
</bodyText>
<subsectionHeader confidence="0.999957">
5.2 Selection Strategies
</subsectionHeader>
<bodyText confidence="0.993426333333333">
We implement the following strategies for sen-
tence selection. In all cases, we use a fixed-size
batch of 200 sentences per iteration.
</bodyText>
<listItem confidence="0.940876818181818">
• Random selection, in which source sentences
are uniformly sampled from PE.
• Similarity selection, where we choose sen-
tences that exhibit the highest content-word n-
gram overlap with S.
• Dissimilarity selection, which selects sen-
tences having the lowest degree of content-
word n-gram overlap with S.
• Active learning with greedy incremental selec-
tion, using a learner to maximize coverage by
combining various input features.
</listItem>
<bodyText confidence="0.988253333333333">
We simulate a total of 30 iterations, with the
original 1,000 sample seed corpus growing to
7,000 sentence pairs.
</bodyText>
<subsectionHeader confidence="0.999513">
5.3 Simulation Results
</subsectionHeader>
<bodyText confidence="0.9999809">
We track SMT performance at each iteration in
two ways. The first and most effective method is
to simply use an objective measure of translation
quality, such as BLEU (Papineni et al., 2001). Fig-
ure 2(a) illustrates the variation in BLEU scores
across iterations for each selection strategy. We
note that the proposed active learning strategy per-
forms significantly better at every iteration than
random, similarity, and dissimilarity-based selec-
tion. At the end of 30 iterations, the BLEU
score gained 2.46 points, a relative improvement
of 45.9%. By contrast, the nearest competitor was
the random selection baseline, whose performance
gained only 1.33 points in BLEU, a 24.8% im-
provement. Note that we tune the phrase-based
SMT feature weights using MERT (Och, 2003)
once in the beginning, and use the same weights
across all iterations. This allowed us to compare
selection methods without variations introduced
by fluctuation of the weights.
</bodyText>
<page confidence="0.99002">
131
</page>
<figure confidence="0.9805501">
0 1 2 9 4 5 6 7 8 9 10 11 12 19 14 15 16 17 18 19 20 21 22 29 24 25 26 27 28 29 30
Its reon
(a) Trajectory of BLEU
0 1 2 3 4 5 5 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Iteratbn
(b) Trajectory of untranslated word ratio
Ra rborn solsction
— 0-- Mo st similar
—A—Most dissi mi kmr
— E—Act&apos;ve lea m ing
8.50
8.00
7.50
7.00
5.50
6.00
5.50
5.00
10%
Random sebotbn
— a— Most s mi
— 311— Most dissimilar
—M—Actiue barring
9%
8%
7%
4%
3%
2%
120.0% 120.0,
100.0% 100
• Diversty
,• UniqUeness
F-
80.0, 000%
2 60.0%
20.0%
Random selection Most similar Vest cirsirril r laarnin Random Melee Moe similar Meal direrriter Active learning
Sere:ton ategy %electron strategy
(c) Directionality match (d) Diversity/Uniqueness
</figure>
<figureCaption confidence="0.99473">
Figure 2: Simulation results for data selection. Batch size at each iteration is 200 sentences.
</figureCaption>
<page confidence="0.990759">
132
</page>
<bodyText confidence="0.999905078431373">
The second method measures test set coverage
in terms of the proportion of untranslated words
in the SMT hypotheses, which arise due to the
absence of appropriate in-context phrase pairs in
the training data. Figure 2(b) shows the varia-
tion in this measure for the four selection tech-
niques. Again, the proposed active learning algo-
rithm outperforms its competitors across nearly all
iterations, with very large improvements in the ini-
tial stages. Overall, the proportion of untranslated
words dropped from 8.74% to 2.28% after 30 iter-
ations, while the closest competitor (dissimilarity
selection) dropped to 2.59%.
It is also instructive to compare the distribu-
tion of the 6,000 sentences selected by each strat-
egy at the end of the simulation to determine
whether they came from the “in-domain” E2P
set or the “out-of-domain” P2E collection. Fig-
ure 2(c) demonstrates that only 1.3% of sentences
were selected from the reversed P2E set by the
proposed active learning strategy. On the other
hand, 70.9% of the sentences selected by the
dissimilarity-based technique came from the P2E
collection, explaining its low BLEU scores on the
E2P test set. Surprisingly, similarity selection also
chose a large fraction of sentences from the P2E
collection; this was traced to a uniform distribu-
tion of very common sentences (e.g. “thank you”,
“okay”, etc.) across the E2P and P2E sets.
Figure 2(d) compares the uniqueness and over-
all n-gram diversity of the 6,000 sentences chosen
by each strategy. The similarity selector received
the lowest score on this scale, explaining the lack
of improvement in coverage as measured by the
proportion of untranslated words in the SMT hy-
potheses. Again, the proposed approach exhibits
the highest degree of uniqueness, underscoring its
value in lowering batch redundancy.
It is interesting to note that dissimilarity selec-
tion is closest to the proposed active learning strat-
egy in terms of coverage, and yet exhibits the
worst BLEU scores. This confirms that, while
there is overlap in their vocabularies, the E2P and
P2E sets differ significantly in terms of longer-
span constructs that influence SMT performance.
These results clearly demonstrate the power
of the proposed strategy in choosing diverse, in-
domain sentences that not only provide superior
performance in terms of BLEU, but also improve
coverage, leading to fewer untranslated concepts
in the SMT hypotheses.
</bodyText>
<sectionHeader confidence="0.995585" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.99994142">
Rapid development of SMT systems for resource-
poor language pairs requires judicious use of hu-
man translation capital. We described a novel ac-
tive learning strategy that automatically learns to
pick, from a large monolingual pool, sentences
that maximize in-domain coverage. In conjunc-
tion with their translations, they are expected to
improve SMT performance at a significantly faster
rate than existing selection techniques.
We introduced two key ideas that distinguish
our approach from previous work. First, we uti-
lize a sample of the candidate pool, rather than an
additional in-domain development set, to learn the
mapping between the features and the sentences
that maximize coverage. This removes the restric-
tion that the pool be derived from the target do-
main distribution; it can be an arbitrary collection
of in- and out-of-domain sentences.
Second, we construct batches using an incre-
mental, greedy selection strategy with parallel
ranking, instead of a traditional batch rank-and-
select approach. This reduces redundancy, allow-
ing more concepts to be covered in a given batch,
and making better use of available resources.
We showed through simulation experiments that
the proposed strategy selects diverse batches of
high-impact, in-domain sentences that result in a
much more rapid improvement in translation per-
formance than random and dissimilarity-based se-
lection. This is reflected in objective indicators of
translation quality (BLEU), and in terms of cover-
age as measured by the proportion of untranslated
words in SMT hypotheses. We plan to evaluate
the scalability of our approach by running simu-
lations on a number of additional language pairs,
domains, and corpus sizes.
An issue with iterative active learning in gen-
eral is the cost of re-training the SMT system for
each batch. Small batches provide for smooth per-
formance trajectories and better error recovery at
an increased computational cost. We are currently
investigating incremental approaches that allow
SMT models to be updated online with minimal
performance loss compared to full re-training.
Finally, there is no inherent limitation in the
proposed framework that ties it to a phrase-based
SMT system. With suitable modifications to the
input feature set, it can be adapted to work with
various SMT architectures, including hierarchical
and syntax-based systems.
</bodyText>
<page confidence="0.998793">
133
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999490529411765">
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In COLING ’04:
Proceedings of the 20th international conference on
Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koby Crammer and Yoram Singer. 2001. Pranking
with ranking. In Advances in Neural Information
Processing Systems 14, pages 641–647. MIT Press.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based in N-gram frequency and TF-IDF. In
Proceedings ofIWSLT, Pittsburgh, PA, October.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28(2-3):133–168.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL ’09: Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 415–423, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ’02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223–231, August.
</reference>
<page confidence="0.99862">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.709366">
<title confidence="0.999706">A Semi-Supervised Batch-Mode Active Learning Strategy for Statistical Machine Translation</title>
<author confidence="0.886372">Sankaranarayanan Ananthakrishnan</author>
<author confidence="0.886372">Rohit Prasad</author>
<author confidence="0.886372">David Stallard</author>
<author confidence="0.886372">Prem</author>
<affiliation confidence="0.816334">BBN</affiliation>
<address confidence="0.9428165">10 Moulton Cambridge, MA,</address>
<abstract confidence="0.99974168">The availability of substantial, in-domain parallel corpora is critical for the development of high-performance statistical machine translation (SMT) systems. Such corpora, however, are expensive to produce due to the labor intensive nature of manual translation. We propose to alleviate this problem with a novel, semibatch-mode learning strategy that attempts to maximize indomain coverage by selecting sentences, which represent a balance between domain match, translation difficulty, and batch diversity. Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outperforms the random selection baseline, but also traditional active learning techniques based on dissimilarity to existing training data. Our approach achieves a relative improvement of 45.9% in BLEU over the seed baseline, while the closest competitor gained only 24.8% with the same number of selected sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>315</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12540" citStr="Blatz et al. (2004)" startWordPosition="1963" endWordPosition="1966">ty scores, and vice-versa. In evaluating this feature, we only consider n-grams up to length five that contain least one content word. Another simple domain similarity feature we use is sentence length. Sentences in conversational domains are typically short, while those in web and newswire domains run longer. 3.2.2 Translation Difficulty All else being equal, the selection strategy should favor sentences that the existing SMT system finds difficult to translate. To this end, we estimate a confidence score for each SMT hypothesis, using a discriminative classification framework reminiscent of Blatz et al. (2004). Confidence estimation is treated as a binary classification problem, where each hypothesized word is labeled “correct” or “incorrect”. Word-level reference labels for training the classifier are obtained from Translation Edit Rate (TER) analysis, which produces the lowest-cost alignment between the hypotheses and the gold-standard references (Snover et al., 2006). A hypothesized word is “correct” if it aligns to itself in this alignment, and “incorrect” otherwise. We derive features for confidence estimation from the phrase derivations used by the decoder in generating the hypotheses. For ea</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 315, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Pranking with ranking.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14,</booktitle>
<pages>641--647</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16087" citStr="Crammer and Singer, 2001" startWordPosition="2540" endWordPosition="2543">ences have higher diversity, and vice-versa. 3.3 Training the Learner The active learner is trained on the pool training set PT. The seed training corpus S serves as the basis for extracting domain similarity features for each sentence in this set. Translation difficulty features are evaluated by decoding sentences in PT with the seed SMT system. Finally, we compute diversity for each sentence in PT based on its preferred order OT according to Equation 2. Learning is semi-supervised as it does not require translation references for either PT or D. Traditional ranking algorithms such as PRank (Crammer and Singer, 2001) work best when the number of ranks is much smaller than the sample size; more than one sample can be assigned the same rank. In the active learning problem, however, each sample is associated with a unique rank. Moreover, the dynamic range of ranks in OT is significantly smaller than that in PE, to which the ranking model is applied, resulting in a mismatch between training and evaluation conditions. We overcome these issues by re-casting the ranking problem as a binary classification task. The top 10% sentences in OT are assigned a “select” label, while the remaining are assigned a contrary </context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Koby Crammer and Yoram Singer. 2001. Pranking with ranking. In Advances in Neural Information Processing Systems 14, pages 641–647. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Low cost portability for statistical machine translation based in N-gram frequency and TF-IDF.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWSLT,</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="2562" citStr="Eck et al. (2005)" startWordPosition="370" endWordPosition="373"> Within an active learning framework, this can be cast as a data selection problem. The goal is to choose, for manual translation, the most informative instances from a large pool of source language sentences. The resulting sentence pairs, in combination with any existing in-domain seed parallel corpus, are expected to provide a significantly higher performance gain than a naive random selection strategy. This process is repeated until a certain level of performance is attained. Previous work on active learning for SMT has focused on unsupervised dissimilarity measures for sentence selection. Eck et al. (2005) describe a selection strategy that attempts to maximize coverage by choosing sentences with the highest proportion of previously unseen n-grams. However, if the pool is not completely in-domain, this strategy may select irrelevant sentences, whose translations are unlikely to improve performance on an in-domain test set. They also propose a technique, based on TF-IDF, to de-emphasize sentences similar to those that have already been selected. However, this strategy is bootstrapped by random initial choices that do not necessarily favor sentences that are difficult to translate. Finally, they </context>
</contexts>
<marker>Eck, Vogel, Waibel, 2005</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Low cost portability for statistical machine translation based in N-gram frequency and TF-IDF. In Proceedings ofIWSLT, Pittsburgh, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="6005" citStr="Freund et al., 1997" startWordPosition="881" endWordPosition="884">n representativeness, translation confidence, and batch diversity. The proposed approach includes a greedy, incremental batch selection strategy, which encourages diversity and reduces redundancy. The following sections detail our active learning approach, including the experimental setup and simulation results that clearly demonstrate its effectiveness. 2 Active Learning Paradigm Active learning has been studied extensively in the context of multi-class labeling problems, and theoretically optimal selection strategies have been identified for simple classification tasks with metric features (Freund et al., 1997). However, natural language applications such as SMT present a significantly higher level of complexity. For instance, SMT model parameters (translation rules, language model n-grams, etc.) are not fixed in number or type, and vary depending on the training instances. This gives rise to the concept of domain. Even large quantities of out-of-domain training data usually do not improve translation performance. As we will see, this causes simple active selection techniques based on dissimilarity or translation difficulty to be ineffective, because they tend to favor out-of-domain sentences. Our p</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Maxim Roy</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>415--423</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3285" citStr="Haffari et al. (2009)" startWordPosition="484" endWordPosition="487">st proportion of previously unseen n-grams. However, if the pool is not completely in-domain, this strategy may select irrelevant sentences, whose translations are unlikely to improve performance on an in-domain test set. They also propose a technique, based on TF-IDF, to de-emphasize sentences similar to those that have already been selected. However, this strategy is bootstrapped by random initial choices that do not necessarily favor sentences that are difficult to translate. Finally, they work exclusively with the source language and do not use any SMT-derived features to guide selection. Haffari et al. (2009) propose a number of features, such as similarity to the seed corpus, translation probability, relative frequencies of n-grams and “phrases” in the seed vs. pool data, etc., for active learning. While many of their experiments use the above features independently to compare their relative efficacy, one of their experiments attempts to predict a rank, as a linear combination of these features, for each candidate sentence. The top-ranked sentences are chosen for manual translation. The latter strategy is particularly relevant to this paper, because the goal of our active 126 Proceedings of the F</context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari, Maxim Roy, and Anoop Sarkar. 2009. Active learning for statistical phrase-based machine translation. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 415–423, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9436" citStr="Koehn et al., 2003" startWordPosition="1435" endWordPosition="1438">es MLP Classifiers Classifier targets Trans. difficulty Pool training Domain match Diversity C 1 C 2 Preferred order Devel. set Figure 1: Flow-diagram of the active learner. 3 Active Learning Architecture Figure 1 illustrates the proposed active learning architecture in the form of a high-level flowdiagram. We begin by randomly sampling a small fraction of the large monolingual pool P to create a pool training set PT, which is used to train the learner. The remainder, which we call the pool evaluation set PE, is set aside for active selection. We also train an initial phrase-based SMT system (Koehn et al., 2003) with the available seed corpus. The pool training set PT, in conjunction with the seed corpus S, initial SMT system, and heldout development set D, is used to derive a number of input features as well as target labels for training two parallel classifiers. 3.1 Preferred Ordering The learner must be able to map input features to an ordering of the pool sentences that attempts to maximize coverage on an unseen test set. We teach it to do this by providing it with an ordering of PT that incrementally maximizes source coverage on D. This preferred ordering algorithm incrementally maps sentences i</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23712" citStr="Och, 2003" startWordPosition="3779" endWordPosition="3780">as BLEU (Papineni et al., 2001). Figure 2(a) illustrates the variation in BLEU scores across iterations for each selection strategy. We note that the proposed active learning strategy performs significantly better at every iteration than random, similarity, and dissimilarity-based selection. At the end of 30 iterations, the BLEU score gained 2.46 points, a relative improvement of 45.9%. By contrast, the nearest competitor was the random selection baseline, whose performance gained only 1.33 points in BLEU, a 24.8% improvement. Note that we tune the phrase-based SMT feature weights using MERT (Och, 2003) once in the beginning, and use the same weights across all iterations. This allowed us to compare selection methods without variations introduced by fluctuation of the weights. 131 0 1 2 9 4 5 6 7 8 9 10 11 12 19 14 15 16 17 18 19 20 21 22 29 24 25 26 27 28 29 30 Its reon (a) Trajectory of BLEU 0 1 2 3 4 5 5 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Iteratbn (b) Trajectory of untranslated word ratio Ra rborn solsction — 0-- Mo st similar —A—Most dissi mi kmr — E—Act&apos;ve lea m ing 8.50 8.00 7.50 7.00 5.50 6.00 5.50 5.00 10% Random sebotbn — a— Most s mi — 311— Most di</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23133" citStr="Papineni et al., 2001" startWordPosition="3688" endWordPosition="3691">xhibit the highest content-word ngram overlap with S. • Dissimilarity selection, which selects sentences having the lowest degree of contentword n-gram overlap with S. • Active learning with greedy incremental selection, using a learner to maximize coverage by combining various input features. We simulate a total of 30 iterations, with the original 1,000 sample seed corpus growing to 7,000 sentence pairs. 5.3 Simulation Results We track SMT performance at each iteration in two ways. The first and most effective method is to simply use an objective measure of translation quality, such as BLEU (Papineni et al., 2001). Figure 2(a) illustrates the variation in BLEU scores across iterations for each selection strategy. We note that the proposed active learning strategy performs significantly better at every iteration than random, similarity, and dissimilarity-based selection. At the end of 30 iterations, the BLEU score gained 2.46 points, a relative improvement of 45.9%. By contrast, the nearest competitor was the random selection baseline, whose performance gained only 1.33 points in BLEU, a 24.8% improvement. Note that we tune the phrase-based SMT feature weights using MERT (Och, 2003) once in the beginnin</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="12907" citStr="Snover et al., 2006" startWordPosition="2016" endWordPosition="2019"> selection strategy should favor sentences that the existing SMT system finds difficult to translate. To this end, we estimate a confidence score for each SMT hypothesis, using a discriminative classification framework reminiscent of Blatz et al. (2004). Confidence estimation is treated as a binary classification problem, where each hypothesized word is labeled “correct” or “incorrect”. Word-level reference labels for training the classifier are obtained from Translation Edit Rate (TER) analysis, which produces the lowest-cost alignment between the hypotheses and the gold-standard references (Snover et al., 2006). A hypothesized word is “correct” if it aligns to itself in this alignment, and “incorrect” otherwise. We derive features for confidence estimation from the phrase derivations used by the decoder in generating the hypotheses. For each target word, we look up the corresponding source phrase that produced it, and use this information to compute a number of features from the translation phrase table and target language model (LM). These include the in-context LM probability of the target word, the forward and reverse phrase translation probabilities, the maximum forward and reverse word-level le</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings AMTA, pages 223–231, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>