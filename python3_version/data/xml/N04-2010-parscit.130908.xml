<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018731">
<title confidence="0.9505805">
Speaker Recognition with Mixtures of Gaussians with Sparse Regression
Matrices
</title>
<author confidence="0.955515">
Constantinos Boulis
</author>
<affiliation confidence="0.8789555">
University of Washington,
Electrical Engineering dept., SSLI Lab
</affiliation>
<email confidence="0.99602">
boulis@ee.washington.edu
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895125">
When estimating a mixture of Gaussians there
are usually two choices for the covariance type
of each Gaussian component. Either diag-
onal or full covariance. Imposing a struc-
ture though may be restrictive and lead to de-
graded performance and/or increased compu-
tations. In this work, several criteria to esti-
mate the structure of regression matrices of a
mixture of Gaussians are introduced and eval-
uated. Most of the criteria attempt to estimate
a discriminative structure, which is suited for
classification tasks. Results are reported on
the 1996 NIST speaker recognition task and
performance is compared with structural EM,
a well-known, non-discriminative, structure-
finding algorithm.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999554543478261">
Most state-of-the-art systems in speech and speaker
recognition use mixtures of Gaussians when fitting a
probability distribution to data. Reasons for this choice
are the easily implementable estimation formulas and the
modeling power of mixtures of Gaussians. For example,
a mixture of diagonal Gaussians can still model depen-
dencies on the global level. An established practice when
applying mixtures of Gaussians is to use either full or di-
agonal covariances. However, imposing a structure may
not be optimum and a more general methodology should
allow for joint estimation of both the structure and pa-
rameter values.&apos;.
The first question we have to answer is what type of
structure we want to estimate. For mixtures of Gaussians
there are three choices. Covariances, inverse covariances
or regression matrices. For all cases, we can see as se-
lecting a structure by introducing zeros in the respective
matrix. The three structures are distinctively different and
zeros in one matrix do not, in general, map to zeros in an-
other matrix. For example, we can have sparse covariance
&apos;Here, we describe the Maximum Likelihood estimation
methodology for both structure and parameters. One alterna-
tive is Bayesian estimation.
but full inverse covariance or sparse inverse covariance
and full regression matrix.
There are no clear theoretical reasons why one choice
of structure is more suitable than others. However, in-
troducing zeros in the inverse covariance can be seen as
deleting arcs in an Undirected Graphical Model (UGM)
where each node represents each dimension of a single
Gaussian (Bilmes, 2000). Similarly, introducing zeros in
the regression matrix can be seen as deleting arcs in a Di-
rected Graphical Model (DGM). There is a rich body of
work on structure learning for UGM and DGM and there-
fore the view of a mixture of Gaussians as a mixture of
DGM or UGM may be advantageous. Under the DGM
framework, the problem of Gaussian parameter estima-
tion can be cast as a problem of estimating linear regres-
sion coefficients. Since the specific problem of selecting
features for linear regression has been encountered in dif-
ferent fields in the past, we adopt the view of a mixture
of Gaussians as a mixture of DGM.
In (Bilmes, 2000), the problem of introducing zeros in
regression matrices of a mixture of Gaussians was pre-
sented. The approach taken was to set to zero the pairs
with the lowest mutual information, i.e. bm = 0
</bodyText>
<equation confidence="0.570516">
i,j ⇐⇒
I(Xi, Xj) ≈ 0, where m is the Gaussian index and bi,j is
the (i, j) element of regression matrix B. The approach
</equation>
<bodyText confidence="0.999850111111111">
was tested for the task of speech recognition in a lim-
ited vocabulary corpus and was shown to offer the same
performance with the mixture of full-covariance Gaus-
sians with 30% less parameters. full covariances. One
issue with the work in (Bilmes, 2000) is that the structure-
estimation criterion that was used was not discriminative.
For classification tasks, like speaker or speech recog-
nition, discriminative parameter estimation approaches
achieve better performance than generative ones, but are
in general hard to estimate especially for a high num-
ber of classes. In this work, a number of discrimina-
tive structure-estimation criteria tailored for the task of
speaker recognition are introduced. We avoid the com-
plexities of discriminative parameter estimation by esti-
mating a discriminative structure and then applying gen-
erative parameter estimation techniques. Thus, overall
the models attempt to model the discriminability between
classes without the numerical and implementation diffi-
</bodyText>
<equation confidence="0.853173">
X1 X2 X3 X4
</equation>
<figureCaption confidence="0.998674">
Figure 1: A Directed Graphical Model
</figureCaption>
<bodyText confidence="0.977285071428571">
culties that such techniques have. A comparison of the
new discriminative structure-estimation criteria with the
structural EM algorithm is also presented.
This paper is structured as follows. In section 2, the
view of a Gaussian as a directed graphical model is
presented. In section 3, discriminative and generative
structure-estimation criteria for the task of speaker recog-
nition are detailed, along with a description of the struc-
tural EM algorithm. In section 4, the application task is
described and the experiments are presented. Finally, in
section 5, a summary and possible connections of this
work with the speaker adaptation problem are discussed.
2 Gaussians as Directed Graphical Models
Suppose that we have a mixture of M Gaussians:
</bodyText>
<equation confidence="0.997249">
M
p(x) = p(z = m)N(x; µm, Em) (1)
m
</equation>
<bodyText confidence="0.9868763">
It is known from linear algebra that any square ma-
trix A can be decomposed as A = LDU, where L is a
lower triangular matrix, D is a diagonal matrix and U is
an upper triangular matrix. In the special case where A
is also symmetric and positive definite the decomposition
becomes A = UT DU where U is an upper triangular
matrix with ones in the main diagonal. Therefore we can
write U = I − B with bij = 0 if i &gt;= j.
The exponent of the Gaussian function can now be
written as (Bilmes, 2000):
</bodyText>
<equation confidence="0.982951833333333">
(˜x − B˜x)T D(˜x − B˜x) (2)
where x˜ = x − µ. The i-th element of (˜x − B˜x) can be
written as:
V
˜xi − E bi,k˜xk (3)
k=i+1
</equation>
<bodyText confidence="0.996332128205128">
with V being the dimensionality of each vector. Equation
3 shows that the problem of Gaussian parameter estima-
tion can be casted as a linear regression problem. Regres-
sion schemes can be represented as Directed Graphical
Models. In fact, the multivariate Gaussian can be rep-
resented as a DGM as shown in Figure 1. Absent arcs
represent zeros in the regression matrix. For example the
B matrix in Figure 1 would have b1,4 = b2,3 = 0.
We can use the EM algorithm to estimate the param-
eters of a mixture of Gaussian θ = [µmBmDm]. This
formulation offers a number of advantages over the tra-
ditional formulation with means and covariances. First,
it avoids inversion of matrices and instead solves V + 1
linear systems of d equations each where d = 1 : V + 1.
If the number of components and dimensionality of input
vectors are high, as it is usually the case in speech recog-
nition applications, the amount of computations saved
can be important. Second, the number of computations
scale down with the number of regression coefficients
set to zero. This is not true in the traditional formu-
lation because introducing zeros in the covariance ma-
trix may result in a non-positive definite matrix and it-
erative techniques should be used to guarantee consis-
tency (Dempster, 1972). Third, for the traditional for-
mulation, adapting a mixture of non-diagonal Gaussians
with linear transformations leads to objective functions
that cannot be maximized analytically. Instead, iterative
maximization techniques, such as gradient descent, are
used. With the new formulation even with arbitrary Gaus-
sians, closed-form update equations are possible. Finally,
the new formulation offers flexibility in tying mecha-
nisms. Regression matrices Bm and variances Dm can
be tied in different ways, for example all the components
can share the same regression matrix but estimate a differ-
ent variance diagonal matrix for each component. Simi-
lar schemes were found to be succesful for speech recog-
nition (Gales, 1999) and this formulation can provide a
model that can extend such tying schemes. The advan-
tages of the new formulation are summarized in Table 1.
</bodyText>
<sectionHeader confidence="0.96809" genericHeader="method">
3 Structure Learning
</sectionHeader>
<bodyText confidence="0.999586608695652">
In general, structure learning in DGM is an NP-hard
problem even when all the variables are observed (Chick-
ering et al., 1994). Our case is further complicated by
the fact that we have a hidden variable (the Gaussian in-
dex). Optimum structure-finding algorithms, like the one
in (Meila and Jordan, 2000) assume a mixture of trees
and therefore are making limiting assumptions about the
space of possible structures. In this paper, no prior as-
sumptions about the space ofpossible structures are made
but this leads to absence of guarantee for an optimum
structure. Two approaches for structure-learning are in-
troduced.
The first approach is to learn a discriminative struc-
ture, i.e. a structure that can discriminate between classes
even though the parameters are estimated in an ML fash-
ion. The algorithm starts from the fully connected model
and deletes arcs, i.e. sets bi,j m = 0 d m = 1 : M (M is the
number of Gaussian components in a mixture). After set-
ting regression coefficients to zero, maximum likelihood
parameter estimation of the sparse mixture is employed.
A number of different structure-estimation criteria were
tested for the speaker recognition task (at the right of each
equation a shorthand for each criterion is defined):
</bodyText>
<table confidence="0.998666133333333">
θ = [µm, Bm, Dm] θ = [µm, Em]
Computations • Each component m=1:M re- • Each component m=1:M re-
quires solution of V+1 linear quires an inversion of a rank V
systems of d equations each, matrix.
d=1:V+1. • Iterative techniques must be
• Computations scale down with employed for the sparse case.
the number of regression coef-
ficients set to zero.
Adaptation Easy EM equations for estimating a Gradient descent techniques for the
linear transformation of a mixture of M-step of EM algorithm for estimat-
arbitrary Gaussians. ing a linear transformation of a mix-
ture of arbitrary Gaussians.
Tying More flexible tying mechanisms Components can share the entire E.
across components, i.e. components
can share B but estimate Dm.
</table>
<tableCaption confidence="0.999953">
Table 1: Comparison between viewing a mixture of Gaussians as a mixture of DGMs and the traditional representation
</tableCaption>
<equation confidence="0.998925444444444">
MI = I(Xi; Xj|target speaker s) (4)
DMIimp = I(Xi; Xj|target speaker s)−
�
(1/N) I(Xi; Xj|impostor n) (5)
n
MIimp = � I(Xi; Xj|impostor n) (6)
n
DMIconf = I(Xi; Xj|target speaker s)−
I(Xi; Xj|target speaker k) (7)
</equation>
<bodyText confidence="0.999994234375">
where I(Xi; Xj) is the mutual information between el-
ements Xi and Xj of input vector X. The mutual in-
formations are estimated by first fitting a mixture of 30
diagonal Gaussians and then applying the methods de-
scribed in (Bilmes, 1999). All but MI and MIimp are
discriminative criteria and all are based on finding the
pairs (i, j) with the lowest values and zeroing the respec-
tive regression coefficients, for every component of the
mixture. MIimp assigns the same speaker-independent
structure for all speakers. For DMIconf target speaker
k is the most confusable for target speaker s in terms of
hits, i.e. when the truth is s, speaker k fires more than
any other target speaker. We can see that different criteria
aim at different goals. MI attempts to avoid the overfit-
ting problem by zeroing regression coefficients between
least marginally dependent feature elements. DMIimp
attempts to discriminate against impostors, MIimp at-
tempts to build a speaker-independent structure which
will be more robustly estimated since there are more data
to estimate the mutual informations and DMIconf at-
tempts to discriminate against the most confusable target
speaker. The most confusable target speaker k for a given
target speaker s should be determined from an indepen-
dent held-out set.
There are three main drawbacks that are shared by all
of the above criteria. First, they are limited by the fact
that all Gaussians will have the same structure. Second,
since we are estimating sparse regression matrices, it is
known that the absence of an arc is equivalent to con-
ditional independencies, yet the above criteria can only
test for marginal independencies. Third, we introduce
another free parameter (the number of regression coef-
ficients to be set to zero) which can be determined from
a held-out set but will require time consuming trial and
error techniques. Nevertheless, they may lead to better
discrimination between speakers.
The second approach we followed was one based on
an ML fashion which may not be optimum for classi-
fication tasks, but can assign a different structure for
each component. We used the structural EM (Friedman,
1997), (Thiesson et al., 1998) and adopt it for the case of
mixtures of Gaussians. Structural EM is an algorithm that
generalizes on the EM algorithm by searching in the com-
bined space of structure and parameters. One approach
to the problem of structure finding would be to start from
the full model, evaluate every possible combination of
arc removals in every Gaussian, and pick the ones with
the least decrease in likelihood. Unfortunately, this ap-
proach can be very expensive since every time we remove
an arc on one of the Gaussians we have to re-estimate all
the parameters, so the EM algorithm must be used for
each combination. Therefore, this approach alternates
parameter search with structure search and can be very
expensive even if we follow greedy approaches. On the
other hand, structural EM interleaves parameter search
with structure search. Instead of following the sequence
Estep → Mstep → structure search, structural EM fol-
lows Estep → structure search → Mstep. By treating
expected data as observed data, the scoring of likelihood
decomposes and therefore local changes do not influence
the likelihood on other parameters. In essence, structural
EM has the same core idea as standard EM. If M is the
structure, Θ are the parameters and n is the iteration in-
dex, then the naive approach would be to do:
</bodyText>
<equation confidence="0.989443333333333">
{Mn, Θn} → {Mn+1, Θn+1} (8)
On the other hand, structural EM follows the sequence:
{Mn, Θn} → {Mn+1, Θn} → {Mn+1, Θn+1} (9)
</equation>
<bodyText confidence="0.999971083333333">
If we replace M with H, i.e. the hidden variables or
sufficient statistics, we will recognize the sequence of
steps as the standard EM algorithm. For a more thor-
ough discussion of structural EM, the reader is referred
to (Friedman, 1997). The paper in (Friedman, 1997) has
a general discussion on the structural EM algorithm for an
arbitrary graphical model. In this paper, we introduced a
greedy pruning algorithm with step size K for mixtures
of Gaussians. The algorithm is summarized in Table 2.
One thing to note about the scoring criterion is that it is
local, i.e. zeroing regression coefficient m, i, j will not
involve computations on other parameters.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999723269230769">
We evaluated our approach in the male subset of the 1996
NIST speaker recognition task (Przybocki and Martin,
1998). The problem can be described as following. Given
21 target speakers, perform 21 binary classifications (one
for each target speaker) for each one of the test sentences.
Each one of the binary classifications is a YES if the sen-
tence belongs to the target speaker and NO otherwise.
Under this setting, one sentence may be decided to have
been generated by more than one speaker, in which case
there will be at least one false alarm. Also, some of the
test sentences were spoken by non-target speakers (im-
postors) in which case the correct answer would be 21
NO. All speakers are male and the data are from the
Switchboard database (Godfrey et al., 1992). There are
approximately 2 minutes of training data for each target
speaker. All the training data for a speaker come from
the same session and the testing data come from different
sessions, but from the same handset type and phone num-
ber (matched conditions). The algorithms were evaluated
on sentence sizes of three and thirty seconds. The fea-
tures are 20-dimensional MFCC vectors, cepstrum mean
normalized and with all silences and pauses removed. In
the test data there are impostors who don’t appear in the
training data and may be of different gender than the tar-
get speakers.
A mixture of Gaussians is trained on each one of the
target speakers. For impostor modeling, a separate model
is estimated for each gender. There are 43 impostors for
each gender, each impostor with 2 minutes of speech.
Same-gender speakers are pooled together and a mixture
of 100 diagonal Gaussians is estimated on each pool. Im-
postor models remained fixed for all the experiments re-
ported in this work. During testing and because some
of the impostors are of different gender than the target
speakers, each test sentence is evaluated against both im-
postor models and the one with the highest log-likelihood
is chosen. For each test sentence the log-likelihood of
each target speaker’s model is subtracted from the log-
likelihood of the best impostor model. A decision for
YES is made if the difference of the log-likelihoods is
above a threshold. Although in real operation of the sys-
tem the thresholds are parameters that need to be esti-
mated from the training data, in this evaluation the thresh-
olds are optimized for the current test set. Therefore the
results reported should be viewed as a best case scenario,
but are nevertheless useful for comparing different ap-
proaches.
The metric used in all experiments was Equal Error
Rate (EER). EER is defined as the point where the proba-
bility of false alarms is equal to the probability of missed
detections. Standard NIST software tools were used for
the evalution of the algorithms (Martin et al., 1997).
It should be noted that the number of components per
Gaussian is kept the same for all speakers. A scheme that
allowed for different number of Gaussians per speaker
did not show any gains. Also, the number of components
is optimized on the test set which will not be the case in
the real operation of the system. However, since there are
only a few discrete values for the number of components
and EER was not particularly sensitive to that parameter,
we do not view this as a major problem.
Table 3 shows the EER obtained for different base-
line systems. Each cell contains two EER numbers, the
left is for 30-second test utterances and the right for 3-
second. For the Diagonal case 35 components were
used, while for the full case 12 components were used.
The Random case corresponds to randomly zeroing
10% of the regression coefficients of a mixture of 16 com-
ponents. This particular combination of number of pa-
rameters pruned and number of components was shown
to provide the best results for a subset of the test set.
All structure-finding experiments are with the same num-
ber of components and percent of regression coefficients
pruned.
Table 4 shows the EER obtained for different baseline
Algorithm: Finding both structure and parameter values using structural EM
Start with the full model for a given number of Gaussians
while (number of pruned regression coefficients &lt; T)
</bodyText>
<equation confidence="0.529281">
E − step: Collect sufficient statistics for given structure, i.e, γm(n) = p(Zn = M|xn, Mold)
</equation>
<bodyText confidence="0.95857625">
StructureSearch: Remove one arc from a Gaussian at a time, i.e. set bm i,j= 0.
The score associated with zeroing a s i,jN˜ing/le regression coefficient is. /
Scorem,i,j = 2DimbmEn γm (n)xjn,m lxn,m − Bi ;n,m) + Dim (bi,j)2 En γm(n)˜xn,m
Order coefficients in ascending order of score. P is the set of the first K coefficients.
Set the new structure Mnew as Mnew = Mold\{P}.
M − step: Calculate the new parameters given Mnew.
This step can be followed by a number of EM iterations to obtain better parameter values.
end
</bodyText>
<tableCaption confidence="0.995703">
Table 2: The Structural EM algorithm for a mixture of Gaussians
</tableCaption>
<table confidence="0.996758">
Full Diagonal Random
6.3/10.3 5.6/9.0 6.3/10.3
</table>
<tableCaption confidence="0.8804315">
Table 3: Baseline EER, left number is for 30-second test
utterances and right number for 3-second
</tableCaption>
<bodyText confidence="0.9995342">
sparse structures. SEM is structural EM. The first col-
umn is zeroing the pairs with the minimum values of the
corresponding criterion and the second column is zeroing
the pairs with the maximum values. The second column
is more of a consistency check. If the min entry of crite-
rion A is lower than the min entry of criterion B then the
max entry of criterion A should be higher than the max
entry of criterion B. For the structural EM, pruning step
sizes of 50 and 100 were tested and no difference was
observed.
</bodyText>
<table confidence="0.99482">
min max
MI 6.3/10.0 6.6/10.6
DMIimp 5.9/9.0 6.6/10.3
MIimp 6.3/10.3 6.3/10.3
DMIconf 5.9/9.3 6.6/10.3
SEM 6.3/9.6 6.6/10.3
</table>
<tableCaption confidence="0.744114333333333">
Table 4: EER for different sparse structures, left number
is for 30 second test utterances and right number for 3-
second.
</tableCaption>
<bodyText confidence="0.99992425">
From Table 4 we can see improved results from the
full-covariance case but results are not better than the
diagonal-covariance case. All criteria appear to perform
similarly. Table 4 also shows that zeroing the regression
coefficients with the maximum of each criterion func-
tion does not lead to systems with much different perfor-
mance. Also from Table 3 we can see that randomly ze-
roing regression coefficients performs approximately the
same as taking the minimum or maximum. These num-
bers, seem to suggest that the structure of a mixture of
Gaussians is not a critical issue for speaker recognition,
at least with the current structure-estimation criteria used.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="method">
5 Summary-Future work
</sectionHeader>
<bodyText confidence="0.999969294117647">
In this work the problem of estimating sparse regression
matrices of mixtures of Gaussians was addressed. Dif-
ferent structure-estimation criteria were evaluated, both
discriminative and generative. The general problem of
finding the optimum structure of a mixture of Gaussians
has direct applications in speaker identification as well as
speech recognition.
Interesting connections can be drawn with Maximum
Likelihood Linear Regression (MLLR) speaker adapta-
tion (Leggetter and Woodland, 1995). Not surprisingly,
the estimation equations for the regression matrix bare
resemblance with the MLLR equations. However, re-
searchers have thus far barely looked into the problem of
structure-finding for speaker adaptation, focusing mostly
on parameter adaptation. An interesting new topic for
speaker adaptation could be joint structure and parameter
adaptation.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999960794871795">
J. Bilmes. 1999. Natural Statistical Models for Auto-
matic Speech Recognition. Ph.D. thesis, U.C. Berke-
ley, Dept. of EECS.
J. Bilmes. 2000. Factored sparse inverse covariance ma-
trices. In Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
D.M. Chickering, D. Geiger, and D.E. Heckerman. 1994.
Learning bayesian networks is NP-hard. Technical Re-
port MSR-TR-94-17, Microsoft Research.
A. P. Dempster. 1972. Covariance selection. Journal of
Biometrics, 28:157–75.
N. Friedman. 1997. Learning belief networks in the pres-
ence of missing values and hidden variables. In Proc.
14th International Conference on Machine Learning
(ICML), pages 125–133.
M. Gales. 1999. Semi-tied covariance matrices for hid-
den markov models. IEEE Transactions on Speech and
Audio Processing, 7:272–281.
J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switch-
board: Telephone speech corpus for research develop-
ment. In Proceedings ofICASSP, pages 517–520.
C. J. Leggetter and P. C. Woodland. 1995. Maximum
likelihood linear regression for speaker adaptation of
continuous density hidden markov models. Computer
Speech and Language, 9:171–185.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of Eu-
rospeech ’97, pages 1895–1898.
M. Meila and M. I. Jordan. 2000. Learning with mixtures
of trees. Journal of Machine Learning Research, 1:1–
48.
M. Przybocki and A. Martin. 1998. NIST speaker recog-
nition evaluations:1996-2001. In Proceedings of the
International Conference on Language Resources and
Evaluation (LREC), pages 331–335.
B. Thiesson, C. Meek, D. Chickering, and D. Heckerman.
1998. Learning mixtures of dag models. Technical
Report MSR-TR-97-30, Microsoft Research.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.333324">
<title confidence="0.984414">Speaker Recognition with Mixtures of Gaussians with Sparse Regression Matrices</title>
<author confidence="0.411863">Constantinos</author>
<affiliation confidence="0.9477935">University of Electrical Engineering dept., SSLI</affiliation>
<email confidence="0.999006">boulis@ee.washington.edu</email>
<abstract confidence="0.992494882352941">When estimating a mixture of Gaussians there are usually two choices for the covariance type of each Gaussian component. Either diagonal or full covariance. Imposing a structure though may be restrictive and lead to degraded performance and/or increased computations. In this work, several criteria to estimate the structure of regression matrices of a mixture of Gaussians are introduced and evaluated. Most of the criteria attempt to estimate a discriminative structure, which is suited for classification tasks. Results are reported on the 1996 NIST speaker recognition task and performance is compared with structural EM, a well-known, non-discriminative, structurefinding algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bilmes</author>
</authors>
<title>Natural Statistical Models for Automatic Speech Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>U.C. Berkeley, Dept. of EECS.</institution>
<contexts>
<context position="10574" citStr="Bilmes, 1999" startWordPosition="1749" endWordPosition="1750">ts, i.e. components can share B but estimate Dm. Table 1: Comparison between viewing a mixture of Gaussians as a mixture of DGMs and the traditional representation MI = I(Xi; Xj|target speaker s) (4) DMIimp = I(Xi; Xj|target speaker s)− � (1/N) I(Xi; Xj|impostor n) (5) n MIimp = � I(Xi; Xj|impostor n) (6) n DMIconf = I(Xi; Xj|target speaker s)− I(Xi; Xj|target speaker k) (7) where I(Xi; Xj) is the mutual information between elements Xi and Xj of input vector X. The mutual informations are estimated by first fitting a mixture of 30 diagonal Gaussians and then applying the methods described in (Bilmes, 1999). All but MI and MIimp are discriminative criteria and all are based on finding the pairs (i, j) with the lowest values and zeroing the respective regression coefficients, for every component of the mixture. MIimp assigns the same speaker-independent structure for all speakers. For DMIconf target speaker k is the most confusable for target speaker s in terms of hits, i.e. when the truth is s, speaker k fires more than any other target speaker. We can see that different criteria aim at different goals. MI attempts to avoid the overfitting problem by zeroing regression coefficients between least</context>
</contexts>
<marker>Bilmes, 1999</marker>
<rawString>J. Bilmes. 1999. Natural Statistical Models for Automatic Speech Recognition. Ph.D. thesis, U.C. Berkeley, Dept. of EECS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
</authors>
<title>Factored sparse inverse covariance matrices.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="2502" citStr="Bilmes, 2000" startWordPosition="379" endWordPosition="380"> not, in general, map to zeros in another matrix. For example, we can have sparse covariance &apos;Here, we describe the Maximum Likelihood estimation methodology for both structure and parameters. One alternative is Bayesian estimation. but full inverse covariance or sparse inverse covariance and full regression matrix. There are no clear theoretical reasons why one choice of structure is more suitable than others. However, introducing zeros in the inverse covariance can be seen as deleting arcs in an Undirected Graphical Model (UGM) where each node represents each dimension of a single Gaussian (Bilmes, 2000). Similarly, introducing zeros in the regression matrix can be seen as deleting arcs in a Directed Graphical Model (DGM). There is a rich body of work on structure learning for UGM and DGM and therefore the view of a mixture of Gaussians as a mixture of DGM or UGM may be advantageous. Under the DGM framework, the problem of Gaussian parameter estimation can be cast as a problem of estimating linear regression coefficients. Since the specific problem of selecting features for linear regression has been encountered in different fields in the past, we adopt the view of a mixture of Gaussians as a</context>
<context position="5767" citStr="Bilmes, 2000" startWordPosition="937" endWordPosition="938"> Gaussians as Directed Graphical Models Suppose that we have a mixture of M Gaussians: M p(x) = p(z = m)N(x; µm, Em) (1) m It is known from linear algebra that any square matrix A can be decomposed as A = LDU, where L is a lower triangular matrix, D is a diagonal matrix and U is an upper triangular matrix. In the special case where A is also symmetric and positive definite the decomposition becomes A = UT DU where U is an upper triangular matrix with ones in the main diagonal. Therefore we can write U = I − B with bij = 0 if i &gt;= j. The exponent of the Gaussian function can now be written as (Bilmes, 2000): (˜x − B˜x)T D(˜x − B˜x) (2) where x˜ = x − µ. The i-th element of (˜x − B˜x) can be written as: V ˜xi − E bi,k˜xk (3) k=i+1 with V being the dimensionality of each vector. Equation 3 shows that the problem of Gaussian parameter estimation can be casted as a linear regression problem. Regression schemes can be represented as Directed Graphical Models. In fact, the multivariate Gaussian can be represented as a DGM as shown in Figure 1. Absent arcs represent zeros in the regression matrix. For example the B matrix in Figure 1 would have b1,4 = b2,3 = 0. We can use the EM algorithm to estimate t</context>
</contexts>
<marker>Bilmes, 2000</marker>
<rawString>J. Bilmes. 2000. Factored sparse inverse covariance matrices. In Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Geiger</author>
<author>D E Heckerman</author>
</authors>
<title>Learning bayesian networks is NP-hard.</title>
<date>1994</date>
<tech>Technical Report MSR-TR-94-17, Microsoft Research.</tech>
<contexts>
<context position="8171" citStr="Chickering et al., 1994" startWordPosition="1343" endWordPosition="1347">ation offers flexibility in tying mechanisms. Regression matrices Bm and variances Dm can be tied in different ways, for example all the components can share the same regression matrix but estimate a different variance diagonal matrix for each component. Similar schemes were found to be succesful for speech recognition (Gales, 1999) and this formulation can provide a model that can extend such tying schemes. The advantages of the new formulation are summarized in Table 1. 3 Structure Learning In general, structure learning in DGM is an NP-hard problem even when all the variables are observed (Chickering et al., 1994). Our case is further complicated by the fact that we have a hidden variable (the Gaussian index). Optimum structure-finding algorithms, like the one in (Meila and Jordan, 2000) assume a mixture of trees and therefore are making limiting assumptions about the space of possible structures. In this paper, no prior assumptions about the space ofpossible structures are made but this leads to absence of guarantee for an optimum structure. Two approaches for structure-learning are introduced. The first approach is to learn a discriminative structure, i.e. a structure that can discriminate between cl</context>
</contexts>
<marker>Chickering, Geiger, Heckerman, 1994</marker>
<rawString>D.M. Chickering, D. Geiger, and D.E. Heckerman. 1994. Learning bayesian networks is NP-hard. Technical Report MSR-TR-94-17, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
</authors>
<title>Covariance selection.</title>
<date>1972</date>
<journal>Journal of Biometrics,</journal>
<pages>28--157</pages>
<contexts>
<context position="7162" citStr="Dempster, 1972" startWordPosition="1190" endWordPosition="1191">version of matrices and instead solves V + 1 linear systems of d equations each where d = 1 : V + 1. If the number of components and dimensionality of input vectors are high, as it is usually the case in speech recognition applications, the amount of computations saved can be important. Second, the number of computations scale down with the number of regression coefficients set to zero. This is not true in the traditional formulation because introducing zeros in the covariance matrix may result in a non-positive definite matrix and iterative techniques should be used to guarantee consistency (Dempster, 1972). Third, for the traditional formulation, adapting a mixture of non-diagonal Gaussians with linear transformations leads to objective functions that cannot be maximized analytically. Instead, iterative maximization techniques, such as gradient descent, are used. With the new formulation even with arbitrary Gaussians, closed-form update equations are possible. Finally, the new formulation offers flexibility in tying mechanisms. Regression matrices Bm and variances Dm can be tied in different ways, for example all the components can share the same regression matrix but estimate a different varia</context>
</contexts>
<marker>Dempster, 1972</marker>
<rawString>A. P. Dempster. 1972. Covariance selection. Journal of Biometrics, 28:157–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
</authors>
<title>Learning belief networks in the presence of missing values and hidden variables.</title>
<date>1997</date>
<booktitle>In Proc. 14th International Conference on Machine Learning (ICML),</booktitle>
<pages>125--133</pages>
<contexts>
<context position="12484" citStr="Friedman, 1997" startWordPosition="2058" endWordPosition="2059">ence of an arc is equivalent to conditional independencies, yet the above criteria can only test for marginal independencies. Third, we introduce another free parameter (the number of regression coefficients to be set to zero) which can be determined from a held-out set but will require time consuming trial and error techniques. Nevertheless, they may lead to better discrimination between speakers. The second approach we followed was one based on an ML fashion which may not be optimum for classification tasks, but can assign a different structure for each component. We used the structural EM (Friedman, 1997), (Thiesson et al., 1998) and adopt it for the case of mixtures of Gaussians. Structural EM is an algorithm that generalizes on the EM algorithm by searching in the combined space of structure and parameters. One approach to the problem of structure finding would be to start from the full model, evaluate every possible combination of arc removals in every Gaussian, and pick the ones with the least decrease in likelihood. Unfortunately, this approach can be very expensive since every time we remove an arc on one of the Gaussians we have to re-estimate all the parameters, so the EM algorithm mus</context>
<context position="14165" citStr="Friedman, 1997" startWordPosition="2345" endWordPosition="2346">erefore local changes do not influence the likelihood on other parameters. In essence, structural EM has the same core idea as standard EM. If M is the structure, Θ are the parameters and n is the iteration index, then the naive approach would be to do: {Mn, Θn} → {Mn+1, Θn+1} (8) On the other hand, structural EM follows the sequence: {Mn, Θn} → {Mn+1, Θn} → {Mn+1, Θn+1} (9) If we replace M with H, i.e. the hidden variables or sufficient statistics, we will recognize the sequence of steps as the standard EM algorithm. For a more thorough discussion of structural EM, the reader is referred to (Friedman, 1997). The paper in (Friedman, 1997) has a general discussion on the structural EM algorithm for an arbitrary graphical model. In this paper, we introduced a greedy pruning algorithm with step size K for mixtures of Gaussians. The algorithm is summarized in Table 2. One thing to note about the scoring criterion is that it is local, i.e. zeroing regression coefficient m, i, j will not involve computations on other parameters. 4 Experiments We evaluated our approach in the male subset of the 1996 NIST speaker recognition task (Przybocki and Martin, 1998). The problem can be described as following. Gi</context>
</contexts>
<marker>Friedman, 1997</marker>
<rawString>N. Friedman. 1997. Learning belief networks in the presence of missing values and hidden variables. In Proc. 14th International Conference on Machine Learning (ICML), pages 125–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gales</author>
</authors>
<title>Semi-tied covariance matrices for hidden markov models.</title>
<date>1999</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>7--272</pages>
<contexts>
<context position="7881" citStr="Gales, 1999" startWordPosition="1296" endWordPosition="1297">mations leads to objective functions that cannot be maximized analytically. Instead, iterative maximization techniques, such as gradient descent, are used. With the new formulation even with arbitrary Gaussians, closed-form update equations are possible. Finally, the new formulation offers flexibility in tying mechanisms. Regression matrices Bm and variances Dm can be tied in different ways, for example all the components can share the same regression matrix but estimate a different variance diagonal matrix for each component. Similar schemes were found to be succesful for speech recognition (Gales, 1999) and this formulation can provide a model that can extend such tying schemes. The advantages of the new formulation are summarized in Table 1. 3 Structure Learning In general, structure learning in DGM is an NP-hard problem even when all the variables are observed (Chickering et al., 1994). Our case is further complicated by the fact that we have a hidden variable (the Gaussian index). Optimum structure-finding algorithms, like the one in (Meila and Jordan, 2000) assume a mixture of trees and therefore are making limiting assumptions about the space of possible structures. In this paper, no pr</context>
</contexts>
<marker>Gales, 1999</marker>
<rawString>M. Gales. 1999. Semi-tied covariance matrices for hidden markov models. IEEE Transactions on Speech and Audio Processing, 7:272–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research development.</title>
<date>1992</date>
<booktitle>In Proceedings ofICASSP,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="15372" citStr="Godfrey et al., 1992" startWordPosition="2549" endWordPosition="2552"> following. Given 21 target speakers, perform 21 binary classifications (one for each target speaker) for each one of the test sentences. Each one of the binary classifications is a YES if the sentence belongs to the target speaker and NO otherwise. Under this setting, one sentence may be decided to have been generated by more than one speaker, in which case there will be at least one false alarm. Also, some of the test sentences were spoken by non-target speakers (impostors) in which case the correct answer would be 21 NO. All speakers are male and the data are from the Switchboard database (Godfrey et al., 1992). There are approximately 2 minutes of training data for each target speaker. All the training data for a speaker come from the same session and the testing data come from different sessions, but from the same handset type and phone number (matched conditions). The algorithms were evaluated on sentence sizes of three and thirty seconds. The features are 20-dimensional MFCC vectors, cepstrum mean normalized and with all silences and pauses removed. In the test data there are impostors who don’t appear in the training data and may be of different gender than the target speakers. A mixture of Gau</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switchboard: Telephone speech corpus for research development. In Proceedings ofICASSP, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Leggetter</author>
<author>P C Woodland</author>
</authors>
<title>Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language,</title>
<date>1995</date>
<pages>9--171</pages>
<marker>Leggetter, Woodland, 1995</marker>
<rawString>C. J. Leggetter and P. C. Woodland. 1995. Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language, 9:171–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martin</author>
<author>G Doddington</author>
<author>T Kamm</author>
<author>M Ordowski</author>
<author>M Przybocki</author>
</authors>
<title>The DET curve in assessment of detection task performance.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech ’97,</booktitle>
<pages>1895--1898</pages>
<contexts>
<context position="17412" citStr="Martin et al., 1997" startWordPosition="2893" endWordPosition="2896">a threshold. Although in real operation of the system the thresholds are parameters that need to be estimated from the training data, in this evaluation the thresholds are optimized for the current test set. Therefore the results reported should be viewed as a best case scenario, but are nevertheless useful for comparing different approaches. The metric used in all experiments was Equal Error Rate (EER). EER is defined as the point where the probability of false alarms is equal to the probability of missed detections. Standard NIST software tools were used for the evalution of the algorithms (Martin et al., 1997). It should be noted that the number of components per Gaussian is kept the same for all speakers. A scheme that allowed for different number of Gaussians per speaker did not show any gains. Also, the number of components is optimized on the test set which will not be the case in the real operation of the system. However, since there are only a few discrete values for the number of components and EER was not particularly sensitive to that parameter, we do not view this as a major problem. Table 3 shows the EER obtained for different baseline systems. Each cell contains two EER numbers, the lef</context>
</contexts>
<marker>Martin, Doddington, Kamm, Ordowski, Przybocki, 1997</marker>
<rawString>A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki. 1997. The DET curve in assessment of detection task performance. In Proceedings of Eurospeech ’97, pages 1895–1898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
<author>M I Jordan</author>
</authors>
<title>Learning with mixtures of trees.</title>
<date>2000</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>1</volume>
<pages>48</pages>
<contexts>
<context position="8348" citStr="Meila and Jordan, 2000" startWordPosition="1373" endWordPosition="1376">matrix but estimate a different variance diagonal matrix for each component. Similar schemes were found to be succesful for speech recognition (Gales, 1999) and this formulation can provide a model that can extend such tying schemes. The advantages of the new formulation are summarized in Table 1. 3 Structure Learning In general, structure learning in DGM is an NP-hard problem even when all the variables are observed (Chickering et al., 1994). Our case is further complicated by the fact that we have a hidden variable (the Gaussian index). Optimum structure-finding algorithms, like the one in (Meila and Jordan, 2000) assume a mixture of trees and therefore are making limiting assumptions about the space of possible structures. In this paper, no prior assumptions about the space ofpossible structures are made but this leads to absence of guarantee for an optimum structure. Two approaches for structure-learning are introduced. The first approach is to learn a discriminative structure, i.e. a structure that can discriminate between classes even though the parameters are estimated in an ML fashion. The algorithm starts from the fully connected model and deletes arcs, i.e. sets bi,j m = 0 d m = 1 : M (M is the</context>
</contexts>
<marker>Meila, Jordan, 2000</marker>
<rawString>M. Meila and M. I. Jordan. 2000. Learning with mixtures of trees. Journal of Machine Learning Research, 1:1– 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>A Martin</author>
</authors>
<title>NIST speaker recognition evaluations:1996-2001.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>331--335</pages>
<contexts>
<context position="14718" citStr="Przybocki and Martin, 1998" startWordPosition="2434" endWordPosition="2437">gh discussion of structural EM, the reader is referred to (Friedman, 1997). The paper in (Friedman, 1997) has a general discussion on the structural EM algorithm for an arbitrary graphical model. In this paper, we introduced a greedy pruning algorithm with step size K for mixtures of Gaussians. The algorithm is summarized in Table 2. One thing to note about the scoring criterion is that it is local, i.e. zeroing regression coefficient m, i, j will not involve computations on other parameters. 4 Experiments We evaluated our approach in the male subset of the 1996 NIST speaker recognition task (Przybocki and Martin, 1998). The problem can be described as following. Given 21 target speakers, perform 21 binary classifications (one for each target speaker) for each one of the test sentences. Each one of the binary classifications is a YES if the sentence belongs to the target speaker and NO otherwise. Under this setting, one sentence may be decided to have been generated by more than one speaker, in which case there will be at least one false alarm. Also, some of the test sentences were spoken by non-target speakers (impostors) in which case the correct answer would be 21 NO. All speakers are male and the data ar</context>
</contexts>
<marker>Przybocki, Martin, 1998</marker>
<rawString>M. Przybocki and A. Martin. 1998. NIST speaker recognition evaluations:1996-2001. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 331–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thiesson</author>
<author>C Meek</author>
<author>D Chickering</author>
<author>D Heckerman</author>
</authors>
<title>Learning mixtures of dag models.</title>
<date>1998</date>
<tech>Technical Report MSR-TR-97-30, Microsoft Research.</tech>
<contexts>
<context position="12509" citStr="Thiesson et al., 1998" startWordPosition="2060" endWordPosition="2063">equivalent to conditional independencies, yet the above criteria can only test for marginal independencies. Third, we introduce another free parameter (the number of regression coefficients to be set to zero) which can be determined from a held-out set but will require time consuming trial and error techniques. Nevertheless, they may lead to better discrimination between speakers. The second approach we followed was one based on an ML fashion which may not be optimum for classification tasks, but can assign a different structure for each component. We used the structural EM (Friedman, 1997), (Thiesson et al., 1998) and adopt it for the case of mixtures of Gaussians. Structural EM is an algorithm that generalizes on the EM algorithm by searching in the combined space of structure and parameters. One approach to the problem of structure finding would be to start from the full model, evaluate every possible combination of arc removals in every Gaussian, and pick the ones with the least decrease in likelihood. Unfortunately, this approach can be very expensive since every time we remove an arc on one of the Gaussians we have to re-estimate all the parameters, so the EM algorithm must be used for each combin</context>
</contexts>
<marker>Thiesson, Meek, Chickering, Heckerman, 1998</marker>
<rawString>B. Thiesson, C. Meek, D. Chickering, and D. Heckerman. 1998. Learning mixtures of dag models. Technical Report MSR-TR-97-30, Microsoft Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>