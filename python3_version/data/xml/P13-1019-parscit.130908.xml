<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9984915">
Improved Bayesian Logistic Supervised Topic Models
with Data Augmentation
</title>
<author confidence="0.999524">
Jun Zhu, Xun Zheng, Bo Zhang
</author>
<affiliation confidence="0.950719666666667">
Department of Computer Science and Technology
TNLIST Lab and State Key Lab of Intelligent Technology and Systems
Tsinghua University, Beijing, China
</affiliation>
<email confidence="0.997403">
{dcszj,dcszb}@tsinghua.edu.cn; vforveri.zheng@gmail.com
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895666666667">
Supervised topic models with a logistic
likelihood have two issues that potential-
ly limit their practical use: 1) response
variables are usually over-weighted by
document word counts; and 2) existing
variational inference methods make strict
mean-field assumptions. We address these
issues by: 1) introducing a regularization
constant to better balance the two parts
based on an optimization formulation of
Bayesian inference; and 2) developing a
simple Gibbs sampling algorithm by intro-
ducing auxiliary Polya-Gamma variables
and collapsing out Dirichlet variables. Our
augment-and-collapse sampling algorithm
has analytical forms of each conditional
distribution without making any restrict-
ing assumptions and can be easily paral-
lelized. Empirical results demonstrate sig-
nificant improvements on prediction per-
formance and time efficiency.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919596491228">
As widely adopted in supervised latent Dirichlet
allocation (sLDA) models (Blei and McAuliffe,
2010; Wang et al., 2009), one way to improve
the predictive power of LDA is to define a like-
lihood model for the widely available document-
level response variables, in addition to the likeli-
hood model for document words. For example, the
logistic likelihood model is commonly used for bi-
nary or multinomial responses. By imposing some
priors, posterior inference is done with the Bayes’
rule. Though powerful, one issue that could limit
the use of existing logistic supervised LDA models
is that they treat the document-level response vari-
able as one additional word via a normalized like-
lihood model. Although some special treatment is
carried out on defining the likelihood of the single
response variable, it is normally of a much small-
er scale than the likelihood of the usually tens or
hundreds of words in each document. As noted
by (Halpern et al., 2012) and observed in our ex-
periments, this model imbalance could result in a
weak influence of response variables on the topic
representations and thus non-satisfactory predic-
tion performance. Another difficulty arises when
dealing with categorical response variables is that
the commonly used normal priors are no longer
conjugate to the logistic likelihood and thus lead to
hard inference problems. Existing approaches re-
ly on variational approximation techniques which
normally make strict mean-field assumptions.
To address the above issues, we present two im-
provements. First, we present a general frame-
work of Bayesian logistic supervised topic models
with a regularization parameter to better balance
response variables and words. Technically, instead
of doing standard Bayesian inference via Bayes’
rule, which requires a normalized likelihood mod-
el, we propose to do regularized Bayesian infer-
ence (Zhu et al., 2011; Zhu et al., 2013b) via solv-
ing an optimization problem, where the posterior
regularization is defined as an expectation of a l-
ogistic loss, a surrogate loss of the expected mis-
classification error; and a regularization parame-
ter is introduced to balance the surrogate classifi-
cation loss (i.e., the response log-likelihood) and
the word likelihood. The general formulation sub-
sumes standard sLDA as a special case.
Second, to solve the intractable posterior infer-
ence problem of the generalized Bayesian logis-
tic supervised topic models, we present a simple
Gibbs sampling algorithm by exploring the ideas
of data augmentation (Tanner and Wong, 1987;
van Dyk and Meng, 2001; Holmes and Held,
2006). More specifically, we extend Polson’s
method for Bayesian logistic regression (Polson
et al., 2012) to the generalized logistic supervised
topic models, which are much more challeng-
</bodyText>
<page confidence="0.968316">
187
</page>
<note confidence="0.9141735">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–195,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963642857143">
ing due to the presence of non-trivial latent vari-
ables. Technically, we introduce a set of Polya-
Gamma variables, one per document, to refor-
mulate the generalized logistic pseudo-likelihood
model (with the regularization parameter) as a s-
cale mixture, where the mixture component is con-
ditionally normal for classifier parameters. Then,
we develop a simple and efficient Gibbs sampling
algorithms with analytic conditional distribution-
s without Metropolis-Hastings accept/reject steps.
For Bayesian LDA models, we can also explore
the conjugacy of the Dirichlet-Multinomial prior-
likelihood pairs to collapse out the Dirichlet vari-
ables (i.e., topics and mixing proportions) to do
collapsed Gibbs sampling, which can have better
mixing rates (Griffiths and Steyvers, 2004). Final-
ly, our empirical results on real data sets demon-
strate significant improvements on time efficiency.
The classification performance is also significantly
improved by using appropriate regularization pa-
rameters. We also provide a parallel implementa-
tion with GraphLab (Gonzalez et al., 2012), which
shows great promise in our preliminary studies.
The paper is structured as follows. Sec. 2 intro-
duces logistic supervised topic models as a general
optimization problem. Sec. 3 presents Gibbs sam-
pling algorithms with data augmentation. Sec. 4
presents experiments. Sec. 5 concludes.
</bodyText>
<sectionHeader confidence="0.993853" genericHeader="method">
2 Logistic Supervised Topic Models
</sectionHeader>
<bodyText confidence="0.9988695">
We now present the generalized Bayesian logistic
supervised topic models.
</bodyText>
<subsectionHeader confidence="0.976881">
2.1 The Generalized Models
</subsectionHeader>
<bodyText confidence="0.9899008125">
We consider binary classification with a training
set D = {(wd, yd)}d1, where the response vari-
able Y takes values from the output space Y =
{0, 1}. A logistic supervised topic model consists
of two parts — an LDA model (Blei et al., 2003)
for describing the words W = {wd}d1, where
wd = {wdn}�d
n=1 denote the words within docu-
ment d, and a logistic classifier for considering the
supervising signal y = {yd}d1. Below, we intro-
duce each of them in turn.
LDA: LDA is a hierarchical Bayesian model
that posits each document as an admixture of K
topics, where each topic Φk is a multinomial dis-
tribution over a V-word vocabulary. For document
d, the generating process is
</bodyText>
<listItem confidence="0.99853475">
1. draw a topic proportion θd ∼ Dir(α)
2. for each word n = 1, 2, ... , Nd:
(a) draw a topics zdn ∼ Mult(θd)
(b) draw the word wdn ∼ Mult(Φzdn)
</listItem>
<bodyText confidence="0.986476352941177">
where Dir(·) is a Dirichlet distribution; Mult(·) is
a multinomial distribution; and Φzdn denotes the
topic selected by the non-zero entry of zdn. For
fully-Bayesian LDA, the topics are random sam-
ples from a Dirichlet prior, Φk ∼ Dir(β).
Let zd = {zdn}�d
n=1 denote the set of topic as-
signments for document d. Let Z = {zd}d1 and
Θ = {θd}d1 denote all the topic assignments
and mixing proportions for the entire corpus. LDA
infers the posterior distribution p(Θ, Z, Φ|W) ∝
p0(Θ, Z, Φ)p(W|Z, Φ), where p0(Θ, Z, Φ) =
(l ld p(θd|α) l ln p(zdn|θd)) l lk p(&amp;quot;Dk|β) is the
joint distribution defined by the model. As noticed
in (Jiang et al., 2012), the posterior distribution
by Bayes’ rule is equivalent to the solution of an
information theoretical optimization problem
</bodyText>
<equation confidence="0.989583">
KL(q(O, Z, Φ)l1p0(O, Z, Φ))−Eq[lob p(W|Z, Φ)]
s.t.: q(O, Z, Φ) E P, (1)
</equation>
<bodyText confidence="0.998895">
where KL(q||p) is the Kullback-Leibler diver-
gence from q to p and P is the space of probability
distributions.
Logistic classifier: To consider binary super-
vising information, a logistic supervised topic
model (e.g., sLDA) builds a logistic classifier
using the topic representations as input features
</bodyText>
<equation confidence="0.9636264">
p(y = 1|η, z) = exp(η⊤2) (2)
1 + exp(η⊤�z)
EN
where z� is a K-vector with �zk = 1 n=1 ff(zkn =
�
</equation>
<bodyText confidence="0.8190255">
1), and ff(·) is an indicator function that equals to
1 if predicate holds otherwise 0. If the classifier
weights η and topic assignments z are given, the
prediction rule is
</bodyText>
<equation confidence="0.997134">
9|,7,. = H(p(y = 1|η, z) &gt; 0.5) = H(η⊤z &gt; 0). (3)
</equation>
<bodyText confidence="0.999015666666667">
Since both η and Z are hidden variables, we
propose to infer a posterior distribution q(η, Z)
that has the minimal expected log-logistic loss
</bodyText>
<equation confidence="0.995861">
R(q(η, Z)) = − � Eq[lo9 p(yd|η, zd)], (4)
</equation>
<bodyText confidence="0.9108714">
d
which is a good surrogate loss for the expected
misclassification loss, Ed EQ[ff(y|&apos;�Zd ≠ yd)], of
a Gibbs classifier that randomly draws a model
η from the posterior distribution and makes pre-
dictions (McAllester, 2003; Germain et al., 2009).
In fact, this choice is motivated from the obser-
vation that logistic loss has been widely used as
a convex surrogate loss for the misclassification
&apos;A K-binary vector with only one entry equaling to 1.
</bodyText>
<equation confidence="0.8204125">
min
q(®,Z,.b)
</equation>
<page confidence="0.990441">
188
</page>
<bodyText confidence="0.999323909090909">
loss (Rosasco et al., 2004) in the task of fully ob-
served binary classification. Also, note that the l-
ogistic classifier and the LDA likelihood are cou-
pled by sharing the latent topic assignments z. The
strong coupling makes it possible to learn a pos-
terior distribution that can describe the observed
words well and make accurate predictions.
Regularized Bayesian Inference: To integrate
the above two components for hybrid learning, a
logistic supervised topic model solves the joint
Bayesian inference problem
</bodyText>
<equation confidence="0.9461495">
L(q(η, Θ, Z, Φ)) + cR(q(η, Z)) (5)
s.t.: q(η, Θ, Z, Φ) E P,
</equation>
<bodyText confidence="0.9994065">
where L(q) = KL(q||p0(η, Θ, Z, Φ)) −
EQ[log p(W|Z,Φ)] is the objective for doing
standard Bayesian inference with the classifier
weights η; p0(η, Θ, Z, Φ) = p0(η)p0(Θ, Z, Φ);
and c is a regularization parameter balancing the
influence from response variables and words.
In general, we define the pseudo-likelihood for
the supervision information
</bodyText>
<equation confidence="0.918746">
ψ(yd|zd, η) = pc(yd|η, zd) = (1 + e(xp(ηT f d))c , (6)
</equation>
<bodyText confidence="0.994807">
which is un-normalized if c\ ≠ 1. But, as we
shall see this un-normalization does not affect
our subsequent inference. Then, the generalized
inference problem (5) of logistic supervised topic
models can be written in the “standard” Bayesian
inference form (1)
</bodyText>
<equation confidence="0.9165965">
L(q(η, Θ, Z, Φ)) − Eq[log ψ(y|Z, η)] (7)
s.t.: q(η, Θ, Z, Φ) E P,
</equation>
<bodyText confidence="0.999914">
where ψ(y|Z,η) = ∏d ψ(yd|zd, η). It is easy
to show that the optimum solution of problem
(5) or the equivalent problem (7) is the posterior
distribution with supervising information, i.e.,
</bodyText>
<equation confidence="0.9989365">
( o, Z, �) - PO(n, o, Z, 4&apos;)P(WIZ, 4,),O(Y|n, Z)
V �,
</equation>
<bodyText confidence="0.99990978125">
where ϕ(y, W) is the normalization constant to
make q a distribution. We can see that when c = 1,
the model reduces to the standard sLDA, which in
practice has the imbalance issue that the response
variable (can be viewed as one additional word) is
usually dominated by the words. This imbalance
was noticed in (Halpern et al., 2012). We will see
that c can make a big difference later.
Comparison with MedLDA: The above for-
mulation of logistic supervised topic models as
an instance of regularized Bayesian inference pro-
vides a direct comparison with the max-margin
supervised topic model (MedLDA) (Jiang et al.,
2012), which has the same form of the optimiza-
tion problems. The difference lies in the posterior
regularization, for which MedLDA uses a hinge
loss of an expected classifier while the logistic su-
pervised topic model uses an expected log-logistic
loss. Gibbs MedLDA (Zhu et al., 2013a) is an-
other max-margin model that adopts the expect-
ed hinge loss as posterior regularization. As we
shall see in the experiments, by using appropriate
regularization constants, logistic supervised topic
models achieve comparable performance as max-
margin methods. We note that the relationship be-
tween a logistic loss and a hinge loss has been
discussed extensively in various settings (Rosas-
co et al., 2004; Globerson et al., 2007). But the
presence of latent variables poses additional chal-
lenges in carrying out a formal theoretical analysis
of these surrogate losses (Lin, 2001) in the topic
model setting.
</bodyText>
<subsectionHeader confidence="0.998265">
2.2 Variational Approximation Algorithms
</subsectionHeader>
<bodyText confidence="0.999939791666666">
The commonly used normal prior for η is non-
conjugate to the logistic likelihood, which makes
the posterior inference hard. Moreover, the laten-
t variables Z make the inference problem harder
than that of Bayesian logistic regression model-
s (Chen et al., 1999; Meyer and Laud, 2002; Pol-
son et al., 2012). Previous algorithms to solve
problem (5) rely on variational approximation
techniques. It is easy to show that the variation-
al method (Wang et al., 2009) is a coordinate de-
scent algorithm to solve problem (5) with the addi-
tional fully-factorized constraint q(η, O, Z, Φ) =
q(η)(∏d q(θd) ∏n q(zdn)) ∏k q(Φk) and a vari-
ational approximation to the expectation of the
log-logistic likelihood, which is intractable to
compute directly. Note that the non-Bayesian
treatment of η as unknown parameters in (Wang
et al., 2009) results in an EM algorithm, which
still needs to make strict mean-field assumptions
together with a variational bound of the expecta-
tion of the log-logistic likelihood. In this paper, we
consider the full Bayesian treatment, which can
principally consider prior distributions and infer
the posterior covariance.
</bodyText>
<sectionHeader confidence="0.994065" genericHeader="method">
3 A Gibbs Sampling Algorithm
</sectionHeader>
<bodyText confidence="0.999907">
Now, we present a simple and efficient Gibbs sam-
pling algorithm for the generalized Bayesian lo-
gistic supervised topic models.
</bodyText>
<equation confidence="0.8953608">
min
q(97,8,Z,4)
min
q(97,8,Z,4)
ϕ(y, W) .
</equation>
<page confidence="0.884111">
189
</page>
<bodyText confidence="0.550824">
,
</bodyText>
<subsectionHeader confidence="0.999574">
3.1 Formulation with Data Augmentation
</subsectionHeader>
<bodyText confidence="0.999977583333333">
Since the logistic pseudo-likelihood ψ(y|Z, η) is
not conjugate with normal priors, it is not easy
to derive the sampling algorithms directly. In-
stead, we develop our algorithms by introducing
auxiliary variables, which lead to a scale mix-
ture of Gaussian components and analytic condi-
tional distributions for automatical Bayesian in-
ference without an accept/reject ratio. Our algo-
rithm represents a first attempt to extend Polson’s
approach (Polson et al., 2012) to deal with highly
non-trivial Bayesian latent variable models. Let us
first introduce the Polya-Gamma variables.
</bodyText>
<equation confidence="0.7955868">
Definition 1 (Polson et al., 2012) A random
variable X has a Polya-Gamma distribution,
denoted by X ∼P9(a, b), if
gk
(i − 1)2/2 + b2/(4π2),
</equation>
<bodyText confidence="0.966473833333333">
where a, b &gt; 0 and each gi ∼ 9(a,1) is an inde-
pendent Gamma random variable.
Let Wd = η⊤2d. Then, using the ideas of data
augmentation (Tanner and Wong, 1987; Polson
et al., 2012), we can show that the generalized
pseudo-likelihood can be expressed as
</bodyText>
<equation confidence="0.819649">
1
F((yd|zd, η) = 2ce �dwdexp − λdωd 2 )p(λd|c, 0)dλd,
</equation>
<bodyText confidence="0.990696888888889">
where κd = c(yd−1/2) and Ad is a Polya-Gamma
variable with parameters a = c and b = 0. This
result indicates that the posterior distribution of
the generalized Bayesian logistic supervised topic
models, i.e., q(η, O, Z, 4)), can be expressed as
the marginal of a higher dimensional distribution
that includes the augmented variables λ. The
complete posterior distribution is
q(η, λ, Θ, Z, Φ) = p0(η, Θ, Z, Φ)p(W|Z, Φ)ϕ(y, λ|Z, η)
</bodyText>
<equation confidence="0.516843">
ψ(y, W)
</equation>
<bodyText confidence="0.997877">
where the pseudo-joint distribution of y and λ is
</bodyText>
<equation confidence="0.94770675">
ϕ(y, λ|Z, η) = � exp p(λd|c, 0).
(κdωd − λdω2 )
d
2
</equation>
<bodyText confidence="0.79856">
d
</bodyText>
<subsectionHeader confidence="0.99857">
3.2 Inference with Collapsed Gibbs Sampling
</subsectionHeader>
<bodyText confidence="0.999857333333333">
Although we can do Gibbs sampling to infer the
complete posterior distribution q(η, λ, O, Z, 4))
and thus q(η, O, Z, 4)) by ignoring λ, the mixing
rate would be slow due to the large sample space.
One way to effectively improve mixing rates
is to integrate out the intermediate variables
(O, 4)) and build a Markov chain whose equi-
librium distribution is the marginal distribution
q(η, λ, Z). We propose to use collapsed Gibbs
sampling, which has been successfully used in
LDA (Griffiths and Steyvers, 2004). For our
model, the collapsed posterior distribution is
</bodyText>
<equation confidence="0.9747938">
q(η, λ, Z) ∝ p0(η)p(W, Z|α, β)ϕ(y, λ|Z, η)
[6(Cd + α)
L δ(α)
p(λd|c 0)1
dim(x) Γ x J
</equation>
<bodyText confidence="0.9139772">
where S(x) = ∏i=1dim(x)( &apos;) , Ck is the number of
Γ(∑i=1 xi)
times the term t being assigned to topic k over the
whole corpus and Ck = {Ctk}Vt= 1; Ckd is the num-
ber of times that terms being associated with topic
k within the d-th document and Cd = {Ckd}Kk=1.
Then, the conditional distributions used in col-
lapsed Gibbs sampling are as follows.
For η: for the commonly used isotropic Gaus-
sian prior p0(η) = Hk N(ηk; 0, v2), we have
</bodyText>
<equation confidence="0.9900122">
2
q(η |Z, λ) ∝ p0(η) rJ exp (κdωd − λdωd )
d 2
×
= N(η; µ, E), (8)
</equation>
<bodyText confidence="0.985973625">
where the posterior mean is µ = E(Ed κd2d) and
the covariance is E = (ν21 I + Ed Ad2d2⊤d)−1. We
can easily draw a sample from a K-dimensional
multivariate Gaussian distribution. The inverse
can be robustly done using Cholesky decomposi-
tion, an O(K3) procedure. Since K is normally
not large, the inversion can be done efficiently.
For Z: The conditional distribution of Z is
</bodyText>
<equation confidence="0.57413">
q(Z|η, λ) ∝
</equation>
<bodyText confidence="0.964767">
By canceling common factors, we can derive the
local conditional of one variable zdn as:
</bodyText>
<equation confidence="0.934867285714286">
q(zkdn = 1  |Z¬, η, λ, wdn = t)
(Ctk,_.. + βt)(Ca,_.. + αk)
∝ exp (ryκdηk
Et Ctk,¬n + EVt=1 βt
)
− λd γ2η2 k + 2γ(1 − γ)ηkAk dn , (9)
2
</equation>
<bodyText confidence="0.9136867">
where C·
·,¬n indicates that term n is excluded from
the corresponding document or topic; γ = Nd1 ;
and Akdn = Nd 1 Ek′ ηk′Cd� n is the discrimi-
nant function value without word n. We can see
that the first term is from the LDA model for ob-
served word counts and the second term is from
the supervising signal y.
For λ: Finally, the conditional distribution of
the augmented variables λ is
</bodyText>
<equation confidence="0.836119727272727">
2
q (λd  |Z, η) ∝ exp (− λdωd )p(λd|c, 0)
2
= PG(λd; c, ωd), (10)
1
X = 2π2
�∞
i=1
= p0(η) K δ(Ck + β) D
ri δ(β) ri
k=1 d=1
</equation>
<figure confidence="0.931985826086956">
κ
ω
exp (
d
d
λdω2 )
d
2
δ(Ck + β)
δ(β)
rs(Cd + α)
L δ(α)
K
ri
k=1
D
ri
d=1
(× exp κdωd
λdWd)].
dd)J
2
2
</figure>
<page confidence="0.899692">
190
</page>
<bodyText confidence="0.594399">
Algorithm 1 for collapsed Gibbs sampling
</bodyText>
<listItem confidence="0.960954727272727">
1: Initialization: set A = 1 and randomly draw
zdn from a uniform distribution.
2: form= 1toMdo
3: draw a classifier from the distribution (8)
4: ford= 1toDdo
5: for each word n in document d do
6: draw the topic using distribution (9)
7: end for
8: draw Ad from distribution (10).
9: end for
10: end for
</listItem>
<bodyText confidence="0.999617545454546">
which is a Polya-Gamma distribution. The equal-
ity has been achieved by using the construction
definition of the general P9(a, b) class through an
exponential tilting of the P9(a, 0) density (Pol-
son et al., 2012). To draw samples from the
Polya-Gamma distribution, we adopt the efficient
method2 proposed in (Polson et al., 2012), which
draws the samples through drawing samples from
the closely related exponentially tilted Jacobi dis-
tribution.
With the above conditional distributions, we can
construct a Markov chain which iteratively draws
samples of rl using Eq. (8), Z using Eq. (9) and
A using Eq. (10), with an initial condition. In our
experiments, we initially set A = 1 and randomly
draw Z from a uniform distribution. In training,
we run the Markov chain for M iterations (i.e., the
burn-in stage), as outlined in Algorithm 1. Then,
we draw a sample rl� as the final classifier to make
predictions on testing data. As we shall see, the
Markov chain converges to stable prediction per-
formance with a few burn-in iterations.
</bodyText>
<subsectionHeader confidence="0.995426">
3.3 Prediction
</subsectionHeader>
<bodyText confidence="0.997526545454545">
To apply the classifier rl� on testing data, we need
to infer their topic assignments. We take the ap-
proach in (Zhu et al., 2012; Jiang et al., 2012),
which uses a point estimate of topics 4) from
training data and makes prediction based on them.
Specifically, we use the MAP estimate 4 to re-
place the probability distribution p(4)). For the
Gibbs sampler, an estimate of 4 using the sam-
ples is Okt OC Ctk + βt. Then, given a testing doc-
ument w, we infer its latent components z using
4) as p(zn = k|z¬n) OC &amp;. (Ck¬n + αk), where
</bodyText>
<footnote confidence="0.925503">
2The basic sampler was implemented in the R package
BayesLogit. We implemented the sampling algorithm in C++
together with our topic model sampler.
</footnote>
<bodyText confidence="0.886505">
Ck¬n is the times that the terms in this document w
assigned to topic k with the n-th term excluded.
</bodyText>
<sectionHeader confidence="0.999582" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999468454545455">
We present empirical results and sensitivity anal-
ysis to demonstrate the efficiency and prediction
performance3 of the generalized logistic super-
vised topic models on the 20Newsgroups (20NG)
data set, which contains about 20,000 postings
within 20 news groups. We follow the same set-
ting as in (Zhu et al., 2012) and remove a stan-
dard list of stop words for both binary and multi-
class classification. For all the experiments, we
use the standard normal prior p0(rl) (i.e., v2 = 1)
and the symmetric Dirichlet priors α = xC3
</bodyText>
<listItem confidence="0.9192262">
1, , =
0.01 x 1, where 1 is a vector with all entries being
1. For each setting, we report the average perfor-
mance and the standard deviation with five ran-
domly initialized runs.
</listItem>
<subsectionHeader confidence="0.991789">
4.1 Binary classification
</subsectionHeader>
<bodyText confidence="0.999741714285714">
Following the same setting in (Lacoste-Jullien et
al., 2009; Zhu et al., 2012), the task is to distin-
guish postings of the newsgroup alt.atheism and
those of the group talk.religion.misc. The training
set contains 856 documents and the test set con-
tains 569 documents. We compare the generalized
logistic supervised LDA using Gibbs sampling
(denoted by gSLDA) with various competitors,
including the standard sLDA using variational
mean-field methods (denoted by vSLDA) (Wang
et al., 2009), the MedLDA model using varia-
tional mean-field methods (denoted by vMedL-
DA) (Zhu et al., 2012), and the MedLDA mod-
el using collapsed Gibbs sampling algorithms (de-
noted by gMedLDA) (Jiang et al., 2012). We al-
so include the unsupervised LDA using collapsed
Gibbs sampling as a baseline, denoted by gLDA.
For gLDA, we learn a binary linear SVM on its
topic representations using SVMLight (Joachims,
1999). The results of DiscLDA (Lacoste-Jullien
et al., 2009) and linear SVM on raw bag-of-words
features were reported in (Zhu et al., 2012). For
gSLDA, we compare two versions – the standard
sLDA with c = 1 and the sLDA with a well-tuned
c value. To distinguish, we denote the latter by
gSLDA+. We set c = 25 for gSLDA+, and set
α = 1 and M = 100 for both gSLDA and gSL-
DA+. As we shall see, gSLDA is insensitive to α,
</bodyText>
<footnote confidence="0.9791295">
3Due to space limit, the topic visualization (similar to that
of MedLDA) is deferred to a longer version.
</footnote>
<page confidence="0.987361">
191
</page>
<figure confidence="0.999634020833333">
Accuracy
0.85
0.75
0.65
0.55
0.8
0.7
0.6
10 15 20 25 30
gSLDA
gSLDA+
vSLDA
vMedLDA
gMedLDA
gLDA+SVM
Train−time (seconds)
103
102
101
100
10−1
10−2
5 10 15 20 25 30
gSLDA
gSLDA+
vSLDA
vMedLDA
gMedLDA
gLDA+SVM
Test−time (seconds)
3.5
2.5
0.5
1.5
4
3
2
0
1
5 10 15 20 25 30
gSLDA
gSLDA+
vSLDA
gMedLDA
vMedLDA
gLDA+SVM
# Topics # Topics # Topics
(a) accuracy (b) training time (c) testing time
</figure>
<figureCaption confidence="0.999988">
Figure 1: Accuracy, training time (in log-scale) and testing time on the 20NG binary data set.
</figureCaption>
<bodyText confidence="0.997776129032258">
c and M in a wide range.
Fig. 1 shows the performance of different meth-
ods with various numbers of topics. For accuracy,
we can draw two conclusions: 1) without making
restricting assumptions on the posterior distribu-
tions, gSLDA achieves higher accuracy than vSL-
DA that uses strict variational mean-field approxi-
mation; and 2) by using the regularization constant
c to improve the influence of supervision informa-
tion, gSLDA+ achieves much better classification
results, in fact comparable with those of MedLDA
models since they have the similar mechanism to
improve the influence of supervision by tuning a
regularization constant. The fact that gLDA+SVM
performs better than the standard gSLDA is due
to the same reason, since the SVM part of gL-
DA+SVM can well capture the supervision infor-
mation to learn a classifier for good prediction,
while standard sLDA can’t well-balance the influ-
ence of supervision. In contrast, the well-balanced
gSLDA+ model successfully outperforms the two-
stage approach, gLDA+SVM, by performing topic
discovery and prediction jointly4.
For training time, both gSLDA and gSLDA+ are
very efficient, e.g., about 2 orders of magnitudes
faster than vSLDA and about 1 order of magnitude
faster than vMedLDA. For testing time, gSLDA
and gSLDA+ are comparable with gMedLDA and
the unsupervised gLDA, but faster than the varia-
tional vMedLDA and vSLDA, especially when K
is large.
</bodyText>
<subsectionHeader confidence="0.958766">
4.2 Multi-class classification
</subsectionHeader>
<bodyText confidence="0.999830666666667">
We perform multi-class classification on the 20NG
data set with all the 20 categories. For multi-
class classification, one possible extension is to
use a multinomial logistic regression model for
categorical variables Y by using topic represen-
tations 2 as input features. However, it is non-
</bodyText>
<footnote confidence="0.8904775">
4The variational sLDA with a well-tuned c is significantly
better than the standard sLDA, but a bit inferior to gSLDA+.
</footnote>
<bodyText confidence="0.999900243902439">
trivial to develop a Gibbs sampling algorithm us-
ing the similar data augmentation idea, due to the
presence of latent variables and the nonlinearity
of the soft-max function. In fact, this is harder
than the multinomial Bayesian logistic regression,
which can be done via a coordinate strategy (Pol-
son et al., 2012). Here, we apply the binary gSL-
DA to do the multi-class classification, following
the “one-vs-all” strategy, which has been shown
effective (Rifkin and Klautau, 2004), to provide
some preliminary analysis. Namely, we learn 20
binary gSLDA models and aggregate their predic-
tions by taking the most likely ones as the final
predictions. We again evaluate two versions of
gSLDA – the standard gSLDA with c = 1 and
the improved gSLDA+ with a well-tuned c value.
Since gSLDA is also insensitive to α and c for the
multi-class task, we set α = 5.6 for both gSLDA
and gSLDA+, and set c = 256 for gSLDA+. The
number of burn-in is set as M = 40, which is suf-
ficiently large to get stable results, as we shall see.
Fig. 2 shows the accuracy and training time. We
can see that: 1) by using Gibbs sampling without
restricting assumptions, gSLDA performs better
than the variational vSLDA that uses strict mean-
field approximation; 2) due to the imbalance be-
tween the single supervision and a large set of
word counts, gSLDA doesn’t outperform the de-
coupled approach, gLDA+SVM; and 3) if we in-
crease the value of the regularization constant c,
supervision information can be better captured to
infer predictive topic representations, and gSL-
DA+ performs much better than gSLDA. In fac-
t, gSLDA+ is even better than the MedLDA that
uses mean-field approximation, while is compara-
ble with the MedLDA using collapsed Gibbs sam-
pling. Finally, we should note that the improve-
ment on the accuracy might be due to the differen-
t strategies on building the multi-class classifier-
s. But given the performance gain in the binary
task, we believe that the Gibbs sampling algorith-
</bodyText>
<page confidence="0.99722">
192
</page>
<figureCaption confidence="0.99609">
Figure 2: Multi-class classification.
</figureCaption>
<tableCaption confidence="0.958752">
Table 1: Split of training time over various steps.
</tableCaption>
<figure confidence="0.6338246">
SAMPLE a SAMPLE 71 SAMPLE Z
K=20 2841.67 (65.80 %) 7.70 (0.18%) 1455.25 (34.02%)
K=30 2417.95 (56.10 %) 10.34 (0.24%) 1888.78 (43.66%)
K=40 2393.77 (49.00 %) 14.66 (0.30%) 2476.82 (50.70%)
K=50 2161.09 (43.67 %) 16.33 (0.33%) 2771.26 (56.00%)
</figure>
<bodyText confidence="0.992149944444444">
m without factorization assumptions is the main
factor for the improved performance.
For training time, gSLDA models are about
10 times faster than variational vSLDA. Table 1
shows in detail the percentages of the training time
(see the numbers in brackets) spent at each sam-
pling step for gSLDA+. We can see that: 1) sam-
pling the global variables 77 is very efficient, while
sampling local variables (A, Z) are much more ex-
pensive; and 2) sampling A is relatively stable as
K increases, while sampling Z takes more time
as K becomes larger. But, the good news is that
our Gibbs sampling algorithm can be easily paral-
lelized to speedup the sampling of local variables,
following the similar architectures as in LDA.
A Parallel Implementation: GraphLab is a
graph-based programming framework for parallel
computing (Gonzalez et al., 2012). It provides a
high-level abstraction of parallel tasks by express-
ing data dependencies with a distributed graph.
GraphLab implements a GAS (gather, apply, scat-
ter) model, where the data required to compute a
vertex (edge) are gathered along its neighboring
components, and modification of a vertex (edge)
will trigger its adjacent components to recompute
their values. Since GAS has been successfully ap-
plied to several machine learning algorithms&apos; in-
cluding Gibbs sampling of LDA, we choose it as a
preliminary attempt to parallelize our Gibbs sam-
pling algorithm. A systematical investigation of
the parallel computation with various architectures
in interesting, but beyond the scope of this paper.
For our task, since there is no coupling among
the 20 binary gSLDA classifiers, we can learn
them in parallel. This suggests an efficient hybrid
multi-core/multi-machine implementation, which
</bodyText>
<footnote confidence="0.772692">
5http://docs.graphlab.org/toolkits.html
</footnote>
<bodyText confidence="0.999865896551724">
can avoid the time consumption of IPC (i.e., inter-
process communication). Namely, we run our ex-
periments on a cluster with 20 nodes where each n-
ode is equipped with two 6-core CPUs (2.93GHz).
Each node is responsible for learning one binary
gSLDA classifier with a parallel implementation
on its 12-cores. For each binary gSLDA mod-
el, we construct a bipartite graph connecting train
documents with corresponding terms. The graph
works as follows: 1) the edges contain the to-
ken counts and topic assignments; 2) the vertices
contain individual topic counts and the augment-
ed variables A; 3) the global topic counts and 77
are aggregated from the vertices periodically, and
the topic assignments and A are sampled asyn-
chronously during the GAS phases. Once start-
ed, sampling and signaling will propagate over the
graph. One thing to note is that since we can-
not directly measure the number of iterations of
an asynchronous model, here we estimate it with
the total number of topic samplings, which is again
aggregated periodically, divided by the number of
tokens. We denote the parallel models by parallel-
gSLDA (c = 1) and parallel-gSLDA+ (c = 256).
From Fig. 2 (b), we can see that the parallel gSL-
DA models are about 2 orders of magnitudes faster
than their sequential counterpart models, which is
very promising. Also, the prediction performance
is not sacrificed as we shall see in Fig. 4.
</bodyText>
<subsectionHeader confidence="0.999978">
4.3 Sensitivity analysis
</subsectionHeader>
<bodyText confidence="0.99804935">
Burn-In: Fig. 3 shows the performance of gSL-
DA+ with different burn-in steps for binary classi-
fication. When M = 0 (see the most left points),
the models are built on random topic assignments.
We can see that the classification performance in-
creases fast and converges to the stable optimum
with about 20 burn-in steps. The training time in-
creases about linearly in general when using more
burn-in steps. Moreover, the training time increas-
es linearly as K increases. In the previous experi-
ments, we set M = 100.
Fig. 4 shows the performance of gSLDA+
and its parallel implementation (i.e., parallel-
gSLDA+) for the multi-class classification with d-
ifferent burn-in steps. We can see when the num-
ber of burn-in steps is larger than 20, the per-
formance of gSLDA+ is quite stable. Again, in
the log-log scale, since the slopes of the lines in
Fig. 4 (b) are close to the constant 1, the train-
ing time grows about linearly as the number of
</bodyText>
<figure confidence="0.999089454545455">
# Topics # Topics
(a) accuracy (b) training time
Accuracy
0.75
0.65
0.55
20 30 40 50 60 70 80 90 100 110
0.8
0.7
0.6
gSLDA
gSLDA+
vSLDA
vMedLDA
gMedLDA
gLDA+SVM
Train−time (seconds)
105
104
103
102
101
100
10−1
20 30 40 50 60 70 80 90 100 11
parallel−gSLDA
parallel−gSLDA+
gSLDA
gSLDA+
vSLDA
vMedLDA
gMedLDA
gLDA+SVM
</figure>
<page confidence="0.925418">
193
</page>
<figureCaption confidence="0.978202428571429">
Figure 3: Performance of gSLDA+ with different
burn-in steps for binary classification. The most
left points are for the settings with no burn in.
Figure 4: Performance of gSLDA+ and parallel-
gSLDA+ with different burn-in steps for multi-
class classification. The most left points are for
the settings with no burn in.
</figureCaption>
<bodyText confidence="0.999783551724138">
burn-in steps increases. Even when we use 40 or
60 burn-in steps, the training time is still compet-
itive, compared with the variational vSLDA. For
parallel-gSLDA+ using GraphLab, the training is
consistently about 2 orders of magnitudes faster.
Meanwhile, the classification performance is also
comparable with that of gSLDA+, when the num-
ber of burn-in steps is larger than 40. In the pre-
vious experiments, we have set M = 40 for both
gSLDA+ and parallel-gSLDA+.
Regularization constant c: Fig. 5 shows the
performance of gSLDA in the binary classification
task with different c values. We can see that in a
wide range, e.g., from 9 to 100, the performance
is quite stable for all the three K values. But for
the standard sLDA model, i.e., c = 1, both the
training accuracy and test accuracy are low, which
indicates that sLDA doesn’t fit the supervision da-
ta well. When c becomes larger, the training ac-
curacy gets higher, but it doesn’t seem to over-fit
and the generalization performance is stable. In
the above experiments, we set c = 25. For multi-
class classification, we have similar observations
and set c = 256 in the previous experiments.
Dirichlet prior α: Fig. 6 shows the perfor-
mance of gSLDA on the binary task with differ-
ent α values. We report two cases with c = 1 and
c = 9. We can see that the performance is quite
stable in a wide range of α values, e.g., from 0.1
</bodyText>
<figureCaption confidence="0.946512">
Figure 6: Accuracy of gSLDA for binary classifi-
cation with different α values in two settings with
c = 1 and c = 9.
</figureCaption>
<bodyText confidence="0.969429">
to 10. We also noted that the change of α does not
affect the training time much.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="conclusions">
5 Conclusions and Discussions
</sectionHeader>
<bodyText confidence="0.999955434782609">
We present two improvements to Bayesian logis-
tic supervised topic models, namely, a general for-
mulation by introducing a regularization parame-
ter to avoid model imbalance and a highly efficient
Gibbs sampling algorithm without restricting as-
sumptions on the posterior distributions by explor-
ing the idea of data augmentation. The algorithm
can also be parallelized. Empirical results for both
binary and multi-class classification demonstrate
significant improvements over the existing logistic
supervised topic models. Our preliminary results
with GraphLab have shown promise on paralleliz-
ing the Gibbs sampling algorithm.
For future work, we plan to carry out more
careful investigations, e.g., using various distribut-
ed architectures (Ahmed et al., 2012; Newman
et al., 2009; Smola and Narayanamurthy, 2010),
to make the sampling algorithm highly scalable
to deal with massive data corpora. Moreover,
the data augmentation technique can be applied
to deal with other types of response variables,
such as count data with a negative-binomial likeli-
hood (Polson et al., 2012).
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.835763">
This work is supported by National Key Foun-
dation R&amp;D Projects (No.s 2013CB329403,
</bodyText>
<figure confidence="0.978047698113207">
1.05
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
100 101 102 103
train accuracy
test accuracy
K = 5
K = 10
K=20
burn−in iterations
(a) accuracy
35
30
25
20
15
10
5
0
0
100 200 300 400 500
K = 5
K = 10
K=20
burn−in iterations
(b) training time
Accuracy
Train−time (seconds)
0.85
gSLDA+
0.75
0.7
parallel−gSLDA+
K = 20
K = 30
K = 40
K = 50
0.5
10−1 100 101 102 103
0.8
0.65
0.6
0.55
burn−in iterations
(a) accuracy
105
gSLDA+
103
102
101
parallel−gSLDA+
100
10−1 100 101 102 103
104
K = m
K=30
K = 40
K=50
burn−in iterations
(b) training time
Train−time (sec)
Accuracy
(a) accuracy
(b) training time
α
(a) c = 1
α
(b) c = 9
11
1.05
10
1
9
0.7
K = 5
K = 10
K = 20
train accuracy
test accuracy
0.8
0.651 2 3 4 √c 6 7 8 9 1
8
7
6
5
4
3
2
1
0.95
Train−time (seconds)
0.9
Accuracy
0.85
0.75
K = 5
K = 10
K = 20
1 2 3 4 √0 6 7 8 9 1
</figure>
<figureCaption confidence="0.995156">
Figure 5: Performance of gSLDA for binary clas-
sification with different c values.
</figureCaption>
<figure confidence="0.996342846153846">
0.85
0.85
0.8
0.8
0.75
0.75
Accuracy
Accuracy
0.7
0.7
0.65
0.65
0.6
</figure>
<equation confidence="0.721988714285714">
K = 5
K = 10
K = 15
K=20
K = 5
K = 10
K = 15
</equation>
<figure confidence="0.973346142857143">
K=20
0.6
0.55
0.5
10−4 10−2 100 102 104
0.55
10−6 10−4 10−2 100 102 104
</figure>
<page confidence="0.992744">
194
</page>
<bodyText confidence="0.96218">
2012CB316301), Tsinghua Initiative Scientific
Research Program No.20121088071, Tsinghua
National Laboratory for Information Science and
Technology, and the 221 Basic Research Plan for
Young Faculties at Tsinghua University.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828270833333">
A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy,
and A. Smola. 2012. Scalable inference in laten-
t variable models. In International Conference on
Web Search and Data Mining (WSDM).
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. arXiv:1003.0783v1.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3:993–1022.
M. Chen, J. Ibrahim, and C. Yiannoutsos. 1999. Pri-
or elicitation, variable selection and Bayesian com-
putation for logistic regression models. Journal of
Royal Statistical Society, Ser. B, (61):223–242.
P. Germain, A. Lacasse, F. Laviolette, and M. Marc-
hand. 2009. PAC-Bayesian learning of linear clas-
sifiers. In International Conference on Machine
Learning (ICML), pages 353–360.
A. Globerson, T. Koo, X. Carreras, and M. Collins.
2007. Exponentiated gradient algorithms for log-
linear structured prediction. In ICML, pages 305–
312.
J.E. Gonzalez, Y. Low, H. Gu, D. Bickson, and
C. Guestrin. 2012. Powergraph: Distributed graph-
parallel computation on natural graphs. In the 10th
USENIX Symposium on Operating Systems Design
and Implementation (OSDI).
T.L. Griffiths and M. Steyvers. 2004. Finding scientif-
ic topics. Proceedings of National Academy of Sci-
ence (PNAS), pages 5228–5235.
Y. Halpern, S. Horng, L. Nathanson, N. Shapiro, and
D. Sontag. 2012. A comparison of dimensionality
reduction techniques for unstructured clinical text.
In ICML 2012 Workshop on Clinical Data Analysis.
C. Holmes and L. Held. 2006. Bayesian auxiliary vari-
able models for binary and multinomial regression.
Bayesian Analysis, 1(1):145–168.
Q. Jiang, J. Zhu, M. Sun, and E.P. Xing. 2012. Monte
Carlo methods for maximum margin supervised top-
ic models. In Advances in Neural Information Pro-
cessing Systems (NIPS).
T. Joachims. 1999. Making large-scale SVM learning
practical. MIT press.
S. Lacoste-Jullien, F. Sha, and M.I. Jordan. 2009. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. Advances in Neural In-
formation Processing Systems (NIPS), pages 897–
904.
Y. Lin. 2001. A note on margin-based loss functions in
classification. Technical Report No. 1044. Universi-
ty of Wisconsin.
D. McAllester. 2003. PAC-Bayesian stochastic model
selection. Machine Learning, 51:5–21.
M. Meyer and P. Laud. 2002. Predictive variable selec-
tion in generalized linear models. Journal ofAmeri-
can Statistical Association, 97(459):859–871.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models.
Journal of Machine Learning Research (JMLR),
(10):1801–1828.
N.G. Polson, J.G. Scott, and J. Windle. 2012. Bayesian
inference for logistic models using Polya-Gamma
latent variables. arXiv:1205.0310v1.
R. Rifkin and A. Klautau. 2004. In defense of one-
vs-all classification. Journal of Machine Learning
Research (JMLR), (5):101–141.
L. Rosasco, E. De Vito, A. Caponnetto, M. Piana, and
A. Verri. 2004. Are loss functions all the same?
Neural Computation, (16):1063–1076.
A. Smola and S. Narayanamurthy. 2010. An architec-
ture for parallel topic models. Very Large Data Base
(VLDB), 3(1-2):703–710.
M.A. Tanner and W.-H. Wong. 1987. The calcu-
lation of posterior distributions by data augmenta-
tion. Journal of the Americal Statistical Association
(JASA), 82(398):528–540.
D. van Dyk and X. Meng. 2001. The art of data aug-
mentation. Journal of Computational and Graphi-
cal Statistics (JCGS), 10(1):1–50.
C. Wang, D.M. Blei, and Li F.F. 2009. Simultaneous
image classification and annotation. IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR).
J. Zhu, N. Chen, and E.P. Xing. 2011. Infinite latent
SVM for classification and multi-task learning. In
Advances in Neural Information Processing Systems
(NIPS), pages 1620–1628.
J. Zhu, A. Ahmed, and E.P. Xing. 2012. MedLDA:
maximum margin supervised topic models. Journal
of Machine Learning Research (JMLR), (13):2237–
2278.
J. Zhu, N. Chen, H. Perkins, and B. Zhang. 2013a.
Gibbs max-margin topic models with fast sampling
algorithms. In International Conference on Ma-
chine Learning (ICML).
J. Zhu, N. Chen, and E.P. Xing. 2013b. Bayesian infer-
ence with posterior regularization and applications
to infinite latent svms. arXiv:1210.1766v2.
</reference>
<page confidence="0.998934">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938787">
<title confidence="0.990542">Improved Bayesian Logistic Supervised Topic with Data Augmentation</title>
<author confidence="0.999752">Jun Zhu</author>
<author confidence="0.999752">Xun Zheng</author>
<author confidence="0.999752">Bo</author>
<affiliation confidence="0.997533333333333">Department of Computer Science and TNLIST Lab and State Key Lab of Intelligent Technology and Tsinghua University, Beijing,</affiliation>
<email confidence="0.999513">vforveri.zheng@gmail.com</email>
<abstract confidence="0.998338590909091">Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ahmed</author>
<author>M Aly</author>
<author>J Gonzalez</author>
<author>S Narayanamurthy</author>
<author>A Smola</author>
</authors>
<title>Scalable inference in latent variable models.</title>
<date>2012</date>
<booktitle>In International Conference on Web Search and Data Mining (WSDM).</booktitle>
<contexts>
<context position="33101" citStr="Ahmed et al., 2012" startWordPosition="5576" endWordPosition="5579">rameter to avoid model imbalance and a highly efficient Gibbs sampling algorithm without restricting assumptions on the posterior distributions by exploring the idea of data augmentation. The algorithm can also be parallelized. Empirical results for both binary and multi-class classification demonstrate significant improvements over the existing logistic supervised topic models. Our preliminary results with GraphLab have shown promise on parallelizing the Gibbs sampling algorithm. For future work, we plan to carry out more careful investigations, e.g., using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora. Moreover, the data augmentation technique can be applied to deal with other types of response variables, such as count data with a negative-binomial likelihood (Polson et al., 2012). Acknowledgments This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 100 101 102 103 train accuracy test accuracy K = 5 K = 10 K=20 burn−in iterations (a) accuracy 35 30 25 20 15 10 5 0 0 100 200 300 400 50</context>
</contexts>
<marker>Ahmed, Aly, Gonzalez, Narayanamurthy, Smola, 2012</marker>
<rawString>A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. Smola. 2012. Scalable inference in latent variable models. In International Conference on Web Search and Data Mining (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<date>2010</date>
<note>Supervised topic models. arXiv:1003.0783v1.</note>
<contexts>
<context position="1272" citStr="Blei and McAuliffe, 2010" startWordPosition="167" endWordPosition="170">ant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency. 1 Introduction As widely adopted in supervised latent Dirichlet allocation (sLDA) models (Blei and McAuliffe, 2010; Wang et al., 2009), one way to improve the predictive power of LDA is to define a likelihood model for the widely available documentlevel response variables, in addition to the likelihood model for document words. For example, the logistic likelihood model is commonly used for binary or multinomial responses. By imposing some priors, posterior inference is done with the Bayes’ rule. Though powerful, one issue that could limit the use of existing logistic supervised LDA models is that they treat the document-level response variable as one additional word via a normalized likelihood model. Alt</context>
</contexts>
<marker>Blei, McAuliffe, 2010</marker>
<rawString>D.M. Blei and J.D. McAuliffe. 2010. Supervised topic models. arXiv:1003.0783v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="5855" citStr="Blei et al., 2003" startWordPosition="872" endWordPosition="875">tudies. The paper is structured as follows. Sec. 2 introduces logistic supervised topic models as a general optimization problem. Sec. 3 presents Gibbs sampling algorithms with data augmentation. Sec. 4 presents experiments. Sec. 5 concludes. 2 Logistic Supervised Topic Models We now present the generalized Bayesian logistic supervised topic models. 2.1 The Generalized Models We consider binary classification with a training set D = {(wd, yd)}d1, where the response variable Y takes values from the output space Y = {0, 1}. A logistic supervised topic model consists of two parts — an LDA model (Blei et al., 2003) for describing the words W = {wd}d1, where wd = {wdn}�d n=1 denote the words within document d, and a logistic classifier for considering the supervising signal y = {yd}d1. Below, we introduce each of them in turn. LDA: LDA is a hierarchical Bayesian model that posits each document as an admixture of K topics, where each topic Φk is a multinomial distribution over a V-word vocabulary. For document d, the generating process is 1. draw a topic proportion θd ∼ Dir(α) 2. for each word n = 1, 2, ... , Nd: (a) draw a topics zdn ∼ Mult(θd) (b) draw the word wdn ∼ Mult(Φzdn) where Dir(·) is a Dirichl</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chen</author>
<author>J Ibrahim</author>
<author>C Yiannoutsos</author>
</authors>
<title>Prior elicitation, variable selection and Bayesian computation for logistic regression models.</title>
<date>1999</date>
<journal>Journal of Royal Statistical Society, Ser. B,</journal>
<pages>61--223</pages>
<contexts>
<context position="11893" citStr="Chen et al., 1999" startWordPosition="1913" endWordPosition="1916">p between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation techniques. It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additional fully-factorized constraint q(η, O, Z, Φ) = q(η)(∏d q(θd) ∏n q(zdn)) ∏k q(Φk) and a variational approximation to the expectation of the log-logistic likelihood, which is intractable to compute directly. Note that the non-Bayesian treatment of η as unknown parameters in (Wang et al., 2009) results in an EM algorithm, which s</context>
</contexts>
<marker>Chen, Ibrahim, Yiannoutsos, 1999</marker>
<rawString>M. Chen, J. Ibrahim, and C. Yiannoutsos. 1999. Prior elicitation, variable selection and Bayesian computation for logistic regression models. Journal of Royal Statistical Society, Ser. B, (61):223–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Germain</author>
<author>A Lacasse</author>
<author>F Laviolette</author>
<author>M Marchand</author>
</authors>
<title>PAC-Bayesian learning of linear classifiers.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>353--360</pages>
<contexts>
<context position="8320" citStr="Germain et al., 2009" startWordPosition="1315" endWordPosition="1318">nction that equals to 1 if predicate holds otherwise 0. If the classifier weights η and topic assignments z are given, the prediction rule is 9|,7,. = H(p(y = 1|η, z) &gt; 0.5) = H(η⊤z &gt; 0). (3) Since both η and Z are hidden variables, we propose to infer a posterior distribution q(η, Z) that has the minimal expected log-logistic loss R(q(η, Z)) = − � Eq[lo9 p(yd|η, zd)], (4) d which is a good surrogate loss for the expected misclassification loss, Ed EQ[ff(y|&apos;�Zd ≠ yd)], of a Gibbs classifier that randomly draws a model η from the posterior distribution and makes predictions (McAllester, 2003; Germain et al., 2009). In fact, this choice is motivated from the observation that logistic loss has been widely used as a convex surrogate loss for the misclassification &apos;A K-binary vector with only one entry equaling to 1. min q(®,Z,.b) 188 loss (Rosasco et al., 2004) in the task of fully observed binary classification. Also, note that the logistic classifier and the LDA likelihood are coupled by sharing the latent topic assignments z. The strong coupling makes it possible to learn a posterior distribution that can describe the observed words well and make accurate predictions. Regularized Bayesian Inference: To</context>
</contexts>
<marker>Germain, Lacasse, Laviolette, Marchand, 2009</marker>
<rawString>P. Germain, A. Lacasse, F. Laviolette, and M. Marchand. 2009. PAC-Bayesian learning of linear classifiers. In International Conference on Machine Learning (ICML), pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Exponentiated gradient algorithms for loglinear structured prediction.</title>
<date>2007</date>
<booktitle>In ICML,</booktitle>
<pages>305--312</pages>
<contexts>
<context position="11416" citStr="Globerson et al., 2007" startWordPosition="1838" endWordPosition="1841">rior regularization, for which MedLDA uses a hinge loss of an expected classifier while the logistic supervised topic model uses an expected log-logistic loss. Gibbs MedLDA (Zhu et al., 2013a) is another max-margin model that adopts the expected hinge loss as posterior regularization. As we shall see in the experiments, by using appropriate regularization constants, logistic supervised topic models achieve comparable performance as maxmargin methods. We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation te</context>
</contexts>
<marker>Globerson, Koo, Carreras, Collins, 2007</marker>
<rawString>A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Exponentiated gradient algorithms for loglinear structured prediction. In ICML, pages 305– 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Gonzalez</author>
<author>Y Low</author>
<author>H Gu</author>
<author>D Bickson</author>
<author>C Guestrin</author>
</authors>
<title>Powergraph: Distributed graphparallel computation on natural graphs.</title>
<date>2012</date>
<booktitle>In the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI).</booktitle>
<contexts>
<context position="5189" citStr="Gonzalez et al., 2012" startWordPosition="764" endWordPosition="767">etropolis-Hastings accept/reject steps. For Bayesian LDA models, we can also explore the conjugacy of the Dirichlet-Multinomial priorlikelihood pairs to collapse out the Dirichlet variables (i.e., topics and mixing proportions) to do collapsed Gibbs sampling, which can have better mixing rates (Griffiths and Steyvers, 2004). Finally, our empirical results on real data sets demonstrate significant improvements on time efficiency. The classification performance is also significantly improved by using appropriate regularization parameters. We also provide a parallel implementation with GraphLab (Gonzalez et al., 2012), which shows great promise in our preliminary studies. The paper is structured as follows. Sec. 2 introduces logistic supervised topic models as a general optimization problem. Sec. 3 presents Gibbs sampling algorithms with data augmentation. Sec. 4 presents experiments. Sec. 5 concludes. 2 Logistic Supervised Topic Models We now present the generalized Bayesian logistic supervised topic models. 2.1 The Generalized Models We consider binary classification with a training set D = {(wd, yd)}d1, where the response variable Y takes values from the output space Y = {0, 1}. A logistic supervised to</context>
<context position="26806" citStr="Gonzalez et al., 2012" startWordPosition="4521" endWordPosition="4524">raining time (see the numbers in brackets) spent at each sampling step for gSLDA+. We can see that: 1) sampling the global variables 77 is very efficient, while sampling local variables (A, Z) are much more expensive; and 2) sampling A is relatively stable as K increases, while sampling Z takes more time as K becomes larger. But, the good news is that our Gibbs sampling algorithm can be easily parallelized to speedup the sampling of local variables, following the similar architectures as in LDA. A Parallel Implementation: GraphLab is a graph-based programming framework for parallel computing (Gonzalez et al., 2012). It provides a high-level abstraction of parallel tasks by expressing data dependencies with a distributed graph. GraphLab implements a GAS (gather, apply, scatter) model, where the data required to compute a vertex (edge) are gathered along its neighboring components, and modification of a vertex (edge) will trigger its adjacent components to recompute their values. Since GAS has been successfully applied to several machine learning algorithms&apos; including Gibbs sampling of LDA, we choose it as a preliminary attempt to parallelize our Gibbs sampling algorithm. A systematical investigation of t</context>
</contexts>
<marker>Gonzalez, Low, Gu, Bickson, Guestrin, 2012</marker>
<rawString>J.E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. 2012. Powergraph: Distributed graphparallel computation on natural graphs. In the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of National Academy of Science (PNAS),</booktitle>
<pages>5228--5235</pages>
<contexts>
<context position="4892" citStr="Griffiths and Steyvers, 2004" startWordPosition="722" endWordPosition="725">late the generalized logistic pseudo-likelihood model (with the regularization parameter) as a scale mixture, where the mixture component is conditionally normal for classifier parameters. Then, we develop a simple and efficient Gibbs sampling algorithms with analytic conditional distributions without Metropolis-Hastings accept/reject steps. For Bayesian LDA models, we can also explore the conjugacy of the Dirichlet-Multinomial priorlikelihood pairs to collapse out the Dirichlet variables (i.e., topics and mixing proportions) to do collapsed Gibbs sampling, which can have better mixing rates (Griffiths and Steyvers, 2004). Finally, our empirical results on real data sets demonstrate significant improvements on time efficiency. The classification performance is also significantly improved by using appropriate regularization parameters. We also provide a parallel implementation with GraphLab (Gonzalez et al., 2012), which shows great promise in our preliminary studies. The paper is structured as follows. Sec. 2 introduces logistic supervised topic models as a general optimization problem. Sec. 3 presents Gibbs sampling algorithms with data augmentation. Sec. 4 presents experiments. Sec. 5 concludes. 2 Logistic S</context>
<context position="15131" citStr="Griffiths and Steyvers, 2004" startWordPosition="2457" endWordPosition="2460">tribution of y and λ is ϕ(y, λ|Z, η) = � exp p(λd|c, 0). (κdωd − λdω2 ) d 2 d 3.2 Inference with Collapsed Gibbs Sampling Although we can do Gibbs sampling to infer the complete posterior distribution q(η, λ, O, Z, 4)) and thus q(η, O, Z, 4)) by ignoring λ, the mixing rate would be slow due to the large sample space. One way to effectively improve mixing rates is to integrate out the intermediate variables (O, 4)) and build a Markov chain whose equilibrium distribution is the marginal distribution q(η, λ, Z). We propose to use collapsed Gibbs sampling, which has been successfully used in LDA (Griffiths and Steyvers, 2004). For our model, the collapsed posterior distribution is q(η, λ, Z) ∝ p0(η)p(W, Z|α, β)ϕ(y, λ|Z, η) [6(Cd + α) L δ(α) p(λd|c 0)1 dim(x) Γ x J where S(x) = ∏i=1dim(x)( &apos;) , Ck is the number of Γ(∑i=1 xi) times the term t being assigned to topic k over the whole corpus and Ck = {Ctk}Vt= 1; Ckd is the number of times that terms being associated with topic k within the d-th document and Cd = {Ckd}Kk=1. Then, the conditional distributions used in collapsed Gibbs sampling are as follows. For η: for the commonly used isotropic Gaussian prior p0(η) = Hk N(ηk; 0, v2), we have 2 q(η |Z, λ) ∝ p0(η) rJ ex</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T.L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of National Academy of Science (PNAS), pages 5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Halpern</author>
<author>S Horng</author>
<author>L Nathanson</author>
<author>N Shapiro</author>
<author>D Sontag</author>
</authors>
<title>A comparison of dimensionality reduction techniques for unstructured clinical text.</title>
<date>2012</date>
<booktitle>In ICML 2012 Workshop on Clinical Data Analysis.</booktitle>
<contexts>
<context position="2128" citStr="Halpern et al., 2012" startWordPosition="312" endWordPosition="315">gistic likelihood model is commonly used for binary or multinomial responses. By imposing some priors, posterior inference is done with the Bayes’ rule. Though powerful, one issue that could limit the use of existing logistic supervised LDA models is that they treat the document-level response variable as one additional word via a normalized likelihood model. Although some special treatment is carried out on defining the likelihood of the single response variable, it is normally of a much smaller scale than the likelihood of the usually tens or hundreds of words in each document. As noted by (Halpern et al., 2012) and observed in our experiments, this model imbalance could result in a weak influence of response variables on the topic representations and thus non-satisfactory prediction performance. Another difficulty arises when dealing with categorical response variables is that the commonly used normal priors are no longer conjugate to the logistic likelihood and thus lead to hard inference problems. Existing approaches rely on variational approximation techniques which normally make strict mean-field assumptions. To address the above issues, we present two improvements. First, we present a general f</context>
<context position="10419" citStr="Halpern et al., 2012" startWordPosition="1678" endWordPosition="1681">[log ψ(y|Z, η)] (7) s.t.: q(η, Θ, Z, Φ) E P, where ψ(y|Z,η) = ∏d ψ(yd|zd, η). It is easy to show that the optimum solution of problem (5) or the equivalent problem (7) is the posterior distribution with supervising information, i.e., ( o, Z, �) - PO(n, o, Z, 4&apos;)P(WIZ, 4,),O(Y|n, Z) V �, where ϕ(y, W) is the normalization constant to make q a distribution. We can see that when c = 1, the model reduces to the standard sLDA, which in practice has the imbalance issue that the response variable (can be viewed as one additional word) is usually dominated by the words. This imbalance was noticed in (Halpern et al., 2012). We will see that c can make a big difference later. Comparison with MedLDA: The above formulation of logistic supervised topic models as an instance of regularized Bayesian inference provides a direct comparison with the max-margin supervised topic model (MedLDA) (Jiang et al., 2012), which has the same form of the optimization problems. The difference lies in the posterior regularization, for which MedLDA uses a hinge loss of an expected classifier while the logistic supervised topic model uses an expected log-logistic loss. Gibbs MedLDA (Zhu et al., 2013a) is another max-margin model that </context>
</contexts>
<marker>Halpern, Horng, Nathanson, Shapiro, Sontag, 2012</marker>
<rawString>Y. Halpern, S. Horng, L. Nathanson, N. Shapiro, and D. Sontag. 2012. A comparison of dimensionality reduction techniques for unstructured clinical text. In ICML 2012 Workshop on Clinical Data Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Holmes</author>
<author>L Held</author>
</authors>
<title>Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis,</title>
<date>2006</date>
<contexts>
<context position="3747" citStr="Holmes and Held, 2006" startWordPosition="559" endWordPosition="562"> regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data augmentation (Tanner and Wong, 1987; van Dyk and Meng, 2001; Holmes and Held, 2006). More specifically, we extend Polson’s method for Bayesian logistic regression (Polson et al., 2012) to the generalized logistic supervised topic models, which are much more challeng187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ing due to the presence of non-trivial latent variables. Technically, we introduce a set of PolyaGamma variables, one per document, to reformulate the generalized logistic pseudo-likelihood model (with the regularization param</context>
</contexts>
<marker>Holmes, Held, 2006</marker>
<rawString>C. Holmes and L. Held. 2006. Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis, 1(1):145–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Jiang</author>
<author>J Zhu</author>
<author>M Sun</author>
<author>E P Xing</author>
</authors>
<title>Monte Carlo methods for maximum margin supervised topic models.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="7071" citStr="Jiang et al., 2012" startWordPosition="1098" endWordPosition="1101">chlet distribution; Mult(·) is a multinomial distribution; and Φzdn denotes the topic selected by the non-zero entry of zdn. For fully-Bayesian LDA, the topics are random samples from a Dirichlet prior, Φk ∼ Dir(β). Let zd = {zdn}�d n=1 denote the set of topic assignments for document d. Let Z = {zd}d1 and Θ = {θd}d1 denote all the topic assignments and mixing proportions for the entire corpus. LDA infers the posterior distribution p(Θ, Z, Φ|W) ∝ p0(Θ, Z, Φ)p(W|Z, Φ), where p0(Θ, Z, Φ) = (l ld p(θd|α) l ln p(zdn|θd)) l lk p(&amp;quot;Dk|β) is the joint distribution defined by the model. As noticed in (Jiang et al., 2012), the posterior distribution by Bayes’ rule is equivalent to the solution of an information theoretical optimization problem KL(q(O, Z, Φ)l1p0(O, Z, Φ))−Eq[lob p(W|Z, Φ)] s.t.: q(O, Z, Φ) E P, (1) where KL(q||p) is the Kullback-Leibler divergence from q to p and P is the space of probability distributions. Logistic classifier: To consider binary supervising information, a logistic supervised topic model (e.g., sLDA) builds a logistic classifier using the topic representations as input features p(y = 1|η, z) = exp(η⊤2) (2) 1 + exp(η⊤�z) EN where z� is a K-vector with �zk = 1 n=1 ff(zkn = � 1), </context>
<context position="10705" citStr="Jiang et al., 2012" startWordPosition="1724" endWordPosition="1727">where ϕ(y, W) is the normalization constant to make q a distribution. We can see that when c = 1, the model reduces to the standard sLDA, which in practice has the imbalance issue that the response variable (can be viewed as one additional word) is usually dominated by the words. This imbalance was noticed in (Halpern et al., 2012). We will see that c can make a big difference later. Comparison with MedLDA: The above formulation of logistic supervised topic models as an instance of regularized Bayesian inference provides a direct comparison with the max-margin supervised topic model (MedLDA) (Jiang et al., 2012), which has the same form of the optimization problems. The difference lies in the posterior regularization, for which MedLDA uses a hinge loss of an expected classifier while the logistic supervised topic model uses an expected log-logistic loss. Gibbs MedLDA (Zhu et al., 2013a) is another max-margin model that adopts the expected hinge loss as posterior regularization. As we shall see in the experiments, by using appropriate regularization constants, logistic supervised topic models achieve comparable performance as maxmargin methods. We note that the relationship between a logistic loss and</context>
<context position="18548" citStr="Jiang et al., 2012" startWordPosition="3119" endWordPosition="3122"> Eq. (9) and A using Eq. (10), with an initial condition. In our experiments, we initially set A = 1 and randomly draw Z from a uniform distribution. In training, we run the Markov chain for M iterations (i.e., the burn-in stage), as outlined in Algorithm 1. Then, we draw a sample rl� as the final classifier to make predictions on testing data. As we shall see, the Markov chain converges to stable prediction performance with a few burn-in iterations. 3.3 Prediction To apply the classifier rl� on testing data, we need to infer their topic assignments. We take the approach in (Zhu et al., 2012; Jiang et al., 2012), which uses a point estimate of topics 4) from training data and makes prediction based on them. Specifically, we use the MAP estimate 4 to replace the probability distribution p(4)). For the Gibbs sampler, an estimate of 4 using the samples is Okt OC Ctk + βt. Then, given a testing document w, we infer its latent components z using 4) as p(zn = k|z¬n) OC &amp;. (Ck¬n + αk), where 2The basic sampler was implemented in the R package BayesLogit. We implemented the sampling algorithm in C++ together with our topic model sampler. Ck¬n is the times that the terms in this document w assigned to topic k</context>
<context position="20611" citStr="Jiang et al., 2012" startWordPosition="3474" endWordPosition="3477">, 2012), the task is to distinguish postings of the newsgroup alt.atheism and those of the group talk.religion.misc. The training set contains 856 documents and the test set contains 569 documents. We compare the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang et al., 2012). We also include the unsupervised LDA using collapsed Gibbs sampling as a baseline, denoted by gLDA. For gLDA, we learn a binary linear SVM on its topic representations using SVMLight (Joachims, 1999). The results of DiscLDA (Lacoste-Jullien et al., 2009) and linear SVM on raw bag-of-words features were reported in (Zhu et al., 2012). For gSLDA, we compare two versions – the standard sLDA with c = 1 and the sLDA with a well-tuned c value. To distinguish, we denote the latter by gSLDA+. We set c = 25 for gSLDA+, and set α = 1 and M = 100 for both gSLDA and gSLDA+. As we shall see, gSLDA is ins</context>
</contexts>
<marker>Jiang, Zhu, Sun, Xing, 2012</marker>
<rawString>Q. Jiang, J. Zhu, M. Sun, and E.P. Xing. 2012. Monte Carlo methods for maximum margin supervised topic models. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="20812" citStr="Joachims, 1999" startWordPosition="3509" endWordPosition="3510">are the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang et al., 2012). We also include the unsupervised LDA using collapsed Gibbs sampling as a baseline, denoted by gLDA. For gLDA, we learn a binary linear SVM on its topic representations using SVMLight (Joachims, 1999). The results of DiscLDA (Lacoste-Jullien et al., 2009) and linear SVM on raw bag-of-words features were reported in (Zhu et al., 2012). For gSLDA, we compare two versions – the standard sLDA with c = 1 and the sLDA with a well-tuned c value. To distinguish, we denote the latter by gSLDA+. We set c = 25 for gSLDA+, and set α = 1 and M = 100 for both gSLDA and gSLDA+. As we shall see, gSLDA is insensitive to α, 3Due to space limit, the topic visualization (similar to that of MedLDA) is deferred to a longer version. 191 Accuracy 0.85 0.75 0.65 0.55 0.8 0.7 0.6 10 15 20 25 30 gSLDA gSLDA+ vSLDA v</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Jullien</author>
<author>F Sha</author>
<author>M I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>897--904</pages>
<contexts>
<context position="19980" citStr="Lacoste-Jullien et al., 2009" startWordPosition="3374" endWordPosition="3377">els on the 20Newsgroups (20NG) data set, which contains about 20,000 postings within 20 news groups. We follow the same setting as in (Zhu et al., 2012) and remove a standard list of stop words for both binary and multiclass classification. For all the experiments, we use the standard normal prior p0(rl) (i.e., v2 = 1) and the symmetric Dirichlet priors α = xC3 1, , = 0.01 x 1, where 1 is a vector with all entries being 1. For each setting, we report the average performance and the standard deviation with five randomly initialized runs. 4.1 Binary classification Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to distinguish postings of the newsgroup alt.atheism and those of the group talk.religion.misc. The training set contains 856 documents and the test set contains 569 documents. We compare the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted b</context>
</contexts>
<marker>Lacoste-Jullien, Sha, Jordan, 2009</marker>
<rawString>S. Lacoste-Jullien, F. Sha, and M.I. Jordan. 2009. DiscLDA: Discriminative learning for dimensionality reduction and classification. Advances in Neural Information Processing Systems (NIPS), pages 897– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lin</author>
</authors>
<title>A note on margin-based loss functions in classification.</title>
<date>2001</date>
<tech>Technical Report No. 1044.</tech>
<institution>University of Wisconsin.</institution>
<contexts>
<context position="11566" citStr="Lin, 2001" startWordPosition="1863" endWordPosition="1864">bs MedLDA (Zhu et al., 2013a) is another max-margin model that adopts the expected hinge loss as posterior regularization. As we shall see in the experiments, by using appropriate regularization constants, logistic supervised topic models achieve comparable performance as maxmargin methods. We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation techniques. It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additiona</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>Y. Lin. 2001. A note on margin-based loss functions in classification. Technical Report No. 1044. University of Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
</authors>
<title>PAC-Bayesian stochastic model selection.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<pages>51--5</pages>
<contexts>
<context position="8297" citStr="McAllester, 2003" startWordPosition="1313" endWordPosition="1314">is an indicator function that equals to 1 if predicate holds otherwise 0. If the classifier weights η and topic assignments z are given, the prediction rule is 9|,7,. = H(p(y = 1|η, z) &gt; 0.5) = H(η⊤z &gt; 0). (3) Since both η and Z are hidden variables, we propose to infer a posterior distribution q(η, Z) that has the minimal expected log-logistic loss R(q(η, Z)) = − � Eq[lo9 p(yd|η, zd)], (4) d which is a good surrogate loss for the expected misclassification loss, Ed EQ[ff(y|&apos;�Zd ≠ yd)], of a Gibbs classifier that randomly draws a model η from the posterior distribution and makes predictions (McAllester, 2003; Germain et al., 2009). In fact, this choice is motivated from the observation that logistic loss has been widely used as a convex surrogate loss for the misclassification &apos;A K-binary vector with only one entry equaling to 1. min q(®,Z,.b) 188 loss (Rosasco et al., 2004) in the task of fully observed binary classification. Also, note that the logistic classifier and the LDA likelihood are coupled by sharing the latent topic assignments z. The strong coupling makes it possible to learn a posterior distribution that can describe the observed words well and make accurate predictions. Regularized</context>
</contexts>
<marker>McAllester, 2003</marker>
<rawString>D. McAllester. 2003. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meyer</author>
<author>P Laud</author>
</authors>
<title>Predictive variable selection in generalized linear models.</title>
<date>2002</date>
<journal>Journal ofAmerican Statistical Association,</journal>
<volume>97</volume>
<issue>459</issue>
<contexts>
<context position="11915" citStr="Meyer and Laud, 2002" startWordPosition="1917" endWordPosition="1920">c loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation techniques. It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additional fully-factorized constraint q(η, O, Z, Φ) = q(η)(∏d q(θd) ∏n q(zdn)) ∏k q(Φk) and a variational approximation to the expectation of the log-logistic likelihood, which is intractable to compute directly. Note that the non-Bayesian treatment of η as unknown parameters in (Wang et al., 2009) results in an EM algorithm, which still needs to make str</context>
</contexts>
<marker>Meyer, Laud, 2002</marker>
<rawString>M. Meyer and P. Laud. 2002. Predictive variable selection in generalized linear models. Journal ofAmerican Statistical Association, 97(459):859–871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>10--1801</pages>
<contexts>
<context position="33122" citStr="Newman et al., 2009" startWordPosition="5580" endWordPosition="5583">el imbalance and a highly efficient Gibbs sampling algorithm without restricting assumptions on the posterior distributions by exploring the idea of data augmentation. The algorithm can also be parallelized. Empirical results for both binary and multi-class classification demonstrate significant improvements over the existing logistic supervised topic models. Our preliminary results with GraphLab have shown promise on parallelizing the Gibbs sampling algorithm. For future work, we plan to carry out more careful investigations, e.g., using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora. Moreover, the data augmentation technique can be applied to deal with other types of response variables, such as count data with a negative-binomial likelihood (Polson et al., 2012). Acknowledgments This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 100 101 102 103 train accuracy test accuracy K = 5 K = 10 K=20 burn−in iterations (a) accuracy 35 30 25 20 15 10 5 0 0 100 200 300 400 500 K = 5 K = 10 K=20 b</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>D. Newman, A. Asuncion, P. Smyth, and M. Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research (JMLR), (10):1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N G Polson</author>
<author>J G Scott</author>
<author>J Windle</author>
</authors>
<title>Bayesian inference for logistic models using Polya-Gamma latent variables.</title>
<date>2012</date>
<pages>1205--0310</pages>
<contexts>
<context position="3848" citStr="Polson et al., 2012" startWordPosition="573" endWordPosition="576">assification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data augmentation (Tanner and Wong, 1987; van Dyk and Meng, 2001; Holmes and Held, 2006). More specifically, we extend Polson’s method for Bayesian logistic regression (Polson et al., 2012) to the generalized logistic supervised topic models, which are much more challeng187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ing due to the presence of non-trivial latent variables. Technically, we introduce a set of PolyaGamma variables, one per document, to reformulate the generalized logistic pseudo-likelihood model (with the regularization parameter) as a scale mixture, where the mixture component is conditionally normal for classifier paramete</context>
<context position="11937" citStr="Polson et al., 2012" startWordPosition="1921" endWordPosition="1925">s has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation techniques. It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additional fully-factorized constraint q(η, O, Z, Φ) = q(η)(∏d q(θd) ∏n q(zdn)) ∏k q(Φk) and a variational approximation to the expectation of the log-logistic likelihood, which is intractable to compute directly. Note that the non-Bayesian treatment of η as unknown parameters in (Wang et al., 2009) results in an EM algorithm, which still needs to make strict mean-field assumpt</context>
<context position="13476" citStr="Polson et al., 2012" startWordPosition="2163" endWordPosition="2166">orithm for the generalized Bayesian logistic supervised topic models. min q(97,8,Z,4) min q(97,8,Z,4) ϕ(y, W) . 189 , 3.1 Formulation with Data Augmentation Since the logistic pseudo-likelihood ψ(y|Z, η) is not conjugate with normal priors, it is not easy to derive the sampling algorithms directly. Instead, we develop our algorithms by introducing auxiliary variables, which lead to a scale mixture of Gaussian components and analytic conditional distributions for automatical Bayesian inference without an accept/reject ratio. Our algorithm represents a first attempt to extend Polson’s approach (Polson et al., 2012) to deal with highly non-trivial Bayesian latent variable models. Let us first introduce the Polya-Gamma variables. Definition 1 (Polson et al., 2012) A random variable X has a Polya-Gamma distribution, denoted by X ∼P9(a, b), if gk (i − 1)2/2 + b2/(4π2), where a, b &gt; 0 and each gi ∼ 9(a,1) is an independent Gamma random variable. Let Wd = η⊤2d. Then, using the ideas of data augmentation (Tanner and Wong, 1987; Polson et al., 2012), we can show that the generalized pseudo-likelihood can be expressed as 1 F((yd|zd, η) = 2ce �dwdexp − λdωd 2 )p(λd|c, 0)dλd, where κd = c(yd−1/2) and Ad is a Polya</context>
<context position="17561" citStr="Polson et al., 2012" startWordPosition="2950" endWordPosition="2954">+ α) L δ(α) K ri k=1 D ri d=1 (× exp κdωd λdWd)]. dd)J 2 2 190 Algorithm 1 for collapsed Gibbs sampling 1: Initialization: set A = 1 and randomly draw zdn from a uniform distribution. 2: form= 1toMdo 3: draw a classifier from the distribution (8) 4: ford= 1toDdo 5: for each word n in document d do 6: draw the topic using distribution (9) 7: end for 8: draw Ad from distribution (10). 9: end for 10: end for which is a Polya-Gamma distribution. The equality has been achieved by using the construction definition of the general P9(a, b) class through an exponential tilting of the P9(a, 0) density (Polson et al., 2012). To draw samples from the Polya-Gamma distribution, we adopt the efficient method2 proposed in (Polson et al., 2012), which draws the samples through drawing samples from the closely related exponentially tilted Jacobi distribution. With the above conditional distributions, we can construct a Markov chain which iteratively draws samples of rl using Eq. (8), Z using Eq. (9) and A using Eq. (10), with an initial condition. In our experiments, we initially set A = 1 and randomly draw Z from a uniform distribution. In training, we run the Markov chain for M iterations (i.e., the burn-in stage), a</context>
<context position="23980" citStr="Polson et al., 2012" startWordPosition="4043" endWordPosition="4047">assification, one possible extension is to use a multinomial logistic regression model for categorical variables Y by using topic representations 2 as input features. However, it is non4The variational sLDA with a well-tuned c is significantly better than the standard sLDA, but a bit inferior to gSLDA+. trivial to develop a Gibbs sampling algorithm using the similar data augmentation idea, due to the presence of latent variables and the nonlinearity of the soft-max function. In fact, this is harder than the multinomial Bayesian logistic regression, which can be done via a coordinate strategy (Polson et al., 2012). Here, we apply the binary gSLDA to do the multi-class classification, following the “one-vs-all” strategy, which has been shown effective (Rifkin and Klautau, 2004), to provide some preliminary analysis. Namely, we learn 20 binary gSLDA models and aggregate their predictions by taking the most likely ones as the final predictions. We again evaluate two versions of gSLDA – the standard gSLDA with c = 1 and the improved gSLDA+ with a well-tuned c value. Since gSLDA is also insensitive to α and c for the multi-class task, we set α = 5.6 for both gSLDA and gSLDA+, and set c = 256 for gSLDA+. The</context>
<context position="33420" citStr="Polson et al., 2012" startWordPosition="5626" endWordPosition="5629">improvements over the existing logistic supervised topic models. Our preliminary results with GraphLab have shown promise on parallelizing the Gibbs sampling algorithm. For future work, we plan to carry out more careful investigations, e.g., using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora. Moreover, the data augmentation technique can be applied to deal with other types of response variables, such as count data with a negative-binomial likelihood (Polson et al., 2012). Acknowledgments This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 100 101 102 103 train accuracy test accuracy K = 5 K = 10 K=20 burn−in iterations (a) accuracy 35 30 25 20 15 10 5 0 0 100 200 300 400 500 K = 5 K = 10 K=20 burn−in iterations (b) training time Accuracy Train−time (seconds) 0.85 gSLDA+ 0.75 0.7 parallel−gSLDA+ K = 20 K = 30 K = 40 K = 50 0.5 10−1 100 101 102 103 0.8 0.65 0.6 0.55 burn−in iterations (a) accuracy 105 gSLDA+ 103 102 101 parallel−gSLDA+ 100 10−1 100 101 102 103 104 K = m K=30 K = 40 K=50 b</context>
</contexts>
<marker>Polson, Scott, Windle, 2012</marker>
<rawString>N.G. Polson, J.G. Scott, and J. Windle. 2012. Bayesian inference for logistic models using Polya-Gamma latent variables. arXiv:1205.0310v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rifkin</author>
<author>A Klautau</author>
</authors>
<title>In defense of onevs-all classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>5--101</pages>
<contexts>
<context position="24146" citStr="Rifkin and Klautau, 2004" startWordPosition="4069" endWordPosition="4072">ures. However, it is non4The variational sLDA with a well-tuned c is significantly better than the standard sLDA, but a bit inferior to gSLDA+. trivial to develop a Gibbs sampling algorithm using the similar data augmentation idea, due to the presence of latent variables and the nonlinearity of the soft-max function. In fact, this is harder than the multinomial Bayesian logistic regression, which can be done via a coordinate strategy (Polson et al., 2012). Here, we apply the binary gSLDA to do the multi-class classification, following the “one-vs-all” strategy, which has been shown effective (Rifkin and Klautau, 2004), to provide some preliminary analysis. Namely, we learn 20 binary gSLDA models and aggregate their predictions by taking the most likely ones as the final predictions. We again evaluate two versions of gSLDA – the standard gSLDA with c = 1 and the improved gSLDA+ with a well-tuned c value. Since gSLDA is also insensitive to α and c for the multi-class task, we set α = 5.6 for both gSLDA and gSLDA+, and set c = 256 for gSLDA+. The number of burn-in is set as M = 40, which is sufficiently large to get stable results, as we shall see. Fig. 2 shows the accuracy and training time. We can see that:</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>R. Rifkin and A. Klautau. 2004. In defense of onevs-all classification. Journal of Machine Learning Research (JMLR), (5):101–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rosasco</author>
<author>E De Vito</author>
<author>A Caponnetto</author>
<author>M Piana</author>
<author>A Verri</author>
</authors>
<title>Are loss functions all the same? Neural Computation,</title>
<date>2004</date>
<marker>Rosasco, De Vito, Caponnetto, Piana, Verri, 2004</marker>
<rawString>L. Rosasco, E. De Vito, A. Caponnetto, M. Piana, and A. Verri. 2004. Are loss functions all the same? Neural Computation, (16):1063–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smola</author>
<author>S Narayanamurthy</author>
</authors>
<title>An architecture for parallel topic models.</title>
<date>2010</date>
<booktitle>Very Large Data Base (VLDB),</booktitle>
<pages>3--1</pages>
<contexts>
<context position="33155" citStr="Smola and Narayanamurthy, 2010" startWordPosition="5584" endWordPosition="5587">ghly efficient Gibbs sampling algorithm without restricting assumptions on the posterior distributions by exploring the idea of data augmentation. The algorithm can also be parallelized. Empirical results for both binary and multi-class classification demonstrate significant improvements over the existing logistic supervised topic models. Our preliminary results with GraphLab have shown promise on parallelizing the Gibbs sampling algorithm. For future work, we plan to carry out more careful investigations, e.g., using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora. Moreover, the data augmentation technique can be applied to deal with other types of response variables, such as count data with a negative-binomial likelihood (Polson et al., 2012). Acknowledgments This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 1.05 1 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 100 101 102 103 train accuracy test accuracy K = 5 K = 10 K=20 burn−in iterations (a) accuracy 35 30 25 20 15 10 5 0 0 100 200 300 400 500 K = 5 K = 10 K=20 burn−in iterations (b) training ti</context>
</contexts>
<marker>Smola, Narayanamurthy, 2010</marker>
<rawString>A. Smola and S. Narayanamurthy. 2010. An architecture for parallel topic models. Very Large Data Base (VLDB), 3(1-2):703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Tanner</author>
<author>W-H Wong</author>
</authors>
<title>The calculation of posterior distributions by data augmentation.</title>
<date>1987</date>
<journal>Journal of the Americal Statistical Association (JASA),</journal>
<volume>82</volume>
<issue>398</issue>
<contexts>
<context position="3699" citStr="Tanner and Wong, 1987" startWordPosition="550" endWordPosition="553">ng an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data augmentation (Tanner and Wong, 1987; van Dyk and Meng, 2001; Holmes and Held, 2006). More specifically, we extend Polson’s method for Bayesian logistic regression (Polson et al., 2012) to the generalized logistic supervised topic models, which are much more challeng187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ing due to the presence of non-trivial latent variables. Technically, we introduce a set of PolyaGamma variables, one per document, to reformulate the generalized logistic pseudo</context>
<context position="13889" citStr="Tanner and Wong, 1987" startWordPosition="2237" endWordPosition="2240"> components and analytic conditional distributions for automatical Bayesian inference without an accept/reject ratio. Our algorithm represents a first attempt to extend Polson’s approach (Polson et al., 2012) to deal with highly non-trivial Bayesian latent variable models. Let us first introduce the Polya-Gamma variables. Definition 1 (Polson et al., 2012) A random variable X has a Polya-Gamma distribution, denoted by X ∼P9(a, b), if gk (i − 1)2/2 + b2/(4π2), where a, b &gt; 0 and each gi ∼ 9(a,1) is an independent Gamma random variable. Let Wd = η⊤2d. Then, using the ideas of data augmentation (Tanner and Wong, 1987; Polson et al., 2012), we can show that the generalized pseudo-likelihood can be expressed as 1 F((yd|zd, η) = 2ce �dwdexp − λdωd 2 )p(λd|c, 0)dλd, where κd = c(yd−1/2) and Ad is a Polya-Gamma variable with parameters a = c and b = 0. This result indicates that the posterior distribution of the generalized Bayesian logistic supervised topic models, i.e., q(η, O, Z, 4)), can be expressed as the marginal of a higher dimensional distribution that includes the augmented variables λ. The complete posterior distribution is q(η, λ, Θ, Z, Φ) = p0(η, Θ, Z, Φ)p(W|Z, Φ)ϕ(y, λ|Z, η) ψ(y, W) where the pse</context>
</contexts>
<marker>Tanner, Wong, 1987</marker>
<rawString>M.A. Tanner and W.-H. Wong. 1987. The calculation of posterior distributions by data augmentation. Journal of the Americal Statistical Association (JASA), 82(398):528–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D van Dyk</author>
<author>X Meng</author>
</authors>
<title>The art of data augmentation.</title>
<date>2001</date>
<journal>Journal of Computational and Graphical Statistics (JCGS),</journal>
<volume>10</volume>
<issue>1</issue>
<marker>van Dyk, Meng, 2001</marker>
<rawString>D. van Dyk and X. Meng. 2001. The art of data augmentation. Journal of Computational and Graphical Statistics (JCGS), 10(1):1–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D M Blei</author>
<author>F F Li</author>
</authors>
<title>Simultaneous image classification and annotation.</title>
<date>2009</date>
<booktitle>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="1292" citStr="Wang et al., 2009" startWordPosition="171" endWordPosition="174">two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency. 1 Introduction As widely adopted in supervised latent Dirichlet allocation (sLDA) models (Blei and McAuliffe, 2010; Wang et al., 2009), one way to improve the predictive power of LDA is to define a likelihood model for the widely available documentlevel response variables, in addition to the likelihood model for document words. For example, the logistic likelihood model is commonly used for binary or multinomial responses. By imposing some priors, posterior inference is done with the Bayes’ rule. Though powerful, one issue that could limit the use of existing logistic supervised LDA models is that they treat the document-level response variable as one additional word via a normalized likelihood model. Although some special t</context>
<context position="12092" citStr="Wang et al., 2009" startWordPosition="1947" endWordPosition="1950">allenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting. 2.2 Variational Approximation Algorithms The commonly used normal prior for η is nonconjugate to the logistic likelihood, which makes the posterior inference hard. Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012). Previous algorithms to solve problem (5) rely on variational approximation techniques. It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additional fully-factorized constraint q(η, O, Z, Φ) = q(η)(∏d q(θd) ∏n q(zdn)) ∏k q(Φk) and a variational approximation to the expectation of the log-logistic likelihood, which is intractable to compute directly. Note that the non-Bayesian treatment of η as unknown parameters in (Wang et al., 2009) results in an EM algorithm, which still needs to make strict mean-field assumptions together with a variational bound of the expectation of the log-logistic likelihood. In this paper, we consider the full Bayesian treatment, which can</context>
<context position="20410" citStr="Wang et al., 2009" startWordPosition="3440" endWordPosition="3443">ing, we report the average performance and the standard deviation with five randomly initialized runs. 4.1 Binary classification Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to distinguish postings of the newsgroup alt.atheism and those of the group talk.religion.misc. The training set contains 856 documents and the test set contains 569 documents. We compare the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang et al., 2012). We also include the unsupervised LDA using collapsed Gibbs sampling as a baseline, denoted by gLDA. For gLDA, we learn a binary linear SVM on its topic representations using SVMLight (Joachims, 1999). The results of DiscLDA (Lacoste-Jullien et al., 2009) and linear SVM on raw bag-of-words features were reported in (Zhu et al., 2012). For gSLDA, we compare two versions – the standard sLDA with c</context>
</contexts>
<marker>Wang, Blei, Li, 2009</marker>
<rawString>C. Wang, D.M. Blei, and Li F.F. 2009. Simultaneous image classification and annotation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>N Chen</author>
<author>E P Xing</author>
</authors>
<title>Infinite latent SVM for classification and multi-task learning.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>1620--1628</pages>
<contexts>
<context position="3048" citStr="Zhu et al., 2011" startWordPosition="448" endWordPosition="451">are no longer conjugate to the logistic likelihood and thus lead to hard inference problems. Existing approaches rely on variational approximation techniques which normally make strict mean-field assumptions. To address the above issues, we present two improvements. First, we present a general framework of Bayesian logistic supervised topic models with a regularization parameter to better balance response variables and words. Technically, instead of doing standard Bayesian inference via Bayes’ rule, which requires a normalized likelihood model, we propose to do regularized Bayesian inference (Zhu et al., 2011; Zhu et al., 2013b) via solving an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring th</context>
</contexts>
<marker>Zhu, Chen, Xing, 2011</marker>
<rawString>J. Zhu, N. Chen, and E.P. Xing. 2011. Infinite latent SVM for classification and multi-task learning. In Advances in Neural Information Processing Systems (NIPS), pages 1620–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>MedLDA: maximum margin supervised topic models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>13</volume>
<pages>2278</pages>
<contexts>
<context position="18527" citStr="Zhu et al., 2012" startWordPosition="3115" endWordPosition="3118">g Eq. (8), Z using Eq. (9) and A using Eq. (10), with an initial condition. In our experiments, we initially set A = 1 and randomly draw Z from a uniform distribution. In training, we run the Markov chain for M iterations (i.e., the burn-in stage), as outlined in Algorithm 1. Then, we draw a sample rl� as the final classifier to make predictions on testing data. As we shall see, the Markov chain converges to stable prediction performance with a few burn-in iterations. 3.3 Prediction To apply the classifier rl� on testing data, we need to infer their topic assignments. We take the approach in (Zhu et al., 2012; Jiang et al., 2012), which uses a point estimate of topics 4) from training data and makes prediction based on them. Specifically, we use the MAP estimate 4 to replace the probability distribution p(4)). For the Gibbs sampler, an estimate of 4 using the samples is Okt OC Ctk + βt. Then, given a testing document w, we infer its latent components z using 4) as p(zn = k|z¬n) OC &amp;. (Ck¬n + αk), where 2The basic sampler was implemented in the R package BayesLogit. We implemented the sampling algorithm in C++ together with our topic model sampler. Ck¬n is the times that the terms in this document </context>
<context position="19999" citStr="Zhu et al., 2012" startWordPosition="3378" endWordPosition="3381"> data set, which contains about 20,000 postings within 20 news groups. We follow the same setting as in (Zhu et al., 2012) and remove a standard list of stop words for both binary and multiclass classification. For all the experiments, we use the standard normal prior p0(rl) (i.e., v2 = 1) and the symmetric Dirichlet priors α = xC3 1, , = 0.01 x 1, where 1 is a vector with all entries being 1. For each setting, we report the average performance and the standard deviation with five randomly initialized runs. 4.1 Binary classification Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to distinguish postings of the newsgroup alt.atheism and those of the group talk.religion.misc. The training set contains 856 documents and the test set contains 569 documents. We compare the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang e</context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2012</marker>
<rawString>J. Zhu, A. Ahmed, and E.P. Xing. 2012. MedLDA: maximum margin supervised topic models. Journal of Machine Learning Research (JMLR), (13):2237– 2278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>N Chen</author>
<author>H Perkins</author>
<author>B Zhang</author>
</authors>
<title>Gibbs max-margin topic models with fast sampling algorithms.</title>
<date>2013</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="3066" citStr="Zhu et al., 2013" startWordPosition="452" endWordPosition="455">ugate to the logistic likelihood and thus lead to hard inference problems. Existing approaches rely on variational approximation techniques which normally make strict mean-field assumptions. To address the above issues, we present two improvements. First, we present a general framework of Bayesian logistic supervised topic models with a regularization parameter to better balance response variables and words. Technically, instead of doing standard Bayesian inference via Bayes’ rule, which requires a normalized likelihood model, we propose to do regularized Bayesian inference (Zhu et al., 2011; Zhu et al., 2013b) via solving an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data au</context>
<context position="10983" citStr="Zhu et al., 2013" startWordPosition="1770" endWordPosition="1773">This imbalance was noticed in (Halpern et al., 2012). We will see that c can make a big difference later. Comparison with MedLDA: The above formulation of logistic supervised topic models as an instance of regularized Bayesian inference provides a direct comparison with the max-margin supervised topic model (MedLDA) (Jiang et al., 2012), which has the same form of the optimization problems. The difference lies in the posterior regularization, for which MedLDA uses a hinge loss of an expected classifier while the logistic supervised topic model uses an expected log-logistic loss. Gibbs MedLDA (Zhu et al., 2013a) is another max-margin model that adopts the expected hinge loss as posterior regularization. As we shall see in the experiments, by using appropriate regularization constants, logistic supervised topic models achieve comparable performance as maxmargin methods. We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic mod</context>
</contexts>
<marker>Zhu, Chen, Perkins, Zhang, 2013</marker>
<rawString>J. Zhu, N. Chen, H. Perkins, and B. Zhang. 2013a. Gibbs max-margin topic models with fast sampling algorithms. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>N Chen</author>
<author>E P Xing</author>
</authors>
<title>Bayesian inference with posterior regularization and applications to infinite latent svms.</title>
<date>2013</date>
<pages>1210--1766</pages>
<contexts>
<context position="3066" citStr="Zhu et al., 2013" startWordPosition="452" endWordPosition="455">ugate to the logistic likelihood and thus lead to hard inference problems. Existing approaches rely on variational approximation techniques which normally make strict mean-field assumptions. To address the above issues, we present two improvements. First, we present a general framework of Bayesian logistic supervised topic models with a regularization parameter to better balance response variables and words. Technically, instead of doing standard Bayesian inference via Bayes’ rule, which requires a normalized likelihood model, we propose to do regularized Bayesian inference (Zhu et al., 2011; Zhu et al., 2013b) via solving an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood. The general formulation subsumes standard sLDA as a special case. Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data au</context>
<context position="10983" citStr="Zhu et al., 2013" startWordPosition="1770" endWordPosition="1773">This imbalance was noticed in (Halpern et al., 2012). We will see that c can make a big difference later. Comparison with MedLDA: The above formulation of logistic supervised topic models as an instance of regularized Bayesian inference provides a direct comparison with the max-margin supervised topic model (MedLDA) (Jiang et al., 2012), which has the same form of the optimization problems. The difference lies in the posterior regularization, for which MedLDA uses a hinge loss of an expected classifier while the logistic supervised topic model uses an expected log-logistic loss. Gibbs MedLDA (Zhu et al., 2013a) is another max-margin model that adopts the expected hinge loss as posterior regularization. As we shall see in the experiments, by using appropriate regularization constants, logistic supervised topic models achieve comparable performance as maxmargin methods. We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007). But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic mod</context>
</contexts>
<marker>Zhu, Chen, Xing, 2013</marker>
<rawString>J. Zhu, N. Chen, and E.P. Xing. 2013b. Bayesian inference with posterior regularization and applications to infinite latent svms. arXiv:1210.1766v2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>