<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000252">
<title confidence="0.988214">
The Feature Subspace Method for SMT System Combination
</title>
<author confidence="0.99974">
Nan Duan1, Mu Li2, Tong Xiao3, Ming Zhou2
</author>
<affiliation confidence="0.822421">
1Tianjin University 2Microsoft Research Asia 3Northeastern University
Tianjin, China Beijing, China Shenyang, China
</affiliation>
<email confidence="0.992962">
{v-naduan,muli,v-toxiao,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.997313" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998695684210526">
Recently system combination has been shown
to be an effective way to improve translation
quality over single machine translation sys-
tems. In this paper, we present a simple and ef-
fective method to systematically derive an en-
semble of SMT systems from one baseline li-
near SMT model for use in system combina-
tion. Each system in the resulting ensemble is
based on a feature set derived from the fea-
tures of the baseline model (typically a subset
of it). We will discuss the principles to deter-
mine the feature sets for derived systems, and
present in detail the system combination mod-
el used in our work. Evaluation is performed
on the data sets for NIST 2004 and NIST 2005
Chinese-to-English machine translation tasks.
Experimental results show that our method can
bring significant improvements to baseline
systems with state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99914096969697">
Research on Statistical Machine Translation
(SMT) has shown substantial progress in recent
years. Since the success of phrase-based methods
(Och and Ney, 2004; Koehn, 2004), models
based on formal syntax (Chiang, 2005) or lin-
guistic syntax (Liu et al., 2006; Marcu et al.,
2006) have also achieved state-of-the-art perfor-
mance. As a result of the increasing numbers of
available machine translation systems, studies on
system combination have been drawing more and
more attention in SMT research.
There have been many successful attempts to
combine outputs from multiple machine transla-
tion systems to further improve translation quali-
ty. A system combination model usually takes n-
best translations of single systems as input, and
depending on the combination strategy, different
methods can be used. Sentence-level combina-
tion methods directly select hypotheses from
original outputs of single SMT systems (Sim et
al., 2007; Hildebrand and Vogel, 2008), while
phrase-level or word–level combination methods
are more complicated and could produce new
translations different from any translations in the
input (Bangalore et al., 2001; Jayaraman and La-
vie, 2005; Matusov et al., 2006; Sim et al.,
2007).
Among all the factors contributing to the suc-
cess of system combination, there is no doubt
that the availability of multiple machine transla-
tion systems is an indispensable premise. Al-
though various approaches to SMT system com-
bination have been explored, including enhanced
combination model structure (Rosti et al., 2007),
better word alignment between translations
(Ayan et al., 2008; He et al., 2008) and improved
confusion network construction (Rosti et al.,
2008), most previous work simply used the en-
semble of SMT systems based on different mod-
els and paradigms at hand and did not tackle the
issue of how to obtain the ensemble in a prin-
cipled way. To our knowledge the only work
discussed this problem is Macherey and Och
(2007), in which they experimented with build-
ing different SMT systems by varying one or
more sub-models (i.e. translation model or dis-
tortion model) of an existing SMT system, and
observed that changes in early-stage model train-
ing introduced most diversities in translation
outputs.
In this paper, we address the problem of build-
ing an ensemble of diversified machine transla-
tion systems from a single translation engine for
system combination. In particular, we propose a
novel Feature Subspace method for the ensemble
construction based on any baseline SMT model
which can be formulated as a standard linear
function. Each system within the ensemble is
based on a group of features directly derived
from the baseline model with minimal efforts
(which is typically a subset of the features used
in the baseline model), and the resulting system
is optimized in the derived feature space accor-
dingly.
We evaluated our method on the test sets for
NIST 2004 and NIST 2005 Chinese-to-English
</bodyText>
<page confidence="0.946341">
1096
</page>
<note confidence="0.9965835">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1096–1104,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9998348">
machine translation tasks using two baseline
SMT systems with state-of-the-art performance.
Experimental results show that the feature sub-
space method can bring significant improve-
ments to both baseline systems.
The rest of the paper is organized as follows.
The motivation of our work is described on Sec-
tion 2. In Section 3, we first give a detailed de-
scription about feature subspace method, includ-
ing the principle to select subspaces from all
possible options, and then an n-gram consensus –
based sentence-level system combination method
is presented. Experimental results are given in
Section 4. Section 5 discusses some related is-
sues and concludes the paper.
</bodyText>
<sectionHeader confidence="0.997531" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999977166666667">
Our motivations for this work can be described
in the following two aspects.
The first aspect is related to the cost of build-
ing single systems for system combination. In
previous work, the SMT systems used in combi-
nation differ mostly in two ways. One is the un-
derlying models adopted by individual systems.
For example, using an ensemble of systems re-
spectively based on phrase-based models, hierar-
chical models or even syntax-based models is a
common practice. The other is the methods used
for feature function estimation such as using dif-
ferent word alignment models, language models
or distortion models. For the first solution, build-
ing a new SMT system with different methodol-
ogy is by no means an easy task even for an ex-
perienced SMT researcher, because it requires
not only considerable effects to develop but also
plenty of time to accumulate enough experiences
to fine tune the system. For the second alterna-
tive, usually it requires time-consuming re-
training for word alignment or language models.
Also some of the feature tweaking in this solu-
tion is system or language specific, thus for any
new systems or language pairs, human engineer-
ing has to be involved. For example, using dif-
ferent word segmentation methods for Chinese
can generate different word alignment results,
and based on which a new SMT system can be
built. Although this may be useful to combina-
tion of Chinese-to-English translation, it is not
applicable to most of other language pairs.
Therefore it will be very helpful if there is a
light-weight method that enables the SMT sys-
tem ensemble to be systematically constructed
based on an existing SMT system.
</bodyText>
<table confidence="0.99659775">
Source 中国 最大 规模 的 海水 淡化工程 落户 舟山
sentence
Ref China&apos;s largest sea water desalini-
translation zation project settles in Zhoushan
Default China &apos;s largest desalination
translation project in Zhoushan
FS_PEF China &apos;s largest sea water
translation desalination project in Zhoushan
</table>
<tableCaption confidence="0.990187333333333">
Table 1: An example of translations generated
from the same decoder but with different feature
settings.
</tableCaption>
<table confidence="0.99763575">
Chinese English p(elf)
1 海水 淡化 desalination 0.4000
2 海水 sea water 0.1748
3 淡化 desalination 0.0923
</table>
<tableCaption confidence="0.961697">
Table 2: Parameters of related phrases for exam-
ples in Table 1.
</tableCaption>
<bodyText confidence="0.999426878787879">
The second aspect motivating our work comes
from the subspace learning method in machine
learning literature (Ho, 1998), in which an en-
semble of classifiers are trained on subspaces of
the full feature space, and final classification re-
sults are based on the vote of all classifiers in the
ensemble. Lopez and Resnik (2006) also showed
that feature engineering could be used to over-
come deficiencies of poor alignment. To illu-
strate the usefulness of feature subspace in the
SMT task, we start with the example shown in
Table 1. In the example, the Chinese source sen-
tence is translated with two settings of a hierar-
chical phrase-based system (Chiang, 2005). In
the default setting all the features are used as
usual in the decoder, and we find that the transla-
tion of the Chinese word 海水 (sea water) is
missing in the output. This can be explained with
the data shown in Table 2. Because of noises and
word alignment errors in the parallel training
data, the inaccurate translation phrase
海水 淡化 =* desalination is assigned with a
high value of the phrase translation probability
feature p (e |f). Although the correct translation
can also be composed by two phrases 海水 =*
sea water and 淡化 =* desalination, its over-
all translation score cannot beat the incorrect one
because the combined phrase translation proba-
bility of these two phrases are much smaller
than p(desalination|海水 淡化) . However, if
we intentionally remove the p (e |f) feature from
the model, the preferred translation can be gener-
ated as shown in the result of FS_PEF because in
</bodyText>
<page confidence="0.991849">
1097
</page>
<bodyText confidence="0.999938882352941">
this way the bad estimation of p(e|f) for this
phrase is avoided.
This example gives us the hint that building
decoders based on subspaces of a standard model
could help with working around some negative
impacts of inaccurate estimations of feature val-
ues for some input sentences. The subspace-
based systems are expected to work similarly to
statistical classifiers trained on subspaces of a
full feature space – though the overall accuracy
of baseline system might be better than any indi-
vidual systems, for a specific sentence some in-
dividual systems could generate better transla-
tions. It is expected that employing an ensemble
of subspace-based systems and making use of
consensus between them will outperform the
baseline system.
</bodyText>
<sectionHeader confidence="0.9665095" genericHeader="method">
3 Feature Subspace Method for SMT
System Ensemble Construction
</sectionHeader>
<bodyText confidence="0.99901725">
In this section, we will present in detail the me-
thod for systematically deriving SMT systems
from a standard linear SMT model based on fea-
ture subspaces for system combination.
</bodyText>
<subsectionHeader confidence="0.998768">
3.1 SMT System Ensemble Generation
</subsectionHeader>
<bodyText confidence="0.9999808">
Nowadays most of the state-of-the-art SMT sys-
tems are based on linear models as proposed in
Och and Ney (2002). Let hm (f, e) be a feature
function, and Am be its weight, an SMT model D
can be formally written as:
</bodyText>
<equation confidence="0.973199333333333">
e* = argmax
e
m
</equation>
<bodyText confidence="0.999969289855073">
Noticing that Equation (1) is a general formu-
lation independent of any specific features, tech-
nically for any subset of features used in D, a
new SMT system can be constructed based on it,
which we call a sub-system.
Next we will use SZ to denote the full feature
space defined by the entire set of features used
in D, and s S SZ is a feature subset that belongs
to p (SZ), the power set of SZ. The derived sub-
system based on subset s S SZ is denoted by ds.
Although in theory we can use all the sub-
systems derived from every feature subset
in p (SZ), it is still desirable to use only some of
them in practice. The reasons for this are two-
fold. First, the number of possible sub-systems
(2 1SZ 1) is exponential to the size of SZ. Even when
the number of features in SZ is relatively small,
i.e. 10, there will be up to 1024 sub-systems in
total, which is a large number for combination
task. Larger feature sets will make the system
combination practically infeasible. Second, not
every sub-system could contribute to the system
combination. For example, feature subsets only
containing very small number of features will
lead to sub-systems with very poor performance;
and the language model feature is too important
to be ignored for a sub-system to achieve reason-
ably good performance.
In our work, we only consider feature sub-
spaces with only one difference from the features
in SZ. For each non- language model feature hi, a
sub-system di is built by removing hi from SZ.
Allowing for the importance of the language
model (LM) feature to an SMT model, we do not
remove any LM feature from any sub-system.
Instead, we try to weaken the strength of a LM
feature by lowering its n-gram order. For exam-
ple, if a 4-gram language model is used in the
baseline system D, then a trigram model can be
used in one sub-system, and a bigram model can
be used in another. In this way more than one
sub-system can be derived based on one LM fea-
ture. When varying a language model feature, the
one-feature difference principle is still kept: if
we lower the order of a language model feature,
no other features are removed or changed.
The remaining issue of using weakened LM
features is that the resulting ensemble is no long-
er strictly based on subspace of SZ. However, this
theoretical imperfection can be remedied by in-
troducing SZ′ , a super-space of SZ to include all
lower-order LM features. In this way, an aug-
mented baseline system D′ can be built based
on SZ′ , and the baseline system D itself can also
be viewed as a sub-system of D′. We will show
in the experimental section that D′ actually per-
forms even slightly better than the original base-
line system D, but results of sub-system combi-
nation are significantly better that both D and D′ .
After the sub-system ensemble is constructed,
each sub-system tunes its feature weights inde-
pendently to optimize the evaluation metrics on
the development set.
Let D = {d1, ..., d,} be the set of sub-systems
obtained by either removing one non-LM feature
or changing the order of a LM feature, and xi be
the n-best list produced by di. Then x(D), the
translation candidate pool to the system combi-
nation model can be written as:
</bodyText>
<equation confidence="0.9656405">
x(D) = U xi (2)
i
</equation>
<bodyText confidence="0.9560995">
The advantage of this method is that it allows
us to systematically build an ensemble of SMT
systems at a very low cost. From the decoding
Am hm (f , e) (1)
</bodyText>
<page confidence="0.812744">
1098
</page>
<bodyText confidence="0.999981785714286">
perspective, all the sub-systems share a common
decoder, with minimal extensions to the baseline
systems to support the use of specified subset of
feature functions to compute the overall score for
translation hypotheses. From the model training
perspective, all the non-LM feature functions can
be estimated once for all sub-systems. The only
exception is the language model feature, which
may be of different values across multiple sub-
systems. However, since lower-order models
have already been contained in higher-order
model for the purpose of smoothing in almost all
statistical language model implementations, there
is also no extra training cost.
</bodyText>
<subsectionHeader confidence="0.99838">
3.2 System Combination Scheme
</subsectionHeader>
<bodyText confidence="0.9996615">
In our work, we use a sentence-level system
combination model to select best translation hy-
pothesis from the candidate pool ℋ(𝒟) . This
method can also be viewed to be a hypotheses re-
ranking model since we only use the existing
translations instead of performing decoding over
a confusion network as done in the word-level
combination method (Rosti et al., 2007).
The score function in our combination model
is formulated as follows:
tions, we also introduce a set of n-gram disa-
greement features in the combination model:
</bodyText>
<equation confidence="0.998632666666667">
𝑕𝑛−(𝑒,ℋ 𝒟 ) = ( 𝑒 − 𝑛+ 1 − 𝐺𝑛(𝑒,𝑒′))
𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒
(6)
</equation>
<bodyText confidence="0.997172210526316">
Because each order of n-gram match introduc-
es two features, the total number of features in
the combination model will be 2𝑚 + 2 if 𝑚 or-
ders of n-gram are to be matched in computing
𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scor-
ing function in Equation (3), the feature weights
of our combination model can also be tuned on a
development data set to optimize the specified
evaluation metrics using the standard Minimum
Error Rate Training (MERT) algorithm (Och
2003).
Our method is similar to the work proposed by
Hildebrand and Vogel (2008). However, except
the language model and translation length, we
only use intra-hypothesis n-gram agreement fea-
tures as Hildebrand and Vogel did and use addi-
tional intra-hypothesis n-gram disagreement fea-
tures as Li et al. (2009) did in their co-decoding
method.
</bodyText>
<equation confidence="0.873082666666667">
𝑒∗ = 𝑎𝑟𝑔𝑚𝑎𝑥 𝜆𝐿𝑀𝑕𝐿𝑀 𝑒 + 𝜆𝑙𝐿 + 𝜓(𝑒, ℋ(𝒟)) 4 Experiments
𝑒 ∈ℋ 𝒟
(3)
</equation>
<bodyText confidence="0.99996425">
where 𝑕𝐿𝑀 𝑒 is the language model score for 𝑒,
𝐿 is the length of 𝑒, and 𝜓(𝑒, ℋ(𝒟)) is a transla-
tion consensus –based scoring function. The
computation of 𝜓 (𝑒, ℋ(𝒟)) is further decom-
posed into weighted linear combination of a set
of n-gram consensus –based features, which are
defined in terms of the order of n-gram to be
matched between current candidate and other
translation in ℋ(𝒟).
Given a translation candidate 𝑒, the n-gram
agreement feature between 𝑒 and other transla-
tions in the candidate pool is defined as:
</bodyText>
<equation confidence="0.9996115">
𝑕𝑛+(𝑒,ℋ 𝒟 ) = 𝐺𝑛 𝑒,𝑒′ (4)
𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒
</equation>
<bodyText confidence="0.999566142857143">
where the function 𝐺𝑛 𝑒, 𝑒′ counts the occur-
rences of n-grams of 𝑒 in 𝑒′:
Here 𝛿(∙,∙) is the indicator function -
𝛿 𝑒𝑖 𝑖+𝑛−1,𝑒′ is 1 when the n-gram 𝑒𝑖 𝑖+𝑛−1 ap-
pears in 𝑒′
In order to give the combination model an op-
portunity to penalize long but inaccurate transla-
</bodyText>
<subsectionHeader confidence="0.961747">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.998280823529412">
Experiments were conducted on the NIST evalu-
ation sets of 2004 (MT04) and 2005 (MT05) for
Chinese-to-English translation tasks. Both corpo-
ra provide 4 reference translations per source
sentence. Parameters were tuned with MERT
algorithm (Och, 2003) on the NIST evaluation
set of 2003 (MT03) for both the baseline systems
and the system combination model. Translation
performance was measured in terms of case-
insensitive NIST version of BLEU score which
computes the brevity penalty using the shortest
reference translation for each segment, and all
the results will be reported in percentage num-
bers. Statistical significance is computed using
the bootstrap re-sampling method proposed by
Koehn (2004). Statistics of the data sets are
summarized in Table 3.
</bodyText>
<table confidence="0.99976575">
Data set #Sentences #Words
MT03 (dev) 919 23,782
MT04 (test) 1,788 47,762
MT05 (test) 1,082 29,258
</table>
<tableCaption confidence="0.999121">
Table 3: Data set statistics.
</tableCaption>
<equation confidence="0.9352908">
𝑒 −𝑛+1
𝐺𝑛 𝑒, 𝑒′ = 𝛿(𝑒𝑖𝑖+𝑛−1, 𝑒′)
𝑖=1
(5)
, otherwise it is 0.
</equation>
<page confidence="0.969694">
1099
</page>
<bodyText confidence="0.999876352941176">
We use the parallel data available for the
NIST 2008 constrained track of Chinese-to-
English machine translation task as bilingual
training data, which contains 5.1M sentence
pairs, 128M Chinese words and 147M English
words after pre-processing. GIZA++ toolkit (Och
and Ney, 2003) is used to perform word align-
ment in both directions with default settings, and
the intersect-diag-grow method is used to gener-
ate symmetric word alignment refinement. The
language model used for all systems is a 5-gram
model trained with the English part of bilingual
data and Xinhua portion of LDC English Giga-
word corpus version 3. In experiments, multiple
language model features with the order ranging
from 2 to 5 can be easily obtained from the 5-
gram one without retraining.
</bodyText>
<subsectionHeader confidence="0.952604">
4.2 System Description
</subsectionHeader>
<bodyText confidence="0.98933984">
Theoretically our method is applicable to all li-
near model –based SMT systems. In our experi-
ments, two in-house developed systems are used
to validate our method. The first one (SYS1) is a
system based on the hierarchical phrase-based
model as proposed in (Chiang, 2005). Phrasal
rules are extracted from all bilingual sentence
pairs, while hierarchical rules with variables are
extracted from selected data sets including
LDC2003E14, LDC2003E07, LDC2005T06 and
LDC2005T10, which contain around 350,000
sentence pairs, 8.8M Chinese words and 10.3M
English words. The second one (SYS2) is a re-
implementation of a phrase-based decoder with
lexicalized reordering model based on maximum
entropy principle proposed by Xiong et al.
(2006). All bilingual data are used to extract
phrases up to length 3 on the source side.
In following experiments, we only consider
removing common features shared by both base-
line systems for feature subspace generation.
Rule penalty feature and lexicalized reordering
feature, which are particular to SYS1 and SYS2,
are not used. We list the features in consideration
as follows:
</bodyText>
<listItem confidence="0.999443">
• PEF and PFE: phrase translation probabili-
ties p(e If) and p (f Ie)
• PEFLEX and PFELEX: lexical weights
plex (e If) and plex (f I e)
• PP: phrase penalty
• WP: word penalty
• BLP: bi-lexicon pair counting how many
entries of a conventional lexicon co-
occurring in a given translation pair
• LM-n: language model with order n
</listItem>
<bodyText confidence="0.999054">
Based on the principle described in Section
3.1, we generate a number of feature subspaces
for each baseline system as follows:
</bodyText>
<listItem confidence="0.89318325">
• For non-LM features (PEF, PFE, PEFLEX,
PFELEX, PP, WP and BLP), we remove one
of them from the full feature space each
time. Thus 7 feature subspaces are generated,
which are denoted as FS−PEF , FS−PFE ,
FS−PEFLEX , FS−PFELEX , FS−PP, FS−WP and
FS−BLP respectively. The 5-gram LM feature
is used in each of them.
• For LM features (LM-n), we change the or-
der from 2 to 5 with all the other non-LM
features present. Thus 4 LM-related feature
subspaces are generated, which are denoted
</listItem>
<bodyText confidence="0.997663277777778">
as FSLM−2, FSLM−3, FSLM−4 and FSLM−5 re-
spectively. FSLM−5 is essentially the full fea-
ture space of baseline system.
For each baseline system, we construct a total
of 11 sub-systems by using above feature sub-
spaces. The baseline system is also contained
within them because of using FSLM−5. We call
all sub-systems are non-baseline sub-systems
except the one derived by using FSLM−5.
By default, the beam size of 60 is used for all
systems in our experiments. The size of n-best
list is set to 20 for each sub-system, and for base-
line systems, this size is set to 220, which equals
to the size of the combined n-best list generated
by total 11 sub-systems. The order of n-gram
agreement and disagreement features used in
sentence-level combination model ranges from
unigram to 4-gram.
</bodyText>
<subsectionHeader confidence="0.999412">
4.3 Evaluation of Oracle Translations
</subsectionHeader>
<bodyText confidence="0.9999647">
We first evaluate the oracle performance on the
n-best lists of baseline systems and on the com-
bined n-best lists of sub-systems generated from
each baseline system.
The oracle translations are obtained by using
the metric of sentence-level BLEU score (Ye et
al., 2007). Table 4 shows the evaluation results,
in which Baseline stands for baseline system
with a 5-gram LM feature, and FS stands for 11
sub-systems derived from the baseline system.
</bodyText>
<table confidence="0.995726333333333">
SYS1 SYS2
BLEU/TER BLEU/TER
MT04 Baseline 49.68/0.6411 49.50/0.6349
FS 51.05/0.6089 50.53/0.6056
MT05 Baseline 48.89/0.5946 48.37/0.5944
FS 50.69/0.5695 49.81/0.5684
</table>
<tableCaption confidence="0.9647335">
Table 4: Oracle BLEU and TER scores on base-
line systems and their generated sub-systems.
</tableCaption>
<page confidence="0.993008">
1100
</page>
<bodyText confidence="0.9995665">
For both SYS1 and SYS2, feature subspace
method achieves higher oracle BLEU and lower
TER scores on both MT04 and MT05 test sets,
which gives the feature subspace method more
potential to achieve higher performance than the
baseline systems.
We then investigate the ratio of translation
candidates in the combined n-best lists of non-
baseline sub-systems that are not included in the
baseline’s n-best list. Table 5 shows the statistics.
</bodyText>
<table confidence="0.988493666666667">
MT04 MT05
SYS1 69.71% 69.69%
SYS2 59.07% 58.54%
</table>
<tableCaption confidence="0.972089">
Table 5: Ratio of unique translation candidates
from non-baseline sub-systems.
</tableCaption>
<bodyText confidence="0.999719571428571">
From Table 5 we can see that only less than
half of the translation candidates of sub-systems
overlap with those the of baseline systems. This
result, together with the oracle BLEU and TER
score estimation, helps eliminate the concern that
no diversities or better translation candidates can
be obtained by using sub-systems.
</bodyText>
<subsectionHeader confidence="0.973389">
4.4 Feature Subspace Method on Single
SMT System
</subsectionHeader>
<bodyText confidence="0.99781996">
Next we validate the effect of feature subspace
method on single SMT systems.
Figure 1 shows the evaluation results of dif-
ferent systems on the MT05 test set. From the
figure we can see that the overall accuracy of
baseline systems is better than any of their de-
rived sub-systems, and except the sub-system
derived by using FSLM_2, the performance of all
the systems are fairly similar.
We then evaluate the system combination me-
thod proposed in Section 3.2 with all the sub-
systems for each baseline system. Table 6 shows
the results on both MT04 and MT05 data sets, in
which FS-Comb denotes the system combination
using 11 sub-systems.
From Table 6 we can see that by using FS-
Comb we obtain about 1.1~1.3 points of BLEU
gains over baseline systems. We also include in
Table 6 the results for Baseline+mLM, which
stands for the augmented baseline system as de-
scribed in Section 3.1 using a bunch of LM fea-
tures from bigram to 5-gram. It can be seen that
both augmented baseline systems outperform
their corresponding baseline systems slightly but
consistently on both data sets.
</bodyText>
<table confidence="0.997698571428572">
MT04 MT05
SYS1 Baseline 39.07 38.72
Baseline+mLM 39.34+ 39.14+
FS-Comb 40.43++ 39.79++
SYS2 Baseline 38.84 38.30
Baseline+mLM 38.95* 38.63+
FS-Comb 39.92++ 39.49++
</table>
<tableCaption confidence="0.986676">
Table 6: Translation results of Baseline, Base-
</tableCaption>
<bodyText confidence="0.9677230625">
line+mLM and FS-Comb (+: significant better
than baseline system with p &lt; 0.05; ++: signifi-
cant better than baseline system with p &lt; 0.01; *:
no significant improvement).
We also investigate the results when we in-
crementally add the n-best list of each sub-
system into a candidate pool to see the effects
when different numbers of sub-systems are used
in combination. In order to decide the sequence
of sub-systems to add, we first evaluate the per-
formance of pair-wise combinations between
each sub-system and its baseline system on the
development set. That is, for each sub-system,
we combine its n-best list with the n-best list of
its baseline system and perform system combina-
tion for MT03 data set. Then we rank the sub-
systems by the pair-wise combination perfor-
mance from high to low, and use this ranking as
the sequence to add n-best lists of sub-systems.
Each time when a new n-best list is added, the
combination performance based on the enlarged
candidate pool is evaluated. Figure 2 shows the
results on both MT04 and MT05 test sets, in
which SYS1-fs and SYS2-fs denote the sub-
systems derived from SYS1 and SYS2 respec-
tively, and X-axis is the number of sub-systems
used for combination each time and Y-axis is the
BLEU score. From the figure we can see that
although in some cases the performance slightly
drops when a new sub-system is added, generally
using more sub-systems always leads to better
results.
</bodyText>
<figureCaption confidence="0.999517">
Figure 1: Performances of different systems.
</figureCaption>
<figure confidence="0.809251523809524">
Baseline
FS-PEF
FS-PFE
FS-PEFLEX
FS-PFELEX
FS-PP
FS-WP
FS-BLP
FS-LM-2
FS-LM-3
FS-LM-4
39
38
37
36
35
34
33
32
31
SYS1 SYS2
</figure>
<page confidence="0.987679">
1101
</page>
<bodyText confidence="0.9999505">
Next we examine the performance of baseline
systems when different beam sizes are used in
decoding. The results on MT05 test set are
shown in Figure3, where X-axis is the beam size.
In Figure 3, SYS1+mLM and SYS2+mLM de-
note augmented baseline systems of SYS1 and
SYS2 with multiple LM features.
From Figure 3 we can see that augmented
baseline systems (with multiple LM features)
outperform the baseline systems (with only one
LM feature) for all beam sizes ranging from 20
to 220. In this experiment we did not observe any
significant performance improvements when us-
ing larger beam sizes than the default setting, but
using more sub-systems in combination almost
always bring improvements.
indicate that almost half of the final translations
are contributed by the non-baseline sub-systems.
</bodyText>
<subsectionHeader confidence="0.988069">
4.5 The Impact of n-best List Size
</subsectionHeader>
<bodyText confidence="0.996351">
In order to find the optimal size of n-best list for
combination, we compare the combination re-
sults of using list sizes from 10-best up to 500-
best for each sub-system.
In this experiment, system combination was
performed on the combined n-best list from total
11 sub-systems with different list size each time.
Figure 4 shows the results on the MT03 dev set
and the MT04 and MT05 test sets for both SYS1
and SYS2. X-axis is the n-best list size of each
sub-system.
</bodyText>
<figure confidence="0.99683408">
42.0
41.5
41.0
40.5
40.0
39.5
39.0
SYS1-fs-05
SYS2-fs-05
SYS1-fs-04
SYS2-fs-04
SYS1-fs-03
SYS2-fs-03
SYS1-fs-05
SYS2-fs-05
SYS1-fs-04
SYS2-fs-04
40.5
40.0
39.5
39.0
38.5
38.0
10 20 50 100 200 500
1 2 3 4 5 6 7 8 9 10 11
</figure>
<figureCaption confidence="0.929004">
Figure 2: Performances on different numbers of
sub-systems.
</figureCaption>
<figure confidence="0.997875571428571">
39.5
SYS1
SYS2
SYS1-mLM
SYS2-mLM
38.5
38.0
</figure>
<figureCaption confidence="0.999726">
Figure 3: Performances on different beam sizes.
</figureCaption>
<table confidence="0.944645666666667">
MT04 MT05
SYS1-fs 44.63% 46.12%
SYS2-fs 47.54% 44.73%
</table>
<tableCaption confidence="0.8915595">
Table 7: Ratio of final translations coming from
non-baseline sub-systems.
</tableCaption>
<bodyText confidence="0.999758">
Finally, we investigate the ratio of final trans-
lations coming from the n-best lists of non-
baseline sub-systems only. Table 7 shows the
results on both MT04 and MT05 test sets, which
</bodyText>
<figureCaption confidence="0.990036">
Figure 4: Performances on different n-best sizes.
</figureCaption>
<bodyText confidence="0.999985444444444">
We can see from the figure that for all data
sets the optimal n-best list size is around 50, but
the improvements are not significant over the
results when 20-best translations are used. The
reason for the small optimal n-best list size could
be that the low-rank hypotheses might introduce
more noises into the combined translation candi-
date pool for sentence-level combination (Hasan
et al., 2007; Hildebrand and Vogel, 2008).
</bodyText>
<subsectionHeader confidence="0.8358695">
4.6 Feature Subspace Method on Multiple
SMT Systems
</subsectionHeader>
<bodyText confidence="0.999410692307692">
In the last experiment, we investigate the effect
of feature subspace method when multiple SMT
systems are used in system combination.
Evaluation results are reported in Table 8. The
system combination method described in Section
3.2 is used to combine outputs from two baseline
systems (with only one 5-gram LM feature) and
sub-systems generated from both baseline sys-
tems (22 in total), with their results denoted as
Baseline Comb (both) and FS Comb (both) re-
spectively. We also include the combination re-
sults of sub-systems based on one baseline sys-
tem for reference in the table.
</bodyText>
<figure confidence="0.940350416666667">
39.0
20
40
60
80
100
120
140
160
180
200
220
</figure>
<page confidence="0.997771">
1102
</page>
<bodyText confidence="0.9929513">
On both MT04 and MT05 test sets, the results
of system combination based on sub-systems are
significantly better than those of baseline sys-
tems, which show that our method can also help
with system combination when more than one
system are used. We can also see that using mul-
tiple systems based on different SMT models and
using our subspace based method can help each
other: the best performance can only be achieved
when both are employed.
</bodyText>
<table confidence="0.9982566">
MT04 MT05
Baseline Comb (both) 39.98 39.43
FS-Comb (SYS1) 40.43 39.79
FS-Comb (SYS2) 39.92 39.49
FS Comb (both) 40.96 40.38
</table>
<tableCaption confidence="0.970615">
Table 8: Performances of sentence-level combi-
nation on multiple SMT systems.
</tableCaption>
<sectionHeader confidence="0.998974" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986363636364">
In this paper, we have presented a novel and ef-
fective Feature Subspace method for the con-
struction of an ensemble of machine translation
systems based on a baseline SMT model which
can be formulated as a standard linear function.
Each system within the ensemble is based on a
subset of features derived from the baseline
model, and the resulting ensemble can be used in
system combination to improve translation quali-
ty. Experimental results on NIST Chinese-to-
English translation tasks show that our method
can bring significant improvements to two base-
line systems with state-of-the-art performance,
and it is expected that our method can be em-
ployed to improve any linear model -based SMT
systems. There is still much room for improve-
ments in the current work. For example, we still
use a simple one-feature difference principle for
feature subspace generation. In the future, we
will explore more possibilities for feature sub-
spaces selection and experiment with our method
in a word-level system combination model.
</bodyText>
<sectionHeader confidence="0.999614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999658163934427">
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion net-
works for combining machine translation systems.
In Proc. COLING, pages 33-40.
Srinivas Bangalore, German Bordel, and Giuseppe
Riccardi. 2001. Computing consensus translation
from multiple machine translation systems. In
Proc. ASRU, pages 351-354.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263-270.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages
98-107.
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via
hypothesis selection from combined n-best lists. In
8th AMTA conference, pages 254-261.
Tin Kam Ho. 1998. The random subspace method for
constructing decision forests. In IEEE Transactions
on Pattern Analysis and Machine Intelligence,
pages 832-844.
Sasa Hasan, Richard Zens, and Hermann Ney. 2007.
Are very large n-best lists useful for SMT? In
Proc. NAACL, Short paper, pages 57-60.
S. Jayaraman and A. Lavie. 2005. Multi-Engine Ma-
chine Translation Guided by Explicit Word Match-
ing. In 10th EAMT conference, pages 143-152.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388-395.
Philipp Koehn. 2004. Phrase-based Model for SMT.
In Computational Linguistics, 28(1): pages 114-
133.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative Decoding: Partial
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. ACL-IJCNLP.
Adam Lopez and Philip Resnik. 2006. Word-Based
Alignment, Phrase-Based Translation: What’s the
link? In 7th AMTA conference, pages 90-99.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. ACL, pages 609-616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phras-
es. In Proc. EMNLP, pages 44-52.
Wolfgang Macherey and Franz Och. 2007. An Empir-
ical Study on Computing Consensus Translations
from Multiple Machine Translation Systems. In
Proc. EMNLP, pages 986-995.
Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment. In Proc. EACL, pages 33-
40.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statis-
</reference>
<page confidence="0.681296">
1103
</page>
<reference confidence="0.999847675">
tical machine translation. In Proc. ACL, pages 295-
302.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages
160-167.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1): pages 19-51.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4): pages 417-
449.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proc. NAACL, pages
228-235.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proc.
ACL, pages 312-319.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. Of the Third ACL Workshop on Sta-
tistical Machine Translation, pages 183-186.
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination.
In ICASSP, pages 105-108.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proc. ACL, pages
521-528.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level Machine Translation Evaluation as a
Ranking Problem: One step aside from BLEU. In
Proc. Of the Second ACL Workshop on Statistical
Machine Translation, pages 240-247.
</reference>
<page confidence="0.997823">
1104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963285">
<title confidence="0.999903">The Feature Subspace Method for SMT System Combination</title>
<author confidence="0.998477">Mu Tong Ming</author>
<affiliation confidence="0.999997">University Research Asia University</affiliation>
<address confidence="0.980645">Tianjin, China Beijing, China Shenyang, China</address>
<email confidence="0.999875">v-naduan@microsoft.com</email>
<email confidence="0.999875">muli@microsoft.com</email>
<email confidence="0.999875">v-toxiao@microsoft.com</email>
<email confidence="0.999875">mingzhou@microsoft.com</email>
<abstract confidence="0.99916955">Recently system combination has been shown to be an effective way to improve translation quality over single machine translation systems. In this paper, we present a simple and effective method to systematically derive an ensemble of SMT systems from one baseline linear SMT model for use in system combination. Each system in the resulting ensemble is based on a feature set derived from the features of the baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
</authors>
<title>Improving alignments for better confusion networks for combining machine translation systems.</title>
<date>2008</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2723" citStr="Ayan et al., 2008" startWordPosition="410" endWordPosition="413">el or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diver</context>
</contexts>
<marker>Ayan, Zheng, Wang, 2008</marker>
<rawString>Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008. Improving alignments for better confusion networks for combining machine translation systems. In Proc. COLING, pages 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="2267" citStr="Bangalore et al., 2001" startWordPosition="338" endWordPosition="341">en many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems </context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proc. ASRU, pages 351-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1353" citStr="Chiang, 2005" startWordPosition="203" endWordPosition="204">e principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level co</context>
<context position="7742" citStr="Chiang, 2005" startWordPosition="1239" endWordPosition="1240">from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment errors in the parallel training data, the inaccurate translation phrase 海水 淡化 =* desalination is assigned with a high value of the phrase translation probability feature p (e |f). Although the correct translation can also be composed by two phrases 海水 =* sea water and 淡化 =* desalination, its overall translation score cannot beat the inco</context>
<context position="18212" citStr="Chiang, 2005" startWordPosition="3060" endWordPosition="3061">ent. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5- gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experiments, two in-house developed systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing c</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmmbased hypothesis for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="2741" citStr="He et al., 2008" startWordPosition="414" endWordPosition="417">mbination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translat</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmmbased hypothesis for combining outputs from machine translation systems. In Proc. EMNLP, pages 98-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In 8th AMTA conference,</booktitle>
<pages>254--261</pages>
<contexts>
<context position="2088" citStr="Hildebrand and Vogel, 2008" startWordPosition="313" endWordPosition="316">. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment betw</context>
<context position="15039" citStr="Hildebrand and Vogel (2008)" startWordPosition="2519" endWordPosition="2522">ures in the combination model: 𝑛−(𝑒,ℋ 𝒟 ) = ( 𝑒 − 𝑛+ 1 − 𝐺𝑛(𝑒,𝑒′)) 𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒 (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2𝑚 + 2 if 𝑚 orders of n-gram are to be matched in computing 𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 𝑒∗ = 𝑎𝑟𝑔𝑚𝑎𝑥 𝜆𝐿𝑀𝐿𝑀 𝑒 + 𝜆𝑙𝐿 + 𝜓(𝑒, ℋ(𝒟)) 4 Experiments 𝑒 ∈ℋ 𝒟 (3) where 𝐿𝑀 𝑒 is the language model score for 𝑒, 𝐿 is the length of 𝑒, and 𝜓(𝑒, ℋ(𝒟)) is a translation consensus –based scoring function. The computation of 𝜓 (𝑒, ℋ(𝒟)) is further decomposed into weighted linear combination of a set of n-gram consensus –based features, which</context>
<context position="27801" citStr="Hildebrand and Vogel, 2008" startWordPosition="4645" endWordPosition="4648"> of final translations coming from the n-best lists of nonbaseline sub-systems only. Table 7 shows the results on both MT04 and MT05 test sets, which Figure 4: Performances on different n-best sizes. We can see from the figure that for all data sets the optimal n-best list size is around 50, but the improvements are not significant over the results when 20-best translations are used. The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008). 4.6 Feature Subspace Method on Multiple SMT Systems In the last experiment, we investigate the effect of feature subspace method when multiple SMT systems are used in system combination. Evaluation results are reported in Table 8. The system combination method described in Section 3.2 is used to combine outputs from two baseline systems (with only one 5-gram LM feature) and sub-systems generated from both baseline systems (22 in total), with their results denoted as Baseline Comb (both) and FS Comb (both) respectively. We also include the combination results of sub-systems based on one basel</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In 8th AMTA conference, pages 254-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tin Kam Ho</author>
</authors>
<title>The random subspace method for constructing decision forests.</title>
<date>1998</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence,</booktitle>
<pages>832--844</pages>
<contexts>
<context position="7204" citStr="Ho, 1998" startWordPosition="1146" endWordPosition="1147"> largest sea water desalinitranslation zation project settles in Zhoushan Default China &apos;s largest desalination translation project in Zhoushan FS_PEF China &apos;s largest sea water translation desalination project in Zhoushan Table 1: An example of translations generated from the same decoder but with different feature settings. Chinese English p(elf) 1 海水 淡化 desalination 0.4000 2 海水 sea water 0.1748 3 淡化 desalination 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in</context>
</contexts>
<marker>Ho, 1998</marker>
<rawString>Tin Kam Ho. 1998. The random subspace method for constructing decision forests. In IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 832-844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Hasan</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Are very large n-best lists useful for SMT?</title>
<date>2007</date>
<booktitle>In Proc. NAACL, Short paper,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="27772" citStr="Hasan et al., 2007" startWordPosition="4641" endWordPosition="4644">nvestigate the ratio of final translations coming from the n-best lists of nonbaseline sub-systems only. Table 7 shows the results on both MT04 and MT05 test sets, which Figure 4: Performances on different n-best sizes. We can see from the figure that for all data sets the optimal n-best list size is around 50, but the improvements are not significant over the results when 20-best translations are used. The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008). 4.6 Feature Subspace Method on Multiple SMT Systems In the last experiment, we investigate the effect of feature subspace method when multiple SMT systems are used in system combination. Evaluation results are reported in Table 8. The system combination method described in Section 3.2 is used to combine outputs from two baseline systems (with only one 5-gram LM feature) and sub-systems generated from both baseline systems (22 in total), with their results denoted as Baseline Comb (both) and FS Comb (both) respectively. We also include the combination results of s</context>
</contexts>
<marker>Hasan, Zens, Ney, 2007</marker>
<rawString>Sasa Hasan, Richard Zens, and Hermann Ney. 2007. Are very large n-best lists useful for SMT? In Proc. NAACL, Short paper, pages 57-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-Engine Machine Translation Guided by Explicit Word Matching.</title>
<date>2005</date>
<booktitle>In 10th EAMT conference,</booktitle>
<pages>143--152</pages>
<contexts>
<context position="2294" citStr="Jayaraman and Lavie, 2005" startWordPosition="342" endWordPosition="346">pts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models a</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-Engine Machine Translation Guided by Explicit Word Matching. In 10th EAMT conference, pages 143-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="1307" citStr="Koehn, 2004" startWordPosition="196" endWordPosition="197">typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di</context>
<context position="16903" citStr="Koehn (2004)" startWordPosition="2842" endWordPosition="2843"> Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version of BLEU score which computes the brevity penalty using the shortest reference translation for each segment, and all the results will be reported in percentage numbers. Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004). Statistics of the data sets are summarized in Table 3. Data set #Sentences #Words MT03 (dev) 919 23,782 MT04 (test) 1,788 47,762 MT05 (test) 1,082 29,258 Table 3: Data set statistics. 𝑒 −𝑛+1 𝐺𝑛 𝑒, 𝑒′ = 𝛿(𝑒𝑖𝑖+𝑛−1, 𝑒′) 𝑖=1 (5) , otherwise it is 0. 1099 We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default s</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Phrase-based Model for SMT.</title>
<date>2004</date>
<journal>In Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>114--133</pages>
<contexts>
<context position="1307" citStr="Koehn, 2004" startWordPosition="196" endWordPosition="197">typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di</context>
<context position="16903" citStr="Koehn (2004)" startWordPosition="2842" endWordPosition="2843"> Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version of BLEU score which computes the brevity penalty using the shortest reference translation for each segment, and all the results will be reported in percentage numbers. Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004). Statistics of the data sets are summarized in Table 3. Data set #Sentences #Words MT03 (dev) 919 23,782 MT04 (test) 1,788 47,762 MT05 (test) 1,082 29,258 Table 3: Data set statistics. 𝑒 −𝑛+1 𝐺𝑛 𝑒, 𝑒′ = 𝛿(𝑒𝑖𝑖+𝑛−1, 𝑒′) 𝑖=1 (5) , otherwise it is 0. 1099 We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default s</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Phrase-based Model for SMT. In Computational Linguistics, 28(1): pages 114-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Nan Duan</author>
<author>Dongdong Zhang</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders. In</title>
<date>2009</date>
<booktitle>Proc. ACL-IJCNLP.</booktitle>
<contexts>
<context position="15267" citStr="Li et al. (2009)" startWordPosition="2554" endWordPosition="2557">re to be matched in computing 𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 𝑒∗ = 𝑎𝑟𝑔𝑚𝑎𝑥 𝜆𝐿𝑀𝐿𝑀 𝑒 + 𝜆𝑙𝐿 + 𝜓(𝑒, ℋ(𝒟)) 4 Experiments 𝑒 ∈ℋ 𝒟 (3) where 𝐿𝑀 𝑒 is the language model score for 𝑒, 𝐿 is the length of 𝑒, and 𝜓(𝑒, ℋ(𝒟)) is a translation consensus –based scoring function. The computation of 𝜓 (𝑒, ℋ(𝒟)) is further decomposed into weighted linear combination of a set of n-gram consensus –based features, which are defined in terms of the order of n-gram to be matched between current candidate and other translation in ℋ(𝒟). Given a translation candidate 𝑒, the n-gram agreement feature between 𝑒 and other translations in the candidate </context>
</contexts>
<marker>Li, Duan, Zhang, Li, Zhou, 2009</marker>
<rawString>Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and Ming Zhou. 2009. Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders. In Proc. ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Word-Based Alignment, Phrase-Based Translation: What’s the link?</title>
<date>2006</date>
<booktitle>In 7th AMTA conference,</booktitle>
<pages>90--99</pages>
<contexts>
<context position="7408" citStr="Lopez and Resnik (2006)" startWordPosition="1180" endWordPosition="1183">desalination project in Zhoushan Table 1: An example of translations generated from the same decoder but with different feature settings. Chinese English p(elf) 1 海水 淡化 desalination 0.4000 2 海水 sea water 0.1748 3 淡化 desalination 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment error</context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>Adam Lopez and Philip Resnik. 2006. Word-Based Alignment, Phrase-Based Translation: What’s the link? In 7th AMTA conference, pages 90-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1392" citStr="Liu et al., 2006" startWordPosition="209" endWordPosition="212">e sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypot</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. In Proc. ACL, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1413" citStr="Marcu et al., 2006" startWordPosition="213" endWordPosition="216"> systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original o</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. EMNLP, pages 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
</authors>
<title>An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>986--995</pages>
<contexts>
<context position="3075" citStr="Macherey and Och (2007)" startWordPosition="474" endWordPosition="477"> multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination. In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard l</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz Och. 2007. An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems. In Proc. EMNLP, pages 986-995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffi ng</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2316" citStr="Matusov et al., 2006" startWordPosition="347" endWordPosition="350"> multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand a</context>
</contexts>
<marker>Matusov, ng, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. EACL, pages 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="9757" citStr="Och and Ney (2002)" startWordPosition="1575" endWordPosition="1578">, for a specific sentence some individual systems could generate better translations. It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system. 3 Feature Subspace Method for SMT System Ensemble Construction In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination. 3.1 SMT System Ensemble Generation Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). Let hm (f, e) be a feature function, and Am be its weight, an SMT model D can be formally written as: e* = argmax e m Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in D, a new SMT system can be constructed based on it, which we call a sub-system. Next we will use SZ to denote the full feature space defined by the entire set of features used in D, and s S SZ is a feature subset that belongs to p (SZ), the power set of SZ. The derived subsystem based on subset s S SZ is denoted by ds. Although in theory we </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ACL, pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14964" citStr="Och 2003" startWordPosition="2508" endWordPosition="2509">ions, we also introduce a set of n-gram disagreement features in the combination model: 𝑛−(𝑒,ℋ 𝒟 ) = ( 𝑒 − 𝑛+ 1 − 𝐺𝑛(𝑒,𝑒′)) 𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒 (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2𝑚 + 2 if 𝑚 orders of n-gram are to be matched in computing 𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 𝑒∗ = 𝑎𝑟𝑔𝑚𝑎𝑥 𝜆𝐿𝑀𝐿𝑀 𝑒 + 𝜆𝑙𝐿 + 𝜓(𝑒, ℋ(𝒟)) 4 Experiments 𝑒 ∈ℋ 𝒟 (3) where 𝐿𝑀 𝑒 is the language model score for 𝑒, 𝐿 is the length of 𝑒, and 𝜓(𝑒, ℋ(𝒟)) is a translation consensus –based scoring function. The computation of 𝜓 (𝑒, ℋ(𝒟)) is further decomposed into weig</context>
<context position="16450" citStr="Och, 2003" startWordPosition="2773" endWordPosition="2774">nslations in the candidate pool is defined as: 𝑛+(𝑒,ℋ 𝒟 ) = 𝐺𝑛 𝑒,𝑒′ (4) 𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒 where the function 𝐺𝑛 𝑒, 𝑒′ counts the occurrences of n-grams of 𝑒 in 𝑒′: Here 𝛿(∙,∙) is the indicator function - 𝛿 𝑒𝑖 𝑖+𝑛−1,𝑒′ is 1 when the n-gram 𝑒𝑖 𝑖+𝑛−1 appears in 𝑒′ In order to give the combination model an opportunity to penalize long but inaccurate transla4.1 Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version of BLEU score which computes the brevity penalty using the shortest reference translation for each segment, and all the results will be reported in percentage numbers. Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004). Statistics of the data sets are summarized in Table 3. Data set #Sentences #Words MT03 (dev) 919 23,782 MT04 (test) 1,788 47,762 MT05 (test) 1,08</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="17435" citStr="Och and Ney, 2003" startWordPosition="2929" endWordPosition="2932">gnificance is computed using the bootstrap re-sampling method proposed by Koehn (2004). Statistics of the data sets are summarized in Table 3. Data set #Sentences #Words MT03 (dev) 919 23,782 MT04 (test) 1,788 47,762 MT05 (test) 1,082 29,258 Table 3: Data set statistics. 𝑒 −𝑛+1 𝐺𝑛 𝑒, 𝑒′ = 𝛿(𝑒𝑖𝑖+𝑛−1, 𝑒′) 𝑖=1 (5) , otherwise it is 0. 1099 We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5- gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experim</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<contexts>
<context position="1293" citStr="Och and Ney, 2004" startWordPosition="192" endWordPosition="195">he baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combinatio</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4): pages 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="2660" citStr="Rosti et al., 2007" startWordPosition="401" endWordPosition="404"> (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t</context>
<context position="14282" citStr="Rosti et al., 2007" startWordPosition="2380" endWordPosition="2383">tiple subsystems. However, since lower-order models have already been contained in higher-order model for the purpose of smoothing in almost all statistical language model implementations, there is also no extra training cost. 3.2 System Combination Scheme In our work, we use a sentence-level system combination model to select best translation hypothesis from the candidate pool ℋ(𝒟) . This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007). The score function in our combination model is formulated as follows: tions, we also introduce a set of n-gram disagreement features in the combination model: 𝑛−(𝑒,ℋ 𝒟 ) = ( 𝑒 − 𝑛+ 1 − 𝐺𝑛(𝑒,𝑒′)) 𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒 (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2𝑚 + 2 if 𝑚 orders of n-gram are to be matched in computing 𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation </context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007. Combining outputs from multiple machine translation systems. In Proc. NAACL, pages 228-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. ACL,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="2660" citStr="Rosti et al., 2007" startWordPosition="401" endWordPosition="404"> (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t</context>
<context position="14282" citStr="Rosti et al., 2007" startWordPosition="2380" endWordPosition="2383">tiple subsystems. However, since lower-order models have already been contained in higher-order model for the purpose of smoothing in almost all statistical language model implementations, there is also no extra training cost. 3.2 System Combination Scheme In our work, we use a sentence-level system combination model to select best translation hypothesis from the candidate pool ℋ(𝒟) . This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007). The score function in our combination model is formulated as follows: tions, we also introduce a set of n-gram disagreement features in the combination model: 𝑛−(𝑒,ℋ 𝒟 ) = ( 𝑒 − 𝑛+ 1 − 𝐺𝑛(𝑒,𝑒′)) 𝑒′ ∈ℋ 𝒟 ,𝑒′ ≠𝑒 (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2𝑚 + 2 if 𝑚 orders of n-gram are to be matched in computing 𝜓(𝑒, ℋ(𝒟)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation </context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved Word-Level System Combination for Machine Translation. In Proc. ACL, pages 312-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proc. Of the Third ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<contexts>
<context position="2806" citStr="Rosti et al., 2008" startWordPosition="423" endWordPosition="426">ranslations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proc. Of the Third ACL Workshop on Statistical Machine Translation, pages 183-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sim</author>
<author>W Byrne</author>
<author>M Gales</author>
<author>H Sahbi</author>
<author>P Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In ICASSP,</booktitle>
<pages>105--108</pages>
<contexts>
<context position="2059" citStr="Sim et al., 2007" startWordPosition="309" endWordPosition="312">he-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In ICASSP, pages 105-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="18675" citStr="Xiong et al. (2006)" startWordPosition="3124" endWordPosition="3127">veloped systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation. Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used. We list the features in consideration as follows: • PEF and PFE: phrase translation probabilities p(e If) and p (f Ie) • PEFLEX and PFELEX: lexical weights plex (e If) and plex (f I e) • PP: phrase penalty • WP: word penalty • BLP: bi-lexicon pair counting how many entries</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proc. ACL, pages 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Ming Zhou</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Sentence level Machine Translation Evaluation as a Ranking Problem: One step aside from BLEU.</title>
<date>2007</date>
<booktitle>In Proc. Of the Second ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="21083" citStr="Ye et al., 2007" startWordPosition="3537" endWordPosition="3540">st list is set to 20 for each sub-system, and for baseline systems, this size is set to 220, which equals to the size of the combined n-best list generated by total 11 sub-systems. The order of n-gram agreement and disagreement features used in sentence-level combination model ranges from unigram to 4-gram. 4.3 Evaluation of Oracle Translations We first evaluate the oracle performance on the n-best lists of baseline systems and on the combined n-best lists of sub-systems generated from each baseline system. The oracle translations are obtained by using the metric of sentence-level BLEU score (Ye et al., 2007). Table 4 shows the evaluation results, in which Baseline stands for baseline system with a 5-gram LM feature, and FS stands for 11 sub-systems derived from the baseline system. SYS1 SYS2 BLEU/TER BLEU/TER MT04 Baseline 49.68/0.6411 49.50/0.6349 FS 51.05/0.6089 50.53/0.6056 MT05 Baseline 48.89/0.5946 48.37/0.5944 FS 50.69/0.5695 49.81/0.5684 Table 4: Oracle BLEU and TER scores on baseline systems and their generated sub-systems. 1100 For both SYS1 and SYS2, feature subspace method achieves higher oracle BLEU and lower TER scores on both MT04 and MT05 test sets, which gives the feature subspace</context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sentence level Machine Translation Evaluation as a Ranking Problem: One step aside from BLEU. In Proc. Of the Second ACL Workshop on Statistical Machine Translation, pages 240-247.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>