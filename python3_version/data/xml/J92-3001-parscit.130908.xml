<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9985875">
Making DATR Work for Speech: Lexicon
Compilation in SUNDIAL
</title>
<author confidence="0.955478">
Francois Andry.
</author>
<affiliation confidence="0.520593">
Cap Gemini Innovation
</affiliation>
<author confidence="0.977584">
Scott McGlashant
</author>
<affiliation confidence="0.990432">
University of Surrey
</affiliation>
<author confidence="0.986977">
Nick J. You&amp;
</author>
<affiliation confidence="0.939847">
Logica Cambridge Ltd.
</affiliation>
<author confidence="0.992623">
Norman M. Fraserl
</author>
<affiliation confidence="0.992674">
University of Surrey
</affiliation>
<author confidence="0.937742">
Simon Thorntonl
</author>
<affiliation confidence="0.76163">
Logica Cambridge Ltd.
</affiliation>
<bodyText confidence="0.998140125">
We present DIALEX, an inheritance-based tool that facilitates the rapid construction of linguis-
tic knowledge bases. Simple lexical entries are added to an application-specific DATR lexicon
that inherits morphosyntactic, syntactic, and lexico-semantic constraints from an application-
independent set of structured base definitions. A lexicon generator expands the DATR lexi-
con out into a disjunctive normal form lexicon. This is then encoded either as an accep-
tance lexicon (in which the constraining features are bit-encoded for use in pruning word
lattices), or as a full lexicon (which is used for assigning interpretations or for generating
messages).
</bodyText>
<sectionHeader confidence="0.995316" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999258111111111">
In this paper we describe DIALEX, a modular inheritance-based tool for the construc-
tion of lexicalized grammar knowledge bases. DIALEX has been developed as part
of the SUNDIAL (Speech UNderstanding and DIALogue) project—currently one of
Europe&apos;s largest collaborative research projects in speech and language technology.&apos;
SUNDIAL&apos;s main project goal is to produce four prototype systems that support rela-
tively unconstrained telephone dialogs for limited domains in each of English, French,
German, and Italian (Peckham 1991). This paper reports work carried out in the devel-
opment of the English and French systems. These share a common application domain,
namely flight enquiries and reservations.
</bodyText>
<note confidence="0.50842">
* Cap Gemini Innovation, 118 Rue de Tocqueville, 75017 Paris, France (andry@capsogeti.fr).
</note>
<footnote confidence="0.357142">
t Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, U.K.
(norman@soc.surrey.ac.uk; scott@soc.surrey.ac.uk).
Logica Cambridge Ltd, Betjeman House, 104 Hills Road, Cambridge, CB2 1LQ, U.K.
(simont@logcam.co.uk; nick@logcam.co.uk).
1 The work reported here was supported in part by the Commission of the European Communities as
</footnote>
<affiliation confidence="0.871523">
part of ESPRIT project P2218, SUNDIAL. Partners in the project are Cap Gemini Innovation, CNET,
IRISA (France); Daimler-Benz, Siemens, University of Erlangen (Germany); CSELT, Saritel (Italy);
Logica Cambridge Ltd, University of Surrey (U.K). We acknowledge with gratitude the helpful
comments and suggestions of the editors and reviewers of this special issue of Computational Linguistics
and of Lionel Moser.
</affiliation>
<note confidence="0.8445995">
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.994274">
The process of writing linguistic knowledge bases has been guided by a number
of design requirements on the SUNDIAL project as a whole.
</bodyText>
<listItem confidence="0.994007333333333">
1. First of all, prototype systems must be capable of understanding speech.
Therefore grammars must be appropriate for the purposes of speech
processing. For example, they must reflect the fact that input to the
parser is a word lattice or graph from which some of the spoken words
(typically short words such as function words) may be missing.
2. Each prototype system must be capable of producing speech. Speech
</listItem>
<bodyText confidence="0.8292465">
generation takes place in two stages. In the first stage, text is generated.
In the second stage, a text-to-speech system outputs the message.
Therefore the linguistic knowledge must also be structured appropriately
for the purposes of text generation.
</bodyText>
<listItem confidence="0.996660666666667">
3. Each system must run in real time or near real time. Therefore the
linguistic knowledge must be structured so as to allow rapid access and
manipulation.
4. Portability to new applications should be simple; work required to write
new linguistic knowledge bases should therefore be kept to a minimum.
5. Duplication of effort must be avoided. This must be true in respect of the
</listItem>
<bodyText confidence="0.970963266666667">
components of each separate prototype system. For example, the same
dialog manager software module has been used in each prototype with
minor customizations for each language (Bilange 1991; McGlashan et al.
1992). The same principle should apply to the design of tools for the
construction of knowledge bases, including lexical knowledge bases.
Thus, the task of adding a new lexical item should only require the
addition of knowledge that is idiosyncratic to that lexical item and not
predictable from what is already present in the knowledge base.
Section 2 of this paper presents an overview of the SUNDIAL DIALEX tool. Section
3 describes the way in which linguistic knowledge is initially expressed in terms of
declarative DAT R theories. Section 4 explains how a compact DATR knowledge base
is expanded out into a fully specified lexicon. Section 5 relates how the lexicon can
be customized for the purposes of real-time speech parsing. Practical experiences of
constructing and using DIALEX are recounted in Section 6. Concluding observations
are drawn in Section 7.
</bodyText>
<sectionHeader confidence="0.626251" genericHeader="method">
2. Overview of the System
</sectionHeader>
<bodyText confidence="0.984994">
In common with contemporary generative theories that are unification based and for
which information is concentrated in the lexicon (Pollard and Sag 1987; Calder et al.
1988), we adopt the sign as our basic unit of linguistic representation. For a given lexi-
cal entry, a sign describes the constraints—morphological, syntactic, and semantic—it
</bodyText>
<page confidence="0.995916">
246
</page>
<note confidence="0.998352">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.786432">
introduces. The sign for intransitive arrives, for example, is:
</bodyText>
<figure confidence="0.992208222222222">
mor:
root : arrive 1
form : arrives .1
major: v
vform : fin
head : tense : pres
person : third
number : sg
syn:
sem :
[type : arrive
thetime : [ type : time I RestT 1
theagent : [ type : object I RestA ]
args: (
syn: head:
sem : type:
[opt
order: dir
adj
_
-
syn:
sem : type :
opt
order: [dir
adj
head
r major: prep I
I. case: at
fme j RestT ]
:
opt
post
: any
:
major: n
case: nom
person : third
_ number: sg
object I RestA ]
:
oblig
pre
: any
:
</figure>
<bodyText confidence="0.999934166666667">
The lexical sign for arrives combines syntactic head features that help to determine
the inflected form, with an args list that constrains its environment within the phrase
of which it is the head; the sem feature represents the semantic structure that will
be assigned to that phrase. The sign shows that the verb may optionally be followed
by a prepositional phrase whose semantics will fill the semantic role thetime.2 The
argument preceding the verb is constrained to be third person singular nominative
(i.e. not object-marked), and supplies the filler for the semantic role theagent.
In the interests of linguistic parsimony and sensible knowledge engineering, it is
necessary for lexicalist approaches to factor away at the lexicon-encoding interface as
many as possible of the commonalities between lexical items. To this end, we adopt
the principles of default inheritance (Gazdar 1987), as embodied in the DATR language
(Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are
morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger
et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such
as passive. To this we have added the area of lexico-semantic relations. In order to
generalize over semantic roles, it is necessary to tie these to functional-syntactic roles,
such as subject, direct object, etc. These in turn are related to order marked arguments
in the args frame. Only the latter appear in the final version of the lexicon.
</bodyText>
<footnote confidence="0.8991265">
2 In our representation of feature structures we follow Prolog conventions, whereby variables are
identified by initial capitals, and a vertical bar introduces the tail of a list.
</footnote>
<page confidence="0.986713">
247
</page>
<note confidence="0.335972">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.998407153846154">
A major issue for approaches such as ours is whether or not regularities in the
lexicon should be expanded out off-line, or remain for lazy evaluation during pars-
ing. We are sympathetic with the latter approach, for reasons of the economies that
can be achieved in lexicon size. However, we believe that a precompiled lexicon is
more appropriate to current speech recognition technology. Parsing typically involves
extremely large lattices of lexical hypotheses with imprecise boundaries, and is thus
computationally expensive. Our experience suggests that the trade-off between lexi-
con size and the cost of online inference is such as to favor lexicon size, in the case
of application-specific lexicons of the size required in the SUNDIAL systems (around
2000 words). For inflection-impoverished English and (somewhat richer) French, which
form the basis of our work, limited morphological decomposition during parsing is
avoided; instead the parser lexicon consists of fully inflected forms.
The parser lexicon we have developed has the following two properties.
</bodyText>
<listItem confidence="0.801650916666667">
1. It is indexed by surface forms, i.e. fully inflected words that are unique
at the phonological level. Efficiency of access is achieved by allowing
some internal disjunctions within entries in cases where the surface form
can be derived from a number of morphosyntactic feature combinations.
2. It consists of two separate knowledge bases: an acceptance lexicon and a
full lexicon. The former is designed for efficient parsing. Only those
features that constrain the ability of a sign to combine are represented.
These include syntactic head features and semantic types. The encoding
technique uses bit-vectors to achieve economy of representation and fast
unification. The full lexicon contains signs with no information missing;
the information in a full lexical entry is therefore a superset of the
corresponding acceptance lexicon entry.
</listItem>
<bodyText confidence="0.986818705882353">
Parsing takes place in two phases: lattice parsing using the acceptance lexicon, involving
heuristic search with intensive computation; and structure building, which operates on
the analysis tree produced by the first phase, using term unification to combine the
entries from the full lexicon corresponding to the lexical entries found by the first
phase.
The lexicon compilation architecture that we present in this paper is outlined in
Figure 1.
At the lexical encoding interface, a human lexicon builder builds an application-
and sublanguage-specific lexicon, using a set of structured base definitions, which gen-
eralize over commonalities and provide macros with which to structure entries (Sec-
tion 3). Both of these are written in DATR; we refer to the output of this as the DATR
lexicon. The lexicon generator then compiles this into a lexicon for which the entries
are directed acyclic graphs (DAGs) indexed by surface forms. For this a set of closure
definitions is used. These constitute a knowledge base forming a set of meta-definitions
to complement the DATR lexicon, as well as rendering explicit what may be implicit in
the latter (Section 4). The resulting entries are encoded in two ways: for the full lexicon
via Prolog term encoding and for the acceptance lexicon via bit coding (Section 5).
</bodyText>
<sectionHeader confidence="0.799976" genericHeader="method">
3. Encoding Linguistic Knowledge
</sectionHeader>
<subsectionHeader confidence="0.988102">
3.1 DATR
</subsectionHeader>
<bodyText confidence="0.9149255">
DATR is a declarative language for representing inheritance networks that support
multiple default inheritance. Knowledge is expressed in DATR in terms of path equa-
</bodyText>
<page confidence="0.997674">
248
</page>
<note confidence="0.939284">
Francois Andry et al. Making DATR Work for Speech
</note>
<figure confidence="0.9806262">
F
LEXICON application structured
BUILDER specific base
lexicon definitions
DATR lexicon
lexicon DATR
generation
closure
definitions
bit term
coding coding
acceptance
lexicon
full
lexicon
</figure>
<figureCaption confidence="0.921544">
Figure 1
DIALEX lexicon compilation architecture.
</figureCaption>
<bodyText confidence="0.999269333333333">
tions. The syntax of paths is a superset of that found in the PATR-II language (Shieber
1986). For example, (1) identifies two different paths in the DAG rooted at Nodel in
an inheritance network.
</bodyText>
<listItem confidence="0.9810515">
(1) Nodel: &lt;syn head case&gt;
Nodel: &lt;syn head number&gt;
</listItem>
<bodyText confidence="0.518514">
The path equations we present in this paper take the following forms:
</bodyText>
<listItem confidence="0.9985348">
(2) a. Nodel: &lt;&gt; == Node2
b. Nodel: Path1 == Valuel
c. Nodel: Pathl == &amp;quot;Path2&amp;quot;
d. Nodel : Path1 == Node2:Path2
e. Nodel: Path1 == Node2:&lt;&gt;
</listItem>
<bodyText confidence="0.99336">
The form shown in (2a) is the special case in which the path at Nodel is empty. This
allows Node1 to inherit all equations available at Node2, except those incompatible with
equations at Node1. Two equations are incompatible if they both make assignments to
the same path. The form shown in (2b) is used to assign values to paths, e.g. &lt;syn head
number&gt; == sg. Alternatively, a value may be copied from elsewhere in the DAG. (2c)
is used to assign to Path1 whatever value is found for Path2 at the original query
node. The double quotes are significant here because they indicate that Path2 must be
evaluated globally. If the quotes were not present, Path1 would be evaluated locally
</bodyText>
<page confidence="0.994266">
249
</page>
<note confidence="0.338679">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.99887825">
and assigned the value of Path2 at Node1 if such a value existed. The form shown in
(2d) assigns to Nodel:Path1 whatever value is found at Node2:Path2. A special case
of this is (2e), which allows extensions of Path1 to be specified at Node2. For example,
evaluating the DAT R theory in (3) yields the theorems for node Ex2 shown in (4).
</bodyText>
<listItem confidence="0.9910538">
(3) Ex1: &lt;head major&gt; == n
&lt;head case&gt; == nom.
Ex2: &lt;syn&gt; == Exl: &lt;&gt; .
(4) Ex2: &lt;syn head major&gt; = n.
Ex2: &lt;syn head case&gt; = nom.
</listItem>
<bodyText confidence="0.855474">
For a more detailed description of DAT R see Evans and Gazdar (1990).
</bodyText>
<subsectionHeader confidence="0.999953">
3.2 The Linguistic Framework
</subsectionHeader>
<bodyText confidence="0.9939355">
Linguistic knowledge is structured in terms of a simple unification categorial grammar
(Calder et al. 1988) in which featural constraints at the levels of morphology, syntax,
and semantics may all occur in a lexical sign. The basic sign structure of lexical entries
is shown in (5).
</bodyText>
<listItem confidence="0.849646333333333">
morphology: [. 1
(5) syntax : [. .
semantics : [. .1
The basic sign structure of the syntax feature value is shown in (6).
(6) syntax: head : [. . 1
args : [.. .] j
</listItem>
<bodyText confidence="0.9999305">
The head feature includes attribute-value structures for such things as tense, person,
number, and definiteness. The args feature is stack-valued, with stack position deter-
mining the order in which arguments may be combined by functional application.
The basic sign structure of the semantics feature value is shown in (7).
</bodyText>
<listItem confidence="0.81170875">
(7) semantics : id :&lt; value &gt; -
type :&lt; value &gt;
modus: [..
role* : [. .1
</listItem>
<bodyText confidence="0.9998874">
Each semantic object has a unique index (id). The type feature locates the object in a
sortal hierarchy. The modus feature specifies a number of constraints imposed on the
interpretation of semantic objects, such as polarity, aspect, and tense. Semantic roles
(such as theagent, thetime, theinstrument) are specified within the inheritance-based
definitions for semantic types.
The signs are defined in terms of a dual-component DAT R lexicon. The base defini-
tions represent an application-independent account of morphosyntax, transitivity, and
lexico-semantic constraints. They define what can be thought of as most of the higher
nodes of an inheritance hierarchy. The base definitions as a whole are, of course, lan-
guage specific, although significant exchange of definitions has been possible during
</bodyText>
<page confidence="0.99502">
250
</page>
<note confidence="0.987182">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.999978555555555">
the parallel development of our English and French DATR theories. The application-
specific lexicon can be thought of as a collection of lower nodes that hook onto the
bottom of the hierarchy defined by the structured base definitions. Whereas the struc-
tured base definitions provide a general morphological, syntactic, and lexico-semantic
account of a language fragment, the application-specific lexicon provides a vocabu-
lary and a task-related lexical semantics. Ideally, a change of application should only
necessitate a change of an application-specific lexicon. Naturally, application-specific
lexicons take much less time to construct than the base lexicon. Much of our discussion
in the rest of this section will focus on the structured base definitions.
</bodyText>
<subsectionHeader confidence="0.750881">
3.3 Morphosyntax
</subsectionHeader>
<bodyText confidence="0.9348524375">
Since the requirements of speech processing in real time rule out online morpholog-
ical parsing, a full-form lexicon must be produced. However, the task of entering all
possible forms of a word into the lexicon by hand would be both time consuming
and repetitive. We therefore provide a subtheory of morphology in the DATR base
definitions so that the grammar writer need only specify exceptional morphology for
each lexeme, leaving the lexicon generator to expand out all of the regular forms.
The surface form of an English verb encodes information relating to finiteness,
tense, number, and person. What is required in the DATR theory is a number of
condition—action statements that say things like:
IF a verb is finite
THEN IF it is present tense AND singular AND third per-
son
THEN its form is &lt;root&gt;+s
ELSE its form is &lt;root&gt;.
The desired effect is achieved by means of DATR&apos;s evaluable paths. The following path
equation is included at the VERB node.
</bodyText>
<listItem confidence="0.967125">
(8) VERB: &lt;mor form&gt; == VERB_MOR:&lt;&gt;
The VERB_MOR node looks like this:
(9) VERB_MOR: &lt;bse&gt; == &amp;quot;&lt;mor root&gt;&amp;quot;
&lt;prp&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; ing)
&lt;psp&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; ed)
</listItem>
<equation confidence="0.814521">
&lt;fin&gt; == &amp;quot;&lt; &amp;quot;&lt;syn head tense&gt;&amp;quot; &amp;quot;&lt;syn head number&gt;&amp;quot;
&amp;quot;&lt;syn head person&gt;&amp;quot; &gt;&amp;quot;
&lt;pres&gt; == &amp;quot;&lt;mor root&gt;&amp;quot;
&lt;pres sg third&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; s)
&lt;past&gt; == &amp;quot;&lt;mor form psp&gt;&amp;quot; .
</equation>
<page confidence="0.957309">
251
</page>
<note confidence="0.30947">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.999414142857143">
The base, present participle, and past participle forms are immediately available. If the
verb is finite it is necessary to construct an evaluable path consisting of tense, number,
and person values. If the tense is past (last line), the form is copied from the form of
the past participle. If the form is present singular third person (second last line), the
form is &lt;root&gt; +s. Otherwise, the present tense form is copied from the root form.
Exceptional forms are stated explicitly, thus overriding default forms. For example,
the following entry for hear specifies that it has exceptional past forms.
</bodyText>
<listItem confidence="0.813751666666667">
(10) HEAR: &lt;&gt; == VERB
&lt;mor root &gt; == hear
&lt;mor form psp&gt; == heard.
</listItem>
<bodyText confidence="0.99987025">
The evaluable path mechanism is also used to set the value of an agreement feature agr
to fps (third person singular) or not _tps. The path equation shown in (11), augmented
by the information at the V_AGREE node (12) then requires subject and verb to share
the same agr feature value. The subject&apos;s agr feature is set by the definitions in (13).3
</bodyText>
<listItem confidence="0.847339222222222">
(11) VERB: &lt;syn args gr_subject syn head agr&gt; ==
V_AGREE:&lt; &amp;quot; &lt;syn head tense&gt;&amp;quot; &amp;quot;(syn head number&gt;&amp;quot;
&amp;quot; &lt;syn head person&gt;&amp;quot; &gt;.
(12) V_AGREE: &lt;pres&gt; == not_tps
&lt;pres sg third&gt; == tps.
(13) NOUN: &lt;syn head agr&gt; ==
N_AGREE: &lt;agr &amp;quot; &lt;syn head number&gt; &amp;quot; &amp;quot; &lt;syn head person&gt; &amp;quot; &gt; .
N_AGREE: &lt;agr&gt; == not_tps
&lt;agr sg third&gt; == tps .
</listItem>
<bodyText confidence="0.999459857142857">
English verb morphology presents no real problems; noun morphology is even simpler.
French morphology is rather more complicated. However, it can be handled by means
of the same general technique of allowing evaluable paths to act as case statements
that select the appropriate morphological form. Instead of a unified account of French
verb morphology there are a number of distinct inflectional paradigms from which
different verbs inherit. A more sophisticated account of subject—verb agreement is also
required.
</bodyText>
<subsectionHeader confidence="0.99288">
3.4 Transitivity
</subsectionHeader>
<bodyText confidence="0.998905333333333">
Consider the relationship between verbs of different transitivity. An intransitive verb
takes a subject only. A transitive verb takes a subject and an object. A ditransitive verb
takes a subject and two objects, one direct and the other indirect. This information
</bodyText>
<footnote confidence="0.8176395">
3 In a few exceptional cases (e.g. am/are/is in the singular of BE) more complex constraints on agreement
are stated in the relevant lexical entries.
</footnote>
<page confidence="0.990014">
252
</page>
<note confidence="0.989209">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.999724666666667">
is easily expressible in terms of an inheritance hierarchy. Facts about subjects are
associated with a top node, for example a node called VERB. Facts about direct objects
are associated with another node, for example, a node called TRANS_V. By saying that
TRANS_V is a VERB, the general information about subjects is inherited at the TRANS_V
node. This relationship can be expressed simply in DATR. A similar treatment can be
adopted for ditransitive verbs (DTRANS_V):
</bodyText>
<listItem confidence="0.6961">
(14) VERB: &lt;syn head major&gt; == v
</listItem>
<equation confidence="0.9703912">
&lt;syn args gr_subject&gt; == GR_SUBJECT:&lt;&gt;.
TRANS_V: &lt;&gt; == VERB
&lt;syn args gr_direct&gt; == GR_DIRECT:&lt;&gt;
DTRANS_V: &lt;&gt; == TRANS_V
&lt;syn args gr_indirect&gt; == GR_INDIRECT:&lt;&gt; .
</equation>
<bodyText confidence="0.99918825">
Entries of the form &lt;syn args gr_subject&gt; == GR_SUBJECT:&lt;&gt; represent a convenient
way of packaging up all information relating to an argument type at a single node
(part of the information stored at this node can be found in (18) below; notice that
different arguments are identified by unique labels such as gr_subj ect and gr_direct).
We have already noted that in our sign representation, arguments are distinguished
by their position in a stack. This ought to render unique argument labels superfluous.
In fact, there are a number of reasons why it is desirable to use unique labels in the
DATR theory. Firstly, they allow individual arguments of a word to be picked out (see
Section 3.4.1 below). Secondly, they allow classes of argument to be identified and
generalizations to be made where appropriate. For example, we show in Section 3.4.2
how order and optionality generalizations can be made over argument types, and how
a system organized around named arguments can be mapped within DATR into an
order-marked system. Finally, grammatical relation labels are much easier for grammar
writers to remember and manipulate than positionally encoded argument structures.
Consider the following partial DATR entry for English infinitival complement
verbs.
</bodyText>
<listItem confidence="0.938543">
(15) INF_COMP_V: &lt;&gt; == VERB
</listItem>
<bodyText confidence="0.850191777777778">
&lt;syn args gr_comp&gt; == GR_COMP:&lt;&gt;
&lt;syn args gr_comp syn args gr_subject&gt;
&amp;quot;&lt;syn args gr_subject&gt;&amp;quot;.
The first line states that an infinitival complement verb inherits from the VERB node,
i.e., it is a verb that must have a subject. The second line introduces a number of
constraints on the complement. These constraints—collected at the GR_COMP node—
include the fact that the complement must be the infinitive form of a verb. The next
line enables the complement to share the subject of the matrix verb, i.e., in a sentence
like Amy wants to fly, Amy is the subject of both want and fly.
</bodyText>
<page confidence="0.990101">
253
</page>
<figure confidence="0.59739775">
Computational Linguistics Volume 18, Number 3
3.4.1 Unevaluated Path Equations. Consider the relationship between the semantics of
a verb and the semantics of its subject. The semantics of the subject must be coindexed
with a semantic role of the verb such as theagent, as shown in (16).
syn : [ args : [ gr_subject : [ sem : A ] ] ]
sem : [ theagent : A ]
This reentrancy can be expressed in DATR as follows:
(17) &lt;sem theagent&gt; == &amp;quot;&lt;syn args gr_subject sem&gt;&amp;quot;
</figure>
<bodyText confidence="0.957655">
The argument labeled gr_subject is typically underspecified in the lexicon and awaits
full specification at parse time. Because of this, the constraint is carried over to the
DAG-encoding phase of lexicon compilation, where it becomes a reentrancy, as de-
scribed in Section 4.
3.4.2 Argument Order and Optionality. While arguments in the structured base def-
initions are identified by grammatical relation labels, such as gr_subj ect, the lexicon
generation process requires arguments encoding order and optionality constraints that
are identified by relative position in an args list. Two techniques are used to produce
DATR theories with arguments structured in this way.
The first technique is to define featural constraints of order and optionality for
each grammatical relation label. Three types of constraint are defined:
dir: indicating whether the argument precedes or follows the functor (pre or
post);
adj: indicating whether the argument is adjacent to the functor or not (next or
any); and
opt: indicating whether the argument is optional or obligatory (opt or oblig).
Arguments identified as gr_subject and gr_oblique, for example, inherit the follow-
ing ordering constraints:
</bodyText>
<equation confidence="0.691056833333333">
(18) GR_SUBJECT: &lt;order dir&gt; == pre
&lt;order adj&gt; == next
&lt;order opt&gt; == oblig.
GR_OBLIQUE: &lt;order dir&gt; == post
&lt;order adj&gt; == any
&lt;order opt&gt; == opt
</equation>
<bodyText confidence="0.999426">
Whereas the subject is obligatory, and precedes the functor and allows for interven-
ing constituents, the oblique argument is optional and may appear in any position
following the functor.
The second technique maps arguments identified by relation labels onto arguments
identified by position in a linked list. Relative position is encoded in terms of the
</bodyText>
<page confidence="0.996631">
254
</page>
<note confidence="0.996232">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.898102666666667">
features first and rest: first identifies the first argument in a list, and rest identifies the
linked list of remaining arguments.
Consider part of the base definition for transitive verbs, as shown in (19).
</bodyText>
<equation confidence="0.390035666666667">
(19) TRANS_V: &lt;&gt; == VERB
&lt;syn args gr_direct&gt; == GR_DIRECT:&lt;&gt;
&lt;syn args&gt; == TVARGS:&lt;&gt; .
</equation>
<bodyText confidence="0.9237105">
Part of the collection of nodes devoted to mapping named arguments onto order-
marked arguments is shown in (20).
</bodyText>
<listItem confidence="0.8398865">
(20) TVARGS: &lt;&gt; == DTVARGS
&lt;rest&gt; == DARGS : &lt;&gt; .
</listItem>
<table confidence="0.615828166666667">
DTVARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_subject&gt;&amp;quot;
&lt;rest&gt; == ARGS1:&lt;&gt; .
DARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_direct&gt;&amp;quot;
&lt;rest&gt; == ARGS3:&lt;&gt; .
ARGS3: &lt;first&gt; == &amp;quot;&lt;syn args oblique1&gt;&amp;quot;
&lt;rest&gt; == ARGS4:&lt;&gt;.
</table>
<bodyText confidence="0.9994086">
TVARGS inherits the value of &lt;first&gt; from DTVARGS, which finds it by evaluating
the path &amp;quot;&lt;syn args gr_subject&gt;.&amp;quot; The &lt;rest&gt; is then inherited from DTVARGS
where the &lt;first&gt; argument of &lt;rest &gt; inherits from &amp;quot;&lt;syn args gr_direct&gt;.&amp;quot;
The &lt;rest&gt; of &lt;rest&gt; then inherits from ARGS3, which specifies the position of
oblique arguments within the args list of transitive verbs.
</bodyText>
<subsectionHeader confidence="0.960592">
3.5 Lexico-Semantic Constraints
</subsectionHeader>
<bodyText confidence="0.999908666666667">
A word lattice is likely to include numerous semantically anomalous but syntactically
well-formed constructions. In a system that aims toward real time speech understand-
ing it is vital that semantic selectional restrictions be introduced as early as possible
in order to eliminate false phrasal hypotheses at an early stage.
Selectional restrictions are typically associated with lexemes. Each content word
has a semantic type, and many words specify the semantic types of their arguments.
For example, the semantic type of tell is inform and the type of its role theexperiencer
(associated with the indirect object) is almost always human in our trial domain. This
can be expressed as follows.
</bodyText>
<listItem confidence="0.977797">
(21) TELL: &lt;&gt; == DTRANS_V
</listItem>
<footnote confidence="0.332845333333333">
&lt;mor root&gt; == tell
&lt;sem type&gt; == inform
&lt;sem theexperiencer type&gt; == human.
</footnote>
<page confidence="0.973242">
255
</page>
<note confidence="0.517839">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.9993465">
Certain argument types can be assigned default semantic types. For example, by de-
fault the semantic type of subjects must be sentient (a superclass of human). This
works for a large majority of verbs. Of course, defaults of this kind can be overridden
for individual lexemes (such as the verb rain) or word classes (such as copular verbs).
</bodyText>
<subsectionHeader confidence="0.994028">
3.6 Example: The French Noun Phrase
</subsectionHeader>
<bodyText confidence="0.978308714285714">
By way of example, we show how two entries from the French SUNDIAL lexicon, the
determiner le (&apos;the.mAsc&apos;) and the common noun passager (&apos;passenger&apos;), are encoded
in DATR; to put the following section in context, we also show the DIALEX output.
In a simple French noun phrase, we treat the common noun as the head, with the
determiner as an optional argument. Therefore, most of the information is associated
with the common noun.
A common noun inherits from the node NOUN:
</bodyText>
<table confidence="0.9589180625">
(22) NOUN: &lt;&gt; == WORD
&lt;syn head major&gt; == n
&lt;syn head gender&gt; == masc
&lt;syn head case&gt; == nom
&lt;syn args gr_determiner&gt; == GR_DETERMINER:&lt;&gt;
&lt;syn args gr_determiner syn head gender&gt;
&amp;quot; &lt;syn head gender&gt;&amp;quot;
&lt;syn args&gt; == NOUNARGS: &lt;&gt;
&lt;syn head number&gt; ==
&amp;quot;(syn args gr_determiner syn head number&gt;&amp;quot;
&lt;syn head def &gt; ==
&amp;quot; &lt;syn args gr_determiner syn head def &gt;&amp;quot;
&lt;sem type&gt; == entity.
NOUN itself inherits general word features, such as default morphology, from the node
WORD:
(23) WORD: &lt;mor f orm&gt; == &amp;quot;&lt;mor root&gt;&amp;quot; .
</table>
<bodyText confidence="0.833719428571428">
Syntactic and semantic default values such as category (n), gender (masc), case (nom),
and semantic type (entity) are given at the NOUN node. Some of these values may be
overridden, for example in the definition of passager:
(24) Pas sager : &lt;&gt; == NOUN
&lt;mor root&gt; == passager
&lt;sem type&gt; == passenger.
The number and definiteness of the noun phrase are specified by the determiner when
</bodyText>
<page confidence="0.994908">
256
</page>
<note confidence="0.967129">
Francois Andry et al. Making DATR Work for Speech
</note>
<figure confidence="0.9750318">
WORD
ARGS...
GR_DETERMINER NOUNARGS
NOUN
common-nouns
</figure>
<figureCaption confidence="0.987427">
Figure 2
</figureCaption>
<bodyText confidence="0.9064188">
Inheritance graph for French common nouns.
present, whereas the gender of the determiner is copied from the common noun.
Where a feature value is already specified for both noun and determiner at parse
time, the values must be the same if combination is to take place. The definitions for
GR_DETERMINER and NOUNARGS are shown in (25):
</bodyText>
<equation confidence="0.648784166666667">
(25) GR_DETERMINER: &lt;syn head major&gt; == det
&lt;order adj&gt; == any
&lt;order opt&gt; == opt
&lt;order dir&gt; == pre
NOUNARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_determiner&gt;&amp;quot;
&lt;rest&gt; == ARGS: &lt;&gt;.
</equation>
<bodyText confidence="0.999685">
The definition of GR_DETERMINER specifies order and optionality information as well as
the syntactic category (clet). NOUNARGS defines the mapping of case-marked to order-
marked arguments, for simple determiner—noun NPs.
The inheritance graph for this set of DATR sentences is illustrated in Figure 2.
In fact, common nouns may be more complex than our example suggests; they
may have several obliques, for example. Fortunately, DATR allows the creation of
intermediate nodes between the NOUN node and the common nouns, and these nodes
specify distinctive properties of each distinct class of nouns. For example, a RELDAY
node has been created for French in order to describe common grammatical properties
for relative day references such as lendemain (&apos;tomorrow&apos;) and veille (&apos;the day before&apos;).
In the same spirit, NPs with genitive postmodifiers such as le numero du vol (&apos;the number
of the flight/the flight number&apos;), where two nouns are combined, use the node GNOUN,
which specifies general features of the arguments of the head noun.
The definition of the determiner node, DET, is simple in comparison with the NOUN
node, inheriting only from the WORD node. Example (26) shows the definition of DET,
</bodyText>
<page confidence="0.978147">
257
</page>
<figure confidence="0.846127">
Computational Linguistics Volume 18, Number 3
</figure>
<figureCaption confidence="0.717077">
Figure 3
</figureCaption>
<bodyText confidence="0.554597">
DAG lexicon entries for le and passager.
together with entries for le and la (&apos;the.FEm&apos;).
</bodyText>
<listItem confidence="0.668781">
(26) DET: &lt;&gt; == WORD
</listItem>
<bodyText confidence="0.98077175">
&lt;syn head major&gt; == det
&lt;syn head def&gt; == the
&lt;syn head number&gt; == sg
&lt;syn head gender&gt; =. masc
&lt;sem modus def&gt; == the.
le: &lt;&gt; == DET
&lt;mor root&gt; == le.
la: &lt;&gt; == DET
&lt;mor root&gt; == la
&lt;syn head gender&gt; == fem.
The lexical entries for le and passager produced by the DAG-encoding phase of com-
pilation (see Section 4) are shown in Figure 3.
</bodyText>
<figure confidence="0.975611457142857">
_
_
_
-
args :
first :
root : le 1
form : le i
-
- major: det -
syn: head: gender: masc
def : the
_ number : sg _
sem: modus [ def: the ] ]
mor:
root : passager 1
form : passager i
major: n
case: nom
head : gender: masc
def : A
number: B
- _
major : det
gender: masc
syn: head:
def : A
number: B
[opt : opt
order : dir : pre
adj : any
syn:
mor:
sem :
type : passenger]
</figure>
<page confidence="0.996008">
258
</page>
<note confidence="0.997722">
Francois Andry et al. Making DATR Work for Speech
</note>
<sectionHeader confidence="0.811654" genericHeader="method">
4. Lexicon Generation
</sectionHeader>
<subsectionHeader confidence="0.999676">
4.1 Obtaining the DNF Lexicon
</subsectionHeader>
<bodyText confidence="0.884731264705882">
In order to generalize across morphological instantiations, a DATR theory makes use of
nodes at the level of the lexeme. In general, the constraints in a lexeme cannot be simply
represented as a union of paths. This is due to the fact that the sentences making up
the definition of a lexeme for which morphosyntactic variations exist implicitly contain
disjunctions. Because we require the lexicon to be disjoint, our strategy is to cash out
all embedded disjunctions that reference each surface form. The lexicon thus obtained
can be described as being in disjunctive normal form (DNF). This DNF-lexicon will
contain all lexical signs, where a lexical sign incorporates both the surface form and
the corresponding lexeme.
In order to govern the expansion of information in the DATR lexicon, it is necessary
to make a closed world of the feature space defined there. The values that features
may take may be implicit in the DATR lexicon; however such implicit knowledge is not
necessarily complete. Nothing prevents arbitrary extension of the lexicon by additional
features and values, and this may lead to unwanted interactions with existing rules.
We therefore enumerate the possible values of features in a knowledge base known as
the closure definitions. This enumeration is designed to be recursive, to take into account
category-valued features such as the args list. Figure 4 gives an example of closure
definitions, for a sign with only syn and mor attributes. These state the features that
make up a sign; the definition is recursively introduced at the level of &lt;syn args&gt;.
A closure definition takes the form:
cdef (Feature, Fields, FieldVals, FCRs).
A complex feature is composed of fields—either these are atomic valued, and enumer-
ated or declared as open class in FieldVals; or they are complex and their definitions
are to be found elsewhere.
cdef (sign, [syn,mor] , [mor : form=&gt;syn: vform] ) .
cdef (syn, [head, args] , _ ,_) .
cdef (head, [major , type , vf orm , tense , number , person] ,
major== [n, v, det , prep] ,
vf orm== [fin , bse ,prp , psi)] ,
tense== [pres , past] ,
number== [sg, pi] ,
person== [first , second , third]
,
[vf orm:f in =&gt; [tense , person , number] ]
</bodyText>
<footnote confidence="0.2981365">
cdef ( args setof (sign) ,_ ,._) .
cdef (nor, [f orm, root] , [open (f orm) , open (root)] , .
Figure 4
Example closure definitions.
</footnote>
<page confidence="0.982139">
259
</page>
<note confidence="0.520625">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.9825425">
Besides providing closure for DNF lexicon expansion, these definitions have a
number of uses:
</bodyText>
<listItem confidence="0.9460811875">
1. they are used to determine which possible paths the compiler should try
to evaluate in order to build a DAG representation of a lexical sign. The
search proceeds depth-first through the closure definitions, ignoring
those fringes of the search space for which no evaluation is possible.
Values of paths, constraints representing unevaluable reentrancies, and
consistent combinations of these are returned;
2. they provide a filter on the output of the DATR lexicon. Only those
features present in the closure definitions are output. Constraints
incorporating functional labels such as gr_subject are no longer needed;
3. they include a complete set of knowledge definitions for our semantic
representation language (SIL), which is inheritance based. The
inheritance hierarchy for semantic types, for example, is used in bit
coding (Section 5), so that semantic selectional restrictions can be tested
during parsing;
4. they furnish a set of declarative templates against which bit coding and
DAG-term conversion may be carried out.
</listItem>
<bodyText confidence="0.998925823529412">
In addition to an enumeration of feature values, the closure definitions contain Feature
Cooccurrence Restrictions (FCRs) (Gazdar et al. 1985). In principle these could be
encoded in the DATR lexicon, for example, using the feature-value unspec to represent
negative occurrence. Their presence here is not only to restrict the possible feature
combinations that can appear at the head of a sign, but also to detect dependencies
that govern DNF expansion.
The DNF lexicon is obtained as follows. Those features on which the surface form
of a full lexical sign depend, which we shall refer to as its surface form dependency
features, may be derived from the FCRs contained in the closure definitions. Then for
each pair consisting of a DATR node A and a possible assignment to its unassigned
surface form dependency features 1, generate a new DATR node AI&apos;, which inherits
from A and contains the feature assignments in 43. The DATR theory for e is then used
to produce the set of evaluated and unevaluated constraint sentences that describe it.
For example, the base lexical entry for arrive is defined at the DATR node Arr iv e
which is underspecified for the paths &lt;syn head tense&gt;, &lt;syn head person&gt;, and
&lt;syn head number&gt;. For the assignment of values pres, third, sg (respectively) to
these paths, the node Arrivel_presthirdsg is created.
</bodyText>
<subsectionHeader confidence="0.999965">
4.2 Producing Unevaluated Paths
</subsectionHeader>
<bodyText confidence="0.955897909090909">
As we have shown, reentrancies can be specified in DATR using global inheritance; see,
for example, (15) in Section 3.4.1. However, such sentences may not appear directly
in the DAG representation, either because they include paths not derivable within the
closure definitions, or because interaction with higher-ranking exceptions may lead
to weaker equivalences being derived. Any DATR sentence that does not explicitly
introduce a value is treated as a candidate reentrancy constraint; at the stage where
constraint sentences are being derived from a DATR theory, all unevaluated constraint
sentences are retained. In the case of Arrivei_pre sthirdsg, the following constraint
sentences are derived by inheritance from Verb:
(27) &lt;syn args first sem&gt; = &lt;syn args gr_subject sem&gt; .
&lt;sem theagent&gt; = &lt;syn args gr_subject sem&gt; .
</bodyText>
<page confidence="0.996639">
260
</page>
<note confidence="0.997106">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.999057166666667">
DATR inference takes the form of successive reduction of right-hand sides; in (27),
neither sentence is further reducible—both would be ignored by a standard DATR
theorem-prover. 13y passing both constraints to the DAG-building procedure however,
where equality is reflexive as well as transitive (Section 4.3), the two constraints may be
combined to derive the reentrancy between &lt;sem theagent&gt; and &lt;syn args first
sem&gt;.
</bodyText>
<subsectionHeader confidence="0.999947">
4.3 DAG Building and Disjunction Optimization
</subsectionHeader>
<bodyText confidence="0.99936575">
The constraint sentences derived for a DATR node A or for an extension of it A4&apos;
are of the form Path = Value or Pathl = Path2. If consistent, they can be used to
build a DAG corresponding to A. Our DAG-building procedure is based on one
described in Gazdar and Mellish (1989). It builds DAGs by unification of constraints,
so that directionality is irrelevant. For this to succeed, the input constraints must not
contain inconsistencies. This property of correctness is only partially guaranteed by
the constraint-derivation stage, which will tolerate an unevaluated constraint whose
left-hand side is a proper prefix of an evaluated one (but not vice versa), as in (28).
</bodyText>
<equation confidence="0.5605675">
(28) &lt;sem theagent type&gt; = object.
&lt;sem theagent&gt; = &lt;syn args gr_subject sem&gt; .
</equation>
<bodyText confidence="0.9900146">
This will work so long as a contradictory type is not derivable elsewhere. The form of
encoded DAGs is known as normal form (Bouma 1990); that is, if two DAGs share a com-
mon sub-DAG, this is explicitly represented in both, with the exception of unevaluated
sharing sub-DAGs that are represented as Prolog variables. Once the DAG is built,
any remaining unwanted paths are filtered out. In the case of Arrivel_presthirdsg,
this amounts to removing those sub-DAGs introduced at paths containing gr_subje ct
and gr_obliquet
Although the closure definitions ensure that the number of surface form depen-
dency feature assignments for each lexeme is finite, in practice for languages like
English where a number of morphosyntactic feature combinations map onto a smaller
set of surface forms, the DNF lexicon will have more entries than there are distinct sur-
face forms. In cases where a number of entries differ only in a single feature, a phase
of disjunction optimization serves to reduce these, according to the simple equivalence:
(01 A 02 A • • • 04 V WI A 02 A • • • On) =-=- (01 V OD A 02 A • • • On•
Apart from this optimization, the lexicon produced is in DNF form.
</bodyText>
<sectionHeader confidence="0.915571" genericHeader="method">
5. Bit Coding
</sectionHeader>
<subsectionHeader confidence="0.999962">
5.1 Motivation and Requirements
</subsectionHeader>
<bodyText confidence="0.999989888888889">
The last step toward the production of data structures for efficient parsing and gen-
eration is the construction of two separate lexicons: a Prolog term encoding of the
DAGs and a compact bit-encoded lexicon. The motivation for two separate lexicons
is the decision to split the task of parsing into its two aspects: determining grammat-
icality and assigning an interpretation. Since in speech recognition there is also the
added complication of identifying the best-scoring sequence of words from a lattice of
hypotheses, and since an interpretation is only needed for the best sequence, not for
every acceptable one, it is more efficient to separate these tasks. This involves sepa-
rating lexical entries into those features that are constraining (i.e. which affect a sign&apos;s
</bodyText>
<page confidence="0.98979">
261
</page>
<note confidence="0.633081">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.9998778">
capacity to combine with others) and those that simply contribute to its eventual in-
terpretation. The former set is used to produce the bit-coded &apos;acceptance&apos; lexicon, the
latter to form a term-encoded &apos;full&apos; lexicon.
As well as being used in sentence interpretation, the full lexicon is also used in
sentence generation. However, we shall concentrate here on the bit-encoded acceptance
lexicon.
Since the search space when parsing a typical word hypothesis lattice is potentially
great, the acceptance lexicon must be both compact and suitable for processing by ef-
ficient low-level operations. Bit encoding allows unification of feature structures to be
performed by Boolean operations on bit strings, which enables a parser to be imple-
mented in an efficient programming language such as C; it also provides a convenient
representation of disjunctions and negations of feature values.
Two distinct kinds of bit coding are used to represent semantic types and syntactic
head features: both produce vectors of bits that can be stored as integers or lists of
integers.
</bodyText>
<subsectionHeader confidence="0.999879">
5.2 Semantic Type Coding
</subsectionHeader>
<bodyText confidence="0.996243">
The principal semantic type of a lexical entry is a node in a tree-structured (single-
inheritance) sortal hierarchy. Coding for types in the hierarchy is straightforward:
</bodyText>
<listItem confidence="0.997673666666667">
• a terminal node has one unique bit set;
• a nonterminal node is represented by the bitwise Boolean OR of the
codings for the nodes it dominates.
</listItem>
<bodyText confidence="0.9994782">
This scheme requires as many bits as there are terminal nodes in the tree and, assuming
that every nonterminal node dominates at least two subnodes, assigns a unique bit
vector to every node. (A simple example is given in Figure 5). The most specific types
are represented by a bit vector containing a single &apos;1,&apos; and the most general by a vector
with all its bits set. Unification of two types is performed by bitwise AND; since the
hierarchy is tree structured the result of this will be the coding of the more specific
type, or 0 indicating failure if the types are incompatible. The same coding scheme
would also serve if the hierarchy were extended to a multiple-inheritance graph, the
only difference being that bitwise AND could then result in a type distinct from either
of its arguments.
</bodyText>
<subsectionHeader confidence="0.999859">
5.3 Syntactic Feature-Value Coding
</subsectionHeader>
<bodyText confidence="0.999945428571429">
Our approach to the encoding of the feature structures used to represent syntactic
categories is very similar to that proposed in Nakazawa et al. (1988) for implementing
GPSG-style grammars.
A set of features is represented by a bit vector in which for every n-valued feature,
n + 1 bits are assigned, one associated with each value and one bit indicating that the
feature is not present. A value of &apos;0&apos; for a bit means that the feature does not have the
corresponding value; a &apos;1&apos; indicates that the value is a possible one. If the value of a
feature can be specified precisely, the corresponding bit is set, and all the others for
that feature are cleared. Hence the negation of a feature-value pair can be represented
by clearing a bit, and a disjunction of values by setting more than one bit in the
representation of a feature. This fact can be utilized to pack lexical entries together:
if two entries differ only in one atomic-valued feature, they can be combined into a
single entry by this method. Unification is again performed by bitwise AND; failure
is indicated if all the bits for some feature are turned off, meaning that the structures
</bodyText>
<page confidence="0.990413">
262
</page>
<note confidence="0.91463">
Francois Andry et al. Making DATR Work for Speech
</note>
<figure confidence="0.985641357142857">
Type Bit Vector
A 1111111
B 1110000
C 0001111
D 1100000
E 0010000
F 0001000
Type Bit Vector
G 0000110
H 0000001
I 1000000
J 0100000
K 0000100
L 0000010
</figure>
<figureCaption confidence="0.990606">
Figure 5
</figureCaption>
<bodyText confidence="0.982758230769231">
Bit coding of the semantic type hierarchy.
being unified have no common value for this feature. Since this operation only turns
bits off, unification of bit vectors is order-independent (commutative and associative).
The bit vector representation is straightforward for encoding flat feature-value
structures, but presents difficulties when features have categories as values, given the
requirement that the possible values for all features can be enumerated in order to
produce bit vectors of finite size. Although a general solution can be proposed that
uses some pattern of bits to indicate a recursive feature and associates with this feature
another bit vector of the same length (the solution adopted by Nakazawa et al. 1988),
we have chosen a more ad hoc encoding, specifying in advance which features can be
recursive and representing them by pointers to similarly coded structures. The features
that are recursive are the list of arguments of a functor sign and the slash feature used
to handle long-distance dependencies.&apos; (This approach enables the parser to process
</bodyText>
<footnote confidence="0.865209">
4 We follow GPSG in the use of the category-valued feature slash as a propagating device to handle
extraction phenomena. For example in the question &apos;what did you say?&apos;, the phrase &apos;did you say?&apos; can
</footnote>
<page confidence="0.99091">
263
</page>
<table confidence="0.900529777777778">
Computational Linguistics Volume 18, Number 3
major v 1000100001110001 case *
major n ill L case gen
major det case obj
major * case nom
vform fin tense *
vform bse tense past
vform psp tense pres
vform prp vform *
</table>
<figureCaption confidence="0.851489">
Figure 6
</figureCaption>
<bodyText confidence="0.979499666666667">
Sample bit vector for head features major, vform, tense, and case.
signs more efficiently, but unfortunately makes it partly dependent on their structure).
The bit encoding procedure takes as input a DAG representation of individual
lexical entries and is guided in its translation by the closure definitions. A set of
declarations is used to indicate which features are to be included in the acceptance
lexicon, and how they are to be encoded: using either of the bit representations dis-
cussed above, or simply as a list of encodings of their constituents. If no coding type
is specified for a feature, then it is simply ignored.
As a simple example, consider the following partially specified feature structure:
</bodyText>
<equation confidence="0.989727333333333">
[major : v ]
(29) [ head :
vform : fin
</equation>
<bodyText confidence="0.97309025">
Assume that the closure definitions specify values for the head features major, vform,
tense and case, and the FCR:
case = major : n
Then if the node head is declared for bit coding, the vector shown in Figure 6 will
be produced. (The symbol &apos;&apos; stands for &apos;not present&apos;). Note that bits have been set
for all values of the unspecified feature tense, indicating that nothing is known about
its value, but that only the &apos;&apos; bit is set for the feature case, since the FCR blocks its
presence for entries whose major feature is not n.
</bodyText>
<subsectionHeader confidence="0.999302">
5.4 Variable Sharing
</subsectionHeader>
<bodyText confidence="0.971154888888889">
Although the representation of variables and their instantiation to (more or fully) spec-
ified values is straightforward, the implementation of variable sharing or reentrancy
presents a serious problem for bit coding schemes, since there is no means of rep-
resenting identifiable variables. We have adopted a two-fold solution, depending on
the type of the variable. For sign-valued variables, and other large scale structures,
sharing is achieved by means of pointers to common data objects.
This approach cannot be extended down to the level of bit-coded features, since
these involve data below the level of the machine word. Instead a solution based on
be partially characterized, in our notation, as
</bodyText>
<construct confidence="0.629415">
syn: { head: [ major: y
args : 0
slash: [ first: [ syn: [ head: [ major: n ] ] ] ]
</construct>
<bodyText confidence="0.773769">
indicating that it is a sentence from which a noun phrase has been extracted.
</bodyText>
<page confidence="0.996392">
264
</page>
<note confidence="0.996949">
Francois Andry et al. Making DATR Work for Speech
</note>
<bodyText confidence="0.999661">
the use of bit masks has been adopted. The key to this is the recognition that variable
sharing between structures is a limited form of unification, carried out between a
restricted set of their features. If two feature structures represented by bit vectors 0-1
and 02 share a variable for the feature 0, a mask ii is constructed in which all the
bits representing ct. are cleared, and all the rest are set. The values for q5 in the two bit
vectors are unified in the result of the expression:
</bodyText>
<subsectionHeader confidence="0.505822">
/31 A 632 V P)
</subsectionHeader>
<bodyText confidence="0.99984">
Note that a single mask may represent more than one variable shared between two
structures.
A disadvantage of this technique is that it requires the construction of masks for
all possible feature structures within a sign between which variables may be shared.
In practice we can assume that this means only the recursively nested signs of the
args list and slash, and so need relatively few masks.
A description of the two-stage parsing procedure can be found in Andry and
Thornton (1991).
</bodyText>
<sectionHeader confidence="0.957565" genericHeader="evaluation">
6. Implementation and Coverage
</sectionHeader>
<bodyText confidence="0.999926363636364">
DIALEX is implemented in Quintus Prolog; benchmark tests indicate that compilation
time is linear in the size of the lexicon. Development of very large scale lexicons is
somewhat hindered by the current lack of effective debugging tools. We have, how-
ever, succeeded in constructing lexicons that cover a broad range of syntactic phe-
nomena in both French and English. For example, the English DATR lexicon covers all
distinctive lexical forms in our corpus gathered from simulations of flight enquiry dia-
logues (Fraser and Gilbert 1991). Furthermore, one of the major advantages of DAT R&apos;s
inheritance-based approach is ease of adding new lexical entries. For example, a large
number of entries for cities is required in the flight information domain. With the def-
inition of a CITY_PROP node to specify general properties of proper nouns identifying
cities, individual cities such as Paris are simple and quick to define:
</bodyText>
<listItem confidence="0.924315">
(30) Paris: &lt;&gt; == CITY_PROP
</listItem>
<bodyText confidence="0.966522">
&lt;mor root&gt; == paris
&lt;sem thecity value&gt; == paris .
Extending the lexicon to include new verbs, especially verbs with idiosyncratic prop-
erties like try, takes more time and effort.
This paper has been mainly concerned with the definition and compilation of
lexicons for understanding. In fact, SUNDIAL applications are such that a produc-
tion lexicon shares a considerable portion with its recognition counterpart. To this
end, DIALEX has been adapted for compilation of a generation lexicon (Youd and
McGlashan 1992). This is derived from the same DATR definitions but differs from the
parser lexicons in that indexing is based on semantic type and complexity, rather than
the surface string, and inflection is factored away from the lexical entries.
</bodyText>
<sectionHeader confidence="0.933284" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9977385">
In the design of our lexicon compilation tool, we have shown how linguistic knowl-
edge can be arranged in terms of a set of DATR structured base definitions that are
</bodyText>
<page confidence="0.975895">
265
</page>
<note confidence="0.358249">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.999864470588235">
portable across applications. Knowledge at the levels of morphology, syntax, and se-
mantics combines in a single reusable DATR database. The fact that this knowledge
is expressed in a high-level representation language does not limit its usefulness. On
the contrary, it allows the designers of base definitions or application lexicons to think
clearly about the structural relations that hold between objects in the representation
and to maximize generalizations. Default inheritance allows generalizations to trickle
down to specific instances, unless overridden. As a consequence, every good gener-
alization captured during the design of structured base definitions represents labor
saved during subsequent application-specific work.
We have also shown how high-level knowledge can be entered by the lexicon
builder at the appropriate conceptual level and then compiled into a lower level form
appropriate for a chosen application. The system we describe produces two kinds of
output: a term-encoded full lexicon for use in sentence interpretation and generation,
and a lower level bit-encoded acceptance lexicon for use in lattice pruning during
speech processing. The modular design of our system makes it particularly easy to
exchange existing coding modules for new ones, thus allowing linguistic knowledge
to be customized for a wide variety of speech or language applications.
</bodyText>
<sectionHeader confidence="0.991343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928350649351">
Andry, Francois, and Thornton, Simon.
(1991). &amp;quot;A parser for speech lattices using
a UCG grammar.&amp;quot; In Proceedings, 2nd
European Conference on Speech
Communication and Technology. Genova,
September 1991, 219-222.
Bilange, Eric. (1991). &amp;quot;A task independent
oral dialogue model.&amp;quot; In Proceedings, 5th
Meeting of the European Chapter of the
Association for Computational Linguistics.
Berlin, April 1991, 83-88.
Bouma, Gosse. (1990). &amp;quot;Defaults in
unification grammar.&amp;quot; In Proceedings, 28th
Annual Meeting of the Association for
Computational Linguistics. Pittsburgh, June
1990, 165-172.
Calder, Jo; Klein, Ewan; and Zeevat, Henk.
(1988). &amp;quot;Unification Categorial Grammar:
a consise extendable grammar for natural
language processing.&amp;quot; In Proceedings,
COLING-88. Budapest, August 1988,
83-86.
Charniak, Eugene, and McDermott, Drew.
(1985). An Introduction to Artificial
Intelligence. Lawrence Erlbaum Associates.
Evans, Roger, and Gazdar, Gerald. (1989).
&amp;quot;Inference in DATR.&amp;quot; In Proceedings, 4th
Meeting of the European Chapter of the
Association for Computational Linguistics.
Manchester, April 1989, 66-71.
Evans, Roger, and Gazdar, Gerald, eds.
(1990). The DATR Papers. Research Report
CSRP 139, School of Cognitive and
Computing Science, University of Sussex.
Flickinger, Daniel P.; Pollard, Carl J.; and
Wasow, Thomas. (1985).
&amp;quot;Structure-sharing in lexical
representation.&amp;quot; In Proceedings, 23rd
Annual Meeting of the Association for
Computational Linguistics. Chicago,
262-267.
Fraser, Norman M., and Gilbert, G. Nigel.
(1991). &amp;quot;Effects of system voice quality on
user utterances in speech dialogue
systems.&amp;quot; In Proceedings, 2nd European
Conference on Speech Communication and
Technology. Genova, September 1991,
57-60.
Gazdar, Gerald. (1987). &amp;quot;Linguistic
applications of default inheritance
mechanisms.&amp;quot; In Linguistic Theory and
Computer Applications, edited by Peter
Whitelock; Harold Somers; Rod Johnson;
and Mary McGee Wood. Academic Press.
Gazdar, Gerald. (1990). &amp;quot;An introduction to
DATR.&amp;quot; In The DATR Papers, edited by
Roger Evans and Gerald Gazdar.
Research Report CSRP 139, School of
Cognitive and Computing Science,
University of Sussex: 1-14.
Gazdar, Gerald; Klein, Ewan; Pullum,
Geoffry; and Sag, Ivan. (1985). Generalized
Phrase Structure Grammar. Harvard
University Press. Cambridge, MA.
Gazdar, Gerald, and Mellish, Chris. (1989).
Natural Language Processing in Prolog.
Addison Wesley.
Hudson, Richard A. (1990). English Word
Grammar. Basil Blackwell.
McGlashan, Scott; Fraser, Norman M.;
Gilbert, G. Nigel; Bilange, Eric;
Heisterkamp, Paul; and Youd, Nick J.
(1992). Dialogue management for
telephone information systems. In
Proceedings of the 3rd Conference on Applied
Natural Language Processing. Trento, April:
245-246.
</reference>
<page confidence="0.998426">
266
</page>
<note confidence="0.991796">
Francois Andry et al. Making DATR Work for Speech
</note>
<reference confidence="0.999792666666667">
Nakazawa, Tsuneko; Neher, Laura; and
Hinrichs, Erhard W. (1988). &amp;quot;Unification
with disjunctive and negative values for
GPSG grammars.&amp;quot; In Proceedings, 8th
European Conference on Artificial Intelligence.
Munich, August 1990,467-472.
Peckham, Jeremy. (1991). &amp;quot;Speech
understanding and dialogue over the
telephone: an overview of the SUNDIAL
project.&amp;quot; In Proceedings, 2nd European
Conference on Speech Communication and
Technology. Genova, September 1991,
1469-1472.
Pollard, Carl, and Sag, Ivan A. (1987).
Information-Based Syntax and Semantics.
CSLI, Stanford, CA.
Shieber, Stuart M. (1986). An Introduction to
Unification-Based Approaches to Grammar.
CSLI, Stanford, CA.
Youd, Nick J.; and McGlashan, Scott. (1992).
Generating utterances in dialogue
systems. In Aspects of Automated Natural
Language Generation: Proceedings of the 6th
International Workshop on Natural Language
Generation, edited by Robert Dale; Eduard
Hovy; Dietmar Rosner; and Olivero Stock.
Academic Press, London.
</reference>
<page confidence="0.997299">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9988735">Making DATR Work for Speech: Lexicon Compilation in SUNDIAL</title>
<author confidence="0.988522">Francois Andry</author>
<title confidence="0.496287">Cap Gemini Innovation</title>
<author confidence="0.929417">Scott McGlashant</author>
<affiliation confidence="0.99226">University of Surrey</affiliation>
<author confidence="0.997346">Nick J You&amp;</author>
<affiliation confidence="0.971832">Logica Cambridge Ltd.</affiliation>
<author confidence="0.965339">Norman M Fraserl</author>
<affiliation confidence="0.98571">University of Surrey</affiliation>
<author confidence="0.578374">Simon Thorntonl</author>
<affiliation confidence="0.672246">Logica Cambridge Ltd.</affiliation>
<abstract confidence="0.998018176470588">We present DIALEX, an inheritance-based tool that facilitates the rapid construction of linguisknowledge bases. Simple lexical entries are added to an application-specific lexicon that inherits morphosyntactic, syntactic, and lexico-semantic constraints from an applicationset of structured base definitions. A lexicon generator expands the lexiout into a normal form lexicon. is then encoded either as an acceplexicon which the constraining features are bit-encoded for use in pruning word or as a lexicon is used for assigning interpretations or for generating messages). In this paper we describe DIALEX, a modular inheritance-based tool for the construction of lexicalized grammar knowledge bases. DIALEX has been developed as part of the SUNDIAL (Speech UNderstanding and DIALogue) project—currently one of Europe&apos;s largest collaborative research projects in speech and language technology.&apos; SUNDIAL&apos;s main project goal is to produce four prototype systems that support relatively unconstrained telephone dialogs for limited domains in each of English, French, German, and Italian (Peckham 1991). This paper reports work carried out in the development of the English and French systems. These share a common application domain, namely flight enquiries and reservations.</abstract>
<note confidence="0.659500076923077">Cap Gemini Innovation, 118 Rue de Tocqueville, 75017 Paris, France (andry@capsogeti.fr). t Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, U.K. (norman@soc.surrey.ac.uk; scott@soc.surrey.ac.uk). Logica Cambridge Ltd, Betjeman House, 104 Hills Road, Cambridge, CB2 1LQ, U.K. (simont@logcam.co.uk; nick@logcam.co.uk). 1 The work reported here was supported in part by the Commission of the European Communities as part of ESPRIT project P2218, SUNDIAL. Partners in the project are Cap Gemini Innovation, CNET, IRISA (France); Daimler-Benz, Siemens, University of Erlangen (Germany); CSELT, Saritel (Italy); Logica Cambridge Ltd, University of Surrey (U.K). We acknowledge with gratitude the helpful and suggestions of the editors and reviewers of this special issue of Linguistics and of Lionel Moser. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 3</note>
<abstract confidence="0.994274935294118">The process of writing linguistic knowledge bases has been guided by a number of design requirements on the SUNDIAL project as a whole. First of all, prototype systems must be capable of Therefore grammars must be appropriate for the purposes of speech processing. For example, they must reflect the fact that input to the parser is a word lattice or graph from which some of the spoken words (typically short words such as function words) may be missing. Each prototype system must be capable of Speech generation takes place in two stages. In the first stage, text is generated. In the second stage, a text-to-speech system outputs the message. Therefore the linguistic knowledge must also be structured appropriately the purposes of 3. Each system must run in real time or near real time. Therefore the linguistic knowledge must be structured so as to allow rapid access and manipulation. 4. Portability to new applications should be simple; work required to write new linguistic knowledge bases should therefore be kept to a minimum. 5. Duplication of effort must be avoided. This must be true in respect of the components of each separate prototype system. For example, the same dialog manager software module has been used in each prototype with minor customizations for each language (Bilange 1991; McGlashan et al. 1992). The same principle should apply to the design of tools for the construction of knowledge bases, including lexical knowledge bases. Thus, the task of adding a new lexical item should only require the addition of knowledge that is idiosyncratic to that lexical item and not predictable from what is already present in the knowledge base. Section 2 of this paper presents an overview of the SUNDIAL DIALEX tool. Section 3 describes the way in which linguistic knowledge is initially expressed in terms of declarative DAT R theories. Section 4 explains how a compact DATR knowledge base is expanded out into a fully specified lexicon. Section 5 relates how the lexicon can be customized for the purposes of real-time speech parsing. Practical experiences of constructing and using DIALEX are recounted in Section 6. Concluding observations are drawn in Section 7. 2. Overview of the System In common with contemporary generative theories that are unification based and for which information is concentrated in the lexicon (Pollard and Sag 1987; Calder et al. we adopt the our basic unit of linguistic representation. For a given lexical entry, a sign describes the constraints—morphological, syntactic, and semantic—it 246 Francois Andry et al. Making DATR Work for Speech The sign for intransitive example, is: mor: root : arrive 1 : arrives major: v vform : fin head : tense : pres person : third number : sg syn: sem : : arrive thetime : [ type : time I RestT 1 theagent : [ type : object I RestA ] args: ( syn: head: sem : type: [opt order: dir adj _ syn: sem : type : opt order: [dir adj head r major: prep I case: at jRestT ] : opt post : any : major: n case: nom person : third _ number: sg object I RestA ] : oblig pre : any : lexical sign for syntactic head features that help to determine the inflected form, with an args list that constrains its environment within the phrase of which it is the head; the sem feature represents the semantic structure that will be assigned to that phrase. The sign shows that the verb may optionally be followed a prepositional phrase whose semantics will fill the semantic role The argument preceding the verb is constrained to be third person singular nominative not object-marked), and supplies the filler for the semantic role In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments the Only the latter appear in the final version of the lexicon. 2 In our representation of feature structures we follow Prolog conventions, whereby variables are identified by initial capitals, and a vertical bar introduces the tail of a list. 247 Computational Linguistics Volume 18, Number 3 A major issue for approaches such as ours is whether or not regularities in the lexicon should be expanded out off-line, or remain for lazy evaluation during parsing. We are sympathetic with the latter approach, for reasons of the economies that can be achieved in lexicon size. However, we believe that a precompiled lexicon is more appropriate to current speech recognition technology. Parsing typically involves extremely large lattices of lexical hypotheses with imprecise boundaries, and is thus computationally expensive. Our experience suggests that the trade-off between lexicon size and the cost of online inference is such as to favor lexicon size, in the case of application-specific lexicons of the size required in the SUNDIAL systems (around 2000 words). For inflection-impoverished English and (somewhat richer) French, which form the basis of our work, limited morphological decomposition during parsing is avoided; instead the parser lexicon consists of fully inflected forms. The parser lexicon we have developed has the following two properties. 1. It is indexed by surface forms, i.e. fully inflected words that are unique at the phonological level. Efficiency of access is achieved by allowing some internal disjunctions within entries in cases where the surface form can be derived from a number of morphosyntactic feature combinations. It consists of two separate knowledge bases: an lexicon a lexicon. former is designed for efficient parsing. Only those features that constrain the ability of a sign to combine are represented. These include syntactic head features and semantic types. The encoding technique uses bit-vectors to achieve economy of representation and fast unification. The full lexicon contains signs with no information missing; the information in a full lexical entry is therefore a superset of the corresponding acceptance lexicon entry. takes place in two phases: parsing the acceptance lexicon, involving search with intensive computation; and building, operates on the analysis tree produced by the first phase, using term unification to combine the entries from the full lexicon corresponding to the lexical entries found by the first phase. The lexicon compilation architecture that we present in this paper is outlined in Figure 1. At the lexical encoding interface, a human lexicon builder builds an applicationsublanguage-specific lexicon, using a set of structured definitions, generalize over commonalities and provide macros with which to structure entries (Section 3). Both of these are written in DATR; we refer to the output of this as the DATR generator compiles this into a lexicon for which the entries directed acyclic graphs (DAGs) indexed by surface forms. For this a set of used. These constitute a knowledge base forming a set of meta-definitions to complement the DATR lexicon, as well as rendering explicit what may be implicit in the latter (Section 4). The resulting entries are encoded in two ways: for the full lexicon via Prolog term encoding and for the acceptance lexicon via bit coding (Section 5). 3. Encoding Linguistic Knowledge 3.1 DATR a declarative language for representing inheritance networks that support default inheritance. Knowledge is expressed in DATR in terms of path equa- 248 Francois Andry et al. Making DATR Work for Speech F BUILDER lexicon structured base definitions DATR lexicon generation DATR definitions coding coding lexicon full lexicon DIALEX lexicon compilation architecture. tions. The syntax of paths is a superset of that found in the PATR-II language (Shieber For example, (1) identifies two different paths in the DAG rooted at an inheritance network.</abstract>
<note confidence="0.666362444444444">(1) Nodel: &lt;syn head case&gt; Nodel: &lt;syn head number&gt; The path equations we present in this paper take the following forms: a. &lt;&gt; == Node2 b. Nodel: Path1 == Valuel c. Nodel: Pathl == &amp;quot;Path2&amp;quot; d. Nodel : Path1 == Node2:Path2 e. Nodel: Path1 == Node2:&lt;&gt; form shown in (2a) is the special case in which the path at is This</note>
<abstract confidence="0.990152596296298">inherit all equations available at those incompatible with at equations are incompatible if they both make assignments to same path. The form shown in (2b) is used to assign values to paths, e.g. &lt;syn == Alternatively, a value may be copied from elsewhere in the DAG. (2c) used to assign to whatever value found for at original query The double quotes are significant here because they indicate that be globally. If the quotes were not present, be evaluated locally 249 Computational Linguistics Volume 18, Number 3 assigned the value of at Node1 such a value existed. The form shown in assigns to whatever value found at A special this is (2e), which allows extensions of be specified at example, the DAT R theory in (3) yields the theorems for node shown in (4). (3) Ex1: &lt;head major&gt; == n &lt;head case&gt; == nom. Ex2: &lt;syn&gt; == Exl: &lt;&gt; . (4) Ex2: &lt;syn head major&gt; = n. Ex2: &lt;syn head case&gt; = nom. For a more detailed description of DAT R see Evans and Gazdar (1990). 3.2 The Linguistic Framework Linguistic knowledge is structured in terms of a simple unification categorial grammar (Calder et al. 1988) in which featural constraints at the levels of morphology, syntax, and semantics may all occur in a lexical sign. The basic sign structure of lexical entries is shown in (5). morphology: [. 1 (5) syntax : [. . semantics : [. .1 basic sign structure of the value is shown in (6). (6) syntax: head : [. . 1 args : [.. .] j includes attribute-value structures for such things as tense, person, and definiteness. The is stack-valued, with stack position determining the order in which arguments may be combined by functional application. basic sign structure of the value is shown in (7). (7) semantics : id :&lt; value &gt; type :&lt; value &gt; modus: [.. role* : [. .1 semantic object has a unique index locates the object in a hierarchy. The specifies a number of constraints imposed on the interpretation of semantic objects, such as polarity, aspect, and tense. Semantic roles as thetime, theinstrument) specified within the inheritance-based definitions for semantic types. signs are defined in terms of a dual-component DAT R lexicon. The definian application-independent account of morphosyntax, transitivity, and lexico-semantic constraints. They define what can be thought of as most of the higher nodes of an inheritance hierarchy. The base definitions as a whole are, of course, language specific, although significant exchange of definitions has been possible during 250 Francois Andry et al. Making DATR Work for Speech parallel development of our English and French DATR theories. The applicationlexicon be thought of as a collection of lower nodes that hook onto the bottom of the hierarchy defined by the structured base definitions. Whereas the structured base definitions provide a general morphological, syntactic, and lexico-semantic account of a language fragment, the application-specific lexicon provides a vocabulary and a task-related lexical semantics. Ideally, a change of application should only necessitate a change of an application-specific lexicon. Naturally, application-specific lexicons take much less time to construct than the base lexicon. Much of our discussion in the rest of this section will focus on the structured base definitions. Since the requirements of speech processing in real time rule out online morphological parsing, a full-form lexicon must be produced. However, the task of entering all possible forms of a word into the lexicon by hand would be both time consuming and repetitive. We therefore provide a subtheory of morphology in the DATR base definitions so that the grammar writer need only specify exceptional morphology for each lexeme, leaving the lexicon generator to expand out all of the regular forms. The surface form of an English verb encodes information relating to finiteness, tense, number, and person. What is required in the DATR theory is a number of condition—action statements that say things like: verb is finite IF is present tense person form is &lt;root&gt;+s form is &lt;root&gt;. desired effect is achieved by means of DATR&apos;s paths. following path is included at the (8) VERB: &lt;mor form&gt; == VERB_MOR:&lt;&gt; looks like this: (9) VERB_MOR: &lt;bse&gt; == &amp;quot;&lt;mor root&gt;&amp;quot; &lt;prp&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; ing) &lt;psp&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; ed) &lt;fin&gt; == &amp;quot;&lt; &amp;quot;&lt;syn head tense&gt;&amp;quot; &amp;quot;&lt;syn head number&gt;&amp;quot; &amp;quot;&lt;syn head person&gt;&amp;quot; &gt;&amp;quot; &lt;pres&gt; == &amp;quot;&lt;mor root&gt;&amp;quot; &lt;pres sg third&gt; == (&amp;quot;&lt;mor root&gt;&amp;quot; s) &lt;past&gt; == &amp;quot;&lt;mor form psp&gt;&amp;quot; . 251 Computational Linguistics Volume 18, Number 3 The base, present participle, and past participle forms are immediately available. If the verb is finite it is necessary to construct an evaluable path consisting of tense, number, and person values. If the tense is past (last line), the form is copied from the form of the past participle. If the form is present singular third person (second last line), the form is &lt;root&gt; +s. Otherwise, the present tense form is copied from the root form. Exceptional forms are stated explicitly, thus overriding default forms. For example, following entry for that it has exceptional past forms. (10) HEAR: &lt;&gt; == VERB root &gt; == &lt;mor form psp&gt; == heard. evaluable path mechanism is also used to set the value of an agreement feature to fps (third person singular) or not _tps. The path equation shown in (11), augmented the information at the (12) then requires subject and verb to share same value. The subject&apos;s is set by the definitions in (11) VERB: &lt;syn args gr_subject syn head agr&gt; == V_AGREE:&lt; &amp;quot; &lt;syn head tense&gt;&amp;quot; &amp;quot;(syn head number&gt;&amp;quot; &amp;quot; &lt;syn head person&gt;&amp;quot; &gt;. (12) V_AGREE: &lt;pres&gt; == not_tps &lt;pres sg third&gt; == tps. (13) NOUN: &lt;syn head agr&gt; == N_AGREE: &lt;agr &amp;quot; &lt;syn head number&gt; &amp;quot; &amp;quot; &lt;syn head person&gt; &amp;quot; &gt; . N_AGREE: &lt;agr&gt; == not_tps &lt;agr sg third&gt; == tps . English verb morphology presents no real problems; noun morphology is even simpler. French morphology is rather more complicated. However, it can be handled by means of the same general technique of allowing evaluable paths to act as case statements that select the appropriate morphological form. Instead of a unified account of French verb morphology there are a number of distinct inflectional paradigms from which different verbs inherit. A more sophisticated account of subject—verb agreement is also required. 3.4 Transitivity Consider the relationship between verbs of different transitivity. An intransitive verb takes a subject only. A transitive verb takes a subject and an object. A ditransitive verb takes a subject and two objects, one direct and the other indirect. This information In a few exceptional cases (e.g. the singular of complex constraints on agreement are stated in the relevant lexical entries. 252 Francois Andry et al. Making DATR Work for Speech is easily expressible in terms of an inheritance hierarchy. Facts about subjects are with a top node, for example a node called about direct objects associated with another node, for example, a node called saying that a VERB, general information about subjects is inherited at the node. This relationship can be expressed simply in DATR. A similar treatment can be adopted for ditransitive verbs (DTRANS_V): (14) VERB: &lt;syn head major&gt; == v &lt;syn args gr_subject&gt; == GR_SUBJECT:&lt;&gt;. TRANS_V: &lt;&gt; == VERB &lt;syn args gr_direct&gt; == GR_DIRECT:&lt;&gt; DTRANS_V: &lt;&gt; == TRANS_V &lt;syn args gr_indirect&gt; == GR_INDIRECT:&lt;&gt; . of the form &lt;syn gr_subject&gt; == GR_SUBJECT:&lt;&gt; a convenient way of packaging up all information relating to an argument type at a single node (part of the information stored at this node can be found in (18) below; notice that arguments are identified by unique labels such gr_subj and gr_direct). We have already noted that in our sign representation, arguments are distinguished by their position in a stack. This ought to render unique argument labels superfluous. In fact, there are a number of reasons why it is desirable to use unique labels in the DATR theory. Firstly, they allow individual arguments of a word to be picked out (see Section 3.4.1 below). Secondly, they allow classes of argument to be identified and generalizations to be made where appropriate. For example, we show in Section 3.4.2 how order and optionality generalizations can be made over argument types, and how a system organized around named arguments can be mapped within DATR into an order-marked system. Finally, grammatical relation labels are much easier for grammar writers to remember and manipulate than positionally encoded argument structures. Consider the following partial DATR entry for English infinitival complement verbs. (15) INF_COMP_V: &lt;&gt; == VERB &lt;syn args gr_comp&gt; == GR_COMP:&lt;&gt; &lt;syn args gr_comp syn args gr_subject&gt; &amp;quot;&lt;syn args gr_subject&gt;&amp;quot;. first line states that an infinitival complement verb inherits from the i.e., it is a verb that must have a subject. The second line introduces a number of on the complement. These constraints—collected at the node— include the fact that the complement must be the infinitive form of a verb. The next line enables the complement to share the subject of the matrix verb, i.e., in a sentence wants to fly, Amy the subject of both fly. 253 Computational Linguistics Volume 18, Number 3 3.4.1 Unevaluated Path Equations. Consider the relationship between the semantics of a verb and the semantics of its subject. The semantics of the subject must be coindexed with a semantic role of the verb such as theagent, as shown in (16). syn : [ args : [ gr_subject : [ sem : A ] ] ] sem : [ theagent : A ] This reentrancy can be expressed in DATR as follows: (17) &lt;sem theagent&gt; == &amp;quot;&lt;syn args gr_subject sem&gt;&amp;quot; argument labeled typically underspecified in the lexicon and awaits full specification at parse time. Because of this, the constraint is carried over to the DAG-encoding phase of lexicon compilation, where it becomes a reentrancy, as described in Section 4. 3.4.2 Argument Order and Optionality. While arguments in the structured base definitions are identified by grammatical relation labels, such as gr_subj ect, the lexicon generation process requires arguments encoding order and optionality constraints that identified by relative position in an args techniques are used to produce DATR theories with arguments structured in this way. The first technique is to define featural constraints of order and optionality for each grammatical relation label. Three types of constraint are defined: dir: indicating whether the argument precedes or follows the functor (pre or post); adj: indicating whether the argument is adjacent to the functor or not (next or any); and indicating whether the argument is optional or obligatory or oblig). identified as gr_subject and gr_oblique, example, inherit the following ordering constraints: (18) GR_SUBJECT: &lt;order dir&gt; == pre &lt;order adj&gt; == next &lt;order opt&gt; == oblig. GR_OBLIQUE: &lt;order dir&gt; == post &lt;order adj&gt; == any &lt;order opt&gt; == opt Whereas the subject is obligatory, and precedes the functor and allows for intervening constituents, the oblique argument is optional and may appear in any position following the functor. The second technique maps arguments identified by relation labels onto arguments identified by position in a linked list. Relative position is encoded in terms of the 254 Francois Andry et al. Making DATR Work for Speech and rest: first the first argument in a list, and the linked list of remaining arguments. Consider part of the base definition for transitive verbs, as shown in (19). (19) TRANS_V: &lt;&gt; == VERB &lt;syn args gr_direct&gt; == GR_DIRECT:&lt;&gt; &lt;syn args&gt; == TVARGS:&lt;&gt; . Part of the collection of nodes devoted to mapping named arguments onto ordermarked arguments is shown in (20). (20) TVARGS: &lt;&gt; == == : &lt;&gt; . DTVARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_subject&gt;&amp;quot; &lt;rest&gt; == ARGS1:&lt;&gt; . DARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_direct&gt;&amp;quot; &lt;rest&gt; == ARGS3:&lt;&gt; . ARGS3: &lt;first&gt; == &amp;quot;&lt;syn args oblique1&gt;&amp;quot; &lt;rest&gt; == ARGS4:&lt;&gt;. the value of finds it by evaluating path &amp;quot;&lt;syn args &lt;rest&gt; is then inherited from the of &lt;rest &gt; inherits from &amp;quot;&lt;syn gr_direct&gt;.&amp;quot; &lt;rest&gt; of &lt;rest&gt; then inherits from specifies the position of arguments within the of transitive verbs. 3.5 Lexico-Semantic Constraints A word lattice is likely to include numerous semantically anomalous but syntactically well-formed constructions. In a system that aims toward real time speech understanding it is vital that semantic selectional restrictions be introduced as early as possible in order to eliminate false phrasal hypotheses at an early stage. Selectional restrictions are typically associated with lexemes. Each content word has a semantic type, and many words specify the semantic types of their arguments. example, the semantic type of is the type of its role with the indirect object) is almost always our trial domain. This can be expressed as follows. (21) TELL: &lt;&gt; == DTRANS_V &lt;mor root&gt; == tell &lt;sem type&gt; == inform &lt;sem theexperiencer type&gt; == human. 255 Computational Linguistics Volume 18, Number 3 Certain argument types can be assigned default semantic types. For example, by dethe semantic type of subjects must be superclass of works for a large majority of verbs. Of course, defaults of this kind can be overridden individual lexemes (such as the verb word classes (such as copular verbs). 3.6 Example: The French Noun Phrase By way of example, we show how two entries from the French SUNDIAL lexicon, the and the common noun are encoded put the following section in context, we also show the DIALEX output. In a simple French noun phrase, we treat the common noun as the head, with the determiner as an optional argument. Therefore, most of the information is associated with the common noun. common noun inherits from the node (22) NOUN: &lt;&gt; == WORD &lt;syn head major&gt; == n &lt;syn head gender&gt; == masc &lt;syn head case&gt; == nom &lt;syn args gr_determiner&gt; == GR_DETERMINER:&lt;&gt; &lt;syn args gr_determiner syn head gender&gt; &amp;quot; &lt;syn head gender&gt;&amp;quot; &lt;syn args&gt; == NOUNARGS: &lt;&gt; &lt;syn head number&gt; == &amp;quot;(syn args gr_determiner syn head number&gt;&amp;quot; &lt;syn head def &gt; == &amp;quot; &lt;syn args gr_determiner syn head def &gt;&amp;quot; &lt;sem type&gt; == entity. inherits general word features, such as default morphology, from the node WORD: (23) WORD: &lt;mor f orm&gt; == &amp;quot;&lt;mor root&gt;&amp;quot; . and semantic values as category (n), gender (masc), case (nom), semantic type (entity) are given at the Some of these values may be for example in the definition of (24) Pas sager : &lt;&gt; == NOUN &lt;mor root&gt; == passager &lt;sem type&gt; == passenger. The number and definiteness of the noun phrase are specified by the determiner when 256 Francois Andry et al. Making DATR Work for Speech WORD ARGS... GR_DETERMINER NOUNARGS NOUN common-nouns Figure 2 Inheritance graph for French common nouns. present, whereas the gender of the determiner is copied from the common noun. Where a feature value is already specified for both noun and determiner at parse time, the values must be the same if combination is to take place. The definitions for shown in (25): &lt;syn head major&gt; == det &lt;order adj&gt; == any &lt;order opt&gt; == opt &lt;order dir&gt; == pre NOUNARGS: &lt;first&gt; == &amp;quot;&lt;syn args gr_determiner&gt;&amp;quot; &lt;rest&gt; == ARGS: &lt;&gt;. definition of specifies order optionality information as well as syntactic category NOUNARGS the mapping of case-marked to ordermarked arguments, for simple determiner—noun NPs. The inheritance graph for this set of DATR sentences is illustrated in Figure 2. In fact, common nouns may be more complex than our example suggests; they may have several obliques, for example. Fortunately, DATR allows the creation of nodes between the and the common nouns, and these nodes distinctive properties of each distinct class of nouns. For example, a node has been created for French in order to describe common grammatical properties relative day references such as and day before&apos;). the same spirit, NPs with genitive postmodifiers such as numero du vol number the flight/the flight number&apos;), where two nouns are combined, use the node which specifies general features of the arguments of the head noun. definition of the determiner node, is simple in comparison with the NOUN only from the Example (26) shows the definition of 257 Computational Linguistics Volume 18, Number 3 Figure 3 lexicon entries for with entries for (26) DET: &lt;&gt; == WORD &lt;syn head major&gt; == det &lt;syn head def&gt; == the &lt;syn head number&gt; == sg &lt;syn head gender&gt; =. masc &lt;sem modus def&gt; == the. le: &lt;&gt; == DET &lt;mor root&gt; == le. la: &lt;&gt; == DET &lt;mor root&gt; == la &lt;syn head gender&gt; == fem. lexical entries for by the DAG-encoding phase of compilation (see Section 4) are shown in Figure 3. _ _ _ args : first : root : le 1 form : le i - major: det syn: head: gender: masc def : the _ number : sg _ sem: modus [ def: the ] ] mor: root : passager 1 form : passager i case: nom head : gender: masc def : A number: B major : det gender: masc syn: head: def : A number: B [opt : opt order : dir : pre adj : any syn: mor: sem : type : passenger] 258 Francois Andry et al. Making DATR Work for Speech 4. Lexicon Generation 4.1 Obtaining the DNF Lexicon In order to generalize across morphological instantiations, a DATR theory makes use of nodes at the level of the lexeme. In general, the constraints in a lexeme cannot be simply represented as a union of paths. This is due to the fact that the sentences making up the definition of a lexeme for which morphosyntactic variations exist implicitly contain disjunctions. Because we require the lexicon to be disjoint, our strategy is to cash out all embedded disjunctions that reference each surface form. The lexicon thus obtained be described as being in disjunctive normal form (DNF). This contain all lexical signs, where a lexical sign incorporates both the surface form and the corresponding lexeme. In order to govern the expansion of information in the DATR lexicon, it is necessary to make a closed world of the feature space defined there. The values that features may take may be implicit in the DATR lexicon; however such implicit knowledge is not necessarily complete. Nothing prevents arbitrary extension of the lexicon by additional features and values, and this may lead to unwanted interactions with existing rules. We therefore enumerate the possible values of features in a knowledge base known as definitions. enumeration is designed to be recursive, to take into account features such as the Figure 4 gives an example of closure for a sign with only These state the features that make up a sign; the definition is recursively introduced at the level of &lt;syn args&gt;. A closure definition takes the form: cdef (Feature, Fields, FieldVals, FCRs). A complex feature is composed of fields—either these are atomic valued, and enumeror declared as open class in they are complex and their definitions are to be found elsewhere. cdef (sign, [syn,mor] , [mor : form=&gt;syn: vform] ) . cdef (syn, [head, args] , _ ,_) . cdef (head, [major , type , vf orm , tense , number , person] , major== [n, v, det , prep] , vf orm== [fin , bse ,prp , psi)] , tense== [pres , past] , number== [sg, pi] , person== [first , second , third] , [vf orm:f in =&gt; [tense , person , number] ] args (sign) ,_ ,._) . cdef (nor, [f orm, root] , [open (f orm) , open (root)] , . Figure 4 Example closure definitions. 259 Computational Linguistics Volume 18, Number 3 Besides providing closure for DNF lexicon expansion, these definitions have a number of uses: 1. they are used to determine which possible paths the compiler should try to evaluate in order to build a DAG representation of a lexical sign. The search proceeds depth-first through the closure definitions, ignoring those fringes of the search space for which no evaluation is possible. Values of paths, constraints representing unevaluable reentrancies, and consistent combinations of these are returned; 2. they provide a filter on the output of the DATR lexicon. Only those features present in the closure definitions are output. Constraints functional labels such as no longer needed; 3. they include a complete set of knowledge definitions for our semantic representation language (SIL), which is inheritance based. The inheritance hierarchy for semantic types, for example, is used in bit coding (Section 5), so that semantic selectional restrictions can be tested during parsing; 4. they furnish a set of declarative templates against which bit coding and DAG-term conversion may be carried out. In addition to an enumeration of feature values, the closure definitions contain Feature Cooccurrence Restrictions (FCRs) (Gazdar et al. 1985). In principle these could be in the DATR lexicon, for example, using the feature-value represent negative occurrence. Their presence here is not only to restrict the possible feature combinations that can appear at the head of a sign, but also to detect dependencies that govern DNF expansion. The DNF lexicon is obtained as follows. Those features on which the surface form a full lexical sign depend, which we shall refer to as its form dependency be derived from the FCRs contained in the closure definitions. Then for each pair consisting of a DATR node A and a possible assignment to its unassigned surface form dependency features 1, generate a new DATR node AI&apos;, which inherits from A and contains the feature assignments in 43. The DATR theory for e is then used to produce the set of evaluated and unevaluated constraint sentences that describe it. example, the base lexical entry for defined at the DATR node Arr iv e is underspecified for the paths &lt;syn tense&gt;, &lt;syn head person&gt;, head number&gt;. the assignment of values third, sg to paths, the node created. 4.2 Producing Unevaluated Paths As we have shown, reentrancies can be specified in DATR using global inheritance; see, for example, (15) in Section 3.4.1. However, such sentences may not appear directly in the DAG representation, either because they include paths not derivable within the closure definitions, or because interaction with higher-ranking exceptions may lead to weaker equivalences being derived. Any DATR sentence that does not explicitly introduce a value is treated as a candidate reentrancy constraint; at the stage where constraint sentences are being derived from a DATR theory, all unevaluated constraint are retained. In the case of sthirdsg, following constraint are derived by inheritance from (27) &lt;syn args first sem&gt; = &lt;syn args gr_subject sem&gt; . &lt;sem theagent&gt; = &lt;syn args gr_subject sem&gt; . 260 Francois Andry et al. Making DATR Work for Speech DATR inference takes the form of successive reduction of right-hand sides; in (27), neither sentence is further reducible—both would be ignored by a standard DATR theorem-prover. 13y passing both constraints to the DAG-building procedure however, where equality is reflexive as well as transitive (Section 4.3), the two constraints may be to derive the reentrancy between &lt;sem &lt;syn args sem&gt;. 4.3 DAG Building and Disjunction Optimization constraint sentences derived for a DATR node A or for an extension of it of the form = Value or Pathl = Path2. consistent, they can be used to build a DAG corresponding to A. Our DAG-building procedure is based on one described in Gazdar and Mellish (1989). It builds DAGs by unification of constraints, so that directionality is irrelevant. For this to succeed, the input constraints must not contain inconsistencies. This property of correctness is only partially guaranteed by the constraint-derivation stage, which will tolerate an unevaluated constraint whose left-hand side is a proper prefix of an evaluated one (but not vice versa), as in (28). theagent type&gt; = object. theagent&gt; = &lt;syn args gr_subject . This will work so long as a contradictory type is not derivable elsewhere. The form of DAGs is known as form 1990); that is, if two DAGs share a common sub-DAG, this is explicitly represented in both, with the exception of unevaluated sharing sub-DAGs that are represented as Prolog variables. Once the DAG is built, remaining unwanted paths are filtered out. In the case of amounts to removing those sub-DAGs introduced at paths containing Although the closure definitions ensure that the number of surface form dependency feature assignments for each lexeme is finite, in practice for languages like English where a number of morphosyntactic feature combinations map onto a smaller set of surface forms, the DNF lexicon will have more entries than there are distinct surface forms. In cases where a number of entries differ only in a single feature, a phase optimization to reduce these, according to the simple equivalence: A • • 04 • • • (01 V • • • Apart from this optimization, the lexicon produced is in DNF form. 5. Bit Coding 5.1 Motivation and Requirements The last step toward the production of data structures for efficient parsing and generation is the construction of two separate lexicons: a Prolog term encoding of the DAGs and a compact bit-encoded lexicon. The motivation for two separate lexicons is the decision to split the task of parsing into its two aspects: determining grammaticality and assigning an interpretation. Since in speech recognition there is also the added complication of identifying the best-scoring sequence of words from a lattice of hypotheses, and since an interpretation is only needed for the best sequence, not for every acceptable one, it is more efficient to separate these tasks. This involves separating lexical entries into those features that are constraining (i.e. which affect a sign&apos;s 261 Computational Linguistics Volume 18, Number 3 capacity to combine with others) and those that simply contribute to its eventual interpretation. The former set is used to produce the bit-coded &apos;acceptance&apos; lexicon, the latter to form a term-encoded &apos;full&apos; lexicon. As well as being used in sentence interpretation, the full lexicon is also used in sentence generation. However, we shall concentrate here on the bit-encoded acceptance lexicon. Since the search space when parsing a typical word hypothesis lattice is potentially great, the acceptance lexicon must be both compact and suitable for processing by efficient low-level operations. Bit encoding allows unification of feature structures to be performed by Boolean operations on bit strings, which enables a parser to be implemented in an efficient programming language such as C; it also provides a convenient representation of disjunctions and negations of feature values. Two distinct kinds of bit coding are used to represent semantic types and syntactic head features: both produce vectors of bits that can be stored as integers or lists of integers. 5.2 Semantic Type Coding The principal semantic type of a lexical entry is a node in a tree-structured (singleinheritance) sortal hierarchy. Coding for types in the hierarchy is straightforward: • a terminal node has one unique bit set; • a nonterminal node is represented by the bitwise Boolean OR of the codings for the nodes it dominates. This scheme requires as many bits as there are terminal nodes in the tree and, assuming that every nonterminal node dominates at least two subnodes, assigns a unique bit vector to every node. (A simple example is given in Figure 5). The most specific types are represented by a bit vector containing a single &apos;1,&apos; and the most general by a vector with all its bits set. Unification of two types is performed by bitwise AND; since the hierarchy is tree structured the result of this will be the coding of the more specific type, or 0 indicating failure if the types are incompatible. The same coding scheme would also serve if the hierarchy were extended to a multiple-inheritance graph, the only difference being that bitwise AND could then result in a type distinct from either of its arguments. Syntactic Coding approach to of the feature structures used to represent syntactic categories is very similar to that proposed in Nakazawa et al. (1988) for implementing GPSG-style grammars. A set of features is represented by a bit vector in which for every n-valued feature, + bits are assigned, one associated with each value and one bit indicating that the feature is not present. A value of &apos;0&apos; for a bit means that the feature does not have the corresponding value; a &apos;1&apos; indicates that the value is a possible one. If the value of a feature can be specified precisely, the corresponding bit is set, and all the others for that feature are cleared. Hence the negation of a feature-value pair can be represented by clearing a bit, and a disjunction of values by setting more than one bit in the representation of a feature. This fact can be utilized to pack lexical entries together: if two entries differ only in one atomic-valued feature, they can be combined into a single entry by this method. Unification is again performed by bitwise AND; failure is indicated if all the bits for some feature are turned off, meaning that the structures 262</abstract>
<title confidence="0.844499">Francois Andry et al. Making DATR Work for Speech</title>
<author confidence="0.615029">Type Bit Vector</author>
<note confidence="0.681585666666667">A 1111111 B 1110000 C 0001111 D 1100000 E 0010000 F 0001000</note>
<title confidence="0.638702">Type Bit Vector</title>
<address confidence="0.767859833333333">G 0000110 H 0000001 I 1000000 J 0100000 K 0000100 L 0000010</address>
<abstract confidence="0.996935637931034">Bit coding of the semantic type hierarchy. being unified have no common value for this feature. Since this operation only turns bits off, unification of bit vectors is order-independent (commutative and associative). The bit vector representation is straightforward for encoding flat feature-value structures, but presents difficulties when features have categories as values, given the requirement that the possible values for all features can be enumerated in order to produce bit vectors of finite size. Although a general solution can be proposed that uses some pattern of bits to indicate a recursive feature and associates with this feature another bit vector of the same length (the solution adopted by Nakazawa et al. 1988), have chosen a more hoc specifying in advance which features can be recursive and representing them by pointers to similarly coded structures. The features that are recursive are the list of arguments of a functor sign and the slash feature used to handle long-distance dependencies.&apos; (This approach enables the parser to process 4 We follow GPSG in the use of the category-valued feature slash as a propagating device to handle extraction phenomena. For example in the question &apos;what did you say?&apos;, the phrase &apos;did you say?&apos; can 263 Computational Linguistics Volume 18, Number 3 major v major n major det major * vform fin vform bse vform psp vform prp 1000100001110001 case * case gen case obj case nom tense * tense past tense pres vform * ill L Figure 6 bit vector for head features vform, tense, efficiently, but unfortunately makes it partly dependent on their structure). The bit encoding procedure takes as input a DAG representation of individual lexical entries and is guided in its translation by the closure definitions. A set of declarations is used to indicate which features are to be included in the acceptance lexicon, and how they are to be encoded: using either of the bit representations discussed above, or simply as a list of encodings of their constituents. If no coding type is specified for a feature, then it is simply ignored. As a simple example, consider the following partially specified feature structure: [major : v ] [ : that the closure definitions specify values for the head features vform, the FCR: = major : if the node declared for bit coding, the vector shown in Figure 6 will be produced. (The symbol &apos;&apos; stands for &apos;not present&apos;). Note that bits have been set all values of the unspecified feature that nothing is known about value, but that only the &apos;&apos; bit is set for the feature the FCR blocks its for entries whose is not 5.4 Variable Sharing Although the representation of variables and their instantiation to (more or fully) specified values is straightforward, the implementation of variable sharing or reentrancy presents a serious problem for bit coding schemes, since there is no means of representing identifiable variables. We have adopted a two-fold solution, depending on the type of the variable. For sign-valued variables, and other large scale structures, sharing is achieved by means of pointers to common data objects. This approach cannot be extended down to the level of bit-coded features, since these involve data below the level of the machine word. Instead a solution based on be partially characterized, in our notation, as syn: { head: [ major: y args : 0 slash: [ first: [ syn: [ head: [ major: n ] ] ] ] indicating that it is a sentence from which a noun phrase has been extracted. 264 Francois Andry et al. Making DATR Work for Speech the use of bit masks has been adopted. The key to this is the recognition that variable sharing between structures is a limited form of unification, carried out between a restricted set of their features. If two feature structures represented by bit vectors 0-1 a variable for the feature 0, a mask constructed in which all the representing cleared, and all the rest are set. The values for the two bit vectors are unified in the result of the expression: A V P) Note that a single mask may represent more than one variable shared between two structures. A disadvantage of this technique is that it requires the construction of masks for all possible feature structures within a sign between which variables may be shared. practice assume that this means only the recursively nested signs of the args list and slash, and so need relatively few masks. A description of the two-stage parsing procedure can be found in Andry and Thornton (1991). 6. Implementation and Coverage DIALEX is implemented in Quintus Prolog; benchmark tests indicate that compilation time is linear in the size of the lexicon. Development of very large scale lexicons is somewhat hindered by the current lack of effective debugging tools. We have, however, succeeded in constructing lexicons that cover a broad range of syntactic phenomena in both French and English. For example, the English DATR lexicon covers all distinctive lexical forms in our corpus gathered from simulations of flight enquiry dialogues (Fraser and Gilbert 1991). Furthermore, one of the major advantages of DAT R&apos;s inheritance-based approach is ease of adding new lexical entries. For example, a large number of entries for cities is required in the flight information domain. With the definition of a CITY_PROP node to specify general properties of proper nouns identifying individual cities such as simple and quick to define: (30) Paris: &lt;&gt; == CITY_PROP &lt;mor root&gt; == paris &lt;sem thecity value&gt; == paris . Extending the lexicon to include new verbs, especially verbs with idiosyncratic proplike more time and effort. This paper has been mainly concerned with the definition and compilation of lexicons for understanding. In fact, SUNDIAL applications are such that a production lexicon shares a considerable portion with its recognition counterpart. To this end, DIALEX has been adapted for compilation of a generation lexicon (Youd and McGlashan 1992). This is derived from the same DATR definitions but differs from the parser lexicons in that indexing is based on semantic type and complexity, rather than the surface string, and inflection is factored away from the lexical entries. 7. Conclusion In the design of our lexicon compilation tool, we have shown how linguistic knowledge can be arranged in terms of a set of DATR structured base definitions that are 265 Computational Linguistics Volume 18, Number 3 portable across applications. Knowledge at the levels of morphology, syntax, and semantics combines in a single reusable DATR database. The fact that this knowledge is expressed in a high-level representation language does not limit its usefulness. On the contrary, it allows the designers of base definitions or application lexicons to think clearly about the structural relations that hold between objects in the representation and to maximize generalizations. Default inheritance allows generalizations to trickle down to specific instances, unless overridden. As a consequence, every good generalization captured during the design of structured base definitions represents labor saved during subsequent application-specific work. We have also shown how high-level knowledge can be entered by the lexicon builder at the appropriate conceptual level and then compiled into a lower level form appropriate for a chosen application. The system we describe produces two kinds of output: a term-encoded full lexicon for use in sentence interpretation and generation, and a lower level bit-encoded acceptance lexicon for use in lattice pruning during speech processing. The modular design of our system makes it particularly easy to exchange existing coding modules for new ones, thus allowing linguistic knowledge to be customized for a wide variety of speech or language applications.</abstract>
<note confidence="0.948872428571428">References Andry, Francois, and Thornton, Simon. (1991). &amp;quot;A parser for speech lattices using UCG grammar.&amp;quot; In 2nd European Conference on Speech and Technology. September 1991, 219-222. Bilange, Eric. (1991). &amp;quot;A task independent dialogue model.&amp;quot; In 5th Meeting of the European Chapter of the Association for Computational Linguistics. Berlin, April 1991, 83-88. Bouma, Gosse. (1990). &amp;quot;Defaults in grammar.&amp;quot; In 28th</note>
<title confidence="0.769082">Annual Meeting of the Association for</title>
<author confidence="0.958481">June</author>
<note confidence="0.874675375">1990, 165-172. Calder, Jo; Klein, Ewan; and Zeevat, Henk. (1988). &amp;quot;Unification Categorial Grammar: a consise extendable grammar for natural processing.&amp;quot; In August 1988, 83-86. Charniak, Eugene, and McDermott, Drew.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francois Andry</author>
<author>Simon Thornton</author>
</authors>
<title>A parser for speech lattices using a UCG grammar.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 2nd European Conference on Speech Communication and Technology.</booktitle>
<pages>219--222</pages>
<location>Genova,</location>
<contexts>
<context position="47710" citStr="Andry and Thornton (1991)" startWordPosition="7863" endWordPosition="7866"> cleared, and all the rest are set. The values for q5 in the two bit vectors are unified in the result of the expression: /31 A 632 V P) Note that a single mask may represent more than one variable shared between two structures. A disadvantage of this technique is that it requires the construction of masks for all possible feature structures within a sign between which variables may be shared. In practice we can assume that this means only the recursively nested signs of the args list and slash, and so need relatively few masks. A description of the two-stage parsing procedure can be found in Andry and Thornton (1991). 6. Implementation and Coverage DIALEX is implemented in Quintus Prolog; benchmark tests indicate that compilation time is linear in the size of the lexicon. Development of very large scale lexicons is somewhat hindered by the current lack of effective debugging tools. We have, however, succeeded in constructing lexicons that cover a broad range of syntactic phenomena in both French and English. For example, the English DATR lexicon covers all distinctive lexical forms in our corpus gathered from simulations of flight enquiry dialogues (Fraser and Gilbert 1991). Furthermore, one of the major </context>
</contexts>
<marker>Andry, Thornton, 1991</marker>
<rawString>Andry, Francois, and Thornton, Simon. (1991). &amp;quot;A parser for speech lattices using a UCG grammar.&amp;quot; In Proceedings, 2nd European Conference on Speech Communication and Technology. Genova, September 1991, 219-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bilange</author>
</authors>
<title>A task independent oral dialogue model.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 5th Meeting of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>83--88</pages>
<location>Berlin,</location>
<contexts>
<context position="3958" citStr="Bilange 1991" startWordPosition="586" endWordPosition="587">iately for the purposes of text generation. 3. Each system must run in real time or near real time. Therefore the linguistic knowledge must be structured so as to allow rapid access and manipulation. 4. Portability to new applications should be simple; work required to write new linguistic knowledge bases should therefore be kept to a minimum. 5. Duplication of effort must be avoided. This must be true in respect of the components of each separate prototype system. For example, the same dialog manager software module has been used in each prototype with minor customizations for each language (Bilange 1991; McGlashan et al. 1992). The same principle should apply to the design of tools for the construction of knowledge bases, including lexical knowledge bases. Thus, the task of adding a new lexical item should only require the addition of knowledge that is idiosyncratic to that lexical item and not predictable from what is already present in the knowledge base. Section 2 of this paper presents an overview of the SUNDIAL DIALEX tool. Section 3 describes the way in which linguistic knowledge is initially expressed in terms of declarative DAT R theories. Section 4 explains how a compact DATR knowle</context>
</contexts>
<marker>Bilange, 1991</marker>
<rawString>Bilange, Eric. (1991). &amp;quot;A task independent oral dialogue model.&amp;quot; In Proceedings, 5th Meeting of the European Chapter of the Association for Computational Linguistics. Berlin, April 1991, 83-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
</authors>
<title>Defaults in unification grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>165--172</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="37474" citStr="Bouma 1990" startWordPosition="6128" endWordPosition="6129">It builds DAGs by unification of constraints, so that directionality is irrelevant. For this to succeed, the input constraints must not contain inconsistencies. This property of correctness is only partially guaranteed by the constraint-derivation stage, which will tolerate an unevaluated constraint whose left-hand side is a proper prefix of an evaluated one (but not vice versa), as in (28). (28) &lt;sem theagent type&gt; = object. &lt;sem theagent&gt; = &lt;syn args gr_subject sem&gt; . This will work so long as a contradictory type is not derivable elsewhere. The form of encoded DAGs is known as normal form (Bouma 1990); that is, if two DAGs share a common sub-DAG, this is explicitly represented in both, with the exception of unevaluated sharing sub-DAGs that are represented as Prolog variables. Once the DAG is built, any remaining unwanted paths are filtered out. In the case of Arrivel_presthirdsg, this amounts to removing those sub-DAGs introduced at paths containing gr_subje ct and gr_obliquet Although the closure definitions ensure that the number of surface form dependency feature assignments for each lexeme is finite, in practice for languages like English where a number of morphosyntactic feature comb</context>
</contexts>
<marker>Bouma, 1990</marker>
<rawString>Bouma, Gosse. (1990). &amp;quot;Defaults in unification grammar.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, June 1990, 165-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo Calder</author>
<author>Ewan Klein</author>
<author>Henk Zeevat</author>
</authors>
<title>Unification Categorial Grammar: a consise extendable grammar for natural language processing.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, COLING-88.</booktitle>
<pages>83--86</pages>
<location>Budapest,</location>
<contexts>
<context position="5044" citStr="Calder et al. 1988" startWordPosition="759" endWordPosition="762">hich linguistic knowledge is initially expressed in terms of declarative DAT R theories. Section 4 explains how a compact DATR knowledge base is expanded out into a fully specified lexicon. Section 5 relates how the lexicon can be customized for the purposes of real-time speech parsing. Practical experiences of constructing and using DIALEX are recounted in Section 6. Concluding observations are drawn in Section 7. 2. Overview of the System In common with contemporary generative theories that are unification based and for which information is concentrated in the lexicon (Pollard and Sag 1987; Calder et al. 1988), we adopt the sign as our basic unit of linguistic representation. For a given lexical entry, a sign describes the constraints—morphological, syntactic, and semantic—it 246 Francois Andry et al. Making DATR Work for Speech introduces. The sign for intransitive arrives, for example, is: mor: root : arrive 1 form : arrives .1 major: v vform : fin head : tense : pres person : third number : sg syn: sem : [type : arrive thetime : [ type : time I RestT 1 theagent : [ type : object I RestA ] args: ( syn: head: sem : type: [opt order: dir adj _ - syn: sem : type : opt order: [dir adj head r major: p</context>
<context position="13113" citStr="Calder et al. 1988" startWordPosition="2090" endWordPosition="2093">e existed. The form shown in (2d) assigns to Nodel:Path1 whatever value is found at Node2:Path2. A special case of this is (2e), which allows extensions of Path1 to be specified at Node2. For example, evaluating the DAT R theory in (3) yields the theorems for node Ex2 shown in (4). (3) Ex1: &lt;head major&gt; == n &lt;head case&gt; == nom. Ex2: &lt;syn&gt; == Exl: &lt;&gt; . (4) Ex2: &lt;syn head major&gt; = n. Ex2: &lt;syn head case&gt; = nom. For a more detailed description of DAT R see Evans and Gazdar (1990). 3.2 The Linguistic Framework Linguistic knowledge is structured in terms of a simple unification categorial grammar (Calder et al. 1988) in which featural constraints at the levels of morphology, syntax, and semantics may all occur in a lexical sign. The basic sign structure of lexical entries is shown in (5). morphology: [. 1 (5) syntax : [. . semantics : [. .1 The basic sign structure of the syntax feature value is shown in (6). (6) syntax: head : [. . 1 args : [.. .] j The head feature includes attribute-value structures for such things as tense, person, number, and definiteness. The args feature is stack-valued, with stack position determining the order in which arguments may be combined by functional application. The basi</context>
</contexts>
<marker>Calder, Klein, Zeevat, 1988</marker>
<rawString>Calder, Jo; Klein, Ewan; and Zeevat, Henk. (1988). &amp;quot;Unification Categorial Grammar: a consise extendable grammar for natural language processing.&amp;quot; In Proceedings, COLING-88. Budapest, August 1988, 83-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Drew McDermott</author>
</authors>
<title>An Introduction to Artificial Intelligence. Lawrence Erlbaum Associates.</title>
<date>1985</date>
<contexts>
<context position="6861" citStr="Charniak and McDermott 1985" startWordPosition="1079" endWordPosition="1082">s constrained to be third person singular nominative (i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the final version of the lexicon. 2 In our representation of feature structures we follow Prolog conventions, whereby variables are identified by initial capitals, and a vertical bar introduces</context>
</contexts>
<marker>Charniak, McDermott, 1985</marker>
<rawString>Charniak, Eugene, and McDermott, Drew. (1985). An Introduction to Artificial Intelligence. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>Inference in DATR.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 4th Meeting of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>66--71</pages>
<location>Manchester,</location>
<contexts>
<context position="6732" citStr="Evans and Gazdar 1989" startWordPosition="1061" endWordPosition="1064">e followed by a prepositional phrase whose semantics will fill the semantic role thetime.2 The argument preceding the verb is constrained to be third person singular nominative (i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the final version of the lexicon. 2 In our representation of fea</context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger, and Gazdar, Gerald. (1989). &amp;quot;Inference in DATR.&amp;quot; In Proceedings, 4th Meeting of the European Chapter of the Association for Computational Linguistics. Manchester, April 1989, 66-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
<author>eds</author>
</authors>
<date>1990</date>
<booktitle>The DATR Papers. Research Report CSRP 139, School of Cognitive and</booktitle>
<institution>Computing Science, University of Sussex.</institution>
<marker>Evans, Gazdar, eds, 1990</marker>
<rawString>Evans, Roger, and Gazdar, Gerald, eds. (1990). The DATR Papers. Research Report CSRP 139, School of Cognitive and Computing Science, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel P Flickinger</author>
<author>Carl J Pollard</author>
<author>Thomas Wasow</author>
</authors>
<title>Structure-sharing in lexical representation.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>262--267</pages>
<location>Chicago,</location>
<contexts>
<context position="6885" citStr="Flickinger et al. 1985" startWordPosition="1083" endWordPosition="1086">son singular nominative (i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the final version of the lexicon. 2 In our representation of feature structures we follow Prolog conventions, whereby variables are identified by initial capitals, and a vertical bar introduces the tail of a list. 247</context>
</contexts>
<marker>Flickinger, Pollard, Wasow, 1985</marker>
<rawString>Flickinger, Daniel P.; Pollard, Carl J.; and Wasow, Thomas. (1985). &amp;quot;Structure-sharing in lexical representation.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics. Chicago, 262-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman M Fraser</author>
<author>G Nigel Gilbert</author>
</authors>
<title>Effects of system voice quality on user utterances in speech dialogue systems.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 2nd European Conference on Speech Communication and Technology.</booktitle>
<pages>57--60</pages>
<location>Genova,</location>
<contexts>
<context position="48278" citStr="Fraser and Gilbert 1991" startWordPosition="7952" endWordPosition="7955">sing procedure can be found in Andry and Thornton (1991). 6. Implementation and Coverage DIALEX is implemented in Quintus Prolog; benchmark tests indicate that compilation time is linear in the size of the lexicon. Development of very large scale lexicons is somewhat hindered by the current lack of effective debugging tools. We have, however, succeeded in constructing lexicons that cover a broad range of syntactic phenomena in both French and English. For example, the English DATR lexicon covers all distinctive lexical forms in our corpus gathered from simulations of flight enquiry dialogues (Fraser and Gilbert 1991). Furthermore, one of the major advantages of DAT R&apos;s inheritance-based approach is ease of adding new lexical entries. For example, a large number of entries for cities is required in the flight information domain. With the definition of a CITY_PROP node to specify general properties of proper nouns identifying cities, individual cities such as Paris are simple and quick to define: (30) Paris: &lt;&gt; == CITY_PROP &lt;mor root&gt; == paris &lt;sem thecity value&gt; == paris . Extending the lexicon to include new verbs, especially verbs with idiosyncratic properties like try, takes more time and effort. This p</context>
</contexts>
<marker>Fraser, Gilbert, 1991</marker>
<rawString>Fraser, Norman M., and Gilbert, G. Nigel. (1991). &amp;quot;Effects of system voice quality on user utterances in speech dialogue systems.&amp;quot; In Proceedings, 2nd European Conference on Speech Communication and Technology. Genova, September 1991, 57-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Linguistic applications of default inheritance mechanisms.&amp;quot; In Linguistic Theory and Computer Applications, edited by Peter Whitelock; Harold Somers; Rod Johnson; and Mary McGee Wood.</title>
<date>1987</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="6674" citStr="Gazdar 1987" startWordPosition="1053" endWordPosition="1054">e. The sign shows that the verb may optionally be followed by a prepositional phrase whose semantics will fill the semantic role thetime.2 The argument preceding the verb is constrained to be third person singular nominative (i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the fi</context>
</contexts>
<marker>Gazdar, 1987</marker>
<rawString>Gazdar, Gerald. (1987). &amp;quot;Linguistic applications of default inheritance mechanisms.&amp;quot; In Linguistic Theory and Computer Applications, edited by Peter Whitelock; Harold Somers; Rod Johnson; and Mary McGee Wood. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>An introduction to DATR.&amp;quot; In The DATR Papers, edited by Roger Evans and Gerald Gazdar.</title>
<date>1990</date>
<tech>Research Report CSRP 139,</tech>
<pages>1--14</pages>
<institution>School of Cognitive and Computing Science, University of Sussex:</institution>
<contexts>
<context position="6818" citStr="Gazdar 1990" startWordPosition="1076" endWordPosition="1077">rgument preceding the verb is constrained to be third person singular nominative (i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the final version of the lexicon. 2 In our representation of feature structures we follow Prolog conventions, whereby variables are identified by init</context>
<context position="12975" citStr="Gazdar (1990)" startWordPosition="2072" endWordPosition="2073">would be evaluated locally 249 Computational Linguistics Volume 18, Number 3 and assigned the value of Path2 at Node1 if such a value existed. The form shown in (2d) assigns to Nodel:Path1 whatever value is found at Node2:Path2. A special case of this is (2e), which allows extensions of Path1 to be specified at Node2. For example, evaluating the DAT R theory in (3) yields the theorems for node Ex2 shown in (4). (3) Ex1: &lt;head major&gt; == n &lt;head case&gt; == nom. Ex2: &lt;syn&gt; == Exl: &lt;&gt; . (4) Ex2: &lt;syn head major&gt; = n. Ex2: &lt;syn head case&gt; = nom. For a more detailed description of DAT R see Evans and Gazdar (1990). 3.2 The Linguistic Framework Linguistic knowledge is structured in terms of a simple unification categorial grammar (Calder et al. 1988) in which featural constraints at the levels of morphology, syntax, and semantics may all occur in a lexical sign. The basic sign structure of lexical entries is shown in (5). morphology: [. 1 (5) syntax : [. . semantics : [. .1 The basic sign structure of the syntax feature value is shown in (6). (6) syntax: head : [. . 1 args : [.. .] j The head feature includes attribute-value structures for such things as tense, person, number, and definiteness. The args</context>
</contexts>
<marker>Gazdar, 1990</marker>
<rawString>Gazdar, Gerald. (1990). &amp;quot;An introduction to DATR.&amp;quot; In The DATR Papers, edited by Roger Evans and Gerald Gazdar. Research Report CSRP 139, School of Cognitive and Computing Science, University of Sussex: 1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffry Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="34001" citStr="Gazdar et al. 1985" startWordPosition="5564" endWordPosition="5567">ing functional labels such as gr_subject are no longer needed; 3. they include a complete set of knowledge definitions for our semantic representation language (SIL), which is inheritance based. The inheritance hierarchy for semantic types, for example, is used in bit coding (Section 5), so that semantic selectional restrictions can be tested during parsing; 4. they furnish a set of declarative templates against which bit coding and DAG-term conversion may be carried out. In addition to an enumeration of feature values, the closure definitions contain Feature Cooccurrence Restrictions (FCRs) (Gazdar et al. 1985). In principle these could be encoded in the DATR lexicon, for example, using the feature-value unspec to represent negative occurrence. Their presence here is not only to restrict the possible feature combinations that can appear at the head of a sign, but also to detect dependencies that govern DNF expansion. The DNF lexicon is obtained as follows. Those features on which the surface form of a full lexical sign depend, which we shall refer to as its surface form dependency features, may be derived from the FCRs contained in the closure definitions. Then for each pair consisting of a DATR nod</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffry; and Sag, Ivan. (1985). Generalized Phrase Structure Grammar. Harvard University Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<title>Natural Language Processing in Prolog.</title>
<date>1989</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="36861" citStr="Gazdar and Mellish (1989)" startWordPosition="6028" endWordPosition="6031">le—both would be ignored by a standard DATR theorem-prover. 13y passing both constraints to the DAG-building procedure however, where equality is reflexive as well as transitive (Section 4.3), the two constraints may be combined to derive the reentrancy between &lt;sem theagent&gt; and &lt;syn args first sem&gt;. 4.3 DAG Building and Disjunction Optimization The constraint sentences derived for a DATR node A or for an extension of it A4&apos; are of the form Path = Value or Pathl = Path2. If consistent, they can be used to build a DAG corresponding to A. Our DAG-building procedure is based on one described in Gazdar and Mellish (1989). It builds DAGs by unification of constraints, so that directionality is irrelevant. For this to succeed, the input constraints must not contain inconsistencies. This property of correctness is only partially guaranteed by the constraint-derivation stage, which will tolerate an unevaluated constraint whose left-hand side is a proper prefix of an evaluated one (but not vice versa), as in (28). (28) &lt;sem theagent type&gt; = object. &lt;sem theagent&gt; = &lt;syn args gr_subject sem&gt; . This will work so long as a contradictory type is not derivable elsewhere. The form of encoded DAGs is known as normal form</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald, and Mellish, Chris. (1989). Natural Language Processing in Prolog. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Basil Blackwell.</publisher>
<contexts>
<context position="6899" citStr="Hudson 1990" startWordPosition="1087" endWordPosition="1088">(i.e. not object-marked), and supplies the filler for the semantic role theagent. In the interests of linguistic parsimony and sensible knowledge engineering, it is necessary for lexicalist approaches to factor away at the lexicon-encoding interface as many as possible of the commonalities between lexical items. To this end, we adopt the principles of default inheritance (Gazdar 1987), as embodied in the DATR language (Evans and Gazdar 1989). Areas where abstractions may be made over the lexicon are morphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickinger et al. 1985; Hudson 1990), and combinations of these leading to lexical rules such as passive. To this we have added the area of lexico-semantic relations. In order to generalize over semantic roles, it is necessary to tie these to functional-syntactic roles, such as subject, direct object, etc. These in turn are related to order marked arguments in the args frame. Only the latter appear in the final version of the lexicon. 2 In our representation of feature structures we follow Prolog conventions, whereby variables are identified by initial capitals, and a vertical bar introduces the tail of a list. 247 Computational</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Hudson, Richard A. (1990). English Word Grammar. Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McGlashan</author>
<author>Norman M Fraser</author>
<author>G Nigel Gilbert</author>
<author>Eric Bilange</author>
<author>Paul Heisterkamp</author>
<author>Nick J Youd</author>
</authors>
<title>Dialogue management for telephone information systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing.</booktitle>
<pages>245--246</pages>
<location>Trento,</location>
<contexts>
<context position="3982" citStr="McGlashan et al. 1992" startWordPosition="588" endWordPosition="591"> purposes of text generation. 3. Each system must run in real time or near real time. Therefore the linguistic knowledge must be structured so as to allow rapid access and manipulation. 4. Portability to new applications should be simple; work required to write new linguistic knowledge bases should therefore be kept to a minimum. 5. Duplication of effort must be avoided. This must be true in respect of the components of each separate prototype system. For example, the same dialog manager software module has been used in each prototype with minor customizations for each language (Bilange 1991; McGlashan et al. 1992). The same principle should apply to the design of tools for the construction of knowledge bases, including lexical knowledge bases. Thus, the task of adding a new lexical item should only require the addition of knowledge that is idiosyncratic to that lexical item and not predictable from what is already present in the knowledge base. Section 2 of this paper presents an overview of the SUNDIAL DIALEX tool. Section 3 describes the way in which linguistic knowledge is initially expressed in terms of declarative DAT R theories. Section 4 explains how a compact DATR knowledge base is expanded out</context>
</contexts>
<marker>McGlashan, Fraser, Gilbert, Bilange, Heisterkamp, Youd, 1992</marker>
<rawString>McGlashan, Scott; Fraser, Norman M.; Gilbert, G. Nigel; Bilange, Eric; Heisterkamp, Paul; and Youd, Nick J. (1992). Dialogue management for telephone information systems. In Proceedings of the 3rd Conference on Applied Natural Language Processing. Trento, April: 245-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuneko Nakazawa</author>
<author>Laura Neher</author>
<author>Erhard W Hinrichs</author>
</authors>
<title>Unification with disjunctive and negative values for GPSG grammars.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 8th European Conference on Artificial Intelligence.</booktitle>
<location>Munich,</location>
<contexts>
<context position="41770" citStr="Nakazawa et al. (1988)" startWordPosition="6837" endWordPosition="6840">ts set. Unification of two types is performed by bitwise AND; since the hierarchy is tree structured the result of this will be the coding of the more specific type, or 0 indicating failure if the types are incompatible. The same coding scheme would also serve if the hierarchy were extended to a multiple-inheritance graph, the only difference being that bitwise AND could then result in a type distinct from either of its arguments. 5.3 Syntactic Feature-Value Coding Our approach to the encoding of the feature structures used to represent syntactic categories is very similar to that proposed in Nakazawa et al. (1988) for implementing GPSG-style grammars. A set of features is represented by a bit vector in which for every n-valued feature, n + 1 bits are assigned, one associated with each value and one bit indicating that the feature is not present. A value of &apos;0&apos; for a bit means that the feature does not have the corresponding value; a &apos;1&apos; indicates that the value is a possible one. If the value of a feature can be specified precisely, the corresponding bit is set, and all the others for that feature are cleared. Hence the negation of a feature-value pair can be represented by clearing a bit, and a disjun</context>
<context position="43716" citStr="Nakazawa et al. 1988" startWordPosition="7168" endWordPosition="7171">ration only turns bits off, unification of bit vectors is order-independent (commutative and associative). The bit vector representation is straightforward for encoding flat feature-value structures, but presents difficulties when features have categories as values, given the requirement that the possible values for all features can be enumerated in order to produce bit vectors of finite size. Although a general solution can be proposed that uses some pattern of bits to indicate a recursive feature and associates with this feature another bit vector of the same length (the solution adopted by Nakazawa et al. 1988), we have chosen a more ad hoc encoding, specifying in advance which features can be recursive and representing them by pointers to similarly coded structures. The features that are recursive are the list of arguments of a functor sign and the slash feature used to handle long-distance dependencies.&apos; (This approach enables the parser to process 4 We follow GPSG in the use of the category-valued feature slash as a propagating device to handle extraction phenomena. For example in the question &apos;what did you say?&apos;, the phrase &apos;did you say?&apos; can 263 Computational Linguistics Volume 18, Number 3 maj</context>
</contexts>
<marker>Nakazawa, Neher, Hinrichs, 1988</marker>
<rawString>Nakazawa, Tsuneko; Neher, Laura; and Hinrichs, Erhard W. (1988). &amp;quot;Unification with disjunctive and negative values for GPSG grammars.&amp;quot; In Proceedings, 8th European Conference on Artificial Intelligence. Munich, August 1990,467-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Peckham</author>
</authors>
<title>Speech understanding and dialogue over the telephone: an overview of the SUNDIAL project.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 2nd European Conference on Speech Communication and Technology.</booktitle>
<pages>1469--1472</pages>
<location>Genova,</location>
<contexts>
<context position="1444" citStr="Peckham 1991" startWordPosition="206" endWordPosition="207">ich is used for assigning interpretations or for generating messages). 1. Introduction In this paper we describe DIALEX, a modular inheritance-based tool for the construction of lexicalized grammar knowledge bases. DIALEX has been developed as part of the SUNDIAL (Speech UNderstanding and DIALogue) project—currently one of Europe&apos;s largest collaborative research projects in speech and language technology.&apos; SUNDIAL&apos;s main project goal is to produce four prototype systems that support relatively unconstrained telephone dialogs for limited domains in each of English, French, German, and Italian (Peckham 1991). This paper reports work carried out in the development of the English and French systems. These share a common application domain, namely flight enquiries and reservations. * Cap Gemini Innovation, 118 Rue de Tocqueville, 75017 Paris, France (andry@capsogeti.fr). t Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, U.K. (norman@soc.surrey.ac.uk; scott@soc.surrey.ac.uk). Logica Cambridge Ltd, Betjeman House, 104 Hills Road, Cambridge, CB2 1LQ, U.K. (simont@logcam.co.uk; nick@logcam.co.uk). 1 The work reported here was supported in part by the Commis</context>
</contexts>
<marker>Peckham, 1991</marker>
<rawString>Peckham, Jeremy. (1991). &amp;quot;Speech understanding and dialogue over the telephone: an overview of the SUNDIAL project.&amp;quot; In Proceedings, 2nd European Conference on Speech Communication and Technology. Genova, September 1991, 1469-1472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Information-Based Syntax and Semantics.</title>
<date>1987</date>
<location>CSLI, Stanford, CA.</location>
<contexts>
<context position="5023" citStr="Pollard and Sag 1987" startWordPosition="755" endWordPosition="758">describes the way in which linguistic knowledge is initially expressed in terms of declarative DAT R theories. Section 4 explains how a compact DATR knowledge base is expanded out into a fully specified lexicon. Section 5 relates how the lexicon can be customized for the purposes of real-time speech parsing. Practical experiences of constructing and using DIALEX are recounted in Section 6. Concluding observations are drawn in Section 7. 2. Overview of the System In common with contemporary generative theories that are unification based and for which information is concentrated in the lexicon (Pollard and Sag 1987; Calder et al. 1988), we adopt the sign as our basic unit of linguistic representation. For a given lexical entry, a sign describes the constraints—morphological, syntactic, and semantic—it 246 Francois Andry et al. Making DATR Work for Speech introduces. The sign for intransitive arrives, for example, is: mor: root : arrive 1 form : arrives .1 major: v vform : fin head : tense : pres person : third number : sg syn: sem : [type : arrive thetime : [ type : time I RestT 1 theagent : [ type : object I RestA ] args: ( syn: head: sem : type: [opt order: dir adj _ - syn: sem : type : opt order: [di</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl, and Sag, Ivan A. (1987). Information-Based Syntax and Semantics. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<location>CSLI, Stanford, CA.</location>
<contexts>
<context position="11340" citStr="Shieber 1986" startWordPosition="1776" endWordPosition="1777">it coding (Section 5). 3. Encoding Linguistic Knowledge 3.1 DATR DATR is a declarative language for representing inheritance networks that support multiple default inheritance. Knowledge is expressed in DATR in terms of path equa248 Francois Andry et al. Making DATR Work for Speech F LEXICON application structured BUILDER specific base lexicon definitions DATR lexicon lexicon DATR generation closure definitions bit term coding coding acceptance lexicon full lexicon Figure 1 DIALEX lexicon compilation architecture. tions. The syntax of paths is a superset of that found in the PATR-II language (Shieber 1986). For example, (1) identifies two different paths in the DAG rooted at Nodel in an inheritance network. (1) Nodel: &lt;syn head case&gt; Nodel: &lt;syn head number&gt; The path equations we present in this paper take the following forms: (2) a. Nodel: &lt;&gt; == Node2 b. Nodel: Path1 == Valuel c. Nodel: Pathl == &amp;quot;Path2&amp;quot; d. Nodel : Path1 == Node2:Path2 e. Nodel: Path1 == Node2:&lt;&gt; The form shown in (2a) is the special case in which the path at Nodel is empty. This allows Node1 to inherit all equations available at Node2, except those incompatible with equations at Node1. Two equations are incompatible if they bo</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M. (1986). An Introduction to Unification-Based Approaches to Grammar. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick J Youd</author>
<author>Scott McGlashan</author>
</authors>
<title>Generating utterances in dialogue systems.</title>
<date>1992</date>
<booktitle>In Aspects of Automated Natural Language Generation: Proceedings of the 6th International Workshop on Natural Language Generation,</booktitle>
<publisher>Academic Press,</publisher>
<location>London.</location>
<note>edited by</note>
<contexts>
<context position="49207" citStr="Youd and McGlashan 1992" startWordPosition="8100" endWordPosition="8103">ntifying cities, individual cities such as Paris are simple and quick to define: (30) Paris: &lt;&gt; == CITY_PROP &lt;mor root&gt; == paris &lt;sem thecity value&gt; == paris . Extending the lexicon to include new verbs, especially verbs with idiosyncratic properties like try, takes more time and effort. This paper has been mainly concerned with the definition and compilation of lexicons for understanding. In fact, SUNDIAL applications are such that a production lexicon shares a considerable portion with its recognition counterpart. To this end, DIALEX has been adapted for compilation of a generation lexicon (Youd and McGlashan 1992). This is derived from the same DATR definitions but differs from the parser lexicons in that indexing is based on semantic type and complexity, rather than the surface string, and inflection is factored away from the lexical entries. 7. Conclusion In the design of our lexicon compilation tool, we have shown how linguistic knowledge can be arranged in terms of a set of DATR structured base definitions that are 265 Computational Linguistics Volume 18, Number 3 portable across applications. Knowledge at the levels of morphology, syntax, and semantics combines in a single reusable DATR database. </context>
</contexts>
<marker>Youd, McGlashan, 1992</marker>
<rawString>Youd, Nick J.; and McGlashan, Scott. (1992). Generating utterances in dialogue systems. In Aspects of Automated Natural Language Generation: Proceedings of the 6th International Workshop on Natural Language Generation, edited by Robert Dale; Eduard Hovy; Dietmar Rosner; and Olivero Stock. Academic Press, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>