<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000754">
<title confidence="0.998456">
A Cascaded Classification Approach to Semantic Head Recognition
</title>
<author confidence="0.9535265">
Lukas Michelbacher Alok Kothari Martin Forst†
Christina Lioma Hinrich Sch¨utze
</author>
<affiliation confidence="0.9921965">
Institute for NLP
University of Stuttgart
</affiliation>
<email confidence="0.891547">
{michells,kotharak,liomaca}@ims.uni-stuttgart.de
</email>
<affiliation confidence="0.317927">
†Microsoft
</affiliation>
<email confidence="0.994013">
martin.forst@microsoft.com
</email>
<sectionHeader confidence="0.993733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999314294117647">
Most NLP systems use tokenization as part
of preprocessing. Generally, tokenizers are
based on simple heuristics and do not recog-
nize multi-word units (MWUs) like hot dog
or black hole unless a precompiled list of
MWUs is available. In this paper, we propose
a new cascaded model for detecting MWUs
of arbitrary length for tokenization, focusing
on noun phrases in the physics domain. We
adopt a classification approach because – un-
like other work on MWUs – tokenization re-
quires a completely automatic approach. We
achieve an accuracy of 68% for recognizing
non-compositional MWUs and show that our
MWU recognizer improves retrieval perfor-
mance when used as part of an information re-
trieval system.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999904132075472">
Most NLP systems use tokenization as part of pre-
processing. Generally, tokenizers are based on sim-
ple heuristics and do not recognize multi-word units
(MWUs) like hot dog or black hole. Our long-term
goal is to build MWU-aware tokenizers that are used
as part of the standard toolkit for NLP preprocessing
alongside part-of-speech and named-entity tagging.
We define an MWU as a sequence of words that
has properties that cannot be inferred from the com-
ponent words (cf. e.g. Manning and Sch¨utze (1999,
Ch. 5), Sag et al. (2002)). The most important
of these properties is non-compositionality, the fact
that the meaning of a phrase cannot be predicted
from the meanings of its component words. For ex-
ample, a hot dog is not a hot animal but a sausage in
a bun and a black hole in astrophysics is a region of
space with special properties, not a dark cavity.
The correct recognition of MWUs is an important
building block of many NLP tasks. For example, in
information retrieval (IR) the query hot dog should
not retrieve documents that only contain the words
hot and dog individually, outside of the phrase hot
dog.
In this study, we focus on noun phrases in the
physics domain. For specialized domains such as
physics, adaptable and reliable MWU recognition
is of particular importance because comprehensive
and up-to-date lists of MWUs are not available
and would have to be created by hand. We chose
noun phrases because domain-specific terminology
is commonly encoded in noun phrase MWUs; other
types of phrases – e.g., verb constructions – rarely
give rise to fixed domain-specific multi-word se-
quences that should be treated as a unit.
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. The importance of
syntactic heads for many NLP tasks is generally ac-
cepted. For example, in coreference resolution iden-
tity of syntactic heads is predictive of coreference;
in parse disambiguation, the syntactic head of a noun
phrase is a powerful feature for resolving attachment
ambiguities. However, in all of these cases, the syn-
tactic head is only an approximation of the informa-
tion that is really needed; the underlying assumption
made when using the syntactic head as a substitute
for the entire phrase is that the syntactic head is rep-
resentative of the phrase. This is not the case when
the phrase is non-compositional.
We define the semantic head of a noun phrase as
the non-compositional part of a phrase. Semantic
heads would serve most NLP tasks better than syn-
tactic heads. For example, a coreference resolution
system is misled if it looks at syntactic heads to de-
</bodyText>
<page confidence="0.982581">
793
</page>
<note confidence="0.9579875">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 793–803,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997055074468085">
termine possible coreference of a hot dog ... the dog and MWUs is to apply statistical association mea-
in I first ate a hot dog and then fed the dog. This is sures (AMs) to n-grams extracted from a corpus
not the case for a system that makes the decision – often combined with various linguistic heuris-
based on the semantic heads hot dog of a hot dog tics and other filters, resulting in candidate lists.
and dog of the dog. Choueka (1988) and the XTRACT system (Smadja,
The specific NLP application we evaluate in this 1993) are well-known examples of this approach.
paper is information retrieval. We will show that se- More recent approaches such as Pecina (2010)
mantic head recognition improves the performance and Ramisch et al. (2010) combine classifiers with
of an information retrieval system. association measures. Although our approach is
We introduce a cascaded classification framework classification-based as well, our data set has a more
for recognizing semantic heads that allows us to treat realistic size than Pecina (2010)’s (1 billion words
noun phrases of arbitrary length. We use a number vs 1.5 million words) and we work on noun phrases
of previously proposed features for recognizing non- of arbitrary length (instead of just bigrams). The
compositionality and semantic heads. In addition, mwetoolkit1 by Ramisch et al. (2010) aims to
we compare three features that measure contextual be a software package for lexicographers and its
similarity. features are limited to a small set of association
Our main contributions in this paper are as fol- measures that do not consider marginal frequencies.
lows. First, we introduce the notion of semantic Neither of these two studies includes evaluation in
head, in analogy to syntactic head, and propose se- the context of an application.
mantic head recognition as a new component of NLP Lin (1999) defines a decision criterion for non-
preprocessing. Second, we develop a cascaded clas- compositional phrases based on the change in the
sification framework for semantic head recognition. mutual information of a phrase when substituting
Third, we investigate the utility of contextual simi- one word for a similar one based on an automatically
larity for detecting non-compositionality and show constructed thesaurus. The method reaches 15.7%
that it significantly enhances a baseline semantic precision and 13.7% recall.
head recognizer. However, we also identify a num- In terms of the extraction of domain-specific
ber of challenges of using contextual similarity in MWUs, cross-language methods have been pro-
high-confidence semantic head recognition. Fourth, posed that make use of the fact that an MWU in one
we show that our approach to semantic head recog- language might be expressed as a single word in an-
nition improves the performance of an IR system. other. Caseli et al. (2009) utilize word alignments
Section 2 discusses previous work. In Section 3 in a parallel corpus; Attia et al. (2010) exploit the
we introduce semantic heads and present our cas- links between article names of different-language
caded model for semantic head recognition. In Sec- Wikipedias to search for many-to-one translations.
tion 4, we describe our data and three different mea- We did not pursue a cross-language approach be-
sures of contextual similarity. Section 5 introduces cause we strive for a self-contained method of MWU
the classifier and its features. Section 6 presents recognition that operates on a single textual re-
classification results and discussion. Section 7 de- source.
scribes the information retrieval experiments. In Non-compositionality and distributional se-
Section 8 we present our conclusions. mantics. In recent years, a number of studies have
2 Related Work investigated the relationship between distributional
While there is a large number of publications on semantics and non-compositionality. These studies
MWUs and collocation extraction, the general prob- compute the similarity between words and phrases
lem of automatic MWU detection for the specific represented as semantic vectors in a word space
purpose of tokenization has not been investigated model. A semantic vector of a word is the accumu-
before to our knowledge. lation of the particular contexts in which the word
The classic approach to identifying collocations
794
1http://sourceforge.net/projects/
mwetoolkit/
appears. The underlying idea is similar to Lin’s:
the meaning of a non-compositional phrase some-
how deviates from what one would expect given the
semantic vectors of parts of the phrase. The stan-
dard measure to compare semantic vectors is cosine
similarity. The questions that arise are (i) which
vectors to compare, (ii) how to combine the vectors
of the parts and (iii) from what point on a certain
dissimilarity indicates non-compositionality. To our
knowledge, there are no generally accepted answers
to these questions.
Regarding (i), Schone and Jurafsky (2001) com-
pare the semantic vector of a phrase p and the vec-
tors of its component words in two ways: one in-
cludes the contexts of p in the construction of the
semantic vectors of the parts and one does not. Re-
garding (ii), they suggest weighted or unweighted
sums of the semantic vectors of the parts.
Baldwin et al. (2003) investigate semantic decom-
posability of noun-noun compounds and verb con-
structions. They address (i) by comparing the se-
mantic vectors of phrases with the vectors of their
parts individually to detect meaning changes; e.g.,
they compare vice president to vice and president.
We propose a new method that compares phrases
with their alternative phrases, in the spirit of Lin
(1999)’s substitution approach (see Section 4.3).
Our rationale is that context features should be
based on contexts that are syntactically similar to the
phrase in question.
With respect to (iii), the above-mentioned studies
use ad hoc thresholds to separate compositional and
non-compositional phrases but do not offer a princi-
pled decision criterion.2 In contrast, we train a sta-
tistical classifier to learn a decision criterion.
There is a larger body of work concerning non-
compositionality which revolves around the prob-
lem of literal (compositional) vs. non-literal (non-
compositional) usage of idiomatic verb construc-
tions like to break the ice or to spill the beans.
Some studies approach the problem with semantic
vector comparisons in the style of Schone and Ju-
rafsky (2001), e.g Katz and Giesbrecht (2006) and
Cook et al. (2007). Other approaches use word-
alignment (e.g. Moir´on and Tiedemann (2006)) or
</bodyText>
<footnote confidence="0.8852335">
2Lin (1999) uses a well-defined criterion but his approach is
not based on vector similarity.
</footnote>
<bodyText confidence="0.9996415">
a combination of heuristic and linguistic features
(e.g. Diab and Bhutada (2009), Li and Sporleder
(2010)). Even though there is some methodologi-
cal overlap between our approach and some of the
verb-oriented studies, we believe that verb construc-
tions have properties that are quite different from
noun phrases. For example, our definition of alter-
native vector relies on the fact that most noun phrase
MWUs are fixed and exhibit no syntactic variability.
In contrast, verb constructions are often discontinu-
ous.
The motivation for most work on MWU detec-
tion is lexicography, terminology extraction or the
creation of machine-readable dictionaries. Our mo-
tivation – tokenization in a preprocessing setting – is
different from this earlier work.
</bodyText>
<sectionHeader confidence="0.953011" genericHeader="method">
3 Semantic Heads and Cascaded Model
</sectionHeader>
<bodyText confidence="0.999706103448276">
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. We define the
semantic head of a noun phrase as the largest non-
compositional part of the phrase that contains the
syntactic head. For example, black hole is the se-
mantic head of unusual black hole and afterglow is
the semantic head of bright optical afterglow; in the
latter case syntactic and semantic heads coincide.
Semantic heads would serve most NLP tasks bet-
ter than syntactic heads. The attachment ambiguity
of the last noun phrase in he bought the hot dogs in a
packet can be easily resolved for the semantic head
hot dogs (food is often in a packet), but not as easily
for the syntactic head dogs (dogs are usually not in
packets). Indeed, we will show in Section 7 that se-
mantic head recognition improves the performance
of an IR system.
The semantic head is either a single noun or a non-
compositional noun phrase. In the latter case, the
modifier(s) introduce(s) a non-compositional, un-
predictable shift of meaning; hot shifts the mean-
ing of dog from live animal to food. In contrast,
the compositional meaning shift caused by small
in small dog is transparent. The semantic head al-
ways contains the syntactic head; for compositional
phrases, syntactic head and semantic head are iden-
tical.
To determine the semantic head of a phrase, we
use a cascaded classification approach. The cascade
</bodyText>
<page confidence="0.993444">
795
</page>
<listItem confidence="0.99256575">
(1) neutron star
(2) unusual black hole
(3) bright optical afterglow
(4) small moment of inertia
</listItem>
<figureCaption confidence="0.99266">
Figure 1: Example phrases with modifiers. Peripheral
elements are set in italics, syntactic heads in bold.
</figureCaption>
<bodyText confidence="0.97836815">
comes into play in all aspects of our study: the rat-
ing experiments with human subjects, data extrac-
tion, feature design and classification itself.
We need a cascade because we want to recog-
nize the semantic head in noun phrases of arbitrary
length. The starting point is a phrase of length n:
p = wi ... wn. We distinguish between the syntac-
tic head of a phrase and the remaining words, the
modifiers. Figure 1 shows phrases of varying syn-
tactic complexity. The syntactic head is marked in
bold. The model accommodates pre-nominal modi-
fiers as in examples (1) through (3) and post-nominal
modifiers like PPs in example (4).
Among the modifiers, there is a distinguished ele-
ment, the peripheral element u (italicized in the ex-
amples). The remaining words are called the rest
v. We can now represent any phrase p as p = uv.3
The element u is always the outermost modifier. of-
PPs are treated as a single modifier and they take
precedence over pre-nominal modification because
this analysis is dominant in our gold standard data.
This means that in the phrase small moment of iner-
tia, small (and not of inertia) is the peripheral ele-
ment u.
Cascaded classification then operates as shown in
Figure 2. In each iteration, the classifier decides
whether the relation between the current peripheral
element u and the rest v is compositional (C) or non-
compositional (NC). If the relation is NC, process-
ing stops and uv is returned as the semantic head
of p. If the relation is compositional, u is discarded
and classification continues with v as the new input
phrase, which again is represented in the form uV.
In case there is no more peripheral element u, i.e.
the new v is a single word, it is returned as the se-
mantic head of p.
Table 1 shows two examples. For the fully com-
positional phrase bright optical afterglow, the pro-
3We use the abstract representation p = uv even though u
can appear after v in the surface form of p.
</bodyText>
<figure confidence="0.625759555555556">
function recognize semantic head(p)
u ← peripheral(p)
v ← rest(p)
while decision(u, v) =6 NC do
u ← peripheral(v)
if u = ∅ then
return v
v ← rest(v)
return uv
</figure>
<figureCaption confidence="0.994561">
Figure 2: Cascaded classification of p
</figureCaption>
<table confidence="0.9985205">
step u v decision
1 bright optical afterglow C
2 optical afterglow C
3 ∅ afterglow
1 small moment of inertia C
2 of inertia moment NC
</table>
<tableCaption confidence="0.999838">
Table 1: Cascaded decision processes
</tableCaption>
<bodyText confidence="0.999330285714286">
cess runs all the way down to the syntactic head af-
terglow which is also the semantic head. In the sec-
ond case, the process stops earlier, in step 2, because
the classifier finds that the relation between moment
and of inertia is NC. This means that the semantic
head of small moment of inertia is moment of iner-
tia.
</bodyText>
<sectionHeader confidence="0.957928" genericHeader="method">
4 Corpus and Feature Definitions
</sectionHeader>
<subsectionHeader confidence="0.995555">
4.1 Candidate phrases
</subsectionHeader>
<bodyText confidence="0.9996441875">
As our corpus, we use the iSearch collection, a
one billion word collection of documents from the
physics domain (Lykke et al., 2010). We tokenized
the collection by splitting on white space and adding
sentence boundaries and part-of-speech tags to the
output. With part-of-speech information, the iden-
tification of MWU candidates is easy, fast and reli-
able.
We extracted all noun phrases from the collection
that consist of a head noun with up to four modifiers
– almost all domain-specific terminology in our col-
lection is captured by this pattern. The pre-nominal
modifiers can be nouns, proper nouns, adjectives or
cardinal numbers.
The baseline accuracy of a classifier that always
chooses compositionality is very high (&gt; 90%) for
</bodyText>
<page confidence="0.877229">
796
</page>
<equation confidence="0.9934045">
V = v V =�v = R1
= R2
= N
U = u O11 O12
U =�u O21 O22
= C1 = C2
</equation>
<tableCaption confidence="0.9951865">
Table 2: 2-by-2 contingency tables with observed and
marginal frequencies
</tableCaption>
<bodyText confidence="0.999965416666667">
phrases of the type [noun] of the/a [noun] (sg.)
(e.g. rest of the paper) and [noun] of [noun] (pl.)
(e.g. series of papers). We therefore restrict post-
nominal modifiers to prepositional phrases with the
word of followed by a non-modified, indefinite, sin-
gular noun, e.g., speed of light or moment of inertia.
Out of all phrases extracted with part-of-speech
patterns, we keep only the ones that appear more of-
ten than 50 times because it is hard to compute re-
liable features for less frequent phrases. All experi-
ments were carried out with lemmatized word forms.
We refer to lemmas as words if not noted otherwise.
</bodyText>
<subsectionHeader confidence="0.997378">
4.2 Association measures
</subsectionHeader>
<bodyText confidence="0.9617684">
Statistical association measures are frequently used
for MWU detection and collocation extraction (e.g.
Schone and Jurafsky (2001), Evert and Krenn
(2001), Pecina (2010)).
We use all measures used by Schone and Jurafsky
(2001) that can be derived from a phrase’s contin-
gency table. These measures are Student’s t-score,
z-score, χ2, pointwise mutual information (MI),
Dice coefficient, frequency, log-likelihood (G2) and
symmetric conditional probability.
We define the AMs in Table 3 based on the no-
tation for the contingency table shown in Table 2
(cf. Evert (2004)). Oij is observed frequency and
Eij = RiCj
N expected frequency.
The AMs are designed to deal with two random
variables U and V that traditionally represent single
words. In our model, we use U to represent periph-
eral elements u and V for rests v.
association measure formula
</bodyText>
<equation confidence="0.714706941176471">
O11−E11
student’s t-score (amt)
√O11
O11−E11
z-score (amz)
√E11
(Oij−Eij)2
Eij
log O11
E11
Dice coefficient (amD) 2O11
R1+C1
frequency (amf) O11
log-likelihood (amG2) 2 E
i,j
symmetric conditional
probability (amscp)
</equation>
<tableCaption confidence="0.998032">
Table 3: Association measures
</tableCaption>
<subsectionHeader confidence="0.949929">
4.3 Word space model
</subsectionHeader>
<bodyText confidence="0.999805941176471">
As our baseline, we use two methods of compar-
ing semantic vectors: sj1 and sj2, both introduced
by Schone and Jurafsky (2001). They experimented
with variants of sj1 and sj2, but found no large differ-
ences. In addition, we introduce our own approach
alt.
Method sj1 compares the semantic vector of a
phrase p with the sum of the vectors of its parts.
Method sj2 is like sj1, except the contexts of p are
not part of the semantic vectors of the parts. Method
alt compares the semantic vector of a phrase with its
alternative vector. In the definitions below, s repre-
sents a vector similarity measure, w(p) a general se-
mantic vector of a phrase p and w∗(wi) the semantic
vector of a part wi of a phrase p that does not include
the contexts of occurrences of wi that were part of p
itself.
</bodyText>
<equation confidence="0.98847">
sj1 s(w(black hole), w(black) + w(hole))
sj2 s(w(blackhole), w∗(black) + w∗(hole))
w(u, hole)); u =� black
</equation>
<bodyText confidence="0.99353">
For the third comparison, we build the alternative
vector as follows. For a phrase p = uv with pe-
ripheral element u and rest v, we call the phrase
</bodyText>
<figure confidence="0.992772181818182">
�
chi-square (amχ2)
i,j
pointwise mutual infor-
mation (amMI)
Oij
Oij log Eij
O112
R1C1
alt s(w(black hole), E
u
</figure>
<page confidence="0.96434">
797
</page>
<bodyText confidence="0.987839464788733">
p&apos; = u&apos;v an alternative phrase if the rest v is the We computed raw agreement of each rater with
same and u&apos; =� u. E.g., giant star is an alternative the gold standard as the percentage of correctly rec-
phrase of neutron star and isolated neutron star is ognized semantic heads – this is the task that the
an alternative of young neutron star. The alterna- classifier addresses. Agreement is quite high at
tive vector of p is then the semantic vector that is 86.5%, 88.3% and 88.5% for the three raters. In
computed from the contexts of all of p’s alternative addition, we calculated chance-corrected agreement
phrases. The alternative vector is a representation with Cohen’s n on the first decision task against the
of the contexts of v except for those modified by u. gold standard (see Section 6). As expected, agree-
This technique bears resemblance to the substitution ment decreases, but is still substantial at 74.0%,
approach of Lin (1999). The difference is that he 78.2% and 71.8% for the three raters.
relies on a similarity thesaurus for substitution and 5 Classifier
monitors the change in mutual information for each We use the Stanford maximum entropy classifier for
substitution individually whereas we substitute with our experiment.4 We randomly split the data into a
general alternative modifiers and combine the alter- training set of 1300 and a held-out test set of 260
native contexts into one vector for comparison. pairs.
Previous work has compared the semantic vector We use the eight AMs and the cosine similari-
of a phrase with the vectors of its components. Our ties simsj1, simsj2 and simalt described in Sec-
approach is more “head-centric” and only compares tion 4.3 as features for the classifier. Cosine similar-
phrases in the same syntactic configuration. Our ity should be small if a phrase is non-compositional
question is: Is the typical context of the head hole and large if it is compositional. In other words, if the
if it occurs with a modifier that is not black different contexts of the candidate phrase are too dissimilar to
from when it occurs with the modifier black? the contexts of the sum of its parts or to the alterna-
We used a bag-of-words model and a window of tive phrases, then we suspect non-compositionality.
±10 words for contexts to create semantic vectors. Feature values are binned into 5 bins. We ap-
We only kept the content words in the window which plied a log transformation to the four AMs with large
we defined as words that are tagged as either a noun, values: amf, amG2, amX2 and amz. For our ap-
verb, adjective or adverb. To add information about plication there is little difference between statistical
the variability of syntactic contexts in which phrases significance at p &lt; .001 and p &lt; .00001. The
occur, we add the words immediately before and af- log transformation reduces the large gap in magni-
ter the phrase with positional markers (−1 and +1, tude between high significance and very high signif-
respectively) to the vector. These words were not icance. If co-occurrence of u and v in uv is below
subject to the content-word filter. The dimension- chance, then we set the association scores to 0 since
ality of the vectors is then 3V where V is the size this is an indication of compositionality (even if it is
of the vocabulary: V dimensions each for bag-of- highly significant).
words, left and right syntactic contexts. We did not Since AMs have been shown to be correlated (e.g.
include vectors for the stop word of for sj1 and sj2. Pecina (2010)), we first perform feature selection on
4.4 Non-compositionality judgments the AM features. We tested accuracy of all 2r − 1
Since the domain of the corpus is physics, highly non-empty combinations of the r = 8 AM features
specialized vocabulary had to be judged. We em- on the task of deciding whether the first decision
ployed domain experts as raters (one engineering during the classification of a phrase was C or NC.
and two physics graduate students). We then selected those AM features that were part
In line with the cascaded model, the raters where of at least one top 10 result in each fold. Those fea-
asked to identify the semantic head of each candi- tures were amt, amf and amscp.
date phrase. If at least two raters agreed on a seman- The main experiment combines these three se-
tic head of a phrase we made this choice the seman-
tic head in the gold standard. The final gold standard
comprises 1560 phrases.
798
4http://nlp.stanford.edu/software/
classifier.shtml
lected AM features with all possible subsets of con- type freq definition
text features. We train on the 1300-element training
set and test on the 260-element test set.
6 Results and Discussion
We ran three evaluation modes: dec-1st, dec-all, and
semh. Mode dec-1st only evaluates the first deci-
sion for each phrase; the baseline in this case is .554
since 55.4% of the first decisions are C. In mode
dec-all, we evaluate all decisions that were made in
the course of recognizing the semantic head. This
mode emphasizes the correct recognition of seman-
tic heads in phrases where multiple correct decisions
in a row are necessary. We define the confidence
for multi-decision classification as the product of
the confidence values of all intermediate decisions.
There is no obvious baseline for dec-all because the
number of decisions depends on the classifier – a
classifier whose first decision on a four-word phrase
is NC makes one decision, another one may make
three. The mode semh evaluates how many semantic
heads were recognized correctly. This mode directly
evaluates the task of semantic head recognition. The
baseline for semh is the tokenizer that always returns
the syntactic head; this baseline is .488.5 Table 4
shows 8 × 3 runs, corresponding to the three modes
tested on the AM features (amt, amf, and amscp)
and the eight possible subsets of the three context
features.
For all modes, the best result is achieved with base
AMs combined with the simalt feature; the accura-
cies are .692, .703 and .680. The improvements over
the baselines (for dec-1st and semh) are statistically
significant at p &lt; .01 (binomial test, n = 260).
For semh, accuracy without any context features
is .603; this is significantly better than the .488 base-
line (p &lt; .01). Performance with only the base AM
features is significantly lower than the best context
feature experiment (.680) at p &lt; .01 and signifi-
cantly lower than the worst context feature exper-
iment (.653) at p &lt; .1. However, the differences
between the context feature runs are not significant.
When the semantic head recognizer processes a
phrase, there are four possible results. Result rsemh:
rsemh 92 sem. head correct (7� synt. head)
rsynth 85 sem. head correct (= synt. head)
r+ 48 sem. head too long
r_ 35 sem. head too short
all 260
Table 5: Distribution of result types
the semantic head is correctly recognized and it is
distinct from the syntactic head. Result rsynth: the
semantic head is correctly recognized and it is iden-
tical to the syntactic head. Result r+: the semantic
head is not correctly recognized because the cascade
was stopped too early, i.e., a compositional modifier
that should have been removed was kept. Result r_:
the semantic head is not correctly recognized be-
cause the cascade was stopped too late, i.e., a modi-
fier causing a non-compositional meaning shift was
removed. Table 5 shows the distribution of result
types. It shows that r+ is the more common error:
the classifier more often regards compositional rela-
tions as non-compositional than vice versa.
Table 6 shows the top 20 classifications where
the semantic head was not the same as the syntac-
tic head sorted by confidence in descending order.
In the third column “phrase ... ” we list the candi-
dates with semantic heads in bold. The columns to
the right show the predicted semantic head and the
feature values. All five errors in the list are of type
r+.
Two r+ phrases are schematic view and many oth-
ers. The two phrases are clearly compositional and
the classifier failed even though the context feature
points in the direction of compositionality with a
value greater than .5. It can be argued that many oth-
ers is a trivial example that does not require complex
machinery to be identified as compositional, e.g. by
using a stop list. We included it in the analysis since
we want to be able to process arbitrary phrases with-
out additional hand-crafted resources.
Another incorrect classification occurs with the
phrase massive star birth6 for which star birth was
annotated as the semantic head. Here we have a case
where the peripheral element massive does not mod-
5The baseline could be improved with simple heuristics, e.g.
“uv contains capital letter” → NC. However, this feature only
results in a 2% improvement compared to the baseline.
6i.e. the birth of a massive star, a certain type of star with
very high mass
799
mode baseline context feature context feature subsets
</bodyText>
<equation confidence="0.991798">
simalt - • • • • - - -
simsj1 - - • – • • - •
simsj2 - - - • • - • •
</equation>
<table confidence="0.993273666666667">
dec-1st .554 .604 .692 .669 .685 .677 .654 .654 .662
dec-all - .615 .703 .681 .696 .688 .666 .669 .675
semh .488 .603 .680 .657 .673 .665 .653 .653 .661
</table>
<tableCaption confidence="0.9979105">
Table 4: Performance for base AM features plus context feature subsets. A ’•’ indicates the use of the corresponding
context feature.
</tableCaption>
<bodyText confidence="0.999758655172414">
ify the syntactic head birth but massive star is itself
a complex modifier. In the test set, 5% of the phrases
exhibit structural ambiguities of this type. Our sys-
tem cannot currently deal with this phenomenon.
The remaining r+ phrases are peculiar velocity
and local group. However, Wikipedia lists both
phrases with an individual entry defining the former
as the true velocity of an object, relative to a rest
frame7 and the latter as the group of galaxies that
includes Earth’s galaxy, the Milky Way8. Both def-
initions provide evidence for non-compositionality
since the velocity is not peculiar (as in strange) and
the scope of local is not clear without further knowl-
edge. Arguably, in these cases our method chose a
justifiable semantic head, but the raters disagreed.9
For NLP preprocessing, it is acceptable to sacri-
fice recall and only make high-confidence decisions
on semantic heads. A tokenizer that reliably detects
a subset of MWUs is better than one that recognizes
none. However, our attempts to use the simalt rec-
ognizer (bold in Table 4) in this way were not suc-
cessful. Precision is .680 for confidence &gt; .7 and
does not exceed .770 for higher confidence values.
To understand this effect, we analyzed the distri-
bution of simalt scores. Surprisingly, moderate sim-
ilarity between .4 and .6 is a more reliable indicator
for NC than low similarity &lt;.3. Our intuition for
using distributional semantics in Section 2 was that
low similarity indicates non-compositionality. This
</bodyText>
<footnote confidence="0.994982857142857">
7http://en.wikipedia.org/wiki/Peculiar—
velocity
8http://en.wikipedia.org/wiki/Local—
group
9Further evidence that local group is non-compositional is
the fact that one of the domain experts annotated the phrase as
non-compositional but was overruled by the other two.
</footnote>
<bodyText confidence="0.99983444">
does not seem to hold for the lowest similarity val-
ues possibly because they are often extreme cases
in terms of distribution and frequency and then give
rise to unreliable decisions. This means that the con-
text features enhance the overall performance of the
classifier, but they are unreliable and do not support
the high-confidence decisions we need in NLP pre-
processing.
For comparison, the classifier that only uses AM
features achieves 90% precision at 14% recall with
confidence &gt; .7 – although it has lower overall ac-
curacy than the simalt recognizer. We are still in
the process of analyzing these results and decided to
use the AM-only recognizer for the IR experiment
because it has more predictable performance.
In summary, the results show that, for the recogni-
tion of semantic heads, basic AMs offer a significant
improvement over the baseline. We have shown that
some wrong decisions are defensible even though
the gold standard data suggests otherwise. Context
features further increase performance significantly,
but surprisingly, they are not of clear benefit for
a high-confidence classifier that is targeted towards
recognizing a smaller subset of semantic heads with
high confidence.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="method">
7 Information Retrieval Experiment
</sectionHeader>
<bodyText confidence="0.9999505">
Typically, IR systems do not process non-
compositional phrases as one semantic entity,
missing out on potentially important information
captured by non-compositionality. This section
illustrates one way of adjusting the retrieval process
so that non-compositional phrases are processed as
semantic entities that may enhance retrieval perfor-
mance. The underlying hypothesis is that, given
</bodyText>
<page confidence="0.991361">
800
</page>
<table confidence="0.993891976190476">
c. type phrase (semantic head in bold) predicted semantic head
.99 rsemh ellipsoidal figure of equilibrium ellipsoidal figure of equilibrium
.99 rsemh point spread function point spread function
.99 r+ massive star birth massive star birth
.98 rsemh high angular resolution imaging high angular resolution imaging
.98 rsemh integral field spectrograph integral field spectrograph
.98 r+ local group local group
.98 rsemh neutral kaon system neutral kaon system
.97 rsemh IRAF task IRAF task
.92 rsemh easy axis easy axis
.89 r+ schematic view schematic view
.87 rsemh differential resistance differential resistance
.86 rsemh TiO band TiO band
.86 r+ many others many others
.86 rsemh VLBA observation VLBA observation
.85 r+ peculiar velocity peculiar velocity
.84 rsemh computation time computation time
.83 rsemh Land factor Land factor
.83 rsemh interference filter interference filter
.83 rsemh line formation calculations line formation calculations
.82 rsemh Wess-Zumino-Witten term Wess-Zumino-Witten term
amt amf amcp simalt
18.03 325 6.23e-01 .219
95.03 9056 2.33e-01 .529
19.99 402 4.81e-03 .134
13.07 179 1.27e-03 .173
24.20 586 4.12e-02 .279
153.54 24759 8.73e-03 .650
1.38 108 4.17e-03 .171
49.07 2411 2.96e-02 .517
44.66 2019 2.79e-03 .599
40.56 1651 8.06e-03 .612
31.71 1034 6.38e-04 .548
36.84 1372 2.21e-03 .581
97.76 9806 6.54e-03 .708
43.95 2004 9.35e-04 .648
167.63 28689 2.37e-02 .800
43.80 1967 1.35e-03 .657
21.15 453 6.30e-04 .360
31.44 1002 1.27e-03 .574
14.20 203 1.96e-03 .381
9.60 94 8.12e-05 .291
</table>
<tableCaption confidence="0.970278">
Table 6: The 20 most confident classifications where the prediction is semantic head =� syntactic head. “c.” = confi-
dence
</tableCaption>
<bodyText confidence="0.9990999375">
a query that contains a non-compositional phrase,
boosting the retrieval weight of documents that
contain this phrase will improve overall retrieval
performance.
We do this boosting using Indri’s10 combination
of the language modeling and inference network
approaches (Metzler and Croft, 2004), which al-
lows assigning different degrees of belief to differ-
ent parts of the query. This belief can be drawn from
any suitable external evidence of relevance. In our
case, this source of evidence is the knowledge that
certain query terms constitute a non-compositional
phrase. Under this approach, and using the #weight
and #combine operators for combining beliefs, the
relevance of a document D to a query Q is computed
as the probability that D generates Q, P(Q|D):
</bodyText>
<equation confidence="0.645093333333333">
� P(t|D) wt (W =1: wt) (1)
P(Q|D) = W t∈Q
t∈Q
</equation>
<bodyText confidence="0.998706">
where t is a term and wt is the belief weight as-
signed to t. The higher wt is, the higher the rank
of documents containing t. In this work, we dis-
</bodyText>
<footnote confidence="0.493238">
10http://www.lemurproject.org/
</footnote>
<bodyText confidence="0.999864666666666">
tinguish between two types of query terms: terms
occurring in non-compositional phrases (Qnc), and
the remaining query terms (Qc). Terms t E Qnc
receive belief weight wnc and terms t E Qc belief
weight wc, (wnc + wc = 1 and wnc, wc E [0,1]).
To boost the ranking of documents containing non-
compositional phrases, we increase wnc at the ex-
pense of wc. We estimate P(t|D) in Eq. 1 using
Dirichlet smoothing (Zhai and Lafferty, 2002).
We use Indri for indexing and retrieval without
removing stopwords or stemming. This choice is
motivated by two reasons: (i) We do not have a
domain-specific stopword list or stemmer. (ii) Base-
line performance is higher when keeping stopwords
and without stemming, rather than without stop-
words and with stemming.
We use the iSearch collection discussed in Sec-
tion 4. It comprises 453,254 documents and a
set of 65 queries with relevance assessments. To
match documents to queries without any treat-
ment of non-compositionality (baseline run), we
use the Kullback-Leibler language model with
Dirichlet smoothing (KL-Dir) (Zhai and Lafferty,
2002). We applied the preprocessing described
</bodyText>
<page confidence="0.992564">
801
</page>
<table confidence="0.999922625">
run MAP REC P20
baseline 0.0663 770 0.1385
real NC 0.0718 844 0.1538
pseudo NC1 0.0664 788 0.1385
pseudo NC2 0.0658 782 0.1462
pseudo NC3 0.0671 777 0.1477
pseudo NC4 0.0681 807 0.1462
pseudo NC5 0.0670 783 0.1423
</table>
<tableCaption confidence="0.989043">
Table 7: IR performance without considering non-
compositionality (baseline), versus boosting real and
pseudo non-compositionality (real NC, pseudo NCi).
</tableCaption>
<bodyText confidence="0.99987103125">
in Section 4 to the queries and identified non-
compositional phrases with the base AM classifier
from Section 5. Our approach for boosting the
weight of these non-compositional phrases uses
the same retrieval model enhanced with belief
weights as described in Eq. 1 (real NC run). In
addition, we include five runs that boost the weight
of pseudo non-compositional phrases that were
created randomly from the query text (pseudo NC
runs). These pseudo non-compositional phrases
have exactly the same length as the observed non-
compositional phrases for each query. We measure
retrieval performance in terms of mean average
precision (MAP), precision at 20 (P20), and recall
(REC, number of relevant documents retrieved
– total is 2878). For each evaluation measure
separately, we tune the following parameters and
report the best performance: (i) the smoothing
parameter p of the KL-Dir retrieval model (p E
1100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000}, following Zhai and Lafferty (2002)); (ii)
the belief weights w,,,,, w, E 10.1, ... , 0.9} in steps
of 0.1 while preserving w,,,, + w, = 1 at all times.
Table 7 displays retrieval performance of our
approach against the baseline and five runs with
pseudo non-compositional phrases. We see a 9.61%
improvement in the number of relevant retrieved
documents over the baseline. MAP and P20 also
show improvements. Our approach is better than
any of the 5 random runs on all three metrics – the
probability of getting such a good result by chance
is 1&lt; .05, and thus the improvements are statis-
</bodyText>
<page confidence="0.993894">
21
</page>
<bodyText confidence="0.9969938">
tically significant. On doing a query-wise analysis
of MAP scores, we find that large improvements
over the baseline occur when a non-compositional
phrase aligns with what the user is looking for. The
system seems to retrieve more relevant documents
in that case. E.g., the improvement in MAP is
0.0977 for query #19. The user was looking for
“articles ... on making tunable vertical cavity sur-
face emitting laser diodes” and laser diodes was
one of the non-compositional phrases recognized.
On the other hand, a decrease in MAP occurs for
non-compositional phrases unrelated to the infor-
mation need. In query #4 the user is looking for
“protein-protein interaction, the surface charge dis-
tribution of these proteins and how this has been in-
vestigated with Electrostatic Force Microscopy” and
though non-compositional phrases such as Force Mi-
croscopy are recognized, these do not reflect the core
information need “The proteins of interest are the
Avidin-Biotin and IgG-anti-IgG systems”.
</bodyText>
<sectionHeader confidence="0.99822" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999985">
We have presented an approach to improving to-
kenization in NLP preprocessing that is based on
the notion of semantic head. Semantic heads are
– in analogy to syntactic heads – the core meaning
units of phrases that cannot be further semantically
decomposed. To perform semantic head recogni-
tion for tokenization, we defined a novel cascaded
model and implemented it as a statistical classifier
that used previously proposed and new context fea-
tures. We have shown that the classifier significantly
outperforms the baseline and that context features
increase performance. We reached an accuracy of
68% and argued that even a semantic head recog-
nizer restricted to high-confidence decisions is use-
ful – because reliably recognizing a subset of se-
mantic heads is better than recognizing none. We
showed that context features increase the accuracy
of the classifier, but undermine the confidence as-
sessments of the classifier, a result we are still ana-
lyzing. Finally, we showed that even in its prelim-
inary current form the semantic head recognizer is
able to improve the performance of an IR system.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996262333333333">
This work was funded by DFG projects SFB 732 and
WordGraph. We also thank the anonymous review-
ers for their comments.
</bodyText>
<page confidence="0.996622">
802
</page>
<sectionHeader confidence="0.995814" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923979381443">
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic ex-
traction of arabic multiword expressions. In Proceed-
ings of the 2010 Workshop on Multiword Expressions,
pages 19–27, Beijing, China. Coling 2010 Organizing
Committee.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 Workshop on Multiword Expres-
sions, pages 89–96, Sapporo, Japan. Association for
Computational Linguistics.
Helena Caseli, Aline Villavicencio, Andr´e Machado,
and Maria Jos´e Finatto. 2009. Statistically-driven
alignment-based multiword expression identification
for technical domains. In Proceedings of the 2009
Workshop on Multiword Expressions, pages 1–8, Sin-
gapore. Association for Computational Linguistics.
Yaacov Choueka. 1988. Looking for needles in a
haystack. In Proceedings of RIAO88, pages 609–623.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the 2007 on Mul-
tiword Expressions, pages 41–48, Prague, Czech Re-
public. Association for Computational Linguistics.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction mwe token classification. In Proceedings of
the 2009 Workshop on Multiword Expressions, pages
17–22, Singapore. Association for Computational Lin-
guistics.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 188–195.
Association for Computational Linguistics.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut f¨ur maschinelle Sprachverarbeitung (IMS), Uni-
versit¨at Stuttgart.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the 2006 Workshop on Multiword Expres-
sions, pages 12–19, Sydney, Australia. Association for
Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Linguistic cues
for distinguishing literal and non-literal usages. In
Coling 2010: Posters, pages 683–691, Beijing, China.
Coling 2010 Organizing Committee.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 317–324, College Park, Maryland,
USA. Association for Computational Linguistics.
Marianne Lykke, Birger Larsen, Haakon Lund, and Pe-
ter Ingwersen. 2010. Developing a test collection for
the evaluation of integrated search. In Advances in In-
formation Retrieval, 32nd European Conference on IR
Research, ECIR 2010, Milton Keynes, UK, March 28-
31, 2010. Proceedings, pages 627–630.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, MA.
Donald Metzler and W. Bruce Croft. 2004. Combining
the language model and inference network approaches
to retrieval. Inf. Process. Manage., 40(5):735–750.
B.V. Moir´on and J¨org Tiedemann. 2006. Identify-
ing Idiomatic Expressions Using Automatic Word-
Alignment. In Multi-Word-Expressions in a Multilin-
gual Context, page 33.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):138–158.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for multiword
expression identification. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC’10), Valletta, Malta.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics, pages 1–
15, Mexico City.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the 2001 Conference on Empirical Methods in Natu-
ral Language Processing, pages 100–108, Pittsburgh,
Pennsylvania, USA. Association for Computational
Linguistics.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational linguistics, 19(1):143–177.
ChengXiang Zhai and John D. Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR,
pages 49–56. ACM.
</reference>
<page confidence="0.999164">
803
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914310">
<title confidence="0.999853">A Cascaded Classification Approach to Semantic Head Recognition</title>
<author confidence="0.9787305">Michelbacher Alok Kothari Martin Christina Lioma Hinrich</author>
<affiliation confidence="0.9973935">Institute for University of</affiliation>
<email confidence="0.996522">martin.forst@microsoft.com</email>
<abstract confidence="0.997961555555556">Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recogmulti-word units (MWUs) like dog hole a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classification approach because – unlike other work on MWUs – tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohammed Attia</author>
<author>Antonio Toral</author>
<author>Lamia Tounsi</author>
<author>Pavel Pecina</author>
<author>Josef van Genabith</author>
</authors>
<title>Automatic extraction of arabic multiword expressions.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 2010 Workshop on Multiword Expressions,</booktitle>
<pages>pages</pages>
<location>Beijing, China. Coling</location>
<marker>Attia, Toral, Tounsi, Pecina, van Genabith, 2010</marker>
<rawString>Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina, and Josef van Genabith. 2010. Automatic extraction of arabic multiword expressions. In Proceedings of the 2010 Workshop on Multiword Expressions, pages 19–27, Beijing, China. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multiword Expressions,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9051" citStr="Baldwin et al. (2003)" startWordPosition="1439" endWordPosition="1442">questions that arise are (i) which vectors to compare, (ii) how to combine the vectors of the parts and (iii) from what point on a certain dissimilarity indicates non-compositionality. To our knowledge, there are no generally accepted answers to these questions. Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Regarding (ii), they suggest weighted or unweighted sums of the semantic vectors of the parts. Baldwin et al. (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. They address (i) by comparing the semantic vectors of phrases with the vectors of their parts individually to detect meaning changes; e.g., they compare vice president to vice and president. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)’s substitution approach (see Section 4.3). Our rationale is that context features should be based on contexts that are syntactically similar to the phrase in question. With respect to (iii), the above-mentioned studies</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL 2003 Workshop on Multiword Expressions, pages 89–96, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena Caseli</author>
<author>Aline Villavicencio</author>
<author>Andr´e Machado</author>
<author>Maria Jos´e Finatto</author>
</authors>
<title>Statistically-driven alignment-based multiword expression identification for technical domains.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Multiword Expressions,</booktitle>
<pages>1--8</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="6657" citStr="Caseli et al. (2009)" startWordPosition="1065" endWordPosition="1068">ompositionality and show constructed thesaurus. The method reaches 15.7% that it significantly enhances a baseline semantic precision and 13.7% recall. head recognizer. However, we also identify a num- In terms of the extraction of domain-specific ber of challenges of using contextual similarity in MWUs, cross-language methods have been prohigh-confidence semantic head recognition. Fourth, posed that make use of the fact that an MWU in one we show that our approach to semantic head recog- language might be expressed as a single word in annition improves the performance of an IR system. other. Caseli et al. (2009) utilize word alignments Section 2 discusses previous work. In Section 3 in a parallel corpus; Attia et al. (2010) exploit the we introduce semantic heads and present our cas- links between article names of different-language caded model for semantic head recognition. In Sec- Wikipedias to search for many-to-one translations. tion 4, we describe our data and three different mea- We did not pursue a cross-language approach besures of contextual similarity. Section 5 introduces cause we strive for a self-contained method of MWU the classifier and its features. Section 6 presents recognition that</context>
</contexts>
<marker>Caseli, Villavicencio, Machado, Finatto, 2009</marker>
<rawString>Helena Caseli, Aline Villavicencio, Andr´e Machado, and Maria Jos´e Finatto. 2009. Statistically-driven alignment-based multiword expression identification for technical domains. In Proceedings of the 2009 Workshop on Multiword Expressions, pages 1–8, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaacov Choueka</author>
</authors>
<title>Looking for needles in a haystack.</title>
<date>1988</date>
<booktitle>In Proceedings of RIAO88,</booktitle>
<pages>609--623</pages>
<contexts>
<context position="4238" citStr="Choueka (1988)" startWordPosition="691" endWordPosition="692">2011 Conference on Empirical Methods in Natural Language Processing, pages 793–803, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics termine possible coreference of a hot dog ... the dog and MWUs is to apply statistical association meain I first ate a hot dog and then fed the dog. This is sures (AMs) to n-grams extracted from a corpus not the case for a system that makes the decision – often combined with various linguistic heurisbased on the semantic heads hot dog of a hot dog tics and other filters, resulting in candidate lists. and dog of the dog. Choueka (1988) and the XTRACT system (Smadja, The specific NLP application we evaluate in this 1993) are well-known examples of this approach. paper is information retrieval. We will show that se- More recent approaches such as Pecina (2010) mantic head recognition improves the performance and Ramisch et al. (2010) combine classifiers with of an information retrieval system. association measures. Although our approach is We introduce a cascaded classification framework classification-based as well, our data set has a more for recognizing semantic heads that allows us to treat realistic size than Pecina (201</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Yaacov Choueka. 1988. Looking for needles in a haystack. In Proceedings of RIAO88, pages 609–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 on Multiword Expressions,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10264" citStr="Cook et al. (2007)" startWordPosition="1630" endWordPosition="1633">udies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exhibit </context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2007. Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. In Proceedings of the 2007 on Multiword Expressions, pages 41–48, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Pravin Bhutada</author>
</authors>
<title>Verb noun construction mwe token classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Multiword Expressions,</booktitle>
<pages>17--22</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="10514" citStr="Diab and Bhutada (2009)" startWordPosition="1668" endWordPosition="1671">oncerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exhibit no syntactic variability. In contrast, verb constructions are often discontinuous. The motivation for most work on MWU detection is lexicography, terminology extraction or the creation of machine-readable dictionaries. Our motivation – tokenization i</context>
</contexts>
<marker>Diab, Bhutada, 2009</marker>
<rawString>Mona Diab and Pravin Bhutada. 2009. Verb noun construction mwe token classification. In Proceedings of the 2009 Workshop on Multiword Expressions, pages 17–22, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
<author>Brigitte Krenn</author>
</authors>
<title>Methods for the qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>188--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17143" citStr="Evert and Krenn (2001)" startWordPosition="2813" endWordPosition="2816">prepositional phrases with the word of followed by a non-modified, indefinite, singular noun, e.g., speed of light or moment of inertia. Out of all phrases extracted with part-of-speech patterns, we keep only the ones that appear more often than 50 times because it is hard to compute reliable features for less frequent phrases. All experiments were carried out with lemmatized word forms. We refer to lemmas as words if not noted otherwise. 4.2 Association measures Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase’s contingency table. These measures are Student’s t-score, z-score, χ2, pointwise mutual information (MI), Dice coefficient, frequency, log-likelihood (G2) and symmetric conditional probability. We define the AMs in Table 3 based on the notation for the contingency table shown in Table 2 (cf. Evert (2004)). Oij is observed frequency and Eij = RiCj N expected frequency. The AMs are designed to deal with two random variables U and V that traditionally represent single words. In our model, w</context>
</contexts>
<marker>Evert, Krenn, 2001</marker>
<rawString>Stefan Evert and Brigitte Krenn. 2001. Methods for the qualitative evaluation of lexical association measures. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 188–195. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<booktitle>Ph.D. thesis, Institut f¨ur maschinelle Sprachverarbeitung (IMS),</booktitle>
<institution>Universit¨at Stuttgart.</institution>
<contexts>
<context position="17556" citStr="Evert (2004)" startWordPosition="2880" endWordPosition="2881">ot noted otherwise. 4.2 Association measures Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase’s contingency table. These measures are Student’s t-score, z-score, χ2, pointwise mutual information (MI), Dice coefficient, frequency, log-likelihood (G2) and symmetric conditional probability. We define the AMs in Table 3 based on the notation for the contingency table shown in Table 2 (cf. Evert (2004)). Oij is observed frequency and Eij = RiCj N expected frequency. The AMs are designed to deal with two random variables U and V that traditionally represent single words. In our model, we use U to represent peripheral elements u and V for rests v. association measure formula O11−E11 student’s t-score (amt) √O11 O11−E11 z-score (amz) √E11 (Oij−Eij)2 Eij log O11 E11 Dice coefficient (amD) 2O11 R1+C1 frequency (amf) O11 log-likelihood (amG2) 2 E i,j symmetric conditional probability (amscp) Table 3: Association measures 4.3 Word space model As our baseline, we use two methods of comparing semant</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Stefan Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, Institut f¨ur maschinelle Sprachverarbeitung (IMS), Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Workshop on Multiword Expressions,</booktitle>
<pages>12--19</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="10241" citStr="Katz and Giesbrecht (2006)" startWordPosition="1625" endWordPosition="1628">o (iii), the above-mentioned studies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the 2006 Workshop on Multiword Expressions, pages 12–19, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Caroline Sporleder</author>
</authors>
<title>Linguistic cues for distinguishing literal and non-literal usages.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>683--691</pages>
<location>Beijing, China. Coling</location>
<contexts>
<context position="10539" citStr="Li and Sporleder (2010)" startWordPosition="1672" endWordPosition="1675">lity which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the fact that most noun phrase MWUs are fixed and exhibit no syntactic variability. In contrast, verb constructions are often discontinuous. The motivation for most work on MWU detection is lexicography, terminology extraction or the creation of machine-readable dictionaries. Our motivation – tokenization in a preprocessing setting</context>
</contexts>
<marker>Li, Sporleder, 2010</marker>
<rawString>Linlin Li and Caroline Sporleder. 2010. Linguistic cues for distinguishing literal and non-literal usages. In Coling 2010: Posters, pages 683–691, Beijing, China. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>317--324</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="5666" citStr="Lin (1999)" startWordPosition="914" endWordPosition="915">s). The compositionality and semantic heads. In addition, mwetoolkit1 by Ramisch et al. (2010) aims to we compare three features that measure contextual be a software package for lexicographers and its similarity. features are limited to a small set of association Our main contributions in this paper are as fol- measures that do not consider marginal frequencies. lows. First, we introduce the notion of semantic Neither of these two studies includes evaluation in head, in analogy to syntactic head, and propose se- the context of an application. mantic head recognition as a new component of NLP Lin (1999) defines a decision criterion for nonpreprocessing. Second, we develop a cascaded clas- compositional phrases based on the change in the sification framework for semantic head recognition. mutual information of a phrase when substituting Third, we investigate the utility of contextual simi- one word for a similar one based on an automatically larity for detecting non-compositionality and show constructed thesaurus. The method reaches 15.7% that it significantly enhances a baseline semantic precision and 13.7% recall. head recognizer. However, we also identify a num- In terms of the extraction </context>
<context position="9432" citStr="Lin (1999)" startWordPosition="1501" endWordPosition="1502"> ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Regarding (ii), they suggest weighted or unweighted sums of the semantic vectors of the parts. Baldwin et al. (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. They address (i) by comparing the semantic vectors of phrases with the vectors of their parts individually to detect meaning changes; e.g., they compare vice president to vice and president. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)’s substitution approach (see Section 4.3). Our rationale is that context features should be based on contexts that are syntactically similar to the phrase in question. With respect to (iii), the above-mentioned studies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idioma</context>
<context position="20201" citStr="Lin (1999)" startWordPosition="3343" endWordPosition="3344">on star. The alterna- classifier addresses. Agreement is quite high at tive vector of p is then the semantic vector that is 86.5%, 88.3% and 88.5% for the three raters. In computed from the contexts of all of p’s alternative addition, we calculated chance-corrected agreement phrases. The alternative vector is a representation with Cohen’s n on the first decision task against the of the contexts of v except for those modified by u. gold standard (see Section 6). As expected, agreeThis technique bears resemblance to the substitution ment decreases, but is still substantial at 74.0%, approach of Lin (1999). The difference is that he 78.2% and 71.8% for the three raters. relies on a similarity thesaurus for substitution and 5 Classifier monitors the change in mutual information for each We use the Stanford maximum entropy classifier for substitution individually whereas we substitute with our experiment.4 We randomly split the data into a general alternative modifiers and combine the alter- training set of 1300 and a held-out test set of 260 native contexts into one vector for comparison. pairs. Previous work has compared the semantic vector We use the eight AMs and the cosine similariof a phras</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 317–324, College Park, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianne Lykke</author>
<author>Birger Larsen</author>
<author>Haakon Lund</author>
<author>Peter Ingwersen</author>
</authors>
<title>Developing a test collection for the evaluation of integrated search.</title>
<date>2010</date>
<booktitle>In Advances in Information Retrieval, 32nd European Conference on IR Research, ECIR 2010,</booktitle>
<pages>627--630</pages>
<location>Milton Keynes, UK,</location>
<contexts>
<context position="15602" citStr="Lykke et al., 2010" startWordPosition="2553" endWordPosition="2556">afterglow C 3 ∅ afterglow 1 small moment of inertia C 2 of inertia moment NC Table 1: Cascaded decision processes cess runs all the way down to the syntactic head afterglow which is also the semantic head. In the second case, the process stops earlier, in step 2, because the classifier finds that the relation between moment and of inertia is NC. This means that the semantic head of small moment of inertia is moment of inertia. 4 Corpus and Feature Definitions 4.1 Candidate phrases As our corpus, we use the iSearch collection, a one billion word collection of documents from the physics domain (Lykke et al., 2010). We tokenized the collection by splitting on white space and adding sentence boundaries and part-of-speech tags to the output. With part-of-speech information, the identification of MWU candidates is easy, fast and reliable. We extracted all noun phrases from the collection that consist of a head noun with up to four modifiers – almost all domain-specific terminology in our collection is captured by this pattern. The pre-nominal modifiers can be nouns, proper nouns, adjectives or cardinal numbers. The baseline accuracy of a classifier that always chooses compositionality is very high (&gt; 90%) </context>
</contexts>
<marker>Lykke, Larsen, Lund, Ingwersen, 2010</marker>
<rawString>Marianne Lykke, Birger Larsen, Haakon Lund, and Peter Ingwersen. 2010. Developing a test collection for the evaluation of integrated search. In Advances in Information Retrieval, 32nd European Conference on IR Research, ECIR 2010, Milton Keynes, UK, March 28-31, 2010. Proceedings, pages 627–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Combining the language model and inference network approaches to retrieval.</title>
<date>2004</date>
<journal>Inf. Process. Manage.,</journal>
<volume>40</volume>
<issue>5</issue>
<contexts>
<context position="33877" citStr="Metzler and Croft, 2004" startWordPosition="5598" endWordPosition="5601">21e-03 .581 97.76 9806 6.54e-03 .708 43.95 2004 9.35e-04 .648 167.63 28689 2.37e-02 .800 43.80 1967 1.35e-03 .657 21.15 453 6.30e-04 .360 31.44 1002 1.27e-03 .574 14.20 203 1.96e-03 .381 9.60 94 8.12e-05 .291 Table 6: The 20 most confident classifications where the prediction is semantic head =� syntactic head. “c.” = confidence a query that contains a non-compositional phrase, boosting the retrieval weight of documents that contain this phrase will improve overall retrieval performance. We do this boosting using Indri’s10 combination of the language modeling and inference network approaches (Metzler and Croft, 2004), which allows assigning different degrees of belief to different parts of the query. This belief can be drawn from any suitable external evidence of relevance. In our case, this source of evidence is the knowledge that certain query terms constitute a non-compositional phrase. Under this approach, and using the #weight and #combine operators for combining beliefs, the relevance of a document D to a query Q is computed as the probability that D generates Q, P(Q|D): � P(t|D) wt (W =1: wt) (1) P(Q|D) = W t∈Q t∈Q where t is a term and wt is the belief weight assigned to t. The higher wt is, the h</context>
</contexts>
<marker>Metzler, Croft, 2004</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2004. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5):735–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B V Moir´on</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Identifying Idiomatic Expressions Using Automatic WordAlignment. In Multi-Word-Expressions in a Multilingual Context,</title>
<date>2006</date>
<pages>33</pages>
<marker>Moir´on, Tiedemann, 2006</marker>
<rawString>B.V. Moir´on and J¨org Tiedemann. 2006. Identifying Idiomatic Expressions Using Automatic WordAlignment. In Multi-Word-Expressions in a Multilingual Context, page 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>Lexical association measures and collocation extraction. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--1</pages>
<contexts>
<context position="4465" citStr="Pecina (2010)" startWordPosition="727" endWordPosition="728">g and MWUs is to apply statistical association meain I first ate a hot dog and then fed the dog. This is sures (AMs) to n-grams extracted from a corpus not the case for a system that makes the decision – often combined with various linguistic heurisbased on the semantic heads hot dog of a hot dog tics and other filters, resulting in candidate lists. and dog of the dog. Choueka (1988) and the XTRACT system (Smadja, The specific NLP application we evaluate in this 1993) are well-known examples of this approach. paper is information retrieval. We will show that se- More recent approaches such as Pecina (2010) mantic head recognition improves the performance and Ramisch et al. (2010) combine classifiers with of an information retrieval system. association measures. Although our approach is We introduce a cascaded classification framework classification-based as well, our data set has a more for recognizing semantic heads that allows us to treat realistic size than Pecina (2010)’s (1 billion words noun phrases of arbitrary length. We use a number vs 1.5 million words) and we work on noun phrases of previously proposed features for recognizing non- of arbitrary length (instead of just bigrams). The c</context>
<context position="17158" citStr="Pecina (2010)" startWordPosition="2817" endWordPosition="2818">th the word of followed by a non-modified, indefinite, singular noun, e.g., speed of light or moment of inertia. Out of all phrases extracted with part-of-speech patterns, we keep only the ones that appear more often than 50 times because it is hard to compute reliable features for less frequent phrases. All experiments were carried out with lemmatized word forms. We refer to lemmas as words if not noted otherwise. 4.2 Association measures Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase’s contingency table. These measures are Student’s t-score, z-score, χ2, pointwise mutual information (MI), Dice coefficient, frequency, log-likelihood (G2) and symmetric conditional probability. We define the AMs in Table 3 based on the notation for the contingency table shown in Table 2 (cf. Evert (2004)). Oij is observed frequency and Eij = RiCj N expected frequency. The AMs are designed to deal with two random variables U and V that traditionally represent single words. In our model, we use U to repr</context>
<context position="22777" citStr="Pecina (2010)" startWordPosition="3787" endWordPosition="3788">and +1, tude between high significance and very high signifrespectively) to the vector. These words were not icance. If co-occurrence of u and v in uv is below subject to the content-word filter. The dimension- chance, then we set the association scores to 0 since ality of the vectors is then 3V where V is the size this is an indication of compositionality (even if it is of the vocabulary: V dimensions each for bag-of- highly significant). words, left and right syntactic contexts. We did not Since AMs have been shown to be correlated (e.g. include vectors for the stop word of for sj1 and sj2. Pecina (2010)), we first perform feature selection on 4.4 Non-compositionality judgments the AM features. We tested accuracy of all 2r − 1 Since the domain of the corpus is physics, highly non-empty combinations of the r = 8 AM features specialized vocabulary had to be judged. We em- on the task of deciding whether the first decision ployed domain experts as raters (one engineering during the classification of a phrase was C or NC. and two physics graduate students). We then selected those AM features that were part In line with the cascaded model, the raters where of at least one top 10 result in each fol</context>
</contexts>
<marker>Pecina, 2010</marker>
<rawString>Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evaluation, 44(1-2):138–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Christian Boitet</author>
</authors>
<title>mwetoolkit: a framework for multiword expression identification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="4540" citStr="Ramisch et al. (2010)" startWordPosition="736" endWordPosition="739">hot dog and then fed the dog. This is sures (AMs) to n-grams extracted from a corpus not the case for a system that makes the decision – often combined with various linguistic heurisbased on the semantic heads hot dog of a hot dog tics and other filters, resulting in candidate lists. and dog of the dog. Choueka (1988) and the XTRACT system (Smadja, The specific NLP application we evaluate in this 1993) are well-known examples of this approach. paper is information retrieval. We will show that se- More recent approaches such as Pecina (2010) mantic head recognition improves the performance and Ramisch et al. (2010) combine classifiers with of an information retrieval system. association measures. Although our approach is We introduce a cascaded classification framework classification-based as well, our data set has a more for recognizing semantic heads that allows us to treat realistic size than Pecina (2010)’s (1 billion words noun phrases of arbitrary length. We use a number vs 1.5 million words) and we work on noun phrases of previously proposed features for recognizing non- of arbitrary length (instead of just bigrams). The compositionality and semantic heads. In addition, mwetoolkit1 by Ramisch et </context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010. mwetoolkit: a framework for multiword expression identification. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>1--15</pages>
<location>Mexico City.</location>
<contexts>
<context position="1525" citStr="Sag et al. (2002)" startWordPosition="229" endWordPosition="232">rieval performance when used as part of an information retrieval system. 1 Introduction Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog or black hole. Our long-term goal is to build MWU-aware tokenizers that are used as part of the standard toolkit for NLP preprocessing alongside part-of-speech and named-entity tagging. We define an MWU as a sequence of words that has properties that cannot be inferred from the component words (cf. e.g. Manning and Sch¨utze (1999, Ch. 5), Sag et al. (2002)). The most important of these properties is non-compositionality, the fact that the meaning of a phrase cannot be predicted from the meanings of its component words. For example, a hot dog is not a hot animal but a sausage in a bun and a black hole in astrophysics is a region of space with special properties, not a dark cavity. The correct recognition of MWUs is an important building block of many NLP tasks. For example, in information retrieval (IR) the query hot dog should not retrieve documents that only contain the words hot and dog individually, outside of the phrase hot dog. In this stu</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics, pages 1– 15, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="8734" citStr="Schone and Jurafsky (2001)" startWordPosition="1378" endWordPosition="1381">locations 794 1http://sourceforge.net/projects/ mwetoolkit/ appears. The underlying idea is similar to Lin’s: the meaning of a non-compositional phrase somehow deviates from what one would expect given the semantic vectors of parts of the phrase. The standard measure to compare semantic vectors is cosine similarity. The questions that arise are (i) which vectors to compare, (ii) how to combine the vectors of the parts and (iii) from what point on a certain dissimilarity indicates non-compositionality. To our knowledge, there are no generally accepted answers to these questions. Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Regarding (ii), they suggest weighted or unweighted sums of the semantic vectors of the parts. Baldwin et al. (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. They address (i) by comparing the semantic vectors of phrases with the vectors of their parts individually to detect meaning changes; e.g., they compare vice president to vice and president. We prop</context>
<context position="10209" citStr="Schone and Jurafsky (2001)" startWordPosition="1619" endWordPosition="1623">rase in question. With respect to (iii), the above-mentioned studies use ad hoc thresholds to separate compositional and non-compositional phrases but do not offer a principled decision criterion.2 In contrast, we train a statistical classifier to learn a decision criterion. There is a larger body of work concerning noncompositionality which revolves around the problem of literal (compositional) vs. non-literal (noncompositional) usage of idiomatic verb constructions like to break the ice or to spill the beans. Some studies approach the problem with semantic vector comparisons in the style of Schone and Jurafsky (2001), e.g Katz and Giesbrecht (2006) and Cook et al. (2007). Other approaches use wordalignment (e.g. Moir´on and Tiedemann (2006)) or 2Lin (1999) uses a well-defined criterion but his approach is not based on vector similarity. a combination of heuristic and linguistic features (e.g. Diab and Bhutada (2009), Li and Sporleder (2010)). Even though there is some methodological overlap between our approach and some of the verb-oriented studies, we believe that verb constructions have properties that are quite different from noun phrases. For example, our definition of alternative vector relies on the</context>
<context position="17119" citStr="Schone and Jurafsky (2001)" startWordPosition="2809" endWordPosition="2812">ct postnominal modifiers to prepositional phrases with the word of followed by a non-modified, indefinite, singular noun, e.g., speed of light or moment of inertia. Out of all phrases extracted with part-of-speech patterns, we keep only the ones that appear more often than 50 times because it is hard to compute reliable features for less frequent phrases. All experiments were carried out with lemmatized word forms. We refer to lemmas as words if not noted otherwise. 4.2 Association measures Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase’s contingency table. These measures are Student’s t-score, z-score, χ2, pointwise mutual information (MI), Dice coefficient, frequency, log-likelihood (G2) and symmetric conditional probability. We define the AMs in Table 3 based on the notation for the contingency table shown in Table 2 (cf. Evert (2004)). Oij is observed frequency and Eij = RiCj N expected frequency. The AMs are designed to deal with two random variables U and V that traditionally represent singl</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 100–108, Pittsburgh, Pennsylvania, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational linguistics,</booktitle>
<pages>19--1</pages>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational linguistics, 19(1):143–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>John D Lafferty</author>
</authors>
<title>Two-stage language models for information retrieval.</title>
<date>2002</date>
<booktitle>In SIGIR,</booktitle>
<pages>49--56</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="35000" citStr="Zhai and Lafferty, 2002" startWordPosition="5798" endWordPosition="5801">Q|D) = W t∈Q t∈Q where t is a term and wt is the belief weight assigned to t. The higher wt is, the higher the rank of documents containing t. In this work, we dis10http://www.lemurproject.org/ tinguish between two types of query terms: terms occurring in non-compositional phrases (Qnc), and the remaining query terms (Qc). Terms t E Qnc receive belief weight wnc and terms t E Qc belief weight wc, (wnc + wc = 1 and wnc, wc E [0,1]). To boost the ranking of documents containing noncompositional phrases, we increase wnc at the expense of wc. We estimate P(t|D) in Eq. 1 using Dirichlet smoothing (Zhai and Lafferty, 2002). We use Indri for indexing and retrieval without removing stopwords or stemming. This choice is motivated by two reasons: (i) We do not have a domain-specific stopword list or stemmer. (ii) Baseline performance is higher when keeping stopwords and without stemming, rather than without stopwords and with stemming. We use the iSearch collection discussed in Section 4. It comprises 453,254 documents and a set of 65 queries with relevance assessments. To match documents to queries without any treatment of non-compositionality (baseline run), we use the Kullback-Leibler language model with Dirichl</context>
<context position="37057" citStr="Zhai and Lafferty (2002)" startWordPosition="6119" endWordPosition="6122">t were created randomly from the query text (pseudo NC runs). These pseudo non-compositional phrases have exactly the same length as the observed noncompositional phrases for each query. We measure retrieval performance in terms of mean average precision (MAP), precision at 20 (P20), and recall (REC, number of relevant documents retrieved – total is 2878). For each evaluation measure separately, we tune the following parameters and report the best performance: (i) the smoothing parameter p of the KL-Dir retrieval model (p E 1100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000}, following Zhai and Lafferty (2002)); (ii) the belief weights w,,,,, w, E 10.1, ... , 0.9} in steps of 0.1 while preserving w,,,, + w, = 1 at all times. Table 7 displays retrieval performance of our approach against the baseline and five runs with pseudo non-compositional phrases. We see a 9.61% improvement in the number of relevant retrieved documents over the baseline. MAP and P20 also show improvements. Our approach is better than any of the 5 random runs on all three metrics – the probability of getting such a good result by chance is 1&lt; .05, and thus the improvements are statis21 tically significant. On doing a query-wise </context>
</contexts>
<marker>Zhai, Lafferty, 2002</marker>
<rawString>ChengXiang Zhai and John D. Lafferty. 2002. Two-stage language models for information retrieval. In SIGIR, pages 49–56. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>