<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.3478245">
Supertagging: An Approach to Almost
Parsing
</title>
<author confidence="0.467577">
Srinivas Bangalore. Aravind K. joshit
</author>
<affiliation confidence="0.252597">
AT&amp;T Labs — Research University of Pennsylvania
</affiliation>
<bodyText confidence="0.998765882352941">
In this paper, we have proposed novel methods for robust parsing that integrate the flexibility
of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our
thesis is that the computation of linguistic structure can be localized if lexical items are associated
with rich descriptions (supertags) that impose complex constraints in a local context. The su-
pertags are designed such that only those elements on which the lexical item imposes constraints
appear within a given supertag. Further, each lexical item is associated with as many supertags
as the number of different syntactic contexts in which the lexical item can appear. This makes
the number of different descriptions for each lexical item much larger than when the descriptions
are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can
be resolved by using statistical distributions of supertag co-occurrences collected from a corpus
of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Gram-
mar (LTAG) framework. The supertags in LTAG combine both phrase structure information and
dependency information in a single representation. Supertag disambiguation results in a repre-
sentation that is effectively a parse (an almost parse), and the parser need &amp;quot;only&amp;quot; combine the
individual supertags. This method of parsing can also be used to parse sentence fragments such as
in spoken utterances where the disambiguated supertag sequence may not combine into a single
structure.
</bodyText>
<sectionHeader confidence="0.992163" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999972933333333">
In this paper, we present a robust parsing approach called supertagging that integrates
the flexibility of linguistically motivated lexical descriptions with the robustness of
statistical techniques. The idea underlying the approach is that the computation of
linguistic structure can be localized if lexical items are associated with rich descriptions
(supertags) that impose complex constraints in a local context. This makes the number
of different descriptions for each lexical item much larger than when the descriptions
are less complex, thus increasing the local ambiguity for a parser. However, this local
ambiguity can be resolved by using statistical distributions of supertag co-occurrences
collected from a corpus of parses. Supertag disambiguation results in a representation
that is effectively a parse (an almost parse).
In the linguistic context, there can be many ways of increasing the complexity of
descriptions of lexical items. The idea is to associate lexical items with descriptions that
allow for all and only those elements on which the lexical item imposes constraints to
be within the same description. Further, it is necessary to associate each lexical item
with as many descriptions as the number of different syntactic contexts in which the
</bodyText>
<note confidence="0.623924">
* 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com
</note>
<affiliation confidence="0.9986185">
Department of Computer and Information Sciences and Institute for Research in Cognitive Science,
University of Pennsylvania, Philadelphia, PA 19104. E-mail: joshi@linc.cis.upenn.edu
</affiliation>
<note confidence="0.852417">
® 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.99979928">
lexical item can appear. This, of course, increases the local ambiguity for the parser.
The parser has to decide which complex description out of the set of descriptions
associated with each lexical item is to be used for a given reading of a sentence, even
before combining the descriptions together. The obvious solution is to put the burden
of this job entirely on the parser. The parser will eventually disambiguate all the de-
scriptions and pick one per lexical item, for a given reading of the sentence. However,
there is an alternate method of parsing that reduces the amount of disambiguation
done by the parser. The idea is to locally check the constraints that are associated
with the descriptions of lexical items to filter out incompatible descriptions.1 During
this disambiguation, the system can also exploit statistical information that can be
associated with the descriptions based on their distribution in a corpus of parses.
We first employed these ideas in the context of Lexicalized Tree Adjoining gram-
mars (LTAG) in Joshi and Srinivas (1994). Although presented with respect to LTAG,
these techniques are applicable to other lexicalized grammars as well. In this paper, we
present vastly improved supertag disambiguation results—from previously published
68% accuracy to 92% accuracy using a larger training corpus and better smoothing
techniques. The layout of the paper is as follows: In Section 2, we present an overview
of the robust parsing approaches. A brief introduction to Lexicalized Tree Adjoining
grammars is presented in Section 3. Section 4 illustrates the goal of supertag disam-
biguation through an example. Various methods and their performance results for
supertag disambiguation are discussed in detail in Section 5 and Section 6. In Sec-
tion 7, we discuss the efficiency gained in performing supertag disambiguation before
parsing. A robust and lightweight dependency analyzer that uses the supertag out-
put is briefly presented in Section 8. In Section 9, we will discuss the applicability of
supertag disambiguation to other lexicalized grammars.
</bodyText>
<sectionHeader confidence="0.99797" genericHeader="keywords">
2. Related Approaches
</sectionHeader>
<bodyText confidence="0.9999845">
In recent years, there have been a number of attempts at robust parsing of natural lan-
guage. They can be broadly categorized under two paradigms—finite-state-grammar-
based parsers and statistical parsers. We briefly present these two paradigms and
situate our approach to robust parsing relative to these paradigms.
</bodyText>
<subsectionHeader confidence="0.970895">
2.1 Finite-State-Grammar-based Parsers
</subsectionHeader>
<bodyText confidence="0.9999896">
Finite-state-grammar-based approaches to parsing are exemplified by the parsing sys-
tems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995),
Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These sys-
tems use grammars that are represented as cascaded finite-state regular expression
recognizers. The regular expressions are usually hand-crafted. Each recognizer in the
cascade provides a locally optimal output. The output of these systems is mostly in the
form of noun groups and verb groups rather than constituent structure, often called
a shallow parse. There are no clause-level attachments or modifier attachments in the
shallow parse. These parsers always produce one output, since they use the longest-
match heuristic to resolve cases of ambiguity when more than one regular expression
</bodyText>
<footnote confidence="0.897454333333333">
1 The use of descriptions for primitives to capture constraints locally has a precursor in Al. The Waltz
algorithm (Waltz 1975) for labeling vertices of polygonal solid objects can be thought of in these terms.
Waltz made the description of vertices more complex by including information about the incident
edges, associated surfaces and other information. This increases the local ambiguity but the local
constraints on the complex descriptions are strong enough to efficiently disambiguate the descriptions.
Of course, Waltz did not use statistical information for disambiguation. See also Joshi (1998).
</footnote>
<page confidence="0.989508">
238
</page>
<note confidence="0.703479">
Bangalore and Joshi Supertagging
</note>
<bodyText confidence="0.999508333333333">
matches the input string at a given position. At present none of these systems use
any statistical information to resolve ambiguity. The grammar itself can be partitioned
into domain-independent and domain-specific regular expressions, which implies that
porting to a new domain would involve rewriting the domain-dependent expressions.
This approach has proved to be quite successful as a preprocessor in information
extraction systems (Hobbs et al. 1995; Grishman 1995).
</bodyText>
<subsectionHeader confidence="0.999738">
2.2 Statistical Parsers
</subsectionHeader>
<bodyText confidence="0.999676888888889">
Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued
by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman
(1995), Collins (1996), and Charniak (1997), this approach decouples the issue of well-
formedness of an input string from the problem of assigning a structure to it. These
systems attempt to assign some structure to every input string. The rules to assign a
structure to an input are extracted automatically from hand-annotated parses of large
corpora, which are then subjected to smoothing to obtain reasonable coverage of the
language. The resultant set of rules are not linguistically transparent and are not easily
modifiable. Lexical and structural ambiguity is resolved using probability information
that is encoded in the rules. This allows the system to assign the most-likely structure
to each input. The output of these systems consists of constituent analysis, the degree
of detail of which is dependent on the detail of annotation present in the treebank that
is used to train the system.
There are also parsers that use probabilistic (weighting) information in conjunction
with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi
and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the proba-
bilistic information is primarily used to rank the parses produced by the parser and
not so much for the purpose of robustness of the system.
</bodyText>
<sectionHeader confidence="0.896116" genericHeader="introduction">
3. Lexicalized Grammars
</sectionHeader>
<bodyText confidence="0.999958772727273">
Lexicalized grammars are particularly well-suited for the specification of natural lan-
guage grammars. The lexicon plays a central role in linguistic formalisms such as LFG
(Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987),
CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991),
Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992).
Parsing, lexical semantics, and machine translation, to name a few areas, have all ben-
efited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the
syntactic and semantic information in the lexicon. We discuss the merits of lexical-
ization and other related issues in the context of partial parsing and briefly discuss
Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of
the class of lexicalized grammars.
Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and
Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and Joshi 1988; Vijay-Shanker
and Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting grammar formalism unlike
context-free grammars and head grammars, which are string-rewriting formalisms.
The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree
is associated with at least one lexical item on its frontier. The lexical item associated
with an elementary tree is called the anchor of that tree. An elementary tree serves as a
complex description of the anchor and provides a domain of locality over which the an-
chor can specify syntactic and semantic (predicate argument) constraints. Elementary
trees are of two kinds: (a) initial trees and (b) auxiliary trees. In an FB-LTAG gram-
mar for natural language, initial trees are phrase structure trees of simple sentences
</bodyText>
<page confidence="0.995718">
239
</page>
<note confidence="0.881091">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.999603571428571">
containing no recursion, while recursive structures are represented by auxiliary trees.
Elementary trees are combined by substitution and adjunction operations. The result
of combining the elementary trees is the derived tree and the process of combining the
elementary trees to yield a parse of the sentence is represented by the derivation tree.
The derivation tree can also be interpreted as a dependency tree with unlabeled arcs
between words of the sentence. A more detailed discussion of LTAGs with an example
and some of the key properties of elementary trees is presented in Appendix A.
</bodyText>
<sectionHeader confidence="0.966831" genericHeader="method">
4. Supertags
</sectionHeader>
<bodyText confidence="0.999925028571428">
Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et
al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce)
the part-of-speech ambiguity The POS taggers are all local in the sense that they use
information from a limited context in deciding which tag(s) to choose for each word.
As is well known, these taggers are quite successful.
In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG),
each lexical item is associated with at least one elementary structure (tree). The elemen-
tary structures of LTAG localize dependencies, including long-distance dependencies,
by requiring that all and only the dependent elements be present within the same
structure. As a result of this localization, a lexical item may be (and, in general, al-
most always is) associated with more than one elementary structure. We will call these
elementary structures supertags, in order to distinguish them from the standard part-
of-speech tags. Note that even when a word has a unique standard part of speech, say
a verb (V), there will usually be more than one supertag associated with this word.
Since there is only one supertag for each word (assuming there is no global ambiguity)
when the parse is complete, an LTAG parser (Schabes, Abeille, and Joshi 1988) needs
to search a large space of supertags to select the right one for each word before com-
bining them for the parse of a sentence. It is this problem of supertag disambiguation
that we address in this paper.
Since LTAGs are lexicalized, we are presented with a novel opportunity to elimi-
nate or substantially reduce the supertag assignment ambiguity by using local informa-
tion, such as local lexical dependencies, prior to parsing. As in standard part-of-speech
disambiguation, we can use local statistical information in the form of n-gram models
based on the distribution of supertags in an LTAG parsed corpus. Moreover, since
the supertags encode dependency information, we can also use information about the
distribution of distances between a given supertag and its dependent supertags.
Note that as in standard part-of-speech disambiguation, supertag disambiguation
could have been done by a parser. However, carrying out part-of-speech disambigua-
tion prior to parsing makes the job of the parser much easier and therefore speeds
it up. Supertag disambiguation reduces the work of the parser even further. After
supertag disambiguation, we would have effectively completed the parse and the
parser need &amp;quot;only&amp;quot; combine the individual structures; hence the term &amp;quot;almost pars-
ing.&amp;quot; This method can also be used to associate a structure to sentence fragments and
in cases where the supertag sequence after disambiguation may not combine into a
single structure.
</bodyText>
<subsectionHeader confidence="0.999544">
4.1 Example of Supertagging
</subsectionHeader>
<bodyText confidence="0.970037">
LTAGs, by virtue of possessing the Extended Domain of Locality (EDL) property,&apos; as-
sociate with each lexical item, one elementary tree for each syntactic environment that
</bodyText>
<footnote confidence="0.562513">
2 EDL is described in Appendix B.
</footnote>
<page confidence="0.990907">
240
</page>
<note confidence="0.874281">
Bangalore and Joshi Supertagging
</note>
<tableCaption confidence="0.977544">
Table 1
</tableCaption>
<figure confidence="0.9729626">
Examples of syntactic environments where the supertags shown in Figure 1 would be used.
Supertag Construction Example
ai Nominal Predicative this is the purchase
az Noun Phrase the price
a3 Topicalization Almost everything, the price includes
ce4 Adjectival Predicative this is ancillary
ct6 Noun Phrase the company
01 Determiner the company
/32 Nominal Modifier purchase order
a6 Nominal Predicative what is the price
Subject Extraction
a7 Imperative include the share price
i33 Determiner two hundred men
04 Adjectival Modifier ancillary unit
as Nominal Predicative which are the companies
Subject Extraction
a9 Noun Phrase purchases have not increased.
ato Nominal Predicative this is the price
an Transitive Verb the price includes everything
an Adjectival Predicative what is ancillary
</figure>
<subsectionHeader confidence="0.680343">
Subject Extraction
</subsectionHeader>
<bodyText confidence="0.96332475">
an Noun Phrase companies have not been profitable
the lexical item may appear in. As a result, each lexical item is invariably associated
with more than one elementary tree. We call the elementary structures associated with
each lexical item super parts-of-speech (super POS) or supertags.3 Figure 1 illustrates
a few elementary trees associated with each word of the sentence: the purchase price
includes two ancillary companies. Table 1 provides an example context in which each
supertag shown in Figure 1 would be used.
The example in Figure 2 illustrates the initial set of supertags assigned to each
word of the sentence: the purchase price includes two ancillary companies. The order of the
supertags for each lexical item in the example is not relevant. Figure 2 also shows
the final supertag sequence assigned by the supertagger, which picks the best su-
pertag sequence using statistical information (described in Section 6) about individual
supertags and their dependencies on other supertags. The chosen supertags are com-
bined to derive a parse. Without the supertagger, the parser would have to process
combinations of the entire set of trees (at least the 17 trees shown); with it the parser
need only process combinations of 7 trees.
</bodyText>
<sectionHeader confidence="0.702867" genericHeader="method">
5. Reducing Supertag Ambiguity Using Structural Information
</sectionHeader>
<bodyText confidence="0.99924225">
The structure of the supertag can be best seen as providing admissibility constraints
on syntactic environments in which it may be used. Some of these constraints can be
checked locally. The following are a few constraints that can be used to determine the
admissibility of a syntactic environment for a supertag:4
</bodyText>
<footnote confidence="0.960873">
3 For the purpose of this paper, we suppress the features associated with the supertags.
4 Mitch Marcus pointed out that these tests are similar to the generalized shaper tests used in the
Harvard Predictive Analyzer (Kuno 1966).
</footnote>
<page confidence="0.992125">
241
</page>
<figure confidence="0.858755333333333">
Computational Linguistics Volume 25, Number 2
D NP*
the
</figure>
<equation confidence="0.959910731707317">
Sr
/\
NP0i VP
/\
V NPI
I I
N
purchase
al
Nr
NP
purchase
a9
NP
price
az
NPI
Net VP
I /
rp V NPI
I I
e N
wk.
as
aio
NP
price
A.
Na1,1 VP
\
V NI1
I
Warder r
Ce3
\
NI b VP
V NP,
includes
a7
/\
Nig VP
V NP,1
includes
all
A
Mg VP
V NPil
includes
DetP r
two
/33
DetP,.
two
a4
ancillary
04
A
NP1 S,
Pat VP
/
efl V AP,
I I
aorillery
NP
companies
a5
s,
NPo VP
I /
v
I I
-a
a5
NP
companies
aia
NP
companies
«13
companies.
NP
N
</equation>
<figure confidence="0.5917957">
D NP*
the purchase
th 02
N
purchase
02
ancillary
a2 «11 #3 #4
the purchase price includes two ancillary
.....
</figure>
<figureCaption confidence="0.866551">
Figure 1
</figureCaption>
<bodyText confidence="0.902055">
A selection of the supertags associated with each word of the sentence: the purchase price
includes two ancillary companies.
</bodyText>
<listItem confidence="0.9594215">
• Span of the supertag: Span of a supertag is the minimum number of
lexical items that the supertag can cover. Each substitution site of a
supertag will cover at least one lexical item in the input. A simple rule
can be used to eliminate supertags based on the span constraint: if the
span of a supertag is larger than the input string, then the supertag
cannot be used in any parse of the input string.
</listItem>
<page confidence="0.993722">
242
</page>
<table confidence="0.962507857142857">
Bangalore and Joshi Supertagging
Sent: the purchase price includes two ancillary companies.
Initial /A ai az a3 03 a4 as
Assignment 02 a6 a7 04 as
a9 an an a 12 (313
Final 01 02 O 2 an 03 04 an
Assignment
</table>
<tableCaption confidence="0.893732">
Figure 2
Supertag disambiguation for the sentence: the purchase price includes two ancillary
companies.
Table 2
</tableCaption>
<table confidence="0.92118575">
Supertag ambiguity with and without the use of structural constraints.
System Total # of Words Average # of Supertags /Word
Without Structural Constraints 48,783 47.0
With Structural Constraints 48,783 25.0
</table>
<listItem confidence="0.994345714285714">
• Left (Right) span constraint: If the span of the supertag to the left (right)
of the anchor is larger than the length of the string to the left (right) of
the word that anchors the supertag, then the supertag cannot be used in
any parse of the input string.
• Lexical items in the supertag: A supertag can be eliminated if the
terminals appearing on the frontier of the supertag do not appear in the
input string.
</listItem>
<bodyText confidence="0.998587166666667">
Supertags with the built-in lexical item by, that represent passive constructions are
typically eliminated from being considered during the parse of an active sentence.
More generally, these constraints can be used to eliminate supertags that cannot
have their features satisfied in the context of the input string. An example of this is
the elimination of supertag that requires a wh+ NP when the input string does not
contain wh-words.
Table 2 indicates the decrease in supertag ambiguity for 2,012 WSJ sentences
(48,763 words) by using the structural constraints relative to the supertag ambigu-
ity without the structural constraints.5
These filters prove to be very effective in reducing supertag ambiguity. The graph
in Figure 3 plots the number of supertags at the sentence level for sentences of length
2 to 50 words with and without the filters. As can be seen from the graph, the supertag
ambiguity is significantly lower when the filters are used. The graph in Figure 4 shows
the percentage drop in supertag ambiguity due to filtering for sentences of length 2 to
50 words. As can be seen, the average reduction in supertag ambiguity is about 50%.
This means that given a sentence, close to 50% of the supertags can be eliminated
even before parsing begins by just using structural constraints of the supertags. This
reduction in supertag ambiguity speeds up the parser significantly. In fact, the supertag
</bodyText>
<page confidence="0.706196">
5 WSJ Section 20 of the Penn Treebank.
243
</page>
<table confidence="0.588825111111111">
Computational Linguistics Volume 25, Number 2
# of Supertgas x 103
2.80
1
I
. .
. .
?.. • • ! • •
9 1.1 l 6..... 9...;
)1. i &apos; 1 si i
_ A. ‘. .1.
T ‘g..: 1 &apos;. • .. :
.ii i.*A
TV :,... ri . , .
. . .
.-. i .....;
. ......i
0.00 10 00 20 00 30 00 40 00 50 00
</table>
<figureCaption confidence="0.946327">
Figure 3
</figureCaption>
<bodyText confidence="0.987422">
Comparison of number of supertags with and without filtering for sentences of length 2 to 50
words.
ambiguity in XTAG system is so large that the parser is prohibitively slow without
the use of these filters.
Table 3 tabulates the reduction of supertag ambiguity due to the filters against
various parts of speech.6 Verbs in all their forms contribute most to the problem of
supertag ambiguity and most of the supertag ambiguity for verbs is due to light verbs
and verb particles. The filters are very effective in eliminating over 50% of the verb
anchored supertags.
Even though structural constraints are effective in reducing supertag ambiguity,
the search space for the parser is still sufficiently large. In the next few sections, we
present stochastic and rule-based approaches to supertag disambiguation.
</bodyText>
<page confidence="0.878787">
6 The description of the part-of-speech tags is provided in Marcus, Santorini, and Marcirtkiewicz (1993).
</page>
<figure confidence="0.99452125">
Without Filters
&apos;With Filters
Sentence Length
2.60
2.40
2.20
2.00
1.80
1.60
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
244
Bangalore and Joshi Supertagging
Percentage
80.00
75.00
70.00
65.00
60.00
55.00
50.00
45.00
40.00
35.00
30.00
25.00
20.00
15.00
10.00
5.00
0.00
•
Sentence Length
10 00 20 00 30 00 40 00 50 00
</figure>
<figureCaption confidence="0.997092">
Figure 4
</figureCaption>
<bodyText confidence="0.6886605">
Percentage drop in the number of supertags with and without filtering for sentences of length
2 to 50 words.
</bodyText>
<sectionHeader confidence="0.750029" genericHeader="method">
6. Models, Data, Experiments, and Results
</sectionHeader>
<bodyText confidence="0.999902833333333">
Before proceeding to discuss the various models for supertag disambiguation, we
would like to trace the time course of development of this work. We do this not only
to show the improvements made to the early work reported in our 1994 paper (Joshi
and Srinivas 1994), but also to explain the rationale for choosing certain models of
supertag disambiguation over others. We summarize the early work in the following
subsection.
</bodyText>
<subsectionHeader confidence="0.998461">
6.1 Early Work
</subsectionHeader>
<bodyText confidence="0.999989166666667">
As reported in Joshi and Srinivas (1994), we experimented with a trigram model as
well as the dependency model for supertag disambiguation. The trigram model that
was trained on (part-of-speech, supertag) pairs, instead of (words, supertag) pairs,
collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ
sentences produced a correct supertag for 68% of the words in the test set. We have
since significantly improved the performance of the trigram model by using a larger
</bodyText>
<page confidence="0.995552">
245
</page>
<note confidence="0.414026">
Computational Linguistics Volume 25, Number 2
</note>
<tableCaption confidence="0.947973">
Table 3
The effect of filters on supertag ambiguity tabulated against part of speech.
</tableCaption>
<table confidence="0.999573627906976">
POS Average # of Supertags Average # of Supertags Percentage Drop
without Filters with Filters in Supertag Ambiguity
VBP 516.5 250.0 51.6
VB 435.8 224.9 48.4
VBD 209.0 100.7 51.8
VBN 188.2 74.7 60.3
MD 167.2 121.0 27.6
VBZ 165.1 71.6 56.6
VBG 100.7 49.8 50.5
RP 34.5 30.9 10.5
IN 24.3 20.9 14.0
JJS 23.8 12.7 46.9
WRB 23.1 14.3 38.2
JJR 22.7 14.2 37.7
JJ 21.7 13.5 37.9
, 20.0 10.7 46.6
NN 19.8 10.7 46.0
NNS 17.0 10.5 38.6
NNP 15.0 10.2 31.9
NNPS 15.0 10.2 32.1
LS 15.0 15.0 0.0
FW 15.0 15.0 0.0
-RRB- 15.0 10.7 28.4
-LRB- 15.0 12.3 18.0
RBR 14.9 9.5 36.3
RBS 14.9 6.1 59.2
CC 14.8 3.4 76.9
EX 14.0 5.8 58.7
CD 13.3 9.9 25.8
TO 11.3 10.8 4.5
PRP 10.7 5.3 50.2
UH 10.0 3.0 70.0
RB 10.0 5.3 46.4
6.0 3.2 46.7
• 5.5 3.2 42.1
PDT 5.4 4.9 9.0
WP 4.6 2.9 35.8
WP$ 4.0 1.8 56.2
DT 3.9 3.1 21.8
PRP$ 3.8 2.9 22.2
. 3.0 1.0 65.4
POS 2.5 2.1 13.9
WDT 1.2 1.1 5.5
</table>
<bodyText confidence="0.99675375">
training set and incorporating smoothing techniques. We present a detailed discussion
of the model and its performance on a range of corpora in Section 6.5. In Section 6.2,
we briefly mention the dependency model of supertagging that was reported in the
earlier work.
</bodyText>
<subsectionHeader confidence="0.956384">
6.2 Dependency Model
</subsectionHeader>
<bodyText confidence="0.999592666666667">
In an n-gram model for disambiguating supertags, dependencies between supertags
that appear beyond the n-word window cannot be incorporated. This limitation can
be overcome if no a priori bound is set on the size of the window but instead a
</bodyText>
<page confidence="0.925904">
246
</page>
<bodyText confidence="0.988845619047619">
Bangalore and Joshi Supertagging
probability distribution of the distances of the dependent supertags for each supertag
is maintained. We define dependency between supertags in the obvious way: A su-
pertag is dependent on another supertag if the former substitutes or adjoins into the
latter. Thus, the substitution and the foot nodes of a supertag can be seen as specify-
ing dependency requirements of the supertag. The probability with which a supertag
depends on another supertag is collected from a corpus of sentences annotated with
derivation structures. Given a set of supertags for each word and the dependency
information between pairs of supertags, the objective of the dependency model is to
compute the most likely dependency linkage that spans the entire string. The result
of producing the dependency linkage is a sequence of supertags, one for each word
of the sentence along with the dependency information.
Since first reported in Joshi and Srinivas (1994), we have not continued experiments
using this model of supertagging, primarily for two reasons. We are restrained by
the lack of a large corpus of LTAG parsed derivation structures that is needed to
reliably estimate the various parameters of this model. We are currently in the process
of collecting a large LTAG parsed WSJ corpus, with each sentence annotated with
the correct derivation. A second reason for the disuse of the dependency model for
supertagging is that the objective of supertagging is to see how far local techniques can
be used to disambiguate supertags even before parsing begins. The dependency model,
in contrast, is too much like full parsing and is contrary to the spirit of supertagging.
</bodyText>
<subsectionHeader confidence="0.975596">
6.3 N-gram Models with Smoothing
</subsectionHeader>
<bodyText confidence="0.999957208333333">
We have improved the performance of the trigram model by incorporating smoothing
techniques into the model and training the model on a larger training corpus. We
have also proposed some new models for supertag disambiguation. In this section,
we discuss these developments in detail.
Two sets of data are used for training and testing the models for supertag dis-
ambiguation. The first set has been collected by parsing the Wall Street Journal&apos;, IBM
Manual, and ATIS corpora using the wide-coverage English grammar being developed
as part of the XTAG system (Doran et al. 1994). The correct derivation from all the
derivations produced by the XTAG system was picked for each sentence from these
corpora.
The second and larger data set was collected by converting the Penn Treebank
parses of the Wall Street Journal sentences. The objective was to associate each lexical
item of a sentence with a supertag, given the phrase structure parse of the sentence.
This process involved a number of heuristics based on local tree contexts. The heuris-
tics made use of information about the labels of a word&apos;s dominating nodes (parent,
grandparent, and great-grandparent), labels of its siblings (left and right) and siblings
of its parent. An example of the result of this conversion is shown in Figure 5. It
must be noted that this conversion is not perfect and is correct only to a first order
of approximation owing mostly to errors in conversion and lack of certain kinds of
information such as distinction between adjunct and argument preposition phrases,
in the Penn Treebank parses. Even though the converted supertag corpus can be re-
fined further, the corpus in its present form has proved to be an invaluable resource
in improving the performance of the supertag models as is discussed in the following
sections.
</bodyText>
<page confidence="0.8975645">
7 Sentences of length &lt; 15 words.
247
</page>
<table confidence="0.801089928571429">
Computational Linguistics Volume 25, Number 2
( (&amp;quot;S&amp;quot;
(&amp;quot;NP-SBJ&amp;quot; (&amp;quot;NNP&amp;quot; &amp;quot;Mr.&amp;quot;) (&amp;quot;NNP&amp;quot; &amp;quot;Vinken&amp;quot;) )
(&amp;quot;VP&amp;quot; (&amp;quot;VBZ&amp;quot; &amp;quot;is&amp;quot;)
(&amp;quot;NP-PRD&amp;quot;
(&amp;quot;NP&amp;quot; (&amp;quot;NN&amp;quot; &amp;quot;chairman&amp;quot;) )
(&amp;quot;PP&amp;quot; (&amp;quot;IN&amp;quot; &amp;quot;of&amp;quot;)
(&amp;quot;NP&amp;quot;
(&amp;quot;NP&amp;quot; (&amp;quot;NNP&amp;quot; &amp;quot;Elsevier&amp;quot;) (&amp;quot;NNP&amp;quot; &amp;quot;N.V.&amp;quot;) )
utu)
(&amp;quot;NP&amp;quot; (&amp;quot;DT&amp;quot; &amp;quot;the&amp;quot;) (&amp;quot;NNP&amp;quot; &amp;quot;Dutch&amp;quot;) (&amp;quot;VBG&amp;quot; &amp;quot;publishing&amp;quot;) (&amp;quot;NN&amp;quot; &amp;quot;group
&amp;quot;) )))))
(u.n ..u) )) (noun modifier)
Mr.//NNP//B_Nn (head noun)
Vinken//NNP//A_NXN (auxiliary verb)
is//VBZ//B_Vvx (predicative noun)
chairman//NN//A_nx0N1 (noun-attached preposition)
of//IN//B_nxPnx (noun modifier)
Elsevier//NNP//B_Nn (head noun)
N.V.//NNP//A_NXN (appositive comma)
,//,//B_nxPUnxpu (determiner)
the//DT//B_Dnx (noun modifier)
Dutch//NNP//B_Nn (participle verb, nominal modifier)
publishing//VBG//B_Vn (head noun)
group//NN//A_NXN (sentence punctuation)
Figure 5
The phrase structure tree and the supertags obtained from the phrase structure tree for the
WSJ sentence: Mr. Vinken is chairman of Elsevier NV, the Dutch publishing group.
</table>
<subsectionHeader confidence="0.970662">
6.4 Unigram Model
</subsectionHeader>
<bodyText confidence="0.999965875">
Using structural information to filter out supertags that cannot be used in any parse
of the input string reduces the supertag ambiguity but obviously does not eliminate
it completely. One method of disambiguating the supertags assigned to each word
is to order the supertags by the lexical preference that the word has for them. The
frequency with which a certain supertag is associated with a word is a direct measure
of its lexical preference for that supertag. Associating frequencies with the supertags
and using them to associate a particular supertag with a word is clearly the simplest
means of disambiguating supertags. Therefore a unigram model is given by:
</bodyText>
<equation confidence="0.975867">
Supertag(wi) = tk argmaxtyr(tk I wi). (1)
</equation>
<bodyText confidence="0.811595">
where
</bodyText>
<equation confidence="0.977741">
Pr(tk I w,) = frequency(tk, wi)
(2)
frequency(w)
</equation>
<bodyText confidence="0.9999312">
Thus, the most frequent supertag that a word is associated with in a training
corpus is selected as the supertag for the word according to the unigram model. For
the words that do not appear in the training corpus we back off to the part of speech
of the word and use the most frequent supertag associated with that part of speech
as the supertag for the word.
</bodyText>
<page confidence="0.986831">
248
</page>
<note confidence="0.499653">
Bangalore and Joshi Supertagging
</note>
<tableCaption confidence="0.997886">
Table 4
</tableCaption>
<table confidence="0.95482325">
Results from the unigram supertag model.
Data Set Training Set Test Set Top n Supertags % Success
XTAG Parses 8,000 3,000 n = 1 73.4%
n = 2 80.2%
n = 3 80.8%
Converted Penn Treebank Parses 1,000,000 47,000 n =1 77.2%
n = 2 87.0%
n = 3 91.5%
</table>
<subsubsectionHeader confidence="0.796571">
6.4.1 Experiments and Results. We tested the performance of the unigram model on
</subsubsectionHeader>
<bodyText confidence="0.999873933333333">
the previously discussed two sets of data. The words are first assigned standard parts
of speech using a conventional tagger (Church 1988) and then are assigned supertags
according to the unigram model. A word in a sentence is considered correctly su-
pertagged if it is assigned the same supertag as it is associated with in the correct
parse of the sentence. The results of these experiments are tabulated in Table 4.
Although the performance of the unigram model for supertagging is significantly
lower than the performance of the unigram model for part-of-speech tagging (91%
accuracy), it performed much better than expected considering the size of the supertag
set is much larger than the size of part-of-speech tag set. One of the reasons for this
high performance is that the most frequent supertag for the most frequent words—
determiners, nouns, and auxiliary verbs—is the correct supertag most of the time.
Also, backing off to the part of speech helps in supertagging unknown words, which
most often are nouns. The bulk of the errors committed by the unigram model is
incorrectly tagged verbs (subcategorization and transformation), prepositions (noun
attached vs. verb attached) and nouns (head vs. modifier noun).
</bodyText>
<subsectionHeader confidence="0.798591">
6.5 N-gram Model
</subsectionHeader>
<bodyText confidence="0.976074">
We first explored the use of trigram model of supertag disambiguation in Joshi and
Srinivas (1994). The trigram model was trained on (part-of-speech, supertag) pairs
collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ
sentences. It produced a correct supertag for 68% of the words in the test set. A major
drawback of this early work was that it used no lexical information in the supertagging
process as the training material consisted of (part-of-speech, supertag) pairs. Since that
early work, we have improved the performance of the model by incorporating lexical
information and sophisticated smoothing techniques, as well as training on larger
training sets. In this section, we present the details and the performance evaluation of
this model.
In a unigram model, a word is always associated with the supertag that is most
preferred by the word, irrespective of the context in which the word appears. An
alternate method that is sensitive to context is the n-gram model. The n-gram model
takes into account the contextual dependency probabilities between supertags within
a window of n words in associating supertags to words. Thus, the most probable
supertag sequence for an n-word sentence is given by:
= argmaxTPr(Ti, T2, . . • , TN) * Pr(Wi, W2/ • / WN T1, T2/ • . • TN) (3)
where Ti is the supertag for word K.
</bodyText>
<page confidence="0.990219">
249
</page>
<note confidence="0.623079">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.977374">
To compute this using only local information, we approximate, assuming that the
probability of a word depends only on its supertag
</bodyText>
<equation confidence="0.8896475">
Pr(Wi, W2, • • • WN Tl, T2/ • • • TN) Pr(Wi Ti) (4)
1=1
</equation>
<bodyText confidence="0.915057">
and also use an n-gram (trigram, in this case) approximation
</bodyText>
<equation confidence="0.8481">
Pr(Ti, T2, . . , TN) H P r(T I T1-2,T1-1) (5)
</equation>
<bodyText confidence="0.999119111111111">
The term Pr(T, ITi_2, Ti_i) is known as the contextual probability since it indicates
the size of the context used in the model and the term Pr(Wj T,) is called the word
emit probability since it is the probability of emitting the word W, given the tag Ti.
These probabilities are estimated using a corpus where each word is tagged with its
correct supertag.
The contextual probabilities were estimated using the relative frequency estimates
of the contexts in the training corpus. To estimate the probabilities for contexts that
do not appear in the training corpus, we used the Good-Turing discounting technique
(Good 1953) combined with Katz&apos;s back off model (Katz 1987). The idea here is to
discount the frequencies of events that occur in the corpus by an amount related to
their frequencies and utilize this discounted probability mass in the back off model to
distribute to unseen events. Thus, the Good-Turing discounting technique estimates
the frequency of unseen events based on the distribution of the frequency of the counts
of observed events in the corpus. If r is the observed frequency of an event, and N,.
is the number of events with the observed frequency r, and N is the total number
of events, then the probability of an unseen event is given by N1/ N. Furthermore,
the frequencies of the observed events are adjusted so that the total probability of all
events sums to one. The adjusted frequency for observed events, r*, is computed as
</bodyText>
<equation confidence="0.957872">
Nr
r* (r +1) *+i (6)
Nr
</equation>
<bodyText confidence="0.9996725">
Once the frequencies of the observed events are discounted and the frequencies
for unseen events are estimated, Katz&apos;s back off model is used. In this technique, if the
observed frequency of an &lt;n-gram, supertag&gt; sequence is zero then its probability
is computed based on the observed frequency of an (n - 1)-gram sequence. Thus,
</bodyText>
<equation confidence="0.9981934">
15r(T3IT1, T2) = Pr(T3IT1, T2) if Pr(T3IT1, T2) &gt; 0
a(Ti, T2) *15r(T3IT2) if Pr(T2IT1) &gt; 0
= Pr(T3IT2) otherwise
Pr(T2IT1) = Pr(T2IT1) if Pr(T2ITi) &gt; 0
= 0 (T1) * Pri(T2) otherwise
</equation>
<bodyText confidence="0.9996114">
where a(Ti,T)) and 13(Tk) are constants to ensure that the probabilities sum to one.
The word emit probability for the (word, supertag) pairs that appear in the training
corpus is computed using the relative frequency estimates as shown in Equation 7. For
the (word, supertag) pairs that do not appear in the corpus, the word emit probability
is estimated as shown in Equation 8. Some of the word features used in our imple-
</bodyText>
<page confidence="0.857279">
250
</page>
<bodyText confidence="0.860653">
Bangalore and Joshi Supertagging
mentation include prefixes and suffixes of length less than or equal to three characters,
capitalization, and digit features.
</bodyText>
<equation confidence="0.9923215">
Pr(WilTi) =N (W„ T,)
if N (Wi, T,) &gt; 0 (7)
N(Ti)
= Pr(UNKIT,) * Pr(word_features(W,)IT,) otherwise (8)
</equation>
<bodyText confidence="0.99829325">
The counts for the (word, supertag) pairs for the words that do not appear in the
corpus is estimated using the leaving-one-out technique (Niesler and Woodland 1996;
Ney, Essen, and Kneser 1995). A token UNK is associated with each supertag and its
count NuNK is estimated by:
</bodyText>
<equation confidence="0.9997102">
Ni(T1)
Pr(LINKITi) =
N(TI) +
Pr(UNKITj) * N(Ti)
1 — Pr(UNKITI)
</equation>
<bodyText confidence="0.9934190625">
where N1(TI) is the number of words that are associated with the supertag Tj that
appear in the corpus exactly once. N(T1) is the frequency of the supertag Tj and
NuNK(TI) is the estimated count of UNK in 7.1. The constant n is introduced so as to
ensure that the probability is not greater than one, especially for supertags that are
sparsely represented in the corpus.
We use word features similar to the ones used in Weischedel et al. (1993), such
as capitalization, hyphenation, and endings of words, for estimating the word emit
probability of unknown words.
6.5.1 Experiments and Results. We tested the performance of the trigram model on
various domains such as the Wall Street Journal (WSJ), the IBM Manual corpus and the
ATIS corpus. For the IBM Manual corpus and the ATIS domains, a supertag annotated
corpus was collected using the parses of the XTAG system (Doran et al. 1994) and
selecting the correct analysis for each sentence. The corpus was then randomly split
into training and test material. Supertag performance is measured as the percentage
of words that are correctly supertagged by a model when compared with the key for
the words in the test corpus.
</bodyText>
<subsubsectionHeader confidence="0.455105">
Experiment 1: (Performance on the Wall Street Journal corpus). We used the two sets of
</subsubsectionHeader>
<bodyText confidence="0.999870142857143">
data, from the XTAG parses and from the conversion of the Penn Treebank parses to
evaluate the performance of the trigram model. Table 5 shows the performance on the
two sets of data. The first data set, data collected from the XTAG parses, was split
into 8,000 words of training and 3,000 words of test material. The data collected from
converting the Penn Treebank was used in two experiments differing in the size of the
training corpus-200,000 words&apos; and 1,000,000 words9—and tested on 47,000 words&apos;&apos;&apos;.
A total of 300 different supertags were used in these experiments.
</bodyText>
<subsubsectionHeader confidence="0.741221">
Experiment 2: (Performance on the IBM Manual Corpus and ATIS). For testing the perfor-
</subsubsectionHeader>
<bodyText confidence="0.54586">
mance of the trigram supertagger on the IBM Manual corpus, a set of 14,000 words
</bodyText>
<listItem confidence="0.619093666666667">
8 Sentences in WSJ Sections 15 through 18 of Penn Treebank.
9 Sentences in WSJ Sections 00 through 24, except Section 20 of Penn Treebank.
10 Sentences in WSJ Section 20 of Penn Treebank.
</listItem>
<equation confidence="0.946427">
NUNK(Ti) =
</equation>
<page confidence="0.96539">
251
</page>
<table confidence="0.925220866666667">
Computational Linguistics Volume 25, Number 2
Table 5
Performance of the supertagger on the WSJ corpus.
Data Set Size of Training Size of % Correct
Training Set Test Set
(Words) (Words)
XTAG Parses 8,000 Unigram 3,000 73.4%
(Baseline)
Trigram 3,000 86.0%
Converted 200,000 Unigram
Penn Treebank (Baseline) 47,000 75.3%
Parses Trigram 47,000 90.9%
1,000,000 Unigram
(Baseline) 47,000 77.2%
Trigram 47,000 92.2%
</table>
<tableCaption confidence="0.992114">
Table 6
</tableCaption>
<table confidence="0.970390375">
Performance of the supertagger on the IBM Manual corpus and ATIS corpus.
Corpus Size of Training Set (Words) Training Size of Test Set (Words) % Correct
IBM Manual 14,000 Unigram
(Baseline) 1,000 77.8%
Trigram 1,000 90.3%
ATIS 1,500 Unigram
(Baseline) 400 85.7%
Trigram 400 93.8%
</table>
<bodyText confidence="0.9983941875">
correctly supertagged was used as the training corpus and a set of 1,000 words was
used as a test corpus. The performance of the supertagger on this corpus is shown
in Table 6. Performance on the ATIS corpus was evaluated using a set of 1,500 words
correctly supertagged as the training corpus and a set of 400 words as a test corpus.
The performance of the supertagger on the ATIS corpus is also shown in Table 6.
As expected, the performance on the ATIS corpus is higher than that of the WSJ
and the IBM Manual corpus despite the extremely small training corpus. Also, the
performance of the IBM Manual corpus is better than the WSJ corpus when the size
of the training corpus is taken into account. The baseline for the ATIS domain is
remarkably high due to the repetitive constructions and limited vocabulary in that
domain. This is also true for the IBM Manual corpus, although to a lesser extent.
The trigram model of supertagging is attractive for limited domains since it performs
quite well with relatively insignificant amounts of training material. The performance
of the supertagger can be improved in an iterative fashion by using the supertagger
to supertag larger amounts of training material, which can be quickly hand-corrected
and used to train a better-performing supertagger.
</bodyText>
<subsectionHeader confidence="0.972482">
6.5.2 Effect of Lexical versus Contextual Information. Lexical information contributes
</subsectionHeader>
<bodyText confidence="0.999891571428571">
most to the performance of a POS tagger, since the baseline performance of assigning
the most likely POS for each word produces 91% accuracy (Brill 1993). Contextual
information contributes relatively a small amount towards the performance, improv-
ing it from 91% to 96-97%, a 5.5% improvement. In contrast, contextual information
has greater effect on the performance of the supertagger. As can be seen, from the
above experiments, the baseline performance of the supertagger is about 77% and the
performance improves to about 92% with the inclusion of contextual information, an
</bodyText>
<page confidence="0.950778">
252
</page>
<bodyText confidence="0.94802">
Bangalore and joshi Supertagging
improvement of 19.5%. The relatively low baseline performance for the supertagger
is a direct consequence of the fact that there are many more supertags per word than
there are POS tags. Further, since many combinations of supertags are not possible,
contextual information has a larger effect on the performance of the supertagger.
</bodyText>
<subsectionHeader confidence="0.859416">
6.6 Error-driven Transformation-based Tagger
</subsectionHeader>
<bodyText confidence="0.990432605263158">
In an error-driven transformation-based (EDTB) tagger (Brill 1993), a set of pattern-
action templates that include predicates that test for features of words appearing in
the context of interest are defined. These templates are then instantiated with the ap-
propriate features to obtain transformation rules. The effectiveness of a transformation
rule to correct an error and the relative order of application of the rules are learned
using a corpus. The learning procedure takes a gold corpus in which the words have
been correctly annotated and a training corpus that is derived from the gold corpus by
removing the annotations. The objective in the learning phase is to learn the optimum
ordering of rule applications so as to minimize the number of tag mismatches between
the training and the reference corpus.
6.6.1 Experiments and Results. A EDTB model has been trained using templates
defined on a three-word window. We trained the templates on 200,000 words&apos; and
tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of
90%. The EDTB model provides a great deal of flexibility to integrate domain-specific
and linguistic information into the model. However, a major drawback of this approach
is that the training procedure is extremely slow, which prevented us from training on
the 1,000,000 word corpus.
7. Supertagging before Parsing
The output of the supertagger, an almost parse, has been used in a variety of applica-
tions including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d)
and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Do-
ran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling
(Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical de-
scription needed for most applications.
The output of the supertagger has also been used as a front end to a lexicalized
grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptu-
alized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the
parser looks up the lexicon and selects all the supertags associated with each word of
the sentence to be parsed. In the second stage, the parser searches the lattice of selected
supertags in an attempt to combine them using substitution and adjunction operations
so as to yield a derivation that spans the input string. At the end of the second stage,
the parser would not only have parsed the input, but would have associated a small
set of (usually one) supertags with each word.
The supertagger can be used as a front end to a lexicalized grammar parser so
as to prune the search-space of the parser even before parsing begins. It should be
clear that by reducing the number of supertags that are selected in the first stage, the
search-space for the second stage can be reduced significantly and hence the parser
can be made more efficient. Supertag disambiguation techniques, as discussed in the
</bodyText>
<page confidence="0.834951666666667">
11 WSJ Sections 15 to 18 of the Penn Treebank.
12 WSJ Section 20 of the Penn Treebank.
253
</page>
<note confidence="0.530579">
Computational Linguistics Volume 25, Number 2
</note>
<tableCaption confidence="0.862407">
Table 7
Performance improvement of 3-best supertagger over the 1-best supertagger on the WSJ
corpus.
</tableCaption>
<table confidence="0.998921181818182">
Data Set Size of Size of Training % Correct
Test Set Training Set
(Words) (Words)
Converted 47,000 200,000 Trigram 90.9%
Penn Treebank (Best Supertag)
Parses Trigram 95.8%
(3-Best Supertags)
1,000,000 Trigram 92.2%
(Best Supertag)
Trigram 97.1%
(3-Best Supertags)
</table>
<bodyText confidence="0.999924033333333">
previous sections, attempt to disambiguate the supertags selected in the first pass,
based on lexical preferences and local lexical dependencies, so as to ideally select one
supertag for each word. Once the supertagger selects the appropriate supertag for
each word, the second stage of the parser is needed only to combine the individual
supertags to arrive at the parse of the input. Tested on about 1,300 WSJ sentences with
each word in the sentence correctly supertagged, the LTAG parser took approximately
4 seconds per sentence to yield a parse (combine the supertags and perform feature
unification). In contrast, the same 1,300 WSJ sentences without the supertag annotation
took nearly 120 seconds per sentence to yield a parse. Thus the parsing speedup gained
by this integration is a factor of about 30.
In the XTAG system, we have integrated the trigram supertagger as a front end to
an LTAG parser to pick the appropriate supertag for each word even before parsing
begins. However, a drawback of this approach is that the parser would fail completely
if any word of the input is incorrectly tagged by the supertagger. This problem could be
circumvented to an extent by extending the supertagger to produce n-best supertags
for each word. Although this extension would increase the load on the parser, it
would certainly improve the chances of arriving at a parse for a sentence. In fact,
Table 7 presents the performance of the supertagger that selects, at most, the top three
supertags for each word. The optimum number of supertags to output to balance
the success rate of the parser against the efficiency of the parser must be determined
empirically.
A more serious limitation of this approach is that it fails to parse ill-formed and
extragrammatical strings such as those encountered in spoken utterances and unre-
stricted texts. This is due to the fact that the Earley-style LTAG parser attempts to
combine the supertags to construct a parse that spans the entire string. In cases where
the supertag sequence for a string cannot be combined into a unified structure, the
parser fails completely. One possible extension to account for ill-formed and extra-
grammatical strings is to extend the Earley parser to produce partial parses for the
fragments whose supertags can be combined. An alternate method of computing de-
pendency linkages robustly is presented in the next section.
</bodyText>
<subsectionHeader confidence="0.475886">
8. Lightweight Dependency Analyzer
</subsectionHeader>
<bodyText confidence="0.978163">
Supertagging associates each word with a unique supertag. To establish the depen-
dency links among the words of the sentence, we exploit the dependency requirements
</bodyText>
<page confidence="0.976503">
254
</page>
<bodyText confidence="0.978769428571429">
Bangalore and Joshi Supertagging
encoded in the supertags. Substitution nodes and foot nodes in supertags serve as slots
that must be filled by the arguments of the anchor of the supertag. A substitution slot
of a supertag is filled by the complements of the anchor while the foot node of a
supertag is filled by a word that is being modified by the supertag. These argument
slots have a polarity value reflecting their orientation with respect to the anchor of
the supertag. Also associated with a supertag is a list of internal nodes (including
the root node) that appear in the supertag. Using the structural information coupled
with the argument requirements of a supertag, a simple heuristic-based, linear time,
deterministic algorithm (which we call a lightweight dependency analyzer (LDA))
produces dependency linkages not necessarily spanning the entire sentence. The LDA
can produce a number of partial linkages, since it is driven primarily by the need to
satisfy local constraints without being driven to construct a single dependency link-
age that spans the entire input. This, in fact, contributes to the robustness of LDA and
promises to be a useful tool for parsing sentence fragments that are rampant in speech
utterances, as exemplified by the Switchboard corpus.
Tested on section 20 of the Wall Street Journal corpus, which contained 47,333
dependency links in the gold standard, the LDA, trained on 200,000 words, produced
38,480 dependency links correctly, resulting in a recall score of 82.3%. Also, a total of
41,009 dependency links were produced by the LDA, resulting in a precision score of
93.8%. A detailed evaluation of the LDA is presented in Srinivas (199M).
</bodyText>
<sectionHeader confidence="0.46386" genericHeader="method">
9. Applicability of Supertagging to other Lexicalized Grammars
</sectionHeader>
<bodyText confidence="0.999939214285714">
Although we have presented supertagging in the context of LTAG, it is applicable to
other lexicalized grammar formalisms such as CCG (Steedman 1997), HPSG (Pollard
and Sag 1987), and LFG (Kaplan and Bresnan 1983). We have implemented a broad
coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories
based on the XTAG English grammar. These categories have been used to tag the
same training and test corpora used in the supertagging experiments discussed in this
paper and a supertagger to disambiguate the CCG categories has been developed. We
are presently analyzing the performance of the supertagger using the LTAG trees and
the CCG categories.
The idea of supertagging can also be applied to a grammar in HPSG formalism
indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al.
1995). A more direct approach would be to tag words with feature structures that
represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used
in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags.
An approach that is closely related to supertagging is the reductionist approach to
parsing that is being carried out under the Constraint Grammar framework (Karlsson
et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each
word is associated with the set of possible functional tags that it may be assigned
in the language. This constitutes the lexicon. The grammar consists of a set of rules
that eliminate functional tags for words based on the context of a sentence. Parsing
a sentence in this framework amounts to eliminating as many implausible functional
tags as possible for each word, given the context of the sentence. The resultant out-
put structure might contain significant syntactic ambiguity, which may not have been
eliminated by the rule applications, thus producing almost parses. Thus, the reduc-
tionist approach to parsing is similar to supertagging in that both view parsing as
tagging with rich descriptions. However, the key difference is that the tagging is done
in a probabilistic setting in the supertagging approach while it is rule based in the
constraint grammar approach.
</bodyText>
<page confidence="0.993658">
255
</page>
<note confidence="0.642455">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.99915625">
We are currently developing supertaggers for other languages. In collaboration
with Anne Abeille and Marie-Helene Candito of the University of Paris, using their
French TAG grammar, we have developed a supertagger for French. We are currently
working on evaluating the performance of this supertagger. Also, the annotated cor-
pora necessary for training supertaggers for Korean and Chinese are under develop-
ment at the University of Pennsylvania.
A version of the supertagger trained on the WSJ corpus is available under GNU
Public License from http: / / www.cis.upenn.edu / —xtag / swrelease.html.
</bodyText>
<sectionHeader confidence="0.912482" genericHeader="method">
10. Conclusions
</sectionHeader>
<bodyText confidence="0.999917666666667">
In this paper, we have presented a novel approach to robust parsing distinguished from
the previous approaches to robust parsing by integrating the flexibility of linguistically
motivated lexical descriptions with the robustness of statistical techniques. By associat-
ing rich descriptions (supertags) that impose complex constraints in a local context, we
have been able to use local computational models for effective supertag disambigua-
tion. A trigram supertag disambiguation model, trained on 1,000,000 (word, supertag)
pairs of the Wall Street Journal corpus, performs at an accuracy level of 92.2%. After
disambiguation, we have effectively completed the parse of the sentence, creating an
almost parse, in that the parser need only combine the selected structures to arrive at
a parse for the sentence. We have presented a lightweight dependency analyzer (LDA)
that takes the output of the supertagger and uses the dependency requirements of the
supertags to produce a dependency linkage for a sentence. This method can also serve
to parse sentence fragments in cases where the supertag sequence after disambigua-
tion may not combine to form a single structure. This approach is applicable to all
lexicalized grammar parsers.
</bodyText>
<subsectionHeader confidence="0.503708">
Appendix A: Feature-based Lexicalized Tree Adjoining Grammar
</subsectionHeader>
<bodyText confidence="0.999506045454545">
Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting gram-
mar formalism, unlike context-free Grammars and head grammars, which are string-
rewriting formalisms. FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs),
which were first developed in Joshi, Levy, and Takahashi (1975) and later extended
to include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker and
Joshi 1991) and lexicalization (Schabes, Abeille, and Joshi 1988). For a more recent and
comprehensive reference, see Joshi and Schabes (1996).
The primitive elements of FB-LTAGs are called elementary trees. Each elemen-
tary tree is associated with at least one lexical item on its frontier. The lexical item
associated with an elementary tree is called the anchor of that tree. An elementary
tree serves as a complex description of the anchor and provides a domain of locality
over which the anchor can specify syntactic and semantic (predicate argument) con-
straints. Elementary trees are of two kinds: (a) Initial Trees and (b) Auxiliary Trees. In
an FB-LTAG grammar for natural language, initial trees are phrase structure trees of
simple sentences containing no recursion, while recursive structures are represented
by auxiliary trees.
Examples of initial trees (as) and auxiliary trees (i3s) are shown in Figure 6. Nodes
on the frontier of initial trees are marked as substitution sites by a &amp;quot;i&amp;quot;, while exactly
one node on the frontier of an auxiliary tree, whose label matches the label of the root
of the tree, is marked as a foot node by a &amp;quot;*&amp;quot;. The other nodes on the frontier of an
auxiliary tree are marked as substitution sites.
Each node of an elementary tree is associated with two feature structures (FS),
</bodyText>
<page confidence="0.994485">
256
</page>
<figure confidence="0.695659470588235">
Bangalore and Joshi
Supertagging
S, mode: ind/imp]
[agr : &lt;3&gt; )1
mode: &lt;4
NP
[agr : &lt;1)]
VP agr : &lt;3&gt; 31
mode &lt;4&gt; (
{
agr : &lt;1&gt;
mode : &lt;2&gt;1
NPri- [agr : &lt;3&gt; (1
Detfq [agr: &lt;1&gt; [1
N [agr : &lt;1&gt;]
[agr : [3rdsing : +
pert: 3
nuns: sing
company
V {passive : +
agr :
mode : &lt;2&gt; ppartl
mode: pparti
acquired
DetP]
[agr : &lt;1,1
D [agr &lt;i&gt; J]
agr: nuns: sing/Our]
pers : 3
the
a2 a3
VP, n
iagr : &lt;1&gt;
mode: &lt;2&gt;
</figure>
<table confidence="0.992618222222222">
V agr : &lt;1&gt; VP* []
mode: &lt;2 [mode: gel]
mode: ind V agr: &lt;1&gt;
agr: pers : 3 mode : &lt;2
num: sing [mode: get]
{
3rdsing : + being
is
B1 B2
</table>
<figureCaption confidence="0.753053">
Figure 6
</figureCaption>
<bodyText confidence="0.92151375">
Elementary trees for the sentence: the company is being acquired.
the top and the bottom. The bottom FS contains information relating to the subtree
rooted at the node, and the top FS contains information relating to the supertree at
that node.13 Features may get their values from three different sources:
</bodyText>
<listItem confidence="0.999363">
• Morphology of anchor: from the morphological information of the lexical
items that anchor the tree.
• Structural characteristics: from the structure of the tree itself (for
</listItem>
<page confidence="0.885225">
13 Nodes marked for substitution are associated with only the top FS.
</page>
<figure confidence="0.930183571428571">
VP,[]
Iagr : &lt;1&gt; [1 1
mode: &lt;2&gt; [1
VP* E
[mode: ppart]
257
Computational Linguistics Volume 25, Number 2
</figure>
<figureCaption confidence="0.847945">
Figure 7
Substitution and adjunction in LTAG.
example, the mode = id/imp feature on the root node in the a3 tree in
Figure 6).
</figureCaption>
<bodyText confidence="0.985922828571428">
• The derivation process: from unification with features from trees that
adjoin or substitute.
Elementary trees are combined by substitution and adjunction operations. Sub-
stitution inserts elementary trees at the substitution nodes of other elementary trees.
Figure 7(a) shows two elementary trees and the tree resulting from the substitution
of one tree into the other. In this operation, a node marked for substitution in an
elementary tree is replaced by another elementary tree whose root label matches the
label of the node. The top FS of the resulting node is the result of unification of the
top features of the two original nodes, while the bottom FS of the resulting node is
simply the bottom features of the root node of the substituting tree.
In an adjunction operation, an auxiliary tree is inserted into an elementary tree.
Figure 7(b) shows an auxiliary tree adjoining into an elementary tree and the result
of the adjunction. The root and foot nodes of the auxiliary tree must match the node
label at which the auxiliary tree adjoins. The node being adjoined to splits, and its top
FS unifies with the top FS of the root node of the auxiliary tree, while its bottom FS
unifies with the bottom FS of the foot node of the auxiliary tree. Figure 7(b) shows
an auxiliary tree and an elementary tree, and the tree resulting from an adjunction
operation. For a parse to be well-formed, the top and bottom FS at each node should
be unified at the end of a parse.
The result of combining the elementary trees shown in Figure 6 is the derived
tree, shown in Figure 8(a). The process of combining the elementary trees to yield a
parse of the sentence is represented by the derivation tree, shown in Figure 8(b). The
nodes of the derivation tree are the tree names that are anchored by the appropriate
lexical items. The combining operation is indicated by the type of the arcs (a broken
line indicates substitution and a bold line indicates adjunction) while the address of
the operation is indicated as part of the node label. The derivation tree can also be
interpreted as a dependency tree with unlabeled arcs between words of the sentence,
as shown in Figure 8(c).
A broad-coverage grammar system, XTAG, has been implemented in the LTAG
formalism. In this section, we briefly discuss some aspects related to XTAG for the
sake of completeness. A more detailed report on XTAG can be found in XTAG-Group
(1995). The XTAG system consists of a morphological analyzer, a part-of-speech tag-
ger, a wide-coverage LTAG English grammar, a predictive left-to-right Earley-style
parser for LTAG (Schabes 1990), and an X-windows interface for grammar develop-
ment (Doran et al. 1994). The input sentence is subjected to morphological analysis
</bodyText>
<figure confidence="0.969814551724138">
(a)
(b)
bf
t U If
br
If
b U bl
258
Bangalore and Joshi Supertagging
the
agr : &lt;1&gt; 3rdsing
num: sing
pers : 3
mode : &lt;2&gt; Ind
VPr{agr : &lt;1&gt;
mode : &lt;2
DeiP [agr : &lt;1,-] N [agr : &lt;1&gt;] V [agr : &lt;1&gt; VP [agr : &lt;3&gt; j &lt;5&gt; [1
D tagr : &lt;IA company mode: &lt;2&gt;i Ingr : &lt;3&gt; mode : &lt;4&gt; ge &lt;6&gt; ppartl
is V mode: &lt;4&gt; VP [agr :
mode:
being V {passive :
agr : &lt;5&gt;
mode: &lt;6&gt;
acquired
(a)
anx1V [acquired] acquired
aNXdxN [company] I3Vvx [being] company being
aDXD [the] 13Vvx [is] the is
(b) (c)
</figure>
<figureCaption confidence="0.707782">
Figure 8
(a) Derived tree, (b) derivation tree, and (c) dependency tree for the sentence: the company is
being acquired.
</figureCaption>
<bodyText confidence="0.996863666666667">
and is tagged with parts of speech before being sent to the parser. The parser retrieves
the elementary trees that the words of the sentence anchor and combines them by
adjunction and substitution operations to derive a parse of the sentence. The gram-
mar of XTAG has been used to parse sentences from ATIS, IBM Manual and WSJ
corpora (TAG-Group 1995). The resulting XTAG corpus contains sentences from these
domains along with all the derivations for each sentence. The derivations provide
</bodyText>
<page confidence="0.994565">
259
</page>
<table confidence="0.306088333333333">
Computational Linguistics Volume 25, Number 2
predicate argument relationships for the parsed sentences.
Appendix B: Key Properties of LTAGs
</table>
<bodyText confidence="0.9985035">
In this section, we define the key properties of LTAGs: lexicalization, Extended Domain
of Locality (EDL), and factoring of recursion from the domain of dependency (FRD),
and discuss how these properties are realized in natural language grammars written
in LTAGs. A more detailed discussion about these properties is presented in Joshi
(1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and
Schabes (1996).
</bodyText>
<subsectionHeader confidence="0.952166">
Definition
</subsectionHeader>
<bodyText confidence="0.994808">
A grammar is lexicalized if it consists of:
</bodyText>
<listItem confidence="0.9990442">
• a finite set of elementary structures (strings, trees, directed acyclic
graphs, etc.), each structure anchored on a lexical item.
• lexical items, each associated with at least one of the elementary
structures of the grammar
• a finite set of operations combining these structures.
</listItem>
<bodyText confidence="0.9997715">
This property proves to be linguistically crucial since it establishes a direct link
between the lexicon and the syntactic structures defined in the grammar. In fact, in lex-
icalized grammars all we have is the lexicon, which projects the elementary structures
of each lexical item; there is no independent grammar.
</bodyText>
<subsectionHeader confidence="0.921305">
Definition
</subsectionHeader>
<bodyText confidence="0.990558">
The Extended Domain of Locality (EDL) property has two parts:
</bodyText>
<listItem confidence="0.951767">
1. Every elementary structure must contain all and only the arguments of
the anchor in the same structure.
2. For each lexical item, the grammar must contain an elementary structure
for each syntactic environment the lexical item might appear in.
</listItem>
<bodyText confidence="0.5996904">
Part (1) of EDL allows the anchor to impose syntactic and semantic constraints on
its arguments directly since they appear in the same elementary structure that it an-
chors. Hence, all elements that appear within one elementary structure are considered
to be local. This property also defines how large an elementary structure in a grammar
can be. Figure 9 shows trees for the following example sentences:
</bodyText>
<listItem confidence="0.999617">
(1) John seems to like Mary.
(2) John hit Mary.
(3) Who did John hit?
</listItem>
<bodyText confidence="0.977061666666667">
Figure 9(a) shows the elementary tree anchored by seem that is used to derive a raising
analysis for sentence 1. Notice that the elements appearing in the tree are only those
that serve as arguments to the anchor and nothing else. In particular, the subject NP
</bodyText>
<page confidence="0.979219">
260
</page>
<figure confidence="0.992202666666667">
Bangalore and Joshi Supertagging
VPfin
V VPinf*
seems
Sr NPI Sr
NP01 VP
/\
V NP/
NA
hit
NP01 VP
/\
V NPrl.
hit
(a) (b) (c)
</figure>
<figureCaption confidence="0.466126333333333">
Figure 9
(a) Tree for raising analysis, anchored by seems; (b) transitive tree; (c) object extraction tree for
the verb hit.
</figureCaption>
<bodyText confidence="0.999382954545455">
(John in sentence 1) does not appear in the elementary tree for seem since it does not
serve as an argument for seem. Figure 9(b) shows the elementary tree anchored by the
transitive verb hit in which both the subject NP and object NP are realized within the
same elementary tree.
LTAG is distinguished from other grammar formalisms by possessing part (2) of
the EDL property. In LTAGs, there is one elementary tree for every syntactic environ-
ment that the anchor may appear in. Each elementary tree encodes the linear order
of the arguments of the anchor in a particular syntactic environment. For example, a
transitive verb such as hit is associated with both the elementary tree shown in Fig-
ure 9(b) for a declarative transitive sentence such as sentence 2, and the elementary
tree shown in Figure 9(c) for an object extracted transitive sentence such as sentence 3.
Notice that the object noun phrase is realized to the left of the subject noun phrase in
the object extraction tree.
As a consequence of the fact that LTAGs possess the part (2) of the EDL property,
the derivation structures in LTAGs contain the information of a dependency structure.
Another aspect of EDL is that the arguments of the anchor can be filled in any order.
This is possible because the elementary structures allocate a slot for each argument of
the anchor in each syntactic environment that the anchor appears in.
There can be many ways of constructing the elementary structures of a grammar so
as to possess the EDL property. However, by requiring that the constructed elementary
structures be &amp;quot;minimal,&amp;quot; the third property of LTAGs namely, factoring of recursion
from the domain of dependencies, follows as a corollary of EDL.
</bodyText>
<sectionHeader confidence="0.770763" genericHeader="conclusions">
Definition
</sectionHeader>
<bodyText confidence="0.999721285714286">
Factoring of recursion from the domain of dependencies (FRD): Recursion is factored
away from the domain for the statement of dependencies.
In LTAGs, recursive constructs are represented as auxiliary trees. They combine
with elementary trees by the operation of adjunction. Elementary trees define the
domain for stating dependencies such as agreement, subcategorization, and filler-gap
dependencies. Auxiliary trees, by adjunction to elementary trees, account for the long-
distance behavior of these dependencies.
</bodyText>
<page confidence="0.988228">
261
</page>
<note confidence="0.702976">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.999666833333333">
An additional advantage of a grammar possessing FRD and EDL properties is that
feature structures in these grammars are extremely simple. Since the recursion has been
factored out of the domain of dependency, and since the domain is large enough for
agreement, subcategorization, and filler-gap dependencies, feature structures in such
systems do not involve any recursion. In fact they reduce to typed terms that can be
combined by simple term-like unification.
</bodyText>
<sectionHeader confidence="0.983389" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.937570285714286">
This work was done when the first author
was at the University of Pennsylvania. It
was partially supported by NSF grant
NSF-STC SBR 8920230, ARPA grant
N00014-94 and ARO grant
DAAH04-94-G0426. We would like to thank
Steven Abney, Raman Chandrasekar,
Christine Doran, Beth Ann Hockey, Mark
Liberman, Mitch Marcus, and Mark
Steedman for useful comments and
discussions which have helped shape this
work. We also thank the reviewers for their
insightful comments and suggestions to
improve an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.989978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998296860465116">
Abney, Steven. 1990. Rapid incremental
parsing with repair. In Proceedings of the
6th New OED Conference: Electronic Text
Research, pages 1-9, University of
Waterloo, Waterloo, Ontario, Canada.
Alshawi, Hiyan and David Carter. 1994.
Training and scaling preference functions
for disambiguation. Computational
Linguistics, 20(4):635-648.
Appelt, D., J. Hobbs, J. Bear, D. J. Israel, and
M. Tyson. 1993. FASTUS: A finite-state
processor for information extraction from
real-world text. In Proceedings of IJCAI-93,
Chambery, France, September.
Black, Ezra, Fred Jelinek, John Lafferty,
David M. Magerman, Robert Mercer, and
Salim Roukos. 1993. Towards
History-based Grammars: Using Richer
Models for Probabilistic Parsing. In
Proceedings of the 31st Annual Meeting,
pages 31-37, Columbus, OH. Association
for Computational Linguistics.
Bod, Rens and Ronald Kaplan. 1998. A
probabilistic corpus-driven model for
lexical-functional analysis. In Proceedings
of COLING-ACL &apos;98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, Montreal,
Quebec, Canada, August.
Brill, Eric. 1993. Automatic grammar
induction and parsing free text: A
transformation-based approach. In
Proceedings of the 31st Annual Meeting,
Columbus, OH. Association for
Computational Linguistics.
Chandrasekar, R., Christine Doran, and B.
Srinivas. 1996. Motivations and methods
for text simplification. In Proceedings of the
16th International Conference on
Computational Linguistics (COLING&apos;96),
Copenhagen, Denmark, August.
Chandrasekar, R. and B. Srinivas. 1997a.
Automatic induction of rules for text
simplification. Knowledge-based Systems,
10:183-190.
Chandrasekar, R. and B. Srinivas. 1997b.
Gleaning information from the web:
Using syntax to filter out irrelevant
information. In Proceedings of AAAI 1997
Spring Symposium on NLP on the World Wide
Web.
Chandrasekar, R. and B. Srinivas. 1997c.
Using supertags in document filtering:
The effect of increased context on
information retrieval effectiveness. In
Proceedings of Recent Advances in NLP
(RANLP) &apos;97, Tzigov Chark, Bulgaria,
September.
Chandrasekar, R. and B. Srinivas. 1997d.
Using syntactic information in document
filtering: A comparative study of
part-of-speech tagging and supertagging.
In Proceedings of RIA0&apos;97, Montreal,
Quebec, Canada, June.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelligence
AAAI, pages 47-66, Menlo Park, CA.
Chomsky, Noam. 1992. A Minimalist
Approach to Linguistic Theory. MIT Working
Papers in Linguistics, Occasional Papers
in Linguistics, No. 1.
Church, Kenneth Ward. 1988. A stochastic
parts program and noun phrase parser
for unrestricted text. In 2nd Applied
Natural Language Processing Conference,
pages 136-143, Austin, TX.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting, Santa Cruz, CA.
Association for Computational
Linguistics.
Doran, Christine, Dania Egedi, Beth Ann
</reference>
<page confidence="0.99184">
262
</page>
<note confidence="0.780801">
Bangalore and Joshi Supertagging
</note>
<reference confidence="0.997477426229509">
Hockey, B. Srinivas, and Martin Zaidel.
1994. XTAG System—A wide coverage
grammar for English. In Proceedings of the
17th International Conference on
Computational Linguistics (COLING&apos;94),
Kyoto, Japan, August.
Doran, Christine, Michael Niv, Breckenridge
Baldwin, Jeffrey Reynar, and B. Srinivas.
1997. Mother of Pen: A Multi-tier pattern
description language. In Proceedings of the
International Workshop on Lexically Driven
Information Extraction, Frascati, Italy, July.
Doran, Christine and B. Srinivas. 1994. A
wide-coverage CCG parser. In Proceedings
of the 3rd TAG+ Conference, Paris, France.
Fujisaki, T., F. Jelinek, J. Cocke, E. Black,
and T. Nishino. 1989. A probabilistic .
parsing method for sentence
disambiguation. In Proceedings of the 1st
Annual International Workshop of Parsing
Technologies, Pittsburgh, PA.
Gazdar, G., E. Klein, G. Pullum, and I. Sag.
1985. Generalized Phrase Structure Grammar.
Harvard University Press, Cambridge,
MA.
Good, I. J. 1953. The population frequenceis
of species and the estimation of
population parameters. Biometrika 40 (3
and 4), pages 237-264.
Grishman, Ralph. 1995. Where&apos;s the syntax?
The New York University MUC-6 System.
In Proceedings of the Sixth Message
Understanding Conference, Columbia, MD.
Gross, Maurice. 1984. Lexicon-grammar and
the syntactic analysis of French. In
Proceedings of the 10th International
Conference on Computational Linguistics
(COLING&apos;84), Stanford, CA.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded finite-state transducer for
extracting information from
natural-language text. In E. Roche and Y.
Schabes, editors, Finite State Devices for
Natural Language Processing. MIT Press,
Cambridge, MA.
Hobbs, Jerry R., Douglas E. Appelt, John
Bear, David Israel, Andy Kehler, Megumi
Kamayama, David Martin, Karen Myers,
and Mabry Tyson. 1995. SRI International
FASTUS system MUC-6 test results and
analysis. In Proceedings of the Sixth Message
Understanding Conference, Columbia, MD.
Jelinek, Fred, John Lafferty, David M.
Magerman, Robert Mercer, Adwait
Ratnaparkhi, and Salim Roukos. 1994.
Decision tree parsing using a hidden
derivation model. In Proceedings from the
ARPA Workshop on Human Language
Technology Workshop, March.
Joshi, Aravind K. 1960. Computation of
syntactic structure. In Advances in
Documentation and Library Science,
volume III, Part 2. Interscience Publishers,
Inc., NY.
Joshi, Aravind K. 1985. Tree adjoining
grammars: How much context sensitivity
is required to provide a reasonable
structural description? In D. Dowty, I.
Karttunen, and A. Zwicky, editors, Natural
Language Parsing. Cambridge University
Press, Cambridge, U.K., pages 206-250.
Joshi, Aravind K. 1987. An introduction to
tree adjoining grammars. In A. Manaster
Ramer, editor, Mathematics of Language.
John Benjamins, Amsterdam.
Joshi, Aravind K. 1998. Role of constrained
computational systems in natural
language processing. Artificial Intelligence,
103:117-132.
Joshi, Aravind K. and Philip Hopely. 1997.
A parser from antiquity. Natural Language
Engineering, 2(4).
Joshi, Aravind K., L. Levy, and M.
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences.
Joshi, Aravind K. and Yves Schabes, 1996.
Tree-adjoining grammars. In Handbook of
Formal Languages and Automata.
Springer-Verlag, Berlin.
Joshi, Aravind K. and B. Srinivas. 1994.
Disambiguation of super parts of speech
(or supertags): Almost parsing. In
Proceedings of the 15th International
Conference on Computational Linguistics
(COLING&apos;94), Kyoto, Japan, August.
Kaplan, Ronald and Joan Bresnan. 1983.
Lexical-functional grammar: A formal
system for grammatical representation. In
J. Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, MA.
Karlsson, F., A. Voutilainen, J. Heikkila, and
A. Anttila. 1994. Constraint Grammar: A
Language-Independent System for Parsing
Unrestricted Text. Mouton de Gruyter,
Berlin and NY.
Karttunen, L. J-P. Chanod, G. Grefenstette,
and A. Schiller. 1997. Regular expressions
for language engineering. Natural
Language Engineering, 2(4).
Kasper, Robert, Bernd Kiefer, Klaus Netter,
and K. Vijay-Shanker. 1995. Compilation
of HPSG to TAG. In Proceedings of the 33rd
Annual Meeting, Cambridge, MA.
Association for Computational
Linguistics.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
</reference>
<page confidence="0.965693">
263
</page>
<note confidence="0.361562">
Computational Linguistics Volume 25, Number 2
</note>
<reference confidence="0.999207991803279">
Speech and SignalProcessing, 35(3):400-401.
Kempe, Andre. 1994. Probabilistic Tagging
with Feature Structures. In Proceedings of
the 15th International Conference on
Computational Linguistics (COLING&apos;94),
Kyoto, Japan, August.
Kroch, Anthony S. and Aravind K. Joshi.
1985. The linguistic relevance of tree
adjoining grammars. Technical Report
MS-CIS-85-16, Department of Computer
and Information Science, University of
Pennsylvania.
Kuno, S. 1966. Harvard predictive analyzer.
In David G. Hays, editor, Readings in
Automatic Language Processing. American
Elsevier Pub. Co., NY.
Magerman, David M. 1995. Statistical
decision-tree models for parsing. In
Proceedings of the 33rd Annual Meeting.
Association for Computational
Linguistics.
Marcus, Mitchell M., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313-330.
Nagao, Makoto. 1994. Varieties of heuristics
in sentence processing. In Current Issues in
Natural Language Processing: In Honour of
Don Walker. Giardini with Kluwer.
Ney, Herman, Ute Essen, and Reinhard
Kneser. 1995. On the estimation of &apos;small&apos;
probabilities by leaving-one-out. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 17(2).
Niesler, T. R. and P. C. Woodland. 1996. A
variable-length category-based n-gram
language model. In Proceedings, IEEE
ICASSP.
Pollard, Carl and Ivan A. Sag. 1987.
Information-Based Syntax and Semantics.
Vol. 1: Fundamentals. CSLI.
Roche, Emmanuel. 1993. Analyse syntaxique
transformationelle du francais par
transducteurs et lexique-grammaire. Ph.D.
thesis, Universite Paris 7.
Schabes, Yves. 1990. Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, Computer Science
Department, University of Pennsylvania.
Schabes, Yves, Anne Abeilld, and Aravind
K. Joshi. 1988. Parsing strategies with
lexicalized&apos; grammars: Application to
Tree Adjoining Grammars. In Proceedings
of the 12th International Conference on
Computational Linguistics (COLING&apos;88),
Budapest, Hungary, August.
Schabes, Yves and Aravind K. Joshi. 1991.
Parsing with lexicalized tree adjoining
grammar. In M. Tomita, editor, Current
Issues in Parsing Technologies. Kluwer
Academic Publishers.
Schabes, Y, M. Roth, and R. Osborne. 1993.
Parsing the Wall Street Journal with the
inside-outside algorithm. In Proceedings of
the European ACL.
Sleator, Daniel and Davy Temperley. 1991.
Parsing English with a Link Grammar.
Technical Report CMU-CS-91-196,
Department of Computer Science,
Carnegie Mellon University.
Srinivas, B. 1996. &amp;quot;Almost parsing&amp;quot;
technique for language modeling. In
Proceedings of ICSLP96 Conference,
Philadelphia, PA.
Srinivas, B. 1997a. Complexity of Lexical
Descriptions and its Relevance to Partial
Parsing. Ph.D. thesis, University of
Pennsylvania.
Srinivas, B. 1997b. Performance evaluation
of supertagging for partial parsing. In
Proceedings of the International Workshop on
Parsing Technologies, September.
Srinivas, B., Christine Doran, and Seth
Kulick. 1995. Heuristics and parse
ranking. In Proceedings of the 4th Annual
International Workshop on Parsing
Technologies, Prague, September.
Steedman, Mark. 1987. Combinatory
grammars and parasitic gaps. Natural
Language and Linguistic Theory, 5:403-439.
Steedman, Mark, editor. 1997. The Syntactic
Interface. MIT Press, Cambridge, MA and
London, England.
Tapanainen, Pasi and Timo jarvinen. 1994.
Syntactic analysis of natural language
using linguistic rules and corpus-based
patterns. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING&apos;94), Kyoto, Japan,
August.
Vijay-Shanker, K. 1987. A Study of Tree
Adjoining Grammars. Ph.D. thesis,
Department of Computer and Information
Science, University of Pennsylvania.
Vijay-Shanker, K. and Aravind K. Joshi.
1991. Unification based tree adjoining
grammars. In J. Wedekind, editor,
Unification-based Grammars. MIT Press,
Cambridge, MA.
Voutilainen, Atro. 1994. Designing a Parsing
Grammar. Publications of the Department
of General Linguistics, University of
Helsinki.
Waltz, D. 1975. Understanding line
drawings of scenes with shadows. In P.
Winston, editor, Psychology of Computer
Vision, MIT Press.
Weischedel, Ralph, Richard Schwartz, Jeff
Palmucci, Marie Meteer, and Lance
Ramshaw. 1993. Coping with ambiguity
and unknown words through
</reference>
<page confidence="0.979337">
264
</page>
<reference confidence="0.90750975">
Bangalore and Joshi Supertagging
probabilistic models. Computational adjoining grammar for English. Technical
Linguistics, 19(2):359-382, June. Report IRCS 95-03, University of
XTAG-Group, The. 1995. A lexicalized tree Pennsylvania.
</reference>
<page confidence="0.99837">
265
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.724372">
<title confidence="0.995557">Supertagging: An Approach to Almost Parsing</title>
<author confidence="0.800702">K Aravind</author>
<affiliation confidence="0.836591">AT&amp;T Labs — Research University of Pennsylvania</affiliation>
<abstract confidence="0.99630394117647">In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need &amp;quot;only&amp;quot; combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Rapid incremental parsing with repair.</title>
<date>1990</date>
<booktitle>In Proceedings of the 6th New OED Conference: Electronic Text Research,</booktitle>
<pages>1--9</pages>
<institution>University of Waterloo,</institution>
<location>Waterloo, Ontario, Canada.</location>
<contexts>
<context position="5896" citStr="Abney (1990)" startWordPosition="882" endWordPosition="883"> Section 8. In Section 9, we will discuss the applicability of supertag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers alway</context>
</contexts>
<marker>Abney, 1990</marker>
<rawString>Abney, Steven. 1990. Rapid incremental parsing with repair. In Proceedings of the 6th New OED Conference: Electronic Text Research, pages 1-9, University of Waterloo, Waterloo, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>David Carter</author>
</authors>
<title>Training and scaling preference functions for disambiguation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="9023" citStr="Alshawi and Carter (1994)" startWordPosition="1354" endWordPosition="1357">es are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Tempe</context>
</contexts>
<marker>Alshawi, Carter, 1994</marker>
<rawString>Alshawi, Hiyan and David Carter. 1994. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635-648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
<author>J Hobbs</author>
<author>J Bear</author>
<author>D J Israel</author>
<author>M Tyson</author>
</authors>
<title>FASTUS: A finite-state processor for information extraction from real-world text.</title>
<date>1993</date>
<booktitle>In Proceedings of IJCAI-93,</booktitle>
<location>Chambery, France,</location>
<contexts>
<context position="5918" citStr="Appelt et al. (1993)" startWordPosition="884" endWordPosition="887"> Section 9, we will discuss the applicability of supertag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, </context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Tyson, 1993</marker>
<rawString>Appelt, D., J. Hobbs, J. Bear, D. J. Israel, and M. Tyson. 1993. FASTUS: A finite-state processor for information extraction from real-world text. In Proceedings of IJCAI-93, Chambery, France, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David M Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards History-based Grammars: Using Richer Models for Probabilistic Parsing.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>31--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="8982" citStr="Black et al. (1993)" startWordPosition="1348" endWordPosition="1351"> language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Jos</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993</marker>
<rawString>Black, Ezra, Fred Jelinek, John Lafferty, David M. Magerman, Robert Mercer, and Salim Roukos. 1993. Towards History-based Grammars: Using Richer Models for Probabilistic Parsing. In Proceedings of the 31st Annual Meeting, pages 31-37, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
<author>Ronald Kaplan</author>
</authors>
<title>A probabilistic corpus-driven model for lexical-functional analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="51240" citStr="Bod and Kaplan 1998" startWordPosition="8320" endWordPosition="8323"> and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags. An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language. This constitutes the lexicon. The grammar consists of a set of rules that eliminate functional tags for words based on the context of a sentence. Parsing a sentence in this framework amounts to eliminating as many impla</context>
</contexts>
<marker>Bod, Kaplan, 1998</marker>
<rawString>Bod, Rens and Ronald Kaplan. 1998. A probabilistic corpus-driven model for lexical-functional analysis. In Proceedings of COLING-ACL &apos;98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="11831" citStr="Brill 1993" startWordPosition="1781" endWordPosition="1782">ees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the dependent elem</context>
<context position="41528" citStr="Brill 1993" startWordPosition="6766" endWordPosition="6767">upertagging is attractive for limited domains since it performs quite well with relatively insignificant amounts of training material. The performance of the supertagger can be improved in an iterative fashion by using the supertagger to supertag larger amounts of training material, which can be quickly hand-corrected and used to train a better-performing supertagger. 6.5.2 Effect of Lexical versus Contextual Information. Lexical information contributes most to the performance of a POS tagger, since the baseline performance of assigning the most likely POS for each word produces 91% accuracy (Brill 1993). Contextual information contributes relatively a small amount towards the performance, improving it from 91% to 96-97%, a 5.5% improvement. In contrast, contextual information has greater effect on the performance of the supertagger. As can be seen, from the above experiments, the baseline performance of the supertagger is about 77% and the performance improves to about 92% with the inclusion of contextual information, an 252 Bangalore and joshi Supertagging improvement of 19.5%. The relatively low baseline performance for the supertagger is a direct consequence of the fact that there are man</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Brill, Eric. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of the 31st Annual Meeting, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>Christine Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96),</booktitle>
<location>Copenhagen, Denmark,</location>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>Chandrasekar, R., Christine Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96), Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. Knowledge-based Systems,</title>
<date>1997</date>
<pages>10--183</pages>
<contexts>
<context position="43890" citStr="Chandrasekar and Srinivas 1997" startWordPosition="7131" endWordPosition="7134">defined on a three-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Chandrasekar, R. and B. Srinivas. 1997a. Automatic induction of rules for text simplification. Knowledge-based Systems, 10:183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Gleaning information from the web: Using syntax to filter out irrelevant information.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI 1997 Spring Symposium on NLP on the World Wide Web.</booktitle>
<contexts>
<context position="43890" citStr="Chandrasekar and Srinivas 1997" startWordPosition="7131" endWordPosition="7134">defined on a three-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Chandrasekar, R. and B. Srinivas. 1997b. Gleaning information from the web: Using syntax to filter out irrelevant information. In Proceedings of AAAI 1997 Spring Symposium on NLP on the World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Using supertags in document filtering: The effect of increased context on information retrieval effectiveness.</title>
<date>1997</date>
<booktitle>In Proceedings of Recent Advances in NLP (RANLP) &apos;97,</booktitle>
<location>Tzigov Chark, Bulgaria,</location>
<contexts>
<context position="43890" citStr="Chandrasekar and Srinivas 1997" startWordPosition="7131" endWordPosition="7134">defined on a three-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Chandrasekar, R. and B. Srinivas. 1997c. Using supertags in document filtering: The effect of increased context on information retrieval effectiveness. In Proceedings of Recent Advances in NLP (RANLP) &apos;97, Tzigov Chark, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Using syntactic information in document filtering: A comparative study of part-of-speech tagging and supertagging.</title>
<date>1997</date>
<booktitle>In Proceedings of RIA0&apos;97,</booktitle>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="43890" citStr="Chandrasekar and Srinivas 1997" startWordPosition="7131" endWordPosition="7134">defined on a three-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Chandrasekar, R. and B. Srinivas. 1997d. Using syntactic information in document filtering: A comparative study of part-of-speech tagging and supertagging. In Proceedings of RIA0&apos;97, Montreal, Quebec, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence AAAI,</booktitle>
<pages>47--66</pages>
<location>Menlo Park, CA.</location>
<contexts>
<context position="7980" citStr="Charniak (1997)" startWordPosition="1193" endWordPosition="1194">ical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This all</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence AAAI, pages 47-66, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>A Minimalist Approach to Linguistic Theory. MIT Working Papers in Linguistics, Occasional Papers in Linguistics,</title>
<date>1992</date>
<volume>1</volume>
<contexts>
<context position="9672" citStr="Chomsky 1992" startWordPosition="1459" endWordPosition="1460">995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and J</context>
</contexts>
<marker>Chomsky, 1992</marker>
<rawString>Chomsky, Noam. 1992. A Minimalist Approach to Linguistic Theory. MIT Working Papers in Linguistics, Occasional Papers in Linguistics, No. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In 2nd Applied Natural Language Processing Conference,</booktitle>
<pages>136--143</pages>
<location>Austin, TX.</location>
<contexts>
<context position="11794" citStr="Church 1988" startWordPosition="1775" endWordPosition="1776">ted by auxiliary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring</context>
<context position="31265" citStr="Church 1988" startWordPosition="5042" endWordPosition="5043"> word and use the most frequent supertag associated with that part of speech as the supertag for the word. 248 Bangalore and Joshi Supertagging Table 4 Results from the unigram supertag model. Data Set Training Set Test Set Top n Supertags % Success XTAG Parses 8,000 3,000 n = 1 73.4% n = 2 80.2% n = 3 80.8% Converted Penn Treebank Parses 1,000,000 47,000 n =1 77.2% n = 2 87.0% n = 3 91.5% 6.4.1 Experiments and Results. We tested the performance of the unigram model on the previously discussed two sets of data. The words are first assigned standard parts of speech using a conventional tagger (Church 1988) and then are assigned supertags according to the unigram model. A word in a sentence is considered correctly supertagged if it is assigned the same supertag as it is associated with in the correct parse of the sentence. The results of these experiments are tabulated in Table 4. Although the performance of the unigram model for supertagging is significantly lower than the performance of the unigram model for part-of-speech tagging (91% accuracy), it performed much better than expected considering the size of the supertag set is much larger than the size of part-of-speech tag set. One of the re</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth Ward. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In 2nd Applied Natural Language Processing Conference, pages 136-143, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="7959" citStr="Collins (1996)" startWordPosition="1190" endWordPosition="1191">tems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded i</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting, Santa Cruz, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christine Doran</author>
</authors>
<title>Dania Egedi,</title>
<location>Beth Ann</location>
<marker>Doran, </marker>
<rawString>Doran, Christine, Dania Egedi, Beth Ann</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas Hockey</author>
<author>Martin Zaidel</author>
</authors>
<title>XTAG System—A wide coverage grammar for English.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (COLING&apos;94),</booktitle>
<location>Kyoto, Japan,</location>
<marker>Hockey, Zaidel, 1994</marker>
<rawString>Hockey, B. Srinivas, and Martin Zaidel. 1994. XTAG System—A wide coverage grammar for English. In Proceedings of the 17th International Conference on Computational Linguistics (COLING&apos;94), Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>Michael Niv</author>
<author>Breckenridge Baldwin</author>
<author>Jeffrey Reynar</author>
<author>B Srinivas</author>
</authors>
<title>Mother of Pen: A Multi-tier pattern description language.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Lexically Driven Information Extraction,</booktitle>
<location>Frascati, Italy,</location>
<contexts>
<context position="43953" citStr="Doran et al. 1997" startWordPosition="7140" endWordPosition="7143">and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and selects all the supertags associated with each word of the sentence</context>
</contexts>
<marker>Doran, Niv, Baldwin, Reynar, Srinivas, 1997</marker>
<rawString>Doran, Christine, Michael Niv, Breckenridge Baldwin, Jeffrey Reynar, and B. Srinivas. 1997. Mother of Pen: A Multi-tier pattern description language. In Proceedings of the International Workshop on Lexically Driven Information Extraction, Frascati, Italy, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>B Srinivas</author>
</authors>
<title>A wide-coverage CCG parser.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd TAG+ Conference,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="50497" citStr="Doran and Srinivas 1994" startWordPosition="8199" endWordPosition="8202">n 200,000 words, produced 38,480 dependency links correctly, resulting in a recall score of 82.3%. Also, a total of 41,009 dependency links were produced by the LDA, resulting in a precision score of 93.8%. A detailed evaluation of the LDA is presented in Srinivas (199M). 9. Applicability of Supertagging to other Lexicalized Grammars Although we have presented supertagging in the context of LTAG, it is applicable to other lexicalized grammar formalisms such as CCG (Steedman 1997), HPSG (Pollard and Sag 1987), and LFG (Kaplan and Bresnan 1983). We have implemented a broad coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories based on the XTAG English grammar. These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature</context>
</contexts>
<marker>Doran, Srinivas, 1994</marker>
<rawString>Doran, Christine and B. Srinivas. 1994. A wide-coverage CCG parser. In Proceedings of the 3rd TAG+ Conference, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A probabilistic . parsing method for sentence disambiguation.</title>
<date>1989</date>
<booktitle>In Proceedings of the 1st Annual International Workshop of Parsing Technologies,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="7834" citStr="Fujisaki et al. 1989" startWordPosition="1169" endWordPosition="1172">e also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparen</context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>Fujisaki, T., F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. A probabilistic . parsing method for sentence disambiguation. In Proceedings of the 1st Annual International Workshop of Parsing Technologies, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9479" citStr="Gazdar et al. 1985" startWordPosition="1427" endWordPosition="1430">that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) a</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum, and I. Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequenceis of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika</journal>
<volume>40</volume>
<issue>3</issue>
<pages>237--264</pages>
<contexts>
<context position="34691" citStr="Good 1953" startWordPosition="5616" endWordPosition="5617">ITi_2, Ti_i) is known as the contextual probability since it indicates the size of the context used in the model and the term Pr(Wj T,) is called the word emit probability since it is the probability of emitting the word W, given the tag Ti. These probabilities are estimated using a corpus where each word is tagged with its correct supertag. The contextual probabilities were estimated using the relative frequency estimates of the contexts in the training corpus. To estimate the probabilities for contexts that do not appear in the training corpus, we used the Good-Turing discounting technique (Good 1953) combined with Katz&apos;s back off model (Katz 1987). The idea here is to discount the frequencies of events that occur in the corpus by an amount related to their frequencies and utilize this discounted probability mass in the back off model to distribute to unseen events. Thus, the Good-Turing discounting technique estimates the frequency of unseen events based on the distribution of the frequency of the counts of observed events in the corpus. If r is the observed frequency of an event, and N,. is the number of events with the observed frequency r, and N is the total number of events, then the </context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. J. 1953. The population frequenceis of species and the estimation of population parameters. Biometrika 40 (3 and 4), pages 237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Where&apos;s the syntax? The New York University MUC-6 System.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference,</booktitle>
<location>Columbia, MD.</location>
<contexts>
<context position="5949" citStr="Grishman (1995)" startWordPosition="890" endWordPosition="891">icability of supertag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch</context>
<context position="7742" citStr="Grishman 1995" startWordPosition="1157" endWordPosition="1158">riptions. Of course, Waltz did not use statistical information for disambiguation. See also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reaso</context>
</contexts>
<marker>Grishman, 1995</marker>
<rawString>Grishman, Ralph. 1995. Where&apos;s the syntax? The New York University MUC-6 System. In Proceedings of the Sixth Message Understanding Conference, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Lexicon-grammar and the syntactic analysis of French.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics (COLING&apos;84),</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="9559" citStr="Gross 1984" startWordPosition="1441" endWordPosition="1442">, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicaliz</context>
</contexts>
<marker>Gross, 1984</marker>
<rawString>Gross, Maurice. 1984. Lexicon-grammar and the syntactic analysis of French. In Proceedings of the 10th International Conference on Computational Linguistics (COLING&apos;84), Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Mark Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: A cascaded finite-state transducer for extracting information from natural-language text.</title>
<date>1997</date>
<booktitle>Finite State Devices for Natural Language Processing.</booktitle>
<editor>In E. Roche and Y. Schabes, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5970" citStr="Hobbs et al. (1997)" startWordPosition="892" endWordPosition="895">rtag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch heuristic to resolve</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1997</marker>
<rawString>Hobbs, Jerry R., Douglas Appelt, John Bear, David Israel, Megumi Kameyama, Mark Stickel, and Mabry Tyson. 1997. FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In E. Roche and Y. Schabes, editors, Finite State Devices for Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas E Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Andy Kehler</author>
<author>Megumi Kamayama</author>
<author>David Martin</author>
<author>Karen Myers</author>
<author>Mabry Tyson</author>
</authors>
<title>results and analysis.</title>
<date>1995</date>
<booktitle>SRI International FASTUS system MUC-6 test</booktitle>
<location>Columbia, MD.</location>
<contexts>
<context position="7726" citStr="Hobbs et al. 1995" startWordPosition="1153" endWordPosition="1156">sambiguate the descriptions. Of course, Waltz did not use statistical information for disambiguation. See also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kehler, Kamayama, Martin, Myers, Tyson, 1995</marker>
<rawString>Hobbs, Jerry R., Douglas E. Appelt, John Bear, David Israel, Andy Kehler, Megumi Kamayama, David Martin, Karen Myers, and Mabry Tyson. 1995. SRI International FASTUS system MUC-6 test results and analysis. In Proceedings of the Sixth Message Understanding Conference, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David M Magerman</author>
<author>Robert Mercer</author>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
</authors>
<title>Decision tree parsing using a hidden derivation model.</title>
<date>1994</date>
<booktitle>In Proceedings from the ARPA Workshop on Human Language Technology Workshop,</booktitle>
<contexts>
<context position="7926" citStr="Jelinek et al. (1994)" startWordPosition="1184" endWordPosition="1187">n position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probabil</context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>Jelinek, Fred, John Lafferty, David M. Magerman, Robert Mercer, Adwait Ratnaparkhi, and Salim Roukos. 1994. Decision tree parsing using a hidden derivation model. In Proceedings from the ARPA Workshop on Human Language Technology Workshop, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Computation of syntactic structure.</title>
<date>1960</date>
<booktitle>In Advances in Documentation and Library Science, volume III, Part 2. Interscience</booktitle>
<publisher>Publishers, Inc., NY.</publisher>
<contexts>
<context position="5882" citStr="Joshi, (1960)" startWordPosition="880" endWordPosition="881">ly presented in Section 8. In Section 9, we will discuss the applicability of supertag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These</context>
</contexts>
<marker>Joshi, 1960</marker>
<rawString>Joshi, Aravind K. 1960. Computation of syntactic structure. In Advances in Documentation and Library Science, volume III, Part 2. Interscience Publishers, Inc., NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree adjoining grammars: How much context sensitivity is required to provide a reasonable structural description? In</title>
<date>1985</date>
<pages>206--250</pages>
<editor>D. Dowty, I. Karttunen, and A. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.,</location>
<contexts>
<context position="61729" citStr="Joshi (1985" startWordPosition="10075" endWordPosition="10076"> XTAG corpus contains sentences from these domains along with all the derivations for each sentence. The derivations provide 259 Computational Linguistics Volume 25, Number 2 predicate argument relationships for the parsed sentences. Appendix B: Key Properties of LTAGs In this section, we define the key properties of LTAGs: lexicalization, Extended Domain of Locality (EDL), and factoring of recursion from the domain of dependency (FRD), and discuss how these properties are realized in natural language grammars written in LTAGs. A more detailed discussion about these properties is presented in Joshi (1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and Schabes (1996). Definition A grammar is lexicalized if it consists of: • a finite set of elementary structures (strings, trees, directed acyclic graphs, etc.), each structure anchored on a lexical item. • lexical items, each associated with at least one of the elementary structures of the grammar • a finite set of operations combining these structures. This property proves to be linguistically crucial since it establishes a direct link between the lexicon and the syntactic structures defined in the grammar. In f</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind K. 1985. Tree adjoining grammars: How much context sensitivity is required to provide a reasonable structural description? In D. Dowty, I. Karttunen, and A. Zwicky, editors, Natural Language Parsing. Cambridge University Press, Cambridge, U.K., pages 206-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In A. Manaster Ramer, editor,</editor>
<location>Amsterdam.</location>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind K. 1987. An introduction to tree adjoining grammars. In A. Manaster Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Role of constrained computational systems in natural language processing.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>103--117</pages>
<contexts>
<context position="7232" citStr="Joshi (1998)" startWordPosition="1084" endWordPosition="1085">ression 1 The use of descriptions for primitives to capture constraints locally has a precursor in Al. The Waltz algorithm (Waltz 1975) for labeling vertices of polygonal solid objects can be thought of in these terms. Waltz made the description of vertices more complex by including information about the incident edges, associated surfaces and other information. This increases the local ambiguity but the local constraints on the complex descriptions are strong enough to efficiently disambiguate the descriptions. Of course, Waltz did not use statistical information for disambiguation. See also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 198</context>
</contexts>
<marker>Joshi, 1998</marker>
<rawString>Joshi, Aravind K. 1998. Role of constrained computational systems in natural language processing. Artificial Intelligence, 103:117-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Philip Hopely</author>
</authors>
<title>A parser from antiquity.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="5995" citStr="Joshi and Hopely (1997)" startWordPosition="896" endWordPosition="899">o other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch heuristic to resolve cases of ambiguity when </context>
</contexts>
<marker>Joshi, Hopely, 1997</marker>
<rawString>Joshi, Aravind K. and Philip Hopely. 1997. A parser from antiquity. Natural Language Engineering, 2(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences.</journal>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, Aravind K., L. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1996</date>
<booktitle>In Handbook of Formal Languages and Automata.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="10336" citStr="Joshi and Schabes 1996" startWordPosition="1550" endWordPosition="1553"> translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and Joshi 1988; Vijay-Shanker and Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting grammar formalism unlike context-free grammars and head grammars, which are string-rewriting formalisms. The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints. Elementary trees are of two kinds: (a) initial tre</context>
<context position="54910" citStr="Joshi and Schabes (1996)" startWordPosition="8872" endWordPosition="8875">ers. Appendix A: Feature-based Lexicalized Tree Adjoining Grammar Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting grammar formalism, unlike context-free Grammars and head grammars, which are stringrewriting formalisms. FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs), which were first developed in Joshi, Levy, and Takahashi (1975) and later extended to include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker and Joshi 1991) and lexicalization (Schabes, Abeille, and Joshi 1988). For a more recent and comprehensive reference, see Joshi and Schabes (1996). The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints. Elementary trees are of two kinds: (a) Initial Trees and (b) Auxiliary Trees. In an FB-LTAG grammar for natural language, initial trees are phrase structure trees of simple s</context>
<context position="61826" citStr="Joshi and Schabes (1996)" startWordPosition="10088" endWordPosition="10091">r each sentence. The derivations provide 259 Computational Linguistics Volume 25, Number 2 predicate argument relationships for the parsed sentences. Appendix B: Key Properties of LTAGs In this section, we define the key properties of LTAGs: lexicalization, Extended Domain of Locality (EDL), and factoring of recursion from the domain of dependency (FRD), and discuss how these properties are realized in natural language grammars written in LTAGs. A more detailed discussion about these properties is presented in Joshi (1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and Schabes (1996). Definition A grammar is lexicalized if it consists of: • a finite set of elementary structures (strings, trees, directed acyclic graphs, etc.), each structure anchored on a lexical item. • lexical items, each associated with at least one of the elementary structures of the grammar • a finite set of operations combining these structures. This property proves to be linguistically crucial since it establishes a direct link between the lexicon and the syntactic structures defined in the grammar. In fact, in lexicalized grammars all we have is the lexicon, which projects the elementary structures</context>
</contexts>
<marker>Joshi, Schabes, 1996</marker>
<rawString>Joshi, Aravind K. and Yves Schabes, 1996. Tree-adjoining grammars. In Handbook of Formal Languages and Automata. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>B Srinivas</author>
</authors>
<title>Disambiguation of super parts of speech (or supertags): Almost parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94),</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="4377" citStr="Joshi and Srinivas (1994)" startWordPosition="657" endWordPosition="660">ions and pick one per lexical item, for a given reading of the sentence. However, there is an alternate method of parsing that reduces the amount of disambiguation done by the parser. The idea is to locally check the constraints that are associated with the descriptions of lexical items to filter out incompatible descriptions.1 During this disambiguation, the system can also exploit statistical information that can be associated with the descriptions based on their distribution in a corpus of parses. We first employed these ideas in the context of Lexicalized Tree Adjoining grammars (LTAG) in Joshi and Srinivas (1994). Although presented with respect to LTAG, these techniques are applicable to other lexicalized grammars as well. In this paper, we present vastly improved supertag disambiguation results—from previously published 68% accuracy to 92% accuracy using a larger training corpus and better smoothing techniques. The layout of the paper is as follows: In Section 2, we present an overview of the robust parsing approaches. A brief introduction to Lexicalized Tree Adjoining grammars is presented in Section 3. Section 4 illustrates the goal of supertag disambiguation through an example. Various methods an</context>
<context position="22905" citStr="Joshi and Srinivas 1994" startWordPosition="3660" endWordPosition="3663">00 244 Bangalore and Joshi Supertagging Percentage 80.00 75.00 70.00 65.00 60.00 55.00 50.00 45.00 40.00 35.00 30.00 25.00 20.00 15.00 10.00 5.00 0.00 • Sentence Length 10 00 20 00 30 00 40 00 50 00 Figure 4 Percentage drop in the number of supertags with and without filtering for sentences of length 2 to 50 words. 6. Models, Data, Experiments, and Results Before proceeding to discuss the various models for supertag disambiguation, we would like to trace the time course of development of this work. We do this not only to show the improvements made to the early work reported in our 1994 paper (Joshi and Srinivas 1994), but also to explain the rationale for choosing certain models of supertag disambiguation over others. We summarize the early work in the following subsection. 6.1 Early Work As reported in Joshi and Srinivas (1994), we experimented with a trigram model as well as the dependency model for supertag disambiguation. The trigram model that was trained on (part-of-speech, supertag) pairs, instead of (words, supertag) pairs, collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ sentences produced a correct supertag for 68% of the words in the test set. We have since signi</context>
<context position="26073" citStr="Joshi and Srinivas (1994)" startWordPosition="4209" endWordPosition="4212">g can be seen as specifying dependency requirements of the supertag. The probability with which a supertag depends on another supertag is collected from a corpus of sentences annotated with derivation structures. Given a set of supertags for each word and the dependency information between pairs of supertags, the objective of the dependency model is to compute the most likely dependency linkage that spans the entire string. The result of producing the dependency linkage is a sequence of supertags, one for each word of the sentence along with the dependency information. Since first reported in Joshi and Srinivas (1994), we have not continued experiments using this model of supertagging, primarily for two reasons. We are restrained by the lack of a large corpus of LTAG parsed derivation structures that is needed to reliably estimate the various parameters of this model. We are currently in the process of collecting a large LTAG parsed WSJ corpus, with each sentence annotated with the correct derivation. A second reason for the disuse of the dependency model for supertagging is that the objective of supertagging is to see how far local techniques can be used to disambiguate supertags even before parsing begin</context>
<context position="32468" citStr="Joshi and Srinivas (1994)" startWordPosition="5233" endWordPosition="5236">tag set. One of the reasons for this high performance is that the most frequent supertag for the most frequent words— determiners, nouns, and auxiliary verbs—is the correct supertag most of the time. Also, backing off to the part of speech helps in supertagging unknown words, which most often are nouns. The bulk of the errors committed by the unigram model is incorrectly tagged verbs (subcategorization and transformation), prepositions (noun attached vs. verb attached) and nouns (head vs. modifier noun). 6.5 N-gram Model We first explored the use of trigram model of supertag disambiguation in Joshi and Srinivas (1994). The trigram model was trained on (part-of-speech, supertag) pairs collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ sentences. It produced a correct supertag for 68% of the words in the test set. A major drawback of this early work was that it used no lexical information in the supertagging process as the training material consisted of (part-of-speech, supertag) pairs. Since that early work, we have improved the performance of the model by incorporating lexical information and sophisticated smoothing techniques, as well as training on larger training sets. In t</context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Joshi, Aravind K. and B. Srinivas. 1994. Disambiguation of super parts of speech (or supertags): Almost parsing. In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94), Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-functional grammar: A formal system for grammatical representation.</title>
<date>1983</date>
<booktitle>The Mental Representation of Grammatical Relations.</booktitle>
<editor>In J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9452" citStr="Kaplan and Bresnan 1983" startWordPosition="1422" endWordPosition="1425"> system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree A</context>
<context position="50421" citStr="Kaplan and Bresnan 1983" startWordPosition="8187" endWordPosition="8190">h contained 47,333 dependency links in the gold standard, the LDA, trained on 200,000 words, produced 38,480 dependency links correctly, resulting in a recall score of 82.3%. Also, a total of 41,009 dependency links were produced by the LDA, resulting in a precision score of 93.8%. A detailed evaluation of the LDA is presented in Srinivas (199M). 9. Applicability of Supertagging to other Lexicalized Grammars Although we have presented supertagging in the context of LTAG, it is applicable to other lexicalized grammar formalisms such as CCG (Steedman 1997), HPSG (Pollard and Sag 1987), and LFG (Kaplan and Bresnan 1983). We have implemented a broad coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories based on the XTAG English grammar. These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Ka</context>
</contexts>
<marker>Kaplan, Bresnan, 1983</marker>
<rawString>Kaplan, Ronald and Joan Bresnan. 1983. Lexical-functional grammar: A formal system for grammatical representation. In J. Bresnan, editor, The Mental Representation of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>J Heikkila</author>
<author>A Anttila</author>
</authors>
<title>Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter,</title>
<date>1994</date>
<location>Berlin and NY.</location>
<contexts>
<context position="51445" citStr="Karlsson et al. 1994" startWordPosition="8353" endWordPosition="8356">ertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags. An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language. This constitutes the lexicon. The grammar consists of a set of rules that eliminate functional tags for words based on the context of a sentence. Parsing a sentence in this framework amounts to eliminating as many implausible functional tags as possible for each word, given the context of the sentence. The resultant output structure might contain significant syntactic ambiguity, which may not have been eliminated by the </context>
</contexts>
<marker>Karlsson, Voutilainen, Heikkila, Anttila, 1994</marker>
<rawString>Karlsson, F., A. Voutilainen, J. Heikkila, and A. Anttila. 1994. Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin and NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J-P Chanod Karttunen</author>
<author>G Grefenstette</author>
<author>A Schiller</author>
</authors>
<title>Regular expressions for language engineering.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="6024" citStr="Karttunen et al. (1997)" startWordPosition="901" endWordPosition="904"> 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch heuristic to resolve cases of ambiguity when more than one regular express</context>
</contexts>
<marker>Karttunen, Grefenstette, Schiller, 1997</marker>
<rawString>Karttunen, L. J-P. Chanod, G. Grefenstette, and A. Schiller. 1997. Regular expressions for language engineering. Natural Language Engineering, 2(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
<author>Bernd Kiefer</author>
<author>Klaus Netter</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Compilation of HPSG to TAG.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="51038" citStr="Kasper et al. 1995" startWordPosition="8287" endWordPosition="8290">3). We have implemented a broad coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories based on the XTAG English grammar. These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags. An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language. This constitutes </context>
</contexts>
<marker>Kasper, Kiefer, Netter, Vijay-Shanker, 1995</marker>
<rawString>Kasper, Robert, Bernd Kiefer, Klaus Netter, and K. Vijay-Shanker. 1995. Compilation of HPSG to TAG. In Proceedings of the 33rd Annual Meeting, Cambridge, MA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and SignalProcessing,</journal>
<pages>35--3</pages>
<contexts>
<context position="34739" citStr="Katz 1987" startWordPosition="5624" endWordPosition="5625">ity since it indicates the size of the context used in the model and the term Pr(Wj T,) is called the word emit probability since it is the probability of emitting the word W, given the tag Ti. These probabilities are estimated using a corpus where each word is tagged with its correct supertag. The contextual probabilities were estimated using the relative frequency estimates of the contexts in the training corpus. To estimate the probabilities for contexts that do not appear in the training corpus, we used the Good-Turing discounting technique (Good 1953) combined with Katz&apos;s back off model (Katz 1987). The idea here is to discount the frequencies of events that occur in the corpus by an amount related to their frequencies and utilize this discounted probability mass in the back off model to distribute to unseen events. Thus, the Good-Turing discounting technique estimates the frequency of unseen events based on the distribution of the frequency of the counts of observed events in the corpus. If r is the observed frequency of an event, and N,. is the number of events with the observed frequency r, and N is the total number of events, then the probability of an unseen event is given by N1/ N</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and SignalProcessing, 35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Kempe</author>
</authors>
<title>Probabilistic Tagging with Feature Structures.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94),</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="51146" citStr="Kempe 1994" startWordPosition="8306" endWordPosition="8307">on the XTAG English grammar. These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags. An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language. This constitutes the lexicon. The grammar consists of a set of rules that eliminate functional tags for words based on the co</context>
</contexts>
<marker>Kempe, 1994</marker>
<rawString>Kempe, Andre. 1994. Probabilistic Tagging with Feature Structures. In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94), Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony S Kroch</author>
<author>Aravind K Joshi</author>
</authors>
<title>The linguistic relevance of tree adjoining grammars.</title>
<date>1985</date>
<tech>Technical Report MS-CIS-85-16,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="61760" citStr="Kroch and Joshi (1985)" startWordPosition="10078" endWordPosition="10081">s sentences from these domains along with all the derivations for each sentence. The derivations provide 259 Computational Linguistics Volume 25, Number 2 predicate argument relationships for the parsed sentences. Appendix B: Key Properties of LTAGs In this section, we define the key properties of LTAGs: lexicalization, Extended Domain of Locality (EDL), and factoring of recursion from the domain of dependency (FRD), and discuss how these properties are realized in natural language grammars written in LTAGs. A more detailed discussion about these properties is presented in Joshi (1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and Schabes (1996). Definition A grammar is lexicalized if it consists of: • a finite set of elementary structures (strings, trees, directed acyclic graphs, etc.), each structure anchored on a lexical item. • lexical items, each associated with at least one of the elementary structures of the grammar • a finite set of operations combining these structures. This property proves to be linguistically crucial since it establishes a direct link between the lexicon and the syntactic structures defined in the grammar. In fact, in lexicalized grammars al</context>
</contexts>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Kroch, Anthony S. and Aravind K. Joshi. 1985. The linguistic relevance of tree adjoining grammars. Technical Report MS-CIS-85-16, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kuno</author>
</authors>
<title>Harvard predictive analyzer. In</title>
<date>1966</date>
<booktitle>Readings in Automatic Language Processing.</booktitle>
<editor>David G. Hays, editor,</editor>
<publisher>American Elsevier Pub. Co., NY.</publisher>
<contexts>
<context position="17455" citStr="Kuno 1966" startWordPosition="2668" endWordPosition="2669">trees. 5. Reducing Supertag Ambiguity Using Structural Information The structure of the supertag can be best seen as providing admissibility constraints on syntactic environments in which it may be used. Some of these constraints can be checked locally. The following are a few constraints that can be used to determine the admissibility of a syntactic environment for a supertag:4 3 For the purpose of this paper, we suppress the features associated with the supertags. 4 Mitch Marcus pointed out that these tests are similar to the generalized shaper tests used in the Harvard Predictive Analyzer (Kuno 1966). 241 Computational Linguistics Volume 25, Number 2 D NP* the Sr /\ NP0i VP /\ V NPI I I N purchase al Nr NP purchase a9 NP price az NPI Net VP I / rp V NPI I I e N wk. as aio NP price A. Na1,1 VP \ V NI1 I Warder r Ce3 \ NI b VP V NP, includes a7 /\ Nig VP V NP,1 includes all A Mg VP V NPil includes DetP r two /33 DetP,. two a4 ancillary 04 A NP1 S, Pat VP / efl V AP, I I aorillery NP companies a5 s, NPo VP I / v I I -a a5 NP companies aia NP companies «13 companies. NP N D NP* the purchase th 02 N purchase 02 ancillary a2 «11 #3 #4 the purchase price includes two ancillary ..... Figure 1 A s</context>
</contexts>
<marker>Kuno, 1966</marker>
<rawString>Kuno, S. 1966. Harvard predictive analyzer. In David G. Hays, editor, Readings in Automatic Language Processing. American Elsevier Pub. Co., NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7943" citStr="Magerman (1995)" startWordPosition="1188" endWordPosition="1189">none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information t</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, David M. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell M Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell M., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>Varieties of heuristics in sentence processing.</title>
<date>1994</date>
<booktitle>In Current Issues in Natural Language Processing: In Honour of</booktitle>
<note>Giardini with Kluwer.</note>
<contexts>
<context position="8996" citStr="Nagao (1994)" startWordPosition="1352" endWordPosition="1353">ant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link</context>
</contexts>
<marker>Nagao, 1994</marker>
<rawString>Nagao, Makoto. 1994. Varieties of heuristics in sentence processing. In Current Issues in Natural Language Processing: In Honour of Don Walker. Giardini with Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On the estimation of &apos;small&apos; probabilities by leaving-one-out.</title>
<date>1995</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>17</volume>
<issue>2</issue>
<marker>Ney, Essen, Kneser, 1995</marker>
<rawString>Ney, Herman, Ute Essen, and Reinhard Kneser. 1995. On the estimation of &apos;small&apos; probabilities by leaving-one-out. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Niesler</author>
<author>P C Woodland</author>
</authors>
<title>A variable-length category-based n-gram language model.</title>
<date>1996</date>
<booktitle>In Proceedings, IEEE ICASSP.</booktitle>
<contexts>
<context position="36914" citStr="Niesler and Woodland 1996" startWordPosition="5993" endWordPosition="5996">as shown in Equation 7. For the (word, supertag) pairs that do not appear in the corpus, the word emit probability is estimated as shown in Equation 8. Some of the word features used in our imple250 Bangalore and Joshi Supertagging mentation include prefixes and suffixes of length less than or equal to three characters, capitalization, and digit features. Pr(WilTi) =N (W„ T,) if N (Wi, T,) &gt; 0 (7) N(Ti) = Pr(UNKIT,) * Pr(word_features(W,)IT,) otherwise (8) The counts for the (word, supertag) pairs for the words that do not appear in the corpus is estimated using the leaving-one-out technique (Niesler and Woodland 1996; Ney, Essen, and Kneser 1995). A token UNK is associated with each supertag and its count NuNK is estimated by: Ni(T1) Pr(LINKITi) = N(TI) + Pr(UNKITj) * N(Ti) 1 — Pr(UNKITI) where N1(TI) is the number of words that are associated with the supertag Tj that appear in the corpus exactly once. N(T1) is the frequency of the supertag Tj and NuNK(TI) is the estimated count of UNK in 7.1. The constant n is introduced so as to ensure that the probability is not greater than one, especially for supertags that are sparsely represented in the corpus. We use word features similar to the ones used in Weis</context>
</contexts>
<marker>Niesler, Woodland, 1996</marker>
<rawString>Niesler, T. R. and P. C. Woodland. 1996. A variable-length category-based n-gram language model. In Proceedings, IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1987</date>
<journal>Information-Based Syntax and Semantics.</journal>
<volume>1</volume>
<publisher>Fundamentals. CSLI.</publisher>
<contexts>
<context position="9508" citStr="Pollard and Sag 1987" startWordPosition="1432" endWordPosition="1435">ghting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the cla</context>
<context position="50386" citStr="Pollard and Sag 1987" startWordPosition="8181" endWordPosition="8184">Wall Street Journal corpus, which contained 47,333 dependency links in the gold standard, the LDA, trained on 200,000 words, produced 38,480 dependency links correctly, resulting in a recall score of 82.3%. Also, a total of 41,009 dependency links were produced by the LDA, resulting in a precision score of 93.8%. A detailed evaluation of the LDA is presented in Srinivas (199M). 9. Applicability of Supertagging to other Lexicalized Grammars Although we have presented supertagging in the context of LTAG, it is applicable to other lexicalized grammar formalisms such as CCG (Steedman 1997), HPSG (Pollard and Sag 1987), and LFG (Kaplan and Bresnan 1983). We have implemented a broad coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories based on the XTAG English grammar. These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed. We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HP</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1987. Information-Based Syntax and Semantics. Vol. 1: Fundamentals. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Analyse syntaxique transformationelle du francais par transducteurs et lexique-grammaire.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Universite Paris</institution>
<contexts>
<context position="5932" citStr="Roche (1993)" startWordPosition="888" endWordPosition="889">scuss the applicability of supertag disambiguation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use</context>
</contexts>
<marker>Roche, 1993</marker>
<rawString>Roche, Emmanuel. 1993. Analyse syntaxique transformationelle du francais par transducteurs et lexique-grammaire. Ph.D. thesis, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="60025" citStr="Schabes 1990" startWordPosition="9775" endWordPosition="9776"> is indicated as part of the node label. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence, as shown in Figure 8(c). A broad-coverage grammar system, XTAG, has been implemented in the LTAG formalism. In this section, we briefly discuss some aspects related to XTAG for the sake of completeness. A more detailed report on XTAG can be found in XTAG-Group (1995). The XTAG system consists of a morphological analyzer, a part-of-speech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Earley-style parser for LTAG (Schabes 1990), and an X-windows interface for grammar development (Doran et al. 1994). The input sentence is subjected to morphological analysis (a) (b) bf t U If br If b U bl 258 Bangalore and Joshi Supertagging the agr : &lt;1&gt; 3rdsing num: sing pers : 3 mode : &lt;2&gt; Ind VPr{agr : &lt;1&gt; mode : &lt;2 DeiP [agr : &lt;1,-] N [agr : &lt;1&gt;] V [agr : &lt;1&gt; VP [agr : &lt;3&gt; j &lt;5&gt; [1 D tagr : &lt;IA company mode: &lt;2&gt;i Ingr : &lt;3&gt; mode : &lt;4&gt; ge &lt;6&gt; ppartl is V mode: &lt;4&gt; VP [agr : mode: being V {passive : agr : &lt;5&gt; mode: &lt;6&gt; acquired (a) anx1V [acquired] acquired aNXdxN [company] I3Vvx [being] company being aDXD [the] 13Vvx [is] the is (</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes, Yves. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Computer Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeilld</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with lexicalized&apos; grammars: Application to Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING&apos;88),</booktitle>
<location>Budapest, Hungary,</location>
<marker>Schabes, Abeilld, Joshi, 1988</marker>
<rawString>Schabes, Yves, Anne Abeilld, and Aravind K. Joshi. 1988. Parsing strategies with lexicalized&apos; grammars: Application to Tree Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics (COLING&apos;88), Budapest, Hungary, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing with lexicalized tree adjoining grammar.</title>
<date>1991</date>
<booktitle>Current Issues in Parsing Technologies.</booktitle>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="9590" citStr="Schabes and Joshi 1991" startWordPosition="1444" endWordPosition="1447">k et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-L</context>
</contexts>
<marker>Schabes, Joshi, 1991</marker>
<rawString>Schabes, Yves and Aravind K. Joshi. 1991. Parsing with lexicalized tree adjoining grammar. In M. Tomita, editor, Current Issues in Parsing Technologies. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>M Roth</author>
<author>R Osborne</author>
</authors>
<title>Parsing the Wall Street Journal with the inside-outside algorithm.</title>
<date>1993</date>
<booktitle>In Proceedings of the European ACL.</booktitle>
<marker>Schabes, Roth, Osborne, 1993</marker>
<rawString>Schabes, Y, M. Roth, and R. Osborne. 1993. Parsing the Wall Street Journal with the inside-outside algorithm. In Proceedings of the European ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Department of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="9633" citStr="Sleator and Temperley 1991" startWordPosition="1450" endWordPosition="1453">and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vija</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Sleator, Daniel and Davy Temperley. 1991. Parsing English with a Link Grammar. Technical Report CMU-CS-91-196, Department of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Almost parsing&amp;quot; technique for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP96 Conference,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="44014" citStr="Srinivas 1996" startWordPosition="7150" endWordPosition="7151"> at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and selects all the supertags associated with each word of the sentence to be parsed. In the second stage, the parser searches the l</context>
</contexts>
<marker>Srinivas, 1996</marker>
<rawString>Srinivas, B. 1996. &amp;quot;Almost parsing&amp;quot; technique for language modeling. In Proceedings of ICSLP96 Conference, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Complexity of Lexical Descriptions and its Relevance to Partial Parsing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="43890" citStr="Srinivas 1997" startWordPosition="7133" endWordPosition="7134">e-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>Srinivas, B. 1997a. Complexity of Lexical Descriptions and its Relevance to Partial Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Performance evaluation of supertagging for partial parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<contexts>
<context position="43890" citStr="Srinivas 1997" startWordPosition="7133" endWordPosition="7134">e-word window. We trained the templates on 200,000 words&apos; and tested on 47,000 words&apos; of the WSJ corpus. The model performed at an accuracy of 90%. The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model. However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus. 7. Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications. The output of the supertagger has also been used as a front end to a lexicalized grammar parser. As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988). In the first stage, the parser looks up the lexicon and sele</context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>Srinivas, B. 1997b. Performance evaluation of supertagging for partial parsing. In Proceedings of the International Workshop on Parsing Technologies, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
<author>Christine Doran</author>
<author>Seth Kulick</author>
</authors>
<title>Heuristics and parse ranking.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th Annual International Workshop on Parsing Technologies,</booktitle>
<location>Prague,</location>
<marker>Srinivas, Doran, Kulick, 1995</marker>
<rawString>Srinivas, B., Christine Doran, and Seth Kulick. 1995. Heuristics and parse ranking. In Proceedings of the 4th Annual International Workshop on Parsing Technologies, Prague, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Combinatory grammars and parasitic gaps.</title>
<date>1987</date>
<booktitle>Natural Language and Linguistic Theory,</booktitle>
<pages>5--403</pages>
<contexts>
<context position="9529" citStr="Steedman 1987" startWordPosition="1437" endWordPosition="1438">nction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized gra</context>
</contexts>
<marker>Steedman, 1987</marker>
<rawString>Steedman, Mark. 1987. Combinatory grammars and parasitic gaps. Natural Language and Linguistic Theory, 5:403-439.</rawString>
</citation>
<citation valid="true">
<title>The Syntactic Interface.</title>
<date>1997</date>
<editor>Steedman, Mark, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA and London, England.</location>
<contexts>
<context position="5970" citStr="(1997)" startWordPosition="895" endWordPosition="895">uation to other lexicalized grammars. 2. Related Approaches In recent years, there have been a number of attempts at robust parsing of natural language. They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers. We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms. 2.1 Finite-State-Grammar-based Parsers Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al. (1997), Joshi and Hopely (1997), and Karttunen et al. (1997). These systems use grammars that are represented as cascaded finite-state regular expression recognizers. The regular expressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch heuristic to resolve</context>
<context position="7980" citStr="(1997)" startWordPosition="1194" endWordPosition="1194">rmation to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This all</context>
</contexts>
<marker>1997</marker>
<rawString>Steedman, Mark, editor. 1997. The Syntactic Interface. MIT Press, Cambridge, MA and London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo jarvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94),</booktitle>
<location>Kyoto, Japan,</location>
<marker>Tapanainen, jarvinen, 1994</marker>
<rawString>Tapanainen, Pasi and Timo jarvinen. 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns. In Proceedings of the 15th International Conference on Computational Linguistics (COLING&apos;94), Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="10247" citStr="Vijay-Shanker 1987" startWordPosition="1539" endWordPosition="1540">991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and Joshi 1988; Vijay-Shanker and Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting grammar formalism unlike context-free grammars and head grammars, which are string-rewriting formalisms. The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and sema</context>
<context position="54748" citStr="Vijay-Shanker 1987" startWordPosition="8851" endWordPosition="8852">ses where the supertag sequence after disambiguation may not combine to form a single structure. This approach is applicable to all lexicalized grammar parsers. Appendix A: Feature-based Lexicalized Tree Adjoining Grammar Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting grammar formalism, unlike context-free Grammars and head grammars, which are stringrewriting formalisms. FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs), which were first developed in Joshi, Levy, and Takahashi (1975) and later extended to include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker and Joshi 1991) and lexicalization (Schabes, Abeille, and Joshi 1988). For a more recent and comprehensive reference, see Joshi and Schabes (1996). The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints. Elementary t</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker, K. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unification based tree adjoining grammars.</title>
<date>1991</date>
<editor>In J. Wedekind, editor, Unification-based Grammars.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10311" citStr="Vijay-Shanker and Joshi 1991" startWordPosition="1546" endWordPosition="1549">lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort. Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and Joshi 1988; Vijay-Shanker and Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting grammar formalism unlike context-free grammars and head grammars, which are string-rewriting formalisms. The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints. Elementary trees are of t</context>
<context position="54779" citStr="Vijay-Shanker and Joshi 1991" startWordPosition="8853" endWordPosition="8856">ag sequence after disambiguation may not combine to form a single structure. This approach is applicable to all lexicalized grammar parsers. Appendix A: Feature-based Lexicalized Tree Adjoining Grammar Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting grammar formalism, unlike context-free Grammars and head grammars, which are stringrewriting formalisms. FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs), which were first developed in Joshi, Levy, and Takahashi (1975) and later extended to include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker and Joshi 1991) and lexicalization (Schabes, Abeille, and Joshi 1988). For a more recent and comprehensive reference, see Joshi and Schabes (1996). The primitive elements of FB-LTAGs are called elementary trees. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor of that tree. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints. Elementary trees are of two kinds: (a) Init</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1991</marker>
<rawString>Vijay-Shanker, K. and Aravind K. Joshi. 1991. Unification based tree adjoining grammars. In J. Wedekind, editor, Unification-based Grammars. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Designing a Parsing Grammar.</title>
<date>1994</date>
<journal>Publications of the</journal>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="51463" citStr="Voutilainen 1994" startWordPosition="8357" endWordPosition="8358">G trees and the CCG categories. The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995). A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994). For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags. An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994). In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language. This constitutes the lexicon. The grammar consists of a set of rules that eliminate functional tags for words based on the context of a sentence. Parsing a sentence in this framework amounts to eliminating as many implausible functional tags as possible for each word, given the context of the sentence. The resultant output structure might contain significant syntactic ambiguity, which may not have been eliminated by the rule applications,</context>
</contexts>
<marker>Voutilainen, 1994</marker>
<rawString>Voutilainen, Atro. 1994. Designing a Parsing Grammar. Publications of the Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Waltz</author>
</authors>
<title>Understanding line drawings of scenes with shadows. In</title>
<date>1975</date>
<booktitle>Psychology of Computer Vision,</booktitle>
<editor>P. Winston, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6755" citStr="Waltz 1975" startWordPosition="1015" endWordPosition="1016">xpressions are usually hand-crafted. Each recognizer in the cascade provides a locally optimal output. The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse. There are no clause-level attachments or modifier attachments in the shallow parse. These parsers always produce one output, since they use the longestmatch heuristic to resolve cases of ambiguity when more than one regular expression 1 The use of descriptions for primitives to capture constraints locally has a precursor in Al. The Waltz algorithm (Waltz 1975) for labeling vertices of polygonal solid objects can be thought of in these terms. Waltz made the description of vertices more complex by including information about the incident edges, associated surfaces and other information. This increases the local ambiguity but the local constraints on the complex descriptions are strong enough to efficiently disambiguate the descriptions. Of course, Waltz did not use statistical information for disambiguation. See also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use a</context>
</contexts>
<marker>Waltz, 1975</marker>
<rawString>Waltz, D. 1975. Understanding line drawings of scenes with shadows. In P. Winston, editor, Psychology of Computer Vision, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Richard Schwartz</author>
<author>Jeff Palmucci</author>
<author>Marie Meteer</author>
<author>Lance Ramshaw</author>
</authors>
<title>Coping with ambiguity and unknown words through Bangalore and Joshi Supertagging probabilistic models. Computational adjoining grammar for English.</title>
<date>1993</date>
<tech>Technical Linguistics, 19(2):359-382,</tech>
<institution>University of XTAG-Group, The.</institution>
<contexts>
<context position="11818" citStr="Weischedel et al. 1993" startWordPosition="1777" endWordPosition="1780">ary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the d</context>
<context position="37534" citStr="Weischedel et al. (1993)" startWordPosition="6104" endWordPosition="6107">1996; Ney, Essen, and Kneser 1995). A token UNK is associated with each supertag and its count NuNK is estimated by: Ni(T1) Pr(LINKITi) = N(TI) + Pr(UNKITj) * N(Ti) 1 — Pr(UNKITI) where N1(TI) is the number of words that are associated with the supertag Tj that appear in the corpus exactly once. N(T1) is the frequency of the supertag Tj and NuNK(TI) is the estimated count of UNK in 7.1. The constant n is introduced so as to ensure that the probability is not greater than one, especially for supertags that are sparsely represented in the corpus. We use word features similar to the ones used in Weischedel et al. (1993), such as capitalization, hyphenation, and endings of words, for estimating the word emit probability of unknown words. 6.5.1 Experiments and Results. We tested the performance of the trigram model on various domains such as the Wall Street Journal (WSJ), the IBM Manual corpus and the ATIS corpus. For the IBM Manual corpus and the ATIS domains, a supertag annotated corpus was collected using the parses of the XTAG system (Doran et al. 1994) and selecting the correct analysis for each sentence. The corpus was then randomly split into training and test material. Supertag performance is measured </context>
</contexts>
<marker>Weischedel, Schwartz, Palmucci, Meteer, Ramshaw, 1993</marker>
<rawString>Weischedel, Ralph, Richard Schwartz, Jeff Palmucci, Marie Meteer, and Lance Ramshaw. 1993. Coping with ambiguity and unknown words through Bangalore and Joshi Supertagging probabilistic models. Computational adjoining grammar for English. Technical Linguistics, 19(2):359-382, June. Report IRCS 95-03, University of XTAG-Group, The. 1995. A lexicalized tree Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>