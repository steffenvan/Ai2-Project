<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.964076">
A Game-Theoretic Approach to Generating Spatial Descriptions
</title>
<author confidence="0.891844">
Dave Golland Percy Liang Dan Klein
</author>
<affiliation confidence="0.502318">
UC Berkeley UC Berkeley UC Berkeley
</affiliation>
<address confidence="0.656521">
Berkeley, CA 94720 Berkeley, CA 94720 Berkeley, CA 94720
</address>
<email confidence="0.998938">
dsg@cs.berkeley.edu pliang@cs.berkeley.edu klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999515714285714">
Language is sensitive to both semantic and
pragmatic effects. To capture both effects,
we model language use as a cooperative game
between two players: a speaker, who gener-
ates an utterance, and a listener, who responds
with an action. Specifically, we consider the
task of generating spatial references to ob-
jects, wherein the listener must accurately
identify an object described by the speaker.
We show that a speaker model that acts op-
timally with respect to an explicit, embedded
listener model substantially outperforms one
that is trained to directly generate spatial de-
scriptions.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.904569666666667">
Language is about successful communication be-
tween a speaker and a listener. For example, if the
goal is to reference the target object O1 in Figure 1,
a speaker might choose one of the following two ut-
terances:
(a) right of O2 (b) on O3
Although both utterances are semantically correct,
(a) is ambiguous between O1 and O3, whereas (b)
unambiguously identifies O1 as the target object,
and should therefore be preferred over (a). In this
paper, we present a game-theoretic model that cap-
tures this communication-oriented aspect of lan-
guage interpretation and generation.
Successful communication can be broken down
into semantics and pragmatics. Most computational
</bodyText>
<figureCaption confidence="0.9950034">
Figure 1: An example of a 3D model of a room. The
speaker’s goal is to reference the target object O1 by de-
scribing its spatial relationship to other object(s). The
listener’s goal is to guess the object given the speaker’s
description.
</figureCaption>
<bodyText confidence="0.999939666666667">
work on interpreting language focuses on compo-
sitional semantics (Zettlemoyer and Collins, 2005;
Wong and Mooney, 2007; Piantadosi et al., 2008),
which is concerned with verifying the truth of a sen-
tence. However, what is missing from this truth-
oriented view is the pragmatic aspect of language—
that language is used to accomplish an end goal, as
exemplified by speech acts (Austin, 1962). Indeed,
although both utterances (a) and (b) are semantically
valid, only (b) is pragmatically felicitous: (a) is am-
biguous and therefore violates the Gricean maxim
of manner (Grice, 1975). To capture this maxim, we
develop a model of pragmatics based on game the-
ory, in the spirit of J¨ager (2008) but extended to the
stochastic setting. We show that Gricean maxims
</bodyText>
<page confidence="0.962484">
410
</page>
<note confidence="0.818748">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.989825421052632">
fall out naturally as consequences of the model.
An effective way to empirically explore the prag-
matic aspects of language is to work in the grounded
setting, where the basic idea is to map language to
some representation of the non-linguistic world (Yu
and Ballard, 2004; Feldman and Narayanan, 2004;
Fleischman and Roy, 2007; Chen and Mooney,
2008; Frank et al., 2009; Liang et al., 2009). Along
similar lines, past work has also focused on inter-
preting natural language instructions (Branavan et
al., 2009; Eisenstein et al., 2009; Kollar et al., 2010),
which takes into account the goal of the communi-
cation. This work differs from ours in that it does
not clarify the formal relationship between pragmat-
ics and the interpretation task. Pragmatics has also
been studied in the context of dialog systems. For
instance, DeVault and Stone (2007) present a model
of collaborative language between multiple agents
that takes into account contextual ambiguities.
We present our pragmatic model in a grounded
setting where a speaker must describe a target object
to a listener via spatial description (such as in the
example given above). Though we use some of the
techniques from work on the semantics of spatial de-
scriptions (Regier and Carlson, 2001; Gorniak and
Roy, 2004; Tellex and Roy, 2009), we empirically
demonstrate that having a model of pragmatics en-
ables more successful communication.
2 Language as a Game
To model Grice’s cooperative principle (Grice,
1975), we formulate the interaction between a
speaker S and a listener L as a cooperative game, that
is, one in which S and L share the same utility func-
tion. For simplicity, we focus on the production and
interpretation of single utterances, where the speaker
and listener have access to a shared context. To sim-
plify notation, we suppress writing the dependence
on the context.
</bodyText>
<subsectionHeader confidence="0.562687">
The Communication Game
</subsectionHeader>
<listItem confidence="0.991228333333333">
1. In order to communicate a target o to L, S pro-
duces an utterance w chosen according to a
strategy pS(w 1 o).
2. L interprets w and responds with a guess g ac-
cording to a strategy pL(g I w).
3. S and L collectively get a utility of U(o, g).
</listItem>
<figure confidence="0.468730333333333">
target utterance guess
speaker listener
o w g
ps(w  |o) pl(g  |w)
U
utility
</figure>
<figureCaption confidence="0.9932228">
Figure 2: Diagram representing the communication
game. A target, o, is given to the speaker that generates
an utterance w. Based on this utterance, the listener gen-
erates a guess g. If o = g, then both the listener and
speaker get a utility of 1, otherwise they get a utility of 0.
</figureCaption>
<bodyText confidence="0.704128">
This communication game is described graphi-
</bodyText>
<equation confidence="0.450540666666667">
on O3
1
near O3
0
right of O2
0
</equation>
<figureCaption confidence="0.876423">
Figure 3: Three instances of the communication game on
the scenario in Figure 1. For each instance, the target o,
utterance w, guess g, and the resulting utility U are shown
in their respective positions. A utility of 1 is awarded only
when the guess matches the target.
</figureCaption>
<bodyText confidence="0.999904833333333">
cally in Figure 2. Figure 3 shows several instances of
the communication game being played for the sce-
nario in Figure 1.
Grice’s maxim of manner encourages utterances
to be unambiguous, which motivates the following
utility, which we call (communicative) success:
</bodyText>
<equation confidence="0.681366">
U(o, g) def = ff[o = g], (1)
</equation>
<bodyText confidence="0.998595">
where the indicator function ff[o = g] is 1 if o =
g and 0 otherwise. Hence, a utility-maximizing
speaker will attempt to produce unambiguous utter-
ances because they increase the probability that the
listener will correctly guess the target.
</bodyText>
<page confidence="0.997749">
411
</page>
<bodyText confidence="0.99925675">
Given a speaker strategy pS(w  |o), a listener
strategy pL(g  |w), and a prior distribution over tar-
gets p(o), the expected utility obtained by S and L is
as follows:
</bodyText>
<equation confidence="0.9727628">
EU(S, L) = � p(o)pS(w|o)pL(g|w)U(o, g)
o,w,g
�
= p(o)pS(w|o)pL(o|w). (2)
o,w
</equation>
<sectionHeader confidence="0.791751" genericHeader="method">
3 From Reflex Speaker to Rational
Speaker
</sectionHeader>
<bodyText confidence="0.995025272727273">
Having formalized the language game, we now ex-
plore various speaker and listener strategies. First,
let us consider literal strategies. A literal speaker
(denoted S:LITERAL) chooses uniformly from the
set of utterances consistent with a target object, i.e.,
the ones which are semantically valid;1 a literal lis-
tener (denoted L:LITERAL) guesses an object con-
sistent with the utterance uniformly at random.
In the running example (Figure 1), where the tar-
get object is O1, there are two semantically valid ut-
terances:
</bodyText>
<listItem confidence="0.737326333333333">
(a) right of O2 (b) on O3
S:LITERAL selects (a) or (b) each with probability
2. If S:LITERAL chooses (a), L:LITERAL will guess
</listItem>
<equation confidence="0.431696">
1
</equation>
<bodyText confidence="0.999630928571429">
the target object O1 correctly with probability 12; if
S:LITERAL chooses (b), L:LITERAL will guess cor-
rectly with probability 1. Therefore, the expected
utility EU(S:LITERAL, L:LITERAL) = 34.
We say S:LITERAL is an example of a reflex
speaker because it chooses an utterance without
taking the listener into account. A general reflex
speaker is depicted in Figure 4(a), where each edge
represents a potential utterance.
Suppose we now have a model of some listener
L. Motivated by game theory, we would optimize
the expected utility (2) given pL(g  |w). We call
the resulting speaker S(L) the rational speaker with
respect to listener L. Solving for this strategy yields:
</bodyText>
<equation confidence="0.996880333333333">
pS(L)(w  |o) = ff[w = w*], where
w* = argmax pL(o  |w&apos;). (3)
w/
</equation>
<footnote confidence="0.845907">
1Semantic validity is approximated by a set of heuristic rules
(e.g. left is all positions with smaller x-coordinates).
</footnote>
<figure confidence="0.567764">
S(L)
w1
S
w1
o w2
w3
w3
(a) Reflex speaker (b) Rational speaker
</figure>
<figureCaption confidence="0.911634">
Figure 4: (a) A reflex speaker (S) directly selects an ut-
terance based only on the target object. Each edge rep-
</figureCaption>
<bodyText confidence="0.922950928571429">
resents a different choice of utterance. (b) A rational
speaker (S(L)) selects an utterance based on an embed-
ded model of the listener (L). Each edge in the first layer
represents a different choice the speaker can make, and
each edge in the second layer represents a response of the
listener.
Intuitively, S(L) chooses an utterance, w*, such that,
if listener L were to interpret w*, the probability of
L guessing the target would be maximized.2 The ra-
tional speaker is depicted in Figure 4(b), where, as
before, each edge at the first level represents a possi-
ble choice for the speaker, but there is now a second
layer representing the response of the listener.
To see how an embedded model of the listener
improves communication, again consider our run-
ning example in Figure 1. A speaker can describe
the target object O1 using either w1 = on O3 or
w2 = right of O2. Suppose the embedded listener
is L:LITERAL, which chooses uniformly from the
set of objects consistent with the given utterance.
In this scenario, pL:LITERAL(O1  |w1) = 1 because
w1 unambiguously describes the target object, but
pL:LITERAL(O1  |w2) = 12. The rational speaker
S(L:LITERAL) would therefore choose w1, achiev-
ing a utility of 1, which is an improvement over the
reflex speaker S:LITERAL’s utility of 34.
2If there are ties, any distribution over the utterances having
the same utility is optimal.
</bodyText>
<figure confidence="0.89266875">
o w2 92
L
91
93
</figure>
<page confidence="0.986961">
412
</page>
<sectionHeader confidence="0.6353195" genericHeader="method">
4 From Literal Speaker to Learned
Speaker
</sectionHeader>
<bodyText confidence="0.999976166666667">
In the previous section, we showed that a literal
strategy, one that considers only semantically valid
choices, can be used to directly construct a reflex
speaker S:LITERAL or an embedded listener in a
rational speaker S(L:LITERAL). This section fo-
cuses on an orthogonal direction: improving literal
strategies with learning. Specifically, we construct
learned strategies from log-linear models trained on
human annotations. These learned strategies can
then be used to construct reflex and rational speaker
variants—S:LEARNED and S(L:LEARNED), respec-
tively.
</bodyText>
<subsectionHeader confidence="0.999818">
4.1 Training a Log-Linear Speaker/Listener
</subsectionHeader>
<bodyText confidence="0.999227230769231">
We train the speaker, S:LEARNED, (similarly, lis-
tener, L:LEARNED) on training examples which
comprise the utterances produced by the human an-
notators (see Section 6.1 for details on how this
data was collected). Each example consists of a 3D
model of a room in a house that specifies the 3D po-
sitions of each object and the coordinates of a 3D
camera. When training the speaker, each example is
a pair (o, w), where o is the input target object and
w is the output utterance. When training the listener,
each example is (w, g), where w is the input utter-
ance and g is the output guessed object.
For now, an utterance w consists of two parts:
</bodyText>
<listItem confidence="0.99798325">
• A spatial preposition w.r (e.g., right of) from a
set of possible prepositions.3
• A reference object w.o (e.g., O3) from the set
of objects in the room.
</listItem>
<bodyText confidence="0.997521666666667">
We consider more complex utterances in Section 5.
Both S:LEARNED and L:LEARNED are
parametrized by log-linear models:
</bodyText>
<equation confidence="0.907095">
pS:LEARNED(w|o; θS) a exp{θ�S φ(o, w)} (4)
pL:LEARNED(g|w; θL) a exp{θ�L φ(g, w)} (5)
</equation>
<bodyText confidence="0.99806232">
where φ(·, ·) is the feature vector (see below), θS
and θL are the parameter vectors for speaker and lis-
tener. Note that the speaker and listener use the same
3We chose 10 prepositions commonly used by people to de-
scribe objects in a preliminary data gathering experiment. This
list includes multi-word units, which function equivalently to
prepositions, such as left of.
set of features, but they have different parameters.
Furthermore, the first normalization sums over pos-
sible utterances w while the second normalization
sums over possible objects g in the scene. The two
parameter vectors are trained to optimize the log-
likelihood of the training data under the respective
models.
Features We now describe the features φ(o, w).
These features draw inspiration from Landau and
Jackendoff (1993) and Tellex and Roy (2009).
Each object o in the 3D scene is represented by
its bounding box, which is the smallest rectangular
prism containing o. The following are functions of
the camera, target (or guessed object) o, and the ref-
erence object w.o in the utterance. The full set of
features is obtained by conjoining these functions
with indicator functions of the form ff[w.r = r],
where r ranges over the set of valid prepositions.
</bodyText>
<listItem confidence="0.981639611111111">
• Proximity functions measure the distance be-
tween o and w.o. This is implemented as the
minimum over all the pairwise Euclidean dis-
tances between the corners of the bounding
boxes. We also have indicator functions for
whether o is the closest object, among the top
5 closest objects, and among the top 10 closest
objects to w.o.
• Topological functions measure containment be-
tween o and w.o: vol(o n w.o)/vol(o) and
vol(o n w.o)/vol(w.o). To simplify volume
computation, we approximate each object by a
bounding box that is aligned with the camera
axes.
• Projection functions measure the relative posi-
tion of the bounding boxes with respect to one
another. Specifically, let v be the vector from
the center of w.o to the center of o. There is a
</listItem>
<bodyText confidence="0.889801">
function for the projection of v onto each of the
axes defined by the camera orientation (see Fig-
ure 5). Additionally, there is a set of indicator
functions that capture the relative magnitude of
these projections. For example, there is a indi-
cator function denoting whether the projection
of v onto the camera’s x-axis is the largest of
all three projections.
</bodyText>
<page confidence="0.998593">
413
</page>
<figureCaption confidence="0.94795525">
Figure 5: The projection features are computed by pro-
jecting a vector v extending from the center of the ref-
erence object to the center of the target object onto the
camera axes f., and fy.
</figureCaption>
<sectionHeader confidence="0.971171" genericHeader="method">
5 Handling Complex Utterances
</sectionHeader>
<bodyText confidence="0.9999395">
So far, we have only considered speakers and lis-
teners that deal with utterances consisting of one
preposition and one reference object. We now ex-
tend these strategies to handle more complex utter-
ances. Specifically, we consider utterances that con-
form to the following grammar:4
</bodyText>
<figure confidence="0.50487525">
[noun] N → something  |O1  |O2  |· · ·
[relation] R → in front of  |on  |· · ·
[conjunction] NP → N RP*
[relativization] RP → R NP
</figure>
<bodyText confidence="0.8842595">
This grammar captures two phenomena of lan-
guage use, conjunction and relativization.
</bodyText>
<listItem confidence="0.955602416666667">
• Conjunction is useful when one spatial relation
is insufficient to disambiguate the target object.
For example, in Figure 1, right of O2 could re-
fer to the vase or the table, but using the con-
junction right of O2 and on O3 narrows down
the target object to just the vase.
• The main purpose of relativization is to refer
to objects without a precise nominal descrip-
tor. With complex utterances, it is possible to
chain relative prepositional phrases, for exam-
ple, using on something right of O2 to refer to
the vase.
</listItem>
<bodyText confidence="0.992525666666667">
4Naturally, we disallow direct reference to the target object.
Given an utterance w, we define its complexity |w|
as the number of applications of the relativization
rule, RP → R NP, used to produce w. We had only
considered utterances of complexity 1 in previous
sections.
</bodyText>
<subsectionHeader confidence="0.971898">
5.1 Example Utterances
</subsectionHeader>
<bodyText confidence="0.998416285714286">
To illustrate the types of utterances available under
the grammar, again consider the scene in Figure 1.
Utterances of complexity 2 can be generated ei-
ther using the relativization rule exclusively, or both
the conjunction and relativization rules. The rela-
tivization rule can be used to generate the following
utterances:
</bodyText>
<listItem confidence="0.983609714285714">
• on something that is right of O2
• right of something that is left of O3
Applying the conjunction rule leads to the following
utterances:
• right of O2 and on O3
• right of O2 and under O1
• left of O1 and left of O3
</listItem>
<bodyText confidence="0.9998915">
Note that we inserted the words that is after each N
and the word and between every adjacent pair of RPs
generated via the conjunction rule. This is to help a
human listener interpret an utterance.
</bodyText>
<subsectionHeader confidence="0.997847">
5.2 Extending the Rational Speaker
</subsectionHeader>
<bodyText confidence="0.999938705882353">
Suppose we have a rational speaker S(L) defined in
terms of an embedded listener L which operates over
utterances of complexity 1. We first extend L to in-
terpret arbitrary utterances of our grammar. The ra-
tional speaker (defined in (2)) automatically inherits
this extension.
Compositional semantics allows us to define the
interpretation of complex utterances in terms of sim-
pler ones. Specifically, each node in the parse tree
has a denotation, which is computed recursively
in terms of the node’s children via a set of sim-
ple rules. Usually, denotations are represented as
lambda-calculus functions, but for us, they will be
distributions over objects in the scene. As a base
case for interpreting utterances of complexity 1, we
can use either L:LITERAL or L:LEARNED (defined
in Sections 3 and 4).
</bodyText>
<page confidence="0.995453">
414
</page>
<bodyText confidence="0.9984194">
Given a subtree w rooted at u E IN, NP, RP}, we
define the denotation of w, JwK, to be a distribution
over the objects in the scene in which the utterance
was generated. The listener strategy pL(g|w) = JwK
is recursively as follows:
</bodyText>
<listItem confidence="0.997424">
• If w is rooted at N with a single child x, then JwK
is the uniform distribution over N(x), the set of
objects consistent with the word x.
• If w is rooted at NP, we recursively compute the
distributions over objects g for each child tree,
multiply the probabilities, and renormalize (Hin-
ton, 1999).
• If w is rooted at RP with relation r, we recursively
compute the distribution over objects g0 for the
child NP tree. We then appeal to the base case
to produce a distribution over objects g which are
related to g0 via relation r.
</listItem>
<equation confidence="0.95054">
This strategy is defined formally as follows:
pL(g  |w) 0c
(6)
</equation>
<bodyText confidence="0.999740941176471">
Figure 6 shows an example of this bottom-
up denotation computation for the utterance
on something right of O2 with respect to the scene
in Figure 1. The denotation starts with the lowest
NP node JO2K, which places all the mass on O2
in the scene. Moving up the tree, we compute
the denotation of the RP, Jright of O2K, using the
RP case of (6), which results in a distribution that
places equal mass on O1 and O3.5 The denotation
of the N node JsomethingK is a flat distribution over
all the objects in the scene. Continuing up the tree,
the denotation of the NP is computed by taking a
product of the object distributions, and turns out
to be exactly the same split distribution as its RP
child. Finally, the denotation at the root is computed
by applying the base case to on and the resulting
distribution from the previous step.
</bodyText>
<footnote confidence="0.997186666666667">
5It is worth mentioning that this split distribution between
O1 and O3 represents the ambiguity mentioned in Section 3
when discussing the shortcomings of S:LITERAL.
</footnote>
<figureCaption confidence="0.999147">
Figure 6: The listener model maps an utterance to a dis-
tribution over objects in the room. Each internal NP or RP
node is a distribution over objects in the room.
</figureCaption>
<bodyText confidence="0.99484">
Generation So far, we have defined the listener
strategy pL(g  |w). Given target o, the rational
speaker S(L) with respect to this listener needs to
compute argmaxw pL(o  |w) as dictated by (3). This
maximization is performed by enumerating all utter-
ances of bounded complexity.
</bodyText>
<subsectionHeader confidence="0.999837">
5.3 Modeling Listener Confusion
</subsectionHeader>
<bodyText confidence="0.975724272727273">
One shortcoming of the previous approach for ex-
tending a listener is that it falsely assumes that a lis-
tener can reliably interpret a simple utterance just as
well as it can a complex utterance.
We now describe a more realistic speaker which
is robust to listener confusion. Let α E [0, 1] be
a focus parameter which determines the confusion
level. Suppose we have a listener L. When presented
with an utterance w, for each application of the rela-
tivization rule, we have a 1− α probability of losing
focus. If we stay focused for the entire utterance
(with probability α|w|), then we interpret the utter-
ance according to pL. Otherwise (with probability
1 − α|w|), we guess an object at random according
to prnd(g  |w). We then use (3) to define the rational
speaker S(L) with respect the following “confused
listener” strategy:
pL(g  |w) = α|w|pL(g  |w) + (1 − α|w|)prnd(g  |w).
(7)
As α —* 0, the confused listener is more likely to
make a random guess, and thus there is a stronger
penalty against using more complex utterances. As
</bodyText>
<figure confidence="0.801675625">
ff[g E N(x)] w = (N x)
k
11 pL(g  |wj) w = (NP w1 ... wk)
j=1
pL(g  |(r, g0))pL(g0  |w0) w = (RP (R r) w0)
{
E
s&apos;
</figure>
<page confidence="0.997352">
415
</page>
<bodyText confidence="0.9945645">
α —* 1, the confused listener converges to pL and the
penalty for using complex utterances vanishes.
</bodyText>
<subsectionHeader confidence="0.991626">
5.4 The Taboo Setting
</subsectionHeader>
<bodyText confidence="0.999926826086957">
Notice that the rational speaker as defined so far
does not make full use of our grammar. Specifi-
cally, the rational speaker will never use the “wild-
card” noun something nor the relativization rule in
the grammar because an NP headed by the wildcard
something can always be replaced by the object ID
to obtain a higher utility. For instance, in Figure 6,
the NP spanning something right of O2 can be re-
placed by O3.
However, it is not realistic to assume that all ob-
jects can be referenced directly. To simulate scenar-
ios where some objects cannot be referenced directly
(and to fully exercise our grammar), we introduce
the taboo setting. In this setting, we remove from
the lexicon some fraction of the object IDs which are
closest to the target object. Since the tabooed objects
cannot be referenced directly, a speaker must resort
to use of the wildcard something and relativization.
For example, in Figure 7, we enable tabooing
around the target O1. This prevents the speaker from
referring directly to O3, so the speaker is forced to
describe O3 via the relativization rule, for example,
producing something right of O2.
</bodyText>
<figureCaption confidence="0.9878025">
Figure 7: With tabooing enabled around O1, O3 can no
longer be referred to directly (represented by an X).
</figureCaption>
<sectionHeader confidence="0.99739" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998435">
We now present our empirical results, showing that
rational speakers, who have embedded models of lis-
</bodyText>
<figureCaption confidence="0.974056333333333">
Figure 8: Mechanical Turk speaker task: Given the tar-
get object (e.g., O1), a human speaker must choose an
utterance to describe the object (e.g., right of O2).
</figureCaption>
<bodyText confidence="0.988269">
teners, can communicate more successfully than re-
flex speakers, who do not.
</bodyText>
<subsectionHeader confidence="0.993927">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999466344827586">
We collected 43 scenes (rooms) from the Google
Sketchup 3D Warehouse, each containing an aver-
age of 22 objects (household items and pieces of fur-
niture arranged in a natural configuration). For each
object o in a scene, we create a scenario, which rep-
resents an instance of the communication game with
o as the target object. There are a total of 2,860 sce-
narios, which we split evenly into a training set (de-
noted TR) and a test set (denoted TS).
We created the following two Amazon Mechani-
cal Turk tasks, which enable humans to play the lan-
guage game on the scenarios:
Speaker Task In this task, human annotators play
the role of speakers in the language game. They are
prompted with a target object o and asked to each
produce an utterance w (by selecting a preposition
w.r from a dropdown list and clicking on a reference
object w.o) that best informs a listener of the identity
of the target object.
For each training scenario o, we asked three
speakers to produce an utterance w. The three result-
ing (o, w) pairs are used to train the learned reflex
speaker (S:LITERAL). These pairs were also used to
train the learned reflex listener (L:LITERAL), where
the target o is treated as the guessed object. See Sec-
tion 4.1 for the details of the training procedure.
Listener Task In this task, human annotators play
the role of listeners. Given an utterance generated by
a speaker (human or not), the human listener must
</bodyText>
<page confidence="0.997057">
416
</page>
<figure confidence="0.555574">
Question: What object is right of O2 ?
</figure>
<figureCaption confidence="0.89421975">
Figure 9: Mechanical Turk listener task: a human listener
is prompted with an utterance generated by a speaker
(e.g., right of O2), and asked to click on an object (shown
by the red arrow).
</figureCaption>
<bodyText confidence="0.948218333333333">
guess the target object that the speaker saw by click-
ing on an object. The purpose of the listener task is
to evaluate speakers, as described in the next section.
</bodyText>
<table confidence="0.9991835">
Speaker Success Exact Match
S:LITERAL [reflex] 4.62% 1.11%
S(L:LITERAL) [rational] 33.65% 2.91%
S:LEARNED [reflex] 38.36% 5.44%
S(L:LEARNED) [rational] 52.63% 14.03%
S:HUMAN 41.41% 19.95%
</table>
<tableCaption confidence="0.843796">
Table 1: Comparison of various speakers on communica-
tive success and exact match, where only utterances of
complexity 1 are allowed. The rational speakers (with
</tableCaption>
<bodyText confidence="0.598099875">
respect to both the literal listener L:LITERAL and the
learned listener L:LEARNED) perform better than their
reflex counterparts. While the human speaker (composed
of three people) has higher exact match (it is better at
mimicking itself), the rational speaker S(L:LEARNED)
actually achieves higher communicative success than the
human listener.
define the exact match of a speaker S as follows:
</bodyText>
<figure confidence="0.679268333333333">
O2
O1
O3
6.2 Evaluation 1 E EpS:HUMAN(w  |o)pS(w  |o).
MATCH(S) = w
|TS |oETS (9)
</figure>
<bodyText confidence="0.977841952380952">
Utility (Communicative Success) We primarily
evaluate a speaker by its ability to communicate suc-
cessfully with a human listener. For each test sce-
nario, we asked three listeners to guess the object.
We use pL:HUMAN(g  |w) to denote the distribution
over guessed objects g given prompt w. For exam-
ple, if two of the three listeners guessed O1, then
pL:HUMAN(O1  |w) = 2. The expected utility (2) is
then computed by averaging the utility (communica-
tive success) over the test scenarios TS:
Exact Match As a secondary evaluation metric,
we also measure the ability of our speaker to exactly
match an utterance produced by a human speaker.
Note that since there are many ways of describing
an object, exact match is neither necessary nor suffi-
cient for successful communication.
We asked three human speakers to each pro-
duce an utterance w given a target o. We use
pS:HUMAN(w  |o) to denote this distribution; for ex-
ample, pS:HUMAN(right of O2  |o) = 13 if exactly one
of the three speakers uttered right of O2. We then
</bodyText>
<subsectionHeader confidence="0.986587">
6.3 Reflex versus Rational Speakers
</subsectionHeader>
<bodyText confidence="0.99999396">
We first evaluate speakers in the setting where only
utterances of complexity 1 are allowed. Table 1
shows the results on both success and exact match.
First, our main result is that the two rational speak-
ers S(L:LITERAL) and S(L:LEARNED), which each
model a listener explicitly, perform significantly bet-
ter than the corresponding reflex speakers, both in
terms of success and exact match.
Second, it is natural that the speakers that in-
volve learning (S:LITERAL and S(L:LITERAL))
outperform the speakers that only consider the
literal meaning of utterances (S:LEARNED and
S(L:LEARNED)), as the former models capture sub-
tler preferences using features.
Finally, we see that in terms of exact match, the
human speaker S:HUMAN performs the best (this
is not surprising because human exact match is es-
sentially the inter-annotator agreement), but in terms
of communicative success, S(L:LEARNED) achieves
a higher success rate than S:HUMAN, suggesting
that the game-theoretic modeling undertaken by the
rational speakers is effective for communication,
which is ultimate goal of language.
Note that exact match is low even for the “human
speaker”, since there are often many equally good
</bodyText>
<equation confidence="0.863827333333333">
SUCCESS(S) = EU(S, L:HUMAN) (8)
1 E pS(w|o)pL:HUMAN(o|w).
|TS |oETS �
</equation>
<page confidence="0.962832">
417
</page>
<figure confidence="0.9978472">
0.2 0.4 0.6 0.8 1.0
α
0.2 0.4 0.6 0.8 1.0
α
success
0.52
0.51
0.49
0.5
average lwl
2.0
1.8
1.6
1.4
1.2
</figure>
<figureCaption confidence="0.976800666666667">
Figure 10: Communicative success as a function of focus
parameter α without tabooing on TSDEV. The optimal
value of α is obtained at 0.79.
</figureCaption>
<bodyText confidence="0.999801125">
ways to evoke an object. At the same time, the suc-
cess rates for all speakers are rather low, reflecting
the fundamental difficulty of the setting: sometimes
it is impossible to unambiguously evoke the target
object via short utterances. In the next section, we
show that we can improve the success rate by al-
lowing the speakers to generate more complex utter-
ances.
</bodyText>
<subsectionHeader confidence="0.963109">
6.4 Generating More Complex Utterances
</subsectionHeader>
<bodyText confidence="0.999966153846154">
We now evaluate the rational speaker
S(L:LEARNED) when it is allowed to generate
utterances of complexity 1 or 2. Recall from
Section 5.3 that the speaker depends on a focus
parameter α, which governs the embedded listener’s
ability to interpret the utterance. We divided the test
set (TS) in two halves: TSDEV, which we used to
tune the value of α and TSFINAL, which we used to
evaluate success rates.
Figure 10 shows the communicative success as
a function of α on TSDEV. When α is small, the
embedded listener is confused more easily by more
complex utterances; therefore the speaker tends to
choose mostly utterances of complexity 1. As α
increases, the utterances increase in complexity, as
does the success rate. However, when α approaches
1, the utterances are too complex and the success
rate decreases. The dependence between α and av-
erage utterance complexity is shown in Figure 11.
Table 2 shows the success rates on TSFINAL for
α → 0 (all utterances have complexity 1), α = 1 (all
utterances have complexity 2), and α tuned to max-
imize the success rate based on TSDEV. Setting α
in this manner allows us to effectively balance com-
plexity and ambiguity, resulting in an improvement
in the success rate.
</bodyText>
<figureCaption confidence="0.968979">
Figure 11: Average utterance complexity as a function of
</figureCaption>
<table confidence="0.996677625">
the focus parameter α on TSDEV. Higher values of α
yield more complex utterances. Success
Taboo Success Success
Amount (α → 0) (α = 1) (α = α*) α*
0% 51.78% 50.99% 54.53% 0.79
5% 38.75% 40.83% 43.12% 0.89
10% 29.57% 29.69% 30.30% 0.80
30% 12.40% 13.04% 12.98% 0.81
</table>
<tableCaption confidence="0.99636">
Table 2: Communicative success (on TSFINAL) of the
</tableCaption>
<bodyText confidence="0.8778294">
rational speaker S(L:LEARNED) for various values of α
across different taboo amounts. When the taboo amount
is small, small values of α lead to higher success rates. As
the taboo amount increases, larger values of α (resulting
in more complex utterances) are better.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999465">
Starting with the view that the purpose of language
is successful communication, we developed a game-
theoretic model in which a rational speaker gener-
ates utterances by explicitly taking the listener into
account. On the task of generating spatial descrip-
tions, we showed the rational speaker substantially
outperforms a baseline reflex speaker that does not
have an embedded model. Our results therefore sug-
gest that a model of the pragmatics of communica-
tion is an important factor to consider for generation.
Acknowledgements This work was supported by
the National Science Foundation through a Gradu-
ate Research Fellowship to the first two authors. We
also would like to acknowledge Surya Murali, the
designer of the 3D Google Sketchup models, and
thank the anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9921735">
J. L. Austin. 1962. How to do Things with Words: The
William James Lectures delivered at Harvard Univer-
</reference>
<page confidence="0.983158">
418
</page>
<reference confidence="0.998442815789473">
sity in 1955. Oxford, Clarendon, UK.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128–135. Omnipress.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
J. Feldman and S. Narayanan. 2004. Embodied meaning
in a neural theory of language. Brain and Language,
89:385–392.
M. Fleischman and D. Roy. 2007. Representing inten-
tions in a cognitive model of language acquisition: Ef-
fects of phrase structure on situated verb learning. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
M. C. Frank, N. D. Goodman, and J. B. Tenenbaum.
2009. Using speakers’ referential intentions to model
early cross-situational word learning. Psychological
Science, 20(5):578–585.
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. In Journal of Artificial
Intelligence Research, volume 21, pages 429–470.
H. P. Grice. 1975. Syntax and Semantics; Logic and
Conversation. 3:Speech Acts:41–58.
G. Hinton. 1999. Products of experts. In International
Conference on Artificial Neural Networks (ICANN).
G. J¨ager. 2008. Game theory in semantics and pragmat-
ics. Technical report, University of T¨ubingen.
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward
understanding natural language directions. In Human-
Robot Interaction, pages 259–266.
Barbara Landau and Ray Jackendoff. 1993. ”what”
and ”where” in spatial language and spatial cognition.
Behavioral and Brain Sciences, 16(2spatial preposi-
tions analysis, cross linguistic conceptual similarities;
comments/response):217–238.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
T Regier and LA Carlson. 2001. Journal of experimen-
tal psychology. general; grounding spatial language in
perception: an empirical and computational investiga-
tion. 130(2):273–298.
Stefanie Tellex and Deb Roy. 2009. Grounding spatial
prepositions for video search. In ICMI.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960–967, Prague, Czech Republic.
Association for Computational Linguistics.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. In Asso-
ciation for the Advancement of Artificial Intelligence
(AAAI), pages 488–493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658–666.
</reference>
<page confidence="0.99867">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.878762">
<title confidence="0.99992">A Game-Theoretic Approach to Generating Spatial Descriptions</title>
<author confidence="0.999976">Dave Golland Percy Liang Dan Klein</author>
<affiliation confidence="0.999626">UC Berkeley UC Berkeley UC Berkeley</affiliation>
<address confidence="0.999853">Berkeley, CA 94720 Berkeley, CA 94720 Berkeley, CA 94720</address>
<email confidence="0.998895">dsg@cs.berkeley.edupliang@cs.berkeley.eduklein@cs.berkeley.edu</email>
<abstract confidence="0.991974866666667">Language is sensitive to both semantic and pragmatic effects. To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Specifically, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to do Things with Words: The William James Lectures delivered at Harvard University in 1955.</title>
<date>1962</date>
<location>Oxford, Clarendon, UK.</location>
<contexts>
<context position="2159" citStr="Austin, 1962" startWordPosition="336" endWordPosition="337">example of a 3D model of a room. The speaker’s goal is to reference the target object O1 by describing its spatial relationship to other object(s). The listener’s goal is to guess the object given the speaker’s description. work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequen</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>J. L. Austin. 1962. How to do Things with Words: The William James Lectures delivered at Harvard University in 1955. Oxford, Clarendon, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3236" citStr="Branavan et al., 2009" startWordPosition="505" endWordPosition="508">ing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as i</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>128--135</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="3077" citStr="Chen and Mooney, 2008" startWordPosition="479" endWordPosition="482">08) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual a</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>D. L. Chen and R. J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In International Conference on Machine Learning (ICML), pages 128–135. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Matthew Stone</author>
</authors>
<title>Managing ambiguities across utterances in dialogue.</title>
<date>2007</date>
<contexts>
<context position="3574" citStr="DeVault and Stone (2007)" startWordPosition="561" endWordPosition="564">of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some of the techniques from work on the semantics of spatial descriptions (Regier and Carlson, 2001; Gorniak and Roy, 2004; Tellex and Roy, 2009), we empirically demonstrate that having a model of pragmatics enables more successful communication. 2 Language as a Game To model Grice’s cooperative</context>
</contexts>
<marker>DeVault, Stone, 2007</marker>
<rawString>David DeVault and Matthew Stone. 2007. Managing ambiguities across utterances in dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="3261" citStr="Eisenstein et al., 2009" startWordPosition="509" endWordPosition="512">, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above</context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Feldman</author>
<author>S Narayanan</author>
</authors>
<title>Embodied meaning in a neural theory of language.</title>
<date>2004</date>
<journal>Brain and Language,</journal>
<pages>89--385</pages>
<contexts>
<context position="3028" citStr="Feldman and Narayanan, 2004" startWordPosition="471" endWordPosition="474">atics based on game theory, in the spirit of J¨ager (2008) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between mul</context>
</contexts>
<marker>Feldman, Narayanan, 2004</marker>
<rawString>J. Feldman and S. Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and Language, 89:385–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>D Roy</author>
</authors>
<title>Representing intentions in a cognitive model of language acquisition: Effects of phrase structure on situated verb learning.</title>
<date>2007</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3054" citStr="Fleischman and Roy, 2007" startWordPosition="475" endWordPosition="478">n the spirit of J¨ager (2008) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes in</context>
</contexts>
<marker>Fleischman, Roy, 2007</marker>
<rawString>M. Fleischman and D. Roy. 2007. Representing intentions in a cognitive model of language acquisition: Effects of phrase structure on situated verb learning. In Association for the Advancement of Artificial Intelligence (AAAI), Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Frank</author>
<author>N D Goodman</author>
<author>J B Tenenbaum</author>
</authors>
<title>Using speakers’ referential intentions to model early cross-situational word learning.</title>
<date>2009</date>
<journal>Psychological Science,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="3097" citStr="Frank et al., 2009" startWordPosition="483" endWordPosition="486"> stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We prese</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2009</marker>
<rawString>M. C. Frank, N. D. Goodman, and J. B. Tenenbaum. 2009. Using speakers’ referential intentions to model early cross-situational word learning. Psychological Science, 20(5):578–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Grounded semantic composition for visual scenes.</title>
<date>2004</date>
<journal>In Journal of Artificial Intelligence Research,</journal>
<volume>21</volume>
<pages>429--470</pages>
<contexts>
<context position="4000" citStr="Gorniak and Roy, 2004" startWordPosition="630" endWordPosition="633"> does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some of the techniques from work on the semantics of spatial descriptions (Regier and Carlson, 2001; Gorniak and Roy, 2004; Tellex and Roy, 2009), we empirically demonstrate that having a model of pragmatics enables more successful communication. 2 Language as a Game To model Grice’s cooperative principle (Grice, 1975), we formulate the interaction between a speaker S and a listener L as a cooperative game, that is, one in which S and L share the same utility function. For simplicity, we focus on the production and interpretation of single utterances, where the speaker and listener have access to a shared context. To simplify notation, we suppress writing the dependence on the context. The Communication Game 1. I</context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>Peter Gorniak and Deb Roy. 2004. Grounded semantic composition for visual scenes. In Journal of Artificial Intelligence Research, volume 21, pages 429–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Syntax and Semantics; Logic and Conversation. 3:Speech Acts:41–58.</title>
<date>1975</date>
<contexts>
<context position="2349" citStr="Grice, 1975" startWordPosition="365" endWordPosition="366">given the speaker’s description. work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation</context>
<context position="4198" citStr="Grice, 1975" startWordPosition="662" endWordPosition="663">odel of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some of the techniques from work on the semantics of spatial descriptions (Regier and Carlson, 2001; Gorniak and Roy, 2004; Tellex and Roy, 2009), we empirically demonstrate that having a model of pragmatics enables more successful communication. 2 Language as a Game To model Grice’s cooperative principle (Grice, 1975), we formulate the interaction between a speaker S and a listener L as a cooperative game, that is, one in which S and L share the same utility function. For simplicity, we focus on the production and interpretation of single utterances, where the speaker and listener have access to a shared context. To simplify notation, we suppress writing the dependence on the context. The Communication Game 1. In order to communicate a target o to L, S produces an utterance w chosen according to a strategy pS(w 1 o). 2. L interprets w and responds with a guess g according to a strategy pL(g I w). 3. S and </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. P. Grice. 1975. Syntax and Semantics; Logic and Conversation. 3:Speech Acts:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In International Conference on Artificial Neural Networks (ICANN).</booktitle>
<contexts>
<context position="17067" citStr="Hinton, 1999" startWordPosition="2878" endWordPosition="2880">y 1, we can use either L:LITERAL or L:LEARNED (defined in Sections 3 and 4). 414 Given a subtree w rooted at u E IN, NP, RP}, we define the denotation of w, JwK, to be a distribution over the objects in the scene in which the utterance was generated. The listener strategy pL(g|w) = JwK is recursively as follows: • If w is rooted at N with a single child x, then JwK is the uniform distribution over N(x), the set of objects consistent with the word x. • If w is rooted at NP, we recursively compute the distributions over objects g for each child tree, multiply the probabilities, and renormalize (Hinton, 1999). • If w is rooted at RP with relation r, we recursively compute the distribution over objects g0 for the child NP tree. We then appeal to the base case to produce a distribution over objects g which are related to g0 via relation r. This strategy is defined formally as follows: pL(g |w) 0c (6) Figure 6 shows an example of this bottomup denotation computation for the utterance on something right of O2 with respect to the scene in Figure 1. The denotation starts with the lowest NP node JO2K, which places all the mass on O2 in the scene. Moving up the tree, we compute the denotation of the RP, J</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>G. Hinton. 1999. Products of experts. In International Conference on Artificial Neural Networks (ICANN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J¨ager</author>
</authors>
<title>Game theory in semantics and pragmatics.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>University of T¨ubingen.</institution>
<marker>J¨ager, 2008</marker>
<rawString>G. J¨ager. 2008. Game theory in semantics and pragmatics. Technical report, University of T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kollar</author>
<author>S Tellex</author>
<author>D Roy</author>
<author>N Roy</author>
</authors>
<title>Toward understanding natural language directions.</title>
<date>2010</date>
<booktitle>In HumanRobot Interaction,</booktitle>
<pages>259--266</pages>
<contexts>
<context position="3283" citStr="Kollar et al., 2010" startWordPosition="513" endWordPosition="516">1 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some </context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward understanding natural language directions. In HumanRobot Interaction, pages 259–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Landau</author>
<author>Ray Jackendoff</author>
</authors>
<title>what” and ”where” in spatial language and spatial cognition. Behavioral and Brain Sciences, 16(2spatial prepositions analysis, cross linguistic conceptual similarities;</title>
<date>1993</date>
<pages>217--238</pages>
<contexts>
<context position="11880" citStr="Landau and Jackendoff (1993)" startWordPosition="1969" endWordPosition="1972">0 prepositions commonly used by people to describe objects in a preliminary data gathering experiment. This list includes multi-word units, which function equivalently to prepositions, such as left of. set of features, but they have different parameters. Furthermore, the first normalization sums over possible utterances w while the second normalization sums over possible objects g in the scene. The two parameter vectors are trained to optimize the loglikelihood of the training data under the respective models. Features We now describe the features φ(o, w). These features draw inspiration from Landau and Jackendoff (1993) and Tellex and Roy (2009). Each object o in the 3D scene is represented by its bounding box, which is the smallest rectangular prism containing o. The following are functions of the camera, target (or guessed object) o, and the reference object w.o in the utterance. The full set of features is obtained by conjoining these functions with indicator functions of the form ff[w.r = r], where r ranges over the set of valid prepositions. • Proximity functions measure the distance between o and w.o. This is implemented as the minimum over all the pairwise Euclidean distances between the corners of th</context>
</contexts>
<marker>Landau, Jackendoff, 1993</marker>
<rawString>Barbara Landau and Ray Jackendoff. 1993. ”what” and ”where” in spatial language and spatial cognition. Behavioral and Brain Sciences, 16(2spatial prepositions analysis, cross linguistic conceptual similarities; comments/response):217–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3118" citStr="Liang et al., 2009" startWordPosition="487" endWordPosition="490"> We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic mode</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Piantadosi</author>
<author>N D Goodman</author>
<author>B A Ellis</author>
<author>J B Tenenbaum</author>
</authors>
<title>A Bayesian model of the acquisition of compositional semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="1914" citStr="Piantadosi et al., 2008" startWordPosition="292" endWordPosition="295">er (a). In this paper, we present a game-theoretic model that captures this communication-oriented aspect of language interpretation and generation. Successful communication can be broken down into semantics and pragmatics. Most computational Figure 1: An example of a 3D model of a room. The speaker’s goal is to reference the target object O1 by describing its spatial relationship to other object(s). The listener’s goal is to guess the object given the speaker’s description. work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochastic setting. We show that G</context>
</contexts>
<marker>Piantadosi, Goodman, Ellis, Tenenbaum, 2008</marker>
<rawString>S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum. 2008. A Bayesian model of the acquisition of compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Regier</author>
<author>LA Carlson</author>
</authors>
<title>Journal of experimental psychology. general; grounding spatial language in perception: an empirical and computational investigation.</title>
<date>2001</date>
<pages>130--2</pages>
<contexts>
<context position="3977" citStr="Regier and Carlson, 2001" startWordPosition="626" endWordPosition="629">ffers from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some of the techniques from work on the semantics of spatial descriptions (Regier and Carlson, 2001; Gorniak and Roy, 2004; Tellex and Roy, 2009), we empirically demonstrate that having a model of pragmatics enables more successful communication. 2 Language as a Game To model Grice’s cooperative principle (Grice, 1975), we formulate the interaction between a speaker S and a listener L as a cooperative game, that is, one in which S and L share the same utility function. For simplicity, we focus on the production and interpretation of single utterances, where the speaker and listener have access to a shared context. To simplify notation, we suppress writing the dependence on the context. The </context>
</contexts>
<marker>Regier, Carlson, 2001</marker>
<rawString>T Regier and LA Carlson. 2001. Journal of experimental psychology. general; grounding spatial language in perception: an empirical and computational investigation. 130(2):273–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Deb Roy</author>
</authors>
<title>Grounding spatial prepositions for video search.</title>
<date>2009</date>
<booktitle>In ICMI.</booktitle>
<contexts>
<context position="4023" citStr="Tellex and Roy, 2009" startWordPosition="634" endWordPosition="637">ormal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above). Though we use some of the techniques from work on the semantics of spatial descriptions (Regier and Carlson, 2001; Gorniak and Roy, 2004; Tellex and Roy, 2009), we empirically demonstrate that having a model of pragmatics enables more successful communication. 2 Language as a Game To model Grice’s cooperative principle (Grice, 1975), we formulate the interaction between a speaker S and a listener L as a cooperative game, that is, one in which S and L share the same utility function. For simplicity, we focus on the production and interpretation of single utterances, where the speaker and listener have access to a shared context. To simplify notation, we suppress writing the dependence on the context. The Communication Game 1. In order to communicate </context>
<context position="11906" citStr="Tellex and Roy (2009)" startWordPosition="1974" endWordPosition="1977">eople to describe objects in a preliminary data gathering experiment. This list includes multi-word units, which function equivalently to prepositions, such as left of. set of features, but they have different parameters. Furthermore, the first normalization sums over possible utterances w while the second normalization sums over possible objects g in the scene. The two parameter vectors are trained to optimize the loglikelihood of the training data under the respective models. Features We now describe the features φ(o, w). These features draw inspiration from Landau and Jackendoff (1993) and Tellex and Roy (2009). Each object o in the 3D scene is represented by its bounding box, which is the smallest rectangular prism containing o. The following are functions of the camera, target (or guessed object) o, and the reference object w.o in the utterance. The full set of features is obtained by conjoining these functions with indicator functions of the form ff[w.r = r], where r ranges over the set of valid prepositions. • Proximity functions measure the distance between o and w.o. This is implemented as the minimum over all the pairwise Euclidean distances between the corners of the bounding boxes. We also </context>
</contexts>
<marker>Tellex, Roy, 2009</marker>
<rawString>Stefanie Tellex and Deb Roy. 2009. Grounding spatial prepositions for video search. In ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1888" citStr="Wong and Mooney, 2007" startWordPosition="288" endWordPosition="291">erefore be preferred over (a). In this paper, we present a game-theoretic model that captures this communication-oriented aspect of language interpretation and generation. Successful communication can be broken down into semantics and pragmatics. Most computational Figure 1: An example of a 3D model of a room. The speaker’s goal is to reference the target object O1 by describing its spatial relationship to other object(s). The listener’s goal is to guess the object given the speaker’s description. work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochast</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>488--493</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2999" citStr="Yu and Ballard, 2004" startWordPosition="467" endWordPosition="470">velop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochastic setting. We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of colla</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>C. Yu and D. H. Ballard. 2004. On the integration of grounding language and learning objects. In Association for the Advancement of Artificial Intelligence (AAAI), pages 488–493, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1865" citStr="Zettlemoyer and Collins, 2005" startWordPosition="284" endWordPosition="287">he target object, and should therefore be preferred over (a). In this paper, we present a game-theoretic model that captures this communication-oriented aspect of language interpretation and generation. Successful communication can be broken down into semantics and pragmatics. Most computational Figure 1: An example of a 3D model of a room. The speaker’s goal is to reference the target object O1 by describing its spatial relationship to other object(s). The listener’s goal is to guess the object given the speaker’s description. work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but e</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>