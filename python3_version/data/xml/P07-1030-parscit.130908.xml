<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008873">
<title confidence="0.999393">
Fully Unsupervised Discovery of Concept-Specific Relationships
by Web Mining
</title>
<author confidence="0.968934">
Dmitry Davidov
</author>
<affiliation confidence="0.784222333333333">
ICNC
The Hebrew University
Jerusalem 91904, Israel
</affiliation>
<email confidence="0.993943">
dmitry@alice.nc.huji.ac.il
</email>
<author confidence="0.992645">
Ari Rappoport
</author>
<affiliation confidence="0.821882333333333">
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
</affiliation>
<email confidence="0.871724">
www.cs.huji.ac.il/∼arir
</email>
<author confidence="0.99333">
Moshe Koppel
</author>
<affiliation confidence="0.9914095">
Dept. of Computer Science
Bar-Ilan University
</affiliation>
<address confidence="0.690915">
Ramat-Gan 52900, Israel
</address>
<email confidence="0.996369">
koppel@cs.biu.ac.il
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999758538461539">
We present a web mining method for discov-
ering and enhancing relationships in which a
specified concept (word class) participates.
We discover a whole range of relationships
focused on the given concept, rather than
generic known relationships as in most pre-
vious work. Our method is based on cluster-
ing patterns that contain concept words and
other words related to them. We evaluate the
method on three different rich concepts and
find that in each case the method generates a
broad variety of relationships with good pre-
cision.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972862745098">
The huge amount of information available on the
web has led to a flurry of research on methods for
automatic creation of structured information from
large unstructured text corpora. The challenge is to
create as much information as possible while pro-
viding as little input as possible.
A lot of this research is based on the initial insight
(Hearst, 1992) that certain lexical patterns (‘X is a
country’) can be exploited to automatically gener-
ate hyponyms of a specified word. Subsequent work
(to be discussed in detail below) extended this initial
idea along two dimensions.
One objective was to require as small a user-
provided initial seed as possible. Thus, it was ob-
served that given one or more such lexical patterns,
a corpus could be used to generate examples of hy-
ponyms that could then, in turn, be exploited to gen-
erate more lexical patterns. The larger and more reli-
able sets of patterns thus generated resulted in larger
and more precise sets of hyponyms and vice versa.
The initial step of the resulting alternating bootstrap
process – the user-provided input – could just as well
consist of examples of hyponyms as of lexical pat-
terns.
A second objective was to extend the information
that could be learned from the process beyond hy-
ponyms of a given word. Thus, the approach was
extended to finding lexical patterns that could pro-
duce synonyms and other standard lexical relations.
These relations comprise all those words that stand
in some known binary relation with a specified word.
In this paper, we introduce a novel extension of
this problem: given a particular concept (initially
represented by two seed words), discover relations
in which it participates, without specifying their
types in advance. We will generate a concept class
and a variety of natural binary relations involving
that class.
An advantage of our method is that it is particu-
larly suitable for web mining, even given the restric-
tions on query amounts that exist in some of today’s
leading search engines.
The outline of the paper is as follows. In the next
section we will define more precisely the problem
we intend to solve. In section 3, we will consider re-
lated work. In section 4 we will provide an overview
of our solution and in section 5 we will consider the
details of the method. In section 6 we will illustrate
and evaluate the results obtained by our method. Fi-
nally, in section 7 we will offer some conclusions
and considerations for further work.
</bodyText>
<page confidence="0.927728">
232
</page>
<note confidence="0.32991">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998792688172043">
2 Problem Definition 1992; Pantel et al, 2004), synonymy (Roark and
In several studies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; Davi-
Pantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland
it has been shown that relatively unsupervised and and Charniak, 1999).
language-independent methods could be used to In addition to these basic types, several stud-
generate many thousands of sets of words whose ies deal with the discovery and labeling of more
semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb re-
amination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and noun-
why these words have been grouped together into compound relationships (Moldovan et al, 2004).
a single concept, it is important to emphasize that Studying relationships between tagged named en-
the method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006)
nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that as-
of the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of
and comparison with standard lists indicate that the pairs into several clusters, where each cluster corre-
generated sets correspond to concepts with high pre- sponds to one of a known relationship type. These
cision. studies, however, focused on the classification of
We wish now to build on that result in the fol- pairs that were either given or extracted using some
lowing way. Given a large corpus (such as the web) supervision, rather than on discovery and definition
and two or more examples of some concept X, au- of which relationships are actually in the corpus.
tomatically generate examples of one or more rela- Several papers report on methods for using the
tions R C X x Y , where Y is some concept and R web to discover instances of binary relations. How-
is some binary relationship between elements of X ever, each of these assumes that the relations them-
and elements of Y . selves are known in advance (implicitly or explic-
We can think of the relations we wish to gener- itly) so that the method can be provided with seed
ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al,
the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), rela-
one-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exem-
tals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfon-
regions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006).
countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006),
given class X, we would like to generate not one but it has been shown that related pairs can be gener-
possibly many different such relations. ated without pre-specifying the nature of the rela-
The only input we require, aside from a corpus, tion sought. However, this work does not focus on
is a small set of examples of some class. However, differentiating among different relations, so that the
since such sets can be generated in entirely unsuper- generated relations might conflate a number of dis-
vised fashion, our challenge is effectively to gener- tinct ones.
ate relations directly from a corpus given no addi- It should be noted that some of these papers utilize
tional information of any kind. The key point is that language and domain-dependent preprocessing in-
we do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and
types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while
3 Related Work others take advantage of handcrafted databases such
As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al,
addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006).
in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern dis-
itly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised
work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two
233 pairs of words; however, relationship types were not
discovered explicitly.
4 Outline of the Method 5.1 Generalizing the seed
We will use two concept words contained in a con- The first step is to take the seed, which might con-
cept class C to generate a collection of distinct re- sist of as few as two concept words, and generate
lations in which C participates. In this section we many (ideally, all, when the concept is a closed set
offer a brief overview of our method. of words) members of the class to which they be-
Step 1: Use a seed consisting of two (or more) ex- long. We do this as follows, essentially implement-
ample words to automatically obtain other examples ing a simplified version of the method of Davidov
that belong to the same class. Call these concept and Rappoport (2006). For any pair of seed words
words. (For instance, if our example words were Si and Sj, search the corpus for word patterns of the
France and Angola, we would generate more coun- form SiHSj, where H is a high-frequency word in
try names.) the corpus (we used the 100 most frequent words
Step 2: For each concept word, collect instances in the corpus). Of these, we keep all those pat-
of contexts in which the word appears together with terns, which we call symmetric patterns, for which
one other content word. Call this other word a tar- SjHSi is also found in the corpus. Repeat this pro-
get word for that concept word. (For example, for cess to find symmetric patterns with any of the struc-
France we might find ‘Paris is the capital of France’. tures HSHS, SHSH or SHHS. It was shown in
Paris would be a target word for France.) (Davidov and Rappoport, 2006) that pairs of words
Step 3: For each concept word, group the contexts that often appear together in such symmetric pat-
in which it appears according to the target word that terns tend to belong to the same class (that is, they
appears in the context. (Thus ‘X is the capital of Y ’ share some notable aspect of their semantics). Other
would likely be grouped with ‘Y ’s capital is X’.) words in the class can thus be generated by search-
Step 4: Identify similar context groups that ap- ing a sub-corpus of documents including at least two
pear across many different concept words. Merge concept words for those words X that appear in a
these into a single concept-word-independent clus- sufficient number of instances of both the patterns
ter. (The group including the two contexts above SiHX and XHSi, where Si is a word in the class.
would appear, with some variation, for other coun- The same can be done for the other three pattern
tries as well, and all these would be merged into structures. The process can be bootstrapped as more
a single cluster representing the relation capital- words are added to the class.
of(X,Y).) Note that our method differs from that of Davidov
Step 5: For each cluster, output the relation con- and Rappoport (2006) in that here we provide an ini-
sisting of all &lt;concept word, target word&gt; pairs that tial seed pair, representing our target concept, while
appear together in a context included in the cluster. there the goal is grouping of as many words as pos-
(The cluster considered above would result in a set sible into concept classes. The focus of our paper is
of pairs consisting of a country and its capital. Other on relations involving a specific concept.
clusters generated by the same seed might include 5.2 Collecting contexts
countries and their languages, countries and the re- For each concept word S, we search the corpus for
gions in which they are located, and so forth.) distinct contexts in which S appears. (For our pur-
5 Details of the Method poses, a context is a window with exactly five words
In this section we consider the details of each of or punctuation marks before or after the concept
the above-enumerated steps. It should be noted word; we choose 10,000 of these, if available.) We
that each step can be performed using standard web call the aggregate text found in all these context win-
searches; no special pre-processed corpus is re- dows the S-corpus.
quired. From among these contexts, we choose all pat-
234 terns of the form H1SH2XH3 or H1XH2SH3,
where:
</bodyText>
<listItem confidence="0.888627777777778">
• X is a word that appears with frequency below
f1 in the S-corpus and that has sufficiently high
pointwise mutual information with S. We use
these two criteria to ensure that X is a content
word and that it is related to S. The lower the
threshold f1, the less noise we allow in, though
possibly at the expense of recall. We used f1 =
1, 000 occurrences per million words.
• H2 is a string of words each of which occurs
</listItem>
<bodyText confidence="0.8439771">
with frequency above f2 in the S-corpus. We
want H2 to consist mainly of words common
in the context of S in order to restrict patterns
to those that are somewhat generic. Thus, in
the context of countries we would like to retain
words like capital while eliminating more spe-
cific words that are unlikely to express generic
patterns. We used f2 = 100 occurrences per
million words (there is room here for automatic
optimization, of course).
</bodyText>
<listItem confidence="0.9677933">
• H1 and H3 are either punctuation or words that
occur with frequency above f3 in the S-corpus.
This is mainly to ensure that X and S aren’t
fragments of multi-word expressions. We used
f3 = 100 occurrences per million words.
• We call these patterns, S-patterns and we call
X the target of the S-pattern. The idea is that S
and X very likely stand in some fixed relation
to each other where that relation is captured by
the S-pattern.
</listItem>
<subsectionHeader confidence="0.998252">
5.3 Grouping S-patterns
</subsectionHeader>
<bodyText confidence="0.999803777777778">
If S is in fact related to X in some way, there might
be a number of S-patterns that capture this relation-
ship. For each X, we group all the S-patterns that
have X as a target. (Note that two S-patterns with
two different targets might be otherwise identical,
so that essentially the same pattern might appear in
two different groups.) We now merge groups with
large (more than 2/3) overlap. We call the resulting
groups, S-groups.
</bodyText>
<subsectionHeader confidence="0.999086">
5.4 Identifying pattern clusters
</subsectionHeader>
<bodyText confidence="0.999984941176471">
If the S-patterns in a given S-group actually capture
some relationship between S and the target, then
one would expect that similar groups would appear
for a multiplicity of concept words S. Suppose that
we have S-groups for three different concept words
S such that the pairwise overlap among the three
groups is more than 2/3 (where for this purpose two
patterns are deemed identical if they differ only at S
and X). Then the set of patterns that appear in two or
three of these S-groups is called a cluster core. We
now group all patterns in other S-groups that have an
overlap of more than 2/3 with the cluster core into a
candidate pattern pool P. The set of all patterns in
P that appear in at least two S-groups (among those
that formed P) pattern cluster. A pattern cluster that
has patterns instantiated by at least half of the con-
cept words is said to represent a relation.
</bodyText>
<subsectionHeader confidence="0.981103">
5.5 Refining relations
</subsectionHeader>
<bodyText confidence="0.999981789473684">
A relation consists of pairs (S, X) where S is a con-
cept word and X is the target of some S-pattern in a
given pattern cluster. Note that for a given S, there
might be one or many values of X satisfying the re-
lation. As a final refinement, for each given S, we
rank all such X according to pointwise mutual in-
formation with S and retain only the highest 2/3. If
most values of S have only a single corresponding X
satisfying the relation and the rest have none, we try
to automatically fill in the missing values by search-
ing the corpus for relevant S-patterns for the missing
values of S. (In our case the corpus is the web, so
we perform additional clarifying queries.)
Finally, we delete all relations in which all con-
cept words are related to most target words and all
relations in which the concept words and the target
words are identical. Such relations can certainly be
of interest (see Section 7), but are not our focus in
this paper.
</bodyText>
<subsectionHeader confidence="0.934916">
5.6 Notes on required Web resources
</subsectionHeader>
<bodyText confidence="0.99981325">
In our implementation we use the Google search
engine. Google restricts individual users to 1,000
queries per day and 1,000 pages per query. In each
stage we conducted queries iteratively, each time
downloading all 1,000 documents for the query.
In the first stage our goal was to discover sym-
metric relationships from the web and consequently
discover additional concept words. For queries in
this stage of our algorithm we invoked two require-
ments.
First, the query should contain at least two con-
cept words. This proved very effective in reduc-
</bodyText>
<page confidence="0.913836">
235
</page>
<bodyText confidence="0.998831986111111">
ing ambiguity. Thus of 1,000 documents for the erated relation, authoritative resources must be mar-
query bass, 760 deal with music, while if we add to shaled as a gold standard. For purposes of evalu-
the query a second word from the intended concept ation, we ran our algorithm on three representative
(e.g., barracuda), then none of the 1,000 documents domains – countries, fish species and star constel-
deal with music and the vast majority deal with fish, lations – and tracked down gold standard resources
as intended. (encyclopedias, academic texts, informative web-
Second, we avoid doing overlapping queries. To sites, etc) for the bulk of the relations generated in
do this we used Google’s ability to exclude from each domain.
search results those pages containing a given term This choice of domains allowed us to explore
(in our case, one of the concept words). different aspects of algorithmic behavior. Country
We performed up to 300 different queries for in- and constellation domains are both well defined and
dividual concepts in the first stage of our algorithm. closed domains. However they are substantially dif-
In the second stage, we used web queries to as- ferent.
semble S-corpora. On average, about 1/3 of the con- Country names is a relatively large domain which
cept words initially lacked sufficient data and we has very low lexical ambiguity, and a large number
performed up to twenty additional queries for each of potentially useful relations. The main challenge
rare concept word to fill its corpus. in this domain was to capture it well.
In the last stage, when clusters are constructed, Constellation names, in contrast, are a relatively
we used web queries for filling missing pairs of one- small but highly ambiguous domain. They are used
to-one or several-to-several relationships. The to- in proper names, mythology, names of entertainment
tal number of filling queries for a specific concept facilities etc. Our evaluation examined how well the
was below 1,000, and we needed only the first re- algorithm can deal with such ambiguity.
sults of these queries. Empirically, it took between The fish domain contains a very high number of
0.5 to 6 day limits (i.e., 500–6,000 queries) to ex- members. Unlike countries, it is a semi-open non-
tract relationships for a concept, depending on its homogenous domain with a very large number of
size (the number of documents used for each query subclasses and groups. Also, unlike countries, it
was at most 100). Obviously this strategy can be does not contain many proper nouns, which are em-
improved by focused crawling from primary Google pirically generally easier to identify in patterns. So
hits, which can drastically reduce the required num- the main challenge in this domain is to extract un-
ber of queries. blurred relationships and not to diverge from the do-
6 Evaluation main during the concept acquisition phase.
In this section we wish to consider the variety of re- We do not show here all-to-all relationships such
lations that can be generated by our method from a as fish parts (common to all or almost all fish), be-
given seed and to measure the quality of these rela- cause we focus on relationships that separate be-
tions in terms of their precision and recall. tween members of the concept class, which are
With regard to precision, two claims are being harder to acquire and evaluate.
made. One is that the generated relations correspond 6.1 Countries
to identifiable relations. The other claim is that to Our seed consisted of two country names. The in-
the extent that a generated relation can be reason- tended result for the first stage of the algorithm
ably identified, the generated pairs do indeed belong was a list of countries. There are 193 countries in
to the identified relation. (There is a small degree of the world (www.countrywatch.com) some of which
circularity in this characterization but this is proba- have multiple names so that the total number of
bly the best we can hope for.) commonly used country names is 243. Of these,
As a practical matter, it is extremely difficult to 223 names (comprising 180 countries) are charac-
measure precision and recall for relations that have ter strings with no white space. Since we consider
not been pre-determined in any way. For each gen- only single word names, these 223 are the names we
236 hope to capture in this stage.
Using the seed words France and Angola, we
obtained 202 country names (comprising 167 dis-
tinct countries) as well as 32 other names (consisting
mostly of names of other geopolitical entities). Us-
ing the list of 223 single word countries as our gold
standard, this gives precision of 0.90 and recall of
0.86. (Ten other seed pairs gave results ranging in
precision: 0.86-0.93 and recall: 0.79-0.90.)
The second part of the algorithm generated a set
of 31 binary relations. Of these, 25 were clearly
identifiable relations many of which are shown in
Table 1. Note that for three of these there are stan-
dard exhaustive lists against which we could mea-
sure both precision and recall; for the others shown,
sources were available for measuring precision but
no exhaustive list was available from which to mea-
sure recall, so we measured coverage (the number
of countries for which at least one target concept is
found as related).
Another eleven meaningful relations were gener-
ated for which we did not compute precision num-
bers. These include celebrity-from, animal-of, lake-
in, borders-on and enemy-of. (The set of relations
generated by other seed pairs differed only slightly
from those shown here for France and Angola.)
</bodyText>
<subsectionHeader confidence="0.999065">
6.2 Fish species
</subsectionHeader>
<bodyText confidence="0.999343076923077">
In our second experiment, our seed consisted of two
fish species, barracuda and bluefish. There are 770
species listed in WordNet of which 447 names are
character strings with no white space. The first stage
of the algorithm returned 305 of the species listed
in Wordnet, another 37 species not listed in Word-
net, as well as 48 other names (consisting mostly
of other sea creatures). The second part of the al-
gorithm generated a set of 15 binary relations all of
which are meaningful. Those for which we could
find some gold standard are listed in Table 2.
Other relations generated include served-with,
bait-for, food-type, spot-type, and gill-type.
</bodyText>
<subsectionHeader confidence="0.994293">
6.3 Constellations
</subsectionHeader>
<bodyText confidence="0.994627166666667">
Our seed consisted of two constellation names,
Orion and Cassiopeia. There are 88 standard
constellations (www.astro.wisc.edu) some of which
have multiple names so that the total number of com-
monly used constellations is 98. Of these, 87 names
(77 constellations) are strings with no white space.
</bodyText>
<table confidence="0.999873733333334">
Relationship Prec. Rec/Cov
Sample pattern
(Sample pair)
capital-of 0.92 R=0.79
in (x), capital of (y),
(Luanda, Angola)
language-spoken-in 0.92 R=0.60
to (x) or other (y) speaking
(Spain, Spanish)
in-region 0.73 R=0.71
throughout (x), from (y) to
(America, Canada)
city-in 0.82 C=0.95
west (x) – forecast for (y).
(England, London)
river-in 0.92 C=0.68
central (x), on the (y) river
(China, Haine)
mountain-range-in 0.77 C=0.69
the (x) mountains in (y) ,
(Chella, Angola)
sub-region-of 0.81 C=0.81
the (y) region of (x),
(Veneto, Italy)
industry-of 0.70 C=0.90
the (x) industry in (y) ,
(Oil, Russia)
island-in 0.98 C=0.55
, (x) island, (y) ,
(Bathurst, Canada)
president-of 0.86 C=0.51
president (x) of (y) has
(Bush, USA)
political-position-in 0.81 C=0.75
former (x) of (y) face
(President, Ecuador)
political-party-of 0.91 C=0.53
the (x) party of (y) ,
(Labour, England)
festival-of 0.90 C=0.78
the (x) festival, (y) ,
(Tanabata, Japan)
religious-denomination-of 0.80 C=0.62
the (x) church in (y) ,
(Christian, Rome)
</table>
<tableCaption confidence="0.999916">
Table 1: Results on seed { France, Angola }.
</tableCaption>
<page confidence="0.9068">
237
</page>
<table confidence="0.999921766666667">
Relationship Prec. Cov
Sample pattern
(Sample pair)
region-found-in 0.83 0.80
best (x) fishing in (y) .
(Walleye, Canada)
sea-found-in 0.82 0.64
of (x) catches in the (y) sea
(Shark, Adriatic)
lake-found-in 0.79 0.51
lake (y) is famous for (x) ,
(Marion, Catfish)
habitat-of 0.78 0.92
, (x) and other (y) fish
(Menhaden, Saltwater)
also-called 0.91 0.58
. (y) , also called (x) ,
(Lemonfish, Ling)
eats 0.90 0.85
the (x) eats the (y) and
(Perch, Minnow)
color-of 0.95 0.85
the (x) was (y) color
(Shark, Gray)
used-for-food 0.80 0.53
catch (x) – best for (y) or
(Bluefish, Sashimi)
in-family 0.95 0.60
the (x) family , includes (y) ,
(Salmonid, Trout)
</table>
<tableCaption confidence="0.999815">
Table 2: Results on seed { barracud, bluefish }.
</tableCaption>
<bodyText confidence="0.9999421875">
The first stage of the algorithm returned 81 constel-
lation names (77 distinct constellations) as well as
38 other names (consisting mostly of names of indi-
vidual stars). Using the list of 87 single word con-
stellation names as our gold standard, this gives pre-
cision of 0.68 and recall of 0.93.
The second part of the algorithm generated a set
of ten binary relations. Of these, one concerned
travel and entertainment (constellations are quite
popular as names of hotels and lounges) and another
three were not interesting. Apparently, the require-
ment that half the constellations appear in a relation
limited the number of viable relations since many
constellations are quite obscure. The six interesting
relations are shown in Table 3 along with precision
and coverage.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="introduction">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999895953488372">
In this paper we have addressed a novel type of prob-
lem: given a specific concept, discover in fully un-
supervised fashion, a range of relations in which it
participates. This can be extremely useful for study-
ing and researching a particular concept or field of
study.
As others have shown as well, two concept words
can be sufficient to generate almost the entire class
to which the words belong when the class is well-
defined. With the method presented in this paper,
using no further user-provided information, we can,
for a given concept, automatically generate a diverse
collection of binary relations on this concept. These
relations need not be pre-specified in any way. Re-
sults on the three domains we considered indicate
that, taken as an aggregate, the relations that are gen-
erated for a given domain paint a rather clear picture
of the range of information pertinent to that domain.
Moreover, all this was done using standard search
engine methods on the web. No language-dependent
tools were used (not even stemming); in fact, we re-
produced many of our results using Google in Rus-
sian.
The method depends on a number of numerical
parameters that control the subtle tradeoff between
quantity and quality of generated relations. There is
certainly much room for tuning of these parameters.
The concept and target words used in this paper
are single words. Extending this to multiple-word
expressions would substantially contribute to the ap-
plicability of our results.
In this research we effectively disregard many re-
lationships of an all-to-all nature. However, such
relationships can often be very useful for ontology
construction, since in many cases they introduce
strong connections between two different concepts.
Thus, for fish we discovered that one of the all-to-
all relationships captures a precise set of fish body
parts, and another captures swimming verbs. Such
relations introduce strong and distinct connections
between the concept of fish and the concepts of fish-
body-parts and swimming. Such connections may
be extremely useful for ontology construction.
</bodyText>
<page confidence="0.991168">
238
</page>
<table confidence="0.999808619047619">
Relationship Prec. Cov
Sample pattern
(Sample pair)
nearby-constellation 0.87 0.70
constellation (x), near (y),
(Auriga, Taurus)
star-in 0.82 0.76
star (x) in (y) is
(Antares, Scorpius)
shape-of 0.90 0.55
, (x) is depicted as (y).
(Lacerta, Lizard)
abbreviated-as 0.93 0.90
. (x) abbr (y),
(Hidra, Hya)
cluster-types-in 0.92 1.00
famous (x) cluster in (y),
(Praesepe, Cancer)
location 0.82 0.70
, (x) is a (y) constellation
(Draco, Circumpolar)
</table>
<tableCaption confidence="0.999868">
Table 3: Results on seed { Orion, Cassiopeia }.
</tableCaption>
<sectionHeader confidence="0.99594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888316666667">
Agichtein, E., Gravano, L., 2000. Snowball: Extracting
relations from large plain-text collections. Proceedings
of the 5th ACM International Conference on Digital
Libraries.
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
Workshop on Ontology Learning and Population at
COLING-ACL ’06.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ’99.
Chklovski T., Pantel P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
’04.
Costello, F., Veale, T., Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds, COLING-ACL ’06.
Davidov, D., Rappoport, A., 2006. Efficient unsupervised
discovery of word categories using symmetric patterns
and high frequency words. COLING-ACL ’06.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., Yates, A., 2004.
Methods for domain-independent information extrac-
tion from the web: an experimental comparison. AAAI
’04.
Hasegawa, T., Sekine, S., Grishman, R., 2004. Discover-
ing relations among named entities from large corpora.
ACL ’04.
Hassan, H., Hassan, A., Emam, O., 2006. unsupervised
information extraction approach using graph mutual
reinforcement. EMNLP ’06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ’92.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. Workshop on Comput. Lexical
Semantics at HLT-NAACL ’04.
Pantel, P., Ravichandran, D., Hovy, E., 2004. Towards
terascale knowledge acquisition. COLING ’04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006.
Names and similarities on the web: fact extraction in
the fast lane. COLING-ACL ’06.
Roark, B., Charniak, E., 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lex-
icon construction. ACL ’98.
Rosenfeld B., Feldman, R.: URES : an unsupervised
web relation extraction system. Proceedings, ACL ’06
Poster Sessions.
Sekine, S., 2006 On-demand information extraction.
COLING-ACL ’06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ’06.
Suchanek F. M., G. Ifrim, G. Weikum. 2006. LEILA:
learning to extract information by linguistic analysis.
Workshop on Ontology Learning and Population at
COLING-ACL ’06.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ’06.
Widdows, D., Dorow, B., 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ’02.
</reference>
<page confidence="0.998637">
239
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548976">
<title confidence="0.999536">Fully Unsupervised Discovery of Concept-Specific Relationships</title>
<author confidence="0.8720415">by Web Mining Dmitry Davidov</author>
<affiliation confidence="0.993363">ICNC The Hebrew University</affiliation>
<address confidence="0.999919">Jerusalem 91904, Israel</address>
<email confidence="0.999042">dmitry@alice.nc.huji.ac.il</email>
<author confidence="0.996572">Ari Rappoport</author>
<affiliation confidence="0.9998985">Institute of Computer Science The Hebrew University</affiliation>
<address confidence="0.99955">Jerusalem 91904, Israel</address>
<author confidence="0.99739">Moshe Koppel</author>
<affiliation confidence="0.999954">Dept. of Computer Science Bar-Ilan University</affiliation>
<address confidence="0.99886">Ramat-Gan 52900, Israel</address>
<email confidence="0.999197">koppel@cs.biu.ac.il</email>
<abstract confidence="0.981465">We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>Proceedings of the 5th ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="5935" citStr="Agichtein and Gravano, 2000" startWordPosition="963" endWordPosition="966">of some concept X, au- of which relationships are actually in the corpus. tomatically generate examples of one or more rela- Several papers report on methods for using the tions R C X x Y , where Y is some concept and R web to discover instances of binary relations. Howis some binary relationship between elements of X ever, each of these assumes that the relations themand elements of Y . selves are known in advance (implicitly or explicWe can think of the relations we wish to gener- itly) so that the method can be provided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that r</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Agichtein, E., Gravano, L., 2000. Snowball: Extracting relations from large plain-text collections. Proceedings of the 5th ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Alfonseca</author>
<author>M Ruiz-Casado</author>
<author>M Okumura</author>
<author>P Castells</author>
</authors>
<title>Towards large-scale non-taxonomic relation extraction: estimating the precision of rote extractors.</title>
<date>2006</date>
<booktitle>Workshop on Ontology Learning and Population at COLING-ACL ’06.</booktitle>
<marker>Alfonseca, Ruiz-Casado, Okumura, Castells, 2006</marker>
<rawString>Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells, P., 2006. Towards large-scale non-taxonomic relation extraction: estimating the precision of rote extractors. Workshop on Ontology Learning and Population at COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<journal>ACL</journal>
<volume>99</volume>
<marker>Berland, Charniak, 1999</marker>
<rawString>Berland, M., Charniak, E., 1999. Finding parts in very large corpora. ACL ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>VerbOcean: mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<journal>EMNLP</journal>
<volume>04</volume>
<contexts>
<context position="4297" citStr="Chklovski and Pantel, 2004" startWordPosition="687" endWordPosition="690">dies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; DaviPantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004). a single concept, it is important to emphasize that Studying relationships between tagged named enthe method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006) nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that asof the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of and comparison with standard lists indicate that the pairs into several clusters, where</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Chklovski T., Pantel P., 2004. VerbOcean: mining the web for fine-grained semantic verb relations. EMNLP ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Costello</author>
<author>T Veale</author>
<author>S Dunne</author>
</authors>
<title>Using WordNet to automatically deduce relations between words in noun-noun compounds,</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<marker>Costello, Veale, Dunne, 2006</marker>
<rawString>Costello, F., Veale, T., Dunne, S., 2006. Using WordNet to automatically deduce relations between words in noun-noun compounds, COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="3800" citStr="Davidov and Rappoport, 2006" startWordPosition="610" endWordPosition="613">section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. 232 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Problem Definition 1992; Pantel et al, 2004), synonymy (Roark and In several studies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; DaviPantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004).</context>
<context position="9637" citStr="Davidov and Rappoport, 2006" startWordPosition="1594" endWordPosition="1597">names.) the corpus (we used the 100 most frequent words Step 2: For each concept word, collect instances in the corpus). Of these, we keep all those patof contexts in which the word appears together with terns, which we call symmetric patterns, for which one other content word. Call this other word a tar- SjHSi is also found in the corpus. Repeat this proget word for that concept word. (For example, for cess to find symmetric patterns with any of the strucFrance we might find ‘Paris is the capital of France’. tures HSHS, SHSH or SHHS. It was shown in Paris would be a target word for France.) (Davidov and Rappoport, 2006) that pairs of words Step 3: For each concept word, group the contexts that often appear together in such symmetric patin which it appears according to the target word that terns tend to belong to the same class (that is, they appears in the context. (Thus ‘X is the capital of Y ’ share some notable aspect of their semantics). Other would likely be grouped with ‘Y ’s capital is X’.) words in the class can thus be generated by searchStep 4: Identify similar context groups that ap- ing a sub-corpus of documents including at least two pear across many different concept words. Merge concept words </context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Davidov, D., Rappoport, A., 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Methods for domain-independent information extraction from the web: an experimental comparison.</title>
<date>2004</date>
<journal>AAAI</journal>
<volume>04</volume>
<contexts>
<context position="6049" citStr="Etzioni et al, 2004" startWordPosition="982" endWordPosition="985">la- Several papers report on methods for using the tions R C X x Y , where Y is some concept and R web to discover instances of binary relations. Howis some binary relationship between elements of X ever, each of these assumes that the relations themand elements of Y . selves are known in advance (implicitly or explicWe can think of the relations we wish to gener- itly) so that the method can be provided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that related pairs can be generpossibly many different such relations. ated without pre-specifying the nature of the rel</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Etzioni, O., Cafarella, M., Downey, D., Popescu, A., Shaked, T., Soderland, S., Weld, D., Yates, A., 2004. Methods for domain-independent information extraction from the web: an experimental comparison. AAAI ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<journal>ACL</journal>
<volume>04</volume>
<contexts>
<context position="4581" citStr="Hasegawa et al, 2004" startWordPosition="731" endWordPosition="734">d to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004). a single concept, it is important to emphasize that Studying relationships between tagged named enthe method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006) nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that asof the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of and comparison with standard lists indicate that the pairs into several clusters, where each cluster corregenerated sets correspond to concepts with high pre- sponds to one of a known relationship type. These cision. studies, however, focused on the classification of We wish now to build on that result in the fol- pairs that were either given or extracted using some lo</context>
<context position="7409" citStr="Hasegawa et al, 2004" startWordPosition="1207" endWordPosition="1210">. However, differentiating among different relations, so that the since such sets can be generated in entirely unsuper- generated relations might conflate a number of disvised fashion, our challenge is effectively to gener- tinct ones. ate relations directly from a corpus given no addi- It should be noted that some of these papers utilize tional information of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two 233 pairs of words; however,</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Hasegawa, T., Sekine, S., Grishman, R., 2004. Discovering relations among named entities from large corpora. ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>A Hassan</author>
<author>O Emam</author>
</authors>
<title>unsupervised information extraction approach using graph mutual reinforcement.</title>
<date>2006</date>
<journal>EMNLP</journal>
<volume>06</volume>
<contexts>
<context position="4602" citStr="Hassan et al, 2006" startWordPosition="735" endWordPosition="738">ese basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004). a single concept, it is important to emphasize that Studying relationships between tagged named enthe method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006) nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that asof the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of and comparison with standard lists indicate that the pairs into several clusters, where each cluster corregenerated sets correspond to concepts with high pre- sponds to one of a known relationship type. These cision. studies, however, focused on the classification of We wish now to build on that result in the fol- pairs that were either given or extracted using some lowing way. Given a lar</context>
</contexts>
<marker>Hassan, Hassan, Emam, 2006</marker>
<rawString>Hassan, H., Hassan, A., Emam, O., 2006. unsupervised information extraction approach using graph mutual reinforcement. EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<journal>COLING</journal>
<volume>92</volume>
<contexts>
<context position="1297" citStr="Hearst, 1992" startWordPosition="193" endWordPosition="194">d on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision. 1 Introduction The huge amount of information available on the web has led to a flurry of research on methods for automatic creation of structured information from large unstructured text corpora. The challenge is to create as much information as possible while providing as little input as possible. A lot of this research is based on the initial insight (Hearst, 1992) that certain lexical patterns (‘X is a country’) can be exploited to automatically generate hyponyms of a specified word. Subsequent work (to be discussed in detail below) extended this initial idea along two dimensions. One objective was to require as small a userprovided initial seed as possible. Thus, it was observed that given one or more such lexical patterns, a corpus could be used to generate examples of hyponyms that could then, in turn, be exploited to generate more lexical patterns. The larger and more reliable sets of patterns thus generated resulted in larger and more precise sets</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M., 1992. Automatic acquisition of hyponyms from large text corpora. COLING ’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
<author>M Tatu</author>
<author>D Antohe</author>
<author>R Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>Workshop on Comput. Lexical Semantics at HLT-NAACL ’04.</booktitle>
<contexts>
<context position="4399" citStr="Moldovan et al, 2004" startWordPosition="703" endWordPosition="706">v and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004). a single concept, it is important to emphasize that Studying relationships between tagged named enthe method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006) nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that asof the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of and comparison with standard lists indicate that the pairs into several clusters, where each cluster corregenerated sets correspond to concepts with high pre- sponds to one of a known relat</context>
<context position="7565" citStr="Moldovan et al, 2004" startWordPosition="1234" endWordPosition="1237">number of disvised fashion, our challenge is effectively to gener- tinct ones. ate relations directly from a corpus given no addi- It should be noted that some of these papers utilize tional information of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two 233 pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method 5.1 Generalizing the seed We will use two concept words contained in a con- The </context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., Girju, R., 2004. Models for the semantic classification of noun phrases. Workshop on Comput. Lexical Semantics at HLT-NAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="3634" citStr="Pantel et al, 2004" startWordPosition="584" endWordPosition="587"> more precisely the problem we intend to solve. In section 3, we will consider related work. In section 4 we will provide an overview of our solution and in section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. 232 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Problem Definition 1992; Pantel et al, 2004), synonymy (Roark and In several studies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; DaviPantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>Pantel, P., Ravichandran, D., Hovy, E., 2004. Towards terascale knowledge acquisition. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: fact extraction in the fast lane.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="6251" citStr="Pasca et al, 2006" startWordPosition="1012" endWordPosition="1015">each of these assumes that the relations themand elements of Y . selves are known in advance (implicitly or explicWe can think of the relations we wish to gener- itly) so that the method can be provided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that related pairs can be generpossibly many different such relations. ated without pre-specifying the nature of the relaThe only input we require, aside from a corpus, tion sought. However, this work does not focus on is a small set of examples of some class. However, differentiating among different relations, so that t</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006. Names and similarities on the web: fact extraction in the fast lane. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<journal>ACL</journal>
<volume>98</volume>
<marker>Roark, Charniak, 1998</marker>
<rawString>Roark, B., Charniak, E., 1998. Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction. ACL ’98.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Rosenfeld</author>
<author>R URES Feldman</author>
</authors>
<title>an unsupervised web relation extraction system.</title>
<booktitle>Proceedings, ACL ’06</booktitle>
<location>Poster Sessions.</location>
<marker>Rosenfeld, Feldman, </marker>
<rawString>Rosenfeld B., Feldman, R.: URES : an unsupervised web relation extraction system. Proceedings, ACL ’06 Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="6134" citStr="Sekine, 2006" startWordPosition="996" endWordPosition="997"> and R web to discover instances of binary relations. Howis some binary relationship between elements of X ever, each of these assumes that the relations themand elements of Y . selves are known in advance (implicitly or explicWe can think of the relations we wish to gener- itly) so that the method can be provided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that related pairs can be generpossibly many different such relations. ated without pre-specifying the nature of the relaThe only input we require, aside from a corpus, tion sought. However, this work does</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Sekine, S., 2006 On-demand information extraction. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S Ponzetto</author>
</authors>
<title>WikiRelate! computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<journal>AAAI</journal>
<volume>06</volume>
<contexts>
<context position="6456" citStr="Strube and Ponzetto, 2006" startWordPosition="1045" endWordPosition="1048">ovided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that related pairs can be generpossibly many different such relations. ated without pre-specifying the nature of the relaThe only input we require, aside from a corpus, tion sought. However, this work does not focus on is a small set of examples of some class. However, differentiating among different relations, so that the since such sets can be generated in entirely unsuper- generated relations might conflate a number of disvised fashion, our challenge is effectively to gener- tinct ones. ate relations directly from a co</context>
<context position="7682" citStr="Strube and Ponzetto, 2006" startWordPosition="1251" endWordPosition="1254">rpus given no addi- It should be noted that some of these papers utilize tional information of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two 233 pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method 5.1 Generalizing the seed We will use two concept words contained in a con- The first step is to take the seed, which might concept class C to generate a collection of distinct re- sist of as few a</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Strube, M., Ponzetto, S., 2006. WikiRelate! computing semantic relatedness using Wikipedia. AAAI ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Ifrim</author>
<author>G Weikum</author>
</authors>
<title>LEILA: learning to extract information by linguistic analysis.</title>
<date>2006</date>
<booktitle>Workshop on Ontology Learning and Population at COLING-ACL ’06.</booktitle>
<contexts>
<context position="7325" citStr="Suchanek et al, 2006" startWordPosition="1192" endWordPosition="1195">ought. However, this work does not focus on is a small set of examples of some class. However, differentiating among different relations, so that the since such sets can be generated in entirely unsuper- generated relations might conflate a number of disvised fashion, our challenge is effectively to gener- tinct ones. ate relations directly from a corpus given no addi- It should be noted that some of these papers utilize tional information of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (H</context>
</contexts>
<marker>Suchanek, Ifrim, Weikum, 2006</marker>
<rawString>Suchanek F. M., G. Ifrim, G. Weikum. 2006. LEILA: learning to extract information by linguistic analysis. Workshop on Ontology Learning and Population at COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="7759" citStr="Turney, 2006" startWordPosition="1264" endWordPosition="1265">n of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two 233 pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method 5.1 Generalizing the seed We will use two concept words contained in a con- The first step is to take the seed, which might concept class C to generate a collection of distinct re- sist of as few as two concept words, and generate lations in which C participates. In this se</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Turney, P., 2006. Expressing implicit semantic relations without supervision. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A graph model for unsupervised Lexical acquisition.</title>
<date>2002</date>
<journal>COLING</journal>
<volume>02</volume>
<contexts>
<context position="3705" citStr="Widdows and Dorow, 2002" startWordPosition="595" endWordPosition="598">ll consider related work. In section 4 we will provide an overview of our solution and in section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. 232 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Problem Definition 1992; Pantel et al, 2004), synonymy (Roark and In several studies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; DaviPantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nou</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Widdows, D., Dorow, B., 2002. A graph model for unsupervised Lexical acquisition. COLING ’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>