<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.994552">
Unsupervised Semantic Role Induction via Split-Merge Clustering
</title>
<author confidence="0.998017">
Joel Lang and Mirella Lapata
</author>
<affiliation confidence="0.9996545">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.985655">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.998739">
J.Lang-3@sms.ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998668" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996100055555556">
In this paper we describe an unsupervised
method for semantic role induction which
holds promise for relieving the data acqui-
sition bottleneck associated with supervised
role labelers. We present an algorithm that it-
eratively splits and merges clusters represent-
ing semantic roles, thereby leading from an
initial clustering to a final clustering of bet-
ter quality. The method is simple, surpris-
ingly effective, and allows to integrate lin-
guistic knowledge transparently. By com-
bining role induction with a rule-based com-
ponent for argument identification we obtain
an unsupervised end-to-end semantic role la-
beling system. Evaluation on the CoNLL
2008 benchmark dataset demonstrates that
our method outperforms competitive unsuper-
vised approaches by a wide margin.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97492875">
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the re-
lations that hold between a predicate and its argu-
ments, abstracting over surface syntactic configura-
tions. In the example sentences below. window oc-
cupies different syntactic positions — it is the object
of broke in sentences (1a,b), and the subject in (1c)
— while bearing the same semantic role, i.e., the
physical object affected by the breaking event. Anal-
ogously, rock is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
</bodyText>
<listItem confidence="0.641755">
(1) a. [Joe]A0 broke the [window]A1 with a
</listItem>
<bodyText confidence="0.972812481481482">
[rock]A2.
b. The [rock]A2 broke the [window]A1.
c. The [window]A1 broke.
The semantic roles in the examples are labeled
in the style of PropBank (Palmer et al., 2005), a
broad-coverage human-annotated corpus of seman-
tic roles and their syntactic realizations. Under the
PropBank annotation framework (which we will as-
sume throughout this paper) each predicate is as-
sociated with a set of core roles (named A0, A1,
A2, and so on) whose interpretations are specific to
that predicate1 and a set of adjunct roles (e.g., loca-
tion or time) whose interpretation is common across
predicates. This type of semantic analysis is admit-
tedly shallow but relatively straightforward to auto-
mate and useful for the development of broad cov-
erage, domain-independent language understanding
systems. Indeed, the analysis produced by existing
semantic role labelers has been shown to benefit a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al., 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al., 2005).
Since both argument identification and labeling
can be readily modeled as classification tasks, most
state-of-the-art systems to date conceptualize se-
</bodyText>
<footnote confidence="0.936799333333333">
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
</footnote>
<page confidence="0.905433">
1117
</page>
<note confidence="0.9802145">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995248">
mantic role labeling as a supervised learning prob-
lem. Current approaches have high performance —
a system will recall around 81% of the arguments
correctly and 95% of those will be assigned a cor-
rect semantic role (see M`arquez et al. (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al., 2008).
Unfortunately, the reliance on role-annotated data
which is expensive and time-consuming to produce
for every language and domain, presents a major
bottleneck to the widespread application of semantic
role labeling. Given the data requirements for super-
vised systems and the current paucity of such data,
unsupervised methods offer a promising alternative.
They require no human effort for training thus lead-
ing to significant savings in time and resources re-
quired for annotating text. And their output can be
used in different ways, e.g., as a semantic prepro-
cessing step for applications that require broad cov-
erage understanding or as training material for su-
pervised algorithms.
In this paper we present a simple approach to un-
supervised semantic role labeling. Following com-
mon practice, our system proceeds in two stages.
It first identifies the semantic arguments of a pred-
icate and then assigns semantic roles to them. Both
stages operate over syntactically analyzed sentences
without access to any data annotated with semantic
roles. Argument identification is carried out through
a small set of linguistically-motivated rules, whereas
role induction is treated as a clustering problem. In
this setting, the goal is to assign argument instances
to clusters such that each cluster contains arguments
corresponding to a specific semantic role and each
role corresponds to exactly one cluster. We formu-
late a clustering algorithm that executes a series of
split and merge operations in order to transduce an
initial clustering into a final clustering of better qual-
ity. Split operations leverage syntactic cues so as to
create “pure” clusters that contain arguments of the
same role whereas merge operations bring together
argument instances of a particular role located in
different clusters. We test the effectiveness of our
induction method on the CoNLL 2008 benchmark
dataset and demonstrate improvements over compet-
itive unsupervised methods by a wide margin.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986209302326">
As mentioned earlier, much previous work has
focused on building supervised SRL systems
(M`arquez et al., 2008). A few semi-supervised ap-
proaches have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(F¨urstenau and Lapata, 2009) or across different lan-
guages (Pad´o and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) attempt
to increase the coverage of PropBank by leveraging
existing labeled data. Rather than annotating new
sentences that contain previously unseen verbs, they
find syntactically similar verbs and use their annota-
tions as surrogate training data.
Swier and Stevenson (2004) induce role labels
with a bootstrapping scheme where the set of la-
beled instances is iteratively expanded using a clas-
sifier trained on previously labeled instances. Their
method is unsupervised in that it starts with a dataset
containing no role annotations at all. However, it re-
quires significant human effort as it makes use of
VerbNet (Kipper et al., 2000) in order to identify the
arguments of predicates and make initial role assign-
ments. VerbNet is a broad coverage lexicon orga-
nized into verb classes each of which is explicitly
associated with argument realization and semantic
role specifications.
Abend et al. (2009) propose an algorithm that
identifies the arguments of predicates by relying
only on part of speech annotations, without, how-
ever, assigning semantic roles. In contrast, Lang
and Lapata (2010) focus solely on the role induction
problem which they formulate as the process of de-
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that makes
use of the close relationship between syntactic func-
tions and semantic roles. Grenager and Manning
</bodyText>
<page confidence="0.905032">
1118
</page>
<bodyText confidence="0.998786659574468">
(2006) propose a directed graphical model which re- with the CoNLL 2008 benchmark dataset used for
lates a verb, its semantic roles, and their possible evaluation in our experiments.
syntactic realizations. Latent variables represent the Given a dependency parse of a sentence, our sys-
semantic roles of arguments and role induction cor- tem identifies argument instances and assigns them
responds to inferring the state of these latent vari- to clusters. Thereafter, argument instances can be
ables. labeled with an identifier corresponding to the clus-
Our own work also follows the unsupervised ter they have been assigned to, similar to PropBank
learning paradigm. We formulate the induction of core labels (e.g., A0, A1).
semantic roles as a clustering problem and propose a 4 Argument Identification
split-merge algorithm which iteratively manipulates In the supervised setting, a classifier is employed
clusters representing semantic roles. The motiva- in order to decide for each node in the parse tree
tion behind our approach was to design a concep- whether it represents a semantic argument or not.
tually simple system, that allows for the incorpo- Nodes classified as arguments are then assigned a se-
ration of linguistic knowledge in a straightforward mantic role. In the unsupervised setting, we slightly
and transparent manner. For example, arguments reformulate argument identification as the task of
occurring in similar syntactic positions are likely to discarding as many non-semantic arguments as pos-
bear the same semantic role and should therefore sible. This means that the argument identification
be grouped together. Analogously, arguments that component does not make a final positive decision
are lexically similar are likely to represent the same for any of the argument candidates; instead, this de-
semantic role. We operationalize these notions us- cision is deferred to role induction. The rules given
ing a scoring function that quantifies the compatibil- in Table 1 are used to discard or select argument can-
ity between arbitrary cluster pairs. Like Lang and didates. They primarily take into account the parts of
Lapata (2010) and Grenager and Manning (2006) speech and the syntactic relations encountered when
our method operates over syntactically parsed sen- traversing the dependency tree from predicate to ar-
tences, without, however, making use of any infor- gument. For each candidate, the first matching rule
mation pertaining to semantic roles (e.g., in form of is applied.
a lexical resource or manually annotated data). Per- We will exemplify how the argument identifica-
forming role-semantic analysis without a treebank- tion component works for the predicate expect in the
trained parser is an interesting research direction, sentence “The company said it expects its sales to
however, we leave this to future work. remain steady” whose parse tree is shown in Fig-
3 Learning Setting ure 1. Initially, all words save the predicate itself
We follow the general architecture of supervised se- are treated as argument candidates. Then, the rules
mantic role labeling systems. Given a sentence and from Table 1 are applied as follows. Firstly, words
a designated verb, the SRL task consists of identify- the and to are discarded based on their part of speech
ing the arguments of the verbal predicate (argument (rule (1)); then, remain is discarded because the path
identification) and labeling them with semantic roles ends with the relation IM and said is discarded as
(role induction). the path ends with an upward-leading OBJ relation
In our case neither argument identification nor (rule (2)). Rule (3) does not match and is therefore
role induction relies on role-annotated data or other not applied. Next, steady is discarded because there
semantic resources although we assume that the in- is a downward-leading OPRD relation along the path
put sentences are syntactically analyzed. Our ap- and the words company and its are discarded be-
proach is not tied to a specific syntactic representa- cause of the OBJ relations along the path (rule (4)).
tion — both constituent- and dependency-based rep- Rule (5) does not apply but words it and sales are
resentations could be used. However, we opted for a kept as likely arguments (rule (6)). Finally, rule (7)
dependency-based representation, as it simplifies ar- does not apply, because there are no candidates left.
gument identification considerably and is consistent
1119
</bodyText>
<listItem confidence="0.9604342">
1. Discard a candidate if it is a determiner, in-
finitival marker, coordinating conjunction, or
punctuation.
2. Discard a candidate if the path of relations
from predicate to candidate ends with coordi-
nation, subordination, etc. (see the Appendix
for the full list of relations).
3. Keep a candidate if it is the closest subject
(governed by the subject-relation) to the left
of a predicate and the relations from predi-
cate p to the governor g of the candidate are
all upward-leading (directed as g —* p).
4. Discard a candidate if the path between the
predicate and the candidate, excluding the last
relation, contains a subject relation, adjectival
modifier relation, etc. (see the Appendix for
the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if the predicate is its parent.
7. Keep a candidate if the path from predicate
</listItem>
<figureCaption confidence="0.790574647058824">
to candidate leads along several verbal nodes
(verb chain) and ends with arbitrary relation.
8. Discard all remaining candidates.
Table 1: Argument identification rules.
5 Split-Merge Role Induction
Figure 1: A sample dependency parse with depen-
dency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative com-
plement), PRD (predicative complement), and IM
(infinitive marker). See Surdeanu et al. (2008) for
more details on this variant of dependency syntax.
located in many clusters. The degree of dislocation
is reduced in the consecutive merge phase, in which
clusters that are likely to represent the same role are
merged.
5.1 Split Phase
Initially, all arguments of a particular verb are placed
</figureCaption>
<bodyText confidence="0.907742">
in a single cluster. The goal then is to partition this
cluster in such a way that the split-off clusters have
high purity, i.e., contain argument instances of the
same role. Towards this end, we characterize each
argument instance by a key, formed by concatenat-
ing the following syntactic cues:
</bodyText>
<listItem confidence="0.9943032">
• verb voice (active/passive);
• argument linear position relative to predicate
(left/right);
• syntactic relation of argument to its governor;
• preposition used for argument realization.
</listItem>
<bodyText confidence="0.999351342857143">
A cluster is allocated for each key and all argument
instances with a matching key are assigned to that
cluster. Since each cluster encodes fine-grained syn-
tactic distinctions, we assume that arguments occur-
ring in the same position are likely to bear the same
semantic role. The assumption is largely supported
by our empirical results (see Section 7); the clusters
emerging from the initial split phase have a purity
of approximately 90%. While the incorporation of
additional cues (e.g., indicating the part of speech
of the subject or transitivity) would result in even
greater purity, it would also create problematically
small clusters, thereby negatively affecting the suc-
cessive merge phase.
We treat role induction as a clustering problem with
the goal of assigning argument instances (i.e., spe-
cific arguments occurring in an input sentence) to
clusters such that these represent semantic roles. In
accordance with PropBank, we induce a separate set
of clusters for each verb and each cluster thus repre-
sents a verb-specific role.
Our algorithm works by iteratively splitting and
merging clusters of argument instances in order to
arrive at increasingly accurate representations of se-
mantic roles. Although splits and merges could be
arbitrarily interleaved, our algorithm executes a sin-
gle split operation (split phase), followed by a se-
ries of merges (merge phase). The split phase par-
titions the seed cluster containing all argument in-
stances of a particular verb into more fine-grained
(sub-)clusters. This initial split results in a clustering
with high purity but low collocation, i.e., argument
instances in each cluster tend to belong to the same
role but argument instances of a particular role are
1120
</bodyText>
<subsectionHeader confidence="0.998243">
5.2 Merge Phase
</subsectionHeader>
<bodyText confidence="0.999945076923077">
The split phase creates clusters with high purity,
however, argument instances of a particular role are
often scattered amongst many clusters resulting in a
cluster assignment with low collocation. The goal
of the merge phase is to improve collocation by ex-
ecuting a series of merge steps. At each step, pairs
of clusters are considered for merging. Each pair is
scored by a function that reflects how likely the two
clusters are to contain arguments of the same role
and the best scoring pair is chosen for merging. In
the following, we will specify which pairs of clus-
ters are considered (candidate search), how they are
scored, and when the merge phase terminates.
</bodyText>
<subsubsectionHeader confidence="0.676692">
5.2.1 Candidate Search
</subsubsectionHeader>
<bodyText confidence="0.998432323529412">
In principle, we could simply enumerate and score
all possible cluster pairs at each iteration. In practice
however, such a procedure has a number of draw-
backs. Besides being inefficient, it requires a scoring
function with comparable scores for arbitrary pairs
of clusters. For example, let a, b, c, and d denote
clusters. Then, score(a,b) and score(c,d) must be
comparable. This is a stronger requirement than de-
manding that only scores involving some common
cluster (e.g., score(a,b) and score(a,c)) be com-
parable. Moreover, it would be desirable to ex-
clude pairings involving small clusters (i.e., with
few instances) as scores for these tend to be unre-
liable. Rather than considering all cluster pairings,
we therefore select a specific cluster at each step and
score merges between this cluster and certain other
clusters. If a sufficiently good merge is found, it is
executed, otherwise the clustering does not change.
In addition, we prioritize merges between large clus-
ters and avoid merges between small clusters.
Algorithm 1 implements our merging procedure.
Each pass through the inner loop (lines 4–12) selects
a different cluster to consider at that step. Then,
merges between the selected cluster and all larger
clusters are considered. The highest-scoring merge
is executed, unless all merges are ruled out, i.e., have
a score below the threshold a. After each comple-
tion of the inner loop, the thresholds contained in
the scoring function (discussed below) are adjusted
and this is repeated until some termination criterion
is met (discussed in Section 5.2.3).
Algorithm 1: Cluster merging procedure. Oper-
ation merge(Li,Lj) merges cluster Li into cluster
Lj and removes Li from the list L.
</bodyText>
<equation confidence="0.8696364">
1 while not done do
L +— a list of all clusters sorted by number
of instances in descending order
i +— 1
while i &lt; length(L) do
j +— arg max
0&lt;j�&lt;i
if score(Li,Lj) &gt; a then
merge(Li,Lj)
end
else
i +— i+1
end
end
adjust thresholds
</equation>
<sectionHeader confidence="0.485816" genericHeader="method">
14 end
</sectionHeader>
<subsectionHeader confidence="0.630667">
5.2.2 Scoring Function
</subsectionHeader>
<bodyText confidence="0.998625">
Our scoring function quantifies whether two clusters
are likely to contain arguments of the same role and
was designed to reflect the following criteria:
</bodyText>
<listItem confidence="0.98849325">
1. whether the arguments found in the two clus-
ters are lexically similar;
2. whether clause-level constraints are satisfied,
specifically the constraint that all arguments
of a particular clause have different semantic
roles, i.e., are assigned to different clusters;
3. whether the arguments present in the two clus-
ters have similar parts of speech.
</listItem>
<bodyText confidence="0.9951545">
Qualitatively speaking, criteria (2) and (3) provide
negative evidence in the sense that they can be used
to rule out incorrect merges but not to identify cor-
rect ones. For example, two clusters with drastically
different parts of speech are unlikely to represent
the same role. However, the converse is not neces-
sarily true as part of speech similarity does not im-
ply role-semantic similarity. Analogously, the fact
that clause-level constraints are not met provides ev-
idence against a merge, but the fact that these are
satisfied is not reliable evidence in favor of a merge.
In contrast, lexical similarity implies that the clus-
</bodyText>
<figure confidence="0.969028076923077">
2
3
4
5
6
7
8
9
10
11
12
13
score(Li,Lj�)
</figure>
<page confidence="0.991207">
1121
</page>
<bodyText confidence="0.999950133333333">
ters are likely to represent the same semantic role.
It is reasonable to assume that due to selectional re-
strictions, verbs will be associated with lexical units
that are semantically related and assume similar syn-
tactic positions (e.g., eat prefers as an object edible
things such as apple, biscuit, meat), thus bearing the
same semantic role. Unavoidably, lexical similarity
will be more reliable for arguments with overt lex-
ical content as opposed to pronouns, however this
should not impact the scoring of sufficiently large
clusters.
Each of the criteria mentioned above is quantified
through a separate score and combined into an over-
all similarity function, which scores two clusters c
and c&apos; as follows:
</bodyText>
<equation confidence="0.998448666666667">
score(c,c&apos;) = { 0 if pos(c,c&apos;) &lt; R, (2)
0 if cons(c,c&apos;) &lt; y,
lex(c,c&apos;) otherwise.
</equation>
<bodyText confidence="0.999793210526316">
The particular form of this function is motivated by
the distinction between positive and negative evi-
dence. When the part-of-speech similarity (pos) is
below a certain threshold R or when clause-level
constraints (cons) are satisfied to a lesser extent than
threshold y, the score takes value zero and the merge
is ruled out. If this is not the case, the lexical similar-
ity score (lex) determines the magnitude of the over-
all score. In the remainder of this section we will
explain how the individual scores (pos, cons, and
lex) are defined and then move on to discuss how
the thresholds R and y are adjusted.
Lexical Similarity We measure lexical similar-
ity between two clusters through cosine similarity.
Specifically, each cluster is represented as a vec-
tor whose components correspond to the occurrence
frequencies of the argument head words in the clus-
ter. The similarity on such vectors x and y is then
quantified as:
</bodyText>
<equation confidence="0.8984455">
x�y
lex(x,y) = cossim(x,y) = IIxII IIyII (3)
</equation>
<bodyText confidence="0.997332631578947">
Clause-Level Constraints Arguments occurring
in the same clause cannot bear the same role. There-
fore, clusters should not merge if the resulting clus-
ter contains (many) arguments of the same clause.
For two clusters c and c&apos; we assess how well they
satisfy this clause-level constraint by computing:
cons (c, c&apos; = 1 _ 2 * viol (c, c&apos;) 4
(&apos; ) NC+NC&apos; ( )
where viol(c,c&apos;) refers to the number of pairs of in-
stances (d,d&apos;) E c x c&apos; for which d and d&apos; occur in
the same clause (each instance can participate in at
most one pair) and NC and NC&apos; are the number of
instances in clusters c and c&apos;, respectively.
Part-of-speech Similarity Part-of-speech similar-
ity is also measured through cosine-similarity (equa-
tion (3)). Clusters are again represented as vectors x
and y whose components correspond to argument
part-of-speech tags and values to their occurrence
frequency.
</bodyText>
<subsubsectionHeader confidence="0.690754">
5.2.3 Threshold Adaptation and Termination
</subsubsectionHeader>
<bodyText confidence="0.999986157894737">
As mentioned earlier the thresholds R and y which
parametrize the scoring function are adjusted at each
iteration. The idea is to start with a very restrictive
setting (high values) in which the negative evidence
rules out merges more strictly, and then to gradually
relax the requirement for a merge by lowering the
threshold values. This procedure prioritizes reliable
merges over less reliable ones.
More concretely, our threshold adaptation pro-
cedure starts with R and y both set to value 0.95.
Then R is lowered by 0.05 at each step, leaving y
unchanged. When R becomes zero, y is lowered
by 0.05 and R is reset to 0.95. Then R is iteratively
decreased again until it becomes zero, after which y
is decreased by another 0.05. This is repeated until y
becomes zero, at which point the algorithm termi-
nates. Note that the termination criterion is not tied
explicitly to the number of clusters, which is there-
fore determined automatically.
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999673125">
In this section we describe how we assessed the per-
formance of our system. We discuss the dataset
on which our experiments were carried out, explain
how our system’s output was evaluated and present
the methods used for comparison with our approach.
Data For evaluation purposes, the system’s out-
put was compared against the CoNLL 2008 shared
task dataset (Surdeanu et al., 2008) which provides
</bodyText>
<page confidence="0.983344">
1122
</page>
<table confidence="0.975913666666667">
Syntactic CO Function Lang and Lapata F1 Split -Merge F1
PU F1 PU CO PU CO
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1
</table>
<tableCaption confidence="0.792012">
Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang and
Lapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function.
</tableCaption>
<bodyText confidence="0.995308916666667">
PropBank-style gold standard annotations. The
dataset was taken from the Wall Street Journal por-
tion of the Penn Treebank corpus and converted into
a dependency format (Surdeanu et al., 2008). In
addition to gold standard dependency parses, the
dataset also contains automatic parses obtained from
the MaltParser (Nivre et al., 2007). Although the
dataset provides annotations for verbal and nominal
predicate-argument constructions, we only consid-
ered the former, following previous work on seman-
tic role labeling (M`arquez et al., 2008).
Evaluation Metrics For each verb, we determine
the extent to which argument instances in a cluster
share the same gold standard role (purity) and the
extent to which a particular gold standard role is as-
signed to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, Gj the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can then
be written as:
</bodyText>
<equation confidence="0.997922">
1
PU = åmax|Gj nCi |(5)
N i j
</equation>
<bodyText confidence="0.9992506">
Collocation is defined as follows. For each gold role,
we determine the cluster with the largest number of
instances for that role (the role’s primary cluster)
and then compute the percentage of instances that
belong to the primary cluster for each gold role as:
</bodyText>
<equation confidence="0.958943">
1
CO = åmax|Gj nCi|
N(6)
i
j
</equation>
<bodyText confidence="0.998555">
The per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the micro-
average obtained by weighting the scores for indi-
vidual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
</bodyText>
<equation confidence="0.998016666666667">
F1 = 2xCOxPU
(7)
CO + PU
</equation>
<bodyText confidence="0.999779473684211">
Comparison Models We compared our split-
merge algorithm against two competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as point of com-
parison by other unsupervised semantic role label-
ing systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
Our implementation allocates up to N = 21 clus-
ters2 for each verb, one for each of the 20 most fre-
quent functions in the CoNLL dataset and a default
cluster for all other functions. The second compar-
ison model is the one proposed in Lang and Lapata
(2010) (see Section 2). We used the same model set-
tings (with 10 latent variables) and feature set pro-
posed in that paper. Our method’s only parameter is
the threshold a which we heuristically set to 0.1. On
average our method induces 10 clusters per verb.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.997662">
Our results are summarized in Table 2. We re-
port cluster purity (PU), collocation (CO) and their
harmonic mean (F1) for the baseline (Syntactic
Function), Lang and Lapata’s (2010) model and
our split-merge algorithm (Split-Merge) on four
</bodyText>
<footnote confidence="0.997362">
2This is the number of gold standard roles.
</footnote>
<page confidence="0.807868">
1123
</page>
<table confidence="0.998854928571429">
Verb Freq Syntactic CO Function Split -Merge F1
PU F1 PU CO
say 15238 91.4 91.3 91.4 93.6 81.7 87.2
make 4250 68.6 71.9 70.2 73.3 72.9 73.1
go 2109 45.1 56.0 49.9 52.7 51.9 52.3
increase 1392 59.7 68.4 63.7 68.8 71.4 70.1
know 983 62.4 72.7 67.1 63.7 65.9 64.8
tell 911 61.9 76.8 68.6 77.5 70.8 74.0
consider 753 63.5 65.6 64.5 79.2 61.6 69.3
acquire 704 75.9 79.7 77.7 80.1 76.6 78.3
meet 574 76.7 76.0 76.3 88.0 69.7 77.8
send 506 69.6 63.8 66.6 83.6 65.8 73.6
open 482 63.1 73.4 67.9 77.6 62.2 69.1
break 246 53.7 58.9 56.2 68.7 53.3 60.0
</table>
<tableCaption confidence="0.996997">
Table 3: Clustering results for individual verbs with
</tableCaption>
<bodyText confidence="0.988775129032258">
our split-merge algorithm and the syntactic function
baseline.
datasets. These result from the combination of au-
tomatic parses with automatically identified argu-
ments (auto/auto), gold parses with automatic argu-
ments (gold/auto), automatic parses with gold argu-
ments (auto/gold) and gold parses with gold argu-
ments (gold/gold). Bold-face is used to highlight the
best performing system under each measure on each
dataset (e.g., auto/auto, gold/auto and so on).
On all datasets, our method achieves the highest
purity and outperforms both comparison models by
a wide margin which in turn leads to a considerable
increase in F1. On the auto/auto dataset the split-
merge algorithm results in 9% higher purity than the
baseline and increases F1 by 2.8%. Lang and Lap-
ata’s (2010) logistic classifier achieves higher collo-
cation but lags behind our method on the other two
measures.
Not unexpectedly, we observe an increase in per-
formance for all models when using gold standard
parses. On the gold/auto dataset, F1 increases
by 2.7% for the split-merge algorithm, 2.7% for the
logistic classifier, and 5.5% for the syntactic func-
tion baseline. Split-Merge maintains the highest pu-
rity and levels the baseline in terms of F1. Perfor-
mance also increases if gold standard arguments are
used instead of automatically identified arguments.
Consequently, each model attains its best scores on
the gold/gold dataset.
We also assessed the argument identification com-
</bodyText>
<table confidence="0.999910823529412">
Role Syntactic CO Function PU Split F1
PU F1 -Merge
CO
A0 74.5 87.0 80.3 79.0 88.7 83.6
A1 82.3 72.0 76.8 87.1 73.0 79.4
A2 65.0 67.3 66.1 82.8 66.2 73.6
A3 48.7 76.7 59.6 79.6 76.3 77.9
ADV 37.2 77.3 50.2 78.8 37.3 50.6
CAU 81.8 74.4 77.9 84.8 67.2 75.0
DIR 62.7 67.9 65.2 71.0 50.7 59.1
EXT 51.4 87.4 64.7 90.4 87.2 88.8
LOC 71.5 74.6 73.0 82.6 56.7 67.3
MNR 62.6 58.8 60.6 81.5 44.1 57.2
TMP 80.5 74.0 77.1 80.1 38.7 52.2
MOD 68.2 44.4 53.8 90.4 89.6 90.0
NEG 38.2 98.5 55.0 49.6 98.8 66.1
DIS 42.5 87.5 57.2 62.2 75.4 68.2
</table>
<tableCaption confidence="0.985104">
Table 4: Clustering results for individual semantic
</tableCaption>
<bodyText confidence="0.98334532">
roles with our split-merge algorithm and the syntac-
tic function baseline.
ponent on its own (settings auto/auto and gold/auto).
It obtained a precision of 88.1% (percentage of se-
mantic arguments out of those identified) and recall
of 87.9% (percentage of identified arguments out of
all gold arguments). However, note that these fig-
ures are not strictly comparable to those reported
for supervised systems, due to the fact that our ar-
gument identification component only discards non-
argument candidates.
Tables 3 and 4 shows how performance varies
across verbs and roles, respectively. We compare the
syntactic function baseline and the split-merge sys-
tem on the auto/auto dataset. Table 3 presents results
for 12 verbs which we selected so as to exhibit var-
ied occurrence frequencies and alternation patterns.
As can be seen, the macroscopic result — increase
in F1 (shown in bold face) and purity — also holds
across verbs. Some caution is needed in interpret-
ing the results in Table 43 since core roles A0–A3
are defined on a per-verb basis and do not necessar-
ily have a uniform corpus-wide interpretation. Thus,
conflating scores across verbs is only meaningful to
the extent that these labels actually signify the same
</bodyText>
<footnote confidence="0.5827444">
3Results are shown for four core roles (A0–A3) and all sub-
types of the ArgM role, i.e., adjuncts denoting general purpose
(ADV), cause (CAU), direction (DIR), extent (EXT), location
(LOC), manner (MNR), and time (TMP), modal verbs (MOD),
negative markers (NEG), and discourse connectives (DIS).
</footnote>
<page confidence="0.935697">
1124
</page>
<bodyText confidence="0.997663529411765">
role (which is mostly true for A0 and A1). Further- itive and requires no manual effort for training. Cou-
more, the purity scores given here represent the av- pled with a rule-based component for automatically
erage purity of those clusters for which the specified identifying argument candidates our split-merge al-
role is the majority role. We observe that for most gorithm forms an end-to-end system that is capable
roles shown in Table 4 the split-merge algorithm im- of inducing role labels without any supervision.
proves upon the baseline with regard to F1, whereas Our approach holds promise for reducing the data
this is uniformly the case for purity. acquisition bottleneck for supervised systems. It
What are the practical implications of these re- could be usefully employed in two ways: (a) to cre-
sults, especially when considering the collocation- ate preliminary annotations, thus supporting the “an-
purity tradeoff? If we were to annotate the clus- notate automatically, correct manually” methodol-
ters induced by our system, low collocation would ogy used for example to provide high volume anno-
result in higher annotation effort while low purity tation in the Penn Treebank project; and (b) in com-
would result in poorer data quality. Our system im- bination with supervised methods, e.g., by providing
proves purity substantially over the baselines, with- useful out-of-domain data for training. An important
out affecting collocation in a way that would mas- direction for future work lies in investigating how
sively increase the annotation effort. As an exam- the approach generalizes across languages as well as
ple, consider how our system could support humans reducing our system’s reliance on a treebank-trained
in labeling an unannotated corpus. (The following parser.
numbers are derived from the CoNLL dataset4 in the Acknowledgments We are grateful to Charles
auto/auto setting.) We might decide to annotate all Sutton for his valuable feedback on this work. The
induced clusters with more than 10 instances. This authors acknowledge the support of EPSRC (grant
means we would assign labels to 74% of instances in GR/T04540/01).
the dataset (excluding those discarded during argu- Appendix
ment identification) and attain a role classification The relations in Rule (2) from Table 1 are IM↑↓,
with 79.4% precision (purity).5 However, instead PRT↓, COORD↑↓, P↑↓, OBJ↑, PMOD↑, ADV↑,
of labeling all 165,662 instances contained in these SUB↑↓, ROOT↑, TMP↑, SBJ↑, OPRD↑. The sym-
clusters individually we would only have to assign bols ↑ and ↓ denote the direction of the dependency
labels to 2,869 clusters. Since annotating a cluster arc (upward and downward, respectively).
takes roughly the same time as annotating a single The relations in Rule (3) are ADV↑↓, AMOD↑↓,
instance, the annotation effort is reduced by a factor APPO↑↓, BNF↑↓-, CONJ↑↓, COORD↑↓, DIR↑↓,
of about 50. DTV↑↓-, EXT↑↓, EXTR↑↓, HMOD↑↓, IOBJ↑↓,
8 Conclusions LGS↑↓, LOC↑↓, MNR↑↓, NMOD↑↓, OBJ↑↓,
In this paper we presented a novel approach to un- OPRD↑↓, POSTHON↑↓, PRD↑↓, PRN↑↓, PRP↑↓,
supervised role induction which we formulated as a PRT↑↓, PUT↑↓, SBJ↑↓, SUB↑↓, SUFFIX↑↓. De-
clustering problem. We proposed a split-merge al- pendency labels are abbreviated here. A detailed
gorithm that iteratively manipulates clusters repre- description is given in Surdeanu et al. (2008), in
senting semantic roles whilst trading off cluster pu- their Table 4.
rity with collocation. The split phase creates “pure” References
clusters that contain arguments of the same role O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
whereas the merge phase attempts to increase col- supervised Argument Identification for Semantic Role
location by merging clusters which are likely to rep- Labeling. In Proceedings of the 47th Annual Meet-
resent the same role. The approach is simple, intu- ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28–36, Singapore.
4Of course, it makes no sense to label this dataset as it is
already labeled.
5Purity here is slightly lower than the score reported in Ta-
ble 2 (auto/auto setting), because it is computed over a different
number of clusters (only those with at least 10 instances).
1125
</bodyText>
<reference confidence="0.999918556962025">
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547–619.
H. F¨urstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11–20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245–288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192–199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1–8, Sydney, Australia.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691–696. AAAI Press / The MIT Press.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939–
947, Los Angeles, California.
L. M`arquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145–
159, June.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95–135.
S. Pad´o and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307–340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289–310.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12–21,
Prague, Czech Republic.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8–15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. M`arquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159–177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95–102, Barcelona, Spain.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13–16, Boulder, Colorado.
</reference>
<page confidence="0.994507">
1126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.566998">
<title confidence="0.977605">Unsupervised Semantic Role Induction via Split-Merge Clustering</title>
<author confidence="0.845877">Lang</author>
<affiliation confidence="0.98037">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.737011">10 Crichton Street, Edinburgh EH8 9AB,</address>
<abstract confidence="0.994977473684211">In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Dowty</author>
</authors>
<title>Thematic Proto Roles and Argument Selection.</title>
<date>1991</date>
<journal>Language,</journal>
<volume>67</volume>
<issue>3</issue>
<contexts>
<context position="3270" citStr="Dowty (1991)" startWordPosition="497" endWordPosition="498">ing systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1117 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mantic role labeling as a supervised learning problem. Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on Pro</context>
</contexts>
<marker>Dowty, 1991</marker>
<rawString>D. Dowty. 1991. Thematic Proto Roles and Argument Selection. Language, 67(3):547–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H F¨urstenau</author>
<author>M Lapata</author>
</authors>
<title>Graph Aligment for Semi-Supervised Semantic Role Labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11--20</pages>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>H. F¨urstenau and M. Lapata. 2009. Graph Aligment for Semi-Supervised Semantic Role Labeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 11–20, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1333" citStr="Gildea and Jurafsky, 2002" startWordPosition="188" endWordPosition="191">integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin. 1 Introduction Recent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic configurations. In the example sentences below. window occupies different syntactic positions — it is the object of broke in sentences (1a,b), and the subject in (1c) — while bearing the same semantic role, i.e., the physical object affected by the breaking event. Analogously, rock is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Joe]A0 broke the [window]A1 with a [rock]A2. b. The [rock]A2 broke the [window]</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>R Swanson</author>
</authors>
<title>Generalizing Semantic Role Annotations Across Syntactically Similar Verbs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6514" citStr="Gordon and Swanson (2007)" startWordPosition="993" endWordPosition="996">and demonstrate improvements over competitive unsupervised methods by a wide margin. 2 Related Work As mentioned earlier, much previous work has focused on building supervised SRL systems (M`arquez et al., 2008). A few semi-supervised approaches have been developed within a framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (F¨urstenau and Lapata, 2009) or across different languages (Pad´o and Lapata, 2009). Outwith annotation projection, Gordon and Swanson (2007) attempt to increase the coverage of PropBank by leveraging existing labeled data. Rather than annotating new sentences that contain previously unseen verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method is unsupervised in that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it make</context>
</contexts>
<marker>Gordon, Swanson, 2007</marker>
<rawString>A. Gordon and R. Swanson. 2007. Generalizing Semantic Role Annotations Across Syntactically Similar Verbs. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 192–199, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Unsupervised Discovery of a Statistical Verb Lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="10303" citStr="Grenager and Manning (2006)" startWordPosition="1581" endWordPosition="1584">efore sible. This means that the argument identification be grouped together. Analogously, arguments that component does not make a final positive decision are lexically similar are likely to represent the same for any of the argument candidates; instead, this desemantic role. We operationalize these notions us- cision is deferred to role induction. The rules given ing a scoring function that quantifies the compatibil- in Table 1 are used to discard or select argument canity between arbitrary cluster pairs. Like Lang and didates. They primarily take into account the parts of Lapata (2010) and Grenager and Manning (2006) speech and the syntactic relations encountered when our method operates over syntactically parsed sen- traversing the dependency tree from predicate to artences, without, however, making use of any infor- gument. For each candidate, the first matching rule mation pertaining to semantic roles (e.g., in form of is applied. a lexical resource or manually annotated data). Per- We will exemplify how the argument identificaforming role-semantic analysis without a treebank- tion component works for the predicate expect in the trained parser is an interesting research direction, sentence “The company</context>
<context position="26999" citStr="Grenager and Manning, 2006" startWordPosition="4322" endWordPosition="4325">croaverage obtained by weighting the scores for individual verbs proportionately to the number of instances for that verb. Finally, we use the harmonic mean of purity and collocation as a single measure of clustering quality: F1 = 2xCOxPU (7) CO + PU Comparison Models We compared our splitmerge algorithm against two competitive approaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as point of comparison by other unsupervised semantic role labeling systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. Our implementation allocates up to N = 21 clusters2 for each verb, one for each of the 20 most frequent functions in the CoNLL dataset and a default cluster for all other functions. The second comparison model is the one proposed in Lang and Lapata (2010) (see Section 2). We used the same model settings (with 10 latent variables) and feature set proposed in that paper. Our method’s only parameter is the threshold a which we heuristically set to 0.1. On average our method induces 10 clusters per verb. 7 Results Our results are summariz</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>T. Grenager and C. Manning. 2006. Unsupervised Discovery of a Statistical Verb Lexicon. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 1–8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>ClassBased Construction of a Verb Lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>691--696</pages>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="7152" citStr="Kipper et al., 2000" startWordPosition="1093" endWordPosition="1096">se the coverage of PropBank by leveraging existing labeled data. Rather than annotating new sentences that contain previously unseen verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method is unsupervised in that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it makes use of VerbNet (Kipper et al., 2000) in order to identify the arguments of predicates and make initial role assignments. VerbNet is a broad coverage lexicon organized into verb classes each of which is explicitly associated with argument realization and semantic role specifications. Abend et al. (2009) propose an algorithm that identifies the arguments of predicates by relying only on part of speech annotations, without, however, assigning semantic roles. In contrast, Lang and Lapata (2010) focus solely on the role induction problem which they formulate as the process of detecting alternations and finding a canonical syntactic f</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang, and M. Palmer. 2000. ClassBased Construction of a Verb Lexicon. In Proceedings of the 17th AAAI Conference on Artificial Intelligence, pages 691–696. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lang</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised Induction of Semantic Roles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>939--947</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="7611" citStr="Lang and Lapata (2010)" startWordPosition="1164" endWordPosition="1167">that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it makes use of VerbNet (Kipper et al., 2000) in order to identify the arguments of predicates and make initial role assignments. VerbNet is a broad coverage lexicon organized into verb classes each of which is explicitly associated with argument realization and semantic role specifications. Abend et al. (2009) propose an algorithm that identifies the arguments of predicates by relying only on part of speech annotations, without, however, assigning semantic roles. In contrast, Lang and Lapata (2010) focus solely on the role induction problem which they formulate as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that makes use of the close relationship between syntactic functions and semantic roles. Grenager and Manning 1118 (2006) propose a directed graphical model which re- with the CoNLL 2008 benchmark dataset used </context>
<context position="24699" citStr="Lang and Lapata (2010)" startWordPosition="3941" endWordPosition="3944"> present the methods used for comparison with our approach. Data For evaluation purposes, the system’s output was compared against the CoNLL 2008 shared task dataset (Surdeanu et al., 2008) which provides 1122 Syntactic CO Function Lang and Lapata F1 Split -Merge F1 PU F1 PU CO PU CO auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang and Lapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function. PropBank-style gold standard annotations. The dataset was taken from the Wall Street Journal portion of the Penn Treebank corpus and converted into a dependency format (Surdeanu et al., 2008). In addition to gold standard dependency parses, the dataset also contains automatic parses obtained from the MaltParser (Nivre et al., 2007). Although the dataset provides annotations for verbal and nominal predicate-argument constructions, we only considered the former, following previous work on semantic role labeling</context>
<context position="27023" citStr="Lang and Lapata, 2010" startWordPosition="4326" endWordPosition="4329">ting the scores for individual verbs proportionately to the number of instances for that verb. Finally, we use the harmonic mean of purity and collocation as a single measure of clustering quality: F1 = 2xCOxPU (7) CO + PU Comparison Models We compared our splitmerge algorithm against two competitive approaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as point of comparison by other unsupervised semantic role labeling systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. Our implementation allocates up to N = 21 clusters2 for each verb, one for each of the 20 most frequent functions in the CoNLL dataset and a default cluster for all other functions. The second comparison model is the one proposed in Lang and Lapata (2010) (see Section 2). We used the same model settings (with 10 latent variables) and feature set proposed in that paper. Our method’s only parameter is the threshold a which we heuristically set to 0.1. On average our method induces 10 clusters per verb. 7 Results Our results are summarized in Table 2. We report</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>J. Lang and M. Lapata. 2010. Unsupervised Induction of Semantic Roles. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 939– 947, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>K Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic Role Labeling: an Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>159</pages>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. M`arquez, X. Carreras, K. Litkowski, and S. Stevenson. 2008. Semantic Role Labeling: an Introduction to the Special Issue. Computational Linguistics, 34(2):145– 159, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Melli</author>
<author>Y Wang</author>
<author>Y Liu</author>
<author>M M Kashani</author>
<author>Z Shi</author>
<author>B Gu</author>
<author>A Sarkar</author>
<author>F Popowich</author>
</authors>
<title>Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing Document Understanding Workshop,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2983" citStr="Melli et al., 2005" startWordPosition="454" endWordPosition="457">dicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1117 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mantic role labeling as a supervised learning problem. Current approaches have high performance — a system will rec</context>
</contexts>
<marker>Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, Popowich, 2005</marker>
<rawString>G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi, B. Gu, A. Sarkar, and F. Popowich. 2005. Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing Document Understanding Workshop, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit A Chanev</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A Language-independent System for Datadriven Dependency Parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Nivre, Hall, Nilsson, Chanev, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev, S. K¨ubler, S. Marinov, and E. Marsi. 2007. MaltParser: A Language-independent System for Datadriven Dependency Parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Cross-lingual Annotation Projection of Semantic Roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>36--307</pages>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>S. Pad´o and M. Lapata. 2009. Cross-lingual Annotation Projection of Semantic Roles. Journal of Artificial Intelligence Research, 36:307–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2055" citStr="Palmer et al., 2005" startWordPosition="310" endWordPosition="313">ver surface syntactic configurations. In the example sentences below. window occupies different syntactic positions — it is the object of broke in sentences (1a,b), and the subject in (1c) — while bearing the same semantic role, i.e., the physical object affected by the breaking event. Analogously, rock is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Joe]A0 broke the [window]A1 with a [rock]A2. b. The [rock]A2 broke the [window]A1. c. The [window]A1 broke. The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language underst</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J Martin</author>
</authors>
<title>Towards Robust Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4000" citStr="Pradhan et al., 2008" startWordPosition="605" endWordPosition="608">26, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mantic role labeling as a supervised learning problem. Current approaches have high performance — a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role (see M`arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in performance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008). Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling. Given the data requirements for supervised systems and the current paucity of such data, unsupervised methods offer a promising alternative. They require no human effort for training thus leading to significant savings in time and resources required for annotating text. And their output can be used in different ways, e.g., as a semantic preprocessing step for applications that re</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>S. Pradhan, W. Ward, and J. Martin. 2008. Towards Robust Semantic Role Labeling. Computational Linguistics, 34(2):289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using Semantic Roles to Improve Question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2900" citStr="Shen and Lapata, 2007" startWordPosition="441" endWordPosition="444">ore roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1117 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mantic role labeling as a superv</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using Semantic Roles to Improve Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning, pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using Predicate-Argument Structures for Information Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2853" citStr="Surdeanu et al., 2003" startWordPosition="433" endWordPosition="436">r) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1117 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computation</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using Predicate-Argument Structures for Information Extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th CoNLL,</booktitle>
<pages>159--177</pages>
<location>Manchester, England.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, and L. M`arquez. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th CoNLL, pages 159–177, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Swier</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised Semantic Role Labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>95--102</pages>
<location>Barcelona,</location>
<contexts>
<context position="6790" citStr="Swier and Stevenson (2004)" startWordPosition="1033" endWordPosition="1036">framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (F¨urstenau and Lapata, 2009) or across different languages (Pad´o and Lapata, 2009). Outwith annotation projection, Gordon and Swanson (2007) attempt to increase the coverage of PropBank by leveraging existing labeled data. Rather than annotating new sentences that contain previously unseen verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method is unsupervised in that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it makes use of VerbNet (Kipper et al., 2000) in order to identify the arguments of predicates and make initial role assignments. VerbNet is a broad coverage lexicon organized into verb classes each of which is explicitly associated with argument realization and semantic role specif</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>R. Swier and S. Stevenson. 2004. Unsupervised Semantic Role Labelling. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 95–102, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>P Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proceedings of North American Annual Meeting of the Association for Computational Linguistics HLT 2009: Short Papers,</booktitle>
<pages>13--16</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="2944" citStr="Wu and Fung, 2009" startWordPosition="448" endWordPosition="451">terpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se1More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient in the sense of Dowty (1991). 1117 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117–1126, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mantic role labeling as a supervised learning problem. Current approaches ha</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>D. Wu and P. Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proceedings of North American Annual Meeting of the Association for Computational Linguistics HLT 2009: Short Papers, pages 13–16, Boulder, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>