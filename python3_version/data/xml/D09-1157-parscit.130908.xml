<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000601">
<title confidence="0.997194">
Classifying Relations for Biomedical Named Entity Disambiguation
</title>
<author confidence="0.999521">
Xinglong Wang†$ Jun’ichi Tsujii*†$ Sophia Ananiadou†$
</author>
<affiliation confidence="0.999430666666667">
†School of Computer Science, University of Manchester, UK
$National Centre for Text Mining, UK
*Department of Computer Science, University of Tokyo, Japan
</affiliation>
<email confidence="0.997295">
{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784263157895">
Named entity disambiguation concerns
linking a potentially ambiguous mention
of named entity in text to an unambigu-
ous identifier in a standard database. One
approach to this task is supervised classifi-
cation. However, the availability of train-
ing data is often limited, and the avail-
able data sets tend to be imbalanced and,
in some cases, heterogeneous. We pro-
pose a new method that distinguishes a
named entity by finding the informative
keywords in its surrounding context, and
then trains a model to predict whether each
keyword indicates the semantic class of
the entity. While maintaining a compara-
ble performance to supervised classifica-
tion, this method avoids using expensive
manually annotated data for each new do-
main, and thus achieves better portability.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953661016949">
While technology on named entity recognition
(NER) matures, many researchers in the field of
information extraction (IE) gradually shifted their
focus to more complex tasks such as named en-
tity disambiguation and relation extraction. Both
tasks are particularly important for biomedical text
mining, which concerns automatically extracting
facts from the exponentially growing biomedical
literature (Hunter and Cohen, 2006). One type of
facts is relations between biomedical named en-
tities, such as disease-drug relation, gene-disease
relation, protein-protein interaction (PPI), etc. To
automatically extract these facts, advanced natu-
ral language processing techniques such as parsing
have been adopted to analyse the syntactic and se-
mantic structure of text. The idea is that linguistic
structures between the interacting biological enti-
ties may have common characteristics that can be
exploited by similarity measures or machine learn-
ing algorithms. For example, Erkan et al. (2007)
used the shortest path between two genes accord-
ing to edit distance in a dependency tree to de-
fine a kernel function for extracting gene interac-
tions. Miwa et al. (2008) comparably evaluated a
number of kernels for incorporating syntactic fea-
tures, including the bag-of-word kernel, the subset
tree kernel (Moschitti, 2006) and the graph ker-
nel (Airola et al., 2008), and they concluded that
combining all kernels achieved better results than
using any individual one. Miyao et al. (2008)
used syntactic paths as one of the features to train
a support vector machines (SVM) model for PPIs
and also discussed how different parsers and out-
put representations affected the end results.
Another crucial IE task is named entity disam-
biguation, which concerns grounding mentions of
named entities in text to unambiguous concepts as
defined in some standard dictionary or database.
For instance, given a search term Python, users
may like to see the results grouped into the fol-
lowing categories: a type of snake, a programming
language, or a film (Bunescu and Pas¸ca, 2006).
One approach to such lexical disambiguation tasks
is supervised classification. However, such tech-
niques suffer from the knowledge acquisition bot-
tleneck, meaning that manually annotating train-
ing data is costly and can never satisfy the need by
the machine learning algorithms. In addition, su-
pervised techniques may not yield reliable results
when the distributions of the semantic classes are
different in the training and test datasets (Agirre
and Martinez, 2004; Koeling et al., 2005). For ex-
ample, on the task of word sense disambiguation,
a model trained on a dataset where the predom-
inant sense of the word star is “heavenly body”,
may not work well on text mainly composed of
entertainment news. Such problems are also ma-
jor concerns when developing a system to disam-
biguate biomedical named entities (e.g., protein,
</bodyText>
<page confidence="0.868763">
1513
</page>
<note confidence="0.996551">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999835074074074">
gene, and disease), for which some researchers
rely on hand-crafted rules in addition to a small
amount of training data (Morgan and Hirschman,
2007; Hakenberg et al., 2008).
This paper proposes a new disambiguation
method that, instead of classifying each individual
occurrence of an entity, it classifies pair-wise re-
lations between the entity mention in question and
the “cue words” in its adjacent context, where each
cue word is assumed to bear a semantic class. We
then select the cue word that has a positive rela-
tion with the entity, and pass its semantic tag to it.
While an individual entity mention may belong to
a large number of semantic classes, a relation can
only take one of two values: positive or negative,
hence transforming a complex multi-classification
problem into a less complicated binary classifica-
tion task. The remainder of the paper is organised
as follows: Section 2 proposes the disambigua-
tion method and Section 3 introduces the task of
disambiguating the model organisms of biomedi-
cal named entities. Section 4 describes in detail
our proposed method and also a number of base-
line systems for comparison purposes. Section 5
shows the evaluation results and discusses the ad-
vantages and drawback of our system, and we fi-
nally conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.9781935" genericHeader="introduction">
2 Disambiguation as Relation
Classification
</sectionHeader>
<bodyText confidence="0.999946407407408">
The named entity disambiguation task is defined
as follows: given a mention of a named entity in
text, we automatically assign a semantic tag d to
it, where d E D, and D is a pre-compiled dic-
tionary with |D |entries. When |D |is small, the
problem can be approached by supervised classi-
fication. For example, to determine whether an
occurrence of an entity is a protein, a gene or an
RNA, Hatzivassiloglou et al. (2001) compared
performance of 3 supervised classification meth-
ods and reported results near the human agree-
ment rate. Nevertheless, when |D |is large (e.g.,
&gt; 100), the performance of classification may de-
crease, especially when the distribution of d in
training dataset differs from that in the test set. In
other words, when |D |is large, named entity dis-
ambiguation becomes a multi-class classification
task on heterogeneous and imbalanced datasets,
which is challenging for a machine learning model
to learn to discriminate enough between the se-
mantic classes (Japkowicz, 2000).
We propose an alternative method for named
entity disambiguation. Intuitively, in the surround-
ing context of an ambiguous entity, one can of-
ten find “cue words” that are informative indica-
tors of the entity’s semantic category. These cue
words are provided by authors to remind readers
the semantic identity of a named entity. For ex-
ample, in an article about protein p53, phrase “hu-
man protein p53” may be mentioned, where both
human and protein contain semantic information
regarding p53: human indicates the model organ-
ism of p53, and protein suggests the type of this
entity. Such cue words may occur infrequently in
the training data, making it difficult for machine
learning classifiers to capture.
Our method exploits this observation. Given a
sentence, let E be the set of ‘target’ entities (e.g.,
p53) and W of the ‘cue’ words (e.g., human) that
co-occur in a sentence, we define a relation as a
pair r = (e, w), where e E E and w E W, and
r is a positive relation if e belongs to the semantic
class indicated by w, and is a negative one if not.
Then we can disambiguate e by accomplishing the
following steps: 1) identify W and build a set
of relations R = {(e, wz)|wz E W, i = 1, 2, .., n},
where n is the size of W; and 2) classify every
r E R and assign the semantic tag of wj to e such
that rj = (e, wj) is positive. The first task can be
tackled by a dictionary lookup, or by an NER sys-
tem, if manually annotated data is available. The
second is essentially a binary relation classifica-
tion task, and in this work, we use an SVM model
exploiting bag-of-word and syntactic features.
</bodyText>
<sectionHeader confidence="0.993746" genericHeader="method">
3 Species Disambiguation
</sectionHeader>
<bodyText confidence="0.9999655625">
We show the performance of the proposed method
on a task of resolving one major source of am-
biguity in protein and gene entities: model or-
ganisms. Model organisms are species studied to
understand particular biological phenomena. Bi-
ological experiments are often conducted on one
species, with the expectation that the discover-
ies will provide insight into the workings of oth-
ers, including humans, which are more difficult
to study directly. From viruses, prokaryotes, to
plants and animals, there are dozens of organ-
isms commonly used in biological studies, such
as E. coli, Drosophila, Homo sapiens, and hun-
dreds more are frequently mentioned in biologi-
cal research papers. In biomedical articles, entities
of different species are commonly referred to us-
</bodyText>
<page confidence="0.990279">
1514
</page>
<bodyText confidence="0.999977282051282">
ing the same name, causing great ambiguity. For
example, searching a protein sequence database,
RefSeq1 with query “tumor protein p53” resulted
in over 100 proteins, as the name is shared by
many organisms.
The importance of distinguishing model organ-
isms has been recognised by the community of
biomedical text mining. Chen et al. (2005) col-
lected gene names from various source databases
and calculated intra- and inter-species ambigui-
ties. Overall, only 25 (0.02%) official symbols
were ambiguous within the organisms. However,
when official symbols from 21 organisms were
combined, the ambiguity increased substantially
to 21, 279 (14.2%) symbols. Hakenberg et al.
(2008) showed that species disambiguation is one
of the most important steps for term normalisa-
tion and identification, which concerns automat-
ically associating mentions of biomedical enti-
ties in text to unique database identifiers (Mor-
gan et al., 2008). Also, the task of extracting
PPIs in the recent BioCreative Challenge II work-
shop (Hirschman et al., 2007) requires protein
pairs to be recognised and normalised, which in-
evitably involves species disambiguation.
More specifically, given a text, in which men-
tions of biomedical named entities are annotated,
a species disambiguation system automatically as-
signs a species identifier, as in a standard database
of model organisms, to every entity mention. The
types of biomedical named entities concerned in
this study are protein, gene, protein complex and
mRNA/cDNA, and we used identifiers from the
NCBI Taxonomy of model organisms.2 The work
focuses on species disambiguation and assumes
that the entities are already identified. In practice,
an automated named entity recogniser (e.g., AB-
NER (Settles, 2005)) should be used before apply-
ing the systems.
</bodyText>
<sectionHeader confidence="0.997049" genericHeader="method">
4 Approaches
</sectionHeader>
<bodyText confidence="0.999752">
This section describes a number of approaches to
species disambiguation, highlighting the relation
classification method proposed in Section 2.
</bodyText>
<subsectionHeader confidence="0.998395">
4.1 Heuristics Baselines
</subsectionHeader>
<bodyText confidence="0.998945">
The cue words for species are words denoting
names of model organisms (e.g., mouse as in
</bodyText>
<footnote confidence="0.967189">
1http://www.ncbi.nlm.nih.gov/RefSeq
2http://www.ncbi.nlm.nih.gov/sites/
entrez?db=taxonomy
</footnote>
<bodyText confidence="0.999864523809524">
phrase “mouse p53”). Another clue is the pres-
ence of the species-indicating prefixes in gene and
protein names. For instance, prefix ‘h’ in en-
tity “hSos-1” suggests that it is a human protein.
Throughout this paper, we refer to such cue words
(e.g., mouse, hSos-1) as “species words”. Note
that a species “word” may contain multiple tokens
(e.g., E. Coli).
We encoded this knowledge in a rule-based
species tagging system (Wang and Grover, 2008).
The system takes a 2-step approach. First, it marks
up species words in the document using a species-
word detection program,3 which searches every
word in a dictionary of model organisms and as-
signs a species ID to the word if a match is found.
The dictionary was built using the NCBI taxon-
omy4 and the UniProt controlled vocabulary of
species,5 and in total it contains 420,224 species
words for 324,157 species IDs. When species
words are identified, we disambiguate an entity
mention using one of the following rules:
</bodyText>
<listItem confidence="0.986424833333333">
1. previous species word: If the word preceding an entity
is a species word, assign the species ID indicated by
that word to the entity.
2. species word in the same sentence: If a species word
and an entity appear in the same sentence, assign its
species ID to the entity. When more than one species
word co-occurs in the sentence, priority is given to the
species word to the entity’s left with the smallest dis-
tance. If all species words occur to the right of the en-
tity, take the nearest one.
3. majority vote: assign the most frequently occurring
species ID in the document to all entity mentions.
</listItem>
<bodyText confidence="0.997784947368421">
It is expected that the first rule would produce
good precision. However, it can only disam-
biguate the fraction of entities that happen to have
a species word to their immediate left. The second
rule relaxes the first by allowing an entity to take
the species indicated by its nearest species word
in the same sentence, which should increase recall
but decrease precision. Statistics from our dataset
(see Section 5.1) show that only 5.68% entities can
potentially be resolved by rule 1 and 22.16% by
rule 2, while the majority rule can tackle every en-
tity mention in the dataset.
3The species word detector identifies the cue words and
was used in all the systems studied in this paper. We could
not properly evaluate the detector due to the lack of man-
ually annotated data. Its performance, however, would not
affect the comparative evaluation results, and improvement
to species word detection should increase the performance of
these disambiguation systems.
</bodyText>
<footnote confidence="0.9974675">
4ftp://ftp.ncbi.nih.gov/pub/taxonomy/
5http://www.expasy.ch/cgi-bin/speclist
</footnote>
<page confidence="0.985342">
1515
</page>
<subsectionHeader confidence="0.971492">
4.2 Supervised Classification
</subsectionHeader>
<bodyText confidence="0.999879857142857">
The disambiguation problem can be approached as
a classification task. Given an entity mention and
its surrounding context, a machine learning model
classifies the entity into one of the classes, where
each class corresponds to a species ID. We car-
ried out experiments with two classification meth-
ods: multi-class classification and one-class clas-
sification, where a maximum entropy model6 was
used for the former and SVM-light7 for the lat-
ter. In one-class classification, we trained a se-
ries of binary SVM classifiers, each constructing
a separating hyperplane that maximises the mar-
gin between the instances of one specific species
(i.e., the target class) and a set of randomly se-
lected instances of other species (i.e., the outlier
class). We used equal numbers of instances for
both classes in training. The following types of
features were used in both multi-class and one-
class experiments, where the values of n were
set empirically by cross-validation on the training
data:
</bodyText>
<listItem confidence="0.999548913043478">
• leftContext The n word lemmas to the left of the entity
(n = 200).
• rightContext The n word lemmas to the right of the
entity (n = 200).
• leftSpeciesIDs The n species IDs to the left of the entity
(with order, n = 5).
• rightSpeciesIDs The n species IDs to the right of the
entity (with order, n = 5).
• leftNouns The n nouns to the left of the entity (with
order, n = 2).
• leftAdjs The n adjectives to the left of the entity (with
order, n = 2).
• leftSpeciesWords The n species word forms to the left
of the entity (n = 5).
• rightSpeciesWords The n species word forms to the
right of the entity (n = 5).
• firstLetter The first character of the entity itself (e.g.,
‘h’ in hP53).
• documentSpeciesIDs All species IDs that occur in the
document in question.
• useStopWords filter out function words.
• useStopPattern filter out words consisting only of digits
and punctuation characters.
</listItem>
<bodyText confidence="0.9979508">
Feature selection was also carried out for the
one-class classification experiments. We com-
pared two feature selection methods that report-
edly work well on the task of text classification:
information gain (IG) (Yang and Pedersen, 1997)
</bodyText>
<footnote confidence="0.999843666666667">
6http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
7http://svmlight.joachims.org/
</footnote>
<note confidence="0.64454">
ARG1 ARG1 ARG2
</note>
<figureCaption confidence="0.999413">
Figure 1: Predicate argument structure (PAS).
</figureCaption>
<bodyText confidence="0.996959677419355">
and Bi-Normal separation (BNS) (Forman, 2003).
IG measures the decrease in entropy when the
feature is given vs. absent, and is defined as:
IG(Y |X) = H(Y ) − H(Y |X) where H(Y ) is
the uncertainty about the value of Y (i.e., Y ’s en-
tropy), and H(Y |X) is Y ’s conditional entropy
given X. The BNS is defined as: |F−1(x) −
F−1(y)|, where F−1 is the standard Normal distri-
bution’s inverse cumulative probability function,
namely, z-score; x is the ratio between the number
of positive cases containing the feature in ques-
tion, and the total number of positive cases; and y
is the ratio between the number of negative cases
containing the feature, and the total number of
negative cases.
We computed a weight for each feature and then
ranked the features according to their weight, with
respect to each feature selection method. The top
10% features were used in training. Given a test
instance, the one-class classification method first
counts the species words in the document that the
instance appears in, and then applies in sequence
the binary models of each occurring species, start-
ing from the most frequent one. For example, if
a document contains 5 occurrences of human and
3 mouse, we first apply the human species model
to judge whether an entity mention is of human
species, and only if not, the mouse model was ap-
plied. The most-frequent species in the document
was used as backup when none of the binary mod-
els gives positive answers.
</bodyText>
<subsectionHeader confidence="0.946102">
4.3 Relation Classification
4.3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999225666666666">
As for the proposed relation classification method,
in the training phase, we first selected the sen-
tences in which an entity mention and a species
word co-occur, and constructed pair-wise entity-
species relations. We then assigned each relation a
binary label: a relation is positive if the species ID
inferred from the species word matches the gold-
standard species annotation on the entity, and is
negative otherwise. For example, for the sentence
shown in Figure 1, where Drosophila is a species
word, and Kip3 and Klp67A are proteins, relation
(Kip3, Drosophila) is a negative instance and the
</bodyText>
<note confidence="0.4962015">
Drosophila orthologue of Kip3 is Klp67A.
ARG1 ARG1 ARG2
</note>
<page confidence="0.964335">
1516
</page>
<bodyText confidence="0.99846075">
pair (Klp67A, Drosophila) is a positive one.8
For each relation, a vector of features were ex-
tracted. We followed the PPI extraction method
described in (Miyao et al., 2008), where two types
of features were used for a SVM classifier. The
first was bag-of-word features, i.e., the words be-
fore, between and after the pair of entities, where
the words were lemmatised. We added an ad-
ditional feature of the distance between the en-
tity and the cue word. The other type was syn-
tactic features obtained from parsers. For bag-
of-word features, a linear kernel was used, and
for syntactic ones, a subset tree kernel (Mos-
chitti, 2006) was adopted. The syntactic features
were represented in a flat tree format. Figure 2
shows such a feature for the negative instance
(Kip3, Drosophila) from Figure 1. Note that all
species words (e.g., Drosophila) were normalised
to “SPECIESWORD”, and entities (e.g., Kip3) to
“ENTITY”, which not only reduces the noise in
the feature set, but also makes the model more
species-generic. From the training dataset (see
Section 5.1), 25, 413 relations were extracted, of
which 63.3% were positive.
</bodyText>
<construct confidence="0.708060666666667">
(ENJU(noun arg1(SPECIESWORD orthologue))
(prep arg12(of orthologue))
(prep arg12(of ENTITY)))
</construct>
<figureCaption confidence="0.9989495">
Figure 2: A syntactic feature obtained from the ENJU
parser.
</figureCaption>
<bodyText confidence="0.999948666666667">
To identify the species of an entity in unseen
text, we first parsed the sentence, and then listed
all pairs of species words and entities as relations.
Having extracted the bag-of-word and syntactic
features from the instance, the trained model was
applied to judge whether each species-entity rela-
tion was positive. The entity mention in a positive
relation would be tagged with the ID indicated by
the species word, while the mentions in negative
relations would be left untagged. The next section
describes in detail how we extracted the syntactic
features from text.
</bodyText>
<subsectionHeader confidence="0.672991">
4.3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9991685">
Given a sentence, a natural language parser au-
tomatically recognises its syntactic structure and
outputs a parse tree, in which nodes represent
words or syntactic constituents. A path between
</bodyText>
<footnote confidence="0.998761333333333">
8Orthologues are genes/proteins in different species but
have similar sequences. In this example it implies that
Klp67A is a Drosophila protein but Kip3 is not.
</footnote>
<table confidence="0.998602875">
Parser Input Output
C&amp;C POS-tagged GR
ENJU POS-tagged PAS
ENJU-Genia POS-tagged PAS
Minipar Sentence-detected Minipar
RASP Tokenised GR
Stanford POS-tagged SD
Stanford-Genia POS-tagged SD
</table>
<tableCaption confidence="0.6875754">
Table 1: Parsers and their input and output format
a pair of nodes can be interpreted as a syntactic re-
lation between sentence units, which was proved
useful to infer biological relations (e.g., Airola et
al., 2008; Miwa et al., 2008).
</tableCaption>
<bodyText confidence="0.990406">
We experimented with the following parsers
(summarised in Table 1):
</bodyText>
<listItem confidence="0.990263421052632">
• Dependency parsers identify one word as the head
of a sentence and all other words are either a depen-
dent of that word, or else dependent on some other
word that connects to the headword through a sequence
of dependencies. We used Minipar (Lin, 1998) and
RASP (Briscoe et al., 2006) for the experiments;
• Constituent-structured parsers split a sentence into
syntactic constituents such as noun phrases or verb
phrases. We used the Stanford parser (Klein and Man-
ning, 2003), and also a variant of the Stanford parser
(i.e., Stanford-Genia), which was trained on the GE-
NIA treebank (Tateisi et al., 2005) for biomedical text;
• Deep parsers aim to compute in-depth syntactic and
semantic structures based on syntactic theories such as
HPSG (Pollard and Sag, 1994) and CCG (Steedman,
2000). We used the C&amp;C parser (Clark and Curran,
2007), ENJU (Miyao and Tsujii, 2008), and a variant
of ENJU (Hara et al., 2007) adapted for the biomedical
domain (i.e., ENJU-Genia);
</listItem>
<bodyText confidence="0.999973388888889">
There were a number of practical issues to con-
sider when using parsers for this task. Firstly, be-
fore parsing, the text needs to be linguistically pre-
processed, and the quality of this process has a sig-
nificant impact on parsers’ performance. The pre-
processing steps include sentence boundary detec-
tion, tokenisation and part-of-speech (POS) tag-
ging, all of which can be tricky especially when
applied to biomedical text (Grover et al., 2003).
To avoid the noise that can be introduced in the
pre-processing steps and to concentrate on evalu-
ating the performance of the parsers, we used the
same pre-processing tools (Alex et al., 2008a)9
whenever possible. The middle column in Ta-
ble 1 shows how the input text was linguisti-
cally pre-processed with respect to each parser.
A POS-tagged text implies that it was also sen-
tence boundary detected and tokenised Except for
</bodyText>
<footnote confidence="0.966652333333333">
9These particular tools were chosen because they were
adopted to pre-process the ITI-TXM dataset, which we used
in our study.
</footnote>
<page confidence="0.99589">
1517
</page>
<bodyText confidence="0.999664580645161">
RASP and Minipar, all parsers took POS-tagged
text as input. RASP requires POS tags and punctu-
ation labels that were derived from the CLAWS-7
tagset,10 whereas our dataset uses POS labels from
the Penn Treebank tagset (Marcus et al., 1994).
As RASP does not recognise the Penn tagset, we
used its build-in POS tagger. Minipar, on the other
hand, does not support input of tokenised or POS-
tagged text, and therefore took split sentences as
input.
Secondly, the output representations of the
parsers are different and we preferred a format
that depicts relations between words instead of
syntactic constituents. In total, 4 representations
were used: grammatical relation (GR) (Briscoe et
al., 2006), Stanford typed dependency (SD) (de
Marneffe et al., 2006), Minipar’s own representa-
tion (Lin, 1998), and ENJU’s predicate-argument
structure (PAS). All the above representations de-
fine relations of words in triples, where a depen-
dency triple (i.e., GR, SD and Minipar) consists
of head, dependent and relation, and a PAS triple
contains predicate, argument, and relation. Fig-
ure 1 shows a sentence parsed by ENJU in PAS
representation. The right-most column in Table 1
lists the output representation of each parser. A
syntactic path between an entity and a species
word was represented by a sequence of triples,
each following the order of head-dependent or
predicate-argument. These paths were used as
syntactic features for the SVM classifier.
</bodyText>
<subsectionHeader confidence="0.999771">
4.4 Spreading Strategies
</subsectionHeader>
<bodyText confidence="0.97111184">
Except for the majority vote rule, the approaches
described in Sections 4.1 and 4.3 were expected
to yield low recall, because they can only detect
intra-sentential relations, and therefore only be ap-
plied to the entities having at least one species
word appearing in the same sentence.
Since our aim is to disambiguate as many entity
mentions as possible, we would like to “spread”
the decisions from the disambiguated mentions to
their “relatives” in the same document. We define
an entity mention e¯ as another mention e’s rela-
tive under either of the following conditions: a)
if e¯ has the same surface form with e; or, b) if
e¯ is an abbreviation or an antecedent of e, where
abbreviation/antecedent pairs were detected using
the algorithm described in (Schwartz and Hearst,
10http://ucrel.lancs.ac.uk/claws7tags.
html
2003). Given the set of disambiguated mentions,
we then “spread” their species IDs to their rela-
tives in the same document. After this process, the
mentions that do not have any disambiguated rela-
tives would still be missed by the system. In such
cases, we used a “default” species, as determined
by the rule of majority vote (see Section 4.1).
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99646">
5.1 Data and Ontology
</subsectionHeader>
<bodyText confidence="0.999977">
The species disambiguation experiments were
conducted using the ITI-TXM corpus (Alex et al.,
2008b), a collection of full-length biomedical re-
search articles manually annotated with linguistic
and biomedical information for developing auto-
matic information extraction systems. The cor-
pus contains two datasets covering slightly dif-
ferent domains: enriched protein-protein interac-
tion (EPPI) and tissue expression (TE). When-
ever possible, protein, protein complex, gene, and
mRNA/cDNA entities were tagged with NCBI
Taxonomy IDs, denoting their species, and it was
the species annotation that this study used.
The EPPI and TE datasets have different distri-
butions of species. The entities in EPPI belong to
118 species with human being the most frequent at
51.98%. In TE, the entities are across 67 species
and mouse is the most frequent at 44.67%.11 The
inter-annotator agreement of species annotation on
EPPI and TE are 86.45% and 95.11%, respectively.
The species disambiguation systems were de-
veloped on the training portions of the EPPI and
TE corpora, each containing 221 articles, and eval-
uated on a dataset combining the development
test (DEVTEST) datasets of EPPI and TE, contain-
ing 58 and 48 articles, respectively. The com-
bined training dataset contains 96, 992 entity men-
tions belonging to 138 model organisms, while the
DEVTEST dataset contains 23,118 entities of 54
species. The diversity of model organisms in this
corpus highlights the fact that a primary consid-
eration when developing a species disambiguation
system is its ability to distinguish a wide range of
species with minimal additional manual effort.
</bodyText>
<sectionHeader confidence="0.627096" genericHeader="evaluation">
5.2 Results
</sectionHeader>
<subsectionHeader confidence="0.648601">
5.2.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.96421075">
The evaluation was carried out on the DEVTEST
dataset, and the systems are compared using av-
11These figures were obtained from the training split of the
datasets.
</bodyText>
<page confidence="0.974154">
1518
</page>
<table confidence="0.9928222">
micro-avg. macro-avg.
Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01
SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09
SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34
</table>
<tableCaption confidence="0.834096">
Table 2: Evaluation results of the classification systems on
DEVTEST (precision/recall/F1-score, in %)
</tableCaption>
<bodyText confidence="0.999925210526316">
eraged precision, recall and F1 scores over all
species. In more detail, for each model organism
that appears in the DEVTEST dataset, we collect
two lists of entity mentions of that species: one
from the gold-standard DEVTEST dataset, and the
other from the output of a disambiguation system.
Then the list of system output is compared against
the gold-standard list to obtain precision, recall
and F1 score. For each system, the scores ob-
tained from all species are averaged using micro-
average and macro-average. The micro-average is
the mean of the summation of contingency metrics
for all model organisms, so that scores of the more
frequent species influence the mean more than
those of less frequent ones. The macro-average is
the mean of precision, recall, or F1 over all labels,
thus attributing equal weights to each species, and
measuring a system’s adaptability across different
model organisms.
</bodyText>
<subsectionHeader confidence="0.83305">
5.2.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999975918918919">
First of all, Table 2 shows the results of the clas-
sification methods described in Section 4.2. The
multi-classification system using a maximum en-
tropy model (Maxent) yielded the highest overall
micro-averaged F1. Among the SVM-based sys-
tems, the one using IG feature selection achieved
better performance. In particular, it outperformed
the Maxent model in term of macro-averages. The
performance of the SVM model with BNS feature
selection is disappointing, perhaps because the oc-
currences of a feature in each instance are not nor-
mally distributed. As the Maxent system obtained
better results, it was used to compare with other
disambiguation systems.
Table 3 shows the results of a number of meth-
ods described in the previous sections. The meth-
ods are categorised into 4 groups: rule-based
baseline systems, a Maxent classification model,
relation-classification methods, and a hybrid sys-
tem. The difference between the relation classifi-
cation systems is the features adopted. Rel-Context
was trained on only bag-of-word and distance fea-
tures, whereas each other system also used syn-
tactic features provided by a specific parser. For
example, the Rel-RASP system identifies an entity’s
species by finding positive relations between the
entity and its neighbouring species words, using
features including bag-of-word, distance, and de-
pendency paths generated by RASP. The hybrid
system (Hbrd) ran the Rel-ENJU-Genia system on top
of the outcome of Maxent. When a conflict oc-
curs, the species ID is chosen by Rel-ENJU-Genia.
The idea is that the relation classification system
is more accurate than Maxent when it is applica-
ble, and hence would improve precision on dis-
ambiguating the species with few or no training
instances.
Without spreading (shown in the “NO SPRD”
columns of Table 3), most of the rule-based and re-
lation classification systems only work on a subset
of DEVTEST, resulting in low recall: Rule-Sp works
on the small proportion of entities (5.68%) with a
preceding species word, while the other systems
only work on the collection of sentences contain-
ing at least one species word and one entity, which
covers 4.60% sentences and 22.16% entity men-
tions. Rule-Majority, Maxent, and Hbrd, on the other
hand, apply to all entity mentions, and therefore
they are only compared against the others when
spreading was applied.
The results shown in the “NO SPRD” columns
can be viewed as a comparative evaluation of
the usefulness of the syntactic features supplied
by the parsers on this particular task. The rule-
based systems set high baselines: Rule-Sp pro-
duced good precision and Rule-SpSent achieved the
highest micro-averaged F1, thanks to its high
coverage, which is also an upperbound of recall
for the relation classification systems. Neverthe-
less, it is encouraging that the relation classifica-
tion systems obtained higher precision than Rule-
SpSent, which is important, considering the de-
cisions will be transfered to the untagged entity
mentions across the document. Indeed, as shown
in the SPRD columns in Table 3, most relation
classification systems outperformed the Rule-SpSent
baseline when spreading was used. The scores
of the systems using different parser outputs only
vary slightly. Rel-Context, on the other hand, sur-
passed others in terms of micro-averaged preci-
sion, while sacrificing micro-averaged recall and
macro-averaged scores.
Next, the SPRD columns in Table 3 show the re-
sults when the spreading rules were applied, which
</bodyText>
<page confidence="0.979816">
1519
</page>
<table confidence="0.999943785714286">
METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)
Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08
Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00
Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Rel-C&amp;C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14
Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67
Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21
Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44
Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80
Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45
Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64
Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80
</table>
<tableCaption confidence="0.999925">
Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)
</tableCaption>
<bodyText confidence="0.999882681818182">
effectively improved recall (see Section 5.2.3
for discussion on statistical significance tests on
the results). The Maxent system achieved very
good micro-averaged precision, but low macro-
averaged scores. In fact, as shown in Table 4, Max-
ent can only disambiguate 7 species (out of a total
of 54) that have relatively large amount of train-
ing instances,12 and failed completely on other
species. This suggests that Maxent may not be able
to generate good micro-averaged scores when ap-
plied to a dataset where the dominant species are
different from those in the training set. On the
other hand, the relation-classification approaches
have a clear advantage over Maxent as measured
by macro-averaged scores. As shown in Table 4,
Rel-ENJU-Genia worked well on most of the species,
displaying its good adaptability, while achieving
comparable micro-averaged F1 to Maxent. Over-
all, Hbrd, which combines the strengths of relation
classification and the Maxent classification model,
obtained the highest points as measured by every
metric.
</bodyText>
<subsectionHeader confidence="0.919847">
5.2.3 Statistical Significance
</subsectionHeader>
<bodyText confidence="0.999993818181818">
To see whether our methods significantly im-
proved the baseline systems, we performed ran-
domisation tests (Noreen, 1989; Yeh, 2000) on
some of the results shown in Table 3. The in-
tuition of randomisation test is as follows: when
comparing two systems (e.g., A and B), we erase
the labels “output of A” or “output of B” from all
observations. The null hypothesis is that there is
no difference between A and B, and thus any re-
sponse produced by one of the systems could have
as likely come from the other. We shuffle these re-
</bodyText>
<footnote confidence="0.78514325">
12The following 7 species occur most frequently in the
training set: H. sapiens (43.25%), M. musculus (27.05%),
R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis
(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).
</footnote>
<table confidence="0.99963925">
Species Name Pct Mxt Rel Hbrd
H. sapiens 50.13% 76.25 65.33 79.51
M. musculus 13.99% 66.41 58.29 68.27
X. tropicalis 7.35% 64.80 77.72 71.39
D. melanogaster 6.34% 93.17 78.46 95.15
S. cerevisiae 4.79% 90.12 83.32 87.68
R. norvegicus 2.97% 44.04 38.69 51.77
T. aestivum 2.62% 0.00 89.68 23.35
P. americana 2.27% 0.00 98.50 7.76
C. elegans 2.08% 96.83 95.88 97.50
H. herpesvirus 5 1.58% 0.00 54.46 4.27
R. virus 1.45% 0.00 28.54 6.45
H. spumaretrovirus 1.17% 0.00 99.37 2.49
... ... ... ... ...
Macro-average 9.85 30.21 31.80
Micro-average 70.48 67.13 73.70
</table>
<tableCaption confidence="0.810824333333333">
Table 4: The micro-averaged F1 scores (%) of Maxent
(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd with
respect to each of the most frequent 12 species in DEVTEST.
</tableCaption>
<bodyText confidence="0.999033454545454">
sponses R times, reassign each response to A or
B and see how likely such a shuffle produces a
difference in the metric of interest that is at least
as large as the difference observed when using A
and B on the test data. Let r denote the number
of times that such a difference occurred, then as
R → ∞, r+1
R+1 approaches the significance level.
In our case, the metrics tested were micro- and
macro-averaged precision, recall and F1.
Following this procedure, we tested whether the
improvements made by a relation classification
based system (i.e., Rel-ENJU-Genia with SPRD) and
the hybrid system (i.e., Hbrd) over the baseline sys-
tems were statistically significant. We carried out
approximate randomisation with 10,000 shuffles
and the test results are shown in Table 5. The nu-
merical figures in the cells are differences in pre-
cision, recall and F1 between a pair of systems.
The significance levels (i.e., p-values) are indi-
cated by superscript marks, whose correspond-
ing values are displayed in Table 6. For exam-
</bodyText>
<page confidence="0.953056">
1520
</page>
<table confidence="0.999707">
Rule-Majority Rule-Sp Rule-SpSent Maxent
Rel micro-avg 2.77*/3.46*/3.13* 1.95*/2.04*/2.00* 1.57*/2.22*/1.92* -1.57* / -5.02* / -3.35*
macro-avg 15.24*/13.12*/12.13* 3.75a/4.21a/3.20a 9.35*/8.44*/7.10* 21.92*/24.87*/20.35*
Hbrd micro-avg 8.01*/11.27*/9.70* 7.19*/9.85*/8.57* 6.81*/10.04* /8.49* 3.67*/2.78*/2.82b
macro-avg 27.22*/15.72c/13.72d 15.73*/6.82e/4.80f 21.33*/11.05g / 8.70h 33.91i/27.47*/21.95*
</table>
<tableCaption confidence="0.987452">
Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-
proved the baseline systems. The numerical figures in the cells show the differences between the two systems as measured by
precision/recall/F1 in percentage. The superscript marks indicate the significance levels and are explained in Table 6.
</tableCaption>
<bodyText confidence="0.999777166666667">
ple, the difference in micro-averaged precision be-
tween Rel-ENJU-Genia and Rule-Majority on the test
data was 2.77%, and in 10,000 approximate ran-
domisation trials, there was zero times13 that Rel-
ENJU-Genia’s micro-averaged precision is greater
than Rule-Majority’s by at least 2.77% (p &lt; 0.0001).
</bodyText>
<table confidence="0.981918833333333">
MARK VALUE MARK VALUE
* p &lt; 0.0001 a p &lt; 0.06
b p &lt; 0.002 c p &lt; 0.0003
d p &lt; 0.0002 e p &lt; 0.03
f p &lt; 0.05 g p &lt; 0.003
h p &lt; 0.005 i p &lt; 0.07
</table>
<tableCaption confidence="0.998704">
Table 6: p-values.
</tableCaption>
<bodyText confidence="0.999983">
The test results confirmed that, the improve-
ments made by Hbrd are statistically significant
with at least 95% confidence as measured by all
metrics except for macro-averaged precision. The
relation classification approach achieved signifi-
cantly lower performance than Maxent in terms of
micro-averaged scores (hence the “-” sign in the
corresponding cell in Table 5), but in all other
cases it can reject the null hypothesis with very
high confidence (i.e., p &lt; 0.0001).
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.974881382352941">
This paper proposes a method that tackles a com-
plex disambiguation problem by breaking it into
two cascaded simpler tasks of cue word discov-
ery and binary relation classification. We evalu-
ated the method on the task of disambiguating the
model organisms of biomedical named entities,
along with a number of other approaches. As mea-
sured by micro-averaged F1 score, a supervised
classification approach (Maxent) yielded the second
best result. However, it can only disambiguate
a small number of species that have abundant
training instances. With spreading rules, a rela-
tion classification system (Rel-ENJU-Genia) trained
on word and syntactic features from ENJU-Genia
also obtained good micro-averaged F1, while sur-
13The numbers of times are not shown in Table5 for
brevity.
passing Maxent significantly in terms of macro-
averaged scores. Combining these two systems
achieved the best overall performance. Neverthe-
less, we combined the two methods in a rather
crude way, leaving ample room for exploring bet-
ter strategies in the future.
One drawback of the relation classification sys-
tems is that they can not cover all entity mentions
but only the ones with informative keywords co-
occurring in the same sentence. We overcame the
drawback by using spreading rules. For some ap-
plications, however, it may be sufficient to make
predictions exclusively for cases where the sys-
tems are applicable. Also, the predictions with
high confidence can be used as seed training ma-
terial for automatically harvesting more training
data.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999940166666667">
The work reported in this paper is funded by Pfizer
Ltd.. The UK National Centre for Text Mining is
funded by JISC. The ITI-TXM corpus used in the
experiments was developed at School of Informat-
ics, University of Edinburgh, in the TXM project,
which was funded by ITI Life Sciences, Scotland.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999128947368421">
E. Agirre and D. Martinez. 2004. Unsupervised WSD based
on automatically retrieved examples: The importance of
bias. In Proceedings of EMNLP.
A. Airola, S. Pyysalo, J. Bj¨orne, T. Pahikkala, F. Ginter, and
T. Salakoski. 2008. A graph kernel for protein-protein
interaction extraction. In Proceedings of BioNLP.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a.
Assisted curation: does text mining really help? In Pro-
ceedings of the Pacific Symposium on Biocomputing.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b.
The ITI TXM corpus: Tissue expression and protein-
protein interactions. In Proceedings of the Workshop on
Building and Evaluating Resources for Biomedical Text
Mining at LREC.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL Interactive Presentation Sessions.
</reference>
<page confidence="0.810361">
1521
</page>
<reference confidence="0.999869020833334">
R. Bunescu and M. Pas¸ca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name
ambiguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248–256.
S. Clark and J. R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models. Com-
putational Linguistics, 33(4).
M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006.
Generating typed dependency parses from phrase struc-
ture. In Proceedings of LREC.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interaction
sentences using dependency parsing. In Proceedings of
the Joint Conference of EMNLP and CoNLL.
G. Forman. 2003. An extensive empirical study of feature se-
lection metrics for text classification. Journal of Machine
Learning Research, 3:1289–1305.
C. Grover, M. Lapata, and A. Ascarides. 2003. A compar-
ison of parsing technologies for the biomedical domain.
Natural Language Engineering, 1(1):1–38.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of gene
mentions with GNAT. Bioinformatics, 24(16).
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact
of re-training a lexical disambiguation model on domain
adaptation of an HPSG parser. In Proceedings of the 10th
International Conference on Parsing Technology.
V. Hatzivassiloglou, PA Dubou´e, and A. Rzhetsky. 2001.
Disambiguating proteins, genes, and RNA in text: a ma-
chine learning approach. Bioinformatics, 17(Suppl 1).
L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-
itors. 2007. The BioCreative II - Critical Assessment
for Information Extraction in Biology Challenge, volume
9(Suppl 2). Genome Biology.
L. Hunter and K. B. Cohen. 2006. Biomedical language
processing: what’s beyond PubMed. Molecular Cell,
21(5):589–594.
N. Japkowicz. 2000. Learning from imbalanced data sets: a
comparison of various strategies. In Proceedings of AAAI
Workshop on Learning from Imbalanced Data Sets.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of ACL.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of HLT/EMNLP.
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–330.
M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008.
Combining multiple layers of syntactic information for
protein-protein interaction extraction. In Proceedings of
SMBM.
Y. Miyao and J. Tsujii. 2008. Feature forest models for prob-
abilistic HPSG parsing. Computational Linguistics, 34(1).
Y. Miyao, R. Sastre, K. Sagae, T. Matsuzaki, and J. Tsujii.
2008. Task-oriented evaluation of syntactic parsers and
their representations. In Proceedings of ACL-08: HLT.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreAtIvE II gene normalisation. In Proceedings of the
BioCreAtIvE II Workshop, Madrid.
A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.
Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and
L. Hirschman. 2008. Overview of BioCreAtIvE II gene
normalization. Genome Biology, 9(Suppl 2).
A. Moschitti. 2006. Making tree kernels practical for natural
language learning. In Proceedings of EACL.
E. W. Noreen. 1989. Computer Intensive Methods for Test-
ing Hypothesis. John Wiley &amp; Sons.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
A. S. Schwartz and M. A. Hearst. 2003. Identifying abbrevi-
ation definitions in biomedical text. In Proceedings of the
Pacific Symposium on Biocomputing.
B. Settles. 2005. ABNER: An open source tool for automat-
ically tagging genes, proteins, and other entity names in
text. Bioinformatics, 21(14):3191–3192.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syn-
tax annotation for the GENIA corpus. In Proceedings of
IJCNLP.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings of LREC.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings of
ICML.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of COLING.
</reference>
<page confidence="0.993585">
1522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858390">
<title confidence="0.981604">Classifying Relations for Biomedical Named Entity Disambiguation</title>
<affiliation confidence="0.990679333333333">of Computer Science, University of Manchester, Centre for Text Mining, of Computer Science, University of Tokyo,</affiliation>
<abstract confidence="0.99493965">Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a named entity by finding the informative keywords in its surrounding context, and then trains a model to predict whether each keyword indicates the semantic class of the entity. While maintaining a comparable performance to supervised classification, this method avoids using expensive manually annotated data for each new domain, and thus achieves better portability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Unsupervised WSD based on automatically retrieved examples: The importance of bias.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3648" citStr="Agirre and Martinez, 2004" startWordPosition="542" endWordPosition="545">ython, users may like to see the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addi</context>
</contexts>
<marker>Agirre, Martinez, 2004</marker>
<rawString>E. Agirre and D. Martinez. 2004. Unsupervised WSD based on automatically retrieved examples: The importance of bias. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Airola</author>
<author>S Pyysalo</author>
<author>J Bj¨orne</author>
<author>T Pahikkala</author>
<author>F Ginter</author>
<author>T Salakoski</author>
</authors>
<title>A graph kernel for protein-protein interaction extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of BioNLP.</booktitle>
<marker>Airola, Pyysalo, Bj¨orne, Pahikkala, Ginter, Salakoski, 2008</marker>
<rawString>A. Airola, S. Pyysalo, J. Bj¨orne, T. Pahikkala, F. Ginter, and T. Salakoski. 2008. A graph kernel for protein-protein interaction extraction. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Alex</author>
<author>C Grover</author>
<author>B Haddow</author>
<author>M Kabadjov</author>
<author>E Klein</author>
<author>M Matthews</author>
<author>S Roebuck</author>
<author>R Tobin</author>
<author>X Wang</author>
</authors>
<title>Assisted curation: does text mining really help?</title>
<date>2008</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing.</booktitle>
<contexts>
<context position="22398" citStr="Alex et al., 2008" startWordPosition="3622" endWordPosition="3625">r of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can be introduced in the pre-processing steps and to concentrate on evaluating the performance of the parsers, we used the same pre-processing tools (Alex et al., 2008a)9 whenever possible. The middle column in Table 1 shows how the input text was linguistically pre-processed with respect to each parser. A POS-tagged text implies that it was also sentence boundary detected and tokenised Except for 9These particular tools were chosen because they were adopted to pre-process the ITI-TXM dataset, which we used in our study. 1517 RASP and Minipar, all parsers took POS-tagged text as input. RASP requires POS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1</context>
<context position="25535" citStr="Alex et al., 2008" startWordPosition="4126" endWordPosition="4129">reviation/antecedent pairs were detected using the algorithm described in (Schwartz and Hearst, 10http://ucrel.lancs.ac.uk/claws7tags. html 2003). Given the set of disambiguated mentions, we then “spread” their species IDs to their relatives in the same document. After this process, the mentions that do not have any disambiguated relatives would still be missed by the system. In such cases, we used a “default” species, as determined by the rule of majority vote (see Section 4.1). 5 Evaluation 5.1 Data and Ontology The species disambiguation experiments were conducted using the ITI-TXM corpus (Alex et al., 2008b), a collection of full-length biomedical research articles manually annotated with linguistic and biomedical information for developing automatic information extraction systems. The corpus contains two datasets covering slightly different domains: enriched protein-protein interaction (EPPI) and tissue expression (TE). Whenever possible, protein, protein complex, gene, and mRNA/cDNA entities were tagged with NCBI Taxonomy IDs, denoting their species, and it was the species annotation that this study used. The EPPI and TE datasets have different distributions of species. The entities in EPPI b</context>
</contexts>
<marker>Alex, Grover, Haddow, Kabadjov, Klein, Matthews, Roebuck, Tobin, Wang, 2008</marker>
<rawString>B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein, M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a. Assisted curation: does text mining really help? In Proceedings of the Pacific Symposium on Biocomputing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Alex</author>
<author>C Grover</author>
<author>B Haddow</author>
<author>M Kabadjov</author>
<author>E Klein</author>
<author>M Matthews</author>
<author>S Roebuck</author>
<author>R Tobin</author>
<author>X Wang</author>
</authors>
<title>The ITI TXM corpus: Tissue expression and proteinprotein interactions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Building and Evaluating Resources for Biomedical Text Mining at LREC.</booktitle>
<contexts>
<context position="22398" citStr="Alex et al., 2008" startWordPosition="3622" endWordPosition="3625">r of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can be introduced in the pre-processing steps and to concentrate on evaluating the performance of the parsers, we used the same pre-processing tools (Alex et al., 2008a)9 whenever possible. The middle column in Table 1 shows how the input text was linguistically pre-processed with respect to each parser. A POS-tagged text implies that it was also sentence boundary detected and tokenised Except for 9These particular tools were chosen because they were adopted to pre-process the ITI-TXM dataset, which we used in our study. 1517 RASP and Minipar, all parsers took POS-tagged text as input. RASP requires POS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1</context>
<context position="25535" citStr="Alex et al., 2008" startWordPosition="4126" endWordPosition="4129">reviation/antecedent pairs were detected using the algorithm described in (Schwartz and Hearst, 10http://ucrel.lancs.ac.uk/claws7tags. html 2003). Given the set of disambiguated mentions, we then “spread” their species IDs to their relatives in the same document. After this process, the mentions that do not have any disambiguated relatives would still be missed by the system. In such cases, we used a “default” species, as determined by the rule of majority vote (see Section 4.1). 5 Evaluation 5.1 Data and Ontology The species disambiguation experiments were conducted using the ITI-TXM corpus (Alex et al., 2008b), a collection of full-length biomedical research articles manually annotated with linguistic and biomedical information for developing automatic information extraction systems. The corpus contains two datasets covering slightly different domains: enriched protein-protein interaction (EPPI) and tissue expression (TE). Whenever possible, protein, protein complex, gene, and mRNA/cDNA entities were tagged with NCBI Taxonomy IDs, denoting their species, and it was the species annotation that this study used. The EPPI and TE datasets have different distributions of species. The entities in EPPI b</context>
</contexts>
<marker>Alex, Grover, Haddow, Kabadjov, Klein, Matthews, Roebuck, Tobin, Wang, 2008</marker>
<rawString>B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein, M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b. The ITI TXM corpus: Tissue expression and proteinprotein interactions. In Proceedings of the Workshop on Building and Evaluating Resources for Biomedical Text Mining at LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL Interactive Presentation Sessions.</booktitle>
<contexts>
<context position="21080" citStr="Briscoe et al., 2006" startWordPosition="3406" endWordPosition="3409"> SD Stanford-Genia POS-tagged SD Table 1: Parsers and their input and output format a pair of nodes can be interpreted as a syntactic relation between sentence units, which was proved useful to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant</context>
<context position="23460" citStr="Briscoe et al., 2006" startWordPosition="3794" endWordPosition="3797">OS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1994). As RASP does not recognise the Penn tagset, we used its build-in POS tagger. Minipar, on the other hand, does not support input of tokenised or POStagged text, and therefore took split sentences as input. Secondly, the output representations of the parsers are different and we preferred a format that depicts relations between words instead of syntactic constituents. In total, 4 representations were used: grammatical relation (GR) (Briscoe et al., 2006), Stanford typed dependency (SD) (de Marneffe et al., 2006), Minipar’s own representation (Lin, 1998), and ENJU’s predicate-argument structure (PAS). All the above representations define relations of words in triples, where a dependency triple (i.e., GR, SD and Minipar) consists of head, dependent and relation, and a PAS triple contains predicate, argument, and relation. Figure 1 shows a sentence parsed by ENJU in PAS representation. The right-most column in Table 1 lists the output representation of each parser. A syntactic path between an entity and a species word was represented by a sequen</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>E. Briscoe, J. Carroll, and R. Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL Interactive Presentation Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>R. Bunescu and M. Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>H Liu</author>
<author>C Friedman</author>
</authors>
<title>Gene name ambiguity of eukaryotic nomenclatures.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="9213" citStr="Chen et al. (2005)" startWordPosition="1483" endWordPosition="1486"> animals, there are dozens of organisms commonly used in biological studies, such as E. coli, Drosophila, Homo sapiens, and hundreds more are frequently mentioned in biological research papers. In biomedical articles, entities of different species are commonly referred to us1514 ing the same name, causing great ambiguity. For example, searching a protein sequence database, RefSeq1 with query “tumor protein p53” resulted in over 100 proteins, as the name is shared by many organisms. The importance of distinguishing model organisms has been recognised by the community of biomedical text mining. Chen et al. (2005) collected gene names from various source databases and calculated intra- and inter-species ambiguities. Overall, only 25 (0.02%) official symbols were ambiguous within the organisms. However, when official symbols from 21 organisms were combined, the ambiguity increased substantially to 21, 279 (14.2%) symbols. Hakenberg et al. (2008) showed that species disambiguation is one of the most important steps for term normalisation and identification, which concerns automatically associating mentions of biomedical entities in text to unique database identifiers (Morgan et al., 2008). Also, the task</context>
</contexts>
<marker>Chen, Liu, Friedman, 2005</marker>
<rawString>L. Chen, H. Liu, and C. Friedman. 2005. Gene name ambiguity of eukaryotic nomenclatures. Bioinformatics, 21(2):248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="21634" citStr="Clark and Curran, 2007" startWordPosition="3495" endWordPosition="3498">ndencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>A Ozgur</author>
<author>D R Radev</author>
</authors>
<title>Semisupervised classification for extracting protein interaction sentences using dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference of EMNLP and CoNLL.</booktitle>
<contexts>
<context position="2112" citStr="Erkan et al. (2007)" startWordPosition="297" endWordPosition="300">y growing biomedical literature (Hunter and Cohen, 2006). One type of facts is relations between biomedical named entities, such as disease-drug relation, gene-disease relation, protein-protein interaction (PPI), etc. To automatically extract these facts, advanced natural language processing techniques such as parsing have been adopted to analyse the syntactic and semantic structure of text. The idea is that linguistic structures between the interacting biological entities may have common characteristics that can be exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also d</context>
</contexts>
<marker>Erkan, Ozgur, Radev, 2007</marker>
<rawString>G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semisupervised classification for extracting protein interaction sentences using dependency parsing. In Proceedings of the Joint Conference of EMNLP and CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="15974" citStr="Forman, 2003" startWordPosition="2565" endWordPosition="2566">ies IDs that occur in the document in question. • useStopWords filter out function words. • useStopPattern filter out words consisting only of digits and punctuation characters. Feature selection was also carried out for the one-class classification experiments. We compared two feature selection methods that reportedly work well on the task of text classification: information gain (IG) (Yang and Pedersen, 1997) 6http://homepages.inf.ed.ac.uk/ s0450736/maxent_toolkit.html 7http://svmlight.joachims.org/ ARG1 ARG1 ARG2 Figure 1: Predicate argument structure (PAS). and Bi-Normal separation (BNS) (Forman, 2003). IG measures the decrease in entropy when the feature is given vs. absent, and is defined as: IG(Y |X) = H(Y ) − H(Y |X) where H(Y ) is the uncertainty about the value of Y (i.e., Y ’s entropy), and H(Y |X) is Y ’s conditional entropy given X. The BNS is defined as: |F−1(x) − F−1(y)|, where F−1 is the standard Normal distribution’s inverse cumulative probability function, namely, z-score; x is the ratio between the number of positive cases containing the feature in question, and the total number of positive cases; and y is the ratio between the number of negative cases containing the feature,</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>G. Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>M Lapata</author>
<author>A Ascarides</author>
</authors>
<title>A comparison of parsing technologies for the biomedical domain.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="22205" citStr="Grover et al., 2003" startWordPosition="3589" endWordPosition="3592">. We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can be introduced in the pre-processing steps and to concentrate on evaluating the performance of the parsers, we used the same pre-processing tools (Alex et al., 2008a)9 whenever possible. The middle column in Table 1 shows how the input text was linguistically pre-processed with respect to each parser. A POS-tagged text implies that it was also sentence boundary detected and tokenised Except for 9These particular tools were chosen because they were adopted to pre-process the ITI-TXM dataset, which we used in our study. 1517 RASP and Minipar, all parsers took POS-tag</context>
</contexts>
<marker>Grover, Lapata, Ascarides, 2003</marker>
<rawString>C. Grover, M. Lapata, and A. Ascarides. 2003. A comparison of parsing technologies for the biomedical domain. Natural Language Engineering, 1(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hakenberg</author>
<author>C Plake</author>
<author>R Leaman</author>
<author>M Schroeder</author>
<author>G Gonzalez</author>
</authors>
<title>Inter-species normalization of gene mentions with GNAT.</title>
<date>2008</date>
<journal>Bioinformatics,</journal>
<volume>24</volume>
<issue>16</issue>
<contexts>
<context position="4340" citStr="Hakenberg et al., 2008" startWordPosition="655" endWordPosition="658">biguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount of training data (Morgan and Hirschman, 2007; Hakenberg et al., 2008). This paper proposes a new disambiguation method that, instead of classifying each individual occurrence of an entity, it classifies pair-wise relations between the entity mention in question and the “cue words” in its adjacent context, where each cue word is assumed to bear a semantic class. We then select the cue word that has a positive relation with the entity, and pass its semantic tag to it. While an individual entity mention may belong to a large number of semantic classes, a relation can only take one of two values: positive or negative, hence transforming a complex multi-classificati</context>
<context position="9550" citStr="Hakenberg et al. (2008)" startWordPosition="1531" endWordPosition="1534">ple, searching a protein sequence database, RefSeq1 with query “tumor protein p53” resulted in over 100 proteins, as the name is shared by many organisms. The importance of distinguishing model organisms has been recognised by the community of biomedical text mining. Chen et al. (2005) collected gene names from various source databases and calculated intra- and inter-species ambiguities. Overall, only 25 (0.02%) official symbols were ambiguous within the organisms. However, when official symbols from 21 organisms were combined, the ambiguity increased substantially to 21, 279 (14.2%) symbols. Hakenberg et al. (2008) showed that species disambiguation is one of the most important steps for term normalisation and identification, which concerns automatically associating mentions of biomedical entities in text to unique database identifiers (Morgan et al., 2008). Also, the task of extracting PPIs in the recent BioCreative Challenge II workshop (Hirschman et al., 2007) requires protein pairs to be recognised and normalised, which inevitably involves species disambiguation. More specifically, given a text, in which mentions of biomedical named entities are annotated, a species disambiguation system automatical</context>
</contexts>
<marker>Hakenberg, Plake, Leaman, Schroeder, Gonzalez, 2008</marker>
<rawString>J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and G. Gonzalez. 2008. Inter-species normalization of gene mentions with GNAT. Bioinformatics, 24(16).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hara</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technology.</booktitle>
<contexts>
<context position="21708" citStr="Hara et al., 2007" startWordPosition="3509" endWordPosition="3512">xperiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can be introduced in the pre-processing steps and to concentrate on evaluatin</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2007</marker>
<rawString>T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser. In Proceedings of the 10th International Conference on Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>PA Dubou´e</author>
<author>A Rzhetsky</author>
</authors>
<title>Disambiguating proteins, genes, and RNA in text: a machine learning approach.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<marker>Hatzivassiloglou, Dubou´e, Rzhetsky, 2001</marker>
<rawString>V. Hatzivassiloglou, PA Dubou´e, and A. Rzhetsky. 2001. Disambiguating proteins, genes, and RNA in text: a machine learning approach. Bioinformatics, 17(Suppl 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Krallinger</author>
<author>J Wilbur</author>
<author>A Valencia</author>
<author>editors</author>
</authors>
<date>2007</date>
<booktitle>The BioCreative II - Critical Assessment for Information Extraction in Biology Challenge, volume 9(Suppl 2). Genome Biology.</booktitle>
<contexts>
<context position="9905" citStr="Hirschman et al., 2007" startWordPosition="1587" endWordPosition="1590">tra- and inter-species ambiguities. Overall, only 25 (0.02%) official symbols were ambiguous within the organisms. However, when official symbols from 21 organisms were combined, the ambiguity increased substantially to 21, 279 (14.2%) symbols. Hakenberg et al. (2008) showed that species disambiguation is one of the most important steps for term normalisation and identification, which concerns automatically associating mentions of biomedical entities in text to unique database identifiers (Morgan et al., 2008). Also, the task of extracting PPIs in the recent BioCreative Challenge II workshop (Hirschman et al., 2007) requires protein pairs to be recognised and normalised, which inevitably involves species disambiguation. More specifically, given a text, in which mentions of biomedical named entities are annotated, a species disambiguation system automatically assigns a species identifier, as in a standard database of model organisms, to every entity mention. The types of biomedical named entities concerned in this study are protein, gene, protein complex and mRNA/cDNA, and we used identifiers from the NCBI Taxonomy of model organisms.2 The work focuses on species disambiguation and assumes that the entiti</context>
</contexts>
<marker>Hirschman, Krallinger, Wilbur, Valencia, editors, 2007</marker>
<rawString>L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, editors. 2007. The BioCreative II - Critical Assessment for Information Extraction in Biology Challenge, volume 9(Suppl 2). Genome Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hunter</author>
<author>K B Cohen</author>
</authors>
<title>Biomedical language processing: what’s beyond PubMed. Molecular Cell,</title>
<date>2006</date>
<contexts>
<context position="1549" citStr="Hunter and Cohen, 2006" startWordPosition="215" endWordPosition="218">maintaining a comparable performance to supervised classification, this method avoids using expensive manually annotated data for each new domain, and thus achieves better portability. 1 Introduction While technology on named entity recognition (NER) matures, many researchers in the field of information extraction (IE) gradually shifted their focus to more complex tasks such as named entity disambiguation and relation extraction. Both tasks are particularly important for biomedical text mining, which concerns automatically extracting facts from the exponentially growing biomedical literature (Hunter and Cohen, 2006). One type of facts is relations between biomedical named entities, such as disease-drug relation, gene-disease relation, protein-protein interaction (PPI), etc. To automatically extract these facts, advanced natural language processing techniques such as parsing have been adopted to analyse the syntactic and semantic structure of text. The idea is that linguistic structures between the interacting biological entities may have common characteristics that can be exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two g</context>
</contexts>
<marker>Hunter, Cohen, 2006</marker>
<rawString>L. Hunter and K. B. Cohen. 2006. Biomedical language processing: what’s beyond PubMed. Molecular Cell, 21(5):589–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Japkowicz</author>
</authors>
<title>Learning from imbalanced data sets: a comparison of various strategies.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI Workshop on Learning from Imbalanced Data Sets.</booktitle>
<contexts>
<context position="6488" citStr="Japkowicz, 2000" startWordPosition="1011" endWordPosition="1012">r an RNA, Hatzivassiloglou et al. (2001) compared performance of 3 supervised classification methods and reported results near the human agreement rate. Nevertheless, when |D |is large (e.g., &gt; 100), the performance of classification may decrease, especially when the distribution of d in training dataset differs from that in the test set. In other words, when |D |is large, named entity disambiguation becomes a multi-class classification task on heterogeneous and imbalanced datasets, which is challenging for a machine learning model to learn to discriminate enough between the semantic classes (Japkowicz, 2000). We propose an alternative method for named entity disambiguation. Intuitively, in the surrounding context of an ambiguous entity, one can often find “cue words” that are informative indicators of the entity’s semantic category. These cue words are provided by authors to remind readers the semantic identity of a named entity. For example, in an article about protein p53, phrase “human protein p53” may be mentioned, where both human and protein contain semantic information regarding p53: human indicates the model organism of p53, and protein suggests the type of this entity. Such cue words may</context>
</contexts>
<marker>Japkowicz, 2000</marker>
<rawString>N. Japkowicz. 2000. Learning from imbalanced data sets: a comparison of various strategies. In Proceedings of AAAI Workshop on Learning from Imbalanced Data Sets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21271" citStr="Klein and Manning, 2003" startWordPosition="3434" endWordPosition="3438">ul to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Koeling</author>
<author>D McCarthy</author>
<author>J Carroll</author>
</authors>
<title>Domainspecific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="3671" citStr="Koeling et al., 2005" startWordPosition="546" endWordPosition="549">e the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount </context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>R. Koeling, D. McCarthy, and J. Carroll. 2005. Domainspecific sense distributions and predominant sense acquisition. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="21048" citStr="Lin, 1998" startWordPosition="3402" endWordPosition="3403">R Stanford POS-tagged SD Stanford-Genia POS-tagged SD Table 1: Parsers and their input and output format a pair of nodes can be interpreted as a syntactic relation between sentence units, which was proved useful to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao </context>
<context position="23561" citStr="Lin, 1998" startWordPosition="3811" endWordPosition="3812">from the Penn Treebank tagset (Marcus et al., 1994). As RASP does not recognise the Penn tagset, we used its build-in POS tagger. Minipar, on the other hand, does not support input of tokenised or POStagged text, and therefore took split sentences as input. Secondly, the output representations of the parsers are different and we preferred a format that depicts relations between words instead of syntactic constituents. In total, 4 representations were used: grammatical relation (GR) (Briscoe et al., 2006), Stanford typed dependency (SD) (de Marneffe et al., 2006), Minipar’s own representation (Lin, 1998), and ENJU’s predicate-argument structure (PAS). All the above representations define relations of words in triples, where a dependency triple (i.e., GR, SD and Minipar) consists of head, dependent and relation, and a PAS triple contains predicate, argument, and relation. Figure 1 shows a sentence parsed by ENJU in PAS representation. The right-most column in Table 1 lists the output representation of each parser. A syntactic path between an entity and a species word was represented by a sequence of triples, each following the order of head-dependent or predicate-argument. These paths were use</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based evaluation of MINIPAR. In Proceedings of Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="23002" citStr="Marcus et al., 1994" startWordPosition="3722" endWordPosition="3725">lex et al., 2008a)9 whenever possible. The middle column in Table 1 shows how the input text was linguistically pre-processed with respect to each parser. A POS-tagged text implies that it was also sentence boundary detected and tokenised Except for 9These particular tools were chosen because they were adopted to pre-process the ITI-TXM dataset, which we used in our study. 1517 RASP and Minipar, all parsers took POS-tagged text as input. RASP requires POS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1994). As RASP does not recognise the Penn tagset, we used its build-in POS tagger. Minipar, on the other hand, does not support input of tokenised or POStagged text, and therefore took split sentences as input. Secondly, the output representations of the parsers are different and we preferred a format that depicts relations between words instead of syntactic constituents. In total, 4 representations were used: grammatical relation (GR) (Briscoe et al., 2006), Stanford typed dependency (SD) (de Marneffe et al., 2006), Minipar’s own representation (Lin, 1998), and ENJU’s predicate-argument structure</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Miwa</author>
<author>R Satre</author>
<author>Y Miyao</author>
<author>T Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Combining multiple layers of syntactic information for protein-protein interaction extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of SMBM.</booktitle>
<contexts>
<context position="2282" citStr="Miwa et al. (2008)" startWordPosition="328" endWordPosition="331">ation, protein-protein interaction (PPI), etc. To automatically extract these facts, advanced natural language processing techniques such as parsing have been adopted to analyse the syntactic and semantic structure of text. The idea is that linguistic structures between the interacting biological entities may have common characteristics that can be exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Another crucial IE task is named entity disambiguation, which concerns grounding menti</context>
<context position="20726" citStr="Miwa et al., 2008" startWordPosition="3344" endWordPosition="3347">actic constituents. A path between 8Orthologues are genes/proteins in different species but have similar sequences. In this example it implies that Klp67A is a Drosophila protein but Kip3 is not. Parser Input Output C&amp;C POS-tagged GR ENJU POS-tagged PAS ENJU-Genia POS-tagged PAS Minipar Sentence-detected Minipar RASP Tokenised GR Stanford POS-tagged SD Stanford-Genia POS-tagged SD Table 1: Parsers and their input and output format a pair of nodes can be interpreted as a syntactic relation between sentence units, which was proved useful to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stan</context>
</contexts>
<marker>Miwa, Satre, Miyao, Ohta, Tsujii, 2008</marker>
<rawString>M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008. Combining multiple layers of syntactic information for protein-protein interaction extraction. In Proceedings of SMBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="21665" citStr="Miyao and Tsujii, 2008" startWordPosition="3500" endWordPosition="3503"> 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (Grover et al., 2003). To avoid the noise that can be introduced in the pre-proce</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Y. Miyao and J. Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>R Sastre</author>
<author>K Sagae</author>
<author>T Matsuzaki</author>
<author>J Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="2601" citStr="Miyao et al. (2008)" startWordPosition="377" endWordPosition="380">mmon characteristics that can be exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Another crucial IE task is named entity disambiguation, which concerns grounding mentions of named entities in text to unambiguous concepts as defined in some standard dictionary or database. For instance, given a search term Python, users may like to see the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to suc</context>
<context position="18250" citStr="Miyao et al., 2008" startWordPosition="2952" endWordPosition="2955">. We then assigned each relation a binary label: a relation is positive if the species ID inferred from the species word matches the goldstandard species annotation on the entity, and is negative otherwise. For example, for the sentence shown in Figure 1, where Drosophila is a species word, and Kip3 and Klp67A are proteins, relation (Kip3, Drosophila) is a negative instance and the Drosophila orthologue of Kip3 is Klp67A. ARG1 ARG1 ARG2 1516 pair (Klp67A, Drosophila) is a positive one.8 For each relation, a vector of features were extracted. We followed the PPI extraction method described in (Miyao et al., 2008), where two types of features were used for a SVM classifier. The first was bag-of-word features, i.e., the words before, between and after the pair of entities, where the words were lemmatised. We added an additional feature of the distance between the entity and the cue word. The other type was syntactic features obtained from parsers. For bagof-word features, a linear kernel was used, and for syntactic ones, a subset tree kernel (Moschitti, 2006) was adopted. The syntactic features were represented in a flat tree format. Figure 2 shows such a feature for the negative instance (Kip3, Drosoph</context>
</contexts>
<marker>Miyao, Sastre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Y. Miyao, R. Sastre, K. Sagae, T. Matsuzaki, and J. Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Morgan</author>
<author>L Hirschman</author>
</authors>
<title>Overview of BioCreAtIvE II gene normalisation.</title>
<date>2007</date>
<booktitle>In Proceedings of the BioCreAtIvE II Workshop,</booktitle>
<location>Madrid.</location>
<contexts>
<context position="4315" citStr="Morgan and Hirschman, 2007" startWordPosition="651" endWordPosition="654">the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount of training data (Morgan and Hirschman, 2007; Hakenberg et al., 2008). This paper proposes a new disambiguation method that, instead of classifying each individual occurrence of an entity, it classifies pair-wise relations between the entity mention in question and the “cue words” in its adjacent context, where each cue word is assumed to bear a semantic class. We then select the cue word that has a positive relation with the entity, and pass its semantic tag to it. While an individual entity mention may belong to a large number of semantic classes, a relation can only take one of two values: positive or negative, hence transforming a c</context>
</contexts>
<marker>Morgan, Hirschman, 2007</marker>
<rawString>A. A. Morgan and L. Hirschman. 2007. Overview of BioCreAtIvE II gene normalisation. In Proceedings of the BioCreAtIvE II Workshop, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Morgan</author>
<author>Z Lu</author>
<author>X Wang</author>
<author>A M Cohen</author>
<author>J Fluck</author>
<author>P Ruch</author>
<author>A Divoli</author>
<author>K Fundel</author>
<author>R Leaman</author>
<author>J Hakenberg</author>
<author>C Sun</author>
<author>H Liu</author>
<author>R Torres</author>
<author>M Krauthammer</author>
<author>W W Lau</author>
<author>H Liu</author>
<author>C Hsu</author>
<author>M Schuemie</author>
<author>K B Cohen</author>
<author>L Hirschman</author>
</authors>
<title>Overview of BioCreAtIvE II gene normalization. Genome Biology, 9(Suppl 2).</title>
<date>2008</date>
<contexts>
<context position="9797" citStr="Morgan et al., 2008" startWordPosition="1568" endWordPosition="1572">ical text mining. Chen et al. (2005) collected gene names from various source databases and calculated intra- and inter-species ambiguities. Overall, only 25 (0.02%) official symbols were ambiguous within the organisms. However, when official symbols from 21 organisms were combined, the ambiguity increased substantially to 21, 279 (14.2%) symbols. Hakenberg et al. (2008) showed that species disambiguation is one of the most important steps for term normalisation and identification, which concerns automatically associating mentions of biomedical entities in text to unique database identifiers (Morgan et al., 2008). Also, the task of extracting PPIs in the recent BioCreative Challenge II workshop (Hirschman et al., 2007) requires protein pairs to be recognised and normalised, which inevitably involves species disambiguation. More specifically, given a text, in which mentions of biomedical named entities are annotated, a species disambiguation system automatically assigns a species identifier, as in a standard database of model organisms, to every entity mention. The types of biomedical named entities concerned in this study are protein, gene, protein complex and mRNA/cDNA, and we used identifiers from t</context>
</contexts>
<marker>Morgan, Lu, Wang, Cohen, Fluck, Ruch, Divoli, Fundel, Leaman, Hakenberg, Sun, Liu, Torres, Krauthammer, Lau, Liu, Hsu, Schuemie, Cohen, Hirschman, 2008</marker>
<rawString>A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck, P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Hakenberg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W. Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and L. Hirschman. 2008. Overview of BioCreAtIvE II gene normalization. Genome Biology, 9(Suppl 2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2436" citStr="Moschitti, 2006" startWordPosition="351" endWordPosition="352">en adopted to analyse the syntactic and semantic structure of text. The idea is that linguistic structures between the interacting biological entities may have common characteristics that can be exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Another crucial IE task is named entity disambiguation, which concerns grounding mentions of named entities in text to unambiguous concepts as defined in some standard dictionary or database. For instance, given a search term Python, users </context>
<context position="18703" citStr="Moschitti, 2006" startWordPosition="3033" endWordPosition="3035">7A, Drosophila) is a positive one.8 For each relation, a vector of features were extracted. We followed the PPI extraction method described in (Miyao et al., 2008), where two types of features were used for a SVM classifier. The first was bag-of-word features, i.e., the words before, between and after the pair of entities, where the words were lemmatised. We added an additional feature of the distance between the entity and the cue word. The other type was syntactic features obtained from parsers. For bagof-word features, a linear kernel was used, and for syntactic ones, a subset tree kernel (Moschitti, 2006) was adopted. The syntactic features were represented in a flat tree format. Figure 2 shows such a feature for the negative instance (Kip3, Drosophila) from Figure 1. Note that all species words (e.g., Drosophila) were normalised to “SPECIESWORD”, and entities (e.g., Kip3) to “ENTITY”, which not only reduces the noise in the feature set, but also makes the model more species-generic. From the training dataset (see Section 5.1), 25, 413 relations were extracted, of which 63.3% were positive. (ENJU(noun arg1(SPECIESWORD orthologue)) (prep arg12(of orthologue)) (prep arg12(of ENTITY))) Figure 2: </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypothesis.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="34549" citStr="Noreen, 1989" startWordPosition="5608" endWordPosition="5609">set. On the other hand, the relation-classification approaches have a clear advantage over Maxent as measured by macro-averaged scores. As shown in Table 4, Rel-ENJU-Genia worked well on most of the species, displaying its good adaptability, while achieving comparable micro-averaged F1 to Maxent. Overall, Hbrd, which combines the strengths of relation classification and the Maxent classification model, obtained the highest points as measured by every metric. 5.2.3 Statistical Significance To see whether our methods significantly improved the baseline systems, we performed randomisation tests (Noreen, 1989; Yeh, 2000) on some of the results shown in Table 3. The intuition of randomisation test is as follows: when comparing two systems (e.g., A and B), we erase the labels “output of A” or “output of B” from all observations. The null hypothesis is that there is no difference between A and B, and thus any response produced by one of the systems could have as likely come from the other. We shuffle these re12The following 7 species occur most frequently in the training set: H. sapiens (43.25%), M. musculus (27.05%), R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis (3.56%), D. melanogaste</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer Intensive Methods for Testing Hypothesis. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="21560" citStr="Pollard and Sag, 1994" startWordPosition="3482" endWordPosition="3485"> some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when app</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Schwartz</author>
<author>M A Hearst</author>
</authors>
<title>Identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing.</booktitle>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>A. S. Schwartz and M. A. Hearst. 2003. Identifying abbreviation definitions in biomedical text. In Proceedings of the Pacific Symposium on Biocomputing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>ABNER: An open source tool for automatically tagging genes, proteins, and other entity names in text.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>14</issue>
<contexts>
<context position="10610" citStr="Settles, 2005" startWordPosition="1694" endWordPosition="1695">disambiguation. More specifically, given a text, in which mentions of biomedical named entities are annotated, a species disambiguation system automatically assigns a species identifier, as in a standard database of model organisms, to every entity mention. The types of biomedical named entities concerned in this study are protein, gene, protein complex and mRNA/cDNA, and we used identifiers from the NCBI Taxonomy of model organisms.2 The work focuses on species disambiguation and assumes that the entities are already identified. In practice, an automated named entity recogniser (e.g., ABNER (Settles, 2005)) should be used before applying the systems. 4 Approaches This section describes a number of approaches to species disambiguation, highlighting the relation classification method proposed in Section 2. 4.1 Heuristics Baselines The cue words for species are words denoting names of model organisms (e.g., mouse as in 1http://www.ncbi.nlm.nih.gov/RefSeq 2http://www.ncbi.nlm.nih.gov/sites/ entrez?db=taxonomy phrase “mouse p53”). Another clue is the presence of the species-indicating prefixes in gene and protein names. For instance, prefix ‘h’ in entity “hSos-1” suggests that it is a human protein.</context>
</contexts>
<marker>Settles, 2005</marker>
<rawString>B. Settles. 2005. ABNER: An open source tool for automatically tagging genes, proteins, and other entity names in text. Bioinformatics, 21(14):3191–3192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21585" citStr="Steedman, 2000" startWordPosition="3488" endWordPosition="3489">o the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performance. The preprocessing steps include sentence boundary detection, tokenisation and part-of-speech (POS) tagging, all of which can be tricky especially when applied to biomedical text (</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tateisi</author>
<author>A Yakushiji</author>
<author>T Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Syntax annotation for the GENIA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="21401" citStr="Tateisi et al., 2005" startWordPosition="3457" endWordPosition="3460">ed in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&amp;C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. Firstly, before parsing, the text needs to be linguistically preprocessed, and the quality of this process has a significant impact on parsers’ performa</context>
</contexts>
<marker>Tateisi, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syntax annotation for the GENIA corpus. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>C Grover</author>
</authors>
<title>Learning the species of biomedical named entities from annotated corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="11463" citStr="Wang and Grover, 2008" startWordPosition="1817" endWordPosition="1820"> cue words for species are words denoting names of model organisms (e.g., mouse as in 1http://www.ncbi.nlm.nih.gov/RefSeq 2http://www.ncbi.nlm.nih.gov/sites/ entrez?db=taxonomy phrase “mouse p53”). Another clue is the presence of the species-indicating prefixes in gene and protein names. For instance, prefix ‘h’ in entity “hSos-1” suggests that it is a human protein. Throughout this paper, we refer to such cue words (e.g., mouse, hSos-1) as “species words”. Note that a species “word” may contain multiple tokens (e.g., E. Coli). We encoded this knowledge in a rule-based species tagging system (Wang and Grover, 2008). The system takes a 2-step approach. First, it marks up species words in the document using a speciesword detection program,3 which searches every word in a dictionary of model organisms and assigns a species ID to the word if a match is found. The dictionary was built using the NCBI taxonomy4 and the UniProt controlled vocabulary of species,5 and in total it contains 420,224 species words for 324,157 species IDs. When species words are identified, we disambiguate an entity mention using one of the following rules: 1. previous species word: If the word preceding an entity is a species word, a</context>
</contexts>
<marker>Wang, Grover, 2008</marker>
<rawString>X. Wang and C. Grover. 2008. Learning the species of biomedical named entities from annotated corpora. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="15775" citStr="Yang and Pedersen, 1997" startWordPosition="2545" endWordPosition="2548"> of the entity (n = 5). • rightSpeciesWords The n species word forms to the right of the entity (n = 5). • firstLetter The first character of the entity itself (e.g., ‘h’ in hP53). • documentSpeciesIDs All species IDs that occur in the document in question. • useStopWords filter out function words. • useStopPattern filter out words consisting only of digits and punctuation characters. Feature selection was also carried out for the one-class classification experiments. We compared two feature selection methods that reportedly work well on the task of text classification: information gain (IG) (Yang and Pedersen, 1997) 6http://homepages.inf.ed.ac.uk/ s0450736/maxent_toolkit.html 7http://svmlight.joachims.org/ ARG1 ARG1 ARG2 Figure 1: Predicate argument structure (PAS). and Bi-Normal separation (BNS) (Forman, 2003). IG measures the decrease in entropy when the feature is given vs. absent, and is defined as: IG(Y |X) = H(Y ) − H(Y |X) where H(Y ) is the uncertainty about the value of Y (i.e., Y ’s entropy), and H(Y |X) is Y ’s conditional entropy given X. The BNS is defined as: |F−1(x) − F−1(y)|, where F−1 is the standard Normal distribution’s inverse cumulative probability function, namely, z-score; x is the</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="34561" citStr="Yeh, 2000" startWordPosition="5610" endWordPosition="5611">her hand, the relation-classification approaches have a clear advantage over Maxent as measured by macro-averaged scores. As shown in Table 4, Rel-ENJU-Genia worked well on most of the species, displaying its good adaptability, while achieving comparable micro-averaged F1 to Maxent. Overall, Hbrd, which combines the strengths of relation classification and the Maxent classification model, obtained the highest points as measured by every metric. 5.2.3 Statistical Significance To see whether our methods significantly improved the baseline systems, we performed randomisation tests (Noreen, 1989; Yeh, 2000) on some of the results shown in Table 3. The intuition of randomisation test is as follows: when comparing two systems (e.g., A and B), we erase the labels “output of A” or “output of B” from all observations. The null hypothesis is that there is no difference between A and B, and thus any response produced by one of the systems could have as likely come from the other. We shuffle these re12The following 7 species occur most frequently in the training set: H. sapiens (43.25%), M. musculus (27.05%), R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis (3.56%), D. melanogaster (3.33%) an</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>