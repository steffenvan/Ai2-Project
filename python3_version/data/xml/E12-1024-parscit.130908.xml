<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999617">
A Probabilistic Model of Syntactic and Semantic Acquisition from
Child-Directed Utterances and their Meanings
</title>
<author confidence="0.989565">
Tom Kwiatkowski* † Sharon Goldwater* Luke Zettlemoyer† Mark Steedman*
</author>
<email confidence="0.848751">
tomk@cs.washington.edu sgwater@inf.ed.ac.uk lsz@cs.washington.edu steedman@inf.ed.ac.uk
</email>
<affiliation confidence="0.8694575">
* ILCC, School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.822706">
Edinburgh, EH8 9AB, UK
</address>
<sectionHeader confidence="0.973014" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822090909091">
This paper presents an incremental prob-
abilistic learner that models the acquis-
tion of syntax and semantics from a cor-
pus of child-directed utterances paired with
possible representations of their meanings.
These meaning representations approxi-
mate the contextual input available to the
child; they do not specify the meanings of
individual words or syntactic derivations.
The learner then has to infer the meanings
and syntactic properties of the words in the
input along with a parsing model. We use
the CCG grammatical framework and train
a non-parametric Bayesian model of parse
structure with online variational Bayesian
expectation maximization. When tested on
utterances from the CHILDES corpus, our
learner outperforms a state-of-the-art se-
mantic parser. In addition, it models such
aspects of child acquisition as “fast map-
ping,” while also countering previous crit-
icisms of statistical syntactic learners.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9539728">
Children learn language by mapping the utter-
ances they hear onto what they believe those ut-
terances mean. The precise nature of the child’s
prelinguistic representation of meaning is not
known. We assume for present purposes that
it can be approximated by compositional logical
representations such as (1), where the meaning is
a logical expression that describes a relationship
have between the person you refers to and the
object another(x, cookie(x)):
</bodyText>
<equation confidence="0.5823175">
Utterance : you have another cookie (1)
Meaning: have(you, another(x, cookie(x)))
</equation>
<bodyText confidence="0.789089">
Most situations will support a number of plausi-
ble meanings, so the child has to learn in the face
</bodyText>
<affiliation confidence="0.970796">
†Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.575606">
Seattle, WA, 98195, USA
</address>
<bodyText confidence="0.97648">
of propositional uncertainty1, from a set of con-
textually afforded meaning candidates, as here:
</bodyText>
<equation confidence="0.95095875">
Utterance : you have another cookie
{ have(you, another(x, cookie(x)))
eat(you, your(x, cake(x)))
want(i, another(x, cookie(x)))
</equation>
<bodyText confidence="0.999698206896552">
The task is then to learn, from a sequence of such
(utterance, meaning-candidates) pairs, the correct
lexicon and parsing model. Here we present a
probabilistic account of this task with an empha-
sis on cognitive plausibility.
Our criteria for plausibility are that the learner
must not require any language-specific informa-
tion prior to learning and that the learning algo-
rithm must be strictly incremental: it sees each
training instance sequentially and exactly once.
We define a Bayesian model of parse structure
with Dirichlet process priors and train this on a
set of (utterance, meaning-candidates) pairs de-
rived from the CHILDES corpus (MacWhinney,
2000) using online variational Bayesian EM.
We evaluate the learnt grammar in three ways.
First, we test the accuracy of the trained model
in parsing unseen utterances onto gold standard
annotations of their meaning. We show that
it outperforms a state-of-the-art semantic parser
(Kwiatkowski et al., 2010) when run with similar
training conditions (i.e., neither system is given
the corpus based initialization originally used by
Kwiatkowski et al.). We then examine the learn-
ing curves of some individual words, showing that
the model can learn word meanings on the ba-
sis of a single exposure, similar to the fast map-
ping phenomenon observed in children (Carey
and Bartlett, 1978). Finally, we show that our
</bodyText>
<footnote confidence="0.97201">
1Similar to referential uncertainty but relating to propo-
sitions rather than referents.
</footnote>
<note confidence="0.676643">
Candidate
Meanings
</note>
<page confidence="0.961185">
234
</page>
<note confidence="0.9771735">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–244,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999616714285714">
learner captures the step-like learning curves for
word order regularities that Thornton and Tesan
(2007) claim children show. This result coun-
ters Thornton and Tesan’s criticism of statistical
grammar learners—that they tend to exhibit grad-
ual learning curves rather than the abrupt changes
in linguistic competence observed in children.
</bodyText>
<sectionHeader confidence="0.675155" genericHeader="introduction">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999325">
Models of syntactic acquisition, whether they
have addressed the task of learning both syn-
tax and semantics (Siskind, 1992; Villavicencio,
2002; Buttery, 2006) or syntax alone (Gibson
and Wexler, 1994; Sakas and Fodor, 2001; Yang,
2002) have aimed to learn a single, correct, deter-
ministic grammar. With the exception of Buttery
(2006) they also adopt the Principles and Param-
eters grammatical framework, which assumes de-
tailed knowledge of linguistic regularities2. Our
approach contrasts with all previous models in as-
suming a very general kind of linguistic knowl-
edge and a probabilistic grammar. Specifically,
we use the probabilistic Combinatory Categorial
Grammar (CCG) framework, and assume only
that the learner has access to a small set of general
combinatory schemata and a functional mapping
from semantic type to syntactic category. Further-
more, this paper is the first to evaluate a model
of child syntactic-semantic acquisition by parsing
unseen data.
Models of child word learning have focused
on semantics only, learning word meanings from
utterances paired with either sets of concept sym-
bols (Yu and Ballard, 2007; Frank et al., 2008; Fa-
zly et al., 2010) or a compositional meaning rep-
resentation of the type used here (Siskind, 1996).
The models of Alishahi and Stevenson (2008)
and Maurits et al. (2009) learn, as well as word-
meanings, orderings for verb-argument structures
but not the full parsing model that we learn here.
Semantic parser induction as addressed by
Zettlemoyer and Collins (2005, 2007, 2009), Kate
and Mooney (2007), Wong and Mooney (2006,
2007), Lu et al. (2008), Chen et al. (2010),
Kwiatkowski et al. (2010, 2011) and B¨orschinger
et al. (2011) has the same task definition as the
one addressed by this paper. However, the learn-
ing approaches presented in those previous pa-
</bodyText>
<footnote confidence="0.942157">
2This linguistic use of the term ”parameter” is distinct
from the statistical use found elsewhere in this paper.
</footnote>
<bodyText confidence="0.999843545454546">
pers are not designed to be cognitively plausible,
using batch training algorithms, multiple passes
over the data, and language specific initialisations
(lists of noun phrases and additional corpus statis-
tics), all of which we dispense with here. In
particular, our approach is closely related that of
Kwiatkowski et al. (2010) but, whereas that work
required careful initialisation and multiple passes
over the training data to learn a discriminative
parsing model, here we learn a generative parsing
model without either.
</bodyText>
<subsectionHeader confidence="0.998469">
1.2 Overview of the approach
</subsectionHeader>
<bodyText confidence="0.999129928571428">
Our approach takes, as input, a corpus of (ut-
terance, meaning-candidates) pairs {(sZ, {m}Z) :
i = 1, ... , N}, and learns a CCG lexicon A and
the probability of each production a —* b that
could be used in a parse. Together, these define
a probabilistic parser that can be used to find the
most probable meaning for any new sentence.
We learn both the lexicon and production prob-
abilities from allowable parses of the training
pairs. The set of allowable parses {t} for a sin-
gle (utterance, meaning-candidates) pair consists
of those parses that map the utterance onto one of
the meanings. This set is generated with the func-
tional mapping T :
</bodyText>
<equation confidence="0.995341">
{t} = T (s, m), (2)
</equation>
<bodyText confidence="0.999909071428571">
which is defined, following Kwiatkowski et al.
(2010), using only the CCG combinators and a
mapping from semantic type to syntactic category
(presented in in Section 4).
The CCG lexicon A is learnt by reading off
the lexical items used in all parses of all training
pairs. Production probabilities are learnt in con-
junction with A through the use of an incremen-
tal parameter estimation algorithm, online Varia-
tional Bayesian EM, as described in Section 5.
Before presenting the probabilistic model, the
mapping T, and the parameter training algorithm,
we first provide some background on the meaning
representations we use and on CCG.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="method">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999753">
2.1 Meaning Representations
</subsectionHeader>
<bodyText confidence="0.99971125">
We represent the meanings of utterances in first-
order predicate logic using the lambda-calculus.
An example logical expression (henceforth also
referred to as a lambda expression) is:
</bodyText>
<equation confidence="0.943019">
like(eve, mummy) (3)
</equation>
<page confidence="0.988587">
235
</page>
<bodyText confidence="0.999893">
which expresses a logical relationship like be-
tween the entity eve and the entity mummy. In
Section 6.1 we will see how logical expressions
like this are created for a set of child-directed ut-
terances (to use in training our model).
The lambda-calculus uses A operators to define
functions. These may be used to represent func-
tional meanings of utterances but they may also be
used as a ‘glue language’, to compose elements of
first order logical expressions. For example, the
function AxAy.like(y, x) can be combined with
the object mummy to give the phrasal mean-
ing Ay.like(y, mummy) through the lambda-
calculus operation of function application.
</bodyText>
<subsectionHeader confidence="0.988163">
2.2 CCG
</subsectionHeader>
<bodyText confidence="0.999137666666667">
Combinatory Categorial Grammar (CCG; Steed-
man 2000) is a strongly lexicalised linguistic for-
malism that tightly couples syntax and seman-
tics. Each CCG lexical item in the lexicon A is
a triple, written as word ` syntactic category :
logical expression. Examples are:
</bodyText>
<equation confidence="0.99755925">
You ` NP : you
read ` S\NP/NP : AxAy.read(y,x)
the ` NP/N : Af.the(x, f(x))
book ` N : Ax.book(x)
</equation>
<bodyText confidence="0.999078230769231">
A full CCG category X : h has syntactic cate-
gory X and logical expression h. Syntactic cat-
egories may be atomic (e.g., S or NP) or com-
plex (e.g., (S\NP)/NP). Slash operators in com-
plex categories define functions from the range on
the right of the slash to the result on the left in
much the same way as lambda operators do in the
lambda-calculus. The direction of the slash de-
fines the linear order of function and argument.
CCG uses a small set of combinatory rules to
concurrently build syntactic parses and semantic
representations. Two example combinatory rules
are forward (&gt;) and backward (&lt;) application:
</bodyText>
<equation confidence="0.95662">
X/Y : f Y : g ⇒ X : f(g) (&gt;)
Y : g X\Y : f ⇒ X : f(g) (&lt;)
</equation>
<bodyText confidence="0.988564">
Given the lexicon above, the phrase “You read the
book” can be parsed using these rules, as illus-
trated in Figure 1 (with additional notation dis-
cussed in the following section)..
CCG also includes combinatory rules of
forward (&gt; B) and backward (&lt; B) composition:
</bodyText>
<equation confidence="0.4237825">
X/Y : f Y/Z : g ⇒ X/Z : Ax.f(g(x)) (&gt; B)
Y \Z : g X\Y : f ⇒ X\Z : Ax.f(g(x)) (&lt; B)
</equation>
<sectionHeader confidence="0.959203" genericHeader="method">
3 Modelling Derivations
</sectionHeader>
<bodyText confidence="0.981052444444445">
The objective of our learning algorithm is to
learn the correct parameterisation of a probabilis-
tic model P(s, m, t) over (utterance, meaning,
derivation) triples. This model assigns a proba-
bility to each of the grammar productions a → b
used to build the derivation tree t. The probabil-
ity of any given CCG derivation t with sentence
s and semantics m is calculated as the product of
all of its production probabilities.
</bodyText>
<equation confidence="0.980435">
�P(s, m, t) � P(b|a) (4)
a→bEt
</equation>
<bodyText confidence="0.999920243243243">
For example, the derivation in Figure 1 contains
13 productions, and its probability is the product
of the 13 production probabilities. Grammar pro-
ductions may be either syntactic—used to build a
syntactic derivation tree, or lexical—used to gen-
erate logical expressions and words at the leaves
of this tree.
A syntactic production Ch → R expands a
head node Ch into a result R that is either an
ordered pair of syntactic parse nodes hCl, C,i
(for a binary production) or a single parse node
(for a unary production). Only two unary syn-
tactic productions are allowed in the grammar:
START → A to generate A as the top syntactic
node of a parse tree and A → [A]lex to indicate
that A is a leaf node in the syntactic derivation
and should be used to generate a logical expres-
sion and word. Syntactic derivations are built by
recursively applying syntactic productions to non-
leaf nodes in the derivation tree. Each syntactic
production Ch → R has conditional probability
P (R|Ch). There are 3 binary and 5 unary syntac-
tic productions in Figure 1.
Lexical productions have two forms. Logical
expressions are produced from leaf nodes in the
syntactic derivation tree Alex → m with condi-
tional probability P(m|Ale�). Words are then pro-
duced from these logical expressions with condi-
tional probability P(w|m). An example logical
production from Figure 1 is [NP]lex → you. An
example word production is you → You.
Every production a → b used in a parse tree t
is chosen from the set of productions that could
be used to expand a head node a. If there are a
finite K productions that could expand a then a
K-dimensional Multinomial distribution parame-
terised by Ba can be used to model the categorical
</bodyText>
<page confidence="0.990128">
236
</page>
<figure confidence="0.974859333333333">
START
Sdd
NP Sdd\NP
</figure>
<figureCaption confidence="0.9983995">
Figure 1: Derivation of sentence You read the
book with meaning read(you, the(x, book(x))).
</figureCaption>
<bodyText confidence="0.450877">
choice of production:
</bodyText>
<equation confidence="0.933063">
b — Multinomial(Ba) (5)
</equation>
<bodyText confidence="0.99996872">
However, before training a model of language ac-
quisition the dimensionality and contents of both
the syntactic grammar and lexicon are unknown.
In order to maintain a probability model with
cover over the countably infinite number of pos-
sible productions, we define a Dirichlet Process
(DP) prior for each possible production head a.
For the production head a, DP(αa, Ha) assigns
some probability mass to all possible production
targets {b} covered by the base distribution Ha.
It is possible to use the DP as an infinite prior
from which the parameter set of a finite dimen-
sional Multinomial may be drawn provided that
we can choose a suitable partition of {b}. When
calculating the probability of an (s, m, t) triple,
the choice of this partition is easy. For any given
production head a there is a finite set of usable
production targets {b1, ... , bk−1} in t. We create
a partition that includes one entry for each of these
along with a final entry {bk,... } that includes all
other ways in which a could be expanded in dif-
ferent contexts. Then, by applying the distribution
Ga drawn from the DP to this partition, we get a
parameter vector Ba that is equivalent to a draw
from a k dimensional Dirichlet distribution:
</bodyText>
<equation confidence="0.9919975">
Ga — DP(αa, Ha) (6)
Ba = (Ga(b1), ... , Ga(bk−1), Ga({bk, ... })
— Dir(αaH(b1), ... , αaHa(bk−1), (7)
αaHa({bk, ... }))
</equation>
<bodyText confidence="0.9332924">
Together, Equations 4-7 describe the joint distri-
bution P(X, S, B) over the observed training data
X = {(sZ, {m}Z) : i = 1, ... , N}, the latent vari-
ables S (containing the productions used in each
parse t) and the parsing parameters B.
</bodyText>
<sectionHeader confidence="0.989639" genericHeader="method">
4 Generating Parses
</sectionHeader>
<bodyText confidence="0.99998775">
The previous section defined a parameterisation
over parses assuming that the CCG lexicon A was
known. In practice A is empty prior to training
and must be populated with the lexical items from
parses t consistent with training pairs (s, {m}).
The set of allowed parses {t} is defined by the
function T from Equation 2. Here we review the
splitting procedure of Kwiatkowski et al. (2010)
that is used to generate CCG lexical items and de-
scribe how it is used by T to create a packed chart
representation of all parses {t} that are consistent
with s and at least one of the meaning represen-
tations in {m}. In this section we assume that s
is paired at each point with only a single meaning
m. Later we will show how T is used multiple
times to create the set of parses consistent with s
and a set of candidate meanings {m}.
The splitting procedure takes as input a CCG
category X:h, such as NP : a(x, cookie(x)), and
returns a set of category splits. Each category split
is a pair of CCG categories (Cl : ml, C, : mr) that
can be recombined to give X : h using one of the
CCG combinators in Section 2.2. The CCG cat-
egory splitting procedure has two parts: logical
splitting of the category semantics h; and syntac-
tic splitting of the syntactic category X. Each logi-
cal split of h is a pair of lambda expressions (f, g)
in the following set:
</bodyText>
<equation confidence="0.815378">
{(f, g)  |h = f(g) V h = Ax.f(g(x))}, (8)
</equation>
<bodyText confidence="0.8980194">
which means that f and g can be recombined us-
ing either function application or function com-
position to give the original lambda expression
h. An example split of the lambda expression
h = a(x, cookie(x)) is the pair
</bodyText>
<equation confidence="0.977658">
(Ay.a(x, y(x)), Ax.cookie(x)), (9)
</equation>
<bodyText confidence="0.999728285714286">
where Ay.a(x, y(x)) applied to Ax.cookie(x) re-
turns the original expression a(x, cookie(x)).
Syntactic splitting assigns linear order and syn-
tactic categories to the two lambda expressions f
and g. The initial syntactic category X is split by
a reversal of the CCG application combinators in
Section 2.2 if f and g can be recombined to give
</bodyText>
<figure confidence="0.99899275">
(Sdd\NP)/NP
[(Sd.l\NP)/NP]l_
Axay.read(y, x)
read
NP/N
[NP/N]l_
afax.the(x, f(x))
the
N
[N]lex
Ax.book(x)
book
[NP]lex
you
You
NP
</figure>
<page confidence="0.894052">
237
</page>
<subsectionHeader confidence="0.675499">
Syntactic Category Semantic Type Example Phrase
</subsectionHeader>
<bodyText confidence="0.8910104">
Sdcl hev, ti I took it ` Sdcl:Ae.took(i, it, e)
St t I&apos;m angry ` St:angry(i)
Swh he, hev, tii Who took it? ` Swh:AxAe.took(x, it, e)
Sq hev, ti Did you take it? ` Sq:Ae.Q(take(you, it, e))
N he, ti cookie ` N:Ax.cookie(x)
</bodyText>
<note confidence="0.665409">
NP e John ` NP:john
PP hev, ti on John ` PP:Ae.on(john,e)
</note>
<figureCaption confidence="0.944475">
Figure 2: Atomic Syntactic Categories.
</figureCaption>
<equation confidence="0.875182666666667">
h with function application:
{(X/Y : f Y : g), (10)
(Y : g : X\Y : f)|h = f(g)}
</equation>
<bodyText confidence="0.643865333333333">
or by a reversal of the CCG composition combi-
nators if f and g can be recombined to give h with
function composition:
</bodyText>
<equation confidence="0.995997">
{(X/Z : f Z/Y : g, (11)
(Z\Y : g : X\Z : f)|h = Ax.f(g(x))}
</equation>
<bodyText confidence="0.953124">
Unknown category names in the result of a
split (Y in (10) and Z in (11)) are labelled via a
functional mapping cat from semantic type T to
syntactic category:
</bodyText>
<equation confidence="0.998339666666667">
I Atomic(T) if T ∈ Figure 2
cat(T1)/cat(T2) if T = hT1,T2i
cat(T1)\cat(T2) if T = hT1, T2i
</equation>
<bodyText confidence="0.97138056">
which uses the Atomic function illustrated
in Figure 2 to map semantic-type to basic CCG
syntactic category. As an example, the logical
split in (9) supports two CCG category splits, one
for each of the CCG application rules.
(NP/N:Ay.a(x,y(x)), N:Ax.cookie(x)) (12)
(N:Ax.cookie(x), NP\N:Ay.a(x,y(x))) (13)
The parse generation algorithm T uses the func-
tion split to generate all CCG category pairs that
are an allowed split of an input category X:h:
{(Cl:ml, Cr:mr)} = split(X:h),
and then packs a chart representation of {t} in a
top-down fashion starting with a single cell entry
Cm : m for the top node shared by all parses {t}.
For the utterance and meaning in (1) the top parse
node, spanning the entire word-string, is
S:have(you, another(x, cookie(x))).
T cycles over all cell entries in increasingly small
spans and populates the chart with their splits. For
any cell entry X: h spanning more than one word
T generates a set of pairs representing the splits of
X:h. For each split (Cl:ml, Cr:mr) and every bi-
nary partition (wi:k, wk:j) of the word-span T cre-
ates two new cell entries in the chart: (Cl: ml)i:k
and (Cr:mr)k:j.
</bodyText>
<equation confidence="0.7035823">
Input :Sentence [w1, ... , wn], top node Cm:m
Output: Packed parse chart Ch containing {t}
Ch = [ [{}1,...,{}n]1, ... ,[{}1,...,{}n]n ]
Ch[1][n − 1] = Cm:m
for i = n,...,2; j = 1 ... (n − i) + 1 do
for X:h ∈ Ch[j][i] do
for (Cl:ml, Cr:mr) ∈ split(X:h) do
for k = 1,...,i − 1 do
Ch[j][k] ← Cl:ml
Ch[j + k][i − k] ← Cr :mr
</equation>
<figureCaption confidence="0.619656">
Algorithm 1: Generating {t} with T .
Algorithm 1 shows how the learner uses T to
</figureCaption>
<bodyText confidence="0.998259">
generate a packed chart representation of {t} in
the chart Ch. The function T massively overgen-
erates parses for any given natural language. The
probabilistic parsing model introduced in Sec-
tion 3 is used to choose the best parse from the
overgenerated set.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="method">
5 Training
</sectionHeader>
<subsectionHeader confidence="0.996643">
5.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99924125">
The probabilistic model of the grammar describes
a distribution over the observed training data X,
latent variables S, and parameters B. The goal of
training is to estimate the posterior distribution:
</bodyText>
<equation confidence="0.988676">
p(S,B|X) = p(S, X|B)p(B) (14)
p(X)
</equation>
<bodyText confidence="0.999872666666667">
which we do with online Variational Bayesian Ex-
pectation Maximisation (oVBEM; Sato (2001),
Hoffman et al. (2010)). oVBEM is an online
</bodyText>
<equation confidence="0.9576295">
cat(T) =
I
</equation>
<page confidence="0.972039">
238
</page>
<bodyText confidence="0.9998856">
Bayesian extension of the EM algorithm that
accumulates observation pseudocounts na→b for
each of the productions a → b in the grammar.
These pseudocounts define the posterior over pro-
duction probabilities as follows:
</bodyText>
<equation confidence="0.9903575">
(θa→b1, ... , θa→b{,,,... 1))  |X, S ∼ (15)
∞
Dir(αH(b1) + na→b1, ... , E αH(bj) + na→bj)
j=k
</equation>
<bodyText confidence="0.995353222222222">
These pseudocounts are computed in two steps:
oVBE-step For the training pair (si, {m}i)
which supports the set of parses {t}, the expec-
tation E{t}[a → b] of each production a → b is
calculated by creating a packed chart representa-
tion of {t} and running the inside-outside algo-
rithm. This is similar to the E-step in standard
EM apart from the fact that each production is
scored with the current expectation of its parame-
</bodyText>
<table confidence="0.812493">
ter weight �θi−1
a→b, where:
Input :Corpus D = {(si, {m}i)|i = 1, ... , N},
Function T, Semantics to syntactic cate-
gory mapping cat, function lex to read
lexical items off derivations.
Output: Lexicon A, Pseudocounts {na→b}.
</table>
<equation confidence="0.953241230769231">
A = {}, {t} = {}
for i = 1,...,N do
{t}i = {}
for m0 ∈ {m}i do
C,,, = cat(m0)
{t}0 = T (si, C&amp;quot;&apos; :m0)
{t}i = {t}i ∪ {t}0, {t} = {t} ∪ {t}0
A = A ∪ lex ({t}0)
for a → b ∈ {t} do
nia→b = ni−1 a→b + ηi(N × E{t};[a → b] −
ni−1
a→b)
Algorithm 2: Learning A and {na→b}
</equation>
<bodyText confidence="0.992476666666667">
the parameter update step cycles over all produc-
tions in {t} it is not neccessary to store {t}, just
the set of productions that it uses.
</bodyText>
<figure confidence="0.8501714">
Bi−1 a→b = eΨ(αaHa(a→b)+ni�. 6 Experimental Setup
a�b)
( &apos;Jb,lαaHa(a→b&apos;)+ni�.
a-V (16)
e 6.1 Data
</figure>
<bodyText confidence="0.86481475">
and IF is the digamma function (Beal, 2003).
oVBM-step The expectations from the oVBE
step are used to update the pseudocounts in Equa-
tion 15 as follows,
</bodyText>
<equation confidence="0.99769525">
nia→b = ni−1
a→b + ηi(N × E{t}[a → b] − ni−1
a→b)
(17)
</equation>
<bodyText confidence="0.999821">
where ηi is the learning rate and N is the size of
the dataset.
</bodyText>
<subsectionHeader confidence="0.999308">
5.2 The Training Algorithm
</subsectionHeader>
<bodyText confidence="0.99993915625">
Now the training algorithm used to learn the lex-
icon A and pseudocounts {na→b} can be defined.
The algorithm, shown in Algorithm 2, passes over
the training data only once and one training in-
stance at a time. For each (si, {m}i) it uses the
function T |{m}i |times to generate a set of con-
sistent parses {t}0. The lexicon is populated by
using the lex function to read all of the lexical
items off from the derivations in each {t}0. In
the parameter update step, the training algorithm
updates the pseudocounts associated with each of
the productions a → b that have ever been seen
during training according to Equation (17).
Only non-zero pseudocounts are stored in our
model. The count vector is expanded with a new
entry every time a new production is used. While
The Eve corpus, collected by Brown (1973), con-
tains 14,124 English utterances spoken to a sin-
gle child between the ages of 18 and 27 months.
These have been hand annotated by Sagae et al.
(2004) with labelled syntactic dependency graphs.
An example annotation is shown in Figure 3.
While these annotations are designed to rep-
resent syntactic information, the parent-child re-
lationships in the parse can also be viewed as a
proxy for the predicate-argument structure of the
semantics. We developed a template based de-
terministic procedure for mapping this predicate-
argument structure onto logical expressions of the
type discussed in Section 2.1. For example, the
dependency graph in Figure 3 is automatically
transformed into the logical expression
</bodyText>
<construct confidence="0.367416">
λe.have(you,another(y, cookie(y)), e) (18)
∧ on(the(z, table(z)), e),
</construct>
<bodyText confidence="0.999912142857143">
where e is a Davidsonian event variable used to
deal with adverbial and prepositional attachments.
The deterministic mapping to logical expressions
uses 19 templates, three of which are used in this
example: one for the verb and its arguments, one
for the prepositional attachment and one (used
twice) for the quantifier-noun constructions.
</bodyText>
<page confidence="0.994404">
239
</page>
<figure confidence="0.780413">
SUBJ ROOT DET OBJ JCT DET POBJ
pro|you v|have qn|another n|cookie prep|on det|the n|table
You have another cookie on the table
</figure>
<figureCaption confidence="0.999361">
Figure 3: Syntactic dependency graph from Eve corpus.
</figureCaption>
<bodyText confidence="0.999919652173913">
This mapping from graph to logical expression
makes use of a predefined dictionary of allowed,
typed, logical constants. The mapping is success-
ful for 31% of the child-directed utterances in the
Eve corpus3. The remaining data is mostly ac-
counted for by one-word utterances that have no
straightforward interpretation in our typed logi-
cal language (e.g. what; okay; alright; no; yeah;
hmm; yes; uhhuh; mhm; thankyou), missing ver-
bal arguments that cannot be properly guessed
from the context (largely in imperative sentences
such as drink the water), and complex noun con-
structions that are hard to match with a small set
of templates (e.g. as top to a jar). We also re-
move the small number of utterances containing
more than 10 words for reasons of computational
efficiency (see discussion in Section 8).
Following Alishahi and Stevenson (2010), we
generate a context set {mli for each utterance si
by pairing that utterance with its correct logical
expression along with the logical expressions of
the preceding and following (|{mli|−1)/2 utter-
ances.
</bodyText>
<subsectionHeader confidence="0.999961">
6.2 Base Distributions and Learning Rate
</subsectionHeader>
<bodyText confidence="0.9980488125">
Each of the production heads a in the grammar
requires a base distribution Ha and concentration
parameter αa. For word-productions the base dis-
tribution is a geometric distribution over character
strings and spaces. For syntactic-productions the
base distribution is defined in terms of the new
category to be named by cat and the probability
of splitting the rule by reversing either the appli-
cation or composition combinators.
Semantic-productions’ base distributions are
defined by a probabilistic branching process con-
ditioned on the type of the syntactic category.
This distribution prefers less complex logical ex-
pressions. All concentration parameters are set to
1.0. The learning rate for parameter updates is
,qi = (0.8 + i)−0.5.
</bodyText>
<footnote confidence="0.950329">
3Data available at www.tomkwiat.com/resources.html
</footnote>
<figureCaption confidence="0.9876035">
Figure 4: Meaning Prediction: Train on files 1, ... , n
test on file n + 1.
</figureCaption>
<sectionHeader confidence="0.999249" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999839">
7.1 Parsing Unseen Sentences
</subsectionHeader>
<bodyText confidence="0.999912588235294">
We test the parsing model that is learnt by training
on the first i files of the longitudinally ordered Eve
corpus and testing on file i + 1, for i = 1...19.
For each utterance s&apos; in the test file we use the
parsing model to predict a meaning m* and com-
pare this to the target meaning m&apos;. We report the
proportion of utterances for which the prediction
m* is returned correctly both with and without
word-meaning guessing. When a word has never
been seen at training time our parser has the abil-
ity to ‘guess’ a typed logical meaning with place-
holders for constant and predicate names.
For comparison we use the UBL semantic
parser of Kwiatkowski et al. (2010) trained in
a similar setting—i.e., with no language specific
initialisation4. Figure 4 shows accuracy for our
approach with and without guessing, for UBL
</bodyText>
<footnote confidence="0.998477833333333">
4Kwiatkowski et al. (2010) initialise lexical weights in
their learning algorithm using corpus-wide alignment statis-
tics across words and meaning elements. Instead we run
UBL with small positive weight for all lexical items. When
run with Giza++ parameter initialisations, UBL10 achieves
48.1% across folds compared to 49.2% for our approach.
</footnote>
<figure confidence="0.998408466666667">
Accuracy
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.00.0 0.2 0.4 0.6 0.8 1.0
Proportion of Data Seen
Our Approach
Our Approach + Guess
UBL1
UBL10
</figure>
<page confidence="0.987601">
240
</page>
<bodyText confidence="0.9993368">
when run over the training data once (UBL1) and
for UBL when run over the training data 10 times
(UBL10) as in Kwiatkowski et al. (2010). Each
of the points represents accuracy on one of the
19 test files. All of these results are from parsers
trained on utterances paired with a single candi-
date meaning. The lines of best fit show the up-
ward trend in parser performance over time.
Despite only seeing each training instance
once, our approach, due to its broader lexi-
cal search strategy, outperforms both versions of
UBL which performs a greedy search in the space
of lexicons and requires initialisation with co-
occurence statistics between words and logical
constants to guide this search. These statistics are
not justified in a model of language acquisition
and so they are not used here. The low perfor-
mance of all systems is due largely to the sparsity
of the data with 32.9% of all sentences containing
a previously unseen word.
</bodyText>
<subsectionHeader confidence="0.999445">
7.2 Word Learning
</subsectionHeader>
<bodyText confidence="0.99984575">
Due to the sparsity of the data, the training algo-
rithm needs to be able to learn word-meanings on
the basis of very few exposures. This is also a de-
sirable feature from the perspective of modelling
language acquisition as Carey and Bartlett (1978)
have shown that children have the ability to learn
word meanings on the basis of one, or very few,
exposures through the process of fast mapping.
</bodyText>
<figure confidence="0.998282117647059">
1 Meaning 3 Meanings
1.0
0.8
0.6
0.4
0.2
0.0
0 500 1000 1500 2000 0 500 1000 1500 2000
5 Meanings 7 Meanings
1.0
0.8
0.6
0.4
0.2
0.0
0 500 1000 1500 2000 0 500 1000 1500 2000
Number of Utterances Number of Utterances
</figure>
<figureCaption confidence="0.999917">
Figure 5: Learning quantifiers with frequency f.
</figureCaption>
<bodyText confidence="0.99979452173913">
Figure 5 shows the posterior probability of the
correct meanings for the quantifiers ‘a’, ‘another’
and ‘any’ over the course of training with 1, 3,
5 and 7 candidate meanings for each utterance5.
These three words are all of the same class but
have very different frequencies in the training
subset shown (168, 10 and 2 respectively). In all
training settings, the word ‘a’ is learnt gradually
from many observations but the rarer words ‘an-
other’ and ‘any’ are learnt (when they are learnt)
through large updates to the posterior on the ba-
sis of few observations. These large updates re-
sult from a syntactic bootstrapping effect (Gleit-
man, 1990). When the model has great confidence
about the derivation in which an unseen lexical
item occurs, the pseudocounts for that lexical item
get a large update under Equation 17. This large
update has a greater effect on rare words which
are associated with small amounts of probability
mass than it does on common ones that have al-
ready accumulated large pseudocounts. The fast
learning of rare words later in learning correlates
with observations of word learning in children.
</bodyText>
<subsectionHeader confidence="0.999482">
7.3 Word Order Learning
</subsectionHeader>
<bodyText confidence="0.948342083333333">
Figure 6 shows the posterior probability of the
correct SVO word order learnt from increasing
amounts of training data. This is calculated by
summing over all lexical items containing transi-
tive verb semantics and sampling in the space of
parse trees that could have generated them. With
no propositional uncertainty in the training data
the correct word order is learnt very quickly and
stabilises. As the amount of propositional uncer-
tainty increases, the rate at which this rule is learnt
decreases. However, even in the face of ambigu-
ous training data, the model can learn the cor-
rect word-order rule. The distribution over word
orders also exhibits initial uncertainty, followed
by a sharp convergence to the correct analysis.
This ability to learn syntactic regularities abruptly
means that our system is not subject to the crit-
icisms that Thornton and Tesan (2007) levelled
at statistical models of language acquisition—that
their learning rates are too gradual.
5The term ‘fast mapping’ is generally used to refer to
noun learning. We chose to examine quantifier learning here
as there is a greater variation in quantifier frequencies. Fast
mapping of nouns is also achieved.
</bodyText>
<equation confidence="0.995405">
f = 168 a — Af.a(x, f(x))
f = 10 another — Af.another(x, f(x))
f = 2 any — Af.any(x, f(x))
P(mlw)
P(mlw)
</equation>
<page confidence="0.996494">
241
</page>
<figureCaption confidence="0.998431">
Figure 6: Learning SVO word order.
</figureCaption>
<sectionHeader confidence="0.99748" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999991610389611">
We have presented an incremental model of lan-
guage acquisition that learns a probabilistic CCG
grammar from utterances paired with one or
more potential meanings. The model assumes
no language-specific knowledge, but does assume
that the learner has access to language-universal
correspondences between syntactic and semantic
types, as well as a Bayesian prior encouraging
grammars with heavy reuse of existing rules and
lexical items. We have shown that this model
not only outperforms a state-of-the-art semantic
parser, but also exhibits learning curves similar
to children’s: lexical items can be acquired on a
single exposure and word order is learnt suddenly
rather than gradually.
Although we use a Bayesian model, our ap-
proach is different from many of the Bayesian
models proposed in cognitive science and lan-
guage acquisition (Xu and Tenenbaum, 2007;
Goldwater et al., 2009; Frank et al., 2009; Grif-
fiths and Tenenbaum, 2006; Griffiths, 2005; Per-
fors et al., 2011). These models are intended
as ideal observer analyses, demonstrating what
would be learned by a probabilistically optimal
learner. Our learner uses a more cognitively plau-
sible but approximate online learning algorithm.
In this way, it is similar to other cognitively plau-
sible approximate Bayesian learners (Pearl et al.,
2010; Sanborn et al., 2010; Shi et al., 2010).
Of course, despite the incremental nature of our
learning algorithm, there are still many aspects
that could be criticized as cognitively implausi-
ble. In particular, it generates all parses consistent
with each training instance, which can be both
memory- and processor-intensive. It is unlikely
that children do this once they have learnt at least
some of the target language. In future, we plan
to investigate more efficient parameter estimation
methods. One possibility would be an approxi-
mate oVBEM algorithm in which the expectations
in Equation 17 are calculated according to a high
probability subset of the parses {t}. Another op-
tion would be particle filtering, which has been
investigated as a cognitively plausible method for
approximate Bayesian inference (Shi et al., 2010;
Levy et al., 2009; Sanborn et al., 2010).
As a crude approximation to the context in
which an utterance is heard, the logical represen-
tations of meaning that we present to the learner
are also open to criticism. However, Steedman
(2002) argues that children do have access to
structured meaning representations from a much
older apparatus used for planning actions and we
wish to eventually ground these in sensory input.
Despite the limitations listed above, our ap-
proach makes several important contributions to
the computational study of language acquisition.
It is the first model to learn syntax and seman-
tics concurrently; previous systems (Villavicen-
cio, 2002; Buttery, 2006) learnt categorial gram-
mars from sentences where all word meanings
were known. Our model is also the first to be
evaluated by parsing sentences onto their mean-
ings, in contrast to the work mentioned above and
that of Gibson and Wexler (1994), Siskind (1992)
Sakas and Fodor (2001), and Yang (2002). These
all evaluate their learners on the basis of a small
number of predefined syntactic parameters.
Finally, our work addresses a misunderstand-
ing about statistical learners—that their learn-
ing curves must be gradual (Thornton and Tesan,
2007). By demonstrating sudden learning of word
order and fast mapping, our model shows that sta-
tistical learners can account for sudden changes in
children’s grammars. In future, we hope to extend
these results by examining other learning behav-
iors and testing the model on other languages.
</bodyText>
<sectionHeader confidence="0.997285" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99456325">
We thank Mark Johnson for suggesting an analy-
sis of learning rates. This work was funded by the
ERC Advanced Fellowship 24952 GramPlus and
EU IP grant EC-FP7-270273 Xperience.
</bodyText>
<figure confidence="0.999269444444444">
1 Meaning
3 Meanings
0.00 500 1000 1500 2000
5 Meanings
0.00 500 1000 1500 2000
Number of Utterances
0 500 1000 1500 2000
7 Meanings
0 500 1000 1500 2000
Number of Utterances
P(word order) P(word order)
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
vso
svo
ovs
sov
vos
osv
</figure>
<page confidence="0.988816">
242
</page>
<sectionHeader confidence="0.995272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.933498221052632">
Alishahi and Stevenson, S. (2008). A computa-
tional model for early argument structure ac-
quisition. Cognitive Science, 32:5:789–834.
Alishahi, A. and Stevenson, S. (2010). Learning
general properties of semantic roles from usage
data: a computational model. Language and
Cognitive Processes, 25:1.
Beal, M. J. (2003). Variational algorithms for ap-
proximate Bayesian inference. Technical re-
port, Gatsby Institute, UCL.
B¨orschinger, B., Jones, B. K., and Johnson, M.
(2011). Reducing grounded learning tasks
to grammatical inference. In Proceedings of
the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1416–
1425, Edinburgh, Scotland, UK. Association
for Computational Linguistics.
Brown, R. (1973). A First Language: the Early
Stages. Harvard University Press, Cambridge
MA.
Buttery, P. J. (2006). Computational models for
first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Carey, S. and Bartlett, E. (1978). Acquring a sin-
gle new word. Papers and Reports on Child
Language Development, 15.
Chen, D. L., Kim, J., and Mooney, R. J. (2010).
Training a multilingual sportscaster: Using per-
ceptual context to learn language. J. Artif. In-
tell. Res. (JAIR), 37:397–435.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A probabilistic computational model of cross-
situational word learning. Cognitive Science,
34(6):1017–1063.
Frank, M., Goodman, S., and Tenenbaum, J.
(2009). Using speakers referential intentions
to model early cross-situational word learning.
Psychological Science, 20(5):578–585.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2008). A bayesian framework for cross-
situational word-learning. Advances in Neural
Information Processing Systems 20.
Gibson, E. and Wexler, K. (1994). Triggers. Lin-
guistic Inquiry, 25:355–407.
Gleitman, L. (1990). The structural sources of
verb meanings. Language Acquisition, 1:1–55.
Goldwater, S., Griffiths, T. L., and Johnson, M.
(2009). A Bayesian framework for word seg-
mentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Griffiths, T. L., . T. J. B. (2005). Structure and
strength in causal induction. Cognitive Psy-
chology, 51:354–384.
Griffiths, T. L. and Tenenbaum, J. B. (2006). Op-
timal predictions in everyday cognition. Psy-
chological Science.
Hoffman, M., Blei, D. M., and Bach, F. (2010).
Online learning for latent dirichlet allocation.
In NIPS.
Kate, R. J. and Mooney, R. J. (2007). Learning
language semantics from ambiguous supervi-
sion. In Proceedings of the 22nd Conference
on Artificial Intelligence (AAAI-07).
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing proba-
bilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the
Conference on Emperical Methods in Natural
Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical general-
ization in ccg grammar induction for semantic
parsing. In Proceedings of the Conference on
Emperical Methods in Natural Language Pro-
cessing.
Levy, R., Reali, F., and Griffiths, T. (2009). Mod-
eling the effects of memory on human online
sentence processing with particle filters. In Ad-
vances in Neural Information Processing Sys-
tems 21.
Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer,
L. S. (2008). A generative model for parsing
natural language to meaning representations. In
Proceedings of The Conference on Empirical
Methods in Natural Language Processing.
MacWhinney, B. (2000). The CHILDES project:
tools for analyzing talk. Lawrence Erlbaum,
Mahwah, NJ u.a. EN.
Maurits, L., Perfors, A., and Navarro, D. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31th Annual Con-
ference of the Cognitive Science Society.
Pearl, L., Goldwater, S., and Steyvers, M. (2010).
How ideal are we? Incorporating human limi-
</reference>
<page confidence="0.983076">
243
</page>
<reference confidence="0.939872517647058">
tations into Bayesian models of word segmen-
tation. pages 315–326, Somerville, MA. Cas-
cadilla Press.
Perfors, A., Tenenbaum, J. B., and Regier, T.
(2011). The learnability of abstract syntactic
principles. Cognition, 118(3):306 – 338.
Sagae, K., MacWhinney, B., and Lavie, A.
(2004). Adding syntactic annotations to tran-
scripts of parent-child dialogs. In Proceed-
ings of the 4th International Conference on
Language Resources and Evaluation. Lisbon,
LREC.
Sakas, W. and Fodor, J. D. (2001). The struc-
tural triggers learner. In Bertolo, S., editor,
Language Acquisition and Learnability, pages
172–233. Cambridge University Press, Cam-
bridge.
Sanborn, A. N., Griffiths, T. L., and Navarro,
D. J. (2010). Rational approximations to ratio-
nal models: Alternative algorithms for category
learning. Psychological Review.
Sato, M. (2001). Online model selection based
on the variational bayes. Neural Computation,
13(7):1649–1681.
Shi, L., Griffiths, T. L., Feldman, N. H., and San-
born, A. N. (2010). Exemplar models as a
mechanism for performing bayesian inference.
Psychonomic Bulletin &amp; Review, 17(4):443–
464.
Siskind, J. M. (1992). Naive Physics, Event Per-
ception, Lexical Semantics, and Language Ac-
quisition. PhD thesis, Massachusetts Institute
of Technology.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):1–
38.
Steedman, M. (2000). The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, M. (2002). Plans, affordances, and
combinatory grammar. Linguistics and Philos-
ophy, 25.
Thornton, R. and Tesan, G. (2007). Categori-
cal acquisition: Parameter setting in universal
grammar. Biolinguistics, 1.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial gram-
mar. Technical Report UCAM-CL-TR-533,
University of Cambridge, Computer Labora-
tory.
Wong, Y. W. and Mooney, R. (2006). Learning for
semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language
Technology Conference of the NAACL.
Wong, Y. W. and Mooney, R. (2007). Learn-
ing synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of
the Association for Computational Linguistics.
Xu, F. and Tenenbaum, J. B. (2007). Word learn-
ing as Bayesian inference. Psychological Re-
view, 114:245–272.
Yang, C. (2002). Knowledge and Learning in Nat-
ural Language. Oxford University Press, Ox-
ford.
Yu, C. and Ballard, D. H. (2007). A unified model
of early word learning: Integrating statisti-
cal and social cues. Neurocomputing, 70(13-
15):2149 – 2165.
Zettlemoyer, L. S. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence.
Zettlemoyer, L. S. and Collins, M. (2007). Online
learning of relaxed CCG grammars for pars-
ing to logical form. In Proc. of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning.
Zettlemoyer, L. S. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of The
Joint Conference of the Association for Com-
putational Linguistics and International Joint
Conference on Natural Language Processing.
</reference>
<page confidence="0.998412">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.338006">
<title confidence="0.8157115">A Probabilistic Model of Syntactic and Semantic Acquisition Child-Directed Utterances and their Meanings</title>
<author confidence="0.381507">tomkcs washington edu sgwaterinf ed ac uk lszcs washington edu steedmaninf ed ac uk</author>
<affiliation confidence="0.9998295">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.998381">Edinburgh, EH8 9AB, UK</address>
<abstract confidence="0.999217">This paper presents an incremental probabilistic learner that models the acquistion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings. These meaning representations approximate the contextual input available to the child; they do not specify the meanings of individual words or syntactic derivations. The learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model. We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian expectation maximization. When tested on utterances from the CHILDES corpus, our learner outperforms a state-of-the-art semantic parser. In addition, it models such aspects of child acquisition as “fast mapping,” while also countering previous criticisms of statistical syntactic learners.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>A computational model for early argument structure acquisition.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<pages>32--5</pages>
<contexts>
<context position="5567" citStr="Alishahi and Stevenson (2008)" startWordPosition="832" endWordPosition="835">r (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use </context>
</contexts>
<marker>Alishahi, Stevenson, 2008</marker>
<rawString>Alishahi and Stevenson, S. (2008). A computational model for early argument structure acquisition. Cognitive Science, 32:5:789–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>Learning general properties of semantic roles from usage data: a computational model. Language and Cognitive Processes,</title>
<date>2010</date>
<pages>25--1</pages>
<contexts>
<context position="24186" citStr="Alishahi and Stevenson (2010)" startWordPosition="4108" endWordPosition="4111">ining data is mostly accounted for by one-word utterances that have no straightforward interpretation in our typed logical language (e.g. what; okay; alright; no; yeah; hmm; yes; uhhuh; mhm; thankyou), missing verbal arguments that cannot be properly guessed from the context (largely in imperative sentences such as drink the water), and complex noun constructions that are hard to match with a small set of templates (e.g. as top to a jar). We also remove the small number of utterances containing more than 10 words for reasons of computational efficiency (see discussion in Section 8). Following Alishahi and Stevenson (2010), we generate a context set {mli for each utterance si by pairing that utterance with its correct logical expression along with the logical expressions of the preceding and following (|{mli|−1)/2 utterances. 6.2 Base Distributions and Learning Rate Each of the production heads a in the grammar requires a base distribution Ha and concentration parameter αa. For word-productions the base distribution is a geometric distribution over character strings and spaces. For syntactic-productions the base distribution is defined in terms of the new category to be named by cat and the probability of split</context>
</contexts>
<marker>Alishahi, Stevenson, 2010</marker>
<rawString>Alishahi, A. and Stevenson, S. (2010). Learning general properties of semantic roles from usage data: a computational model. Language and Cognitive Processes, 25:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Beal</author>
</authors>
<title>Variational algorithms for approximate Bayesian inference.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Gatsby Institute, UCL.</institution>
<contexts>
<context position="20972" citStr="Beal, 2003" startWordPosition="3575" endWordPosition="3576">exical items off derivations. Output: Lexicon A, Pseudocounts {na→b}. A = {}, {t} = {} for i = 1,...,N do {t}i = {} for m0 ∈ {m}i do C,,, = cat(m0) {t}0 = T (si, C&amp;quot;&apos; :m0) {t}i = {t}i ∪ {t}0, {t} = {t} ∪ {t}0 A = A ∪ lex ({t}0) for a → b ∈ {t} do nia→b = ni−1 a→b + ηi(N × E{t};[a → b] − ni−1 a→b) Algorithm 2: Learning A and {na→b} the parameter update step cycles over all productions in {t} it is not neccessary to store {t}, just the set of productions that it uses. Bi−1 a→b = eΨ(αaHa(a→b)+ni�. 6 Experimental Setup a�b) ( &apos;Jb,lαaHa(a→b&apos;)+ni�. a-V (16) e 6.1 Data and IF is the digamma function (Beal, 2003). oVBM-step The expectations from the oVBE step are used to update the pseudocounts in Equation 15 as follows, nia→b = ni−1 a→b + ηi(N × E{t}[a → b] − ni−1 a→b) (17) where ηi is the learning rate and N is the size of the dataset. 5.2 The Training Algorithm Now the training algorithm used to learn the lexicon A and pseudocounts {na→b} can be defined. The algorithm, shown in Algorithm 2, passes over the training data only once and one training instance at a time. For each (si, {m}i) it uses the function T |{m}i |times to generate a set of consistent parses {t}0. The lexicon is populated by using</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Beal, M. J. (2003). Variational algorithms for approximate Bayesian inference. Technical report, Gatsby Institute, UCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B¨orschinger</author>
<author>B K Jones</author>
<author>M Johnson</author>
</authors>
<title>Reducing grounded learning tasks to grammatical inference.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1416--1425</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.</location>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>B¨orschinger, B., Jones, B. K., and Johnson, M. (2011). Reducing grounded learning tasks to grammatical inference. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416– 1425, Edinburgh, Scotland, UK. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
</authors>
<title>A First Language: the Early Stages.</title>
<date>1973</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="22037" citStr="Brown (1973)" startWordPosition="3770" endWordPosition="3771">e at a time. For each (si, {m}i) it uses the function T |{m}i |times to generate a set of consistent parses {t}0. The lexicon is populated by using the lex function to read all of the lexical items off from the derivations in each {t}0. In the parameter update step, the training algorithm updates the pseudocounts associated with each of the productions a → b that have ever been seen during training according to Equation (17). Only non-zero pseudocounts are stored in our model. The count vector is expanded with a new entry every time a new production is used. While The Eve corpus, collected by Brown (1973), contains 14,124 English utterances spoken to a single child between the ages of 18 and 27 months. These have been hand annotated by Sagae et al. (2004) with labelled syntactic dependency graphs. An example annotation is shown in Figure 3. While these annotations are designed to represent syntactic information, the parent-child relationships in the parse can also be viewed as a proxy for the predicate-argument structure of the semantics. We developed a template based deterministic procedure for mapping this predicateargument structure onto logical expressions of the type discussed in Section </context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Brown, R. (1973). A First Language: the Early Stages. Harvard University Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Buttery</author>
</authors>
<title>Computational models for first language acquisition.</title>
<date>2006</date>
<tech>Technical Report UCAM-CL-TR-675,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="4429" citStr="Buttery, 2006" startWordPosition="654" endWordPosition="655">ges 234–244, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general co</context>
<context position="33560" citStr="Buttery, 2006" startWordPosition="5652" endWordPosition="5653">o the context in which an utterance is heard, the logical representations of meaning that we present to the learner are also open to criticism. However, Steedman (2002) argues that children do have access to structured meaning representations from a much older apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast</context>
</contexts>
<marker>Buttery, 2006</marker>
<rawString>Buttery, P. J. (2006). Computational models for first language acquisition. Technical Report UCAM-CL-TR-675, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carey</author>
<author>E Bartlett</author>
</authors>
<title>Acquring a single new word.</title>
<date>1978</date>
<journal>Papers and Reports on Child Language Development,</journal>
<volume>15</volume>
<contexts>
<context position="3565" citStr="Carey and Bartlett, 1978" startWordPosition="528" endWordPosition="531">he learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1Similar to referential uncertainty but relating to propositions rather than referents. Candidate Meanings 234 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–244, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learni</context>
<context position="27846" citStr="Carey and Bartlett (1978)" startWordPosition="4719" endWordPosition="4722"> and requires initialisation with cooccurence statistics between words and logical constants to guide this search. These statistics are not justified in a model of language acquisition and so they are not used here. The low performance of all systems is due largely to the sparsity of the data with 32.9% of all sentences containing a previously unseen word. 7.2 Word Learning Due to the sparsity of the data, the training algorithm needs to be able to learn word-meanings on the basis of very few exposures. This is also a desirable feature from the perspective of modelling language acquisition as Carey and Bartlett (1978) have shown that children have the ability to learn word meanings on the basis of one, or very few, exposures through the process of fast mapping. 1 Meaning 3 Meanings 1.0 0.8 0.6 0.4 0.2 0.0 0 500 1000 1500 2000 0 500 1000 1500 2000 5 Meanings 7 Meanings 1.0 0.8 0.6 0.4 0.2 0.0 0 500 1000 1500 2000 0 500 1000 1500 2000 Number of Utterances Number of Utterances Figure 5: Learning quantifiers with frequency f. Figure 5 shows the posterior probability of the correct meanings for the quantifiers ‘a’, ‘another’ and ‘any’ over the course of training with 1, 3, 5 and 7 candidate meanings for each ut</context>
</contexts>
<marker>Carey, Bartlett, 1978</marker>
<rawString>Carey, S. and Bartlett, E. (1978). Acquring a single new word. Papers and Reports on Child Language Development, 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>J Kim</author>
<author>R J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>37--397</pages>
<contexts>
<context position="5892" citStr="Chen et al. (2010)" startWordPosition="886" endWordPosition="889">used on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approach is closely related </context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>Chen, D. L., Kim, J., and Mooney, R. J. (2010). Training a multilingual sportscaster: Using perceptual context to learn language. J. Artif. Intell. Res. (JAIR), 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>A probabilistic computational model of crosssituational word learning.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>6</issue>
<contexts>
<context position="5442" citStr="Fazly et al., 2010" startWordPosition="811" endWordPosition="815">guistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learnin</context>
</contexts>
<marker>Fazly, Alishahi, Stevenson, 2010</marker>
<rawString>Fazly, A., Alishahi, A., and Stevenson, S. (2010). A probabilistic computational model of crosssituational word learning. Cognitive Science, 34(6):1017–1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frank</author>
<author>S Goodman</author>
<author>J Tenenbaum</author>
</authors>
<title>Using speakers referential intentions to model early cross-situational word learning.</title>
<date>2009</date>
<journal>Psychological Science,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="31649" citStr="Frank et al., 2009" startWordPosition="5351" endWordPosition="5354">pondences between syntactic and semantic types, as well as a Bayesian prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In part</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2009</marker>
<rawString>Frank, M., Goodman, S., and Tenenbaum, J. (2009). Using speakers referential intentions to model early cross-situational word learning. Psychological Science, 20(5):578–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Frank</author>
<author>N D Goodman</author>
<author>J B Tenenbaum</author>
</authors>
<title>A bayesian framework for crosssituational word-learning.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20.</booktitle>
<contexts>
<context position="5421" citStr="Frank et al., 2008" startWordPosition="807" endWordPosition="810"> general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper.</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2008</marker>
<rawString>Frank, M. C., Goodman, N. D., and Tenenbaum, J. B. (2008). A bayesian framework for crosssituational word-learning. Advances in Neural Information Processing Systems 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<date>1994</date>
<booktitle>Triggers. Linguistic Inquiry,</booktitle>
<pages>25--355</pages>
<contexts>
<context position="4470" citStr="Gibson and Wexler, 1994" startWordPosition="659" endWordPosition="662">ril 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mappi</context>
<context position="33801" citStr="Gibson and Wexler (1994)" startWordPosition="5692" endWordPosition="5695">esentations from a much older apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast mapping, our model shows that statistical learners can account for sudden changes in children’s grammars. In future, we hope to extend these results by examining other learning behaviors and testing the model on other languages. 9 Acknowled</context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, E. and Wexler, K. (1994). Triggers. Linguistic Inquiry, 25:355–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<date>1990</date>
<journal>Language Acquisition,</journal>
<pages>1--1</pages>
<contexts>
<context position="28905" citStr="Gleitman, 1990" startWordPosition="4909" endWordPosition="4911">obability of the correct meanings for the quantifiers ‘a’, ‘another’ and ‘any’ over the course of training with 1, 3, 5 and 7 candidate meanings for each utterance5. These three words are all of the same class but have very different frequencies in the training subset shown (168, 10 and 2 respectively). In all training settings, the word ‘a’ is learnt gradually from many observations but the rarer words ‘another’ and ‘any’ are learnt (when they are learnt) through large updates to the posterior on the basis of few observations. These large updates result from a syntactic bootstrapping effect (Gleitman, 1990). When the model has great confidence about the derivation in which an unseen lexical item occurs, the pseudocounts for that lexical item get a large update under Equation 17. This large update has a greater effect on rare words which are associated with small amounts of probability mass than it does on common ones that have already accumulated large pseudocounts. The fast learning of rare words later in learning correlates with observations of word learning in children. 7.3 Word Order Learning Figure 6 shows the posterior probability of the correct SVO word order learnt from increasing amount</context>
</contexts>
<marker>Gleitman, 1990</marker>
<rawString>Gleitman, L. (1990). The structural sources of verb meanings. Language Acquisition, 1:1–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="31629" citStr="Goldwater et al., 2009" startWordPosition="5347" endWordPosition="5350">anguage-universal correspondences between syntactic and semantic types, as well as a Bayesian prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Goldwater, S., Griffiths, T. L., and Johnson, M. (2009). A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
</authors>
<title>Structure and strength in causal induction.</title>
<date>2005</date>
<pages>51--354</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="31697" citStr="Griffiths, 2005" startWordPosition="5360" endWordPosition="5361">well as a Bayesian prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses consistent with </context>
</contexts>
<marker>Griffiths, 2005</marker>
<rawString>Griffiths, T. L., . T. J. B. (2005). Structure and strength in causal induction. Cognitive Psychology, 51:354–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>J B Tenenbaum</author>
</authors>
<title>Optimal predictions in everyday cognition.</title>
<date>2006</date>
<publisher>Psychological Science.</publisher>
<contexts>
<context position="31680" citStr="Griffiths and Tenenbaum, 2006" startWordPosition="5355" endWordPosition="5359">ntactic and semantic types, as well as a Bayesian prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses</context>
</contexts>
<marker>Griffiths, Tenenbaum, 2006</marker>
<rawString>Griffiths, T. L. and Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hoffman</author>
<author>D M Blei</author>
<author>F Bach</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="19435" citStr="Hoffman et al. (2010)" startWordPosition="3280" endWordPosition="3283">rt representation of {t} in the chart Ch. The function T massively overgenerates parses for any given natural language. The probabilistic parsing model introduced in Section 3 is used to choose the best parse from the overgenerated set. 5 Training 5.1 Parameter Estimation The probabilistic model of the grammar describes a distribution over the observed training data X, latent variables S, and parameters B. The goal of training is to estimate the posterior distribution: p(S,B|X) = p(S, X|B)p(B) (14) p(X) which we do with online Variational Bayesian Expectation Maximisation (oVBEM; Sato (2001), Hoffman et al. (2010)). oVBEM is an online cat(T) = I 238 Bayesian extension of the EM algorithm that accumulates observation pseudocounts na→b for each of the productions a → b in the grammar. These pseudocounts define the posterior over production probabilities as follows: (θa→b1, ... , θa→b{,,,... 1)) |X, S ∼ (15) ∞ Dir(αH(b1) + na→b1, ... , E αH(bj) + na→bj) j=k These pseudocounts are computed in two steps: oVBE-step For the training pair (si, {m}i) which supports the set of parses {t}, the expectation E{t}[a → b] of each production a → b is calculated by creating a packed chart representation of {t} and runni</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Hoffman, M., Blei, D. M., and Bach, F. (2010). Online learning for latent dirichlet allocation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07).</booktitle>
<contexts>
<context position="5824" citStr="Kate and Mooney (2007)" startWordPosition="873" endWordPosition="876">quisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Kate, R. J. and Mooney, R. J. (2007). Learning language semantics from ambiguous supervision. In Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3189" citStr="Kwiatkowski et al., 2010" startWordPosition="466" endWordPosition="469">learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once. We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus (MacWhinney, 2000) using online variational Bayesian EM. We evaluate the learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1Similar to referential uncertainty but relating to propositions rather than referents. Candidate Meanings 234 Proceedings of the 13th Conference of the European Chapter of the Association for Com</context>
<context position="5918" citStr="Kwiatkowski et al. (2010" startWordPosition="890" endWordPosition="893">ly, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approach is closely related that of Kwiatkowski et al.</context>
<context position="7468" citStr="Kwiatkowski et al. (2010)" startWordPosition="1150" endWordPosition="1153">: i = 1, ... , N}, and learns a CCG lexicon A and the probability of each production a —* b that could be used in a parse. Together, these define a probabilistic parser that can be used to find the most probable meaning for any new sentence. We learn both the lexicon and production probabilities from allowable parses of the training pairs. The set of allowable parses {t} for a single (utterance, meaning-candidates) pair consists of those parses that map the utterance onto one of the meanings. This set is generated with the functional mapping T : {t} = T (s, m), (2) which is defined, following Kwiatkowski et al. (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). The CCG lexicon A is learnt by reading off the lexical items used in all parses of all training pairs. Production probabilities are learnt in conjunction with A through the use of an incremental parameter estimation algorithm, online Variational Bayesian EM, as described in Section 5. Before presenting the probabilistic model, the mapping T, and the parameter training algorithm, we first provide some background on the meaning representations we use and on CCG. 2 Background 2.1 M</context>
<context position="14658" citStr="Kwiatkowski et al. (2010)" startWordPosition="2411" endWordPosition="2414"> 4-7 describe the joint distribution P(X, S, B) over the observed training data X = {(sZ, {m}Z) : i = 1, ... , N}, the latent variables S (containing the productions used in each parse t) and the parsing parameters B. 4 Generating Parses The previous section defined a parameterisation over parses assuming that the CCG lexicon A was known. In practice A is empty prior to training and must be populated with the lexical items from parses t consistent with training pairs (s, {m}). The set of allowed parses {t} is defined by the function T from Equation 2. Here we review the splitting procedure of Kwiatkowski et al. (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. In this section we assume that s is paired at each point with only a single meaning m. Later we will show how T is used multiple times to create the set of parses consistent with s and a set of candidate meanings {m}. The splitting procedure takes as input a CCG category X:h, such as NP : a(x, cookie(x)), and returns a set of category splits. Each category split is a pair of CCG </context>
<context position="26004" citStr="Kwiatkowski et al. (2010)" startWordPosition="4408" endWordPosition="4411">earnt by training on the first i files of the longitudinally ordered Eve corpus and testing on file i + 1, for i = 1...19. For each utterance s&apos; in the test file we use the parsing model to predict a meaning m* and compare this to the target meaning m&apos;. We report the proportion of utterances for which the prediction m* is returned correctly both with and without word-meaning guessing. When a word has never been seen at training time our parser has the ability to ‘guess’ a typed logical meaning with placeholders for constant and predicate names. For comparison we use the UBL semantic parser of Kwiatkowski et al. (2010) trained in a similar setting—i.e., with no language specific initialisation4. Figure 4 shows accuracy for our approach with and without guessing, for UBL 4Kwiatkowski et al. (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. Instead we run UBL with small positive weight for all lexical items. When run with Giza++ parameter initialisations, UBL10 achieves 48.1% across folds compared to 49.2% for our approach. Accuracy 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.00.0 0.2 0.4 0.6 0.8 1.0 Proportion of Data Seen Our Approa</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., and Steedman, M. (2010). Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the Conference on Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in ccg grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Emperical Methods in Natural Language Processing.</booktitle>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., and Steedman, M. (2011). Lexical generalization in ccg grammar induction for semantic parsing. In Proceedings of the Conference on Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>F Reali</author>
<author>T Griffiths</author>
</authors>
<title>Modeling the effects of memory on human online sentence processing with particle filters.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<contexts>
<context position="32895" citStr="Levy et al., 2009" startWordPosition="5546" endWordPosition="5549">es consistent with each training instance, which can be both memory- and processor-intensive. It is unlikely that children do this once they have learnt at least some of the target language. In future, we plan to investigate more efficient parameter estimation methods. One possibility would be an approximate oVBEM algorithm in which the expectations in Equation 17 are calculated according to a high probability subset of the parses {t}. Another option would be particle filtering, which has been investigated as a cognitively plausible method for approximate Bayesian inference (Shi et al., 2010; Levy et al., 2009; Sanborn et al., 2010). As a crude approximation to the context in which an utterance is heard, the logical representations of meaning that we present to the learner are also open to criticism. However, Steedman (2002) argues that children do have access to structured meaning representations from a much older apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics co</context>
</contexts>
<marker>Levy, Reali, Griffiths, 2009</marker>
<rawString>Levy, R., Reali, F., and Griffiths, T. (2009). Modeling the effects of memory on human online sentence processing with particle filters. In Advances in Neural Information Processing Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>H T Ng</author>
<author>W S Lee</author>
<author>L S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5872" citStr="Lu et al. (2008)" startWordPosition="882" endWordPosition="885"> learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In particular, our approach</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer, L. S. (2008). A generative model for parsing natural language to meaning representations. In Proceedings of The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES project: tools for analyzing talk.</title>
<date>2000</date>
<journal>Lawrence Erlbaum, Mahwah, NJ u.a. EN.</journal>
<contexts>
<context position="2888" citStr="MacWhinney, 2000" startWordPosition="422" endWordPosition="423">f such (utterance, meaning-candidates) pairs, the correct lexicon and parsing model. Here we present a probabilistic account of this task with an emphasis on cognitive plausibility. Our criteria for plausibility are that the learner must not require any language-specific information prior to learning and that the learning algorithm must be strictly incremental: it sees each training instance sequentially and exactly once. We define a Bayesian model of parse structure with Dirichlet process priors and train this on a set of (utterance, meaning-candidates) pairs derived from the CHILDES corpus (MacWhinney, 2000) using online variational Bayesian EM. We evaluate the learnt grammar in three ways. First, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning. We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al., 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al.). We then examine the learning curves of some individual words, showing that the model can learn word meanings on the basis of a single exposure, similar t</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. (2000). The CHILDES project: tools for analyzing talk. Lawrence Erlbaum, Mahwah, NJ u.a. EN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Maurits</author>
<author>A Perfors</author>
<author>D Navarro</author>
</authors>
<title>Joint acquisition of word order and word reference.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="5593" citStr="Maurits et al. (2009)" startWordPosition="837" endWordPosition="840"> that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this pa</context>
</contexts>
<marker>Maurits, Perfors, Navarro, 2009</marker>
<rawString>Maurits, L., Perfors, A., and Navarro, D. (2009). Joint acquisition of word order and word reference. In Proceedings of the 31th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pearl</author>
<author>S Goldwater</author>
<author>M Steyvers</author>
</authors>
<title>How ideal are we? Incorporating human limitations into Bayesian models of word segmentation.</title>
<date>2010</date>
<pages>315--326</pages>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="32048" citStr="Pearl et al., 2010" startWordPosition="5412" endWordPosition="5415">ly. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses consistent with each training instance, which can be both memory- and processor-intensive. It is unlikely that children do this once they have learnt at least some of the target language. In future, we plan to investigate more efficient parameter estimation methods. One possibility would be an approximate oVBEM algorithm in which the expectations in Equation 17 are</context>
</contexts>
<marker>Pearl, Goldwater, Steyvers, 2010</marker>
<rawString>Pearl, L., Goldwater, S., and Steyvers, M. (2010). How ideal are we? Incorporating human limitations into Bayesian models of word segmentation. pages 315–326, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Perfors</author>
<author>J B Tenenbaum</author>
<author>T Regier</author>
</authors>
<title>The learnability of abstract syntactic principles.</title>
<date>2011</date>
<journal>Cognition,</journal>
<volume>118</volume>
<issue>3</issue>
<pages>338</pages>
<contexts>
<context position="31720" citStr="Perfors et al., 2011" startWordPosition="5362" endWordPosition="5366">n prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses consistent with each training instance,</context>
</contexts>
<marker>Perfors, Tenenbaum, Regier, 2011</marker>
<rawString>Perfors, A., Tenenbaum, J. B., and Regier, T. (2011). The learnability of abstract syntactic principles. Cognition, 118(3):306 – 338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>B MacWhinney</author>
<author>A Lavie</author>
</authors>
<title>Adding syntactic annotations to transcripts of parent-child dialogs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation.</booktitle>
<location>Lisbon, LREC.</location>
<contexts>
<context position="22190" citStr="Sagae et al. (2004)" startWordPosition="3797" endWordPosition="3800">g the lex function to read all of the lexical items off from the derivations in each {t}0. In the parameter update step, the training algorithm updates the pseudocounts associated with each of the productions a → b that have ever been seen during training according to Equation (17). Only non-zero pseudocounts are stored in our model. The count vector is expanded with a new entry every time a new production is used. While The Eve corpus, collected by Brown (1973), contains 14,124 English utterances spoken to a single child between the ages of 18 and 27 months. These have been hand annotated by Sagae et al. (2004) with labelled syntactic dependency graphs. An example annotation is shown in Figure 3. While these annotations are designed to represent syntactic information, the parent-child relationships in the parse can also be viewed as a proxy for the predicate-argument structure of the semantics. We developed a template based deterministic procedure for mapping this predicateargument structure onto logical expressions of the type discussed in Section 2.1. For example, the dependency graph in Figure 3 is automatically transformed into the logical expression λe.have(you,another(y, cookie(y)), e) (18) ∧ </context>
</contexts>
<marker>Sagae, MacWhinney, Lavie, 2004</marker>
<rawString>Sagae, K., MacWhinney, B., and Lavie, A. (2004). Adding syntactic annotations to transcripts of parent-child dialogs. In Proceedings of the 4th International Conference on Language Resources and Evaluation. Lisbon, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sakas</author>
<author>J D Fodor</author>
</authors>
<title>The structural triggers learner.</title>
<date>2001</date>
<booktitle>Language Acquisition and Learnability,</booktitle>
<pages>172--233</pages>
<editor>In Bertolo, S., editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4493" citStr="Sakas and Fodor, 2001" startWordPosition="663" endWordPosition="666">Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type t</context>
<context position="33840" citStr="Sakas and Fodor (2001)" startWordPosition="5698" endWordPosition="5701">sed for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast mapping, our model shows that statistical learners can account for sudden changes in children’s grammars. In future, we hope to extend these results by examining other learning behaviors and testing the model on other languages. 9 Acknowledgements We thank Mark Johnson for sugge</context>
</contexts>
<marker>Sakas, Fodor, 2001</marker>
<rawString>Sakas, W. and Fodor, J. D. (2001). The structural triggers learner. In Bertolo, S., editor, Language Acquisition and Learnability, pages 172–233. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Sanborn</author>
<author>T L Griffiths</author>
<author>D J Navarro</author>
</authors>
<title>Rational approximations to rational models: Alternative algorithms for category learning. Psychological Review.</title>
<date>2010</date>
<contexts>
<context position="32070" citStr="Sanborn et al., 2010" startWordPosition="5416" endWordPosition="5419">a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses consistent with each training instance, which can be both memory- and processor-intensive. It is unlikely that children do this once they have learnt at least some of the target language. In future, we plan to investigate more efficient parameter estimation methods. One possibility would be an approximate oVBEM algorithm in which the expectations in Equation 17 are calculated according </context>
</contexts>
<marker>Sanborn, Griffiths, Navarro, 2010</marker>
<rawString>Sanborn, A. N., Griffiths, T. L., and Navarro, D. J. (2010). Rational approximations to rational models: Alternative algorithms for category learning. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sato</author>
</authors>
<title>Online model selection based on the variational bayes.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context position="19412" citStr="Sato (2001)" startWordPosition="3278" endWordPosition="3279"> a packed chart representation of {t} in the chart Ch. The function T massively overgenerates parses for any given natural language. The probabilistic parsing model introduced in Section 3 is used to choose the best parse from the overgenerated set. 5 Training 5.1 Parameter Estimation The probabilistic model of the grammar describes a distribution over the observed training data X, latent variables S, and parameters B. The goal of training is to estimate the posterior distribution: p(S,B|X) = p(S, X|B)p(B) (14) p(X) which we do with online Variational Bayesian Expectation Maximisation (oVBEM; Sato (2001), Hoffman et al. (2010)). oVBEM is an online cat(T) = I 238 Bayesian extension of the EM algorithm that accumulates observation pseudocounts na→b for each of the productions a → b in the grammar. These pseudocounts define the posterior over production probabilities as follows: (θa→b1, ... , θa→b{,,,... 1)) |X, S ∼ (15) ∞ Dir(αH(b1) + na→b1, ... , E αH(bj) + na→bj) j=k These pseudocounts are computed in two steps: oVBE-step For the training pair (si, {m}i) which supports the set of parses {t}, the expectation E{t}[a → b] of each production a → b is calculated by creating a packed chart represen</context>
</contexts>
<marker>Sato, 2001</marker>
<rawString>Sato, M. (2001). Online model selection based on the variational bayes. Neural Computation, 13(7):1649–1681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shi</author>
<author>T L Griffiths</author>
<author>N H Feldman</author>
<author>A N Sanborn</author>
</authors>
<title>Exemplar models as a mechanism for performing bayesian inference.</title>
<date>2010</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>464</pages>
<contexts>
<context position="32089" citStr="Shi et al., 2010" startWordPosition="5420" endWordPosition="5423">approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be criticized as cognitively implausible. In particular, it generates all parses consistent with each training instance, which can be both memory- and processor-intensive. It is unlikely that children do this once they have learnt at least some of the target language. In future, we plan to investigate more efficient parameter estimation methods. One possibility would be an approximate oVBEM algorithm in which the expectations in Equation 17 are calculated according to a high probabili</context>
</contexts>
<marker>Shi, Griffiths, Feldman, Sanborn, 2010</marker>
<rawString>Shi, L., Griffiths, T. L., Feldman, N. H., and Sanborn, A. N. (2010). Exemplar models as a mechanism for performing bayesian inference. Psychonomic Bulletin &amp; Review, 17(4):443– 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition.</title>
<date>1992</date>
<tech>PhD thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4392" citStr="Siskind, 1992" startWordPosition="650" endWordPosition="651">on for Computational Linguistics, pages 234–244, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner ha</context>
<context position="33817" citStr="Siskind (1992)" startWordPosition="5696" endWordPosition="5697">der apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast mapping, our model shows that statistical learners can account for sudden changes in children’s grammars. In future, we hope to extend these results by examining other learning behaviors and testing the model on other languages. 9 Acknowledgements We thank</context>
</contexts>
<marker>Siskind, 1992</marker>
<rawString>Siskind, J. M. (1992). Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition. PhD thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning wordto-meaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="5522" citStr="Siskind, 1996" startWordPosition="827" endWordPosition="828"> Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”par</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Siskind, J. M. (1996). A computational study of cross-situational techniques for learning wordto-meaning mappings. Cognition, 61(1-2):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9007" citStr="Steedman 2000" startWordPosition="1398" endWordPosition="1400">ection 6.1 we will see how logical expressions like this are created for a set of child-directed utterances (to use in training our model). The lambda-calculus uses A operators to define functions. These may be used to represent functional meanings of utterances but they may also be used as a ‘glue language’, to compose elements of first order logical expressions. For example, the function AxAy.like(y, x) can be combined with the object mummy to give the phrasal meaning Ay.like(y, mummy) through the lambdacalculus operation of function application. 2.2 CCG Combinatory Categorial Grammar (CCG; Steedman 2000) is a strongly lexicalised linguistic formalism that tightly couples syntax and semantics. Each CCG lexical item in the lexicon A is a triple, written as word ` syntactic category : logical expression. Examples are: You ` NP : you read ` S\NP/NP : AxAy.read(y,x) the ` NP/N : Af.the(x, f(x)) book ` N : Ax.book(x) A full CCG category X : h has syntactic category X and logical expression h. Syntactic categories may be atomic (e.g., S or NP) or complex (e.g., (S\NP)/NP). Slash operators in complex categories define functions from the range on the right of the slash to the result on the left in muc</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Plans, affordances, and combinatory grammar.</title>
<date>2002</date>
<journal>Linguistics and Philosophy,</journal>
<volume>25</volume>
<contexts>
<context position="33114" citStr="Steedman (2002)" startWordPosition="5585" endWordPosition="5586">ate more efficient parameter estimation methods. One possibility would be an approximate oVBEM algorithm in which the expectations in Equation 17 are calculated according to a high probability subset of the parses {t}. Another option would be particle filtering, which has been investigated as a cognitively plausible method for approximate Bayesian inference (Shi et al., 2010; Levy et al., 2009; Sanborn et al., 2010). As a crude approximation to the context in which an utterance is heard, the logical representations of meaning that we present to the learner are also open to criticism. However, Steedman (2002) argues that children do have access to structured meaning representations from a much older apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their</context>
</contexts>
<marker>Steedman, 2002</marker>
<rawString>Steedman, M. (2002). Plans, affordances, and combinatory grammar. Linguistics and Philosophy, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Thornton</author>
<author>G Tesan</author>
</authors>
<title>Categorical acquisition: Parameter setting in universal grammar.</title>
<date>2007</date>
<journal>Biolinguistics,</journal>
<volume>1</volume>
<contexts>
<context position="4019" citStr="Thornton and Tesan (2007)" startWordPosition="592" endWordPosition="595">s, showing that the model can learn word meanings on the basis of a single exposure, similar to the fast mapping phenomenon observed in children (Carey and Bartlett, 1978). Finally, we show that our 1Similar to referential uncertainty but relating to propositions rather than referents. Candidate Meanings 234 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–244, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also ado</context>
<context position="30276" citStr="Thornton and Tesan (2007)" startWordPosition="5131" endWordPosition="5134">rees that could have generated them. With no propositional uncertainty in the training data the correct word order is learnt very quickly and stabilises. As the amount of propositional uncertainty increases, the rate at which this rule is learnt decreases. However, even in the face of ambiguous training data, the model can learn the correct word-order rule. The distribution over word orders also exhibits initial uncertainty, followed by a sharp convergence to the correct analysis. This ability to learn syntactic regularities abruptly means that our system is not subject to the criticisms that Thornton and Tesan (2007) levelled at statistical models of language acquisition—that their learning rates are too gradual. 5The term ‘fast mapping’ is generally used to refer to noun learning. We chose to examine quantifier learning here as there is a greater variation in quantifier frequencies. Fast mapping of nouns is also achieved. f = 168 a — Af.a(x, f(x)) f = 10 another — Af.another(x, f(x)) f = 2 any — Af.any(x, f(x)) P(mlw) P(mlw) 241 Figure 6: Learning SVO word order. 8 Discussion We have presented an incremental model of language acquisition that learns a probabilistic CCG grammar from utterances paired with</context>
<context position="34103" citStr="Thornton and Tesan, 2007" startWordPosition="5738" endWordPosition="5741"> and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast mapping, our model shows that statistical learners can account for sudden changes in children’s grammars. In future, we hope to extend these results by examining other learning behaviors and testing the model on other languages. 9 Acknowledgements We thank Mark Johnson for suggesting an analysis of learning rates. This work was funded by the ERC Advanced Fellowship 24952 GramPlus and EU IP grant EC-FP7-270273 Xperience. 1 Meaning 3 Meanings 0.00 500 1000 1500 2000 5 Meanings 0.00 500 1000 1500 2000 Number of Utterances 0 500 1000 1500 2</context>
</contexts>
<marker>Thornton, Tesan, 2007</marker>
<rawString>Thornton, R. and Tesan, G. (2007). Categorical acquisition: Parameter setting in universal grammar. Biolinguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The acquisition of a unification-based generalised categorial grammar.</title>
<date>2002</date>
<tech>Technical Report UCAM-CL-TR-533,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="4413" citStr="Villavicencio, 2002" startWordPosition="652" endWordPosition="653">ional Linguistics, pages 234–244, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small s</context>
<context position="33544" citStr="Villavicencio, 2002" startWordPosition="5649" endWordPosition="5651">crude approximation to the context in which an utterance is heard, the logical representations of meaning that we present to the learner are also open to criticism. However, Steedman (2002) argues that children do have access to structured meaning representations from a much older apparatus used for planning actions and we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of wor</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, A. (2002). The acquisition of a unification-based generalised categorial grammar. Technical Report UCAM-CL-TR-533, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="5847" citStr="Wong and Mooney (2006" startWordPosition="877" endWordPosition="880">een data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additional corpus statistics), all of which we dispense with here. In</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Wong, Y. W. and Mooney, R. (2006). Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y. W. and Mooney, R. (2007). Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xu</author>
<author>J B Tenenbaum</author>
</authors>
<title>Word learning as Bayesian inference. Psychological Review,</title>
<date>2007</date>
<pages>114--245</pages>
<contexts>
<context position="31605" citStr="Xu and Tenenbaum, 2007" startWordPosition="5343" endWordPosition="5346"> learner has access to language-universal correspondences between syntactic and semantic types, as well as a Bayesian prior encouraging grammars with heavy reuse of existing rules and lexical items. We have shown that this model not only outperforms a state-of-the-art semantic parser, but also exhibits learning curves similar to children’s: lexical items can be acquired on a single exposure and word order is learnt suddenly rather than gradually. Although we use a Bayesian model, our approach is different from many of the Bayesian models proposed in cognitive science and language acquisition (Xu and Tenenbaum, 2007; Goldwater et al., 2009; Frank et al., 2009; Griffiths and Tenenbaum, 2006; Griffiths, 2005; Perfors et al., 2011). These models are intended as ideal observer analyses, demonstrating what would be learned by a probabilistically optimal learner. Our learner uses a more cognitively plausible but approximate online learning algorithm. In this way, it is similar to other cognitively plausible approximate Bayesian learners (Pearl et al., 2010; Sanborn et al., 2010; Shi et al., 2010). Of course, despite the incremental nature of our learning algorithm, there are still many aspects that could be cr</context>
</contexts>
<marker>Xu, Tenenbaum, 2007</marker>
<rawString>Xu, F. and Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114:245–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>Knowledge and Learning in Natural Language.</title>
<date>2002</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4506" citStr="Yang, 2002" startWordPosition="667" endWordPosition="668">tional Linguistics learner captures the step-like learning curves for word order regularities that Thornton and Tesan (2007) claim children show. This result counters Thornton and Tesan’s criticism of statistical grammar learners—that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children. 1.1 Related Work Models of syntactic acquisition, whether they have addressed the task of learning both syntax and semantics (Siskind, 1992; Villavicencio, 2002; Buttery, 2006) or syntax alone (Gibson and Wexler, 1994; Sakas and Fodor, 2001; Yang, 2002) have aimed to learn a single, correct, deterministic grammar. With the exception of Buttery (2006) they also adopt the Principles and Parameters grammatical framework, which assumes detailed knowledge of linguistic regularities2. Our approach contrasts with all previous models in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic c</context>
<context position="33857" citStr="Yang (2002)" startWordPosition="5703" endWordPosition="5704"> we wish to eventually ground these in sensory input. Despite the limitations listed above, our approach makes several important contributions to the computational study of language acquisition. It is the first model to learn syntax and semantics concurrently; previous systems (Villavicencio, 2002; Buttery, 2006) learnt categorial grammars from sentences where all word meanings were known. Our model is also the first to be evaluated by parsing sentences onto their meanings, in contrast to the work mentioned above and that of Gibson and Wexler (1994), Siskind (1992) Sakas and Fodor (2001), and Yang (2002). These all evaluate their learners on the basis of a small number of predefined syntactic parameters. Finally, our work addresses a misunderstanding about statistical learners—that their learning curves must be gradual (Thornton and Tesan, 2007). By demonstrating sudden learning of word order and fast mapping, our model shows that statistical learners can account for sudden changes in children’s grammars. In future, we hope to extend these results by examining other learning behaviors and testing the model on other languages. 9 Acknowledgements We thank Mark Johnson for suggesting an analysis</context>
</contexts>
<marker>Yang, 2002</marker>
<rawString>Yang, C. (2002). Knowledge and Learning in Natural Language. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>A unified model of early word learning: Integrating statistical and social cues.</title>
<date>2007</date>
<journal>Neurocomputing,</journal>
<pages>70--13</pages>
<contexts>
<context position="5401" citStr="Yu and Ballard, 2007" startWordPosition="803" endWordPosition="806">els in assuming a very general kind of linguistic knowledge and a probabilistic grammar. Specifically, we use the probabilistic Combinatory Categorial Grammar (CCG) framework, and assume only that the learner has access to a small set of general combinatory schemata and a functional mapping from semantic type to syntactic category. Furthermore, this paper is the first to evaluate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addr</context>
</contexts>
<marker>Yu, Ballard, 2007</marker>
<rawString>Yu, C. and Ballard, D. H. (2007). A unified model of early word learning: Integrating statistical and social cues. Neurocomputing, 70(13-15):2149 – 2165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5787" citStr="Zettlemoyer and Collins (2005" startWordPosition="867" endWordPosition="870">luate a model of child syntactic-semantic acquisition by parsing unseen data. Models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (Yu and Ballard, 2007; Frank et al., 2008; Fazly et al., 2010) or a compositional meaning representation of the type used here (Siskind, 1996). The models of Alishahi and Stevenson (2008) and Maurits et al. (2009) learn, as well as wordmeanings, orderings for verb-argument structures but not the full parsing model that we learn here. Semantic parser induction as addressed by Zettlemoyer and Collins (2005, 2007, 2009), Kate and Mooney (2007), Wong and Mooney (2006, 2007), Lu et al. (2008), Chen et al. (2010), Kwiatkowski et al. (2010, 2011) and B¨orschinger et al. (2011) has the same task definition as the one addressed by this paper. However, the learning approaches presented in those previous pa2This linguistic use of the term ”parameter” is distinct from the statistical use found elsewhere in this paper. pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations (lists of noun phrases and additiona</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, L. S. and Collins, M. (2005). Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, L. S. and Collins, M. (2007). Online learning of relaxed CCG grammars for parsing to logical form. In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of The Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Zettlemoyer, L. S. and Collins, M. (2009). Learning context-dependent mappings from sentences to logical form. In Proceedings of The Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>