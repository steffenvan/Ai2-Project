<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.986507">
Automated Verb Sense Labelling Based on Linked Lexical Resources
</title>
<author confidence="0.992233">
Kostadin Cholakov1 Judith Eckle-Kohler2,3 Iryna Gurevych2,3
</author>
<affiliation confidence="0.8619562">
1 Humboldt-Universit¨at zu Berlin, kostadin.cholakov@anglistik.hu-berlin.de
2 Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit¨at Darmstadt
3 Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.889675">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.990885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894375">
We present a novel approach for creat-
ing sense annotated corpora automatically.
Our approach employs shallow syntactico-
semantic patterns derived from linked lex-
ical resources to automatically identify in-
stances of word senses in text corpora. We
evaluate our labelling method intrinsically
on SemCor and extrinsically by using au-
tomatically labelled corpus text to train a
classifier for verb sense disambiguation.
Testing this classifier on verbs from the
English MASC corpus and on verbs from
the Senseval-3 all-words disambiguation
task shows that it matches the performance
of a classifier which has been trained on
manually annotated data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949789473684">
Sense annotated corpora are important resources
in NLP as they can be used as training data (e.g.,
for word sense disambiguation (WSD) or semantic
role labelling) or as sources for the acquisition of
lexical information (e.g., selectional preference in-
formation). Typically, a particular sense inventory
from a lexical resource is used to annotate some or
all words with word senses from this sense inven-
tory. For instance, various sense-annotated cor-
pora based on WordNet (WN; (Fellbaum, 1998))
exist, such as the data from the Senseval competi-
tions,1 or the SemCor corpus.2 Such corpora are
usually created manually which is expensive and
time consuming. Furthermore, the corpora are of-
ten domain specific (e.g. newspaper texts) which
makes statistical systems trained on them strongly
biased.
We present a novel approach for creating sense
annotated corpora automatically. Our approach
</bodyText>
<footnote confidence="0.989648333333333">
1http://www.senseval.org
2http://www.cse.unt.edu/˜rada/
downloads.html#semcor
</footnote>
<bodyText confidence="0.999672358974359">
employs shallow syntactico-semantic patterns de-
rived from linked lexical resources (LLRs) to auto-
matically identify instances of word senses in text
corpora. We significantly extend previous work on
this task by making two important contributions:
(i) we employ a large-scale LLR for automatically
creating sense annotated data and (ii) we perform
meaningful intrinsic and application-based eval-
uations of our method on large sense annotated
datasets.
LLRs are the result of integrating several
lexical-semantic resources by linking them at the
word sense level. Examples of large LLRs are
the multilingual BabelNet (Navigli and Ponzetto,
2012), an integration of wordnets and Wikipedia3,
or UBY, (Gurevych et al., 2012), the resource we
employ in our work here. UBY is an integration of
multiple resources, such as wordnets, Wikipedia,
Wiktionary (WKT)4, FrameNet (FN; (Baker et al.,
1998)) and VerbNet (VN; (Kipper et al., 2008)) for
English and German.
A distinguishing feature of LLRs is the enriched
sense representation for word senses that are in-
terlinked since different resources provide differ-
ent, often complementary information. Annotat-
ing corpora with such enriched sense representa-
tions turns them into versatile training data for sta-
tistical systems.
Our first contribution (i) also addresses a con-
siderable gap in recent research regarding auto-
mated sense labelling of verbs. Most previous
work is done on nouns. However, verbs pose a
bigger challenge due to their high polysemy and
the fact that, unlike nouns, syntax is of crucial im-
portance because it often reflects particular aspects
of verb meaning. That is why, here we focus on
verbs and present results and evaluations for this
previously neglected part-of-speech (POS). Our
method, however, can be applied to other parts-of
</bodyText>
<footnote confidence="0.999971">
3http://www.wikipedia.org
4http://www.wiktionary.org
</footnote>
<page confidence="0.980471">
68
</page>
<note confidence="0.9930825">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 68–77,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.979321074074074">
speech as well.
Regarding (ii), we are the first to perform mean-
ingful intrinsic and extrinsic evaluations of auto-
matically labelled data on a larger scale. The in-
trinsic evaluation measures the performance of our
method on the manually annotated SemCor cor-
pus. The extrinsic evaluation compares the perfor-
mance of a classifier for verb sense disambigua-
tion (VSD) which has been trained (a) on auto-
matically sense labelled data and (b) on manually
annotated data. Both settings achieve very simi-
lar results which means that competitive VSD can
be performed without the need of costly manually
created training data. This could be beneficial in
languages (e.g., German, Spanish) for which elab-
orate lexical-semantic resources exist but large,
high-quality sense annotated corpora are unavail-
able. Moreover, we experiment with various link-
ings between lexical resources in order to inves-
tigate how different resource combinations affect
the performance of automated sense labelling. We
show that combining all available resources might
not be the best option.
The remainder of the paper is organised as fol-
lows. Section 2 presents our method. Section 3 de-
scribes the data used in the experiments. Section
4 presents the results of the evaluations. Section
</bodyText>
<sectionHeader confidence="0.602825" genericHeader="introduction">
5 analyses in detail the differences between our
</sectionHeader>
<bodyText confidence="0.828115">
method and previous work. Section 6 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.820943" genericHeader="method">
2 Automated Labelling of Verb Senses
</sectionHeader>
<bodyText confidence="0.999729">
This section describes our novel approach for au-
tomated sense labelling of verbs in a corpus, which
exploits the added value of LLRs.
</bodyText>
<subsectionHeader confidence="0.98502">
2.1 Approach
</subsectionHeader>
<bodyText confidence="0.99992975">
Our approach to automatically label corpus in-
stances of verb senses with sense identifiers from
an LLR is based on a pattern-based representation
of verb senses. Such patterns constitute a common
format for the representation of verb senses avail-
able in LLRs and verb instances found in corpora.
The common format we developed resembles a
syntactico-semantic clause pattern which we call
a sense pattern (SP). Based on a comparison of the
derived SPs by means of a similarity metric, verb
instances in a corpus can automatically be labelled
with sense identifiers from an LLR.
SPs can be derived from corpus instances and
from information given in LLRs, in particular,
sense examples and more abstract predicate argu-
ment structure information.
</bodyText>
<subsectionHeader confidence="0.996799">
2.2 Step 1: Creation of SPs from LLRs
</subsectionHeader>
<bodyText confidence="0.999880804878048">
For the creation of SPs, we employ the large-scale
LLR UBY which combines 10 lexical resources
for English and German to make use of the en-
riched verb sense representations provided by the
sense links between various resources available in
UBY. Although our method can work with any
LLR, we choose UBY because the various re-
sources are represented in a standardised format
(Eckle-Kohler et al., 2012) and sense links be-
tween them can uniformly and conveniently be ac-
cessed via the freely available UBY-API.5
Since we evaluate our method on data annotated
with WN senses, we create SPs for enriched WN
senses (see example given in Table 1). We enrich
WN senses by aggregating lexical information that
can be accessed through links given in UBY to
corresponding verb senses in other resources.
In this setting, enrichment means that we make
use of sense examples from WN, from FN via
the WN–FN linking, and from WKT via the
WN–WKT linking. In addition, we use ab-
stract predicate-argument structure information
from VN via the WN–VN linking (see Table 1).6
For phrasal verb senses (e.g., write up) and
other verbal multiword expressions (e.g., know
what’s going on) listed in WN, UBY rarely pro-
vides links to other resources. Therefore, we in-
duced sense links by following the one sense per
collocation assumption.7 Based on this assump-
tion, we linked each sense of a verbal multiword
verb lemma in WN with each sense of the same
multiword lemma in FN and WKT.
From sense examples, we derive two different
kinds of SPs. Based on a fragment of a sense ex-
ample given by a window w around the target verb
lemma we create: (i) lemma SPs (LSPs) consisting
only of lemmas (including the target verb) and (ii)
abstract SPs (ASPs) consisting of the target verb
lemma and items from a fixed, linguistically mo-
tivated vocabulary. This is based on the intuition
that LSPs are important to identify relatively fixed
</bodyText>
<footnote confidence="0.9978925">
5http://code.google.com/p/uby/
6Although VN is linked to sense examples given in the
PropBank corpus, the rationale behind using just abstract
predicate-argument structure information was to explore,
which effect this type of information has on the performance
of an automated labelling algorithm.
7It assumes that nearby words provide strong and consis-
tent clues to the sense of a target word, see Yarowsky (1995).
</footnote>
<page confidence="0.998288">
69
</page>
<table confidence="0.7809001">
WN sense tell%2:32:00:: (let something be known) Corresponding sense patterns (SPs)
WN Tell them that you will be late LSP – tell them that you will be
ASP – tell PP that PP be JJ
WN–FN But an insider told TODAY: ‘ There was no animosity.’ LSP – but an insider tell Today : ‘ there be
ASP – person tell location be feeling
WN–WKT Please tell me the time. LSP – Please tell me the time
ASP – tell PP event
WN–VN Agent[+animate |+ organization] V ASP – PP tell group about communication
Recipient[+animate |+ organization]
about Topic[+communication]
</table>
<tableCaption confidence="0.9303975">
Table 1: Examples of SPs derived from an enriched WN sense in UBY. PP, JJ, and VV are POS tags
from the Penn Treebank tagset, standing for personal pronoun, adjective and full verb.
</tableCaption>
<bodyText confidence="0.995431583333334">
verbal multiword expressions in a corpus, whereas
ASPs are necessary to identify productively used
verb senses that are constrained in their use only
by their syntactic behaviour and particular seman-
tic properties, such as selectional preferences on
their arguments.
The fixed vocabulary used for the creation of
ASPs consists of (i) the target verb lemma, (ii) se-
lected POS tags from the Penn Treebank Tagset
(Marcus et al., 1993), (iii) a list of particular func-
tion words that play an important role in fine-
grained subcategorisation frames of verbs (Eckle-
Kohler and Gurevych, 2012) and (iv) semantic cat-
egories of nouns given by WN semantic fields. We
selected POS tags that play an important role in
syntactic realisations of verbs, e.g. POS tags for
personal pronouns which are potential verb argu-
ments. In our experiments, we tried different sets
of function words and POS tags. For instance,
we found that some function words (e.g., reflex-
ive pronouns) and some POS tags (e.g., those for
past participles and comparative adjectives) intro-
duced too much noise in the data and therefore we
did not select them for the final vocabulary.8
In order to create SPs from sense examples,
we apply POS tagging and lemmatisation using
the TreeTagger (Schmid, 1994) and named entity
tagging using the Stanford Named Entity Recog-
niser (Klein et al., 2003). The named entity
tags attached by the Named Entity Recogniser are
mapped to WN semantic fields.
For the generation of ASPs from sense exam-
ples, we used a window size of w = 7, while
the generation of LSPs has been performed with
w = 5 in order to put a focus on the closely neigh-
bouring lexemes in multiword verb lemmas. The
</bodyText>
<footnote confidence="0.9996035">
8The vocabulary used for the creation of ASPs is available
at http://www.ukp.tu-darmstadt.de/data/.
</footnote>
<bodyText confidence="0.999724615384615">
window size was set empirically using the English
Lexical Sample task of the Senseval-2 dataset as
a development set. The same set was also used
for the development of the linguistically motivated
vocabulary for ASPs.9
From the abstract predicate-argument struc-
ture information given in VN, we derived only
ASPs. For this, we employed the subcategori-
sation frames, as well as the semantic role and
selectional preference information from VN, and
created ASPs based on manually created map-
pings between these information types and the
controlled vocabulary used for ASPs.
</bodyText>
<subsectionHeader confidence="0.994267">
2.3 Step 2: Automated Labelling
</subsectionHeader>
<bodyText confidence="0.998613666666667">
For the automated labelling of verbs in a corpus,
we first derive SPs from each corpus sentence con-
taining a target verb. SPs are derived from corpus
sentences by applying the same procedure as de-
scribed in Step 1 for the creation of SPs from sense
examples, the window size used is w = 7.
To compare two SPs, we propose a similarity
metric based on Dice’s coefficient which calcu-
lates the sum of the weighted number of their com-
mon bi-grams, tri-grams, and four-grams. For-
mally, the similarity score simw E [0..1] of two
SPs p1, p2 is defined as:
</bodyText>
<equation confidence="0.9532255">
|Gn(p1)∩Gn(p2)|·n
(1) simw(p1,p2) =
</equation>
<bodyText confidence="0.990339125">
where w &gt;= 1 is the size of the window around
the target verb, Gn(pz), i E {1, 2} is the set of n-
9However, the Senseval-2 data are annotated with sense
keys of the WN pre-release version 1.7 and therefore, we had
to employ an automated mapping of WN 1.7 pre-release to
WN 3.0 sense keys provided by Rada Mihalcea. Since this
mapping turned out to be rather noisy, we did not use the
Senseval-2 data in our evaluations.
</bodyText>
<figure confidence="0.952117">
4
E
n=2
normw
</figure>
<page confidence="0.968132">
70
</page>
<bodyText confidence="0.943958333333333">
Automated labelling of corpus instances
for each sentence si with verb v
derive LSPi and ASPi
</bodyText>
<equation confidence="0.985109">
forall j = sizeOf(UBY-LSP(v))
compare LSPi with LSPj in UBY-LSP(v):
maxSim(LSPi) = argmaxjscore(LSPi, LSPj)
add sense(argmaxj) to MostSimilarSenses(LSPi)
forall k = sizeOf(UBY-ASP(v))
compare ASPi with ASPk in UBY-ASP(v):
maxSim(ASPi) = argmaxkscore(ASPi, ASPk)
add sense(argmaxk) to MostSimilarSenses(ASPi)
if maxSimi,j &gt;= threshold t and
maxSimi,j &gt;= maxSimi,k
label(si) = random(MostSimilarSenses(LSPi))
else if maxSimi,k &gt;= threshold t
label(si) = random(MostSimilarSenses(ASPi))
</equation>
<tableCaption confidence="0.58305825">
end if
end for
Table 2: Algorithm for labelling corpus instances
with WordNet senses.
</tableCaption>
<bodyText confidence="0.999363592592593">
grams occurring in SP pi, and normw is the nor-
malisation factor defined by the sum of the max-
imum number of common bigrams, trigrams and
fourgrams in the window w. Similarity metrics
based on Dice’s coefficient have often been used
in Lesk-based WSD (Lesk, 1986) to calculate the
overlap of two sets (e.g., Baldwin et al. (2010)). In
our case, however, the elements of the two sets are
bigrams, trigrams and fourgrams, while in Lesk-
based algorithms typically sets of unigrams are
compared, thus not accounting for word order.
Table 2 shows the algorithm used for automated
labelling of corpus instances in pseudo-code. The
algorithm assumes that for each verb v, the corre-
sponding set of SPs derived from UBY sense ex-
amples (UBY-LSP(v) and UBY-ASP(v) in Table
2) has already been computed.
For each corpus sentence containing a target
verb v, the corresponding SPs for verb v derived
from UBY are scored by the similarity metric in
(1). The SPs with the maximum score that is above
a threshold t form the set of most similar senses.
From this set, the algorithm picks one sense ran-
domly as a label. How often this happens, depends
on the value of t: the percentage of randomly se-
lected senses ranges from about 33% for t = 0.14
to about 50% for t = 0.04.
</bodyText>
<sectionHeader confidence="0.9973" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999815725">
Web corpora. For the automated labelling of cor-
pus data with WN senses, we use two very large
web corpora: the English ukWaC corpus (Ba-
roni et al., 2009) and the article pages extracted
from the English Wikipedia using the Java-based
Wikipedia API JWPL (Zesch et al., 2008). Fur-
ther, for the evaluation of our method, we use three
manually sense annotated data sets.
SemCor. We use the SemCor 3.0 corpus which
is annotated with WN 3.0 senses.
MASC. MASC is a balanced subset of 500K
words of written texts and transcribed speech
drawn primarily from the Open American Na-
tional Corpus (OANC).10 The texts come from 19
different genres which allows us to test our method
on real-life data from multiple sources. The cor-
pus is annotated with various types of linguistic
information, including WN 3.0 sense annotations
for instances of selected words. Therefore, MASC
is a lexical sample corpus.
We extracted instances of 16 MASC verbs
(11,997 instances) which have been sense anno-
tated. Most instances are annotated by multiple
annotators and, to create a gold standard, we took
the sense preferred by the majority of annotators
and ignored instances where there were ties.
Senseval-3. In the test corpus of the Senseval-
3 all-words disambiguation task sense annotations
are provided for each content word in a chunk
of the WSJ corpus (5,000 words of running text).
The third annotated data set for our experiment is
formed by extracting all verb instances from this
test corpus. Note that the gold standard annota-
tions in Senseval-3 were made using WN 1.7.1.
In our experiments, we use Rada Mihalcea’s con-
version of the corpus to WN 3.0.11 However, we
found out that some verb instances were converted
to sense labels that do not exist in WN 3.0. Af-
ter removing those instances, there were 305 verbs
with 592 instances left.
</bodyText>
<sectionHeader confidence="0.998142" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999975">
Next, we present the intrinsic and the application-
based evaluations of our method.
</bodyText>
<subsectionHeader confidence="0.878544">
4.1 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.99994275">
We intrinsically evaluate the performance of the
automated labelling algorithm for the Senseval-3
verbs which occur in the SemCor corpus. Occur-
rences of these 152 verbs in SemCor are processed
</bodyText>
<footnote confidence="0.99884925">
10http://www.americannationalcorpus.
org/
11http://www.cse.unt.edu/˜rada/
downloads.html#sensevalsemcor
</footnote>
<page confidence="0.989544">
71
</page>
<table confidence="0.994657428571429">
WN–FN–WKT WN–FN–WKT–VN
t Cov Cov Acc Cov Cov Acc
(Inst.) (Sense) (Inst.) (Sense)
0.04 0.55 0.27 0.32 0.48 0.25 0.35
0.07 0.15 0.17 0.36 0.13 0.15 0.42
0.1 0.11 0.14 0.35 0.10 0.13 0.42
0.14 0.02 0.07 0.41 0.02 0.05 0.47
</table>
<tableCaption confidence="0.973195333333333">
Table 3: Performance of the automated labelling
algorithm evaluated for occurrences of Senseval-3
verbs in SemCor.
</tableCaption>
<bodyText confidence="0.9938608">
by the labelling algorithm with a window size
w = 7 and the automatically annotated WN 3.0
senses are compared with the gold senses available
in SemCor 3.0.
Quantitative Evaluation. We calculated the
accuracy as the percentage of correctly labelled in-
stances and the instance coverage as the percent-
age of labelled instances. The sense coverage is
calculated as the percentage of all predicted (not
annotated) senses relative to all gold verb senses
given in SemCor.
A random sense baseline yields 15% accuracy.
Note that a MFS baseline based on WN would
not be meaningful, because the WordNet MFS is
based on the frequency distribution of annotated
senses in SemCor.
Table 3 shows accuracy and coverage results
of the automated labelling algorithm for different
values of the threshold t and two combinations of
sense links from UBY. Depending on the threshold
t, 2% to 55% of the verb instances in SemCor can
automatically be labelled, and the instance cov-
erage goes largely in parallel to the coverage of
predicted WN senses. Accuracy ranges between
32% and 47% and exceeds the random sense base-
line by a large margin. Lowering the threshold in-
creases the coverage of the labelling method, but
it also leads to a decrease in accuracy of 9 percent-
age points (12 for the configuration with VN).
Adding more patterns from VN via the WN–
VN alignment, leads to a decrease in both instance
and sense coverage combined with an increase in
accuracy. Since SemCor is a rather small corpus,
the increase in instance coverage is not as clear
as for large Web corpora such as the ukWaC cor-
pus. Labelling a 1GB subset of the ukWaC cor-
pus based on patterns derived from the WN–FN–
WKT alignments resulted in 15MB of labelled
data, whereas 25MB labelled data could be created
from the same subset with the additional patterns
from the WN–VN alignment.
Qualitative Analysis. In Table 4, we show ex-
amples of the highest ranking patterns and the cor-
responding labelled SemCor instances for senses
that were correctly and falsely annotated. The ex-
amples in Table 4 show that the similarity metric
assigns the highest values to instances where func-
tion words (e.g., in, to, who) or POS tags (e.g., PP,
VV) from the ASP vocabulary occur in the im-
mediate neighbourhood of the target verb. Since
such functions words play an important role in the
ASPs derived from VN, the VN ASPs possibly
tend to dominate over the SPs derived from sense
examples, which explains the observed decrease in
coverage (see Table 3).
The falsely labelled instances turn out to be ex-
amples of WN senses where the gold sense is very
similar to the automatically attached sense as evi-
dent from the synset definition given in the right-
most column.
</bodyText>
<subsectionHeader confidence="0.979391">
4.2 Extrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.9999575">
We extrinsically evaluate our method for auto-
mated verb sense labelling by using it for learning
a classifier for VSD in a train-test setting. We use
features which have been widely used in super-
vised WSD systems, in particular features based
on dependency parsing. While this might seem
to be in contrast to our labelling algorithm which
is based on shallow linguistic preprocessing, it is
fully justified by the purpose of our extrinsic eval-
uation: The main purpose of the extrinsic evalua-
tion is not to outperform state-of-the-art VSD sys-
tems, but to show that, when operating with rea-
sonable features, a classifier trained on the data
automatically labelled with our method performs
equally well as when this classifier is trained on
manually annotated data.
</bodyText>
<sectionHeader confidence="0.598317" genericHeader="method">
4.2.1 Features
</sectionHeader>
<bodyText confidence="0.99990375">
The training and test data are parsed with the Stan-
ford parser (Klein and Manning, 2003) which pro-
vides Stanford Dependencies output (De Marneffe
et al., 2006) as well as phrase structure trees. We
employ the Stanford Named Entity Recogniser to
identify named entities. We then extract lexical,
syntactic, and semantic features from the parse re-
sults for classification.
Lexical features include the lemmas and POS
tags of the two words before and after the tar-
get verb. To extract syntactic features we select
all dependency relations from the parser output in
</bodyText>
<page confidence="0.991615">
72
</page>
<bodyText confidence="0.9305285">
SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets)
Some of the New York Philharmonic of group person who live 0.29 live%2:42:08:: (live%2:42:08::)
musicians who live in the suburbs spent in location VVD time time
yesterday morning digging themselves VVG
free from snow.
These societies can expect to face diffi- group expect to VV JJ 0.22 expect%2:31:01:: (expect%2:31:01::)
cult times. event
As autumn starts its annual sweep, few JJ attribute JJ person real- 0.22 realize%2:31:00:: – perceive (an idea or
Americans and Canadians realize how ize how JJ PP be in situation) mentally (realize%2:31:01::
fortunate they are in having the world ’s – be fully aware or cognizant of)
</bodyText>
<table confidence="0.30615325">
finest fall coloring.
Dan Morgan told himself he would for- person person VVD PP PP 0.16 forget%2:31:00:: – be unable to re-
get Ann Turner. forget person location member (forget%2:31:01:: – dismiss
from the mind; stop remembering)
</table>
<tableCaption confidence="0.9174205">
Table 4: Examples of SemCor instances with high similarity scores (upper half shows correctly labelled
instances, lower half incorrectly labelled instances.
</tableCaption>
<bodyText confidence="0.9999215">
which the target verb is related to a noun, a pro-
noun, or a named entity. For each selected word,
the lemma of the word (or the named entity tag in
case of proper nouns) is combined with the type
of the dependency relation which exists between
it and the verb to form a separate feature. In a
similar feature, the lemma of the selected word is
replaced by its POS tag. The semantic features
include all synsets found in WN for nominal argu-
ments of the verb. Personal pronouns are mapped
to ‘person’ and the three synsets found in WN 3.0
for this word are taken as features.
</bodyText>
<subsubsectionHeader confidence="0.724734">
4.2.2 Train and Test Data
</subsubsectionHeader>
<bodyText confidence="0.999970727272727">
Using exactly the same method as intrinsically
evaluated in section 4.1, we automatically labelled
occurrences of the 16 MASC verbs and the 305
Senseval-3 verbs in both web corpora with WN
senses. Only occurrences with similarity score
above 0.1 are labelled – all other occurrences are
discarded. We refer to the resulting data as au-
tomatically labelled corpus (ALC) and use it as
training data for statistical VSD.
Instances of the test verbs found in SemCor are
also used as training data in order to compare the
performance of the classifier in a fully supervised
setting.
MASC. There are 22 senses with instances in
MASC which are not found in SemCor. For the
ALC this number is 34. However, in the latter
there are 27 senses, instances of which are un-
seen in MASC. 20 of those represent phrasal verbs
which we attribute to the special treatment of such
verbs in our method.
The classifier cannot correctly classify senses
which are not seen in the training data. The cov-
erage of the ALC is 88.05% and that of SemCor
— 94.8%. The SemCor data can mainly cover
more test instances of 3 verbs — launch, rule, and
transfer — the WN senses of which lack sense
examples or links to other senses in UBY. Un-
like the hand-labelled SemCor data, our automated
sense labelling method is limited to the informa-
tion found in the LLR used. However, there are
also 330 MASC instances covered by the ALC
only. Those are mostly instances of phrasal verbs,
such as rip off and show up. Note that the defini-
tion of coverage we use here makes its values the
upper bounds for the performance of the classifier.
Senseval-3. We also generated training data au-
tomatically for the 305 Senseval verbs. However,
only 152 of those verbs (442 instances) are found
in SemCor. This means we cannot train the classi-
fier for the remaining Senseval verbs. The cover-
age of the SemCor training data for the 152 verbs
which can be classified is 96.15% and that of the
ALC — 95.25%. For all 592 Senseval test in-
stances, the coverage of the ALC is 90.38%.
</bodyText>
<subsectionHeader confidence="0.559182">
4.2.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.9999906">
We trained a separate logistic regression classi-
fier for each test verb in the two datasets us-
ing the WEKA data mining software (Hall et al.,
2009) with default parameters. The classifiers
were trained with features extracted from (i) the
SemCor hand-labelled data and (ii) the ALC.
MASC. The classifier achieves 50.23% accu-
racy when SemCor is used and 49% when the
ALC is employed. The difference in the results is
not statistically significant at p &lt; 0.05. The MFS
</bodyText>
<page confidence="0.997522">
73
</page>
<bodyText confidence="0.999937928571428">
baseline scores at 41.72%.
Senseval-3. The classifier achieves 43.24%
with the ALC. We assigned the MFS to each of
the 143 test verbs not found in SemCor since we
cannot train the classifier for those. The achieved
accuracy is 45.2%. We also measured accuracy
in a setup where no MFS back-off strategy was
employed for SemCor (152 test verbs with 442
instances). When trained on SemCor data, the
classifier achieves 48.64% accuracy compared to
47.51% for the ALC. All differences in the results
are not statistically significant at p &lt; 0.05. Fi-
nally, the MFS baseline accuracy is significantly
lower at 25.34% for all 305 test verbs.
For both test datasets, the overall performance
of the classifier when trained on automatically la-
belled data is very close to the setting in which
manually created training data is employed. We
thus conclude that the quality of the data produced
by our sense labelling method is sufficient and
these data can be directly used for training a statis-
tical VSD classifier. As a reference, the state-of-
the-art supervised VSD system described in Chen
and Palmer (2009) achieves 64.8% accuracy on the
Senseval-2 fine-grained data. However, we cannot
compare to this result due to the different sense in-
ventory which the Senseval-2 data were annotated
with.
</bodyText>
<subsectionHeader confidence="0.782257">
4.2.4 Sense Links
</subsectionHeader>
<bodyText confidence="0.9999764">
In order to investigate the effect of LLRs, we
performed experiments in which sense examples
found in WN only were used. We also experi-
mented with various combinations of the resources
available in UBY to determine the contribution of
each of those to our method. Table 5 shows the re-
sults. The setting which includes only WN has the
worst performance, thus clearly showing the ben-
efits of using LLRs. Next, the inclusion of WKT
improves both coverage and accuracy. We con-
clude that WKT plays an important role in discov-
ering additional verb senses. Finally, similarly to
the results of the intrinsic evaluation, adding VN
to the mix increases slightly the coverage but de-
creases accuracy.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="method">
5 Related Work and Discussion
</sectionHeader>
<bodyText confidence="0.9635846">
Our work is related to previous research on
(i) using a combination of lexical resources for
knowledge-based WSD, (ii) using lexical re-
sources for distant supervision, and (iii) the auto-
mated acquisition of sense-annotated data.
</bodyText>
<table confidence="0.981738">
MASC Senseval
Cov Acc Cov Acc
WN 0.6573 0.3498 0.6372 0.3209
WN–FN 0.8562 0.4810 0.8812 0.4172
WN–FN–WKT 0.8805 0.4900 0.9038 0.4324
WN–FN–WKT–VN 0.8822 0.4688 0.9139 0.4054
</table>
<tableCaption confidence="0.897401">
Table 5: Performance of the various combinations
of lexical resources.
</tableCaption>
<bodyText confidence="0.999089333333334">
Knowledge-based WSD. While the combina-
tion of sense-annotated data and wordnets has
been described for knowledge-based WSD before
(e.g., Navigli and Velardi (2005; Agirre and Soroa
(2009) who use graph algorithms), only recently
Ponzetto and Navigli (2010) have investigated the
impact of the combination of different lexical re-
sources on the performance of WSD. They aligned
WN senses with Wikipedia articles and employed
two simple knowledge-based algorithms, i.e., a
Lesk-based algorithm and a graph-based algo-
rithm, to evaluate the resulting LLR for WSD.
While their evaluation demonstrates that the use
of an LLR boosts the performance of knowledege-
based WSD, it is restricted to nouns only since
Wikipedia provides very few verb senses. More-
over, lexical resources that are rich in lexical-
syntactic information such as VN have not been
involved.
Miller et al. (2012) employ a Lesk-based algo-
rithm which makes use of a combination of WN
and an automatically acquired distributional the-
saurus. Lesk-based algorithms play a central role
in knowledge-based WSD. Based on the overlap
of the context of the target word and sense defi-
nitions in a given sense inventory, they assign the
sense with the highest overlap as disambiguation
result. We were kindly provided with the system
described in Miller et al. (2012) and we were able
to test its performance on our test sets. The sys-
tem achieved only 33.86% and 30.16% accuracy
for the MASC and the Senseval-3 verbs, respec-
tively, which is far below the results we presented.
This low performance is due to the fact that Lesk-
based algorithms do not account for word order.
Such information is important especially for verb
senses, as the syntactic behaviour of a verb reflects
aspects of its meaning.
Distant supervision. Distant supervision is
a learning paradigm similar to semi-supervised
learning. Unlike semi-supervised methods which
typically employ a supervised classifier and a
</bodyText>
<page confidence="0.996779">
74
</page>
<bodyText confidence="0.999911315068494">
small number of seed instances to do bootstrap
learning (Yarowsky, 1995; Mihalcea, 2004; Fujita
and Fujino, 2011), in distant supervision training
data are created in a single run from scratch by
aligning corpus instances with entries in a knowl-
edge base. Distant supervision methods that have
used LLRs as knowledge bases have been previ-
ously applied in relation extraction, e.g. Freebase
(Mintz et al., 2009; Surdeanu et al., 2012) and Ba-
belNet (Krause et al., 2012; Moro et al., 2013).
However, as far as we are aware, we are the first to
apply distant supervision to the task of verb sense
disambiguation.
Acquisition of sense-annotated data. Most
previous work on using lexical resources for au-
tomatically acquiring sense-annotated data either
was mostly restricted to noun senses or, unlike
us, did not present a meaningful evaluation. Lea-
cock et al. (1998) describe the automated creation
of training data for supervised WSD on the ba-
sis of WN as a lexical resource combined with
corpus statistics, but they evaluate their approach
just on one noun, verb, and adjective, and thus
it is unclear whether their results can be gener-
alized. Cuadros and Rigau (2008) used the ap-
proach of Leacock et al. (1998) to automatically
build a large KnowNet from the Web, but they
evaluated this resource only for WSD of nouns.
However, the system based on KnowNet yields re-
sults below the SemCor-MFS baseline. Mihalcea
and Moldovan (1999) use WordNet glosses to ex-
tract sense examples from the Web via a search en-
gine and use this approach in a subsequent paper
(Mihalcea, 2002) to generate a sense tagged cor-
pus. For five randomly selected nouns, they per-
formed a comparative evaluation of a WSD classi-
fier trained on an automatically tagged corpus on
the one hand, and on the manually annotated data
from the Senseval-2 English lexical sample task
on the other hand. The results obtained for these
five nouns seem to be similar but the dataset used
is too small to draw meaningful conclusions and
moreover, it does not cover verbs. Mostow and
Duan (2011) presented a system that extracts ex-
ample contexts for nouns and apply these contexts
in (Duan and Yates, 2010) for WSD by using them
to label text and train a statistical classifier. An
evaluation of this classifier yielded results similar
to those obtained by a supervised WSD system.
K¨ubler and Zhekova (2009) extract example
sentences from several English dictionaries and
various types of corpora, including web corpora.
They employ a Lesk-based algorithm to automati-
cally annotate the target word instances in the ex-
tracted example sentences with WN senses and
use them in one of their experiments as train-
ing data for a WSD classifier. However, the per-
formance of the system decreased significantly
achieving the lowest accuracy among all system
configurations. The authors provide only the over-
all accuracy score, so we do not know how disam-
biguation of verbs was affected.
Summary. We consider the ability to estab-
lish a link between the rich knowledge available in
LLRs and corpora of any kind to be the main ad-
vantage of our automated labelling method. How-
ever, to automatically label a suffcient amount
of data for supervised learning, very large cor-
pora are required. Our method can be extended
to other POS (using sense examples and possibly
other types of lexical information), as well as to
other languages where (linked) lexical resources
are available.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999658">
In this paper, we presented a novel method for cre-
ating sense labelled corpora automatically. We ex-
ploit LLRs and perform large-scale intrinsic and
application-based evaluations. The results of those
evaluations show that the quality of the sense la-
belled corpora created with our method matches
that of manually annotated corpora.
In future research, we plan to use PropBank
(Palmer et al., 2005) in order to extract sense
examples for VN as well. This might improve
the performance of lexical resource combinations
which include VN. We will also apply our method
to languages (e.g., German) for which lexical re-
sources are available but no or little sense anno-
tated corpora exist.
</bodyText>
<sectionHeader confidence="0.998292" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990937666666667">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg- Professor-
ship Program under grant No. I/82806 and by the
German Research Foundation under grant No. GU
798/9-1. We would like to thank the anonymous
reviewers for their valuable feedback.
</bodyText>
<page confidence="0.998357">
75
</page>
<sectionHeader confidence="0.985693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988760220183486">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2009), pages 33–41, Athens, Greece.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics-Volume 1, pages
86–90, Montreal, Canada.
Timothy Baldwin, Sunam Kim, Francis Bond, Sanae
Fujita, David Martinez, and Takaaki Tanaka. 2010.
A Reexamination of MRD-Based Word Sense Dis-
ambiguation. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 9(1):4:1–
4:21.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209–226.
Jinying Chen and Martha Palmer. 2009. Improv-
ing English verb sense disambiguation performance
with linguistically motivated features and clear sense
distinction boundaries. Language Resources and
Evaluation, 43:181–208.
Montse Cuadros and German Rigau. 2008. Knownet:
Building a large net of knowledge from the web.
In 22nd International Conference on Computational
Linguistics (COLING), pages 161–168, Manchester,
UK.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC 2006), pages 449–454, Genoa,
Italy.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
627–635, Los Angeles, USA.
Judith Eckle-Kohler and Iryna Gurevych. 2012.
Subcat-LMF: Fleshing out a standardized format for
subcategorization frame interoperability. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 550–560, Avignon,
France.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-
mann, Michael Matuschek, and Christian M. Meyer.
2012. UBY-LMF – A uniform format for standard-
izing heterogeneous lexical-semantic resources in
ISO-LMF. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 275–282, Istanbul, Turkey.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Sanae Fujita and Akinori Fujino. 2011. Word sense
disambiguation by combining labeled data expan-
sion and semi-supervised learning method. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 676–685,
Chiang Mai, Thailand.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 580–590.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10–
18.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42:21–40.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423–430, Sapporo,
Japan. Association for Computational Linguistics.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 180–183, Edmonton,
Canada.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International Se-
mantic Web Conference, pages 263–278, Boston,
Masachusetts, USA, 11. Springer.
Sandra K¨ubler and Desislava Zhekova. 2009. Semi-
Supervised Learning for Word Sense Disambigua-
tion: Quality vs. Quantity. In Proceedings of the
International Conference RANLP-2009, pages 197–
202, Borovets, Bulgaria.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and word-
net relations for sense identification. Computational
Linguistics, 24(1):147–165.
</reference>
<page confidence="0.848233">
76
</page>
<reference confidence="0.997917239130435">
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference on
Systems Documentation, pages 24–26, Toronto, On-
tario, Canada.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Rada Mihalcea and Dan Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of the American Association for Ar-
tificial Intelligence (AAAI 1999), Orlando, Florida,
USA.
Rada Mihalcea. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the Third Interna-
tional Conference of Language Resources and Eval-
uation (LREC 2002), pages 1407–1411, Las Palmas,
Canary Islands, Spain.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings
of the Conference on Computational Natural Lan-
guage Learning (CoNLL-2004), Boston, MA, USA.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of the 24th
International Conference on Computational Lin-
guistics (COLING 2012), pages 1781–1796, Mum-
bai, India.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore.
Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu,
Roberto Navigli, and Hans Uszkoreit. 2013. Se-
mantic rule filtering for web-scale relation extrac-
tion. In Proceedings of the 12th International
Semantic Web Conference, Sydney, Australia, 10.
Springer.
Jack Mostow and Weisi Duan. 2011. Generating ex-
ample contexts to illustrate a target word sense. In
Proceedings of the Sixth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 105–110, Portland, Oregon, June. Association
for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075–1086.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1):71–
105.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1522–1531, Uppsala,
Sweden.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49, Manchester, UK.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465, Jeju Island, Korea.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd annual meeting on Associa-
tion for Computational Linguistics, pages 189–196,
Cambridge, Massachusetts, USA.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the 6th
International Conference on Language Resources
and Evaluation (LREC 2008), volume 8, pages
1646–1652, Marrakech, Morocco.
</reference>
<page confidence="0.999128">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407278">
<title confidence="0.984838">Automated Verb Sense Labelling Based on Linked Lexical Resources</title>
<author confidence="0.975944">Judith Iryna</author>
<email confidence="0.544904">1Humboldt-Universit¨atzuBerlin,kostadin.cholakov@anglistik.hu-berlin.de</email>
<affiliation confidence="0.91088425">2Ubiquitous Knowledge Processing Lab Dept. of Computer Science, Technische Universit¨at Darmstadt 3Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational</affiliation>
<web confidence="0.978782">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.999223764705882">We present a novel approach for creating sense annotated corpora automatically. Our approach employs shallow syntacticosemantic patterns derived from linked lexical resources to automatically identify instances of word senses in text corpora. We evaluate our labelling method intrinsically on SemCor and extrinsically by using automatically labelled corpus text to train a classifier for verb sense disambiguation. Testing this classifier on verbs from the English MASC corpus and on verbs from the Senseval-3 all-words disambiguation task shows that it matches the performance of a classifier which has been trained on manually annotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="28428" citStr="Agirre and Soroa (2009)" startWordPosition="4653" endWordPosition="4656">ch on (i) using a combination of lexical resources for knowledge-based WSD, (ii) using lexical resources for distant supervision, and (iii) the automated acquisition of sense-annotated data. MASC Senseval Cov Acc Cov Acc WN 0.6573 0.3498 0.6372 0.3209 WN–FN 0.8562 0.4810 0.8812 0.4172 WN–FN–WKT 0.8805 0.4900 0.9038 0.4324 WN–FN–WKT–VN 0.8822 0.4688 0.9139 0.4054 Table 5: Performance of the various combinations of lexical resources. Knowledge-based WSD. While the combination of sense-annotated data and wordnets has been described for knowledge-based WSD before (e.g., Navigli and Velardi (2005; Agirre and Soroa (2009) who use graph algorithms), only recently Ponzetto and Navigli (2010) have investigated the impact of the combination of different lexical resources on the performance of WSD. They aligned WN senses with Wikipedia articles and employed two simple knowledge-based algorithms, i.e., a Lesk-based algorithm and a graph-based algorithm, to evaluate the resulting LLR for WSD. While their evaluation demonstrates that the use of an LLR boosts the performance of knowledegebased WSD, it is restricted to nouns only since Wikipedia provides very few verb senses. Moreover, lexical resources that are rich in</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2009), pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2987" citStr="Baker et al., 1998" startWordPosition="416" endWordPosition="419">cale LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3, or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4, FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems. Our first contribution (i) also addresses a considerable gap in recent research regarding automated sense labelling of verbs. Most previous work is done on nouns. However, verbs pose a bigger challenge due to their high poly</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Sunam Kim</author>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
<author>David Martinez</author>
<author>Takaaki Tanaka</author>
</authors>
<title>A Reexamination of MRD-Based Word Sense Disambiguation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>9</volume>
<issue>1</issue>
<pages>4--21</pages>
<contexts>
<context position="13930" citStr="Baldwin et al. (2010)" startWordPosition="2208" endWordPosition="2211">xk) to MostSimilarSenses(ASPi) if maxSimi,j &gt;= threshold t and maxSimi,j &gt;= maxSimi,k label(si) = random(MostSimilarSenses(LSPi)) else if maxSimi,k &gt;= threshold t label(si) = random(MostSimilarSenses(ASPi)) end if end for Table 2: Algorithm for labelling corpus instances with WordNet senses. grams occurring in SP pi, and normw is the normalisation factor defined by the sum of the maximum number of common bigrams, trigrams and fourgrams in the window w. Similarity metrics based on Dice’s coefficient have often been used in Lesk-based WSD (Lesk, 1986) to calculate the overlap of two sets (e.g., Baldwin et al. (2010)). In our case, however, the elements of the two sets are bigrams, trigrams and fourgrams, while in Leskbased algorithms typically sets of unigrams are compared, thus not accounting for word order. Table 2 shows the algorithm used for automated labelling of corpus instances in pseudo-code. The algorithm assumes that for each verb v, the corresponding set of SPs derived from UBY sense examples (UBY-LSP(v) and UBY-ASP(v) in Table 2) has already been computed. For each corpus sentence containing a target verb v, the corresponding SPs for verb v derived from UBY are scored by the similarity metric</context>
</contexts>
<marker>Baldwin, Kim, Bond, Fujita, Martinez, Tanaka, 2010</marker>
<rawString>Timothy Baldwin, Sunam Kim, Francis Bond, Sanae Fujita, David Martinez, and Takaaki Tanaka. 2010. A Reexamination of MRD-Based Word Sense Disambiguation. ACM Transactions on Asian Language Information Processing (TALIP), 9(1):4:1– 4:21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="15017" citStr="Baroni et al., 2009" startWordPosition="2404" endWordPosition="2408">ach corpus sentence containing a target verb v, the corresponding SPs for verb v derived from UBY are scored by the similarity metric in (1). The SPs with the maximum score that is above a threshold t form the set of most similar senses. From this set, the algorithm picks one sense randomly as a label. How often this happens, depends on the value of t: the percentage of randomly selected senses ranges from about 33% for t = 0.14 to about 50% for t = 0.04. 3 Data Web corpora. For the automated labelling of corpus data with WN senses, we use two very large web corpora: the English ukWaC corpus (Baroni et al., 2009) and the article pages extracted from the English Wikipedia using the Java-based Wikipedia API JWPL (Zesch et al., 2008). Further, for the evaluation of our method, we use three manually sense annotated data sets. SemCor. We use the SemCor 3.0 corpus which is annotated with WN 3.0 senses. MASC. MASC is a balanced subset of 500K words of written texts and transcribed speech drawn primarily from the Open American National Corpus (OANC).10 The texts come from 19 different genres which allows us to test our method on real-life data from multiple sources. The corpus is annotated with various types </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Improving English verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--181</pages>
<contexts>
<context position="26841" citStr="Chen and Palmer (2009)" startWordPosition="4401" endWordPosition="4404">nces in the results are not statistically significant at p &lt; 0.05. Finally, the MFS baseline accuracy is significantly lower at 25.34% for all 305 test verbs. For both test datasets, the overall performance of the classifier when trained on automatically labelled data is very close to the setting in which manually created training data is employed. We thus conclude that the quality of the data produced by our sense labelling method is sufficient and these data can be directly used for training a statistical VSD classifier. As a reference, the state-ofthe-art supervised VSD system described in Chen and Palmer (2009) achieves 64.8% accuracy on the Senseval-2 fine-grained data. However, we cannot compare to this result due to the different sense inventory which the Senseval-2 data were annotated with. 4.2.4 Sense Links In order to investigate the effect of LLRs, we performed experiments in which sense examples found in WN only were used. We also experimented with various combinations of the resources available in UBY to determine the contribution of each of those to our method. Table 5 shows the results. The setting which includes only WN has the worst performance, thus clearly showing the benefits of usin</context>
</contexts>
<marker>Chen, Palmer, 2009</marker>
<rawString>Jinying Chen and Martha Palmer. 2009. Improving English verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries. Language Resources and Evaluation, 43:181–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>Knownet: Building a large net of knowledge from the web.</title>
<date>2008</date>
<booktitle>In 22nd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>161--168</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="31346" citStr="Cuadros and Rigau (2008)" startWordPosition="5126" endWordPosition="5129">pply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on </context>
</contexts>
<marker>Cuadros, Rigau, 2008</marker>
<rawString>Montse Cuadros and German Rigau. 2008. Knownet: Building a large net of knowledge from the web. In 22nd International Conference on Computational Linguistics (COLING), pages 161–168, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.C. De Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pages 449–454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weisi Duan</author>
<author>Alexander Yates</author>
</authors>
<title>Extracting glosses to disambiguate word senses.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>627--635</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="32339" citStr="Duan and Yates, 2010" startWordPosition="5299" endWordPosition="5302">ubsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by using them to label text and train a statistical classifier. An evaluation of this classifier yielded results similar to those obtained by a supervised WSD system. K¨ubler and Zhekova (2009) extract example sentences from several English dictionaries and various types of corpora, including web corpora. They employ a Lesk-based algorithm to automatically annotate the target word instances in the extracted example sentences with WN senses and use them in one of their experiments as training data for a WSD classifier. However, the performance of the system decreased significantly achi</context>
</contexts>
<marker>Duan, Yates, 2010</marker>
<rawString>Weisi Duan and Alexander Yates. 2010. Extracting glosses to disambiguate word senses. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 627–635, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
<author>Iryna Gurevych</author>
</authors>
<title>Subcat-LMF: Fleshing out a standardized format for subcategorization frame interoperability.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012),</booktitle>
<pages>550--560</pages>
<location>Avignon, France.</location>
<marker>Eckle-Kohler, Gurevych, 2012</marker>
<rawString>Judith Eckle-Kohler and Iryna Gurevych. 2012. Subcat-LMF: Fleshing out a standardized format for subcategorization frame interoperability. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 550–560, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
<author>Iryna Gurevych</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
</authors>
<title>UBY-LMF – A uniform format for standardizing heterogeneous lexical-semantic resources in ISO-LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<pages>275--282</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="6888" citStr="Eckle-Kohler et al., 2012" startWordPosition="1029" endWordPosition="1032">tifiers from an LLR. SPs can be derived from corpus instances and from information given in LLRs, in particular, sense examples and more abstract predicate argument structure information. 2.2 Step 1: Creation of SPs from LLRs For the creation of SPs, we employ the large-scale LLR UBY which combines 10 lexical resources for English and German to make use of the enriched verb sense representations provided by the sense links between various resources available in UBY. Although our method can work with any LLR, we choose UBY because the various resources are represented in a standardised format (Eckle-Kohler et al., 2012) and sense links between them can uniformly and conveniently be accessed via the freely available UBY-API.5 Since we evaluate our method on data annotated with WN senses, we create SPs for enriched WN senses (see example given in Table 1). We enrich WN senses by aggregating lexical information that can be accessed through links given in UBY to corresponding verb senses in other resources. In this setting, enrichment means that we make use of sense examples from WN, from FN via the WN–FN linking, and from WKT via the WN–WKT linking. In addition, we use abstract predicate-argument structure info</context>
</contexts>
<marker>Eckle-Kohler, Gurevych, Hartmann, Matuschek, Meyer, 2012</marker>
<rawString>Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann, Michael Matuschek, and Christian M. Meyer. 2012. UBY-LMF – A uniform format for standardizing heterogeneous lexical-semantic resources in ISO-LMF. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), pages 275–282, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1624" citStr="Fellbaum, 1998" startWordPosition="223" endWordPosition="224"> shows that it matches the performance of a classifier which has been trained on manually annotated data. 1 Introduction Sense annotated corpora are important resources in NLP as they can be used as training data (e.g., for word sense disambiguation (WSD) or semantic role labelling) or as sources for the acquisition of lexical information (e.g., selectional preference information). Typically, a particular sense inventory from a lexical resource is used to annotate some or all words with word senses from this sense inventory. For instance, various sense-annotated corpora based on WordNet (WN; (Fellbaum, 1998)) exist, such as the data from the Senseval competitions,1 or the SemCor corpus.2 Such corpora are usually created manually which is expensive and time consuming. Furthermore, the corpora are often domain specific (e.g. newspaper texts) which makes statistical systems trained on them strongly biased. We present a novel approach for creating sense annotated corpora automatically. Our approach 1http://www.senseval.org 2http://www.cse.unt.edu/˜rada/ downloads.html#semcor employs shallow syntactico-semantic patterns derived from linked lexical resources (LLRs) to automatically identify instances o</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Akinori Fujino</author>
</authors>
<title>Word sense disambiguation by combining labeled data expansion and semi-supervised learning method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>676--685</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="30292" citStr="Fujita and Fujino, 2011" startWordPosition="4951" endWordPosition="4954">for the MASC and the Senseval-3 verbs, respectively, which is far below the results we presented. This low performance is due to the fact that Leskbased algorithms do not account for word order. Such information is important especially for verb senses, as the syntactic behaviour of a verb reflects aspects of its meaning. Distant supervision. Distant supervision is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically ac</context>
</contexts>
<marker>Fujita, Fujino, 2011</marker>
<rawString>Sanae Fujita and Akinori Fujino. 2011. Word sense disambiguation by combining labeled data expansion and semi-supervised learning method. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 676–685, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>UBY - a large-scale unified lexical-semantic resource based on LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>580--590</pages>
<contexts>
<context position="2817" citStr="Gurevych et al., 2012" startWordPosition="389" endWordPosition="392">atically identify instances of word senses in text corpora. We significantly extend previous work on this task by making two important contributions: (i) we employ a large-scale LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3, or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4, FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems. Our first contribution (i) also addresses a considerab</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. UBY - a large-scale unified lexical-semantic resource based on LMF. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 580–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="25421" citStr="Hall et al., 2009" startWordPosition="4163" endWordPosition="4166"> the classifier. Senseval-3. We also generated training data automatically for the 305 Senseval verbs. However, only 152 of those verbs (442 instances) are found in SemCor. This means we cannot train the classifier for the remaining Senseval verbs. The coverage of the SemCor training data for the 152 verbs which can be classified is 96.15% and that of the ALC — 95.25%. For all 592 Senseval test instances, the coverage of the ALC is 90.38%. 4.2.3 Results and Analysis We trained a separate logistic regression classifier for each test verb in the two datasets using the WEKA data mining software (Hall et al., 2009) with default parameters. The classifiers were trained with features extracted from (i) the SemCor hand-labelled data and (ii) the ALC. MASC. The classifier achieves 50.23% accuracy when SemCor is used and 49% when the ALC is employed. The difference in the results is not statistically significant at p &lt; 0.05. The MFS 73 baseline scores at 41.72%. Senseval-3. The classifier achieves 43.24% with the ALC. We assigned the MFS to each of the 143 test verbs not found in SemCor since we cannot train the classifier for those. The achieved accuracy is 45.2%. We also measured accuracy in a setup where </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. ACM SIGKDD Explorations Newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of English verbs. Language Resources and Evaluation,</title>
<date>2008</date>
<pages>42--21</pages>
<contexts>
<context position="3027" citStr="Kipper et al., 2008" startWordPosition="423" endWordPosition="426">se annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3, or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4, FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems. Our first contribution (i) also addresses a considerable gap in recent research regarding automated sense labelling of verbs. Most previous work is done on nouns. However, verbs pose a bigger challenge due to their high polysemy and the fact that, unlike nouns, sy</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation, 42:21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan.</location>
<contexts>
<context position="21053" citStr="Klein and Manning, 2003" startWordPosition="3408" endWordPosition="3411"> on dependency parsing. While this might seem to be in contrast to our labelling algorithm which is based on shallow linguistic preprocessing, it is fully justified by the purpose of our extrinsic evaluation: The main purpose of the extrinsic evaluation is not to outperform state-of-the-art VSD systems, but to show that, when operating with reasonable features, a classifier trained on the data automatically labelled with our method performs equally well as when this classifier is trained on manually annotated data. 4.2.1 Features The training and test data are parsed with the Stanford parser (Klein and Manning, 2003) which provides Stanford Dependencies output (De Marneffe et al., 2006) as well as phrase structure trees. We employ the Stanford Named Entity Recogniser to identify named entities. We then extract lexical, syntactic, and semantic features from the parse results for classification. Lexical features include the lemmas and POS tags of the two words before and after the target verb. To extract syntactic features we select all dependency relations from the parser output in 72 SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets) Some of the New York Philharmonic of grou</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</booktitle>
<pages>180--183</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="10888" citStr="Klein et al., 2003" startWordPosition="1704" endWordPosition="1707">verbs, e.g. POS tags for personal pronouns which are potential verb arguments. In our experiments, we tried different sets of function words and POS tags. For instance, we found that some function words (e.g., reflexive pronouns) and some POS tags (e.g., those for past participles and comparative adjectives) introduced too much noise in the data and therefore we did not select them for the final vocabulary.8 In order to create SPs from sense examples, we apply POS tagging and lemmatisation using the TreeTagger (Schmid, 1994) and named entity tagging using the Stanford Named Entity Recogniser (Klein et al., 2003). The named entity tags attached by the Named Entity Recogniser are mapped to WN semantic fields. For the generation of ASPs from sense examples, we used a window size of w = 7, while the generation of LSPs has been performed with w = 5 in order to put a focus on the closely neighbouring lexemes in multiword verb lemmas. The 8The vocabulary used for the creation of ASPs is available at http://www.ukp.tu-darmstadt.de/data/. window size was set empirically using the English Lexical Sample task of the Senseval-2 dataset as a development set. The same set was also used for the development of the l</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D. Manning. 2003. Named entity recognition with character-level models. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 180–183, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Krause</author>
<author>Hong Li</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
</authors>
<title>Large-scale learning of relationextraction rules with distant supervision from the web.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Semantic Web Conference,</booktitle>
<volume>11</volume>
<pages>263--278</pages>
<publisher>Springer.</publisher>
<location>Boston, Masachusetts, USA,</location>
<contexts>
<context position="30646" citStr="Krause et al., 2012" startWordPosition="5010" endWordPosition="5013">sion is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and </context>
</contexts>
<marker>Krause, Li, Uszkoreit, Xu, 2012</marker>
<rawString>Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-scale learning of relationextraction rules with distant supervision from the web. In Proceedings of the 11th International Semantic Web Conference, pages 263–278, Boston, Masachusetts, USA, 11. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Desislava Zhekova</author>
</authors>
<title>SemiSupervised Learning for Word Sense Disambiguation: Quality vs. Quantity.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>197--202</pages>
<location>Borovets, Bulgaria.</location>
<marker>K¨ubler, Zhekova, 2009</marker>
<rawString>Sandra K¨ubler and Desislava Zhekova. 2009. SemiSupervised Learning for Word Sense Disambiguation: Quality vs. Quantity. In Proceedings of the International Conference RANLP-2009, pages 197– 202, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>George A Miller</author>
<author>Martin Chodorow</author>
</authors>
<title>Using corpus statistics and wordnet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="31042" citStr="Leacock et al. (1998)" startWordPosition="5073" endWordPosition="5077">owledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract </context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>Claudia Leacock, George A. Miller, and Martin Chodorow. 1998. Using corpus statistics and wordnet relations for sense identification. Computational Linguistics, 24(1):147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="13864" citStr="Lesk, 1986" startWordPosition="2198" endWordPosition="2199"> maxSim(ASPi) = argmaxkscore(ASPi, ASPk) add sense(argmaxk) to MostSimilarSenses(ASPi) if maxSimi,j &gt;= threshold t and maxSimi,j &gt;= maxSimi,k label(si) = random(MostSimilarSenses(LSPi)) else if maxSimi,k &gt;= threshold t label(si) = random(MostSimilarSenses(ASPi)) end if end for Table 2: Algorithm for labelling corpus instances with WordNet senses. grams occurring in SP pi, and normw is the normalisation factor defined by the sum of the maximum number of common bigrams, trigrams and fourgrams in the window w. Similarity metrics based on Dice’s coefficient have often been used in Lesk-based WSD (Lesk, 1986) to calculate the overlap of two sets (e.g., Baldwin et al. (2010)). In our case, however, the elements of the two sets are bigrams, trigrams and fourgrams, while in Leskbased algorithms typically sets of unigrams are compared, thus not accounting for word order. Table 2 shows the algorithm used for automated labelling of corpus instances in pseudo-code. The algorithm assumes that for each verb v, the corresponding set of SPs derived from UBY sense examples (UBY-LSP(v) and UBY-ASP(v) in Table 2) has already been computed. For each corpus sentence containing a target verb v, the corresponding S</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation, pages 24–26, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9970" citStr="Marcus et al., 1993" startWordPosition="1551" endWordPosition="1554">ation] Table 1: Examples of SPs derived from an enriched WN sense in UBY. PP, JJ, and VV are POS tags from the Penn Treebank tagset, standing for personal pronoun, adjective and full verb. verbal multiword expressions in a corpus, whereas ASPs are necessary to identify productively used verb senses that are constrained in their use only by their syntactic behaviour and particular semantic properties, such as selectional preferences on their arguments. The fixed vocabulary used for the creation of ASPs consists of (i) the target verb lemma, (ii) selected POS tags from the Penn Treebank Tagset (Marcus et al., 1993), (iii) a list of particular function words that play an important role in finegrained subcategorisation frames of verbs (EckleKohler and Gurevych, 2012) and (iv) semantic categories of nouns given by WN semantic fields. We selected POS tags that play an important role in syntactic realisations of verbs, e.g. POS tags for personal pronouns which are potential verb arguments. In our experiments, we tried different sets of function words and POS tags. For instance, we found that some function words (e.g., reflexive pronouns) and some POS tags (e.g., those for past participles and comparative adj</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence (AAAI 1999),</booktitle>
<location>Orlando, Florida, USA.</location>
<contexts>
<context position="31610" citStr="Mihalcea and Moldovan (1999)" startWordPosition="5171" endWordPosition="5174">d not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Most</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings of the American Association for Artificial Intelligence (AAAI 1999), Orlando, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference of Language Resources and Evaluation (LREC</booktitle>
<pages>1407--1411</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="31750" citStr="Mihalcea, 2002" startWordPosition="5199" endWordPosition="5200">lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Bootstrapping large sense tagged corpora. In Proceedings of the Third International Conference of Language Resources and Evaluation (LREC 2002), pages 1407–1411, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="30266" citStr="Mihalcea, 2004" startWordPosition="4949" endWordPosition="4950">30.16% accuracy for the MASC and the Senseval-3 verbs, respectively, which is far below the results we presented. This low performance is due to the fact that Leskbased algorithms do not account for word order. Such information is important especially for verb senses, as the syntactic behaviour of a verb reflects aspects of its meaning. Distant supervision. Distant supervision is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical reso</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2004), Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1781--1796</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="29113" citStr="Miller et al. (2012)" startWordPosition="4761" endWordPosition="4764">010) have investigated the impact of the combination of different lexical resources on the performance of WSD. They aligned WN senses with Wikipedia articles and employed two simple knowledge-based algorithms, i.e., a Lesk-based algorithm and a graph-based algorithm, to evaluate the resulting LLR for WSD. While their evaluation demonstrates that the use of an LLR boosts the performance of knowledegebased WSD, it is restricted to nouns only since Wikipedia provides very few verb senses. Moreover, lexical resources that are rich in lexicalsyntactic information such as VN have not been involved. Miller et al. (2012) employ a Lesk-based algorithm which makes use of a combination of WN and an automatically acquired distributional thesaurus. Lesk-based algorithms play a central role in knowledge-based WSD. Based on the overlap of the context of the target word and sense definitions in a given sense inventory, they assign the sense with the highest overlap as disambiguation result. We were kindly provided with the system described in Miller et al. (2012) and we were able to test its performance on our test sets. The system achieved only 33.86% and 30.16% accuracy for the MASC and the Senseval-3 verbs, respec</context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1781–1796, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="30588" citStr="Mintz et al., 2009" startWordPosition="4999" endWordPosition="5002">ects of its meaning. Distant supervision. Distant supervision is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Hong Li</author>
<author>Sebastian Krause</author>
<author>Feiyu Xu</author>
<author>Roberto Navigli</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Semantic rule filtering for web-scale relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 12th International Semantic Web Conference,</booktitle>
<volume>10</volume>
<publisher>Springer.</publisher>
<location>Sydney, Australia,</location>
<contexts>
<context position="30666" citStr="Moro et al., 2013" startWordPosition="5014" endWordPosition="5017">radigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus </context>
</contexts>
<marker>Moro, Li, Krause, Xu, Navigli, Uszkoreit, 2013</marker>
<rawString>Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu, Roberto Navigli, and Hans Uszkoreit. 2013. Semantic rule filtering for web-scale relation extraction. In Proceedings of the 12th International Semantic Web Conference, Sydney, Australia, 10. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Mostow</author>
<author>Weisi Duan</author>
</authors>
<title>Generating example contexts to illustrate a target word sense.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>105--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="32228" citStr="Mostow and Duan (2011)" startWordPosition="5280" endWordPosition="5283">999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by using them to label text and train a statistical classifier. An evaluation of this classifier yielded results similar to those obtained by a supervised WSD system. K¨ubler and Zhekova (2009) extract example sentences from several English dictionaries and various types of corpora, including web corpora. They employ a Lesk-based algorithm to automatically annotate the target word instances in the extracted example sentences with WN senses and use them in one of their experim</context>
</contexts>
<marker>Mostow, Duan, 2011</marker>
<rawString>Jack Mostow and Weisi Duan. 2011. Generating example contexts to illustrate a target word sense. In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 105–110, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="2741" citStr="Navigli and Ponzetto, 2012" startWordPosition="377" endWordPosition="380">yntactico-semantic patterns derived from linked lexical resources (LLRs) to automatically identify instances of word senses in text corpora. We significantly extend previous work on this task by making two important contributions: (i) we employ a large-scale LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3, or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4, FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="28403" citStr="Navigli and Velardi (2005" startWordPosition="4649" endWordPosition="4652"> related to previous research on (i) using a combination of lexical resources for knowledge-based WSD, (ii) using lexical resources for distant supervision, and (iii) the automated acquisition of sense-annotated data. MASC Senseval Cov Acc Cov Acc WN 0.6573 0.3498 0.6372 0.3209 WN–FN 0.8562 0.4810 0.8812 0.4172 WN–FN–WKT 0.8805 0.4900 0.9038 0.4324 WN–FN–WKT–VN 0.8822 0.4688 0.9139 0.4054 Table 5: Performance of the various combinations of lexical resources. Knowledge-based WSD. While the combination of sense-annotated data and wordnets has been described for knowledge-based WSD before (e.g., Navigli and Velardi (2005; Agirre and Soroa (2009) who use graph algorithms), only recently Ponzetto and Navigli (2010) have investigated the impact of the combination of different lexical resources on the performance of WSD. They aligned WN senses with Wikipedia articles and employed two simple knowledge-based algorithms, i.e., a Lesk-based algorithm and a graph-based algorithm, to evaluate the resulting LLR for WSD. While their evaluation demonstrates that the use of an LLR boosts the performance of knowledegebased WSD, it is restricted to nouns only since Wikipedia provides very few verb senses. Moreover, lexical r</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: A knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>105</pages>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1):71– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<pages>1522--1531</pages>
<location>Uppsala,</location>
<contexts>
<context position="28497" citStr="Ponzetto and Navigli (2010)" startWordPosition="4663" endWordPosition="4666">based WSD, (ii) using lexical resources for distant supervision, and (iii) the automated acquisition of sense-annotated data. MASC Senseval Cov Acc Cov Acc WN 0.6573 0.3498 0.6372 0.3209 WN–FN 0.8562 0.4810 0.8812 0.4172 WN–FN–WKT 0.8805 0.4900 0.9038 0.4324 WN–FN–WKT–VN 0.8822 0.4688 0.9139 0.4054 Table 5: Performance of the various combinations of lexical resources. Knowledge-based WSD. While the combination of sense-annotated data and wordnets has been described for knowledge-based WSD before (e.g., Navigli and Velardi (2005; Agirre and Soroa (2009) who use graph algorithms), only recently Ponzetto and Navigli (2010) have investigated the impact of the combination of different lexical resources on the performance of WSD. They aligned WN senses with Wikipedia articles and employed two simple knowledge-based algorithms, i.e., a Lesk-based algorithm and a graph-based algorithm, to evaluate the resulting LLR for WSD. While their evaluation demonstrates that the use of an LLR boosts the performance of knowledegebased WSD, it is restricted to nouns only since Wikipedia provides very few verb senses. Moreover, lexical resources that are rich in lexicalsyntactic information such as VN have not been involved. Mill</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1522–1531, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="10799" citStr="Schmid, 1994" startWordPosition="1691" endWordPosition="1692">lds. We selected POS tags that play an important role in syntactic realisations of verbs, e.g. POS tags for personal pronouns which are potential verb arguments. In our experiments, we tried different sets of function words and POS tags. For instance, we found that some function words (e.g., reflexive pronouns) and some POS tags (e.g., those for past participles and comparative adjectives) introduced too much noise in the data and therefore we did not select them for the final vocabulary.8 In order to create SPs from sense examples, we apply POS tagging and lemmatisation using the TreeTagger (Schmid, 1994) and named entity tagging using the Stanford Named Entity Recogniser (Klein et al., 2003). The named entity tags attached by the Named Entity Recogniser are mapped to WN semantic fields. For the generation of ASPs from sense examples, we used a window size of w = 7, while the generation of LSPs has been performed with w = 5 in order to put a focus on the closely neighbouring lexemes in multiword verb lemmas. The 8The vocabulary used for the creation of ASPs is available at http://www.ukp.tu-darmstadt.de/data/. window size was set empirically using the English Lexical Sample task of the Senseva</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>455--465</pages>
<location>Jeju Island,</location>
<contexts>
<context position="30612" citStr="Surdeanu et al., 2012" startWordPosition="5003" endWordPosition="5006"> Distant supervision. Distant supervision is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their app</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 455– 465, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="8803" citStr="Yarowsky (1995)" startWordPosition="1351" endWordPosition="1352">abstract SPs (ASPs) consisting of the target verb lemma and items from a fixed, linguistically motivated vocabulary. This is based on the intuition that LSPs are important to identify relatively fixed 5http://code.google.com/p/uby/ 6Although VN is linked to sense examples given in the PropBank corpus, the rationale behind using just abstract predicate-argument structure information was to explore, which effect this type of information has on the performance of an automated labelling algorithm. 7It assumes that nearby words provide strong and consistent clues to the sense of a target word, see Yarowsky (1995). 69 WN sense tell%2:32:00:: (let something be known) Corresponding sense patterns (SPs) WN Tell them that you will be late LSP – tell them that you will be ASP – tell PP that PP be JJ WN–FN But an insider told TODAY: ‘ There was no animosity.’ LSP – but an insider tell Today : ‘ there be ASP – person tell location be feeling WN–WKT Please tell me the time. LSP – Please tell me the time ASP – tell PP event WN–VN Agent[+animate |+ organization] V ASP – PP tell group about communication Recipient[+animate |+ organization] about Topic[+communication] Table 1: Examples of SPs derived from an enric</context>
<context position="30250" citStr="Yarowsky, 1995" startWordPosition="4947" endWordPosition="4948">only 33.86% and 30.16% accuracy for the MASC and the Senseval-3 verbs, respectively, which is far below the results we presented. This low performance is due to the fact that Leskbased algorithms do not account for word order. Such information is important especially for verb senses, as the syntactic behaviour of a verb reflects aspects of its meaning. Distant supervision. Distant supervision is a learning paradigm similar to semi-supervised learning. Unlike semi-supervised methods which typically employ a supervised classifier and a 74 small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Acquisition of sense-annotated data. Most previous work on us</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting lexical semantic knowledge from wikipedia and wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<volume>8</volume>
<pages>1646--1652</pages>
<location>Marrakech, Morocco.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Extracting lexical semantic knowledge from wikipedia and wiktionary. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008), volume 8, pages 1646–1652, Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>