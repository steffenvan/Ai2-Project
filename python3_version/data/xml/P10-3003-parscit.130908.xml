<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001282">
<title confidence="0.891531">
WSD as a Distributed Constraint Optimization Problem
</title>
<author confidence="0.946622">
Siva Reddy
</author>
<affiliation confidence="0.8653415">
IIIT Hyderabad
India
</affiliation>
<email confidence="0.937406">
gvsreddy@students.iiit.ac.in
</email>
<author confidence="0.780294">
Abhilash Inumella
</author>
<affiliation confidence="0.7441395">
IIIT Hyderabad
India
</affiliation>
<email confidence="0.932708">
abhilashi@students.iiit.ac.in
</email>
<sectionHeader confidence="0.99197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99955675">
This work models Word Sense Disam-
biguation (WSD) problem as a Dis-
tributed Constraint Optimization Problem
(DCOP). To model WSD as a DCOP,
we view information from various knowl-
edge sources as constraints. DCOP al-
gorithms have the remarkable property to
jointly maximize over a wide range of util-
ity functions associated with these con-
straints. We show how utility functions
can be designed for various knowledge
sources. For the purpose of evaluation,
we modelled all words WSD as a simple
DCOP problem. The results are competi-
tive with state-of-art knowledge based sys-
tems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991164516129">
Words in a language may carry more than one
sense. The correct sense of a word can be iden-
tified based on the context in which it occurs. In
the sentence, He took all his money from the bank,
bank refers to a financial institution sense instead
of other possibilities like the edge of river sense.
Given a word and its possible senses, as defined
by a dictionary, the problem of Word Sense Dis-
ambiguation (WSD) can be defined as the task of
assigning the most appropriate sense to the word
within a given context.
WSD is one of the oldest problems in com-
putational linguistics which dates back to early
1950’s. A range of knowledge sources have been
found to be useful for WSD. (Agirre and Steven-
son, 2006; Agirre and Martinez, 2001; McRoy,
1992; Hirst, 1987) highlight the importance of
various knowledge sources like part of speech,
morphology, collocations, lexical knowledge base
(sense taxonomy, gloss), sub-categorization, se-
mantic word associations, selectional preferences,
semantic roles, domain, topical word associations,
frequency of senses, collocations, domain knowl-
edge. etc. Methods for WSD exploit information
from one or more of these knowledge sources.
Supervised approaches like (Yarowsky and Flo-
rian, 2002; Lee and Ng, 2002; Martinez et al.,
2002; Stevenson and Wilks, 2001) used collec-
tive information from various knowledge sources
to perform disambiguation. Information from var-
ious knowledge sources is encoded in the form of
a feature vector and models were built by training
on sense-tagged corpora. These approaches pose
WSD as a classification problem. They crucially
rely on hand-tagged sense corpora which is hard
to obtain. Systems that do not need hand-tagging
have also been proposed. Agirre and Martinez
(Agirre and Martinez, 2001) evaluated the contri-
bution of each knowledge source separately. How-
ever, this does not combine information from more
than one knowledge source.
In any case, little effort has been made in for-
malizing the way in which information from var-
ious knowledge sources can be collectively used
within a single framework: a framework that al-
lows interaction of evidence from various knowl-
edge sources to arrive at a global optimal solution.
Here we present a way for modelling informa-
tion from various knowledge sources in a multi
agent setting called distributed constraint opti-
mization problem (DCOP). In DCOP, agents have
constraints on their values and each constraint has
a utility associated with it. The agents communi-
cate with each other and choose values such that a
global optimum solution (maximum utility) is at-
tained. We aim to solve WSD by modelling it as a
DCOP.
To the best of our knowledge, ours is the first
attempt to model WSD as a DCOP. In DCOP
framework, information from various knowledge
sources can be used combinedly to perform WSD.
In section 2, we give a brief introduction of
</bodyText>
<page confidence="0.990926">
13
</page>
<note confidence="0.6034865">
Proceedings of the ACL 2010 Student Research Workshop, pages 13–18,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9982924">
DCOP. Section 3 describes modelling WSD as
a DCOP. Utility functions for various knowledge
sources are described in section 4. In section 5,
we conduct a simple experiment by modelling all-
words WSD problem as a DCOP and perform dis-
ambiguation on Senseval-2 (Cotton et al., 2001)
and Senseval-3 (Mihalcea and Edmonds, 2004)
data-set of all-words task. Next follow the sec-
tions on related work, discussion, future work and
conclusion.
</bodyText>
<sectionHeader confidence="0.784181" genericHeader="introduction">
2 Distributed Constraint Optimization
Problem (DCOP)
</sectionHeader>
<bodyText confidence="0.986949384615385">
A DCOP (Modi, 2003; Modi et al., 2004) consists
of n variables V = x1, x2, ...xn each assigned
to an agent, where the values of the variables are
taken from finite, discrete domains D1, D2,..., Dn
respectively. Only the agent has knowledge and
control over values assigned to variables associ-
ated to it. The goal for the agents is to choose
values for variables such that a given global objec-
tive function is maximized. The objective function
is described as the summation over a set of utility
functions.
DCOP can be formalized as a tuple (A, V, D, C,
F) where
</bodyText>
<listItem confidence="0.9366018">
• A = {a1, a2, ... an} is a set of n agents,
• V = {x1, x2, ... xn} is a set of n variables,
each one associated to an agent,
• D = {D1, D2,... Dn} is a set of finite and
discrete domains each one associated to the
corresponding variable,
• C = {fk : DixDj x... Dm → R} is a set of
constraints described by various utility func-
tions fk. The utility function fk is defined
over a subset of variables V . The domain
of fk represent the constraints Cfk and fk(c)
represents the utility associated with the con-
straint c, where c E Cfk.
�• F = zk · fk is the objective function to be
k
</listItem>
<bodyText confidence="0.988726666666667">
maximized where zk is the weight of the cor-
responding utility function fk
An agent is allowed to communicate only with
its neighbours. Agents communicate with each
other to agree upon a solution which maximizes
the objective function.
</bodyText>
<sectionHeader confidence="0.794216" genericHeader="method">
3 WSD as a DCOP
</sectionHeader>
<bodyText confidence="0.9998745">
Given a sequence of words W= {w1, w2,... wn}
with corresponding admissible senses Dwi =
{s1wi, s2wi ...}, we model WSD as DCOP as fol-
lows.
</bodyText>
<subsectionHeader confidence="0.997713">
3.1 Agents
</subsectionHeader>
<bodyText confidence="0.999818666666667">
Each word wi is treated as an agent. The agent
(word) has knowledge and control of its values
(senses).
</bodyText>
<subsectionHeader confidence="0.99873">
3.2 Variables
</subsectionHeader>
<bodyText confidence="0.9999764">
Sense of a word varies and it is the one to be deter-
mined. We define the sense of a word as its vari-
able. Each agent wi is associated with the variable
swi. The value assigned to this variable indicates
the sense assigned by the algorithm.
</bodyText>
<subsectionHeader confidence="0.990543">
3.3 Domains
</subsectionHeader>
<bodyText confidence="0.994289">
Senses of a word are finite in number. The set of
senses Dwi, is the domain of the variable swi.
</bodyText>
<subsectionHeader confidence="0.954536">
3.4 Constraints
</subsectionHeader>
<bodyText confidence="0.999963769230769">
A constraint specifies a particular configuration of
the agents involved in its definition and has a util-
ity associated with it. For e.g. If cij is a constraint
defined on agents wi and wj, then cij refers to a
particular instantiation of wi and wj, say wi = spwi
and wj = sqwj.
A utility function fk : Cfk → R denote a set of
constraints Cfk = {Dwi x Dwj ... Dwm}, defined
on the agents wi, wj ... wm and also the utilities
associated with the constraints. We model infor-
mation from each knowledge source as a utility
function. In section 4, we describe in detail about
this modelling.
</bodyText>
<subsectionHeader confidence="0.930886">
3.5 Objective function
</subsectionHeader>
<bodyText confidence="0.999978333333333">
As already stated, various knowledge sources are
identified to be useful for WSD. It is desirable to
use information from these sources collectively,
to perform disambiguation. DCOP provides such
framework where an objective function is defined
over all the knowledge sources (fk) as below
</bodyText>
<equation confidence="0.94717">
F = � zk · fk
k
</equation>
<bodyText confidence="0.999958333333333">
where F denotes the total utility associated with
a solution and zk is the weight given to a knowl-
edge source i.e. information from various sources
</bodyText>
<page confidence="0.994378">
14
</page>
<bodyText confidence="0.999670142857143">
can be weighted. (Note: It is desirable to nor-
malize utility functions of different knowledge
sources in order to compare them.)
Every agent (word) choose its value (sense) in a
such a way that the objective function (global solu-
tion) is maximized. This way an agent is assigned
a best value which is the target sense in our case.
</bodyText>
<sectionHeader confidence="0.713937" genericHeader="method">
4 Modelling information from various
knowledge sources
</sectionHeader>
<bodyText confidence="0.9996385">
In this section, we discuss the modelling of infor-
mation from various knowledge sources.
</bodyText>
<subsectionHeader confidence="0.986732">
4.1 Part-of-speech (POS)
</subsectionHeader>
<bodyText confidence="0.9999415">
Consider the word play. It has 47 senses out of
which only 17 senses correspond to noun category.
Based on the POS information of a word wi, its
domain Dwi is restricted accordingly.
</bodyText>
<subsectionHeader confidence="0.95592">
4.2 Morphology
</subsectionHeader>
<bodyText confidence="0.999768666666667">
Noun orange has at least two senses, one corre-
sponding to a color and other to a fruit. But plu-
ral form of this word oranges can only be used in
the fruit sense. Depending upon the morphologi-
cal information of a word wi, its domain Dwi can
be restricted.
</bodyText>
<subsectionHeader confidence="0.988721">
4.3 Domain information
</subsectionHeader>
<bodyText confidence="0.999992428571429">
In the sports domain, cricket likely refers to a
game than an insect. Such information can be cap-
tured using a unary utility function defined for ev-
ery word. If the sense distributions of a word wi
are known, a function f : Dwi —* R is defined
which return higher utility for the senses favoured
by the domain than to the other senses.
</bodyText>
<subsectionHeader confidence="0.996441">
4.4 Sense Relatedness
</subsectionHeader>
<bodyText confidence="0.9989414">
Sense relatedness between senses of two words
wi, wj is captured by a function f : Dwi xDwj —*
R where f returns sense relatedness (utility) be-
tween senses based on sense taxonomy and gloss
overlaps.
</bodyText>
<subsectionHeader confidence="0.837036">
4.5 Discourse
</subsectionHeader>
<bodyText confidence="0.998925545454545">
Discourse constraints can be modelled using a
n-ary function. For instance, to the extent one
sense per discourse (Gale et al., 1992) holds true,
higher utility can be returned to the solutions
which favour same sense to all the occurrences
of a word in a given discourse. This information
can be modeled as follows: If wi, wj, ... wm are
the occurrences of a same word, a function f :
Di x Dj x ... Dm —* R is defined which returns
higher utility when swi = swj = ... swm and for
the rest of the combinations it returns lower utility.
</bodyText>
<subsectionHeader confidence="0.975042">
4.6 Collocations
</subsectionHeader>
<bodyText confidence="0.999084677419355">
Collocations of a word are known to provide
strong evidence for identifying correct sense of the
word. For example: if in a given context bank co-
occur with money, it is likely that bank refers to
financial institution sense rather than the edge of
a river sense. The word cancer has at least two
senses, one corresponding to the astrological sign
and the other a disease. But its derived form can-
cerous can only be used in disease sense. When
the words cancer and cancerous co-occur in a dis-
course, it is likely that the word cancer refers to
disease sense.
Most supervised systems work through colloca-
tions to identify correct sense of a word. If a word
wi co-occurs with its collocate v, collocational in-
formation from v can be modeled by using the fol-
lowing function
coll infrm vwi : Dwi —* R
where coll infrm vwi returns high utility to
collocationally preferred senses of wi than other
senses.
Collocations can also be modeled by assigning
more than one variable to the agents or by adding a
dummy agent which gives collocational informa-
tion but in view of simplicity we do not go into
those details.
Topical word associations, semantic word asso-
ciations, selectional preferences can also be mod-
eled similar to collocations. Complex information
involving more than two entities can be modelled
by using n-ary utility functions.
</bodyText>
<sectionHeader confidence="0.9984995" genericHeader="method">
5 Experiment: DCOP based All Words
WSD
</sectionHeader>
<bodyText confidence="0.999763111111111">
We carried out a simple experiment to test the ef-
fectiveness of DCOP algorithm. We conducted
our experiment in an all words setting and used
only WordNet (Fellbaum, 1998) based relatedness
measures as knowledge source so that results can
be compared with earlier state-of-art knowledge-
based WSD systems like (Agirre and Soroa, 2009;
Sinha and Mihalcea, 2007) which used similar
knowledge sources as ours.
</bodyText>
<page confidence="0.995151">
15
</page>
<bodyText confidence="0.999960571428571">
Our method performs disambiguation on sen-
tence by sentence basis. A utility function based
on semantic relatedness is defined for every pair
of words falling in a particular window size. Re-
stricting utility functions to a window size reduces
the number of constraints. An objective function is
defined as sum of these restricted utility functions
over the entire sentence and thus allowing infor-
mation flow across all the words. Hence, a DCOP
algorithm which aims to maximize this objective
function leads to a globally optimal solution.
In our experiments, we used the best similarity
measure settings of (Sinha and Mihalcea, 2007)
which is a sum of normalized similarity mea-
sures jcn, lch and lesk. We used used Distributed
Pseudotree Optimization Procedure (DPOP) algo-
rithm (Petcu and Faltings, 2005), which solves
DCOP using linear number of messages among
agents. The implementation provided with the
open source toolkit FRODO1 (L´eaut´e et al., 2009)
is used.
</bodyText>
<subsectionHeader confidence="0.960411">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.998893">
To compare our results, we ran our experiments
on SENSEVAL-2 and SENSEVAL -3 English all-
words data sets.
</bodyText>
<subsectionHeader confidence="0.745504">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999936333333333">
Table 1 shows results of our experiments. All
these results are carried out using a window size
of four. Ideally, precision and recall values are ex-
pected to be equal in our setting. But in certain
cases, the tool we used, FRODO, failed to find a
solution with the available memory resources.
Results show that our system performs con-
sistently better than (Sinha and Mihalcea, 2007)
which uses exactly same knowledge sources as
used by us (with an exception of adverbs in
Senseval-2). This shows that DCOP algorithm
perform better than page-rank algorithm used in
their graph based setting. Thus, for knowledge-
based WSD, DCOP framework is a potential al-
ternative to graph based models.
Table 1 also shows the system (Agirre and
Soroa, 2009), which obtained best results for
knowledge based WSD. A direct comparison
between this and our system is not quantita-
tive since they used additional knowledge such
as extended WordNet relations (Mihalcea and
</bodyText>
<footnote confidence="0.976771">
1http://liawww.epfl.ch/frodo/
</footnote>
<table confidence="0.964101095238095">
Moldovan, 2001) and sense disambiguated gloss
present in WordNet3.0.
Senseval-2 All Words data set
noun verb adj adv all
P dcop 67.85 37.37 62.72 56.87 58.63
R dcop 66.44 35.47 61.28 56.65 57.09
F dcop 67.14 36.39 61.99 56.76 57.85
P Sinha07 67.73 36.05 62.21 60.47 58.83
R Sinha07 65.63 32.20 61.42 60.23 56.37
F Sinha07 66.24 34.07 61.81 60.35 57.57
Agirre09 70.40 38.90 58.30 70.1 58.6
MFS 71.2 39.0 61.1 75.4 60.1
Senseval-3 All Words data set
P dcop 62.31 43.48 57.14 100 54.68
R dcop 60.97 42.81 55.17 100 53.51
F dcop 61.63 43.14 56.14 100 54.09
P Sinha07 61.22 45.18 54.79 100 54.86
R Sinha07 60.45 40.57 54.14 100 52.40
F Sinha07 60.83 42.75 54.46 100 53.60
Agirre09 64.1 46.9 62.6 92.9 57.4
MFS 69.3 53.6 63.7 92.9 62.3
</table>
<tableCaption confidence="0.985579">
Table 1: Evaluation results on Senseval-2 and
Senseval-3 data-set of all words task.
</tableCaption>
<subsectionHeader confidence="0.999823">
5.3 Performance analysis
</subsectionHeader>
<bodyText confidence="0.99990175">
We conducted our experiment on a computer with
two 2.94 GHz process and 2 GB memory. Our
algorithm just took 5 minutes 31 seconds on
Senseval-2 data set, and 5 minutes 19 seconds on
Senseval-3 data set. This is a singable reduction
compared to execution time of page rank algo-
rithms employed in both Sinha07 and Agirre09. In
Agirre09, it falls in the range 30 to 180 minutes on
much powerful system with 16 GB memory hav-
ing four 2.66 GHz processors. On our system,
time taken by the page rank algorithm in (Sinha
and Mihalcea, 2007) is 11 minutes when executed
on Senseval-2 data set.
Since DCOP algorithms are truly distributed in
nature the execution times can be further reduced
by running them parallely on multiple processors.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.998678333333333">
Earlier approaches to WSD which encoded infor-
mation from variety of knowledge sources can be
classified as follows:
</bodyText>
<listItem confidence="0.979002">
• Supervised approaches: Most of the super-
vised systems (Yarowsky and Florian, 2002;
</listItem>
<page confidence="0.995901">
16
</page>
<bodyText confidence="0.999203066666667">
Lee and Ng, 2002; Martinez et al., 2002;
Stevenson and Wilks, 2001) rely on the sense
tagged data. These are mainly discrimina-
tive or aggregative models which essentially
pose WSD a classification problem. Dis-
criminative models aim to identify the most
informative feature and aggregative models
make their decisions by combining all fea-
tures. They disambiguate word by word and
do not collectively disambiguate whole con-
text and thereby do not capture all the rela-
tionships (e.g sense relatedness) among all
the words. Further, they lack the ability to
directly represent constraints like one sense
per discourse.
</bodyText>
<listItem confidence="0.69637025">
• Graph based approaches: These approaches
crucially rely on lexical knowledge base.
Graph-based WSD approaches (Agirre and
Soroa, 2009; Sinha and Mihalcea, 2007) per-
</listItem>
<bodyText confidence="0.99083625">
form disambiguation over a graph composed
of senses (nodes) and relations between pairs
of senses (edges). The edge weights encode
information from a lexical knowledge base
but lack an efficient way of modelling in-
formation from other knowledge sources like
collocational information, selectional prefer-
ences, domain information, discourse. Also,
the edges represent binary utility functions
defined over two entities which lacks the abil-
ity to encode ternary, and in general, any N-
ary utility functions.
</bodyText>
<sectionHeader confidence="0.997521" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999965548387097">
This framework provides a convenient way of
integrating information from various knowledge
sources by defining their utility functions. Infor-
mation from different knowledge sources can be
weighed based on the setting at hand. For exam-
ple, in a domain specific WSD setting, sense dis-
tributions play a crucial role. The utility function
corresponding to the sense distributions can be
weighed higher in order to take advantage of do-
main information. Also, different combination of
weights can be tried out for a given setting. Thus
for a given WSD setting, this framework allows us
to find 1) the impact of each knowledge source in-
dividually 2) the best combination of knowledge
sources.
Limitations of DCOP algorithms: Solving
DCOPs is NP-hard. A variety of search algorithms
have therefore been developed to solve DCOPs
(Mailler and Lesser, 2004; Modi et al., 2004;
Petcu and Faltings, 2005) . As the number of
constraints or words increase, the search space in-
creases thereby increasing the time and memory
bounds to solve them. Also DCOP algorithms ex-
hibit a trade-off between memory used and num-
ber of messages communicated between agents.
DPOP (Petcu and Faltings, 2005) use linear num-
ber of messages but requires exponential memory
whereas ADOPT (Modi et al., 2004) exhibits lin-
ear memory complexity but exchange exponential
number of messages. So it is crucial to choose a
suitable algorithm based on the problem at hand.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="method">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999899">
In our experiment, we only used relatedness based
utility functions derived from WordNet. Effect of
other knowledge sources remains to be evaluated
individually and in combination. The best possible
combination of weights of knowledge sources is
yet to be engineered. Which DCOP algorithm per-
forms better WSD and when has to be explored.
</bodyText>
<sectionHeader confidence="0.996159" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999934444444444">
We initiated a new line of investigation into WSD
by modelling it in a distributed constraint opti-
mization framework. We showed that this frame-
work is powerful enough to encode information
from various knowledge sources. Our experimen-
tal results show that a simple DCOP based model
encoding just word similarity constraints performs
comparably with the state-of-the-art knowledge
based WSD systems.
</bodyText>
<sectionHeader confidence="0.96566" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999907333333333">
We would like to thank Prof. Rajeev Sangal and
Asrar Ahmed for their support in coming up with
this work.
</bodyText>
<sectionHeader confidence="0.998265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971623166666666">
Eneko Agirre and David Martinez. 2001. Knowledge
sources for word sense disambiguation. In Text,
Speech and Dialogue, 4th International Conference,
TSD 2001, Zelezna Ruda, Czech Republic, Septem-
ber 11-13, 2001, Lecture Notes in Computer Sci-
ence, pages 1–10. Springer.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
EACL ’09: Proceedings of the 12th Conference of
the European Chapter ofthe Association for Compu-
tational Linguistics, pages 33–41, Morristown, NJ,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.995529">
17
</page>
<reference confidence="0.997577115384615">
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Word Sense Disambiguation:
Algorithms and Applications, volume 33 of Text,
Speech and Language Technology, pages 217–252.
Springer, Dordrecht, The Netherlands.
Scott Cotton, Phil Edmonds, Adam Kilgarriff, and
Martha Palmer. 2001. Senseval-2. http://www.
sle.sharp.co.uk/senseval2.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
’91: Proceedings of the workshop on Speech and
Natural Language, pages 233–237, Morristown, NJ,
USA. Association for Computational Linguistics.
Graeme Hirst. 1987. Semantic interpretation and
the resolution of ambiguity. Cambridge University
Press, New York, NY, USA.
Thomas L´eaut´e, Brammert Ottens, and Radoslaw Szy-
manek. 2009. FRODO 2.0: An open-source
framework for distributed constraint optimization.
In Proceedings of the IJCAI’09 Distributed Con-
straint Reasoning Workshop (DCR’09), pages 160–
164, Pasadena, California, USA, July 13. http:
//liawww.epfl.ch/frodo/.
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
EMNLP ’02: Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing, pages 41–48, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Roger Mailler and Victor Lesser. 2004. Solving
distributed constraint optimization problems using
cooperative mediation. In AAMAS ’04: Proceed-
ings of the Third International Joint Conference on
Autonomous Agents and Multiagent Systems, pages
438–445, Washington, DC, USA. IEEE Computer
Society.
David Martinez, Eneko Agirre, and Lluis M`arquez.
2002. Syntactic features for high precision word
sense disambiguation. In COLING.
Susan W. McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. COMPUTA-
TIONAL LINGUISTICS, 18:1–30.
Rada Mihalcea and Phil Edmonds, editors. 2004.
Proceedings Senseval-3 3rd International Workshop
on Evaluating Word Sense Disambiguation Systems.
ACL, Barcelona, Spain.
Rada Mihalcea and Dan I. Moldovan. 2001. ex-
tended wordnet: progress report. In in Proceedings
ofNAACL Workshop on WordNet and Other Lexical
Resources, pages 95–100.
Pragnesh Jay Modi, Wei-Min Shen, Milind Tambe, and
Makoto Yokoo. 2004. Adopt: Asynchronous dis-
tributed constraint optimization with quality guaran-
tees. Artificial Intelligence, 161:149–180.
Pragnesh Jay Modi. 2003. Distributed constraint opti-
mization for multiagent systems. PhD Thesis.
Adrian Petcu and Boi Faltings. 2005. A scalable
method for multiagent constraint optimization. In
IJCAI’05: Proceedings of the 19th international
joint conference on Artificial intelligence, pages
266–271, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In ICSC ’07: Pro-
ceedings of the International Conference on Seman-
tic Computing, pages 363–369, Washington, DC,
USA. IEEE Computer Society.
Mark Stevenson and Yorick Wilks. 2001. The inter-
action of knowledge sources in word sense disam-
biguation. Comput. Linguist., 27(3):321–349.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation across diverse parameter
spaces. Natural Language Engineering, 8:2002.
</reference>
<page confidence="0.999282">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270680">
<title confidence="0.99753">WSD as a Distributed Constraint Optimization Problem</title>
<author confidence="0.998804">Siva Reddy</author>
<affiliation confidence="0.967463">IIIT Hyderabad</affiliation>
<address confidence="0.700879">India</address>
<email confidence="0.951985">gvsreddy@students.iiit.ac.in</email>
<author confidence="0.747129">Abhilash Inumella</author>
<affiliation confidence="0.863524">IIIT Hyderabad</affiliation>
<address confidence="0.737823">India</address>
<email confidence="0.959838">abhilashi@students.iiit.ac.in</email>
<abstract confidence="0.990040529411765">This work models Word Sense Disambiguation (WSD) problem as a Distributed Constraint Optimization Problem (DCOP). To model WSD as a DCOP, we view information from various knowledge sources as constraints. DCOP algorithms have the remarkable property to jointly maximize over a wide range of utility functions associated with these constraints. We show how utility functions can be designed for various knowledge sources. For the purpose of evaluation, we modelled all words WSD as a simple DCOP problem. The results are competitive with state-of-art knowledge based systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Knowledge sources for word sense disambiguation.</title>
<date>2001</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In Text, Speech and Dialogue, 4th International Conference, TSD</booktitle>
<pages>1--10</pages>
<publisher>Springer.</publisher>
<location>Zelezna Ruda, Czech Republic,</location>
<contexts>
<context position="1514" citStr="Agirre and Martinez, 2001" startWordPosition="246" endWordPosition="249">he context in which it occurs. In the sentence, He took all his money from the bank, bank refers to a financial institution sense instead of other possibilities like the edge of river sense. Given a word and its possible senses, as defined by a dictionary, the problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the word within a given context. WSD is one of the oldest problems in computational linguistics which dates back to early 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from var</context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>Eneko Agirre and David Martinez. 2001. Knowledge sources for word sense disambiguation. In Text, Speech and Dialogue, 4th International Conference, TSD 2001, Zelezna Ruda, Czech Republic, September 11-13, 2001, Lecture Notes in Computer Science, pages 1–10. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter ofthe Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11179" citStr="Agirre and Soroa, 2009" startWordPosition="1929" endWordPosition="1932">t go into those details. Topical word associations, semantic word associations, selectional preferences can also be modeled similar to collocations. Complex information involving more than two entities can be modelled by using n-ary utility functions. 5 Experiment: DCOP based All Words WSD We carried out a simple experiment to test the effectiveness of DCOP algorithm. We conducted our experiment in an all words setting and used only WordNet (Fellbaum, 1998) based relatedness measures as knowledge source so that results can be compared with earlier state-of-art knowledgebased WSD systems like (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007) which used similar knowledge sources as ours. 15 Our method performs disambiguation on sentence by sentence basis. A utility function based on semantic relatedness is defined for every pair of words falling in a particular window size. Restricting utility functions to a window size reduces the number of constraints. An objective function is defined as sum of these restricted utility functions over the entire sentence and thus allowing information flow across all the words. Hence, a DCOP algorithm which aims to maximize this objective function leads to a globally opt</context>
<context position="13088" citStr="Agirre and Soroa, 2009" startWordPosition="2241" endWordPosition="2244">cision and recall values are expected to be equal in our setting. But in certain cases, the tool we used, FRODO, failed to find a solution with the available memory resources. Results show that our system performs consistently better than (Sinha and Mihalcea, 2007) which uses exactly same knowledge sources as used by us (with an exception of adverbs in Senseval-2). This shows that DCOP algorithm perform better than page-rank algorithm used in their graph based setting. Thus, for knowledgebased WSD, DCOP framework is a potential alternative to graph based models. Table 1 also shows the system (Agirre and Soroa, 2009), which obtained best results for knowledge based WSD. A direct comparison between this and our system is not quantitative since they used additional knowledge such as extended WordNet relations (Mihalcea and 1http://liawww.epfl.ch/frodo/ Moldovan, 2001) and sense disambiguated gloss present in WordNet3.0. Senseval-2 All Words data set noun verb adj adv all P dcop 67.85 37.37 62.72 56.87 58.63 R dcop 66.44 35.47 61.28 56.65 57.09 F dcop 67.14 36.39 61.99 56.76 57.85 P Sinha07 67.73 36.05 62.21 60.47 58.83 R Sinha07 65.63 32.20 61.42 60.23 56.37 F Sinha07 66.24 34.07 61.81 60.35 57.57 Agirre09 </context>
<context position="15868" citStr="Agirre and Soroa, 2009" startWordPosition="2701" endWordPosition="2704">ve or aggregative models which essentially pose WSD a classification problem. Discriminative models aim to identify the most informative feature and aggregative models make their decisions by combining all features. They disambiguate word by word and do not collectively disambiguate whole context and thereby do not capture all the relationships (e.g sense relatedness) among all the words. Further, they lack the ability to directly represent constraints like one sense per discourse. • Graph based approaches: These approaches crucially rely on lexical knowledge base. Graph-based WSD approaches (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007) perform disambiguation over a graph composed of senses (nodes) and relations between pairs of senses (edges). The edge weights encode information from a lexical knowledge base but lack an efficient way of modelling information from other knowledge sources like collocational information, selectional preferences, domain information, discourse. Also, the edges represent binary utility functions defined over two entities which lacks the ability to encode ternary, and in general, any Nary utility functions. 7 Discussion This framework provides a convenient way of integra</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In EACL ’09: Proceedings of the 12th Conference of the European Chapter ofthe Association for Computational Linguistics, pages 33–41, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mark Stevenson</author>
</authors>
<title>Knowledge sources for wsd. In Word Sense Disambiguation: Algorithms and Applications,</title>
<date>2006</date>
<journal>Text, Speech and Language Technology,</journal>
<volume>33</volume>
<pages>217--252</pages>
<publisher>Springer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="1487" citStr="Agirre and Stevenson, 2006" startWordPosition="241" endWordPosition="245">can be identified based on the context in which it occurs. In the sentence, He took all his money from the bank, bank refers to a financial institution sense instead of other possibilities like the edge of river sense. Given a word and its possible senses, as defined by a dictionary, the problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the word within a given context. WSD is one of the oldest problems in computational linguistics which dates back to early 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used coll</context>
</contexts>
<marker>Agirre, Stevenson, 2006</marker>
<rawString>Eneko Agirre and Mark Stevenson. 2006. Knowledge sources for wsd. In Word Sense Disambiguation: Algorithms and Applications, volume 33 of Text, Speech and Language Technology, pages 217–252. Springer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cotton</author>
<author>Phil Edmonds</author>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<date>2001</date>
<note>Senseval-2. http://www. sle.sharp.co.uk/senseval2.</note>
<contexts>
<context position="4073" citStr="Cotton et al., 2001" startWordPosition="649" endWordPosition="652">s the first attempt to model WSD as a DCOP. In DCOP framework, information from various knowledge sources can be used combinedly to perform WSD. In section 2, we give a brief introduction of 13 Proceedings of the ACL 2010 Student Research Workshop, pages 13–18, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics DCOP. Section 3 describes modelling WSD as a DCOP. Utility functions for various knowledge sources are described in section 4. In section 5, we conduct a simple experiment by modelling allwords WSD problem as a DCOP and perform disambiguation on Senseval-2 (Cotton et al., 2001) and Senseval-3 (Mihalcea and Edmonds, 2004) data-set of all-words task. Next follow the sections on related work, discussion, future work and conclusion. 2 Distributed Constraint Optimization Problem (DCOP) A DCOP (Modi, 2003; Modi et al., 2004) consists of n variables V = x1, x2, ...xn each assigned to an agent, where the values of the variables are taken from finite, discrete domains D1, D2,..., Dn respectively. Only the agent has knowledge and control over values assigned to variables associated to it. The goal for the agents is to choose values for variables such that a given global objec</context>
</contexts>
<marker>Cotton, Edmonds, Kilgarriff, Palmer, 2001</marker>
<rawString>Scott Cotton, Phil Edmonds, Adam Kilgarriff, and Martha Palmer. 2001. Senseval-2. http://www. sle.sharp.co.uk/senseval2.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA ; London,</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, Cambridge, MA ; London, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In HLT ’91: Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>233--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9056" citStr="Gale et al., 1992" startWordPosition="1557" endWordPosition="1560">can be captured using a unary utility function defined for every word. If the sense distributions of a word wi are known, a function f : Dwi —* R is defined which return higher utility for the senses favoured by the domain than to the other senses. 4.4 Sense Relatedness Sense relatedness between senses of two words wi, wj is captured by a function f : Dwi xDwj —* R where f returns sense relatedness (utility) between senses based on sense taxonomy and gloss overlaps. 4.5 Discourse Discourse constraints can be modelled using a n-ary function. For instance, to the extent one sense per discourse (Gale et al., 1992) holds true, higher utility can be returned to the solutions which favour same sense to all the occurrences of a word in a given discourse. This information can be modeled as follows: If wi, wj, ... wm are the occurrences of a same word, a function f : Di x Dj x ... Dm —* R is defined which returns higher utility when swi = swj = ... swm and for the rest of the combinations it returns lower utility. 4.6 Collocations Collocations of a word are known to provide strong evidence for identifying correct sense of the word. For example: if in a given context bank cooccur with money, it is likely that</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In HLT ’91: Proceedings of the workshop on Speech and Natural Language, pages 233–237, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic interpretation and the resolution of ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1541" citStr="Hirst, 1987" startWordPosition="252" endWordPosition="253">ntence, He took all his money from the bank, bank refers to a financial institution sense instead of other possibilities like the edge of river sense. Given a word and its possible senses, as defined by a dictionary, the problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the word within a given context. WSD is one of the oldest problems in computational linguistics which dates back to early 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledge sources to p</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Graeme Hirst. 1987. Semantic interpretation and the resolution of ambiguity. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L´eaut´e</author>
<author>Brammert Ottens</author>
<author>Radoslaw Szymanek</author>
</authors>
<title>FRODO 2.0: An open-source framework for distributed constraint optimization.</title>
<date>2009</date>
<booktitle>In Proceedings of the IJCAI’09 Distributed Constraint Reasoning Workshop (DCR’09),</booktitle>
<pages>160--164</pages>
<location>Pasadena, California, USA,</location>
<note>http: //liawww.epfl.ch/frodo/.</note>
<marker>L´eaut´e, Ottens, Szymanek, 2009</marker>
<rawString>Thomas L´eaut´e, Brammert Ottens, and Radoslaw Szymanek. 2009. FRODO 2.0: An open-source framework for distributed constraint optimization. In Proceedings of the IJCAI’09 Distributed Constraint Reasoning Workshop (DCR’09), pages 160– 164, Pasadena, California, USA, July 13. http: //liawww.epfl.ch/frodo/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2026" citStr="Lee and Ng, 2002" startWordPosition="315" endWordPosition="318">dge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledge sources to perform disambiguation. Information from various knowledge sources is encoded in the form of a feature vector and models were built by training on sense-tagged corpora. These approaches pose WSD as a classification problem. They crucially rely on hand-tagged sense corpora which is hard to obtain. Systems that do not need hand-tagging have also been proposed. Agirre and Martinez (Agirre and Martinez, 2001) evaluated the contribution of each knowledge source separately. However, this</context>
<context position="15134" citStr="Lee and Ng, 2002" startWordPosition="2589" endWordPosition="2592">range 30 to 180 minutes on much powerful system with 16 GB memory having four 2.66 GHz processors. On our system, time taken by the page rank algorithm in (Sinha and Mihalcea, 2007) is 11 minutes when executed on Senseval-2 data set. Since DCOP algorithms are truly distributed in nature the execution times can be further reduced by running them parallely on multiple processors. 6 Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: • Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; 16 Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. These are mainly discriminative or aggregative models which essentially pose WSD a classification problem. Discriminative models aim to identify the most informative feature and aggregative models make their decisions by combining all features. They disambiguate word by word and do not collectively disambiguate whole context and thereby do not capture all the relationships (e.g sense relatedness) among all the words. Further, they lack the ability to directly represent constraints like one sense per discourse. •</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 41–48, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Mailler</author>
<author>Victor Lesser</author>
</authors>
<title>Solving distributed constraint optimization problems using cooperative mediation.</title>
<date>2004</date>
<booktitle>In AAMAS ’04: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems,</booktitle>
<pages>438--445</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="17262" citStr="Mailler and Lesser, 2004" startWordPosition="2918" endWordPosition="2921">and. For example, in a domain specific WSD setting, sense distributions play a crucial role. The utility function corresponding to the sense distributions can be weighed higher in order to take advantage of domain information. Also, different combination of weights can be tried out for a given setting. Thus for a given WSD setting, this framework allows us to find 1) the impact of each knowledge source individually 2) the best combination of knowledge sources. Limitations of DCOP algorithms: Solving DCOPs is NP-hard. A variety of search algorithms have therefore been developed to solve DCOPs (Mailler and Lesser, 2004; Modi et al., 2004; Petcu and Faltings, 2005) . As the number of constraints or words increase, the search space increases thereby increasing the time and memory bounds to solve them. Also DCOP algorithms exhibit a trade-off between memory used and number of messages communicated between agents. DPOP (Petcu and Faltings, 2005) use linear number of messages but requires exponential memory whereas ADOPT (Modi et al., 2004) exhibits linear memory complexity but exchange exponential number of messages. So it is crucial to choose a suitable algorithm based on the problem at hand. 8 Future Work In </context>
</contexts>
<marker>Mailler, Lesser, 2004</marker>
<rawString>Roger Mailler and Victor Lesser. 2004. Solving distributed constraint optimization problems using cooperative mediation. In AAMAS ’04: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 438–445, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
<author>Lluis M`arquez</author>
</authors>
<title>Syntactic features for high precision word sense disambiguation.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<marker>Martinez, Agirre, M`arquez, 2002</marker>
<rawString>David Martinez, Eneko Agirre, and Lluis M`arquez. 2002. Syntactic features for high precision word sense disambiguation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense discrimination.</title>
<date>1992</date>
<booktitle>COMPUTATIONAL LINGUISTICS,</booktitle>
<pages>18--1</pages>
<contexts>
<context position="1527" citStr="McRoy, 1992" startWordPosition="250" endWordPosition="251">rs. In the sentence, He took all his money from the bank, bank refers to a financial institution sense instead of other possibilities like the edge of river sense. Given a word and its possible senses, as defined by a dictionary, the problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the word within a given context. WSD is one of the oldest problems in computational linguistics which dates back to early 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledg</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>Susan W. McRoy. 1992. Using multiple knowledge sources for word sense discrimination. COMPUTATIONAL LINGUISTICS, 18:1–30.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Proceedings Senseval-3 3rd International Workshop on Evaluating Word Sense Disambiguation Systems. ACL,</booktitle>
<editor>Rada Mihalcea and Phil Edmonds, editors.</editor>
<location>Barcelona, Spain.</location>
<marker>2004</marker>
<rawString>Rada Mihalcea and Phil Edmonds, editors. 2004. Proceedings Senseval-3 3rd International Workshop on Evaluating Word Sense Disambiguation Systems. ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>extended wordnet: progress report.</title>
<date>2001</date>
<booktitle>In in Proceedings ofNAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<pages>95--100</pages>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 2001. extended wordnet: progress report. In in Proceedings ofNAACL Workshop on WordNet and Other Lexical Resources, pages 95–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pragnesh Jay Modi</author>
<author>Wei-Min Shen</author>
<author>Milind Tambe</author>
<author>Makoto Yokoo</author>
</authors>
<title>Adopt: Asynchronous distributed constraint optimization with quality guarantees.</title>
<date>2004</date>
<journal>Artificial Intelligence,</journal>
<pages>161--149</pages>
<contexts>
<context position="4319" citStr="Modi et al., 2004" startWordPosition="686" endWordPosition="689"> pages 13–18, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics DCOP. Section 3 describes modelling WSD as a DCOP. Utility functions for various knowledge sources are described in section 4. In section 5, we conduct a simple experiment by modelling allwords WSD problem as a DCOP and perform disambiguation on Senseval-2 (Cotton et al., 2001) and Senseval-3 (Mihalcea and Edmonds, 2004) data-set of all-words task. Next follow the sections on related work, discussion, future work and conclusion. 2 Distributed Constraint Optimization Problem (DCOP) A DCOP (Modi, 2003; Modi et al., 2004) consists of n variables V = x1, x2, ...xn each assigned to an agent, where the values of the variables are taken from finite, discrete domains D1, D2,..., Dn respectively. Only the agent has knowledge and control over values assigned to variables associated to it. The goal for the agents is to choose values for variables such that a given global objective function is maximized. The objective function is described as the summation over a set of utility functions. DCOP can be formalized as a tuple (A, V, D, C, F) where • A = {a1, a2, ... an} is a set of n agents, • V = {x1, x2, ... xn} is a set</context>
<context position="17281" citStr="Modi et al., 2004" startWordPosition="2922" endWordPosition="2925">ain specific WSD setting, sense distributions play a crucial role. The utility function corresponding to the sense distributions can be weighed higher in order to take advantage of domain information. Also, different combination of weights can be tried out for a given setting. Thus for a given WSD setting, this framework allows us to find 1) the impact of each knowledge source individually 2) the best combination of knowledge sources. Limitations of DCOP algorithms: Solving DCOPs is NP-hard. A variety of search algorithms have therefore been developed to solve DCOPs (Mailler and Lesser, 2004; Modi et al., 2004; Petcu and Faltings, 2005) . As the number of constraints or words increase, the search space increases thereby increasing the time and memory bounds to solve them. Also DCOP algorithms exhibit a trade-off between memory used and number of messages communicated between agents. DPOP (Petcu and Faltings, 2005) use linear number of messages but requires exponential memory whereas ADOPT (Modi et al., 2004) exhibits linear memory complexity but exchange exponential number of messages. So it is crucial to choose a suitable algorithm based on the problem at hand. 8 Future Work In our experiment, we </context>
</contexts>
<marker>Modi, Shen, Tambe, Yokoo, 2004</marker>
<rawString>Pragnesh Jay Modi, Wei-Min Shen, Milind Tambe, and Makoto Yokoo. 2004. Adopt: Asynchronous distributed constraint optimization with quality guarantees. Artificial Intelligence, 161:149–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pragnesh Jay Modi</author>
</authors>
<title>Distributed constraint optimization for multiagent systems.</title>
<date>2003</date>
<tech>PhD Thesis.</tech>
<contexts>
<context position="4299" citStr="Modi, 2003" startWordPosition="684" endWordPosition="685">ch Workshop, pages 13–18, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics DCOP. Section 3 describes modelling WSD as a DCOP. Utility functions for various knowledge sources are described in section 4. In section 5, we conduct a simple experiment by modelling allwords WSD problem as a DCOP and perform disambiguation on Senseval-2 (Cotton et al., 2001) and Senseval-3 (Mihalcea and Edmonds, 2004) data-set of all-words task. Next follow the sections on related work, discussion, future work and conclusion. 2 Distributed Constraint Optimization Problem (DCOP) A DCOP (Modi, 2003; Modi et al., 2004) consists of n variables V = x1, x2, ...xn each assigned to an agent, where the values of the variables are taken from finite, discrete domains D1, D2,..., Dn respectively. Only the agent has knowledge and control over values assigned to variables associated to it. The goal for the agents is to choose values for variables such that a given global objective function is maximized. The objective function is described as the summation over a set of utility functions. DCOP can be formalized as a tuple (A, V, D, C, F) where • A = {a1, a2, ... an} is a set of n agents, • V = {x1, </context>
</contexts>
<marker>Modi, 2003</marker>
<rawString>Pragnesh Jay Modi. 2003. Distributed constraint optimization for multiagent systems. PhD Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Petcu</author>
<author>Boi Faltings</author>
</authors>
<title>A scalable method for multiagent constraint optimization.</title>
<date>2005</date>
<booktitle>In IJCAI’05: Proceedings of the 19th international joint conference on Artificial intelligence,</booktitle>
<pages>266--271</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="12059" citStr="Petcu and Faltings, 2005" startWordPosition="2069" endWordPosition="2072">ow size. Restricting utility functions to a window size reduces the number of constraints. An objective function is defined as sum of these restricted utility functions over the entire sentence and thus allowing information flow across all the words. Hence, a DCOP algorithm which aims to maximize this objective function leads to a globally optimal solution. In our experiments, we used the best similarity measure settings of (Sinha and Mihalcea, 2007) which is a sum of normalized similarity measures jcn, lch and lesk. We used used Distributed Pseudotree Optimization Procedure (DPOP) algorithm (Petcu and Faltings, 2005), which solves DCOP using linear number of messages among agents. The implementation provided with the open source toolkit FRODO1 (L´eaut´e et al., 2009) is used. 5.1 Data To compare our results, we ran our experiments on SENSEVAL-2 and SENSEVAL -3 English allwords data sets. 5.2 Results Table 1 shows results of our experiments. All these results are carried out using a window size of four. Ideally, precision and recall values are expected to be equal in our setting. But in certain cases, the tool we used, FRODO, failed to find a solution with the available memory resources. Results show that </context>
<context position="17308" citStr="Petcu and Faltings, 2005" startWordPosition="2926" endWordPosition="2929">tting, sense distributions play a crucial role. The utility function corresponding to the sense distributions can be weighed higher in order to take advantage of domain information. Also, different combination of weights can be tried out for a given setting. Thus for a given WSD setting, this framework allows us to find 1) the impact of each knowledge source individually 2) the best combination of knowledge sources. Limitations of DCOP algorithms: Solving DCOPs is NP-hard. A variety of search algorithms have therefore been developed to solve DCOPs (Mailler and Lesser, 2004; Modi et al., 2004; Petcu and Faltings, 2005) . As the number of constraints or words increase, the search space increases thereby increasing the time and memory bounds to solve them. Also DCOP algorithms exhibit a trade-off between memory used and number of messages communicated between agents. DPOP (Petcu and Faltings, 2005) use linear number of messages but requires exponential memory whereas ADOPT (Modi et al., 2004) exhibits linear memory complexity but exchange exponential number of messages. So it is crucial to choose a suitable algorithm based on the problem at hand. 8 Future Work In our experiment, we only used relatedness based</context>
</contexts>
<marker>Petcu, Faltings, 2005</marker>
<rawString>Adrian Petcu and Boi Faltings. 2005. A scalable method for multiagent constraint optimization. In IJCAI’05: Proceedings of the 19th international joint conference on Artificial intelligence, pages 266–271, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-basedword sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<booktitle>In ICSC ’07: Proceedings of the International Conference on Semantic Computing,</booktitle>
<pages>363--369</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="11206" citStr="Sinha and Mihalcea, 2007" startWordPosition="1933" endWordPosition="1936"> Topical word associations, semantic word associations, selectional preferences can also be modeled similar to collocations. Complex information involving more than two entities can be modelled by using n-ary utility functions. 5 Experiment: DCOP based All Words WSD We carried out a simple experiment to test the effectiveness of DCOP algorithm. We conducted our experiment in an all words setting and used only WordNet (Fellbaum, 1998) based relatedness measures as knowledge source so that results can be compared with earlier state-of-art knowledgebased WSD systems like (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007) which used similar knowledge sources as ours. 15 Our method performs disambiguation on sentence by sentence basis. A utility function based on semantic relatedness is defined for every pair of words falling in a particular window size. Restricting utility functions to a window size reduces the number of constraints. An objective function is defined as sum of these restricted utility functions over the entire sentence and thus allowing information flow across all the words. Hence, a DCOP algorithm which aims to maximize this objective function leads to a globally optimal solution. In our exper</context>
<context position="12730" citStr="Sinha and Mihalcea, 2007" startWordPosition="2182" endWordPosition="2185">sages among agents. The implementation provided with the open source toolkit FRODO1 (L´eaut´e et al., 2009) is used. 5.1 Data To compare our results, we ran our experiments on SENSEVAL-2 and SENSEVAL -3 English allwords data sets. 5.2 Results Table 1 shows results of our experiments. All these results are carried out using a window size of four. Ideally, precision and recall values are expected to be equal in our setting. But in certain cases, the tool we used, FRODO, failed to find a solution with the available memory resources. Results show that our system performs consistently better than (Sinha and Mihalcea, 2007) which uses exactly same knowledge sources as used by us (with an exception of adverbs in Senseval-2). This shows that DCOP algorithm perform better than page-rank algorithm used in their graph based setting. Thus, for knowledgebased WSD, DCOP framework is a potential alternative to graph based models. Table 1 also shows the system (Agirre and Soroa, 2009), which obtained best results for knowledge based WSD. A direct comparison between this and our system is not quantitative since they used additional knowledge such as extended WordNet relations (Mihalcea and 1http://liawww.epfl.ch/frodo/ Mol</context>
<context position="14699" citStr="Sinha and Mihalcea, 2007" startWordPosition="2519" endWordPosition="2522">on results on Senseval-2 and Senseval-3 data-set of all words task. 5.3 Performance analysis We conducted our experiment on a computer with two 2.94 GHz process and 2 GB memory. Our algorithm just took 5 minutes 31 seconds on Senseval-2 data set, and 5 minutes 19 seconds on Senseval-3 data set. This is a singable reduction compared to execution time of page rank algorithms employed in both Sinha07 and Agirre09. In Agirre09, it falls in the range 30 to 180 minutes on much powerful system with 16 GB memory having four 2.66 GHz processors. On our system, time taken by the page rank algorithm in (Sinha and Mihalcea, 2007) is 11 minutes when executed on Senseval-2 data set. Since DCOP algorithms are truly distributed in nature the execution times can be further reduced by running them parallely on multiple processors. 6 Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: • Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; 16 Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. These are mainly discriminative or aggregative models which essentially pose WSD a</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-basedword sense disambiguation using measures of word semantic similarity. In ICSC ’07: Proceedings of the International Conference on Semantic Computing, pages 363–369, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Comput. Linguist.,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="2077" citStr="Stevenson and Wilks, 2001" startWordPosition="323" endWordPosition="326">for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledge sources to perform disambiguation. Information from various knowledge sources is encoded in the form of a feature vector and models were built by training on sense-tagged corpora. These approaches pose WSD as a classification problem. They crucially rely on hand-tagged sense corpora which is hard to obtain. Systems that do not need hand-tagging have also been proposed. Agirre and Martinez (Agirre and Martinez, 2001) evaluated the contribution of each knowledge source separately. However, this does not combine information from more than one kn</context>
<context position="15185" citStr="Stevenson and Wilks, 2001" startWordPosition="2597" endWordPosition="2600">system with 16 GB memory having four 2.66 GHz processors. On our system, time taken by the page rank algorithm in (Sinha and Mihalcea, 2007) is 11 minutes when executed on Senseval-2 data set. Since DCOP algorithms are truly distributed in nature the execution times can be further reduced by running them parallely on multiple processors. 6 Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: • Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; 16 Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. These are mainly discriminative or aggregative models which essentially pose WSD a classification problem. Discriminative models aim to identify the most informative feature and aggregative models make their decisions by combining all features. They disambiguate word by word and do not collectively disambiguate whole context and thereby do not capture all the relationships (e.g sense relatedness) among all the words. Further, they lack the ability to directly represent constraints like one sense per discourse. • Graph based approaches: These approaches crucially</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Mark Stevenson and Yorick Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Comput. Linguist., 27(3):321–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--2002</pages>
<contexts>
<context position="2008" citStr="Yarowsky and Florian, 2002" startWordPosition="310" endWordPosition="314">ly 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Martinez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, semantic roles, domain, topical word associations, frequency of senses, collocations, domain knowledge. etc. Methods for WSD exploit information from one or more of these knowledge sources. Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledge sources to perform disambiguation. Information from various knowledge sources is encoded in the form of a feature vector and models were built by training on sense-tagged corpora. These approaches pose WSD as a classification problem. They crucially rely on hand-tagged sense corpora which is hard to obtain. Systems that do not need hand-tagging have also been proposed. Agirre and Martinez (Agirre and Martinez, 2001) evaluated the contribution of each knowledge source separat</context>
<context position="15113" citStr="Yarowsky and Florian, 2002" startWordPosition="2584" endWordPosition="2587">. In Agirre09, it falls in the range 30 to 180 minutes on much powerful system with 16 GB memory having four 2.66 GHz processors. On our system, time taken by the page rank algorithm in (Sinha and Mihalcea, 2007) is 11 minutes when executed on Senseval-2 data set. Since DCOP algorithms are truly distributed in nature the execution times can be further reduced by running them parallely on multiple processors. 6 Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: • Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; 16 Lee and Ng, 2002; Martinez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. These are mainly discriminative or aggregative models which essentially pose WSD a classification problem. Discriminative models aim to identify the most informative feature and aggregative models make their decisions by combining all features. They disambiguate word by word and do not collectively disambiguate whole context and thereby do not capture all the relationships (e.g sense relatedness) among all the words. Further, they lack the ability to directly represent constraints like one s</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8:2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>