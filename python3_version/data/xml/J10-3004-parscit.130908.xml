<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.703352">
Disentangling Chat
</title>
<author confidence="0.656091">
Micha Elsner*
</author>
<title confidence="0.256995">
Brown Laboratory for Linguistic
Information Processing (BLLIP)
</title>
<author confidence="0.526767">
Eugene Charniak**
</author>
<sectionHeader confidence="0.5761815" genericHeader="abstract">
Brown Laboratory for Linguistic
Information Processing (BLLIP)
</sectionHeader>
<bodyText confidence="0.75336575">
When multiple conversations occur simultaneously, a listener must decide which conversation
each utterance is part of in order to interpret and respond to it appropriately. We refer to this task
as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various
conversations have been manually disentangled, and evaluate annotator reliability. We propose
a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based
features. The model’s predicted disentanglements are highly correlated with manual annotations.
We conclude by discussing two extensions to the model, specificity tuning and conversation start
detection, both of which are promising but do not currently yield practical improvements.
</bodyText>
<sectionHeader confidence="0.950268" genericHeader="introduction">
1. Motivation
</sectionHeader>
<bodyText confidence="0.999794533333333">
Simultaneous conversations seem to arise naturally in both informal social interactions
and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among
8–10 people found an average of 1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical
conversation, therefore, does not form a contiguous segment of the chatroom transcript,
but is frequently broken up by interposed utterances from other conversations.
Disentanglement (also called thread detection [Shen et al. 2006], thread extraction
[Adams and Martell 2008], and thread/conversation management [Traum 2004]) is
the clustering task of dividing a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level dialogue analysis. For instance,
consider the multi-party exchange in Figure 1.
Contextually, it is clear that this corresponds to two conversations, and Felicia’s1
response excellent is intended for Chanel and Regine. A straightforward reading of the
transcript, however, might interpret it as a response to Gale’s statement immediately
preceding.
</bodyText>
<footnote confidence="0.368203142857143">
* Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: melsner@cs.brown.edu.
** Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.
E-mail: ec@cs.brown.edu.
1 Real user nicknames are replaced with randomly selected identifiers for ethical reasons.
Submission received: 27 January 2009; revised submission received: 1 November 2009; accepted for
publication: 3 March 2010.
</footnote>
<note confidence="0.787219">
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
</note>
<figureCaption confidence="0.976183">
Figure 1
</figureCaption>
<bodyText confidence="0.982285064516129">
Some (abridged) conversation from our corpus.
Humans are adept at disentanglement, even in complicated environments like
crowded cocktail parties or chat rooms; in order to perform this task, they must maintain
a complex mental representation of the ongoing discourse. Moreover, they adjust their
conversational behavior to make the task easier, mentioning names more frequently
than in spoken or two-party typed dialogues (O’Neill and Martin 2003). This need for
adaptation suggests that disentanglement can be challenging even for humans, and
therefore can serve as a useful stress test for computational models of discourse.
Disentanglement has two practical applications. One is the analysis of pre-recorded
transcripts in order to extract some kind of information, such as question–answer pairs
or summaries. These tasks should probably take as input each separate conversation,
rather than the entire transcript. Another application is as part of a user-interface system
for active participants in the chat, in which users target a conversation of interest
which is then highlighted for them. Aoki et al. (2003) created such a system for speech,
which users generally preferred to a conventional system—when the disentanglement
worked!
We begin in Section 2 with an overview of related work. In Section 3, we present a
new corpus of manually annotated chat room data and evaluate annotator reliability. We
give a set of metrics (Section 3.2) describing structural similarity both locally and glob-
ally. In Section 4, we propose a model which uses supervised pairwise classification to
link utterances from the same conversation, followed by a greedy inference stage which
clusters the utterances into conversations. Our system uses time gap and utterance con-
tent features. Experimental results (Section 5) show that its output is highly correlated
with human annotations. Finally, in Sections 6 and 7, we investigate two extensions to
the basic model, specificity tuning and automatic detection of conversation starts.
To our knowledge, this is the first work to evaluate interannotator agreement for
the disentanglement task. It is also the first to use a supervised method to learn weights
for different feature types, rather than relying on cosine distance with uniform or hand-
tuned feature weights. It supplements standard word repetition and time gap features
with other feature types, including very powerful features based on name mentioning,
which is common in Internet Relay Chat.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.9994485">
Several threads of research are direct attempts to solve the disentanglement problem.
The closest to our own work is that of Shen et al. (2006), which performs conversation
disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech,
rather than chat. Other work has slightly different goals than ours: Adams and Martell
(2008) attempt to find all utterances of a specific single conversation in Internet and
Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network
</bodyText>
<page confidence="0.987754">
390
</page>
<note confidence="0.731219">
Elsner and Charniak Disentangling Chat
</note>
<bodyText confidence="0.99956154">
analysis, extracting groups of speakers who talk to one another. This can be considered
a disentanglement task, although, as we will see (Section 5), the assumption that each
speaker participates in only one conversation is flawed.
Adams and Martell (2008) and Shen et al. (2006) publish results on human-
annotated data; although we do not have their corpora, we discuss their evaluation
metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki
et al. (2006) construct an annotated speech corpus, but they give no results for model
performance, only user satisfaction with their conversational system. Camtepe et al.
(2005) and Acar et al. (2005) do give performance results, but only on synthetic data.
Adams and Martell (2008) and Shen et al. (2006) treat disentanglement in the same
way we do, as a clustering task where the objects to be clustered are the individual
utterances. Both their algorithms define a notion of distance (based on the cosine) and
a threshold parameter determining how close to the cluster center an utterance must be
before the clustering algorithm adds it. This stands in contrast to our own supervised
approach, where the distance metric is explicitly tuned on training data. The super-
vised method has the advantage that it can weigh individual feature types based on
their predictivity—unsupervised methods combine features either uniformly or using
heuristic methods. There is also no need for a separate tuning phase to determine the
threshold. On the other hand, supervised methods require labeled training data, and
may be more difficult to adapt to novel domains or corpora.
The remaining papers treat the problem as one of clustering speakers, rather than
utterances. That is, they assume that during the window over which the system oper-
ates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005)
state an explicit assumption that this is true throughout the entire transcript; real speak-
ers, by contrast, often participate in many conversations, sequentially or sometimes
even simultaneously. Aoki et al. (2003) analyze each 30-second segment of the transcript
separately. This makes the single-conversation restriction somewhat less severe, but has
the disadvantage of ignoring all events which occur outside the segment.
Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to
cluster speakers; this assigns each speaker a distribution over conversations rather than
making a hard assignment. However, the algorithm still deals with speakers rather than
utterances, and cannot determine which conversation any particular utterance is part of.
Another problem with these two approaches is the information used for clustering.
Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and
use them to construct an affinity graph between participants by detecting turn-taking
behavior among pairs of speakers. (Turn-taking is typified by short pauses between
utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find
that turn-taking on its own is inadequate. They motivate a richer feature set, which,
however, does not yet appear to be implemented. Acar et al. (2005) add word repetition
to their feature set. However, their approach deals with all word repetitions on an equal
basis, and so degrades quickly in the presence of “noise words” (their term for words
which are shared across conversations) to almost complete failure when only half of the
words are shared.
Adams and Martell (2008) and Shen et al. (2006) use a more robust representation
for lexical features: Term frequency–inverse document frequency (TF–IDF) weighted
unigrams, as often used in information extraction. This feature set works fairly well
alone, although time is also a key feature as in the other studies. Adams and Martell
(2008) also investigate WordNet hypernyms (Miller et al. 1990) as a measure of semantic
relatedness, and use the identity of the speaker of a particular utterance as a feature. It
is unclear from their results whether these latter two features are effective or not.
</bodyText>
<page confidence="0.994432">
391
</page>
<note confidence="0.295454">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.99980794">
To motivate our own approach, we examine some linguistic studies of discourse,
especially analysis of multi-party conversation. O’Neill and Martin (2003) point out
several ways in which multi-party text chat differs from typical two-party conversation.
One key difference is the frequency with which participants mention each others’
names. They hypothesize that name mentioning is a strategy which participants use
to make disentanglement easier, compensating for the lack of cues normally present in
face-to-face dialogue. Mentions (such as Gale’s comments to Arlie in Figure 1) are very
common in our corpus, occurring in 36% of comments, and provide a useful feature.
Another key difference is that participants may create a new conversation (floor) at
any time, a process which Sacks, Schegloff, and Jefferson (1974) calls schisming. During
a schism, a new conversation is formed, not necessarily because of a shift in the topic,
but because certain participants have refocused their attention onto each other, and
away from whoever held the floor in the parent conversation.
Despite these differences, there are still strong similarities between chat and other
conversations such as meetings. Meetings do not typically allow multiple simultaneous
conversations, but analogues to schisms do exist, in the form of digressions or “sub-
ordinate conversations,” in which the speaker addresses someone specific, who is then
expected to answer. Some meeting analysis systems attempt to discover where these
digressions begin, who is involved in them, and who has the floor when they end;
features used in this work are relevant to disentanglement.
The task of automatically determining the intended recipient of an utterance in a
meeting is called addressee identification. It requires detecting digressions and identi-
fying their participants. Several studies attempt this task. Jovanovic and op den Akker
(2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identification
using a complex feature set including linguistic cues like pronouns and discourse mark-
ers, temporal information, and gaze direction. They also find that addressee identity
can be annotated with high reliability (κ = .7 for one set and .8 for another). Traum
(2004) discusses the necessity for addressee identification and disentanglement in the
design of a system for military dialogues involving virtual agents. Subsequent work
(Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy
on addressee identification.
Another meeting-related task is floor tracking, which attempts to determine which
speaker has the floor after each utterance. This task involves modeling the coordination
strategies which speakers use to acquire or give up the floor, and so provides a good
model of an ongoing conversation. A detailed analysis is given in Chen et al. (2006);
Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008)
use a conditional random fields (CRF) model to predict the next speaker in Supreme
Court oral argument transcripts.
A somewhat related area of research involves environments with higher latency
than real-time chat: message boards and e-mail. Content matching approaches tend
to work better in these settings, because while many chat messages are backchannel
responses or discourse requests (Yes, Why? and so forth), longer posts tend to be
contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic
information from message headers can be useful, as can content-based matching such
as detecting quotes from earlier messages. Wang et al. (2008), an analysis of a student
discussion group, is the work on which Adams and Martell (2008) is based, and uses
very similar methodology based on TF-IDF.
Our two-stage classification and partitioning algorithm draws on work on corefer-
ence resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001)
use such an approach, building global clusters based on pairwise decisions made by a
</bodyText>
<page confidence="0.986525">
392
</page>
<bodyText confidence="0.96796775">
Elsner and Charniak Disentangling Chat
classifier. The global partitioning problem was identified as correlation clustering, an
NP-hard problem, by McCallum and Wellner (2004).
Finally, we briefly mention some work which appeared after we developed the
system described here. Wang and Oard (2009) is another system that uses TF–IDF
unigrams, but augments these feature vectors using the information retrieval technique
of message expansion. They report results on our corpus which improve on our own.
Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to
describe semantic relatedness. He finds both techniques ineffective. In addition, he
annotates a large corpus of Internet Relay Chat and similarly finds that annotators have
trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies,
improving on the greedy algorithm that we present.
</bodyText>
<sectionHeader confidence="0.938032" genericHeader="method">
3. Data Set
</sectionHeader>
<bodyText confidence="0.9998563">
Our data set is recorded from IRC channel ##LINUX at free–node.net, using the freely
available gaim client. ##LINUX is an unofficial tech support line for the Linux operating
system, selected because it is one of the most active chat rooms on freenode, leading
to many simultaneous conversations, and because its content is typically inoffensive.
Although it is notionally intended only for tech support, it includes large amounts of
social chat as well, such as the conversation about factory work in Figure 1.
The entire data set contains over 52 hours of chat, but we devote most of our
attention to three annotated sections: development (706 utterances; 2:06 hr) and test
(800 utterances; 1:39 hr), plus a short pilot section on which we tested our annotation
system (359 utterances; 0:58 hr).
</bodyText>
<subsectionHeader confidence="0.998277">
3.1 Annotation
</subsectionHeader>
<bodyText confidence="0.999962235294118">
We recruited and paid seven university students to annotate the test section. All had
at least some familiarity with the Linux OS, although in some cases very slight. Anno-
tation of the test data set typically took them about two hours. In all, we produced six
annotations of the test set.2
We have four annotations of the pilot set, by three volunteers and the experimenters.
The pilot set was used to prototype our annotation software, and also as a validation cor-
pus for our system. The development set was annotated only once, by the experimenter.
This data set is used for training.
Our annotation scheme marks each utterance as part of a single conversation.
Annotators are instructed to create as many or as few conversations as they need to
describe the data. Our instructions state that a conversation can be between any number
of people, and that, “We mean conversation in the typical sense: a discussion in which
the participants are all reacting and paying attention to one another ... it should be clear
that the comments inside a conversation fit together.” The annotation system itself is a
simple Java program with a graphical interface, intended to appear somewhat similar
to a typical chat client. Each speaker’s name is displayed in a different color, and the
system displays the elapsed time between comments, marking especially long pauses
</bodyText>
<page confidence="0.7340195">
2 One additional annotation was discarded because the annotator misunderstood the task.
393
</page>
<note confidence="0.295839">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.9889025">
in red. Annotators group utterances into conversations by clicking and dragging them
onto each other.
</bodyText>
<subsectionHeader confidence="0.998843">
3.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999555035714286">
Before discussing the annotations themselves, we will describe the metrics we use to
compare different annotations; these measure both how much our annotators agree
with each other, and how well our model and various baselines perform. Comparing
clusterings with different numbers of clusters is a non-trivial task, and metrics for
agreement on supervised classification, such as the κ statistic, are not applicable.
To measure global similarity between annotations, we use one-to-one accuracy.
This measure describes how well we can extract whole conversations intact, as required
for summarization or information extraction. To compute it, we pair up conversations
from the two annotations to maximize the total overlap by computing an optimal max-
weight bipartite matching, then report the percentage of overlap found.3 One-to-one
accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi
and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference
resolution.
If we intend to monitor or participate in the conversation as it occurs, we will care
more about local judgments. The local agreement metric is a constrained form of the
Rand index for clusterings (Rand 1971) which counts agreements and disagreements for
pairs within a context k. We consider a particular utterance: The previous k utterances
are each in either the same or a different conversation. The lock score between two
annotators is their average agreement on these k same/different judgments, averaged
over all utterances. For example, loc1 counts pairs of adjacent utterances for which two
annotations agree.
Several related papers use some variant of the F-score metric to measure accuracy.
The most complete treatment is given in Shen et al. (2006). They use a micro-averaged
F-score, which is defined by constructing a multiway matching between conversations
in the two annotations. For a gold conversation i with size ni, and a proposed conver-
sation j with size nj, with overlap of size nij, they define precision and recall (plus the
standard balanced F-score). The F-score of an entire annotation is a weighted sum over
the matching:
</bodyText>
<equation confidence="0.771859333333333">
P= nij nij F(i,j) = 2PR �F = nin maxjF(i, j) (1)
R= ni P + R i
nj
</equation>
<bodyText confidence="0.999844142857143">
This is the F-score we report for comparative purposes. Because the match is multiway,
the score is not symmetric; when measuring agreement between pairs of human anno-
tators (where there is no reason for one to be considered gold), we map the high-entropy
transcript to the lower one (the entropy of a transcript is defined subsequently, in
Equation 2). Micro-averaged F-scores are also popular in work on document clustering.
In general, scores using this metric are correlated with our other measurement of global
consistency, the one-to-one accuracy.
</bodyText>
<footnote confidence="0.746757333333333">
3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to max
flow. The widely used greedy algorithm is a two-approximation, although we have not found large
differences in practice.
</footnote>
<page confidence="0.98649">
394
</page>
<bodyText confidence="0.9734917">
Elsner and Charniak Disentangling Chat
Adams and Martell (2008) also report F-score, but using a somewhat different
definition. They define F-score only between a particular pair of conversations, and
report the score for a single selected conversation. They do not describe how this
reference conversation is chosen. It is also unclear how they determine which proposed
conversation to match to it—the one with the best F-score, or the one which contains
the first (or “root”) utterance of the reference conversation. (The latter, although it may
be more useful for some applications, has an obvious problem—if the conversation is
retrieved perfectly except for the root utterance, the score will be zero.) For these reasons
we do not evaluate their metric.
</bodyText>
<subsectionHeader confidence="0.972445">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9592677">
A statistical examination of our data (Table 1) shows that there is a substantial amount
of disentanglement to do: the average number of conversations active at a time (the
density) is 2.75. Our annotators have high agreement on the local metric (average of
81.1%). On the one-to-one metric, they disagree more, with a mean overlap of 53.0% and
a maximum of only 63.5%. Though this level of agreement is low, naive baselines score
even lower (see Section 5). Therefore the metric does indeed distinguish human-like
from baseline performance. Thus measuring one-to-one overlap with our annotations
is a reasonable evaluation for computational models. However, we feel that the major
source of disagreement is one that can be remedied in future annotation schemes: the
specificity of the individual annotations.
To measure the level of detail in an annotation, we use the information-theoretic
entropy of the random variable, which indicates which conversation an utterance is
in. This variable has as many potential values as the number of conversations in the
transcript, each value having probability proportional to its size. Thus, for a transcript
of length n, with conversations i each having size ni, the entropy is:
� ni n log2 ni (2)
H(c) = n
i
This quantity is non-negative, increasing as the number of conversations grow and their
size becomes more balanced. It reaches its maximum, 9.64 bits for this data set, when
</bodyText>
<tableCaption confidence="0.671162666666667">
Table 1
Statistics on 6 annotations of 800 utterances of chat transcript. Inter-annotator agreement metrics
(below the line) are calculated between distinct pairs of annotations.
</tableCaption>
<table confidence="0.936020333333333">
Mean Max Min
Conversations 81.33 128 50
Average conversation length 10.6 16.0 6.2
Average conversation density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
one-to-one 52.98 63.50 35.63
loc3 81.09 86.53 74.75
Many-to-1 (by entropy) 86.70 94.13 75.50
Shen F (by entropy) 53.87 66.08 35.43
</table>
<page confidence="0.92609">
395
</page>
<note confidence="0.264981">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999854628571428">
each utterance is placed in a separate conversation. In our annotations, it ranges from
3.0 to 6.2. This large variation shows that some annotators are more specific than others,
but does not indicate how much they agree on the general structure. To measure this,
we introduce the many-to-one accuracy. This measurement is asymmetrical, and maps
each of the conversations of the source annotation to the single conversation in the
target with which it has the greatest overlap, then counts the total percentage of overlap.
This is not a statistic to be optimized (indeed, optimization is trivial: Simply make each
utterance in the source into its own conversation), but it can give us some intuition
about specificity. In particular, if one subdivides a coarse-grained annotation to make a
more specific variant, the many-to-one accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones (coarse), we find high many-to-one
accuracy, with a mean of 86%, which implies that the more specific annotations have
mostly the same large-scale boundaries as the coarser ones.
By examining the local metric, we can see even more: Local correlations are good,
at an average of 81.1%. This means that, in the three-sentence window preceding each
sentence, the annotators are often in agreement. If they recognize subdivisions of a large
conversation, these subdivisions tend to be contiguous, not mingled together, which is
why they have little impact on the local measure.
We find reasons for the annotators’ disagreement about appropriate levels of detail
in the linguistic literature. As mentioned, new conversations often break off from old
ones in schisms. Aoki et al. (2006) discuss conversational features associated with
schisming and the related process of affiliation, by which speakers attach themselves to
a conversation. Schisms often branch off from asides or even normal comments (toss-
outs) within an existing conversation. This means that there is no clear beginning to the
new conversation—at the time when it begins, it is not clear that there are two separate
floors, and this will not become clear until distinct sets of speakers and patterns of turn-
taking are established. Speakers, meanwhile, take time to orient themselves to the new
conversation. Example schisms are shown in Figures 2 and 3.
Our annotation scheme requires annotators to mark each utterance as part of a
single conversation, and distinct conversations are not related in any way. If a schism
occurs, the annotator is faced with two options: If it seems short, they may view it as
a mere digression and label it as part of the parent conversation. If it seems to deserve
a place of its own, they will have to separate it from the parent, but this severs the
initial comment (an otherwise unremarkable aside) from its context. One or two of the
annotators actually remarked that this made the task confusing. Our annotators seem
</bodyText>
<figureCaption confidence="0.751683">
Figure 2
</figureCaption>
<bodyText confidence="0.947829">
A schism occurring in our corpus (abridged). The schism-inducing turn is Kandra’s comment,
marked by arrows. Annotators 0 and 2 begin a new conversation with this turn; 1, 4, and 5 group
it with the other utterances shown; 3 creates new conversations for both this turn and Madison’s
question immediately following.
</bodyText>
<page confidence="0.992625">
396
</page>
<note confidence="0.676905">
Elsner and Charniak Disentangling Chat
</note>
<figureCaption confidence="0.91958">
Figure 3
</figureCaption>
<bodyText confidence="0.9956935">
A schism occurring in our corpus (abridged): not all annotators agree on where the thread about
charging for answers to technical questions diverges from the one about setting up Paypal
accounts. The schism begins just after Lai’s second comment (marked with arrows), to which
Gale and Azzie both respond (marked with double arrows). Annotators 1, 2, 4, and 5 begin a new
conversation with Gale’s response. Annotator 0 starts a new conversation with Azzie’s response.
Annotator 3 makes an error, linking the two responses to each other, but not to the parent.
</bodyText>
<figureCaption confidence="0.741448">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.453934">
Utterances versus conversations participated in per speaker on development data.
</subsectionHeader>
<bodyText confidence="0.999500375">
to be either “splitters” or “lumpers”—in other words, each annotator seems to aim for
a consistent level of detail, but each one has their own idea of what this level should be.
As a final observation about the data set, we test the appropriateness of the assump-
tion (used in previous work) that each speaker takes part in only one conversation. In
our data, the average speaker takes part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a speaker is, the more conversations they
participate in, as shown by a plot of conversations versus utterances (Figure 4). The
assumption is not very accurate, especially for speakers with more than 10 utterances.
</bodyText>
<sectionHeader confidence="0.940457" genericHeader="method">
4. Model
</sectionHeader>
<bodyText confidence="0.87653275">
Our model for disentanglement fits into the general class of graph partitioning algo-
rithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including
coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting seg-
mentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First,
</bodyText>
<page confidence="0.983424">
397
</page>
<table confidence="0.571868">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.981773">
Table 2
</tableCaption>
<table confidence="0.941118823529412">
Feature functions with performance on development data.
Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66)
Time The time between x and y in seconds, discretized into logarithmically sized bins.
Speaker x and y have the same speaker.
Mention x-y x mentions the speaker of y (or vice versa). For example, this feature is true for
a pair such as: Felicia “Gale: ... and any utterance spoken by Gale.
Mention same Both x and y mention the same name.
Mention other either x or y mentions a third person’s name.
Discourse (Acc: 52 Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting (hello etc.), an answer (yes, no etc.), or thanks.
Question Either asks a question (explicitly marked with ?).
Long Either is long (&gt; 10 words).
Content (Acc: 50 Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared between x and y which have unigram
probability i, binned logarithmically.
Tech Whether both x and y use technical jargon, neither do, or only one does.
Combined (Acc: 75 Prec: 73 Rec: 68 F: 71)
</table>
<bodyText confidence="0.6908885">
a binary classifier marks each pair of items as alike or different, and second, a consistent
partition is extracted.4
</bodyText>
<subsectionHeader confidence="0.952151">
4.1 Classification
</subsectionHeader>
<bodyText confidence="0.999897">
We use a maximum-entropy classifier (Daum´e 2004) to decide whether a pair of utter-
ances x and y are in same or different conversations. The most likely class is different,
which occurs 57% of the time in the development data. We describe the classifier’s per-
formance in terms of raw accuracy (correct decisions/total), precision and recall of the
same class, and F-score, the harmonic mean of precision and recall. Our classifier uses
several types of features (Table 2). The chat-specific features yield the highest accuracy
and precision. Discourse and content-based features have poor accuracy on their own
(worse than the baseline), because they work best on nearby pairs of utterances, and
tend to fail on more distant pairs. Paired with the time gap feature, however, they boost
accuracy somewhat and produce substantial gains in recall, encouraging the model to
group related utterances together.
The classifier is trained on our single annotation of the 706-utterance development
section and validated against the 359-utterance pilot section.
</bodyText>
<footnote confidence="0.8464495">
4 Our first attempt at this task used a Bayesian generative model. However, we could not define a sharp
enough posterior over new sentences, which made the model unstable and overly sensitive to its prior.
</footnote>
<page confidence="0.986856">
398
</page>
<note confidence="0.677727">
Elsner and Charniak Disentangling Chat
</note>
<figureCaption confidence="0.928873">
Figure 5
</figureCaption>
<bodyText confidence="0.993318166666667">
Distribution of pause length (log-scaled) between utterances in the same conversation.
The time gap, as discussed earlier, is the most widely used feature in previous work.
Our choice of a logarithmic binning scheme is intended to capture two characteristics of
the distribution of pause lengths (shown in Figure 5). The curve has its maximum at 1–
3 seconds, and pauses shorter than a second are less common. This reflects turn-taking
behavior among participants; participants in the same conversation prefer to wait for
each others’ responses before speaking again. On the other hand, the curve is quite
heavy-tailed to the right, leading us to bucket long pauses fairly coarsely. The specific
discretization we adopt for a time gap ∆ is bin(∆) = floor(lo$1.5(∆ + 1)). The particular
choice of 1.5 was chosen by hand to fit the observed scale of the curve.
Our discourse-based features model some pairwise relationships: questions fol-
lowed by answers, short comments reacting to longer ones, greetings at the beginning,
and thanks at the end.
Word repetition is a key feature in nearly every model for segmentation or coher-
ence, so it is no surprise that it is useful here. We discard the 50 most frequent words.
Then we bin all words by their unigram probability (bin(w) = floor(lo$10(p(w))) and
create an integer-valued feature for each bin, equal to the number of repeated words in
that bin. Unigram probabilities are calculated over the entire 52 hours of transcript. The
binning scheme allows us to deal with “noise words” which are repeated coincidentally,
because these occur in high-probability bins where repetitions are given less weight.
The point of the repetition feature is of course to detect sentences with similar
topics. We also find that sentences with technical content are more likely to be related
than non-technical sentences. We label an utterance as technical if it contains a Web
address, a long string of digits, or a term present in a guide for novice Linux users5
but not in a large news corpus (Graff 1995).6 This is a lightweight way to capture
one “semantic dimension” or cluster of related words. The technical word feature was
included because it improves our development classification score slightly, but it does
not have a significant effect on overall performance. Adams (2008) attempts to add
more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds
no improvement.
Pairs of utterances which are widely separated in the discourse are unlikely to be
directly related—even if they are part of the same conversation, the link between them
is probably a long chain of intervening utterances. Thus, if we run our classifier on a
pair of very distant utterances, we expect it to default to the majority class, which in this
case will be different, and this will damage our performance in case the two are really
part of the same conversation. To deal with this, we run our classifier only on utterances
</bodyText>
<footnote confidence="0.94586025">
5 Introduction to Linux: A Hands-on Guide. Machtelt Garrels. Edition 1.25 from
http://tldp.org/LDP/intro-linux/html/intro-linux.html.
6 Our data came from the LA Times, 1994–1997 — helpfully, this corpus predates the current wide coverage
of Linux in the mainstream press.
</footnote>
<page confidence="0.991449">
399
</page>
<figure confidence="0.819533">
Computational Linguistics Volume 36, Number 3
</figure>
<figureCaption confidence="0.667457">
Figure 6
VOTE algorithm.
</figureCaption>
<bodyText confidence="0.9873611">
separated by 129 seconds or less. The cutoff of 129 seconds was chosen because, for
utterances further apart than this, the classifier has no significant advantage over the
majority baseline. For 99.9% of utterances in an ongoing conversation, the previous
utterance in that conversation is within this gap, and so the system has a chance of
correctly linking the two.
On test data, the classifier has a mean accuracy of 68.2 (averaged over annotations).
The mean precision of same conversation is 53.3 and the recall is 71.3, with a mean
F-score of 60. This error rate is high, but the partitioning procedure allows us to recover
from some of the errors, because if nearby utterances are grouped correctly, the bad
decisions will be outvoted by good ones.
</bodyText>
<subsectionHeader confidence="0.987879">
4.2 Partitioning
</subsectionHeader>
<bodyText confidence="0.999556125">
The next step in the process is to cluster the utterances. We wish to find a set of clusters
for which the weighted accuracy of the classifier would be maximal; this is an example
of correlation clustering (Bansal, Blum, and Chawla 2004), which is NP-complete. The
input to our partitioning procedure is a graph with a node for each utterance; if the
classifier connects utterances i and j with probability p, we take the weight wij of edge ij
to be the log odds log(pij/(1 − pij)).7 We create a variable xij for each pair of utterances,
which is 1 if the utterances are placed in the same conversation, and 0 if they are
separated. The log probability of the clustering, treating the edges as independent, is
</bodyText>
<equation confidence="0.646766">
E
</equation>
<bodyText confidence="0.9992212">
ij:i&lt;j wijxij. We attempt to maximize this quantity, subject to the constraint that the xij
must form a legitimate clustering such that xij = xjk = 1 implies xij = xik.
Finding an exact solution proves to be difficult; the problem has a quadratic number
of variables (one for each pair of utterances) and a cubic number of triangle inequality
constraints (three for each triplet).8 With 800 utterances in our test set, even solving
the linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to be
practical.
Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) show
that a relatively good solution can be obtained using a greedy voting algorithm (Fig-
ure 6). In this algorithm, we assign utterance j by examining all previously assigned
</bodyText>
<footnote confidence="0.9846715">
7 The original version of our system used a different weighting scheme, wij = pij − .5. The log-odds ratio
behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms
or tuning (see Section 6), so, for simplicity, we present it here as well.
8 There is a triangle inequality constraint for each triplet i, j, k: (1 − xik) &lt; (1 − xij) + (1 − xjk).
</footnote>
<page confidence="0.991816">
400
</page>
<bodyText confidence="0.921170111111111">
Elsner and Charniak Disentangling Chat
utterances i, and treating the classifier’s judgment wij as a vote for cluster(i). If the
maximum vote is greater than 0, we set cluster(j) = argmaxc zotec. Otherwise j is put
in a new cluster.
If the utterances are considered in order, this is a natural on-line algorithm—it
assigns each utterance as it arrives, without reference to the future. Elsner and Schudy
(2009) show that performance can be improved by approximately 6% on the one-to-one
and F-score metrics using offline randomized and local search methods. The loc3 metric
is insensitive to these more complex search procedures.
</bodyText>
<sectionHeader confidence="0.995967" genericHeader="method">
5. Experiments
</sectionHeader>
<bodyText confidence="0.992924476190476">
We annotate the 800-line test transcript using our system. The annotation obtained has
62 conversations, with mean length 12.90. The average density of conversations is 2.86,
and the entropy is 3.72. This places it within the bounds of our human annotations (see
Table 1), toward the more general end of the spectrum.
As a standard of comparison for our system, we provide results for several
baselines—trivial systems which any useful annotation should outperform.
All different Each utterance is a separate conversation.
All same The whole transcript is a single conversation.
Blocks of k Each consecutive group of k utterances is a conversation.
Pause of k Each pause of k seconds or more separates two conversations.
Speaker Each speaker’s utterances are treated as a monologue.
For each particular metric, we calculate the best baseline result among all of these.
To find the best block size or pause length, we search over multiples of five between
5 and 300. This makes these baselines appear better than they really are, because their
performance is optimized with respect to the test data. (A complete table of baseline
results is shown in Figure 7.)
We also calculate results for two more systems. One is a non-trivial baseline:
Time/mention Our system, using only time gap and mention-based features.
The other is an oracle, designed to test how well a segmentation system designed for
meeting or lecture data might possibly do on this task. If no conversation were ever
interrupted, such a system would be perfect (up to the limit of annotator agreement).
</bodyText>
<figureCaption confidence="0.987202">
Figure 7
</figureCaption>
<bodyText confidence="0.309631">
Metric values for all baselines.
</bodyText>
<page confidence="0.991773">
401
</page>
<table confidence="0.414056">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.70097">
Table 3
Metric values between proposed annotations and human annotations. Model scores typically fall
between inter-annotator agreement and baseline performance.
</tableCaption>
<table confidence="0.9998614">
Annotators Model Time/ment. Perf. Seg. Best Baseline
Mean one-to-one 52.98 41.23 38.62 26.20 35.08 (Pause 35)
Max one-to-one 63.50 52.12 44.12 36.50 56.00 (Pause 65)
Min one-to-one 35.63 31.62 30.62 15.38 27.50 (Blocks 80)
Mean loc3 81.09 72.94 68.69 75.98 62.16 (Speaker)
Max loc3 86.53 74.70 70.93 85.40 69.05 (Speaker)
Min loc3 74.75 70.77 66.37 69.05 54.37 (Speaker)
Mean Shen F 53.87 43.47 41.31 35.50 36.58 (Speaker)
Max Shen F 66.08 57.57 48.85 46.70 46.79 (Speaker)
Min Shen F 35.43 32.97 32.07 21.83 29.09 (Blocks 65)
</table>
<bodyText confidence="0.98669178125">
Perfect segments The transcript is divided into contiguous segments, where all utter-
ances in a segment belong to the same conversation. The conversation assign-
ments are determined by the human annotation whose agreement with the others
is highest.
Our results, in Table 3, are encouraging. On average, annotators agree more with
each other than with any artificial annotation, and more with our model than with
the baselines. For the one-to-one accuracy metric, we cannot claim much beyond these
general results. The range of human variation is quite wide, and there are annotators
who are closer to baselines than to any other human annotator. As explained earlier,
this is because some human annotations are much more specific than others. For very
specific annotations, the best baselines are short blocks or pauses. For the most general,
marking all utterances the same does very well (although for all other annotations, it is
extremely poor).
For the local metric, the results are much clearer. There is no overlap in the ranges;
for every test annotation, agreement is highest with other annotators, then our model,
and finally the baselines. The most competitive baseline is one conversation per speaker,
which makes sense, since if a speaker makes two comments in a four-utterance win-
dow, they are very likely to be related.
The Shen F-score metric seems to perform similarly to the one-to-one accuracy,
which is unsurprising because they are both measures of global consistency. The largest
difference between them is that the speaker baseline outperforms blocks and pauses in
F-score (although not by very much), perhaps because it is more precise.
Shen et al. (2006) report higher F-scores for their own best model: It obtains an
F-score of 61.2, whereas our model’s mean score is only 43.4. Because of the different
corpora, we are unable to explain this difference. Better results are also reported in Wang
and Oard (2009) and Elsner and Schudy (2009) (see Table 4).
Mention information alone is not sufficient for disentanglement; with only name
mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name
mention features are critical for our model. Without them, the classifier’s development
F-score drops from 71 to 56. The disentanglement system’s test performance decreases
proportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline per-
formance. For some utterances, of course, name mentions provide the only reasonable
</bodyText>
<page confidence="0.997253">
402
</page>
<note confidence="0.785633">
Elsner and Charniak Disentangling Chat
</note>
<tableCaption confidence="0.995002">
Table 4
</tableCaption>
<table confidence="0.795266833333333">
Results reported by others on the same task.
Result F-score Notes
this model 43.4
Elsner and Schudy (2009) 50 improved partitioning inference
Wang and Oard (2009) 54 message expansion features
Shen et al. (2006) 61.2 different corpus
</table>
<bodyText confidence="0.999875363636364">
clue to the correct decision, which is why humans mention names in the first place.
But our system is probably overly dependent on them, because they are very reliable
compared to our other features.
Because of the frequency with which conversations interleave, perfect segmenta-
tion alone is not sufficient to optimize either global metric, and generally does not
outperform the baselines. For the local metric, however, it generally does better than the
model. Here, performance depends mainly on whether the system can find the bound-
aries between one conversation and another, and it is less important to link the segments
of a particular conversation to one another, since these different segments often lie
outside the three-utterance horizon. Systems designed to detect segment boundaries,
like those for meetings, might contribute to improvement of this metric.
</bodyText>
<sectionHeader confidence="0.91163" genericHeader="method">
6. Specificity Tuning
</sectionHeader>
<bodyText confidence="0.999769142857143">
Although our analysis shows that individual annotators can produce more or less
specific annotations of the same conversation, the system described here can produce
only a single annotation (for any given set of training data) with a fixed specificity. Now
we attempt to control the specificity parametrically, producing more and less specific
annotations on demand, without retraining the classifier.
The parameter we choose to alter is the bias of our pairwise classifier. A maximum-
entropy classifier has the form:
</bodyText>
<equation confidence="0.994814">
y __ 1
(x) (3)
1 + exp(−(w · x + b))
</equation>
<bodyText confidence="0.999909384615385">
Here w represents the vector of feature weights and b is the bias term; a positive b shifts
all judgments toward high-confidence same conversation decisions and a negative b
shifts them away. To alter the classifier, we add a constant λ to b. In general, increasing
the number and confidence of same decisions leads to larger, coarser partitionings,
and decreasing it creates smaller, finer ones. We measure specificity by examining the
entropy of the output annotation. Although entropy is generally an increasing function
of λ, the relationship is not always smooth, nor is it completely monotonic. Figure 8
plots entropy as a function of λ.
In Figure 9, we plot the one-to-one match between each test annotation and the
altered annotations produced by this method, as a function of the entropy. The unbiased
system creates an annotation with entropy 3.7. Although this yields reasonable results
for all human annotations, each of the annotations has a point of higher performance
at a different bias level. For instance, the line uppermost on the left side of the plot
</bodyText>
<page confidence="0.998246">
403
</page>
<figure confidence="0.716024">
Computational Linguistics Volume 36, Number 3
</figure>
<figureCaption confidence="0.985451">
Figure 8
</figureCaption>
<bodyText confidence="0.9933395">
Entropy of the output annotation produced with bias factor λ on test data. λ = 0 corresponds to
the unbiased system.
shows overlap with a human transcript whose entropy is 3.0 bits; lower-entropy system
annotations correspond better with this annotator’s judgments.
For each human annotation, we evaluate the tuned system’s performance at the
entropy level of the original annotation. (This point is marked by the large dot on
</bodyText>
<figureCaption confidence="0.773308">
Figure 9
</figureCaption>
<bodyText confidence="0.98743">
One-to-one accuracy between biased system annotations and each test annotation, as a function
of entropy. The vertical line (at 3.72 bits) marks the scores obtained by the unbiased system with
λ = 0. The large dot on each line is the score obtained at the entropy of the human annotation.
</bodyText>
<page confidence="0.997886">
404
</page>
<note confidence="0.83589">
Elsner and Charniak Disentangling Chat
</note>
<tableCaption confidence="0.67429875">
Table 5
Metric values between proposed annotations and human annotations on test data. The tuned
model (evaluated at the entropy of the human annotations) improves on one-to-one accuracy but
not on loc3.
</tableCaption>
<table confidence="0.999668857142857">
Unbiased Model Tuned Model
Mean one-to-one 41.23 48.52
Max one-to-one 52.13 58.75
Min one-to-one 31.66 40.88
Mean loc3 72.94 73.64
Max loc3 74.70 75.87
Min loc3 70.77 69.95
</table>
<bodyText confidence="0.99701340909091">
each line in the figure.) To do this, we perform a line search over λ until we produce
a clustering whose entropy is within .25 bits of the original’s, then evaluate. In other
words, we measure performance given an additional piece of supervision—the annota-
tor’s preferred specificity level.
Results on the one-to-one metric are fairly good: Extreme and average scores are
listed in Table 5. The effects of this technique on the local metric are small (and in
many cases negative). This is not entirely surprising, as the local metric is less sensitive
to specificity of annotations. Slight positive effects occur only for the most and least
specific annotation, which are presumably so extreme that specificity begins to have a
slight effect even on local decisions.
Despite fairly large performance increases on the test set, we do not consider this
technique really reliable, because the relationship between the bias parameter and final
score is not smooth. Small changes in the bias can cause large shifts in entropy, and small
changes in entropy can have large effects on quality. (For instance, two annotations have
a sharp decline in score at about entropy 5.7, losing about 5% of performance with a
change of just over .1 bit.) Therefore it is not clear exactly how to choose a bias parameter
which will yield good performance. Matching the entropy of a human annotation seems
to work on the test data, but fails to improve scores on our development data. Moreover,
although for methodological simplicity we assume access to the exact target entropy for
each annotation, it is unlikely that a real user could express their desired specificity
so precisely. Figuring out a way to let the user select the desired entropy remains a
challenge.
</bodyText>
<sectionHeader confidence="0.743744" genericHeader="method">
7. Detecting Conversation Starts
</sectionHeader>
<bodyText confidence="0.999922222222222">
In this section, we investigate better ways to find the beginnings of new conversations.
In the pairwise-linkage representation presented earlier, a new conversation is begun
when none of the previous utterances is strongly linked to the current utterance. This
representation spreads out the responsibility for detecting a new conversation over
many pairwise decisions. We are inspired by the use of discourse-new classifiers (also
called anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to find
NPs which begin coreferential chains. Oracle experiments show that a similar detector
for utterances which begin conversations could improve disentanglement scores if it
were available. We attempt to develop such a detector, but without much success.
</bodyText>
<page confidence="0.997734">
405
</page>
<note confidence="0.454017">
Computational Linguistics Volume 36, Number 3
</note>
<tableCaption confidence="0.7756895">
Table 6
Metric values using an oracle new-conversation detector on test data.
</tableCaption>
<table confidence="0.999787285714286">
Original Model +Oracle New Conversations
Mean one-to-one 41.23 46.75
Max one-to-one 52.13 53.50
Min one-to-one 31.66 42.13
Mean loc3 72.94 73.90
Max loc3 74.70 76.49
Min loc3 70.77 70.72
</table>
<bodyText confidence="0.999738111111111">
As a demonstration of the gains possible if a good classifier could be developed,
we show the oracle improvements possible on the test data, using an optimal new-
conversation detector as a hard constraint on inference (Table 6). The oracle detector
detects a conversation start if it occurs in the majority of human annotations, and the
inference algorithm is forced to start a new conversation if and only if the oracle has
detected one. Good conversation detection is capable of improving not only one-to-one
accuracy but local accuracy as well.
We can track the performance of realistic, non-oracle new-conversation detection
via the precision, recall, and F-score of the new conversation class (Table 7). As a
starting point, we report the accuracy obtained by the pairwise-linkage model and
greedy inference already presented. At 49% F-score, it is clearly not doing a good job.
It is possible to do better than this using information already represented in the
pairwise classifier: The time since the speaker of the utterance last spoke (logarithmi-
cally bucketed), and whether the utterance mentions a name. A better representation for
the problem allows the classifier to make somewhat more effective use of these features.
For reasons we cannot explain, adding discourse features like the presence of a question
or greeting does not improve performance. The simple classifier does improve slightly
on the baseline, up to 51%. These test results, however, are somewhat surprising to us.
On our development corpus, the corresponding scores are 69% and 75%. Because that
corpus contains an average (over three annotations) of 34 conversations, it is likely that
we were misled by coincidentally good results.
On the development set, where the classifier works well, its decisions can be inte-
grated with inference to yield substantial improvements in actual system performance.
Mean loc3 increases from 72% to about 78% and mean one-to-one accuracy from 41%
to about 66%. However, we find no improvement at all on the test data, because
the classifier has very low recall, and the resulting test annotations have far too few
conversations.
</bodyText>
<tableCaption confidence="0.992286">
Table 7
</tableCaption>
<table confidence="0.8896935">
Precision, recall, and F-score of the new conversation class on test data (average 81
conversations).
Precision Recall F-score
Pairwise system 56.08 43.44 48.96
Time/Mention Features 68.06 40.16 50.52
Human Annotators 64.30 61.70 61.14
</table>
<page confidence="0.980031">
406
</page>
<note confidence="0.730819">
Elsner and Charniak Disentangling Chat
</note>
<sectionHeader confidence="0.952166" genericHeader="method">
8. Future Work
</sectionHeader>
<bodyText confidence="0.9998993">
Although our annotators are reasonably reliable, it seems clear that they think of con-
versations as a hierarchy, with digressions and schisms. We are interested to see an an-
notation protocol which more closely follows human intuition. One suggestion (David
Traum, personal communication) is to drop the idea of partitioning entirely and have
annotators mark the data as a graph, linking each utterance to its parents and children
with links of various strengths. Such a scheme might yield more reliable annotations
than our current one, although testing this hypothesis would require new annotation
software and a different set of metrics. Any new annotation project should also investi-
gate whether annotators can define their desired specificity, and with what precision.
Our results on new conversation detection suggest that a high-performance clas-
sifier for this task could improve results substantially. It is also interesting to consider,
given the weakness of our technical words feature and the disappointing results using
Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be use-
fully modeled.
Finally, we are interested to see how well this feature set performs on speech data,
as in Aoki et al. (2003). Spoken conversation is more natural than text chat, but even
when participants are face-to-face, disentanglement remains a problem. On the other
hand, spoken dialogue contains new sources of information, such as prosody and gaze
direction. Turn-taking behavior is also more distinct, which makes the task easier, but
according to Aoki et al. (2006), it is certainly not sufficient.
</bodyText>
<sectionHeader confidence="0.990612" genericHeader="conclusions">
9. Conclusion
</sectionHeader>
<bodyText confidence="0.999952">
This work provides a corpus of annotated data for chat disentanglement, which, along
with our proposed metrics, should allow future researchers to evaluate and compare
their results quantitatively.9 Our annotations are consistent with one another, especially
with respect to local agreement. We show that features based on discourse patterns and
the content of utterances are helpful in disentanglement. The model we present can
outperform a variety of baselines.
</bodyText>
<sectionHeader confidence="0.997686" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994182333333333">
Our thanks to Suman Karumuri, Steve
Sloman, Matt Lease, David McClosky, seven
test annotators, three pilot annotators, three
anonymous conference reviewers, three
anonymous journal reviewers, and the NSF
PIRE grant. We would also like to thank
Craig Martell and David Traum for their
very useful comments at the conference
presentation.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998765782608696">
Acar, Evrim, Seyit Ahmet Camtepe,
Mukkai S. Krishnamoorthy, and B¨ulent
Yener. 2005. Modeling and multiway
analysis of chatroom tensors. In Paul B.
Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang,
Hsinchun Chen, and Ralph C. Merkle,
editors, ISI, volume 3495 of Lecture Notes
in Computer Science. Springer, Berlin,
pages 256–268.
Adams, Paige H. 2008. Conversation Thread
Extraction and Topic Detection in Text-based
Chat. Ph.D. thesis, Naval Postgraduate
School.
Adams, Paige H. and Craig H. Martell. 2008.
Topic detection and extraction in chat.
International Conference on Semantic
Computing, 2:581–588.
Aoki, Paul M., Matthew Romaine,
Margaret H. Szymanski, James D.
Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter’s
cocktail party: A social mobile audio
</reference>
<footnote confidence="0.310622">
9 Our software and data set are publicly available from cs.brown.edu/∼melsner.
</footnote>
<page confidence="0.988808">
407
</page>
<reference confidence="0.986907193277311">
Computational Linguistics Volume 36, Number 3
space supporting multiple simultaneous
conversations. In CHI ’03: Proceedings of
the SIGCHI Conference on Human Factors
in Computing Systems, pages 425–432,
New York, NY.
Aoki, Paul M., Margaret H. Szymanski,
Luke D. Plurkowski, James D. Thornton,
Allison Woodruff, and Weilie Yi. 2006.
Where’s the “party” in “multi-party”?:
Analyzing the structure of small-group
sociable talk. In CSCW ’06: Proceedings of
the 2006 20th Anniversary Conference on
Computer Supported Cooperative Work,
pages 393–402, New York, NY.
Bansal, Nikhil, Avrim Blum, and Shuchi
Chawla. 2004. Correlation clustering.
Machine Learning, 56(1-3):89–113.
Camtepe, Seyit Ahmet, Mark K. Goldberg,
Malik Magdon-Ismail, and Mukkai
Krishnamoorty. 2005. Detecting
conversing groups of chatters: A model,
algorithms, and tests. In IADIS AC,
pages 89–96, Algarve.
Chen, Lei. 2008. Incorporating Nonverbal
Features into Multimodal Models of
Human-to-Human Communication.
Ph.D. thesis, Purdue University.
Chen, Lei, Mary Harper, Amy Franklin,
Travis R. Rose, Irene Kimbara,
Zhongqiang Huang, and Francis
Quek. 2006. A multimodal analysis
of floor control in meetings. In
Proceedings of MLMI 06, pages 36–49,
Bethesda, MD.
Daum´e, III, Hal. 2004. Notes on CG and
LM-BFGS optimization of logistic
regression. Paper available at http://
pub.hal3.name#daume04cg-bfgs.
Implementation available at http://
hal3.name/megam/.
Elsner, Micha and Warren Schudy. 2009.
Bounding and comparing methods for
correlation clustering beyond ILP. In
Proceedings of ILP-NLP, pages 19–27,
Boulder, CO.
Graff, David. 1995. North American News
Text Corpus. Linguistic Data Consortium.
LDC95T21.
Haghighi, Aria and Dan Klein. 2006.
Prototype-driven learning for sequence
models. In Proceedings of HLT-NAACL,
pages 320–327, New York, NY.
Hawes, Timothy, Jimmy Lin, and
Philip Resnik. 2008. Elements of a
computational model for multi-party
discourse: The turn-taking behavior
of supreme court justices. Technical
Report LAMP-TR-147/HCIL-2008-02,
University of Maryland, College Park.
Ilog, Inc. 2003. CPLEX solver. Available at
www-01.ibm.com/software/websphere/
ilog migration.html.
Jovanovic, Natasa and Rieks op den
Akker. 2004. Towards automatic
addressee identification in multi-party
dialogues. In Proceedings of the
5th SIGdial Workshop, pages 89–92,
Cambridge, MA.
Jovanovic, Natasa, Rieks op den Akker,
and Anton Nijholt. 2006. Addressee
identification in face-to-face meetings. In
Proceedings of EACL, Trento.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of HLT-EMNLP, pages 25–32,
Morristown, NJ.
Malioutov, Igor and Regina Barzilay. 2006.
Minimum cut model for spoken lecture
segmentation. In Proceedings of ACL,
pages 25–32, Sydney.
McCallum, Andrew and Ben Wellner.
2004. Conditional models of identity
uncertainty with application to noun
coreference. In Proceedings of the 18th
Annual Conference on Neural Information
Processing Systems (NIPS), pages 905–912,
Vancouver.
Miller, G., A. R. Beckwith, C. Fellbaum,
D. Gross, and K. Miller. 1990. Introduction
to Wordnet: An on-line lexical database.
International Journal of Lexicography,
3(4):235–244.
Ng, Vincent and Claire Cardie. 2002.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In COLING, Taipei.
O’Neill, Jacki and David Martin. 2003. Text
chat in action. In GROUP ’03: Proceedings
of the 2003 International ACM SIGGROUP
Conference on Supporting Group Work,
pages 40–49, New York, NY.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846–850.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1–8,
Boston, MA.
Sacks, Harvey, Emanuel A. Schegloff,
and Gail Jefferson. 1974. A simplest
systematics for the organization of
turn-taking for conversation. Language,
50(4):696–735.
Shen, Dou, Qiang Yang, Jian-Tao Sun,
and Zheng Chen. 2006. Thread detection
in dynamic text message streams. In
</reference>
<page confidence="0.976262">
408
</page>
<note confidence="0.499007">
Elsner and Charniak Disentangling Chat
</note>
<reference confidence="0.999501166666667">
SIGIR ’06: Proceedings of the 29th Annual
International ACM SIGIR Conference,
pages 35–42, New York, NY.
Soon, Wee Meng, Hwee Tou Ng, and
Daniel Chung Yong Lim. 2001. A
machine learning approach to
coreference resolution of noun
phrases. Computational Linguistics,
27(4):521–544.
Traum, D. 2004. Issues in multi-party
dialogues. In F. Dignum, editor, Advances
in Agent Communication. Springer Verlag,
Berlin, pages 201–211.
Traum, David R., Susan Robinson, and
Jens Stephan. 2004. Evaluation of
multi-party virtual reality dialogue
interaction. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC),
pages 1699–1702, Lisbon.
Wang, Lidan and Douglas W. Oard. 2009.
Context-based message expansion for
disentanglement of interleaved text
conversations. In Proceedings of
HLT-NAACL, pages 200–208, Boulder, CO.
Wang, Yi-Chia, Mahesh Joshi, William
Cohen, and Carolyn Ros´e. 2008.
Recovering implicit thread structure in
newsgroup style conversations. In
Proceedings of the 2nd International
Conference on Weblogs and Social Media
(ICWSM II), Seattle, WA.
Yeh, Jen-Yuan and Aaron Harnly. 2006.
Email thread reassembly using similarity
matching. In Conference on Email and
Anti-Spam, Mountain View, CA.
</reference>
<page confidence="0.999108">
409
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9753308">Disentangling Chat Brown Laboratory for Linguistic Information Processing (BLLIP) Brown Laboratory for Linguistic Information Processing (BLLIP)</title>
<abstract confidence="0.96224732">When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. We propose a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based features. The model’s predicted disentanglements are highly correlated with manual annotations. We conclude by discussing two extensions to the model, specificity tuning and conversation start detection, both of which are promising but do not currently yield practical improvements. 1. Motivation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among people found an average of 1.76 conversations active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, does not form a contiguous segment of the chatroom transcript, but is frequently broken up by interposed utterances from other conversations. (also called detection et al. 2006], extraction and Martell 2008], and management 2004]) is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis. For instance, consider the multi-party exchange in Figure 1. it is clear that this corresponds to two conversations, and intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.</abstract>
<email confidence="0.961954">E-mail:melsner@cs.brown.edu.</email>
<note confidence="0.769006">Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912.</note>
<email confidence="0.614341">E-mail:ec@cs.brown.edu.</email>
<note confidence="0.925336333333333">1 Real user nicknames are replaced with randomly selected identifiers for ethical reasons. Submission received: 27 January 2009; revised submission received: 1 November 2009; accepted for publication: 3 March 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 Figure 1</note>
<abstract confidence="0.997821357414448">Some (abridged) conversation from our corpus. Humans are adept at disentanglement, even in complicated environments like crowded cocktail parties or chat rooms; in order to perform this task, they must maintain a complex mental representation of the ongoing discourse. Moreover, they adjust their conversational behavior to make the task easier, mentioning names more frequently than in spoken or two-party typed dialogues (O’Neill and Martin 2003). This need for adaptation suggests that disentanglement can be challenging even for humans, and therefore can serve as a useful stress test for computational models of discourse. Disentanglement has two practical applications. One is the analysis of pre-recorded transcripts in order to extract some kind of information, such as question–answer pairs or summaries. These tasks should probably take as input each separate conversation, rather than the entire transcript. Another application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system—when the disentanglement worked! We begin in Section 2 with an overview of related work. In Section 3, we present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics (Section 3.2) describing structural similarity both locally and globally. In Section 4, we propose a model which uses supervised pairwise classification to link utterances from the same conversation, followed by a greedy inference stage which clusters the utterances into conversations. Our system uses time gap and utterance content features. Experimental results (Section 5) show that its output is highly correlated with human annotations. Finally, in Sections 6 and 7, we investigate two extensions to the basic model, specificity tuning and automatic detection of conversation starts. To our knowledge, this is the first work to evaluate interannotator agreement for the disentanglement task. It is also the first to use a supervised method to learn weights for different feature types, rather than relying on cosine distance with uniform or handtuned feature weights. It supplements standard word repetition and time gap features with other feature types, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen et al. (2006) publish results on humanannotated data; although we do not have their corpora, we discuss their evaluation metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki et al. (2006) construct an annotated speech corpus, but they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. Adams and Martell (2008) and Shen et al. (2006) treat disentanglement in the same way we do, as a clustering task where the objects to be clustered are the individual utterances. Both their algorithms define a notion of distance (based on the cosine) and a threshold parameter determining how close to the cluster center an utterance must be before the clustering algorithm adds it. This stands in contrast to our own supervised approach, where the distance metric is explicitly tuned on training data. The supervised method has the advantage that it can weigh individual feature types based on their predictivity—unsupervised methods combine features either uniformly or using heuristic methods. There is also no need for a separate tuning phase to determine the threshold. On the other hand, supervised methods require labeled training data, and may be more difficult to adapt to novel domains or corpora. The remaining papers treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) state an explicit assumption that this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each 30-second segment of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than making a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these two approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) add word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of “noise words” (their term for words which are shared across conversations) to almost complete failure when only half of the words are shared. Adams and Martell (2008) and Shen et al. (2006) use a more robust representation for lexical features: Term frequency–inverse document frequency (TF–IDF) weighted unigrams, as often used in information extraction. This feature set works fairly well alone, although time is also a key feature as in the other studies. Adams and Martell (2008) also investigate WordNet hypernyms (Miller et al. 1990) as a measure of semantic relatedness, and use the identity of the speaker of a particular utterance as a feature. It is unclear from their results whether these latter two features are effective or not. 391 Computational Linguistics Volume 36, Number 3 To motivate our own approach, we examine some linguistic studies of discourse, especially analysis of multi-party conversation. O’Neill and Martin (2003) point out several ways in which multi-party text chat differs from typical two-party conversation. One key difference is the frequency with which participants mention each others’ names. They hypothesize that name mentioning is a strategy which participants use to make disentanglement easier, compensating for the lack of cues normally present in face-to-face dialogue. Mentions (such as Gale’s comments to Arlie in Figure 1) are very common in our corpus, occurring in 36% of comments, and provide a useful feature. Another key difference is that participants may create a new conversation (floor) at time, a process which Sacks, Schegloff, and Jefferson (1974) calls During a schism, a new conversation is formed, not necessarily because of a shift in the topic, but because certain participants have refocused their attention onto each other, and away from whoever held the floor in the parent conversation. Despite these differences, there are still strong similarities between chat and other conversations such as meetings. Meetings do not typically allow multiple simultaneous conversations, but analogues to schisms do exist, in the form of digressions or “subordinate conversations,” in which the speaker addresses someone specific, who is then expected to answer. Some meeting analysis systems attempt to discover where these digressions begin, who is involved in them, and who has the floor when they end; features used in this work are relevant to disentanglement. The task of automatically determining the intended recipient of an utterance in a is called It requires detecting digressions and identifying their participants. Several studies attempt this task. Jovanovic and op den Akker (2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identification using a complex feature set including linguistic cues like pronouns and discourse markers, temporal information, and gaze direction. They also find that addressee identity be annotated with high reliability = for one set and for another). Traum (2004) discusses the necessity for addressee identification and disentanglement in the design of a system for military dialogues involving virtual agents. Subsequent work (Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy on addressee identification. meeting-related task is which attempts to determine which speaker has the floor after each utterance. This task involves modeling the coordination strategies which speakers use to acquire or give up the floor, and so provides a good model of an ongoing conversation. A detailed analysis is given in Chen et al. (2006); Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008) use a conditional random fields (CRF) model to predict the next speaker in Supreme Court oral argument transcripts. A somewhat related area of research involves environments with higher latency than real-time chat: message boards and e-mail. Content matching approaches tend to work better in these settings, because while many chat messages are backchannel or discourse requests so forth), longer posts tend to be contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic information from message headers can be useful, as can content-based matching such as detecting quotes from earlier messages. Wang et al. (2008), an analysis of a student discussion group, is the work on which Adams and Martell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classification and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat The global partitioning problem was identified as an NP-hard problem, by McCallum and Wellner (2004). Finally, we briefly mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He finds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly finds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Data Set data set is recorded from IRC channel using the freely an unofficial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factory work in Figure 1. The entire data set contains over 52 hours of chat, but we devote most of our attention to three annotated sections: development (706 utterances; 2:06 hr) and test (800 utterances; 1:39 hr), plus a short pilot section on which we tested our annotation system (359 utterances; 0:58 hr). 3.1 Annotation We recruited and paid seven university students to annotate the test section. All had at least some familiarity with the Linux OS, although in some cases very slight. Annotation of the test data set typically took them about two hours. In all, we produced six of the test We have four annotations of the pilot set, by three volunteers and the experimenters. The pilot set was used to prototype our annotation software, and also as a validation corpus for our system. The development set was annotated only once, by the experimenter. This data set is used for training. Our annotation scheme marks each utterance as part of a single conversation. Annotators are instructed to create as many or as few conversations as they need to describe the data. Our instructions state that a conversation can be between any number of people, and that, “We mean conversation in the typical sense: a discussion in which the participants are all reacting and paying attention to one another ... it should be clear that the comments inside a conversation fit together.” The annotation system itself is a simple Java program with a graphical interface, intended to appear somewhat similar to a typical chat client. Each speaker’s name is displayed in a different color, and the system displays the elapsed time between comments, marking especially long pauses 2 One additional annotation was discarded because the annotator misunderstood the task. 393 Computational Linguistics Volume 36, Number 3 in red. Annotators group utterances into conversations by clicking and dragging them onto each other. 3.2 Metrics Before discussing the annotations themselves, we will describe the metrics we use to compare different annotations; these measure both how much our annotators agree with each other, and how well our model and various baselines perform. Comparing clusterings with different numbers of clusters is a non-trivial task, and metrics for on supervised classification, such as the are not applicable. measure global similarity between annotations, we use This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxbipartite matching, then report the percentage of overlap One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi Klein 2006), and is equivalent to CEAF 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care about local judgments. The agreement is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for within a context We consider a particular utterance: The previous each in either the a The between two is their average agreement on these judgments, averaged all utterances. For example, pairs of adjacent utterances for which two annotations agree. related papers use some variant of the to measure accuracy. The most complete treatment is given in Shen et al. (2006). They use a micro-averaged F-score, which is defined by constructing a multiway matching between conversations the two annotations. For a gold conversation size and a proposed conversize with overlap of size they define precision and recall (plus the standard balanced F-score). The F-score of an entire annotation is a weighted sum over the matching: i (1) This is the F-score we report for comparative purposes. Because the match is multiway, the score is not symmetric; when measuring agreement between pairs of human annotators (where there is no reason for one to be considered gold), we map the high-entropy transcript to the lower one (the entropy of a transcript is defined subsequently, in Equation 2). Micro-averaged F-scores are also popular in work on document clustering. In general, scores using this metric are correlated with our other measurement of global consistency, the one-to-one accuracy. 3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to max flow. The widely used greedy algorithm is a two-approximation, although we have not found large differences in practice. 394 Elsner and Charniak Disentangling Chat Adams and Martell (2008) also report F-score, but using a somewhat different definition. They define F-score only between a particular pair of conversations, and report the score for a single selected conversation. They do not describe how this reference conversation is chosen. It is also unclear how they determine which proposed conversation to match to it—the one with the best F-score, or the one which contains the first (or “root”) utterance of the reference conversation. (The latter, although it may be more useful for some applications, has an obvious problem—if the conversation is perfectly the root utterance, the score will be zero.) For these reasons we do not evaluate their metric. 3.3 Discussion A statistical examination of our data (Table 1) shows that there is a substantial amount of disentanglement to do: the average number of conversations active at a time (the is 2.75. Our annotators have high agreement on the local metric (average of 81.1%). On the one-to-one metric, they disagree more, with a mean overlap of 53.0% and a maximum of only 63.5%. Though this level of agreement is low, naive baselines score even lower (see Section 5). Therefore the metric does indeed distinguish human-like from baseline performance. Thus measuring one-to-one overlap with our annotations is a reasonable evaluation for computational models. However, we feel that the major source of disagreement is one that can be remedied in future annotation schemes: the specificity of the individual annotations. To measure the level of detail in an annotation, we use the information-theoretic the random variable, which indicates which conversation an utterance is in. This variable has as many potential values as the number of conversations in the transcript, each value having probability proportional to its size. Thus, for a transcript length with conversations having size the entropy is: � i n This quantity is non-negative, increasing as the number of conversations grow and their becomes more balanced. It reaches its maximum, bits for this data set, when Table 1 Statistics on 6 annotations of 800 utterances of chat transcript. Inter-annotator agreement metrics (below the line) are calculated between distinct pairs of annotations.</abstract>
<author confidence="0.863929">Mean Max Min</author>
<note confidence="0.8681174">Conversations 81.33 128 50 Average conversation length 10.6 16.0 6.2 Average conversation density 2.75 2.92 2.53 Entropy 4.83 6.18 3.00 one-to-one 52.98 63.50 35.63 81.09 86.53 74.75 Many-to-1 (by entropy) 86.70 94.13 75.50 Shen F (by entropy) 53.87 66.08 35.43 395 Computational Linguistics Volume 36, Number 3</note>
<abstract confidence="0.99169499090909">each utterance is placed in a separate conversation. In our annotations, it ranges from 3.0 to 6.2. This large variation shows that some annotators are more specific than others, but does not indicate how much they agree on the general structure. To measure this, introduce the This measurement is asymmetrical, and maps of the conversations of the to the single conversation in the which it has the greatest overlap, then counts the total percentage of overlap. This is not a statistic to be optimized (indeed, optimization is trivial: Simply make each utterance in the source into its own conversation), but it can give us some intuition about specificity. In particular, if one subdivides a coarse-grained annotation to make a more specific variant, the many-to-one accuracy from fine to coarse remains 1. When we map high-entropy annotations (fine) to lower ones (coarse), we find high many-to-one accuracy, with a mean of 86%, which implies that the more specific annotations have mostly the same large-scale boundaries as the coarser ones. By examining the local metric, we can see even more: Local correlations are good, at an average of 81.1%. This means that, in the three-sentence window preceding each sentence, the annotators are often in agreement. If they recognize subdivisions of a large conversation, these subdivisions tend to be contiguous, not mingled together, which is why they have little impact on the local measure. We find reasons for the annotators’ disagreement about appropriate levels of detail in the linguistic literature. As mentioned, new conversations often break off from old ones in schisms. Aoki et al. (2006) discuss conversational features associated with and the related process of by which speakers attach themselves to conversation. Schisms often branch off from asides or even normal comments within an existing conversation. This means that there is no clear beginning to the new conversation—at the time when it begins, it is not clear that there are two separate floors, and this will not become clear until distinct sets of speakers and patterns of turntaking are established. Speakers, meanwhile, take time to orient themselves to the new conversation. Example schisms are shown in Figures 2 and 3. Our annotation scheme requires annotators to mark each utterance as part of a single conversation, and distinct conversations are not related in any way. If a schism occurs, the annotator is faced with two options: If it seems short, they may view it as a mere digression and label it as part of the parent conversation. If it seems to deserve a place of its own, they will have to separate it from the parent, but this severs the initial comment (an otherwise unremarkable aside) from its context. One or two of the annotators actually remarked that this made the task confusing. Our annotators seem Figure 2 A schism occurring in our corpus (abridged). The schism-inducing turn is Kandra’s comment, marked by arrows. Annotators 0 and 2 begin a new conversation with this turn; 1, 4, and 5 group it with the other utterances shown; 3 creates new conversations for both this turn and Madison’s question immediately following. 396 Elsner and Charniak Disentangling Chat Figure 3 A schism occurring in our corpus (abridged): not all annotators agree on where the thread about charging for answers to technical questions diverges from the one about setting up Paypal accounts. The schism begins just after Lai’s second comment (marked with arrows), to which Gale and Azzie both respond (marked with double arrows). Annotators 1, 2, 4, and 5 begin a new conversation with Gale’s response. Annotator 0 starts a new conversation with Azzie’s response. Annotator 3 makes an error, linking the two responses to each other, but not to the parent. Figure 4 Utterances versus conversations participated in per speaker on development data. to be either “splitters” or “lumpers”—in other words, each annotator seems to aim for a consistent level of detail, but each one has their own idea of what this level should be. As a final observation about the data set, we test the appropriateness of the assumption (used in previous work) that each speaker takes part in only one conversation. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66) The time between seconds, discretized into logarithmically sized bins. the same speaker. x-y the speaker of vice versa). For example, this feature is true for pair such as: “Gale: ... any utterance spoken by Gale. same Both the same name. other either a third person’s name. Discourse (Acc: 52 Prec: 47 Rec: 77 F: 58) words Either a greeting an answer or thanks. Either asks a question (explicitly marked with Either is long words). Content (Acc: 50 Prec: 45 Rec: 74 F: 56) The number of words shared between have unigram binned logarithmically. Whether both technical jargon, neither do, or only one does. Combined (Acc: 75 Prec: 73 Rec: 68 F: 71) a binary classifier marks each pair of items as alike or different, and second, a consistent is 4.1 Classification We use a maximum-entropy classifier (Daum´e 2004) to decide whether a pair of utterin The most likely class is which occurs 57% of the time in the development data. We describe the classifier’s perin terms of raw accuracy (correct precision and recall of the and F-score, the harmonic mean of precision and recall. Our classifier uses several types of features (Table 2). The chat-specific features yield the highest accuracy and precision. Discourse and content-based features have poor accuracy on their own (worse than the baseline), because they work best on nearby pairs of utterances, and tend to fail on more distant pairs. Paired with the time gap feature, however, they boost accuracy somewhat and produce substantial gains in recall, encouraging the model to group related utterances together. The classifier is trained on our single annotation of the 706-utterance development section and validated against the 359-utterance pilot section. 4 Our first attempt at this task used a Bayesian generative model. However, we could not define a sharp enough posterior over new sentences, which made the model unstable and overly sensitive to its prior. 398 Elsner and Charniak Disentangling Chat Figure 5 Distribution of pause length (log-scaled) between utterances in the same conversation. The time gap, as discussed earlier, is the most widely used feature in previous work. Our choice of a logarithmic binning scheme is intended to capture two characteristics of the distribution of pause lengths (shown in Figure 5). The curve has its maximum at 1– 3 seconds, and pauses shorter than a second are less common. This reflects turn-taking behavior among participants; participants in the same conversation prefer to wait for each others’ responses before speaking again. On the other hand, the curve is quite heavy-tailed to the right, leading us to bucket long pauses fairly coarsely. The specific we adopt for a time gap + The particular choice of 1.5 was chosen by hand to fit the observed scale of the curve. Our discourse-based features model some pairwise relationships: questions followed by answers, short comments reacting to longer ones, greetings at the beginning, and thanks at the end. Word repetition is a key feature in nearly every model for segmentation or coherence, so it is no surprise that it is useful here. We discard the 50 most frequent words. we bin all words by their unigram probability and create an integer-valued feature for each bin, equal to the number of repeated words in that bin. Unigram probabilities are calculated over the entire 52 hours of transcript. The binning scheme allows us to deal with “noise words” which are repeated coincidentally, because these occur in high-probability bins where repetitions are given less weight. The point of the repetition feature is of course to detect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a Web a long string of digits, or a term present in a guide for novice Linux not in a large news corpus (Graff This is a lightweight way to capture one “semantic dimension” or cluster of related words. The technical word feature was included because it improves our development classification score slightly, but it does not have a significant effect on overall performance. Adams (2008) attempts to add more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds no improvement. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related—even if they are part of the same conversation, the link between them is probably a long chain of intervening utterances. Thus, if we run our classifier on a pair of very distant utterances, we expect it to default to the majority class, which in this will be and this will damage our performance in case the two are really part of the same conversation. To deal with this, we run our classifier only on utterances to Linux: A Hands-on Machtelt Garrels. Edition 1.25 from Our data came from the 1994–1997 — helpfully, this corpus predates the current wide coverage of Linux in the mainstream press. 399 Computational Linguistics Volume 36, Number 3 Figure 6 separated by 129 seconds or less. The cutoff of 129 seconds was chosen because, for utterances further apart than this, the classifier has no significant advantage over the majority baseline. For 99.9% of utterances in an ongoing conversation, the previous utterance in that conversation is within this gap, and so the system has a chance of correctly linking the two. test data, the classifier has a mean accuracy of (averaged over annotations). mean precision of conversation and the recall is with a mean F-score of 60. This error rate is high, but the partitioning procedure allows us to recover from some of the errors, because if nearby utterances are grouped correctly, the bad decisions will be outvoted by good ones. 4.2 Partitioning The next step in the process is to cluster the utterances. We wish to find a set of clusters for which the weighted accuracy of the classifier would be maximal; this is an example clustering Blum, and Chawla 2004), which is NP-complete. The input to our partitioning procedure is a graph with a node for each utterance; if the connects utterances probability we take the weight of edge be the log odds We create a variable for each pair of utterances, which is 1 if the utterances are placed in the same conversation, and 0 if they are separated. The log probability of the clustering, treating the edges as independent, is E We attempt to maximize this quantity, subject to the constraint that the form a legitimate clustering such that implies Finding an exact solution proves to be difficult; the problem has a quadratic number of variables (one for each pair of utterances) and a cubic number of triangle inequality (three for each With 800 utterances in our test set, even solving the linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to be practical. Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) show that a relatively good solution can be obtained using a greedy voting algorithm (Fig- 6). In this algorithm, we assign utterance examining all previously assigned The original version of our system used a different weighting scheme, The log-odds ratio behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms or tuning (see Section 6), so, for simplicity, we present it here as well. There is a triangle inequality constraint for each triplet (1 400 Elsner and Charniak Disentangling Chat and treating the classifier’s judgment as a vote for If the vote is greater than 0, we set Otherwise put in a new cluster. If the utterances are considered in order, this is a natural on-line algorithm—it assigns each utterance as it arrives, without reference to the future. Elsner and Schudy (2009) show that performance can be improved by approximately 6% on the one-to-one F-score metrics using offline randomized and local search methods. The is insensitive to these more complex search procedures. 5. Experiments We annotate the 800-line test transcript using our system. The annotation obtained has conversations, with mean length The average density of conversations is the entropy is This places it within the bounds of our human annotations (see Table 1), toward the more general end of the spectrum. As a standard of comparison for our system, we provide results for several baselines—trivial systems which any useful annotation should outperform. different utterance is a separate conversation. same whole transcript is a single conversation. of consecutive group of is a conversation. of pause of or more separates two conversations. speaker’s utterances are treated as a monologue. For each particular metric, we calculate the best baseline result among all of these. To find the best block size or pause length, we search over multiples of five between 5 and 300. This makes these baselines appear better than they really are, because their performance is optimized with respect to the test data. (A complete table of baseline results is shown in Figure 7.) We also calculate results for two more systems. One is a non-trivial baseline: system, using only time gap and mention-based features. The other is an oracle, designed to test how well a segmentation system designed for meeting or lecture data might possibly do on this task. If no conversation were ever interrupted, such a system would be perfect (up to the limit of annotator agreement). Figure 7 Metric values for all baselines. 401 Computational Linguistics Volume 36, Number 3 Table 3 Metric values between proposed annotations and human annotations. Model scores typically fall between inter-annotator agreement and baseline performance.</abstract>
<note confidence="0.64942825">Annotators Model Time/ment. Perf. Seg. Best Baseline Mean one-to-one 52.98 41.23 38.62 26.20 35.08 (Pause 35) Max one-to-one 63.50 52.12 44.12 36.50 56.00 (Pause 65) Min one-to-one 35.63 31.62 30.62 15.38 27.50 (Blocks 80) 81.09 72.94 68.69 75.98 62.16 (Speaker) 86.53 74.70 70.93 85.40 69.05 (Speaker) 74.75 70.77 66.37 69.05 54.37 (Speaker) Mean Shen F 53.87 43.47 41.31 35.50 36.58 (Speaker)</note>
<author confidence="0.856493">Min Shen F</author>
<abstract confidence="0.957361757575757">segments transcript is divided into contiguous segments, where all utterances in a segment belong to the same conversation. The conversation assignments are determined by the human annotation whose agreement with the others is highest. Our results, in Table 3, are encouraging. On average, annotators agree more with each other than with any artificial annotation, and more with our model than with the baselines. For the one-to-one accuracy metric, we cannot claim much beyond these general results. The range of human variation is quite wide, and there are annotators who are closer to baselines than to any other human annotator. As explained earlier, this is because some human annotations are much more specific than others. For very specific annotations, the best baselines are short blocks or pauses. For the most general, marking all utterances the same does very well (although for all other annotations, it is extremely poor). For the local metric, the results are much clearer. There is no overlap in the ranges; for every test annotation, agreement is highest with other annotators, then our model, and finally the baselines. The most competitive baseline is one conversation per speaker, which makes sense, since if a speaker makes two comments in a four-utterance window, they are very likely to be related. The Shen F-score metric seems to perform similarly to the one-to-one accuracy, which is unsurprising because they are both measures of global consistency. The largest difference between them is that the speaker baseline outperforms blocks and pauses in F-score (although not by very much), perhaps because it is more precise. Shen et al. (2006) report higher F-scores for their own best model: It obtains an F-score of 61.2, whereas our model’s mean score is only 43.4. Because of the different corpora, we are unable to explain this difference. Better results are also reported in Wang and Oard (2009) and Elsner and Schudy (2009) (see Table 4). Mention information alone is not sufficient for disentanglement; with only name and time gap features, mean one-to-one is 38 and 69. However, name mention features are critical for our model. Without them, the classifier’s development F-score drops from 71 to 56. The disentanglement system’s test performance decreases mean one-to-one falls to 36, and mean 63, essentially baseline performance. For some utterances, of course, name mentions provide the only reasonable 402 Elsner and Charniak Disentangling Chat Table 4 Results reported by others on the same task. Result F-score Notes this model 43.4 Elsner and Schudy (2009) 50 improved partitioning inference Wang and Oard (2009) 54 message expansion features Shen et al. (2006) 61.2 different corpus clue to the correct decision, which is why humans mention names in the first place. But our system is probably overly dependent on them, because they are very reliable compared to our other features. Because of the frequency with which conversations interleave, perfect segmentation alone is not sufficient to optimize either global metric, and generally does not outperform the baselines. For the local metric, however, it generally does better than the model. Here, performance depends mainly on whether the system can find the boundaries between one conversation and another, and it is less important to link the segments of a particular conversation to one another, since these different segments often lie outside the three-utterance horizon. Systems designed to detect segment boundaries, like those for meetings, might contribute to improvement of this metric. 6. Specificity Tuning Although our analysis shows that individual annotators can produce more or less specific annotations of the same conversation, the system described here can produce only a single annotation (for any given set of training data) with a fixed specificity. Now we attempt to control the specificity parametrically, producing more and less specific annotations on demand, without retraining the classifier. The parameter we choose to alter is the bias of our pairwise classifier. A maximumentropy classifier has the form: 1 the vector of feature weights and the bias term; a positive judgments toward high-confidence conversation and a negative them away. To alter the classifier, we add a constant In general, increasing number and confidence of leads to larger, coarser partitionings, and decreasing it creates smaller, finer ones. We measure specificity by examining the entropy of the output annotation. Although entropy is generally an increasing function the relationship is not always smooth, nor is it completely monotonic. Figure 8 entropy as a function of In Figure 9, we plot the one-to-one match between each test annotation and the altered annotations produced by this method, as a function of the entropy. The unbiased system creates an annotation with entropy 3.7. Although this yields reasonable results for all human annotations, each of the annotations has a point of higher performance at a different bias level. For instance, the line uppermost on the left side of the plot 403 Computational Linguistics Volume 36, Number 3 Figure 8 of the output annotation produced with bias factor test data. = corresponds to the unbiased system. shows overlap with a human transcript whose entropy is 3.0 bits; lower-entropy system annotations correspond better with this annotator’s judgments. For each human annotation, we evaluate the tuned system’s performance at the entropy level of the original annotation. (This point is marked by the large dot on Figure 9 One-to-one accuracy between biased system annotations and each test annotation, as a function of entropy. The vertical line (at 3.72 bits) marks the scores obtained by the unbiased system with = The large dot on each line is the score obtained at the entropy of the human annotation. 404 Elsner and Charniak Disentangling Chat Table 5 Metric values between proposed annotations and human annotations on test data. The tuned model (evaluated at the entropy of the human annotations) improves on one-to-one accuracy but on Unbiased Model Tuned Model Mean one-to-one 41.23 48.52 Max one-to-one 52.13 58.75 Min one-to-one 31.66 40.88 72.94 73.64 74.70 75.87 70.77 69.95 line in the figure.) To do this, we perform a line search over we produce clustering whose entropy is within bits of the original’s, then evaluate. In other words, we measure performance given an additional piece of supervision—the annotator’s preferred specificity level. Results on the one-to-one metric are fairly good: Extreme and average scores are listed in Table 5. The effects of this technique on the local metric are small (and in many cases negative). This is not entirely surprising, as the local metric is less sensitive to specificity of annotations. Slight positive effects occur only for the most and least specific annotation, which are presumably so extreme that specificity begins to have a slight effect even on local decisions. Despite fairly large performance increases on the test set, we do not consider this technique really reliable, because the relationship between the bias parameter and final score is not smooth. Small changes in the bias can cause large shifts in entropy, and small changes in entropy can have large effects on quality. (For instance, two annotations have a sharp decline in score at about entropy 5.7, losing about 5% of performance with a change of just over .1 bit.) Therefore it is not clear exactly how to choose a bias parameter which will yield good performance. Matching the entropy of a human annotation seems to work on the test data, but fails to improve scores on our development data. Moreover, although for methodological simplicity we assume access to the exact target entropy for each annotation, it is unlikely that a real user could express their desired specificity so precisely. Figuring out a way to let the user select the desired entropy remains a challenge. 7. Detecting Conversation Starts In this section, we investigate better ways to find the beginnings of new conversations. In the pairwise-linkage representation presented earlier, a new conversation is begun when none of the previous utterances is strongly linked to the current utterance. This representation spreads out the responsibility for detecting a new conversation over many pairwise decisions. We are inspired by the use of discourse-new classifiers (also called anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to find NPs which begin coreferential chains. Oracle experiments show that a similar detector for utterances which begin conversations could improve disentanglement scores if it were available. We attempt to develop such a detector, but without much success.</abstract>
<note confidence="0.78165825">405 Computational Linguistics Volume 36, Number 3 Table 6 Metric values using an oracle new-conversation detector on test data. Original Model +Oracle New Conversations Mean one-to-one 41.23 46.75 Max one-to-one 52.13 53.50 Min one-to-one 31.66 42.13</note>
<address confidence="0.521681">72.94 73.90 74.70 76.49 70.77 70.72</address>
<abstract confidence="0.922821472972973">As a demonstration of the gains possible if a good classifier could be developed, we show the oracle improvements possible on the test data, using an optimal newconversation detector as a hard constraint on inference (Table 6). The oracle detector detects a conversation start if it occurs in the majority of human annotations, and the inference algorithm is forced to start a new conversation if and only if the oracle has detected one. Good conversation detection is capable of improving not only one-to-one accuracy but local accuracy as well. We can track the performance of realistic, non-oracle new-conversation detection the precision, recall, and F-score of the conversation (Table 7). As a starting point, we report the accuracy obtained by the pairwise-linkage model and greedy inference already presented. At 49% F-score, it is clearly not doing a good job. It is possible to do better than this using information already represented in the pairwise classifier: The time since the speaker of the utterance last spoke (logarithmically bucketed), and whether the utterance mentions a name. A better representation for the problem allows the classifier to make somewhat more effective use of these features. For reasons we cannot explain, adding discourse features like the presence of a question or greeting does not improve performance. The simple classifier does improve slightly on the baseline, up to 51%. These test results, however, are somewhat surprising to us. On our development corpus, the corresponding scores are 69% and 75%. Because that corpus contains an average (over three annotations) of 34 conversations, it is likely that we were misled by coincidentally good results. On the development set, where the classifier works well, its decisions can be integrated with inference to yield substantial improvements in actual system performance. from 72% to about 78% and mean one-to-one accuracy from 41% to about 66%. However, we find no improvement at all on the test data, because the classifier has very low recall, and the resulting test annotations have far too few conversations. Table 7 recall, and F-score of the conversation on test data (average 81 conversations). Precision Recall F-score Pairwise system 56.08 43.44 48.96 Time/Mention Features 68.06 40.16 50.52 Human Annotators 64.30 61.70 61.14 406 Elsner and Charniak Disentangling Chat 8. Future Work Although our annotators are reasonably reliable, it seems clear that they think of conversations as a hierarchy, with digressions and schisms. We are interested to see an annotation protocol which more closely follows human intuition. One suggestion (David Traum, personal communication) is to drop the idea of partitioning entirely and have annotators mark the data as a graph, linking each utterance to its parents and children with links of various strengths. Such a scheme might yield more reliable annotations than our current one, although testing this hypothesis would require new annotation software and a different set of metrics. Any new annotation project should also investigate whether annotators can define their desired specificity, and with what precision. Our results on new conversation detection suggest that a high-performance classifier for this task could improve results substantially. It is also interesting to consider, given the weakness of our technical words feature and the disappointing results using Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be usefully modeled. Finally, we are interested to see how well this feature set performs on speech data, as in Aoki et al. (2003). Spoken conversation is more natural than text chat, but even when participants are face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody and gaze direction. Turn-taking behavior is also more distinct, which makes the task easier, but according to Aoki et al. (2006), it is certainly not sufficient. 9. Conclusion This work provides a corpus of annotated data for chat disentanglement, which, along with our proposed metrics, should allow future researchers to evaluate and compare results Our annotations are consistent with one another, especially with respect to local agreement. We show that features based on discourse patterns and the content of utterances are helpful in disentanglement. The model we present can outperform a variety of baselines. Acknowledgments Our thanks to Suman Karumuri, Steve Sloman, Matt Lease, David McClosky, seven test annotators, three pilot annotators, three anonymous conference reviewers, three anonymous journal reviewers, and the NSF PIRE grant. We would also like to thank Craig Martell and David Traum for their very useful comments at the conference presentation.</abstract>
<note confidence="0.520752333333333">References Acar, Evrim, Seyit Ahmet Camtepe, Mukkai S. Krishnamoorthy, and B¨ulent Yener. 2005. Modeling and multiway analysis of chatroom tensors. In Paul B. Kantor, Gheorghe Muresan, Fred Roberts,</note>
<author confidence="0.7592465">Daniel Dajun Zeng</author>
<author confidence="0.7592465">Fei-Yue Wang</author>
<author confidence="0.7592465">Hsinchun Chen</author>
<author confidence="0.7592465">Ralph C Merkle</author>
<note confidence="0.756889861111111">volume 3495 of Notes Computer Springer, Berlin, pages 256–268. Paige H. 2008. Thread Extraction and Topic Detection in Text-based Ph.D. thesis, Naval Postgraduate School. Adams, Paige H. and Craig H. Martell. 2008. Topic detection and extraction in chat. International Conference on Semantic 2:581–588. Aoki, Paul M., Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The mad hatter’s cocktail party: A social mobile audio Our software and data set are publicly available from 407 Computational Linguistics Volume 36, Number 3 space supporting multiple simultaneous In ’03: Proceedings of the SIGCHI Conference on Human Factors Computing pages 425–432, New York, NY. Aoki, Paul M., Margaret H. Szymanski, Luke D. Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. 2006. Where’s the “party” in “multi-party”?: Analyzing the structure of small-group talk. In ’06: Proceedings of the 2006 20th Anniversary Conference on Supported Cooperative pages 393–402, New York, NY. Bansal, Nikhil, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. 56(1-3):89–113.</note>
<author confidence="0.273233">Seyit Ahmet Camtepe</author>
<author confidence="0.273233">Mark K Goldberg</author>
<email confidence="0.262382">MalikMagdon-Ismail,andMukkai</email>
<abstract confidence="0.892594">Krishnamoorty. 2005. Detecting conversing groups of chatters: A model, and tests. In</abstract>
<note confidence="0.757142689655172">pages 89–96, Algarve. Lei. 2008. Nonverbal Features into Multimodal Models of Ph.D. thesis, Purdue University. Chen, Lei, Mary Harper, Amy Franklin, Travis R. Rose, Irene Kimbara, Zhongqiang Huang, and Francis Quek. 2006. A multimodal analysis of floor control in meetings. In of MLMI pages 36–49, Bethesda, MD. Daum´e, III, Hal. 2004. Notes on CG and LM-BFGS optimization of logistic Paper available at available at Elsner, Micha and Warren Schudy. 2009. Bounding and comparing methods for correlation clustering beyond ILP. In of pages 19–27, Boulder, CO. David. 1995. American News Linguistic Data Consortium. LDC95T21. Haghighi, Aria and Dan Klein. 2006. Prototype-driven learning for sequence In of pages 320–327, New York, NY. Hawes, Timothy, Jimmy Lin, and Philip Resnik. 2008. Elements of a</note>
<abstract confidence="0.962335">computational model for multi-party discourse: The turn-taking behavior of supreme court justices. Technical</abstract>
<pubnum confidence="0.809508">Report LAMP-TR-147/HCIL-2008-02,</pubnum>
<affiliation confidence="0.956582">University of Maryland, College Park.</affiliation>
<address confidence="0.698619">Ilog, Inc. 2003. CPLEX solver. Available at</address>
<abstract confidence="0.733710769230769">www-01.ibm.com/software/websphere/ Jovanovic, Natasa and Rieks op den Akker. 2004. Towards automatic addressee identification in multi-party In of the SIGdial pages 89–92, Cambridge, MA. Jovanovic, Natasa, Rieks op den Akker, and Anton Nijholt. 2006. Addressee identification in face-to-face meetings. In of Trento. Luo, Xiaoqiang. 2005. On coreference resolution performance metrics. In</abstract>
<note confidence="0.659873428571428">of pages 25–32, Morristown, NJ. Malioutov, Igor and Regina Barzilay. 2006. Minimum cut model for spoken lecture In of pages 25–32, Sydney. McCallum, Andrew and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun In of the 18th Annual Conference on Neural Information Systems pages 905–912, Vancouver. Miller, G., A. R. Beckwith, C. Fellbaum,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evrim Acar</author>
<author>Seyit Ahmet Camtepe</author>
<author>Mukkai S Krishnamoorthy</author>
<author>B¨ulent Yener</author>
</authors>
<title>Modeling and multiway analysis of chatroom tensors.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>3495</volume>
<pages>256--268</pages>
<editor>In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI,</editor>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="5696" citStr="Acar et al. (2005)" startWordPosition="829" endWordPosition="832">es, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen et al. (2006) publish results on humanannotated data; although we do not have their corpora, we discuss their evaluation metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki et al. (2006) construct an annotated speech corpus, but th</context>
<context position="8101" citStr="Acar et al. (2005)" startWordPosition="1207" endWordPosition="1210">han utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) state an explicit assumption that this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each 30-second segment of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than making a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these two approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among</context>
</contexts>
<marker>Acar, Camtepe, Krishnamoorthy, Yener, 2005</marker>
<rawString>Acar, Evrim, Seyit Ahmet Camtepe, Mukkai S. Krishnamoorthy, and B¨ulent Yener. 2005. Modeling and multiway analysis of chatroom tensors. In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI, volume 3495 of Lecture Notes in Computer Science. Springer, Berlin, pages 256–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paige H Adams</author>
</authors>
<title>Conversation Thread Extraction and Topic Detection in Text-based Chat.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Naval Postgraduate School.</institution>
<contexts>
<context position="14529" citStr="Adams (2008)" startWordPosition="2196" endWordPosition="2197">Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classifier. The global partitioning problem was identified as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we briefly mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He finds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly finds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Data Set Our data set is recorded from IRC channel ##LINUX at free–node.net, using the freely available gaim client. ##LINUX is an unofficial tech support line for the Linux operating system, selected because it is on</context>
<context position="32915" citStr="Adams (2008)" startWordPosition="5165" endWordPosition="5166">tect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a Web address, a long string of digits, or a term present in a guide for novice Linux users5 but not in a large news corpus (Graff 1995).6 This is a lightweight way to capture one “semantic dimension” or cluster of related words. The technical word feature was included because it improves our development classification score slightly, but it does not have a significant effect on overall performance. Adams (2008) attempts to add more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds no improvement. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related—even if they are part of the same conversation, the link between them is probably a long chain of intervening utterances. Thus, if we run our classifier on a pair of very distant utterances, we expect it to default to the majority class, which in this case will be different, and this will damage our performance in case the two are really part of the same conversation. To deal wi</context>
<context position="52459" citStr="Adams (2008)" startWordPosition="8306" endWordPosition="8307">uch a scheme might yield more reliable annotations than our current one, although testing this hypothesis would require new annotation software and a different set of metrics. Any new annotation project should also investigate whether annotators can define their desired specificity, and with what precision. Our results on new conversation detection suggest that a high-performance classifier for this task could improve results substantially. It is also interesting to consider, given the weakness of our technical words feature and the disappointing results using Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be usefully modeled. Finally, we are interested to see how well this feature set performs on speech data, as in Aoki et al. (2003). Spoken conversation is more natural than text chat, but even when participants are face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody and gaze direction. Turn-taking behavior is also more distinct, which makes the task easier, but according to Aoki et al. (2006), it is certainly not sufficient. 9. Conclusion This work provides a corpus of annotated</context>
</contexts>
<marker>Adams, 2008</marker>
<rawString>Adams, Paige H. 2008. Conversation Thread Extraction and Topic Detection in Text-based Chat. Ph.D. thesis, Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paige H Adams</author>
<author>Craig H Martell</author>
</authors>
<title>Topic detection and extraction</title>
<date>2008</date>
<booktitle>in chat. International Conference on Semantic Computing,</booktitle>
<pages>2--581</pages>
<contexts>
<context position="1567" citStr="Adams and Martell 2008" startWordPosition="216" endWordPosition="219">ation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among 8–10 people found an average of 1.76 conversations (floors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, does not form a contiguous segment of the chatroom transcript, but is frequently broken up by interposed utterances from other conversations. Disentanglement (also called thread detection [Shen et al. 2006], thread extraction [Adams and Martell 2008], and thread/conversation management [Traum 2004]) is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis. For instance, consider the multi-party exchange in Figure 1. Contextually, it is clear that this corresponds to two conversations, and Felicia’s1 response excellent is intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. * Brown Laboratory for Linguistic Information Proces</context>
<context position="5550" citStr="Adams and Martell (2008)" startWordPosition="804" endWordPosition="807">lying on cosine distance with uniform or handtuned feature weights. It supplements standard word repetition and time gap features with other feature types, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen et al. (2006) publish results on humanannotated data; although we do not have their corpora, we discuss their evaluation </context>
<context position="9341" citStr="Adams and Martell (2008)" startWordPosition="1403" endWordPosition="1406">rs. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) add word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of “noise words” (their term for words which are shared across conversations) to almost complete failure when only half of the words are shared. Adams and Martell (2008) and Shen et al. (2006) use a more robust representation for lexical features: Term frequency–inverse document frequency (TF–IDF) weighted unigrams, as often used in information extraction. This feature set works fairly well alone, although time is also a key feature as in the other studies. Adams and Martell (2008) also investigate WordNet hypernyms (Miller et al. 1990) as a measure of semantic relatedness, and use the identity of the speaker of a particular utterance as a feature. It is unclear from their results whether these latter two features are effective or not. 391 Computational Lingu</context>
<context position="13698" citStr="Adams and Martell (2008)" startWordPosition="2070" endWordPosition="2073">of research involves environments with higher latency than real-time chat: message boards and e-mail. Content matching approaches tend to work better in these settings, because while many chat messages are backchannel responses or discourse requests (Yes, Why? and so forth), longer posts tend to be contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic information from message headers can be useful, as can content-based matching such as detecting quotes from earlier messages. Wang et al. (2008), an analysis of a student discussion group, is the work on which Adams and Martell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classification and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classifier. The global partitioning problem was identified as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we briefly mention some work which appeared after we developed the system described here. Wang a</context>
<context position="20423" citStr="Adams and Martell (2008)" startWordPosition="3130" endWordPosition="3133">d gold), we map the high-entropy transcript to the lower one (the entropy of a transcript is defined subsequently, in Equation 2). Micro-averaged F-scores are also popular in work on document clustering. In general, scores using this metric are correlated with our other measurement of global consistency, the one-to-one accuracy. 3 The matching can be computed efficiently with the so-called Hungarian algorithm or by reduction to max flow. The widely used greedy algorithm is a two-approximation, although we have not found large differences in practice. 394 Elsner and Charniak Disentangling Chat Adams and Martell (2008) also report F-score, but using a somewhat different definition. They define F-score only between a particular pair of conversations, and report the score for a single selected conversation. They do not describe how this reference conversation is chosen. It is also unclear how they determine which proposed conversation to match to it—the one with the best F-score, or the one which contains the first (or “root”) utterance of the reference conversation. (The latter, although it may be more useful for some applications, has an obvious problem—if the conversation is retrieved perfectly except for </context>
</contexts>
<marker>Adams, Martell, 2008</marker>
<rawString>Adams, Paige H. and Craig H. Martell. 2008. Topic detection and extraction in chat. International Conference on Semantic Computing, 2:581–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Matthew Romaine</author>
<author>Margaret H Szymanski</author>
<author>James D Thornton</author>
<author>Daniel Wilson</author>
<author>Allison Woodruff</author>
</authors>
<title>The mad hatter’s cocktail party: A social mobile audio Computational Linguistics Volume 36,</title>
<date>2003</date>
<journal>Number</journal>
<booktitle>In CHI ’03: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<volume>3</volume>
<pages>425--432</pages>
<location>New York, NY.</location>
<contexts>
<context position="3803" citStr="Aoki et al. (2003)" startWordPosition="532" endWordPosition="535">ement can be challenging even for humans, and therefore can serve as a useful stress test for computational models of discourse. Disentanglement has two practical applications. One is the analysis of pre-recorded transcripts in order to extract some kind of information, such as question–answer pairs or summaries. These tasks should probably take as input each separate conversation, rather than the entire transcript. Another application is as part of a user-interface system for active participants in the chat, in which users target a conversation of interest which is then highlighted for them. Aoki et al. (2003) created such a system for speech, which users generally preferred to a conventional system—when the disentanglement worked! We begin in Section 2 with an overview of related work. In Section 3, we present a new corpus of manually annotated chat room data and evaluate annotator reliability. We give a set of metrics (Section 3.2) describing structural similarity both locally and globally. In Section 4, we propose a model which uses supervised pairwise classification to link utterances from the same conversation, followed by a greedy inference stage which clusters the utterances into conversatio</context>
<context position="5429" citStr="Aoki et al. (2003" startWordPosition="786" endWordPosition="789">task. It is also the first to use a supervised method to learn weights for different feature types, rather than relying on cosine distance with uniform or handtuned feature weights. It supplements standard word repetition and time gap features with other feature types, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen </context>
<context position="7871" citStr="Aoki et al. (2003)" startWordPosition="1174" endWordPosition="1177">ine the threshold. On the other hand, supervised methods require labeled training data, and may be more difficult to adapt to novel domains or corpora. The remaining papers treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) state an explicit assumption that this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each 30-second segment of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than making a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these two approach</context>
<context position="52621" citStr="Aoki et al. (2003)" startWordPosition="8333" endWordPosition="8336">ent set of metrics. Any new annotation project should also investigate whether annotators can define their desired specificity, and with what precision. Our results on new conversation detection suggest that a high-performance classifier for this task could improve results substantially. It is also interesting to consider, given the weakness of our technical words feature and the disappointing results using Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be usefully modeled. Finally, we are interested to see how well this feature set performs on speech data, as in Aoki et al. (2003). Spoken conversation is more natural than text chat, but even when participants are face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody and gaze direction. Turn-taking behavior is also more distinct, which makes the task easier, but according to Aoki et al. (2006), it is certainly not sufficient. 9. Conclusion This work provides a corpus of annotated data for chat disentanglement, which, along with our proposed metrics, should allow future researchers to evaluate and compare their results quantitatively.9 Our</context>
</contexts>
<marker>Aoki, Romaine, Szymanski, Thornton, Wilson, Woodruff, 2003</marker>
<rawString>Aoki, Paul M., Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The mad hatter’s cocktail party: A social mobile audio Computational Linguistics Volume 36, Number 3 space supporting multiple simultaneous conversations. In CHI ’03: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 425–432, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Margaret H Szymanski</author>
<author>Luke D Plurkowski</author>
<author>James D Thornton</author>
<author>Allison Woodruff</author>
<author>Weilie Yi</author>
</authors>
<title>Where’s the “party” in “multi-party”?: Analyzing the structure of small-group sociable talk.</title>
<date>2006</date>
<booktitle>In CSCW ’06: Proceedings of the 2006 20th Anniversary Conference on Computer Supported Cooperative Work,</booktitle>
<pages>393--402</pages>
<location>New York, NY.</location>
<contexts>
<context position="6251" citStr="Aoki et al. (2006)" startWordPosition="918" endWordPosition="921">avy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen et al. (2006) publish results on humanannotated data; although we do not have their corpora, we discuss their evaluation metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki et al. (2006) construct an annotated speech corpus, but they give no results for model performance, only user satisfaction with their conversational system. Camtepe et al. (2005) and Acar et al. (2005) do give performance results, but only on synthetic data. Adams and Martell (2008) and Shen et al. (2006) treat disentanglement in the same way we do, as a clustering task where the objects to be clustered are the individual utterances. Both their algorithms define a notion of distance (based on the cosine) and a threshold parameter determining how close to the cluster center an utterance must be before the c</context>
<context position="8856" citStr="Aoki et al. (2006)" startWordPosition="1322" endWordPosition="1325">tions rather than making a hard assignment. However, the algorithm still deals with speakers rather than utterances, and cannot determine which conversation any particular utterance is part of. Another problem with these two approaches is the information used for clustering. Aoki et al. (2003) and Camtepe et al. (2005) detect the arrival times of messages, and use them to construct an affinity graph between participants by detecting turn-taking behavior among pairs of speakers. (Turn-taking is typified by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) add word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of “noise words” (their term for words which are shared across conversations) to almost complete failure when only half of the words are shared. Adams and Martell (2008) and Shen et al. (2006) use a more robust representation for lexical features: Term frequency–inverse document freq</context>
<context position="24776" citStr="Aoki et al. (2006)" startWordPosition="3820" endWordPosition="3823">he coarser ones. By examining the local metric, we can see even more: Local correlations are good, at an average of 81.1%. This means that, in the three-sentence window preceding each sentence, the annotators are often in agreement. If they recognize subdivisions of a large conversation, these subdivisions tend to be contiguous, not mingled together, which is why they have little impact on the local measure. We find reasons for the annotators’ disagreement about appropriate levels of detail in the linguistic literature. As mentioned, new conversations often break off from old ones in schisms. Aoki et al. (2006) discuss conversational features associated with schisming and the related process of affiliation, by which speakers attach themselves to a conversation. Schisms often branch off from asides or even normal comments (tossouts) within an existing conversation. This means that there is no clear beginning to the new conversation—at the time when it begins, it is not clear that there are two separate floors, and this will not become clear until distinct sets of speakers and patterns of turntaking are established. Speakers, meanwhile, take time to orient themselves to the new conversation. Example s</context>
<context position="52971" citStr="Aoki et al. (2006)" startWordPosition="8386" endWordPosition="8389"> technical words feature and the disappointing results using Latent Dirichlet Allocation from Adams (2008), how semantic similarity might be usefully modeled. Finally, we are interested to see how well this feature set performs on speech data, as in Aoki et al. (2003). Spoken conversation is more natural than text chat, but even when participants are face-to-face, disentanglement remains a problem. On the other hand, spoken dialogue contains new sources of information, such as prosody and gaze direction. Turn-taking behavior is also more distinct, which makes the task easier, but according to Aoki et al. (2006), it is certainly not sufficient. 9. Conclusion This work provides a corpus of annotated data for chat disentanglement, which, along with our proposed metrics, should allow future researchers to evaluate and compare their results quantitatively.9 Our annotations are consistent with one another, especially with respect to local agreement. We show that features based on discourse patterns and the content of utterances are helpful in disentanglement. The model we present can outperform a variety of baselines. Acknowledgments Our thanks to Suman Karumuri, Steve Sloman, Matt Lease, David McClosky, </context>
</contexts>
<marker>Aoki, Szymanski, Plurkowski, Thornton, Woodruff, Yi, 2006</marker>
<rawString>Aoki, Paul M., Margaret H. Szymanski, Luke D. Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. 2006. Where’s the “party” in “multi-party”?: Analyzing the structure of small-group sociable talk. In CSCW ’06: Proceedings of the 2006 20th Anniversary Conference on Computer Supported Cooperative Work, pages 393–402, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Bansal</author>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<date>2004</date>
<booktitle>Correlation clustering. Machine Learning,</booktitle>
<pages>56--1</pages>
<marker>Bansal, Blum, Chawla, 2004</marker>
<rawString>Bansal, Nikhil, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Machine Learning, 56(1-3):89–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyit Ahmet Camtepe</author>
<author>Mark K Goldberg</author>
<author>Malik Magdon-Ismail</author>
<author>Mukkai Krishnamoorty</author>
</authors>
<title>Detecting conversing groups of chatters: A model, algorithms, and tests.</title>
<date>2005</date>
<booktitle>In IADIS AC,</booktitle>
<pages>89--96</pages>
<location>Algarve.</location>
<contexts>
<context position="5673" citStr="Camtepe et al. (2005)" startWordPosition="824" endWordPosition="827">res with other feature types, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each speaker participates in only one conversation is flawed. Adams and Martell (2008) and Shen et al. (2006) publish results on humanannotated data; although we do not have their corpora, we discuss their evaluation metrics (Section 3.2) and give a comparison to our own results herein (Section 5). Aoki et al. (2006) construct an annotate</context>
<context position="7655" citStr="Camtepe et al. (2005)" startWordPosition="1143" endWordPosition="1146">ntage that it can weigh individual feature types based on their predictivity—unsupervised methods combine features either uniformly or using heuristic methods. There is also no need for a separate tuning phase to determine the threshold. On the other hand, supervised methods require labeled training data, and may be more difficult to adapt to novel domains or corpora. The remaining papers treat the problem as one of clustering speakers, rather than utterances. That is, they assume that during the window over which the system operates, a particular speaker is engaging in only one conversation. Camtepe et al. (2005) state an explicit assumption that this is true throughout the entire transcript; real speakers, by contrast, often participate in many conversations, sequentially or sometimes even simultaneously. Aoki et al. (2003) analyze each 30-second segment of the transcript separately. This makes the single-conversation restriction somewhat less severe, but has the disadvantage of ignoring all events which occur outside the segment. Acar et al. (2005) attempt to deal with this problem by using a fuzzy algorithm to cluster speakers; this assigns each speaker a distribution over conversations rather than</context>
</contexts>
<marker>Camtepe, Goldberg, Magdon-Ismail, Krishnamoorty, 2005</marker>
<rawString>Camtepe, Seyit Ahmet, Mark K. Goldberg, Malik Magdon-Ismail, and Mukkai Krishnamoorty. 2005. Detecting conversing groups of chatters: A model, algorithms, and tests. In IADIS AC, pages 89–96, Algarve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
</authors>
<title>Incorporating Nonverbal Features into Multimodal Models of Human-to-Human Communication.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="12856" citStr="Chen (2008)" startWordPosition="1939" endWordPosition="1940">ity for addressee identification and disentanglement in the design of a system for military dialogues involving virtual agents. Subsequent work (Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy on addressee identification. Another meeting-related task is floor tracking, which attempts to determine which speaker has the floor after each utterance. This task involves modeling the coordination strategies which speakers use to acquire or give up the floor, and so provides a good model of an ongoing conversation. A detailed analysis is given in Chen et al. (2006); Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008) use a conditional random fields (CRF) model to predict the next speaker in Supreme Court oral argument transcripts. A somewhat related area of research involves environments with higher latency than real-time chat: message boards and e-mail. Content matching approaches tend to work better in these settings, because while many chat messages are backchannel responses or discourse requests (Yes, Why? and so forth), longer posts tend to be contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that he</context>
</contexts>
<marker>Chen, 2008</marker>
<rawString>Chen, Lei. 2008. Incorporating Nonverbal Features into Multimodal Models of Human-to-Human Communication. Ph.D. thesis, Purdue University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Mary Harper</author>
<author>Amy Franklin</author>
<author>Travis R Rose</author>
<author>Irene Kimbara</author>
<author>Zhongqiang Huang</author>
<author>Francis Quek</author>
</authors>
<title>A multimodal analysis of floor control in meetings.</title>
<date>2006</date>
<booktitle>In Proceedings of MLMI 06,</booktitle>
<pages>36--49</pages>
<location>Bethesda, MD.</location>
<contexts>
<context position="12843" citStr="Chen et al. (2006)" startWordPosition="1935" endWordPosition="1938">discusses the necessity for addressee identification and disentanglement in the design of a system for military dialogues involving virtual agents. Subsequent work (Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy on addressee identification. Another meeting-related task is floor tracking, which attempts to determine which speaker has the floor after each utterance. This task involves modeling the coordination strategies which speakers use to acquire or give up the floor, and so provides a good model of an ongoing conversation. A detailed analysis is given in Chen et al. (2006); Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008) use a conditional random fields (CRF) model to predict the next speaker in Supreme Court oral argument transcripts. A somewhat related area of research involves environments with higher latency than real-time chat: message boards and e-mail. Content matching approaches tend to work better in these settings, because while many chat messages are backchannel responses or discourse requests (Yes, Why? and so forth), longer posts tend to be contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006)</context>
</contexts>
<marker>Chen, Harper, Franklin, Rose, Kimbara, Huang, Quek, 2006</marker>
<rawString>Chen, Lei, Mary Harper, Amy Franklin, Travis R. Rose, Irene Kimbara, Zhongqiang Huang, and Francis Quek. 2006. A multimodal analysis of floor control in meetings. In Proceedings of MLMI 06, pages 36–49, Bethesda, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http:// pub.hal3.name#daume04cg-bfgs. Implementation available at http:// hal3.name/megam/.</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>Daum´e, III, Hal. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http:// pub.hal3.name#daume04cg-bfgs. Implementation available at http:// hal3.name/megam/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Warren Schudy</author>
</authors>
<title>Bounding and comparing methods for correlation clustering beyond ILP.</title>
<date>2009</date>
<booktitle>In Proceedings of ILP-NLP,</booktitle>
<pages>19--27</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="14814" citStr="Elsner and Schudy (2009)" startWordPosition="2236" endWordPosition="2239">). Finally, we briefly mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He finds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly finds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Data Set Our data set is recorded from IRC channel ##LINUX at free–node.net, using the freely available gaim client. ##LINUX is an unofficial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factory</context>
<context position="36004" citStr="Elsner and Schudy 2009" startWordPosition="5679" endWordPosition="5682">independent, is E ij:i&lt;j wijxij. We attempt to maximize this quantity, subject to the constraint that the xij must form a legitimate clustering such that xij = xjk = 1 implies xij = xik. Finding an exact solution proves to be difficult; the problem has a quadratic number of variables (one for each pair of utterances) and a cubic number of triangle inequality constraints (three for each triplet).8 With 800 utterances in our test set, even solving the linear relaxation of the problem with CPLEX (Ilog, Inc. 2003) is too expensive to be practical. Experiments on a variety of heuristic algorithms (Elsner and Schudy 2009) show that a relatively good solution can be obtained using a greedy voting algorithm (Figure 6). In this algorithm, we assign utterance j by examining all previously assigned 7 The original version of our system used a different weighting scheme, wij = pij − .5. The log-odds ratio behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms or tuning (see Section 6), so, for simplicity, we present it here as well. 8 There is a triangle inequality constraint for each triplet i, j, k: (1 − xik) &lt; (1 − xij) + (1 − xjk). 400 Elsner and Charniak Disenta</context>
<context position="41536" citStr="Elsner and Schudy (2009)" startWordPosition="6580" endWordPosition="6583">n F-score metric seems to perform similarly to the one-to-one accuracy, which is unsurprising because they are both measures of global consistency. The largest difference between them is that the speaker baseline outperforms blocks and pauses in F-score (although not by very much), perhaps because it is more precise. Shen et al. (2006) report higher F-scores for their own best model: It obtains an F-score of 61.2, whereas our model’s mean score is only 43.4. Because of the different corpora, we are unable to explain this difference. Better results are also reported in Wang and Oard (2009) and Elsner and Schudy (2009) (see Table 4). Mention information alone is not sufficient for disentanglement; with only name mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name mention features are critical for our model. Without them, the classifier’s development F-score drops from 71 to 56. The disentanglement system’s test performance decreases proportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline performance. For some utterances, of course, name mentions provide the only reasonable 402 Elsner and Charniak Disentangling Chat Table 4 Results reported by oth</context>
</contexts>
<marker>Elsner, Schudy, 2009</marker>
<rawString>Elsner, Micha and Warren Schudy. 2009. Bounding and comparing methods for correlation clustering beyond ILP. In Proceedings of ILP-NLP, pages 19–27, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<tech>LDC95T21.</tech>
<contexts>
<context position="32636" citStr="Graff 1995" startWordPosition="5123" endWordPosition="5124">ulated over the entire 52 hours of transcript. The binning scheme allows us to deal with “noise words” which are repeated coincidentally, because these occur in high-probability bins where repetitions are given less weight. The point of the repetition feature is of course to detect sentences with similar topics. We also find that sentences with technical content are more likely to be related than non-technical sentences. We label an utterance as technical if it contains a Web address, a long string of digits, or a term present in a guide for novice Linux users5 but not in a large news corpus (Graff 1995).6 This is a lightweight way to capture one “semantic dimension” or cluster of related words. The technical word feature was included because it improves our development classification score slightly, but it does not have a significant effect on overall performance. Adams (2008) attempts to add more semantic dimensions learned via Latent Dirichlet Allocation, and similarly finds no improvement. Pairs of utterances which are widely separated in the discourse are unlikely to be directly related—even if they are part of the same conversation, the link between them is probably a long chain of inte</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>Graff, David. 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>320--327</pages>
<location>New York, NY.</location>
<contexts>
<context position="18279" citStr="Haghighi and Klein 2006" startWordPosition="2782" endWordPosition="2785"> task, and metrics for agreement on supervised classification, such as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Haghighi, Aria and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of HLT-NAACL, pages 320–327, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Hawes</author>
<author>Jimmy Lin</author>
<author>Philip Resnik</author>
</authors>
<title>Elements of a computational model for multi-party discourse: The turn-taking behavior of supreme court justices.</title>
<date>2008</date>
<tech>Technical Report LAMP-TR-147/HCIL-2008-02,</tech>
<institution>University of Maryland, College Park.</institution>
<marker>Hawes, Lin, Resnik, 2008</marker>
<rawString>Hawes, Timothy, Jimmy Lin, and Philip Resnik. 2008. Elements of a computational model for multi-party discourse: The turn-taking behavior of supreme court justices. Technical Report LAMP-TR-147/HCIL-2008-02, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inc Ilog</author>
</authors>
<title>CPLEX solver. Available at www-01.ibm.com/software/websphere/ ilog migration.html.</title>
<date>2003</date>
<marker>Ilog, 2003</marker>
<rawString>Ilog, Inc. 2003. CPLEX solver. Available at www-01.ibm.com/software/websphere/ ilog migration.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natasa Jovanovic</author>
<author>Rieks op den Akker</author>
</authors>
<title>Towards automatic addressee identification in multi-party dialogues.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th SIGdial Workshop,</booktitle>
<pages>89--92</pages>
<location>Cambridge, MA.</location>
<marker>Jovanovic, den Akker, 2004</marker>
<rawString>Jovanovic, Natasa and Rieks op den Akker. 2004. Towards automatic addressee identification in multi-party dialogues. In Proceedings of the 5th SIGdial Workshop, pages 89–92, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natasa Jovanovic</author>
</authors>
<title>Rieks op den Akker, and Anton Nijholt.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Trento.</location>
<marker>Jovanovic, 2006</marker>
<rawString>Jovanovic, Natasa, Rieks op den Akker, and Anton Nijholt. 2006. Addressee identification in face-to-face meetings. In Proceedings of EACL, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>25--32</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="18331" citStr="Luo 2005" startWordPosition="2792" endWordPosition="2793"> as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all utterances. For example, loc1 counts pairs of adjac</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Luo, Xiaoqiang. 2005. On coreference resolution performance metrics. In Proceedings of HLT-EMNLP, pages 25–32, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<location>Sydney.</location>
<contexts>
<context position="28042" citStr="Malioutov and Barzilay 2006" startWordPosition="4359" endWordPosition="4362">eaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66) Time The time between x and y in seconds, discretized into logarithmically sized bins. Speaker x and y have the same speaker. Mention x-y x mentions the speaker of y (or vice versa). For example, this feature is true for a pair such as: Felicia “Gale: ... and any utterance spoken by Gale. Mention same Both x and y mention the same name. Mention other either x or y mentions a third person</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Malioutov, Igor and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL, pages 25–32, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 18th Annual Conference on Neural Information Processing Systems (NIPS),</booktitle>
<pages>905--912</pages>
<location>Vancouver.</location>
<contexts>
<context position="14191" citStr="McCallum and Wellner (2004)" startWordPosition="2142" endWordPosition="2145">es from earlier messages. Wang et al. (2008), an analysis of a student discussion group, is the work on which Adams and Martell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classification and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classifier. The global partitioning problem was identified as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we briefly mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He finds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly finds that annotators have trouble agreeing. E</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>McCallum, Andrew and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings of the 18th Annual Conference on Neural Information Processing Systems (NIPS), pages 905–912, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>A R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="9714" citStr="Miller et al. 1990" startWordPosition="1460" endWordPosition="1463"> word repetitions on an equal basis, and so degrades quickly in the presence of “noise words” (their term for words which are shared across conversations) to almost complete failure when only half of the words are shared. Adams and Martell (2008) and Shen et al. (2006) use a more robust representation for lexical features: Term frequency–inverse document frequency (TF–IDF) weighted unigrams, as often used in information extraction. This feature set works fairly well alone, although time is also a key feature as in the other studies. Adams and Martell (2008) also investigate WordNet hypernyms (Miller et al. 1990) as a measure of semantic relatedness, and use the identity of the speaker of a particular utterance as a feature. It is unclear from their results whether these latter two features are effective or not. 391 Computational Linguistics Volume 36, Number 3 To motivate our own approach, we examine some linguistic studies of discourse, especially analysis of multi-party conversation. O’Neill and Martin (2003) point out several ways in which multi-party text chat differs from typical two-party conversation. One key difference is the frequency with which participants mention each others’ names. They </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G., A. R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In COLING,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="48363" citStr="Ng and Cardie 2002" startWordPosition="7670" endWordPosition="7673">t a way to let the user select the desired entropy remains a challenge. 7. Detecting Conversation Starts In this section, we investigate better ways to find the beginnings of new conversations. In the pairwise-linkage representation presented earlier, a new conversation is begun when none of the previous utterances is strongly linked to the current utterance. This representation spreads out the responsibility for detecting a new conversation over many pairwise decisions. We are inspired by the use of discourse-new classifiers (also called anaphoricity detectors) in coreference classification (Ng and Cardie 2002) to find NPs which begin coreferential chains. Oracle experiments show that a similar detector for utterances which begin conversations could improve disentanglement scores if it were available. We attempt to develop such a detector, but without much success. 405 Computational Linguistics Volume 36, Number 3 Table 6 Metric values using an oracle new-conversation detector on test data. Original Model +Oracle New Conversations Mean one-to-one 41.23 46.75 Max one-to-one 52.13 53.50 Min one-to-one 31.66 42.13 Mean loc3 72.94 73.90 Max loc3 74.70 76.49 Min loc3 70.77 70.72 As a demonstration of the</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, Vincent and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In COLING, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacki O’Neill</author>
<author>David Martin</author>
</authors>
<title>Text chat in action.</title>
<date>2003</date>
<booktitle>In GROUP ’03: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work,</booktitle>
<pages>40--49</pages>
<location>New York, NY.</location>
<marker>O’Neill, Martin, 2003</marker>
<rawString>O’Neill, Jacki and David Martin. 2003. Text chat in action. In GROUP ’03: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work, pages 40–49, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="18568" citStr="Rand 1971" startWordPosition="2831" endWordPosition="2832">on extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all utterances. For example, loc1 counts pairs of adjacent utterances for which two annotations agree. Several related papers use some variant of the F-score metric to measure accuracy. The most complete treatment is given in Shen et al. (2006). They use a micro-averaged F-score, which is de</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>Rand, William M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>1--8</pages>
<location>Boston, MA.</location>
<contexts>
<context position="27858" citStr="Roth and Yih 2004" startWordPosition="4328" endWordPosition="4331">n about the data set, we test the appropriateness of the assumption (used in previous work) that each speaker takes part in only one conversation. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-specific (Acc: 73 Prec: 73 Rec: 61 F: 66) Time The time between x and y in seconds, discretized into logarithmically sized bins. Speaker x and y have the same speaker. Mention x-y x mentions the speaker of y (or vice versa). For example, this featu</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Roth, Dan and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of CoNLL-2004, pages 1–8, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Sacks, Harvey, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systematics for the organization of turn-taking for conversation. Language, 50(4):696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Qiang Yang</author>
<author>Jian-Tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Thread detection in dynamic text message streams.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference,</booktitle>
<pages>35--42</pages>
<location>New York, NY.</location>
<contexts>
<context position="1523" citStr="Shen et al. 2006" startWordPosition="210" endWordPosition="213">yield practical improvements. 1. Motivation Simultaneous conversations seem to arise naturally in both informal social interactions and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among 8–10 people found an average of 1.76 conversations (floors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, does not form a contiguous segment of the chatroom transcript, but is frequently broken up by interposed utterances from other conversations. Disentanglement (also called thread detection [Shen et al. 2006], thread extraction [Adams and Martell 2008], and thread/conversation management [Traum 2004]) is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis. For instance, consider the multi-party exchange in Figure 1. Contextually, it is clear that this corresponds to two conversations, and Felicia’s1 response excellent is intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. * Brown </context>
<context position="5340" citStr="Shen et al. (2006)" startWordPosition="773" endWordPosition="776">edge, this is the first work to evaluate interannotator agreement for the disentanglement task. It is also the first to use a supervised method to learn weights for different feature types, rather than relying on cosine distance with uniform or handtuned feature weights. It supplements standard word repetition and time gap features with other feature types, including very powerful features based on name mentioning, which is common in Internet Relay Chat. 2. Related Work Several threads of research are direct attempts to solve the disentanglement problem. The closest to our own work is that of Shen et al. (2006), which performs conversation disentanglement on an online chat corpus. Aoki et al. (2003, 2006) disentangle speech, rather than chat. Other work has slightly different goals than ours: Adams and Martell (2008) attempt to find all utterances of a specific single conversation in Internet and Navy tactical chat. Camtepe et al. (2005) and Acar et al. (2005) perform social network 390 Elsner and Charniak Disentangling Chat analysis, extracting groups of speakers who talk to one another. This can be considered a disentanglement task, although, as we will see (Section 5), the assumption that each sp</context>
<context position="9364" citStr="Shen et al. (2006)" startWordPosition="1408" endWordPosition="1411">by short pauses between utterances; speakers aim neither to interrupt nor leave long gaps.) Aoki et al. (2006) find that turn-taking on its own is inadequate. They motivate a richer feature set, which, however, does not yet appear to be implemented. Acar et al. (2005) add word repetition to their feature set. However, their approach deals with all word repetitions on an equal basis, and so degrades quickly in the presence of “noise words” (their term for words which are shared across conversations) to almost complete failure when only half of the words are shared. Adams and Martell (2008) and Shen et al. (2006) use a more robust representation for lexical features: Term frequency–inverse document frequency (TF–IDF) weighted unigrams, as often used in information extraction. This feature set works fairly well alone, although time is also a key feature as in the other studies. Adams and Martell (2008) also investigate WordNet hypernyms (Miller et al. 1990) as a measure of semantic relatedness, and use the identity of the speaker of a particular utterance as a feature. It is unclear from their results whether these latter two features are effective or not. 391 Computational Linguistics Volume 36, Numbe</context>
<context position="19120" citStr="Shen et al. (2006)" startWordPosition="2915" endWordPosition="2918">s a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all utterances. For example, loc1 counts pairs of adjacent utterances for which two annotations agree. Several related papers use some variant of the F-score metric to measure accuracy. The most complete treatment is given in Shen et al. (2006). They use a micro-averaged F-score, which is defined by constructing a multiway matching between conversations in the two annotations. For a gold conversation i with size ni, and a proposed conversation j with size nj, with overlap of size nij, they define precision and recall (plus the standard balanced F-score). The F-score of an entire annotation is a weighted sum over the matching: P= nij nij F(i,j) = 2PR �F = nin maxjF(i, j) (1) R= ni P + R i nj This is the F-score we report for comparative purposes. Because the match is multiway, the score is not symmetric; when measuring agreement betw</context>
<context position="41249" citStr="Shen et al. (2006)" startWordPosition="6531" endWordPosition="6534">n, agreement is highest with other annotators, then our model, and finally the baselines. The most competitive baseline is one conversation per speaker, which makes sense, since if a speaker makes two comments in a four-utterance window, they are very likely to be related. The Shen F-score metric seems to perform similarly to the one-to-one accuracy, which is unsurprising because they are both measures of global consistency. The largest difference between them is that the speaker baseline outperforms blocks and pauses in F-score (although not by very much), perhaps because it is more precise. Shen et al. (2006) report higher F-scores for their own best model: It obtains an F-score of 61.2, whereas our model’s mean score is only 43.4. Because of the different corpora, we are unable to explain this difference. Better results are also reported in Wang and Oard (2009) and Elsner and Schudy (2009) (see Table 4). Mention information alone is not sufficient for disentanglement; with only name mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name mention features are critical for our model. Without them, the classifier’s development F-score drops from 71 to 56. The disentangleme</context>
</contexts>
<marker>Shen, Yang, Sun, Chen, 2006</marker>
<rawString>Shen, Dou, Qiang Yang, Jian-Tao Sun, and Zheng Chen. 2006. Thread detection in dynamic text message streams. In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference, pages 35–42, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Soon, Wee Meng, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>Issues in multi-party dialogues.</title>
<date>2004</date>
<booktitle>Advances in Agent Communication.</booktitle>
<pages>201--211</pages>
<editor>In F. Dignum, editor,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin,</location>
<contexts>
<context position="1616" citStr="Traum 2004" startWordPosition="223" endWordPosition="224">oth informal social interactions and multi-party typed chat. Aoki et al.’s (2006) study of voice conversations among 8–10 people found an average of 1.76 conversations (floors) active at a time, and a maximum of four. In our chat corpus, the average is even higher, at 2.75. The typical conversation, therefore, does not form a contiguous segment of the chatroom transcript, but is frequently broken up by interposed utterances from other conversations. Disentanglement (also called thread detection [Shen et al. 2006], thread extraction [Adams and Martell 2008], and thread/conversation management [Traum 2004]) is the clustering task of dividing a transcript into a set of distinct conversations. It is an essential prerequisite for any kind of higher-level dialogue analysis. For instance, consider the multi-party exchange in Figure 1. Contextually, it is clear that this corresponds to two conversations, and Felicia’s1 response excellent is intended for Chanel and Regine. A straightforward reading of the transcript, however, might interpret it as a response to Gale’s statement immediately preceding. * Brown Laboratory for Linguistic Information Processing, Brown University, Providence, RI 02912. E-m</context>
<context position="12224" citStr="Traum (2004)" startWordPosition="1845" endWordPosition="1846">. The task of automatically determining the intended recipient of an utterance in a meeting is called addressee identification. It requires detecting digressions and identifying their participants. Several studies attempt this task. Jovanovic and op den Akker (2004) and Jovanovic, op den Akker, and Nijholt (2006) perform addressee identification using a complex feature set including linguistic cues like pronouns and discourse markers, temporal information, and gaze direction. They also find that addressee identity can be annotated with high reliability (κ = .7 for one set and .8 for another). Traum (2004) discusses the necessity for addressee identification and disentanglement in the design of a system for military dialogues involving virtual agents. Subsequent work (Traum, Robinson, and Stephan 2004) develops a rule-based system with high accuracy on addressee identification. Another meeting-related task is floor tracking, which attempts to determine which speaker has the floor after each utterance. This task involves modeling the coordination strategies which speakers use to acquire or give up the floor, and so provides a good model of an ongoing conversation. A detailed analysis is given in</context>
</contexts>
<marker>Traum, 2004</marker>
<rawString>Traum, D. 2004. Issues in multi-party dialogues. In F. Dignum, editor, Advances in Agent Communication. Springer Verlag, Berlin, pages 201–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Susan Robinson</author>
<author>Jens Stephan</author>
</authors>
<title>Evaluation of multi-party virtual reality dialogue interaction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1699--1702</pages>
<location>Lisbon.</location>
<marker>Traum, Robinson, Stephan, 2004</marker>
<rawString>Traum, David R., Susan Robinson, and Jens Stephan. 2004. Evaluation of multi-party virtual reality dialogue interaction. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 1699–1702, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidan Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Context-based message expansion for disentanglement of interleaved text conversations.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>200--208</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="14312" citStr="Wang and Oard (2009)" startWordPosition="2161" endWordPosition="2164">(2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classification and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classifier. The global partitioning problem was identified as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we briefly mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He finds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly finds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. </context>
<context position="41507" citStr="Wang and Oard (2009)" startWordPosition="6575" endWordPosition="6578">ly to be related. The Shen F-score metric seems to perform similarly to the one-to-one accuracy, which is unsurprising because they are both measures of global consistency. The largest difference between them is that the speaker baseline outperforms blocks and pauses in F-score (although not by very much), perhaps because it is more precise. Shen et al. (2006) report higher F-scores for their own best model: It obtains an F-score of 61.2, whereas our model’s mean score is only 43.4. Because of the different corpora, we are unable to explain this difference. Better results are also reported in Wang and Oard (2009) and Elsner and Schudy (2009) (see Table 4). Mention information alone is not sufficient for disentanglement; with only name mention and time gap features, mean one-to-one is 38 and loc3 is 69. However, name mention features are critical for our model. Without them, the classifier’s development F-score drops from 71 to 56. The disentanglement system’s test performance decreases proportionally; mean one-to-one falls to 36, and mean loc3 to 63, essentially baseline performance. For some utterances, of course, name mentions provide the only reasonable 402 Elsner and Charniak Disentangling Chat Ta</context>
</contexts>
<marker>Wang, Oard, 2009</marker>
<rawString>Wang, Lidan and Douglas W. Oard. 2009. Context-based message expansion for disentanglement of interleaved text conversations. In Proceedings of HLT-NAACL, pages 200–208, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Chia Wang</author>
<author>Mahesh Joshi</author>
<author>William Cohen</author>
<author>Carolyn Ros´e</author>
</authors>
<title>Recovering implicit thread structure in newsgroup style conversations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd International Conference on Weblogs and Social Media (ICWSM II),</booktitle>
<location>Seattle, WA.</location>
<marker>Wang, Joshi, Cohen, Ros´e, 2008</marker>
<rawString>Wang, Yi-Chia, Mahesh Joshi, William Cohen, and Carolyn Ros´e. 2008. Recovering implicit thread structure in newsgroup style conversations. In Proceedings of the 2nd International Conference on Weblogs and Social Media (ICWSM II), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Yuan Yeh</author>
<author>Aaron Harnly</author>
</authors>
<title>Email thread reassembly using similarity matching.</title>
<date>2006</date>
<booktitle>In Conference on Email and Anti-Spam,</booktitle>
<location>Mountain View, CA.</location>
<contexts>
<context position="13443" citStr="Yeh and Harnly (2006)" startWordPosition="2029" endWordPosition="2032">in Chen et al. (2006); Chen (2008) also gives a model for detecting floor shifts. Hawes, Lin, and Resnik (2008) use a conditional random fields (CRF) model to predict the next speaker in Supreme Court oral argument transcripts. A somewhat related area of research involves environments with higher latency than real-time chat: message boards and e-mail. Content matching approaches tend to work better in these settings, because while many chat messages are backchannel responses or discourse requests (Yes, Why? and so forth), longer posts tend to be contentful. Of the two tasks, e-mail is easier; Yeh and Harnly (2006) find that heuristic information from message headers can be useful, as can content-based matching such as detecting quotes from earlier messages. Wang et al. (2008), an analysis of a student discussion group, is the work on which Adams and Martell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classification and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disenta</context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>Yeh, Jen-Yuan and Aaron Harnly. 2006. Email thread reassembly using similarity matching. In Conference on Email and Anti-Spam, Mountain View, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>