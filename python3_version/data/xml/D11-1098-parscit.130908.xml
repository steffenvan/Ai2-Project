<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.417779">
Lexical Co-occurrence, Statistical Significance, and Word Association
</title>
<author confidence="0.90994">
Dipak L. Chaudhari Om P. Damani Srivatsan Laxman
</author>
<affiliation confidence="0.956545">
Computer Science and Engg. Computer Science and Engg. Microsoft Research India
IIT Bombay IIT Bombay Bangalore
</affiliation>
<email confidence="0.988395">
dipakc@cse.iitb.ac.in damani@cse.iitb.ac.in slaxman@microsoft.com
</email>
<sectionHeader confidence="0.99459" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998565863636364">
Lexical co-occurrence is an important cue for
detecting word associations. We propose a
new measure of word association based on a
new notion of statistical significance for lex-
ical co-occurrences. Existing measures typ-
ically rely on global unigram frequencies to
determine expected co-occurrence counts. In-
stead, we focus only on documents that con-
tain both terms (of a candidate word-pair) and
ask if the distribution of the observed spans of
the word-pair resembles that under a random
null model. This would imply that the words
in the pair are not related strongly enough
for one word to influence placement of the
other. However, if the words are found to oc-
cur closer together than explainable by the null
model, then we hypothesize a more direct as-
sociation between the words. Through exten-
sive empirical evaluation on most of the pub-
licly available benchmark data sets, we show
the advantages of our measure over existing
co-occurrence measures.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925777777778">
Lexical co-occurrence is an important indicator of
word association and this has motivated several
co-occurrence1 measures for word association like
PMI (Church and Hanks, 1989), LLR (Dunning,
1993), Dice (Dice, 1945), and CWCD (Washtell and
Markert, 2009). In this paper, we present a new mea-
sure of word association based on a new notion of
statistical significance for lexical co-occurrences. In
general, a lexical co-occurrence could refer to a pair
</bodyText>
<footnote confidence="0.806091666666667">
1We use the term co-occurrence to refer to a pair of words
that co-occur in a document with an arbitrary number of inter-
vening words.
</footnote>
<bodyText confidence="0.999807181818182">
of words that co-occur in a large number of docu-
ments; or it could refer to a pair of words that, al-
though co-occur only in a small number of docu-
ments, occur close to each other within those docu-
ments. We formalize these ideas and construct a sig-
nificance test that allows us to detect different kinds
of co-occurrences within a single unified framework
(a feature which is absent in current measures for
co-occurrence). Another distinguishing feature of
our measure is that it is based solely on the co-
occurrence counts in the documents containing both
words of the pair, unlike all existing measures which
also take global unigram frequencies in account.
We need a null hypothesis that can account for
an observed co-occurrence as a pure chance event
and this in-turn requires a corpus generation model.
Documents in a corpus can be assumed to be gen-
erated independent of each other. Existing co-
occurrence measures further assume that each docu-
ment is drawn from a multinomial distribution based
on global unigram frequencies. The main concern
with such a null model is the overbearing influence
of the unigram frequencies on the detection of word
associations. For example, the association between
anomochilidae (dwarf pipe snakes) and snake could
go undetected in our wikipedia corpus, since less
than 0.1% of the pages containing snake also con-
tained anomochilidae. Also, under current models,
the expected span2 of a word pair is very sensitive
to the associated unigram frequencies: the expected
span of a word pair composed of low frequency un-
igrams is much larger than that with high frequency
unigrams. This is contrary to how word associa-
</bodyText>
<footnote confidence="0.93026">
2The span of an occurrence of a word-pair is the ‘unsigned
distance’ between the positions of the corresponding word oc-
currences.
</footnote>
<page confidence="0.925687">
1058
</page>
<note confidence="0.958257">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1058–1068,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.993482018691589">
tions appear in language, where semantic relation- datasets) is 0.02, which is the least average deviation
ships manifest with small inter-word distances irre- among all the measures, the next best deviations be-
spective of the underlying unigram distributions. ing 0.04 and 0.06.
Based on these considerations we employ a null The paper is organized as follows. We present our
model that represents each document as a bag of notion of statistical significance of span distribution
words 3. A random permutation of the associated in Section 2. Algorithm for computing the proposed
bag of words gives a linear representation for the word association measure is described in Section 3.
document. Under this null model, the locations of We discuss related work in Section 4. Performance
an unrelated pair of words will likely be randomly evaluation is presented in Section 5 followed with
distributed in the documents in which they co-occur. conclusions in Section 6.
If the observed span distribution of a word-pair re- 2 Lexically significant co-occurrences
sembles that under the (random permutation) null Evidence for significant lexical co-occurrences can
model, then the relation between the words is not be gathered at two levels in the data – document-
strong enough for one word to influence the place- level and corpus-level. First, at the document level,
ment of the other. However, if the words are found we may find that for a given word-pair, a surpris-
to occur closer together than explainable by our null ingly high proportion of its occurrences within a
model, then we hypothesize a more direct associa- document have smaller spans than they would have
tion between the words. Therefore, this null model by random chance. Second, at the corpus-level,
detects biases in span distributions of word-pairs we may find a pair of words appearing closer-than-
while being agnostic to variations in global unigram random in multiple documents in the corpus. We
frequencies. now describe how to combine both kinds of evidence
In this paper, we propose a new measure of word to decide whether the nearby occurrences of a word-
association based on the statistical significance of pair are statistically significant or not.
the observed span distribution of a word-pair. We Let the frequency f of a word-pair α in a
perform extensive experiments on all the publicly document D, be the maximum number of non-
available benchmark data sets4 and compare our overlapped occurrences of α in D. A set of occur-
measure against other popular co-occurrence mea- rences of a word-pair is said to be non-overlapped if
sures. Our experiments demonstrate the advan- the words corresponding to one occurrence from the
tages of our measure over all the competing mea- set do not appear in-between the words correspond-
sures. The ranked list of word associations output ing to any other occurrence from the set.
by our measure has the best correlation with the
corresponding gold-standard in three (out of seven)
data sets in our experiments, while remaining in
the top three in other four datasets. While differ-
ent measures perform best on different data sets,
our measure outperforms other measures by being
consistently either the best measure or very close
to the best measure on all the data sets. The aver-
age deviation of our measure’s correlation with the
gold-standard from the best measure’s correlation
with the gold-standard (average taken across all the
Let fx denote the maximum number of non-
overlapped occurrences of α in D with span less
�
than a given threshold x. We refer to fx as the span-
constrained frequency of α in D. Note that fx can-
not exceed f.
2.1 Document-level significant co-occurrence
To assess the statistical significance of the word-pair
α we ask if the span-constrained frequency P&apos; (of α)
is more than what we would expect in a document
of size E containing f ‘random’ occurrences of α.
Our intuition is that if two words are associated in
some way, they will often appear close to each other
in the document and so the distribution of the spans
will typically exhibit a bias toward values less than
a suitably chosen threshold x.
3There can be many ways to associate a bag of words with a
document. Details of this association are not important for us,
except that the bag of words provides some kind of quantitative
summary of the words within the document.
4We exclude very small data sets of 80 word pairs or less.
Sizes of the seven datasets we used range from 351 word-pairs
to 83,713 word-pairs.
1059
Definition 1 Consider the null hypothesis that the
linear representation of a document is generated by
choosing a random permutation of the bag of words
associated with the document. Let ` be the length of
the document and f denote the frequency of a word-
pair in the document. For a given a span threshold
�
x, we define πx( fx, f, `) as the probability under the
�
Di.” Note that we view fix as the only random quan-
tity here, with x fixed by the user, and `i and fi
fixed given the document Di and word-pair α. Let
Z = EKi=1 zi; Z models the number of documents
(out of K) that support the hypothesis “α is an &amp;
significant word-pair.” The expected value of Z is
given by
null that the word-pair will appear in the document E(Z) = K E(zi)
with a span-constrained frequency of at least Px. = i=1 πx(gE,x(fi, `i), fi, `i) (2)
Observe that πx(0, f, `) = 1 for any x &gt; 0; K
also, for x &gt; ` we have πx(f, f, `) = 1 (i.e. all f i=1
occurrences will always have span less than x for
x &gt; `). However, for typical values of x (i.e. for
�
x « `) the probability πx(
increasing �fx. For example, consider a document
of length 400 with 4 non-overlapped occurrences
of α. The probabilities of observing at least 4, 3,
2, 1 and 0 occurrences of α within a span of 20
words are 0.007, 0.09, 0.41, 0.83, and 1.0 respec-
tively. Since π20(3, 4, 400) = 0.09, even if 3 of the
4 occurrences of α have span less than 20 words,
there is 9% chance that the occurrences were a con-
sequence of a random event. As a result, if we de-
sired a confidence-level of at least 95%, we would
have to declare observed co-occurrences of α as in-
significant.
Given an E (0 &lt; E &lt; 1) and a span threshold
x (&gt; 0) the document D is said to support the hy-
pothesis “α is an E-significant word-pair within the
�fx, f, `) &lt; E]. We re-
fer to E as the document-level evidence of the lexical
co-occurrence of α.
</bodyText>
<subsectionHeader confidence="0.999527">
2.2 Corpus-level significant co-occurrence
</subsectionHeader>
<bodyText confidence="0.8493164">
We now describe how to aggregate evidence for lex-
ical significance by considering the occurrence of
α across multiple documents in the corpus. Let
{D1, ... , DK} denote the set of K documents (from
out of the entire corpus) that contain at least one
occurrence of α. Let `i be the length of Di, fi
�
be the frequency of α in Di, and, fix be the span-
constrained frequency of α in Di. Define indicator
variables zi, i = 1, ... , K as:
</bodyText>
<equation confidence="0.943584333333333">
� 1 if πx( �fx i , fi, `i) &lt; �
zi = (1)
0 otherwise
</equation>
<bodyText confidence="0.9902128125">
As discussed previously, zi indicates whether “α
is an E-significant word-pair within the document
where gE,x(fi, `i) is given by Definition 2 below.
Definition 2 Given a document of length ` in which
a word-pair has a frequency of f, and given a span
threshold x, we define gE,x(f, `) as the smallest r for
which the inequality [πx(r, f, `) &lt; E] holds.
Note that gE,x(f, `) is well-defined since πx(r, f, `)
is non-increasing with respect to r. For the
example given earlier, g0.2,20(4, 400) = 3 and
g0.05,20(4, 400) = 4. Since each document in the
corpus is assumed to be generated independently,
zi’s are independent random variables and we can
bound the deviation of the observed value of Z from
its expectation using Hoeffding’s Inequality – for
any t &gt; 0, we have
</bodyText>
<equation confidence="0.997658">
P[Z &gt; E(Z) + Kt] &lt; exp(−2Kt2)
= δ (3)
</equation>
<bodyText confidence="0.946517055555556">
Recall that Z models the number of documents sup-
porting the hypothesis “α is an E-significant word-
pair.”). Thus, the upper-bound δ (= exp(−2Kt2)),
0 &lt; δ &lt; 1 denotes the upper-bound on the prob-
ability that just due to random chance, more than
(E(Z) + Kt) documents out of K will support the
hypothesis “α is an E-significant word-pair.” We
call δ the corpus-level evidence of the lexical co-
occurrence α. For example, in our corpus, the word-
pair (canyon, landscape) occurs in K = 416 doc-
uments. For E = 0.1, we have E-significant occur-
rences in Z = 33 documents (out of 416) , while
E(Z) = 14.34. Suppose we want to be 99% sure
that the occurrences of (canyon, landscape) in the
33 documents were a consequence of non-random
phenomena. Let δ = 1 − 0.99 = 0.01. By setting
fx, f, `) decreases with
document” if we have [πx(
</bodyText>
<page confidence="0.761756">
1060
</page>
<table confidence="0.997315083333333">
word-1 word-2
(0.1, 0.1) (0.1, 0.4) (0.4, 0.1)
algae green mold pool
amuse entertain clown amaze
damn hell mad bad
rat dirty ugly disease
sedative drug narcotic calm
topping chocolate flavour caramel
umbrella rain dry shade
unknown known dark secret
worm insect dirt fishing
wrap cover seal bandage
</table>
<tableCaption confidence="0.982218">
Table 1: Examples of word-pairs from Florida dataset
having statistically significant co-occurrences in the
wikipedia corpus for different (e, δ) combinations under
a span constraint of 20 words.
</tableCaption>
<equation confidence="0.987054">
V/
t = lnδ/(−2K) = 0.07, we get E(Z) + Kt =
</equation>
<bodyText confidence="0.951105952380952">
43.46. Only if Z was 44 or more, there would be less
than 1% chance of that being a random phenomena.
Thus, we cannot be 99% sure that the observed co-
occurrences in the 33 documents are non-random.
Hence, our test declares (canyon, landscape) as in-
significant at c = 0.1, δ = 0.01. We now summarize
our significance test in the definition below.
Definition 3 (Significant lexical co-occurrence)
Consider a word-pair α and a set of K documents
containing at least one occurrence each of α. Fix
a span threshold of x (&gt; 0), a document-level
evidence of c (0 &lt; c &lt; 1) and a corpus-level
evidence of δ (0 &lt; δ &lt; 1). Let Z denote
the number of documents (out of K) that sup-
port the hypothesis “α is c-significant within
the document.” The word-pair α is said to be
(E, δ)-significant if we have [Z ≥ E(Z) + Kt],
V/
where t = log δ/(−2K) and E(Z) is given by
Eq. (2). The ratio [Z/(E(Z) + Kt)] is called the
Co-occurrence Significance Ratio (CSR) for α.
</bodyText>
<subsectionHeader confidence="0.986243">
2.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999983810344828">
The significance test of Definition 3 gathers both
document-level and corpus-level evidence from data
in calibrated amounts. Prescribing c fixes the
strength of the document-level hypothesis in our
test, while, δ, controls the extent of corpus-level ev-
idence we need to declare a word-pair as significant.
A small δ demands that there must be multiple doc-
uments in the corpus, each of which, individually
have some evidence of relatedness for the pair of
words.
By running the significance test with different val-
ues of c and δ, the CSR test can be used to detect dif-
ferent types of lexically significant co-occurrences.
For example, the strongest lexical co-occurrences
would have both strong document-level evidence
(low c) as well as high corpus-level evidence (low
δ). Informally, these would represent pairs of words
that appear multiple times with small spans within a
document, in many documents, and in-practice, we
find that multi-word expressions or pairs of words
separated by stop words tend to dominate this type.
On the other hand, a higher c would represent word-
pairs that appear relatively farther apart within a
document, or a higher δ would represent word-pairs
that appear together in relatively fewer documents.
Note that to detect co-occurrences that exclusively
correspond to (say) low c and high δ, we would have
to run the test with low c and high δ, and then re-
move word-pairs that were also found significant at
low c and low δ.
In Table 1, we present some examples of different
types of co-occurrences. The table lists word-pairs
that were found to be statistically significant for dif-
ferent choices of (E, δ). Note that a word-pair is re-
ported under (c = 0.1, δ = 0.4) or (c = 0.4, δ =
0.1) only if it was not also found significant under
other two parameter settings. The strongest corre-
lations are the word-pairs corresponding to (E =
0.1, δ = 0.1) e.g., algae-green, rat-dirty and worm-
insect. Different sets of weaker co-occurrences are
detected depending on whether we relaxed δ or c.
For example, algae-mold is significant at a higher δ,
while algae-pool is significant for higher c.
The semantic notion of word association is an
abstract concept and different kinds of associations
(with potentially different statistical characteriza-
tions) may be preferred by human judges in differ-
ent situations. While in Section 5, we discuss in de-
tail various datasets used, the evaluation methodol-
ogy, and the performance of CSR across datasets,
we wish to point out here that in 3 out of 5 cross-
validation runs for wordsim dataset, the best per-
forming CSR parameters were x = 50w, c = 0.1
and δ = 0.9, while in 3 out of 5 runs for Minnesota
dataset, the best performing CSR parameters were
x = 20w, c = 0.3 and δ = 0.5. This gives us some
indication that different kinds of word associations
were preferred in different data sets.
</bodyText>
<page confidence="0.974981">
1061
</page>
<sectionHeader confidence="0.7585795" genericHeader="method">
3 Computing Co-occurrence Significance
Ratio(CSR)
</sectionHeader>
<bodyText confidence="0.996145375">
There are three main steps for computing CSR
and the pseudocodes for these are listed in Proce-
dures 1, 2 &amp; 3. Of these, the first two can be run
offline since they do not depend on the text corpus.
They need to be run only once, after which CSR can
be computed for any word-pair on any given corpus
of documents. We describe these steps in the sub-
sections below.
</bodyText>
<figure confidence="0.605026666666667">
3.1 Computing histogram histf,`,x(·)
The first step is to compute a histogram for the span-
b
</figure>
<figureCaption confidence="0.727714">
constrained frequency, fx, of a word-pair whose fre-
quency is f in a document of length E, given a cho-
sen span threshold of x (under our null model).
</figureCaption>
<bodyText confidence="0.783376">
Definition 4 Given a document of length E and a
span threshold of x, we define histf,`,x( bfx) as the
number of ways to embed f non-overlapped occur-
rences of a word-pair in the document such that ex-
</bodyText>
<figure confidence="0.895203666666667">
b
actly fx occurrences have span less than x.
Procedure 1 ComputeHist(f, E, x) – Offline
Input f - number of non-overlapped occurrences; ` - document length;
x - span threshold
Computes histf,`,x[·] as per Definition 4
</figure>
<listItem confidence="0.94644">
1: Initialize histf,`,x [bfx] ← 0 for bfx = 0, . . . , f
2: if f &gt; ` then
3: return
4: if f = 0 then
5: histf,`,x[0] ← 1
6: return
7: for i ← 1 to (` − 1) do
8: for j ← (i + 1) to ` do
9: histf_1,`_j,x ← ComputeHist(f − 1, ` − j, x)
10: for k ← 0 to f − 1 do
11: if (j − i) &lt; x then
12: histf,`,x[k + 1] ← histf,`,x[k + 1]
+ histf_1,`_j,x[k]
13: else
14: histf,`,x[k] ← histf,`,x[k] + histf_1,`_j,x[k]
</listItem>
<bodyText confidence="0.99003625">
Procedure 1 lists the pseudocode for computing
the histogram histf,`,x. The main steps involve se-
lecting a start and end position for embedding the
very first occurrence (lines 7-8) and then recursively
calling ComputeHist(·, ·, ·) (line 9). The i-loop
selects a start position for the first occurrence of
the word-pair, and the j-loop selects the end posi-
tion. The recursion step now computes the num-
ber of ways to embed the remaining (f − 1) non-
overlapped occurrences in the remaining (E − j)
positions. Once we have histf−1,`−j, we check
whether the occurrence introduced at positions (i, j)
</bodyText>
<figure confidence="0.356537">
b
</figure>
<bodyText confidence="0.9984715">
will contribute to the fx count. If (j − i) &lt; x,
whenever there are k span-constrained occurrences
in positions (j + 1) to E, there will be (k + 1)
span-constrained occurrences in positions 1 to E.
Thus, we increment histf,`[k + 1] by the quantity
histf−1,`−j[k] (lines 10-12). However, if (j − i) &gt;
x, there is no contribution to the span-constrained
frequency from the (i, j) occurrence, and so we in-
crement histf,`[k] by the quantity histf−1,`−j[k]
(lines 10-11, 13-14). Finally, we note that in our
implementation we use memorization to avoid re-
dundant recursive calls.
</bodyText>
<table confidence="0.562650666666667">
3.2 Computing 7rx(·, f, E) distribution
Procedure 2 ComputePiDist(f, E, x) – Offline
Input f - number of non-overlapped occurrences; ` - document length;
x - span threshold
Computes Distribution πx[f, `, ·] as per Definition 1 and g,,x[f, `] as
per Definition 2
</table>
<listItem confidence="0.907569166666667">
1: N[f, `, x] = Pfk=0 histf,`,x[k]
2: for bfx ← 0 to f do
3: Nx[ bfx, f, `]← Pfk= fx histf,`,x[k]
4: πx [ bfx,f, `] ← Nx[ �fx,f,`]
N[f,`,x]
5: g,,x[f, `] ← min{r  |πx[r, f, `] &lt; e}
</listItem>
<bodyText confidence="0.909361875">
The second offline step is computation of the
7rx(·, f, E) distribution. We store the number of ways
of embedding f non-overlapped occurrences of a
word-pair in a document of length E in the array
b
N[f, E]. Similarly, the array Nx[ fx, f, E] stores the
number of ways of embedding f non-overlapped oc-
currences of the word-pair in a document of length E,
</bodyText>
<subsectionHeader confidence="0.249095">
b
</subsectionHeader>
<bodyText confidence="0.974710428571429">
such that at least fx of the f occurrences have span
less than x. To compute N[f, E, x] and Nx[ bfx,f,E],
we need the histogram histf,`,x[·] which is the out-
put of Procedure 1. Procedure 2 lists the pseu-
docode for computing 7rx(
b bfx, f, E) from N(f, E) and
Nx( fx, f, E) given histf,` from Procedure 1 (For the
sake of readability the pseudocode does not describe
some optimizations that we used in our implementa-
tion).
The Procedure 1 is exponential in f and E but
it does not depend on the data corpus. Hence, we
can run the Procedures 1 and 2 off-line, and publish
the 7rx[] and g,,x[] tables for various x, bfx, f and
</bodyText>
<page confidence="0.964105">
1062
</page>
<figure confidence="0.577677363636364">
E. Using these tables5, anyone wishing to compute
CSR needs to only run Procedure 3.
3.3 Computing CSR for a given word-pair
Procedure 3 ComputeCSR(α, E, 6, x)
Input α - word-pair; E - document-level evidence; δ - corpus-level ev-
idence; x - span threshold; Corpus of documents
Computes CSR(α) - Co-occurrence Significance Ratio (CSR) for α
as per Definition 3
1: D +- {D1, ... , DK} // Set of documents from the corpus that
each contain at least one occurrence of α.
�
</figure>
<listItem confidence="0.968645615384615">
2: t +- log δ/(−2K)
3: Z +- 0 and ZE +- 0
4: for i +- 1 to K do
5: `i = Length of Di
6: fi = Frequency of α in Di
7: �fxi = Span-constrained frequency of α in Di
8: if πx[ �fx i, fi, `i] &lt; E then
9: zi +- 1
10: else
11: zi +- 0
12: Z +- Z + zi r,, r{
13: ZE +- ZE + πx�geLfi, `i, x], fi, `i]
14: CSR(α) = Z/(ZE + Kt)
</listItem>
<bodyText confidence="0.999878916666667">
Procedure 3 implements the significance test
given in Definition 3 and requires that the 7rx[] and
gE,x[] tables have already been computed offline.
The first step is to determine the subset D of docu-
ments containing the given word-pair (line 1). Then
we compute t based on 6 and K (the size of D)
(line 2). Next we determine how many of the K
documents support the hypothesis “α is &amp;significant
within the document” (lines 3-12). The expected
number of documents supporting the hypothesis is
accumulated in ZE (line 13). CSR is then computed
as the ratio of Z to (ZE + Kt) (line 14).
</bodyText>
<subsectionHeader confidence="0.968003">
3.4 Run-time overhead
</subsectionHeader>
<bodyText confidence="0.999950636363636">
The computation of Co-occurrence Significance Ra-
tio (CSR) as given in Definition 3 might appear
more complex than the simple formulae for other
co-occurrence measures given in Table 2. However,
bulk of the complexity in calculating CSR lies in
the one-time (data independent) off-line computa-
tion of the 7rx[] and gE,x[] tables. Once these tables
are published, the cost of comparing CSR for a given
word pair is comparable to the cost of computing
any other (spanned) measure in Table 2. The main
data-dependent computations for a spanned measure
</bodyText>
<footnote confidence="0.852062">
5http://www.cse.iitb.ac.in/˜damani/papers/EMNLP11/
resources.html
</footnote>
<bodyText confidence="0.999809625">
are in determining span-constrained frequencies; all
other steps are simple arithmetic operations or mem-
ory lookups. To illustrate this, Procedure 4 gives de-
tails of computing PMI. The comparison of Proce-
dures 3 and 4 shows their almost parallel structures.
The main overhead in these procedures is incurred
in line 7, where span-constrained frequencies in a
given document are computed.
</bodyText>
<equation confidence="0.844485">
Procedure 4 ComputePMI(a, b)
Input (x, y) - word pair;
Computes PMI (Table 2) for (x, y).
</equation>
<listItem confidence="0.814586454545454">
1: let D = {D1, ... , DK} // set of documents containing at least
one occurrence of α.
2: N = total number of words in corpus
3: (fx,fy) = unigram frequencies of x, y in corpus
4: (px,py) = (fx/N,fy/N)
5: f�= 0
6: for i +- 1 to K do
7: fi = span-constrained frequency of α in Di
8: f�= f+�fi
9: ˆpx,y =f/N
10: PMI = log(ˆns,b )
</listItem>
<bodyText confidence="0.27727">
nsnb
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999952454545455">
Existing word association measures can be divided
into three broad categories: (i) Co-occurrence mea-
sures that rely on co-occurrence frequencies of both
words in a corpus in addition to the individual
unigram frequencies (Table 2), (ii) Distributional
similarity-based measures that characterize a word
by the distribution of other words around it (Agirre
et al., 2009; Bollegala et al., 2007; Chen et al., 2006;
Wandmacher et al., 2008), and (iii) Knowledge-
based measures that use knowledge-sources like
thesauri, semantic networks, or taxonomies (Milne
and Witten, 2008; Hughes and Ramage, 2007;
Gabrilovich and Markovitch, 2007; Yeh et al., 2009;
Strube and Ponzetto, 2006; Finkelstein et al., 2002;
Liberman and Markovitch, 2009).
In this paper, we focus on comparison with
other co-occurrence measures. These measures
are used in several domains like ecology, psy-
chology, medicine, and language processing. Ta-
ble 2 lists several measures chosen from all these
domains. Except Ochiai (Ochiai, 1957; Janson
and Vegelius, 1981) and the recently introduced
</bodyText>
<page confidence="0.751507">
1063
</page>
<table confidence="0.9777921875">
Method Formula
CSR (this work) Z/(E(Z) + Kt)
CWCD (Washtell and ˆf(x,y) 1/max(p(x),p(y))
Markert, 2009)
p(x) M
Dice (Dice, 1945) 2 ˆf (x,y)
f(x)+f(y)
LLR (Dunning, 1993) p(x0,y0)
� p(x0, y0)log
p(x0)p(y0)
x0 ∈ {x, ¬x}
y0 ∈ {y, ¬y}
Jaccard (Jaccard, 1912) ˆf(x,y)
f(x)+f(y)−ˆf(x,y)
Ochiai (Janson and Veg- ˆf(x,y)
elius, 1981)
√f(x)f(y)
Pearson’s χ2 test (ˆf(x&amp;quot;y0)−Eˆf(x0 ,y0))2
,
x0 ∈ {x, ¬x} Ef (x0,y )
y0 ∈ {y, ¬y}
PMI (Church and Hanks, log p(x,y)
1989)
p(x)p(y)
SCI (Washtell and Mark- p(x,y)
ert, 2009)
p(x)√p(y)
T-test ˆf(x,y)−E ˆf(x,y)
ˆf(x,y) 1− ˆf(x,y)
/
N Total number of tokens in the corpus
f(x), f(y) unigram frequencies of x, y in the corpus
</table>
<equation confidence="0.7774822">
p(x), p(y) f(x)/N, f(y)/N
ˆf(x, y) Span-constrained (x, y) word pair frequency in corpus
ˆp(x, y) ˆf(x, y)/N
M Harmonic mean of the spans of ˆf(x, y) occurrences
E ˆf(x, y) Expected value of ˆf(x, y)
</equation>
<tableCaption confidence="0.97128">
Table 2: Co-occurrence measures.
</tableCaption>
<bodyText confidence="0.998343266666667">
CWCD6 (Washtell and Markert, 2009) all other
measures are well-known in the NLP commu-
nity (Pecina and Schlesinger, 2006). Our results
show that Ochiai and Chi-Square have almost iden-
tical performance, differing only in 3rd decimal dig-
its. Rankings produced by Chi-square is almost
monotonic with respect to the rankings produced by
Ochiai. This is because, for most word pairs (x, y),
[f(x) « N], [f(y) « N], [f(x, y) « f(x)], and
[f(x, y) « f(y)]. Therefore three of the four terms
in the Chi-square summation become zero7 and the
fourth term approximates to the square of Ochiai.
Similarly Jaccard and Dice coincide. While present-
ing our experimental results, we report these pairs of
measures together.
</bodyText>
<footnote confidence="0.954322571428571">
6CWCD was reported in (Washtell and Markert, 2009) as
the best performing variant among the so-called windowless (or
spanless) measures. In our experiments, we implemented win-
dowed (spanned) version of the CWCD measure.
7For example, ˆf(x, -y) − Ef(x, -y) = f(x) − N x
pf(x) x pf(-y) = f(x) − 1N x f(x) x f(-y) = f(x) −
n,x f(x) x N = 0.
</footnote>
<table confidence="0.999536523809524">
Aspect Data Set No. of No. of No. of
Respon- Word Filtered
dents Pairs Word
Pairs
Semantic wordsim 16 353 351
relatedness (Finkelstein et al.,
2002)
Edinburg (Kiss et al., 100 325,588 83,713
1973)
Florida (Nelson et 5,019 65,523 59,852
al., 1980)
Free- Goldfarb-Halpern 316 410 384
Association (Goldfarb and
Halpern, 1984)
Kent (Kent and 1,000 14,576 14,086
Rosanoff, 1910)
Minnesota (Russell 1,007 10,447 9,649
and Jenkins, 1954)
White-Abrams 440 745 652
(White and Abrams,
2004)
</table>
<tableCaption confidence="0.999725">
Table 3: Characteristics of data sets used.
</tableCaption>
<sectionHeader confidence="0.955635" genericHeader="method">
5 Performance Evaluation
</sectionHeader>
<bodyText confidence="0.999895166666667">
Two main aspects of word association studied in lit-
erature are: a) semantic relatedness, and b) free as-
sociation. Semantic relatedness encompasses many
different relationships between words, like syn-
onymy, meronymy, antonymy, and functional asso-
ciation (Budanitsky and Hirst, 2006). Free associ-
ation refers to the first response-words that come
to mind when presented with a stimulus. (ESSLLI,
2008). We experiment with all the publicly available
datasets that come with gold standard judgement of
these aspects, except the very small ones with less
than 80 word-pairs8.
</bodyText>
<subsectionHeader confidence="0.988382">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999902833333333">
Details9 of the datasets used in our experiments are
listed in Table 3. Each data set comes with a gold-
standard of human judgments - a ranked list of asso-
ciation scores for the word-pairs in the data set. The
wordsim dataset was prepared by asking the subjects
to estimate the relatedness of the word pairs on a
</bodyText>
<footnote confidence="0.6762161">
8(MillerCharles (Miller and Charles, 1991), Rubenstein-
Goodenough (Rubenstein and Goodenough, 1965) and
TOEFL (Landauer and Dumais, 1997))
9We removed word-pairs containing multiword expressions.
For data sets with more than 10,000 word-pairs, we filtered out
pairs that contain stop words listed in (StopWordList, 2010).
For Edinburg (size 275393 after previous filtering), we further
filtered word-pairs where the response was supported by only
one respondant. Original and filtered data sets are available at
http://www.cse.iitb.ac.in/˜damani/papers/EMNLP11/resources.html
</footnote>
<page confidence="0.91159">
1064
</page>
<table confidence="0.998584727272727">
Edinburg Florida Kent Minnesota White- Goldfarb- wordsim
(83,713) (59,852) (14,086) (9,649) Abrams Halpern (351)
(652) (384)
CSR 0.25 0.30 0.42 0.31 0.34 0.10 0.63
CWCD 0.23 0.23 0.40 0.30 0.21 0.19 0.54
Dice (Jaccard) 0.20 0.27 0.43 0.32 0.21 0.09 0.59
LLR 0.20 0.26 0.40 0.29 0.18 0.03 0.51
Ochiai (χ2) 0.24 0.30 0.43 0.31 0.29 0.08 0.62
PMI 0.22 0.25 0.36 0.26 0.22 0.11 0.69
SCI 0.24 0.27 0.38 0.27 0.23 0.06 0.37
TTest 0.17 0.23 0.37 0.26 0.17 -0.02 0.45
</table>
<tableCaption confidence="0.763673">
Table 4: Comparison of the average Spearman coefficients obtained across five cross-validation runs by different
measures. The best performing measure for each data-set is shown in bold. All standard deviations for Edinburg and
Florida were less than 0.01, for Kent and Minnesota were between 0.01 and 0.02, for White-Abrams were between
0.05 and 0.08, for Goldfarb-Halpern between 0.05 and 0.15 and for wordsim were between 0.02 and 0.15. Number of
word-pairs in each dataset is shown in brackets against its name.
</tableCaption>
<table confidence="0.999626545454545">
Edinburg Florida Kent Minnesota White- Goldfarb- wordsim Worst Avg. Worst
(83,713) (59,852) (14,086) (9,649) Abrams Halpern (351) Rank Deviation Deviation
(652) (384)
CSR 0.00 (1) 0.00 (1) 0.01 (3) 0.01 (2) 0.00 (1) 0.09 (3) 0.06 (2) 3 0.02 0.09
CWCD 0.02 (4) 0.07 (7) 0.03 (4) 0.02 (4) 0.13 (5) 0.00 (1) 0.15 (5) 7 0.06 0.15
Dice (Jaccard) 0.05 (6) 0.03 (3) 0.00 (1) 0.00 (1) 0.13 (5) 0.10 (4) 0.10 (4) 6 0.06 0.13
LLR 0.05 (6) 0.04 (5) 0.03 (4) 0.03 (5) 0.16 (7) 0.16 (7) 0.18 (6) 7 0.09 0.18
Ochiai (χ2) 0.01 (2) 0.00 (1) 0.00 (1) 0.01 (2) 0.05 (2) 0.11 (5) 0.07 (3) 5 0.04 0.11
PMI 0.03 0.05 0.07 (8) 0.06 (7) 0.12 (4) 0.08 (2) 0.00 (1) 8 0.06 0.12
SCI 0.01 (2) 0.03 (3) 0.05 (6) 0.05 (6) 0.11 (3) 0.13 (6) 0.32 (8) 8 0.10 0.32
TTest 0.08 (8) 0.07 (7) 0.06 (7) 0.06 (7) 0.17 (8) 0.21 (8) 0.24 (7) 8 0.13 0.24
</table>
<tableCaption confidence="0.777005">
Table 5: Comparison of deviations from the best performing measure on each data set. Number of word-pairs in each
dataset is shown in brackets against its name. Figures in brackets against the deviation values denote the ranks of the
measures in the corresponding data sets.
</tableCaption>
<bodyText confidence="0.999933153846154">
scale from 0 to 10 (Finkelstein et al., 2002). The
methodology for collecting free association data is
explained at (ESSLLI, 2008): The degree of free as-
sociation between a stimulus (S) and response (R) is
the percentage of respondents who respond R as the
first response when presented with stimulus S.
These datasets are of varying size, and they were
constructed at different point in time, in different ge-
ographies. This allows us to compare different mea-
sures comprehensively under varying range of cir-
cumstances. To the best of our knowledge, no pre-
vious work has reported such a detailed comparison
of co-occurrence measures.
</bodyText>
<subsectionHeader confidence="0.999149">
5.2 Resources Used
</subsectionHeader>
<bodyText confidence="0.9999893">
We use the Wikipedia (Wikipedia, April 2008) cor-
pus with 2.7 million articles (total of 1.24 Giga-
words). We did no pre-processing - no lemmatiza-
tion or function-word removal. When counting doc-
ument size (in words), punctuations were ignored.
Documents larger than 1500 words were partitioned
such that each part was at most 1500 words10. We
indexed the corpus using Lucene search engine li-
brary and used Lucene APIs to obtain various statis-
tics and documents containing given word-pairs.
</bodyText>
<subsectionHeader confidence="0.996815">
5.3 Methodology
</subsectionHeader>
<bodyText confidence="0.871173923076923">
Each measure listed in Table 2 produces a ranked
list of association scores for the word-pairs in a data
set. We evaluate each measure by the Spearman’s
rank correlation between the ranking produced by
the measure and the gold-standard ranking.
The span threshold (or window-width) x is a user-
defined parameter in all measures. In addition, CSR
has the parameters c and δ. For any measure, the
ranking of word-pairs will likely change with chang-
10While this limit can be raised using heavier computing re-
sources, we believe that partitioning documents of sizes greater
than 1500 words was reasonable (especially since typical span
values we used were less than 50, much less than 1500).
</bodyText>
<page confidence="0.886757">
1065
</page>
<table confidence="0.999894833333333">
Method Resource wordsim Esslli
(272)
wordsim sim rel
(353) (203) (252)
PMI Wikipedia 0.69 0.72 0.68 0.32
Ochiai (χ2) Wikipedia 0.62 0.68 0.62 0.44
Significance Ratio (CSR) Wikipedia 0.63 0.70 0.64 0.43
Latent Semantic Analysis (Wandmacher et al., 2008) Newspaper corpus - - - 0.38
Graph Traversal (WN30g) (Agirre et al., 2009)) Wordnet 0.66 0.72 0.56 -
Bag of Words based Distributional Similarity (BoW) (Agirre et al., 2009)) Web corpus 0.65 0.70 0.62 -
Context Window based Distributional Similarity (CW) (Agirre et al., 2009)) Web corpus 0.60 0.77 0.46 -
Hyperlink Graph (Milne and Witten, 2008) Wikipedia hyperlinks graph 0.69 - --
Random Graph Walk (Hughes and Ramage, 2007) WordNet 0.55 - - -
Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) Wikipedia concepts 0.75 - - -
(reimplemented in (Yeh et al., 2009)) (0.71)
Normalized Path-length (lch) (Strube and Ponzetto, 2006) Wikipedia category tree 0.55 - - -
Thesarus based (Jarmasz, 2003) Roget’s thesaurus 0.55 - - -
Latent Semantic Analysis (Finkelstein et al., 2002) Web corpus 0.56 - - -
</table>
<tableCaption confidence="0.8216675">
Table 6: Comparison of co-occurrence based measures with knowledge-based and distributional similarity based
measures. These other measures have not been applied to the free association datasets shown in Table 3. Data for
missing entries is not available. Note that sim and rel are subsets of wordsim dataset. Number of word-pairs in each
dataset is shown in brackets against its name.
</tableCaption>
<bodyText confidence="0.999775153846154">
ing parameter values. Hence we follow the standard
methodology of fixing parameters through cross val-
idation. Specifically, we partition the data into five
folds, four of which are used for training and one
hold-out fold is used for testing. For each mea-
sure, the parameter values that achieve best corre-
lation with human judgments on 4 training folds are
used to predict on the 1 hold-out testing fold. This
experiment is repeated 5 times for different training
and test folds. The average rank correlation obtained
by each measure over 5 cross-validation runs is re-
ported for each dataset. We varied c and δ between
0.01 and 0.90 and x between 5 and 50 words.
</bodyText>
<sectionHeader confidence="0.714238" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.999948733333333">
For each measure and for each data set, the aver-
age correlation over the 5 cross-validation runs is
reported in Table 4. The corresponding standard de-
viations are mentioned in the table’s caption. The
best performing measure in each case is highlighted
in bold. While different measures performed best on
different data sets, the results in Table 4 shows that
CSR performs consistently well across all data sets.
In all data sets the correlation for CSR was always
either the best or close to the best.
As expected, our results are statistically more sig-
nificant for the larger data sets, compared to the
smaller ones. The standard deviations of the results
are small for two largest data sets (less than 0.01
for Edinburg and Florida), gradually increasing (less
than 0.02 for Kent and Minnesota), and becoming
high (upto .15) for the three smallest datasets.
Although, among all measures, CSR has the best
average correlation over all datasets, taking average
of correlations across widely different dataset is not
a meaningful way to decide on which measure to
use. Ideally one would like to access an oracle to
learn which measure will perform best on a particu-
lar unseen application dataset. Short of such an ora-
cle, if one were to pick a fixed measure a-priori, then
one would like to know how much worse off one is
compared to the best measure for that dataset.
To compare different measures from this perspec-
tive, we compute the deviation of the correlation for
each measure from the correlation of the best mea-
sure for each data set. These deviations are reported
in Table 5, along with the corresponding ranks. The
average deviation of CSR over all the data sets is
0.02, which is the least among all the measures, the
next two being 0.04 and 0.06. CSR also has the least
worst-deviation among all measures. Also, CSR is
never ranked worse than 3 in any of the data sets.
This is also the smallest worst-rank among all mea-
sures. Based on these results, we infer that CSR
is overall the best performing co-occurrence based
word association measure.
While the focus of our work is on the co-
occurrence measures, for completeness, we present
all the known results for knowledge and distribu-
tional similarity-based measures on the datasets un-
</bodyText>
<page confidence="0.682591">
1066
</page>
<bodyText confidence="0.923725195121951">
der consideration in Table 6. Note that in (Agirre et Kenneth Ward Church and Patrick Hanks. 1989. Word
al., 2009), the wordsim data set was partitioned into association norms, mutual information and lexicogra-
two sets, namely sim and rel, and in Esslli shared phy. In ACL, pages 76–83.
task (ESSLLI, 2008), a 272 word pair subset of the L. R. Dice. 1945. Measures of the amount of ecological
Edinburgh dataset was chosen. To facilitate compar- association between species. Ecology, 26:297–302.
ison, in addition to CSR, we also present results for Ted Dunning. 1993. Accurate methods for the statistics
PMI and Ochiai (Chi-Square) which are the best per- of surprise and coincidence. Computational Linguis-
forming co-occurrence measures on wordsim, and tics, 19(1):61–74.
Esslli datasets. For co-occurrence-based measures, ESSLLI. 2008. Free association task at lexical seman-
we used 5-fold cross validation, which is inapplica- tics workshop esslli 2008. http://wordspace.
ble for parameterless measures. Results show that collocations.de/doku.php/workshop:
co-occurrence-based measures compare well with esslli:task.
other resource-heavy measures. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
6 Conclusions Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
In this paper, we introduced a new measure called Ruppin. 2002. Placing search in context: the concept
CSR for word-association based on statistical sig- revisited. ACM Trans. Inf. Syst., 20(1):116–131.
nificance of lexical co-occurrences. Our measure, Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
while being agnostic to global unigram frequencies, puting semantic relatedness using wikipedia-based ex-
detects skews in span distributions of word-pairs in plicit semantic analysis. In IJCAI.
documents containing both words. We carried out Robert Goldfarb and Harvey Halpern. 1984. Word asso-
extensive evaluation on several benchmark datasets. ciation responses in normal adult subjects. Journal of
Our experiments demonstrate the advantages of our Psycholinguistic Research, 13(1):37–55.
measure over all the competing measures. T Hughes and D Ramage. 2007. Lexical semantic relat-
Acknowledgments edness with random graph walks. In EMNLP.
This work was supported in part by the Ministry P. Jaccard. 1912. The distribution of the flora of the
of Human Resources Development, Government of alpine zone. New Phytologist, 11:37–50.
India and by the Tata Research Development and Svante Janson and Jan Vegelius. 1981. Measures of eco-
Design Center (TRDDC). We thank Mr. Justin logical association. Oecologia, 49:371–376.
Washtell (University of Leeds) for providing us with Mario Jarmasz. 2003. Rogets thesaurus as a lexical re-
various datasets. source for natural language processing. Technical re-
References port, University of Ottowa.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana G. Kent and A. Rosanoff. 1910. A study of association
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A in insanity. American Journal of Insanity, pages 317–
study on similarity and relatedness using distributional 390.
and wordnet-based approaches. In NAACL-HLT. G. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru An associative thesaurus of english and its computer
Ishizuka. 2007. Measuring semantic similarity be- analysis. In The Computer and Literary Studies, pages
tween words using web search engines. In WWW, 379–382. Edinburgh University Press.
pages 757–766. T. Landauer and S. Dumais. 1997. The latent semantic
Alexander Budanitsky and Graeme Hirst. 2006. Evalu- analysis theory of acquisition, induction, and represen-
ating wordnet-based measures of lexical semantic re- tation of knowledge. In Psychological Review, volume
</bodyText>
<reference confidence="0.99873075">
latedness. Computational Linguists, 32(1):13–47. 104/2, pages 211–240.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei. Sonya Liberman and Shaul Markovitch. 2009. Com-
2006. Novel association measures using web search pact hierarchical explicit semantic representation. In
with double checking. In ACL. Proceedings of the IJCAI 2009 Workshop on User-
1067 Contributed Knowledge and Artificial Intelligence: An
Evolving Synergy (WikiAI09), Pasadena, CA, July.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1–28.
David Milne and Ian H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In ACL.
D. Nelson, C. McEvoy, J. Walling, and J. Wheeler.
1980. The university of south florida homograph
norms. Behaviour Research Methods and Instrumen-
tation, 12:16–37.
A Ochiai. 1957. Zoogeografical studies on the soleoid
fishes found in japan and its neighbouring regions-ii.
Bulletin of the Japanese Society of Scientific Fisheries,
22.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
ACL.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 8(10):627–633, October.
W.A. Russell and J.J. Jenkins. 1954. The complete
minnesota norms for responses to 100 words from the
kent-rosanoff word association test. Technical report,
Office of Naval Research and University of Minnesota.
StopWordList. 2010. http://ir.dcs.gla.ac.
uk/resources/linguistic_utils/stop_
words. The Information Retrieval Group, University
of Glasgow. Accessed: November 15, 2010.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI, pages 1419–1424.
T. Wandmacher, E. Ovchinnikova, and T. Alexandrov.
2008. Does latent semantic analysis reflect human as-
sociations? In European Summer School in Logic,
Language and Information (ESSLLI’08).
Justin Washtell and Katja Markert. 2009. A comparison
of windowless and window-based computational asso-
ciation measures as predictors of syntagmatic human
associations. In EMNLP, pages 628–637.
Katherine K. White and Lise Abrams. 2004. Free as-
sociations and dominance ratings of homophones for
young and older adults. Behavior Research Methods,
Instruments, &amp; Computers, 36(3):408–420.
Wikipedia. April 2008. http://www.wikipedia.
org.
Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre,
and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In ACL work-
shop ”TextGraphs-4: Graph-based Methods for Natu-
ral Language Processing”.
</reference>
<page confidence="0.97654">
1068
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704286">
<title confidence="0.998411">Lexical Co-occurrence, Statistical Significance, and Word Association</title>
<author confidence="0.998618">Dipak L Chaudhari Om P Damani Srivatsan Laxman</author>
<affiliation confidence="0.999439">Computer Science and Engg. Computer Science and Engg. Microsoft Research India</affiliation>
<address confidence="0.717965">IIT Bombay IIT Bombay Bangalore</address>
<email confidence="0.988061">dipakc@cse.iitb.ac.indamani@cse.iitb.ac.inslaxman@microsoft.com</email>
<abstract confidence="0.99965752173913">Lexical co-occurrence is an important cue for detecting word associations. We propose a new measure of word association based on a new notion of statistical significance for lexical co-occurrences. Existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts. Instead, we focus only on documents that contain both terms (of a candidate word-pair) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model. This would imply that the words in the pair are not related strongly enough for one word to influence placement of the other. However, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words. Through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>