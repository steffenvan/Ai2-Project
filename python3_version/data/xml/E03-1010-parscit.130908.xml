<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9879505">
Applications of Automatic Evaluation Methods to Measuring a Capability
of Speech Translation System
</title>
<author confidence="0.502222">
Keiji Yasuda
</author>
<note confidence="0.69106425">
ATR Spoken Language Translation Research Laboratories
2-2-2, Hikaridai,&amp;quot;Keihanna Science City&amp;quot;, Kyoto, 619-0288, Japan
keiji . yasuda@atr . co . jp
Also at Graduate School of Engineering, Doshisha University
</note>
<author confidence="0.795519">
Fumiaki Sugaya
</author>
<affiliation confidence="0.697074">
KDDI R&amp;D Laboratorie
</affiliation>
<address confidence="0.8088235">
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
</address>
<email confidence="0.988307">
fsugaya@kddilabs.jp
</email>
<author confidence="0.931403">
Toshiyuki Takezawa
</author>
<affiliation confidence="0.7690635">
ATR Spoken Language
Translation Research Laboratories
2-2-2, Hikaridai, &amp;quot;Keihanna Science City&amp;quot;,
Kyoto, 619-0288, Japan
</affiliation>
<author confidence="0.934254">
Seiichi Yamamoto Masuzo Yanagida
</author>
<affiliation confidence="0.8575915">
ATR Spoken Language Doshisha University
Translation Research Laboratories 1-3, Tatara-miyakodani, Kyotanabe,
</affiliation>
<address confidence="0.602076">
2-2-2, Hikaridai, &amp;quot;Keihanna Science City&amp;quot;, Kyoto, 610-0394, Japan
Kyoto, 619-0288, Japan myanagid@mail.doshisha.ac.jp
</address>
<email confidence="0.986536">
seiichi.yamamoto@atr.co.jp
</email>
<sectionHeader confidence="0.994233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989188">
The main goal of this paper is to pro-
pose automatic schemes for the trans-
lation paired comparison method. This
method was proposed to precisely eval-
uate a speech translation system&apos;s capa-
bility. Furthermore, the method gives an
objective evaluation result, i.e., a score
of the Test of English for International
Communication (TOEIC). The TOEIC
score is used as a measure of one&apos;s
speech translation capability. However,
this method requires tremendous eval-
uation costs. Accordingly, automatiza-
tion of this method is an important sub-
ject for study. In the proposed method,
currently available automatic evaluation
methods are applied to automate the
translation paired comparison method.
In the experiments, several automatic
evaluation methods (BLEU, NIST, DP-
based method) are applied. The exper-
imental results of these automatic mea-
sures show a good correlation with eval-
uation results of the translation paired
comparison method.
</bodyText>
<sectionHeader confidence="0.936141" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9620815625">
ATR Interpreting Telecommunications Research
Laboratories (ATR-ITL) developed the ATR-
MATRIX (ATR&apos;s Multilingual Automatic Trans-
lation System for Information Exchange) speech
translation system (Takezawa et al., 1998),
which translates both ways between English and
Japanese. ATR-ITL has also been carrying out
comprehensive evaluations of this system through
dialog tests and analyses and has shown the effec-
tiveness of the system for basic travel conversa-
tion (Sugaya et al., 1999).
These experiences, however, indicated that it
would be difficult to enlarge the evaluation target
domain/task by simply adopting the dialog tests
which is employed in the same way for ATR-
MATRIX. Additional measures would be neces-
</bodyText>
<page confidence="0.996677">
371
</page>
<figureCaption confidence="0.946133">
Figure 1: Diagram of translation paired compari-
son method
</figureCaption>
<bodyText confidence="0.998396322580645">
sary in the design of an expanded system in order
to meet performance expectations.
Sugaya et al. (2000) proposed the translation
paired comparison method, which is applicable to
precise evaluation of speech translation systems
with a limited task/domain capability. A major
disadvantage of the translation paired comparison
method is its subjective approach to evaluation.
Such an approach requires large costs and a long
evaluation time. Therefore, automatization of this
method remains an important issue to solve.
Several automatic evaluation methods have
been proposed to achieve efficient development of
MT technology, (Su et al., 1992; Papineni et al.,
2002; NIST, 2002). Both subjective and automatic
evaluation methods are useful for making compar-
isons among different schemes or systems. How-
ever, these techniques are unable to objectively
measure the performance of practical target appli-
cation systems.
In this paper, we propose an automatization
scheme for the translation paired comparison
method that employs available automatic evalua-
tion methods.
Section 2 explains the translation paired com-
parison method, and Section 3 introduces the pro-
posed evaluation scheme. Section 4 describes sev-
eral automatic evaluation methods applied to the
proposed method. Section 5 presents the evalu-
ation results obtained by the proposed methods.
Section 6 presents our conclusions.
</bodyText>
<sectionHeader confidence="0.966363" genericHeader="method">
2 Translation Paired Comparison
Method
</sectionHeader>
<bodyText confidence="0.998846568627451">
The translation paired comparison method can
precisely measure the capability of a speech trans-
lation system. A brief description of the method is
given in this section.
Figure 1 shows a diagram of the translation
paired comparison method in the case of Japanese
to English translation. The Japanese native-
speaking examinees are asked to listen to spo-
ken Japanese text and then write its English trans-
lation on paper. The Japanese text is presented
twice within one minute, with a pause between
the presentations. To measure the English capa-
bility of the Japanese native speakers, the TOEIC
score (TOEIC, 2002) is used. The examinees arc
asked to present an official TOEIC score certifi-
cate confirming that they have officially taken the
test within the past six months.
In the translation paired comparison method,
the translations by the examinees and the outputs
of the system are printed in rows together with the
original Japanese text to form evaluation sheets
for comparison by an evaluator, who is a bilingual
speaker of English and Japanese. Each transcribed
utterance on the evaluation sheets is represented
by the Japanese test text and the two translation
results (i.e., translations by an examinee and by
the system).
The evaluator is asked to follow the procedure
depicted in Figure 2. The meanings of ranks in
the figure are as follows: (A) Perfect: no problem
in both information and grammar; (B) Fair: easy-
to-understand with some unimportant information
missing or flawed grammar; (C) Acceptable: bro-
ken but understandable with effort; (D) Nonsense:
important information has been translated incor-
rectly.
In the evaluation process, the human evaluator
ignores misspellings because the capability to be
measured is not English writing but speech trans-
lation.
From the scores based on these rankings, either
the examinee or the system is considered the &amp;quot;win-
ner&amp;quot; for each utterance. If the ranking and the nat-
uralness are the same for an utterance, the compe-
tition is considered &amp;quot;even&amp;quot;.
To prepare the regression analysis, the num-
ber of &amp;quot;even&amp;quot; utterances are divided in half and
equally assigned as system-won utterances and
human-won utterances. Accordingly, we define
the human winning rate (WH) by the following
equation:
</bodyText>
<equation confidence="0.593333">
WH — (Nhuman — 0.5 x Nevem) I Ntotal (1)
</equation>
<figure confidence="0.974105363636364">
hpane, Ter
Text
Tt Evaluation Paired
Human Sheet
Comparison
A curate Te-d
Japanese Recogmtion
,,raNue, SEER&apos;,
Japan, e-to.English
Lanpuage &apos;Laudation
IMIT)
</figure>
<page confidence="0.893194">
372
</page>
<figureCaption confidence="0.952104">
Figure 2: Procedure of comparison by a bilingual
speaker
</figureCaption>
<bodyText confidence="0.999974">
where Ntotai denotes the total number of utter-
ances in the test set, Nh„ represents the num-
ber of human-won utterances, and N,„, indicates
the number of even (non-winner) utterances, i.e.,
no quality difference between the results of the
TDMT and humans. Details of the regression
analysis are given in Section 5.
</bodyText>
<sectionHeader confidence="0.98799" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.9994618">
The first point to explain is how to automatize the
translation paired comparison method. The basic
idea of the proposed method is to substitute the
human evaluation process of the translation paired
comparison method with an automatic evaluation
</bodyText>
<figureCaption confidence="0.948291">
Figure 3: Procedure of Utterance Unit Evaluation
</figureCaption>
<bodyText confidence="0.999762125">
method&apos;. There are two kinds of units to apply an
automatic evaluation method to the automatization
of the translation paired comparison method. One
is an utterance unit, and the other is a test set unit.
The unit of utterance corresponds to the unit of
segment in BLEU and NIST. Similarly, the unit of
the test set corresponds to the unit of document or
system in BLEU and NIST.
</bodyText>
<subsectionHeader confidence="0.998">
3.1 Utterance Unit Evaluation
</subsectionHeader>
<bodyText confidence="0.999944363636364">
The utterance unit evaluation takes roughly the
same procedure as the translation paired compari-
son method. Figure 3 shows the points of differ-
ence between the translation paired comparison
method and the utterance unit evaluation of the
proposed method. The complete flow can be ob-
tained by substituting Figure 3 for the broken line
area of Figure 2. In the regression analysis of the
utterance unit evaluation, the same procedure as
the original translation paired comparison method
is carried out.
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Test Set Unit Evaluation
</subsectionHeader>
<bodyText confidence="0.968753333333333">
In a sense, the test set unit evaluation follows a dif-
ferent procedure from the translation paired com-
parison method and the utterance unit evaluation.
The flow of the test set unit evaluation is shown
in Figure 4. In the regression analysis of the test
set unit evaluation, the evaluation result by an au-
tomatic evaluation method is used instead of WH.
&apos; An automatic evaluation method for the proposed
method does not have to be a certain kind. However, needless
to add, a precise automatic evaluation method is ideal. The
automatic evaluation methods that we applied to the proposed
method are explained in Section 4.
</bodyText>
<figure confidence="0.948031027777778">
Regression Analysis
Set an examinee to be
compared to a system
9
= 0, 0,
&gt;4,
Set a test utterance
to be evaluated
4.
C:1100:4A A, R, 0, or fl is
4.
Evaluate translations
using automatic
ovaluation
V
111,,,,==&amp;quot;
___*
You — ------- No
—_---
..
Yes ,----gystem&apos;s
Translation
is better?
No
373
Set a target
stem or examinee)
to be scored
Apply test set level
evaluation to the target
Have all
examinees and systems
been evaluated?
Yes
Regressi,s,n 1-lnalysis
No
</figure>
<subsectionHeader confidence="0.762528">
4.2 /V -gram Based Method
</subsectionHeader>
<bodyText confidence="0.99888575">
Papi neni et al. (2002) proposed BLEU, which is
an automatic method for evaluating MT quality
using N-gram matching. The National Institute
of Standards and Technology also proposed an
automatic evaluation method called NISI (2002),
which is a modified method of BLEU. Equation 3
is the BLEU score formulation, and Equation 4 is
the NIST score formulation.
</bodyText>
<figure confidence="0.798299">
SBLEU
exp E w,„ log(p) — max (Lr*ef L 0) }
L sys
n=1
</figure>
<figureCaption confidence="0.999175">
Figure 4: Procedure of Test Set Unit Evaluation (3)
</figureCaption>
<sectionHeader confidence="0.984592" genericHeader="method">
4 Automatic Evaluation Method
</sectionHeader>
<bodyText confidence="0.9999055">
In this section, we briefly describe the automatic
evaluation methods that are applied to the pro-
posed method. Basically, these methods are based
on the same idea, that is, to compare the target
translation for evaluation to high-quality human
reference translations. These methods, then, re-
quire a corpus of high-quality human reference
translations.
</bodyText>
<subsectionHeader confidence="0.832242">
4.1 DP-based Method
</subsectionHeader>
<bodyText confidence="0.993498666666667">
The DP score between a translation output and ref-
erences can be calculated by DP matching (Su et
al., 1992; Takezawa et al., 1999) as follows:
</bodyText>
<equation confidence="0.613206">
=1 to all references 1.
max f — — — Di
</equation>
<bodyText confidence="0.9876625">
(2)
where SDP is the DP score, Ti is the total num-
ber of words in reference i, Si is the number of
substitution words for comparing reference i to
the translation output, /i is the number of inserted
words for comparing reference i to the translation
output, and Di is the number of deleted words
for comparing reference i to the translation out-
put. For the test set unit evaluation using the DP
score, we employ the utterance-weighted average
of utterance-level scores.
where
</bodyText>
<equation confidence="0.9725076">
Pn =
CE{Candidates} En_,,ame{c} counteiip (n — gr ant)
E
EcE{Candidates} En—gramE{C} C ount(n — gram)
wn = N-1
</equation>
<bodyText confidence="0.878396285714286">
and
Lf = the number of words in the reference
re
translation that is closest in length to the
translation being scored
L8y8 = the number of words in the translation
being scored
</bodyText>
<equation confidence="0.862400666666666">
SNI ST —
EN { i Eall nil ...nin in sys output inf °(&amp;quot;--it&apos;n)
Eall zu 1 ...wn in sys output (1)
x exp {,3 log2
L„f
(4)
where
info(wi wn) =
log2 (the number of occurence of wi...wn—i)
</equation>
<bodyText confidence="0.86646">
the number of occurence of wi...wn
Lref = the average number of words in a refer-
ence translation, averaged over all reference
translations
L8y8 = the number of words in the translation
being scored
</bodyText>
<equation confidence="0.8939005">
SDP —
1)]
</equation>
<page confidence="0.988346">
374
</page>
<bodyText confidence="0.9997414">
and 13 is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the system
translation is 2/3 of the average number of words
in the reference translation. For Equations 3 and 4,
N indicates the maximum n-gram length.
</bodyText>
<sectionHeader confidence="0.990231" genericHeader="evaluation">
5 Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.998622">
In this section, we show experimental results of
the original translation paired comparison method
and the proposed method.
</bodyText>
<subsectionHeader confidence="0.959198">
5.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999971222222222">
The target system to be evaluated is Transfer
Driven Machine Translation (TDMT) (Takezawa
et al., 1998). TDMT is a language translation sub-
system of the Japanese-to-English speech trans-
lation system ATR-MATRIX. For evaluation of
TDMT, the input included accurate transcriptions.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excepting the 600s, every hundred-
point range has 5 examinees.
The test set consists of 330 utterances in 23 con-
versations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). Consequently,
this test set has different features from written lan-
guage. Most of the utterances in our task contain
fewer words than the unit of segment used so far
in research with BLEU and NIST. One utterance
contains 11.9 words on average. The standard de-
viation of the number of words is 6.5. The shortest
utterance consists of 1 word, and the longest con-
sists of 32 words. This test set was not used to
train the TDMT system.
For the translations of examinees, all mis-
spellings were corrected by humans because, as
mentioned in Section 2, the human evaluator
ignores misspellings in the original translation
paired comparison method.
</bodyText>
<subsectionHeader confidence="0.9993">
5.2 Evaluation Results by Translation Paired
Comparison Method
</subsectionHeader>
<bodyText confidence="0.999906666666667">
Figure 5 shows the results of a comparison be-
tween TDMT and the examinees. Here, the ab-
scissa represents the TOEIC score, and the ordi-
nate represents WH. In this figure, the straight
line indicates the regression line. The capability-
balanced point between the TDMT subsystem and
</bodyText>
<figure confidence="0.998109777777778">
0.7
0.6
0.5
0.4
0.3
0.2
0.1
300 400 500 600 700 800 900
TOEIC score
</figure>
<figureCaption confidence="0.9979975">
Figure 5: Evaluation results using translation
paired comparison method
</figureCaption>
<table confidence="0.999168857142857">
Human Evaluation
System won HLrns5 won Even
BLEU System won 2937 901 1324
.E (Max n-gam length = 2,
B
Number of ref = 16)
Human won 726 1768 842
Even 239 64 769
31ST System won 3158 1255 1629
Lu
0 (Max n-gam length = 5,
NuriThet&apos; of ref = 16)
,till
Human won 730 1477 870
Even 14 1 436
t,.. System won 2592 676 1072
7 DP
ai
(Number of ref = 16)
Human won 1012 1929 1057
Even 298 r 128 866
</table>
<tableCaption confidence="0.7652395">
Table 1: Detailed results of utterance unit evalua-
tion
</tableCaption>
<bodyText confidence="0.999436">
the examinees was determined to be the point at
which the regression line crossed half the total
number of test utterances, i.e., WH of 0.5. In Fig-
ure 5, this point is 705. Consequently, the transla-
tion capability of the language translation system
equals that of an examinee with a score of around
700 points on the TOEIC. We call this point the
system&apos;s TOEIC score.
</bodyText>
<subsectionHeader confidence="0.9799215">
5.3 Evaluation Results of Utterance Unit
Evaluation
</subsectionHeader>
<bodyText confidence="0.9999075">
In their original forms, the maximum n-gram
length for BLEU (N in Equation 3) is set at 4 and
that for NIST (N in Equation 4) is set at 5. These
settings were established for evaluation of written
language. However, utterances in our test set con-
tain fewer words than in typical written language.
Consequently, for the utterance unit evaluation, we
conducted several experiments while varying N
from 1 to 4 for BLEU and from 1 to 5 for NIST.
Table 1 shows the detailed results of the paired
comparison using automatic evaluations. Figure 6
shows experimental results of the utterance unit
</bodyText>
<figure confidence="0.974536052631579">
•
375
_J
0.38411
Aili ALA_
OT
; ; ) ., L 6, 1 to to
N ,5T ,., I- I &apos; 4 J .,
7 7 4 I- CO Fi F- I--
1: Lui 7 CO Z I- CO CO
COCO Lui Z CO CO CO
Z
0.58
056
0.54
0.52
0.5
III
11
LsreAsesiol
0.48
2 0.46
0.44
° 0.42
0.4
0.38
0
0.58
0.56 7=1
0.54
0.57
0.5 :-
0.48 7
0.46
0.44 H-
0.42 7-
a — _
BLEU (1—gram
</figure>
<figureCaption confidence="0.999908">
Figure 6: Correct ratio of utterance unit evaluation Figure 7: Correct ratio of utterance unit evaluation
</figureCaption>
<bodyText confidence="0.994298833333333">
(Number of references = 1) (Number of references = 16)
evaluation. In this figure, the abscissa represents
the automatic evaluation method used and the n -
gram length, and the ordinate represents the cor-
rect ratio (Reo„,t) calculated by the following
equation:
</bodyText>
<equation confidence="0.963678">
1?r0rrect — Ucorrect I Utotal (5)
</equation>
<bodyText confidence="0.999957657894737">
where Utotai is the total number of translation pairs
consisting of the examinees&apos; translation and the
system&apos;s translation (330 utterances x 29 exam-
inees = 9570 pairs) and U correct is the number
of pairs where the automatic evaluation gives the
same evaluation result as that of the human eval-
uator. The difference between Figures 6 and 7 is
the number of references to be used for automatic
evaluation. In Figure 6, there is 1 reference per ut-
terance, while in Figure 7 there are 16 references
per utterance. In these figures, values in paren-
theses under the abscissa indicate the maximum
n-gram length.
Looking at these figures, the correct ratio of
BLEU changes value depending on the maximum
n-gram length. The maximum n-gram length of
1 or 2 yields a high correct ratio, and that of 3 or
4 yields a low correct ratio. On the other hand,
the correct ratio of NIST is not influenced by the
maximum n-gram length. It seems reasonable to
suppose that these phenomena are due to compu-
tation of the mean of n-gram matching. As shown
in Equations 3 and 4, BLEU applies a geometric
mean and NIST applies an information-weighted
arithmetic mean. Computation of the geometric
mean yields 0 when one of the factors is 0, i.e.,
the BLEU score takes 0 for all of the utterances
whose word count is less than the maximum n-
gram length.
The correct ratio shown in Figures 6 and 7 is
low, i.e., around 0.5. Thus, even state-of-the-
art technology is insufficient to determine better
translation in the utterance unit evaluation. For
a sufficient result of the utterance unit evalua-
tion, we need a more precise automatic evaluation
method or another scheme, for example, major-
ity decision using multiple automatic evaluation
methods.
</bodyText>
<subsectionHeader confidence="0.944416">
5.4 Evaluation Results of Test Set Unit
Evaluation
</subsectionHeader>
<bodyText confidence="0.999098">
In the original BLEU or NIST formulation of the
test set unit (or document or system level) eval-
uation, n-gram matches are computed at the ut-
terance level, but the mean of n-gram matches is
computed at the test-set level. However, consid-
ering the characteristics of the translation paired
comparison method, the average of the utterance-
level scores might be more suitable. Therefore,
we carried out experiments using both the origi-
nal formulation and the average of utterance-level
scores. For the average of utterance-level scores,
considering the experimental results shown in Fig-
ure 7, we used the maximum n-gram length of 2
for BLEU and 5 for NIST.
Figure 8 shows the correlation between auto-
matic measures and WH. In this figure, the ab-
scissa represents the number of references used for
automatic evaluation, and the ordinate represents
</bodyText>
<page confidence="0.998214">
376
</page>
<figureCaption confidence="0.985562">
Figure 8: Correlation between automatic measures
and WH
</figureCaption>
<figure confidence="0.759512">
2 4 8 16
Number of references
</figure>
<figureCaption confidence="0.982176">
Figure 9: Correlation between automatic measures
and TOEIC score
</figureCaption>
<bodyText confidence="0.99719">
correlation. On the other hand, Figure 9 shows
the correlation between automatic measures and
TOEIC score. In this figure, the abscissa and the
ordinate represent the variable as Figure 8.
Figure 10 shows the system&apos;s TOEIC score us-
ing the proposed method. Here, the number of
references is 16. In this figure, the ordinate rep-
resents the system&apos;s TOEIC score, and the broken
line represents the system&apos;s TOEIC score using the
original translation paired comparison method.
In Figures 8, 9 and 10, white bars indicate the
results using the original BLEU score, black bars
indicate the results using the original NIST score,
and gray bars indicate the results using the DP-
based method. The bars with lines indicate the re-
sults using the original BLEU or NIST score, and
those without lines indicate the results using the
average of utterance-level scores.
When we choose an automatic evaluation
Figure 10: System&apos;s TOEIC score by proposed
method
method to apply to the proposed method, there
are two points that needs to be considered. One
is the ability to precisely evaluate human transla-
tions. This ability can be evaluated by the results
in Figures 8 and 9, and it affects confidence inter-
val2 of the system &apos;s TOEIC score. The other point
to consider is the evaluation bias from the human&apos;s
translation to the system&apos;s translation. This affects
system&apos;s actual TOEIC score, which is shown in
Figure 10.
Looking at Figures 8 and 9, all of the auto-
matic measures correlate highly with both WH
and TOEIC score. In particular, the averaged
utterance-level BLEU score shows the highest cor-
relation. However, looking at Figure 10, the sys-
tem&apos;s TOEIC score using this measure deviates
from that of the original translation paired com-
parison method.
From the viewpoint of the system&apos;s TOEIC
score, the DP-based method gives the best result
at 708 points, while the original translation paired
comparison method yielded a score of 705. The
original BLEU also gives a good result at a system
TOEIC score of 712.
Considering the reductions in the evaluation
costs and time, this automatic scheme shows a
good performance and thus is very promising.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9985345">
We proposed automatic schemes for the transla-
tion paired comparison method. In the experi-
</bodyText>
<footnote confidence="0.974206">
2The formula of the confidence interval is mentioned
in the original paper of the translation paired comparison
method (Sugaya et al., 2000).
</footnote>
<figure confidence="0.982706297297297">
2 4
8
Number of references
0.9
-I.
Il BLEU IOnginaD
BLEU ( 2-gram, utterance rnean)-.
NIST (Original)
NIST (5-gram, utterance mean)
DP
0.8
C:1
0.6
16
i Ii
CI BLEU Original)
IIBLEU 2-gram, utterance mean
• NIST (Original)
• NIST (5-gram, utterance mean)
la DP
ilf 11111
Correlation (TOEIC score)
0
0
900
850
g 800
BL EU (0r 1i
BLEU(2-grarn. utterance meanA_
MST ;Orginn0
NIST (5-gram, II terance mean)
11- IN/MIL
UMW REEK -miff
750
&apos;E 700
c.&amp;quot;1&amp;quot;, 650
600
</figure>
<page confidence="0.990391">
377
</page>
<bodyText confidence="0.999972105263158">
ments, we applied currently available automatic
evaluation methods: BLEU, NIST and a DP-based
method. The target system evaluated was TDMT.
We carried out two experiments: an utterance unit
evaluation and a test set unit evaluation. Accord-
ing to the evaluation results, the utterance unit
evaluation was insufficient to automatize the trans-
lation paired comparison method.
However, the test set unit evaluation using the
DP-based method and the original BLEU gave
good evaluation results. The system&apos;s TOEIC
score using the DP-based method was 708 and that
using BLEU was 712, while the original trans-
lation paired comparison method gave a score
around of 705.
To confirm the general effectiveness of the
proposed method, we are conducting experiments
on another system as well as the opposite transla-
tion direction, i.e., English to Japanese translation.
</bodyText>
<sectionHeader confidence="0.997492" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998385875">
The research reported here was supported
in part by a contract with the Telecom-
munications Advancement Organization of
Japan entitled, &amp;quot;A study of speech dialogue
translation technology based on a large
corpus&amp;quot;. It was also supported in part by
the Academic Frontier Project promoted by
Doshisha University.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997539851851852">
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http: //www.nist goy/speech/
tests/mt/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-J.
Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 311-318.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceed-
ings of the 14th International Conference
on Computational Linguistics(COLING),
pages 433-439.
F. Sugaya, T. Takezawa, A. Yokoo, and S. Ya-
mamoto. 1999. End-to-end evaluation in
ATR-MATRIX: speech translation system
between English and Japanese. In Proceed-
ings of Eurospeech, pages 2431-2434.
F. Sugaya, T. Takezawa, A. Yokoo, Y. Sag-
isaka, and S. Yamamoto. 2000. Evalua-
tion of the atr-matrix speech translation sys-
tem with a paired comparison method be-
tween the system and humans. In Proceed-
ings of International Conference on Spo-
ken Language Processing (ICSLP), pages
1105-1108.
T. Takezawa, T. Morimoto, Y. Sagisaka,
N. Campbell, H. Iida, F. Sugaya, A. Yokoo,
and S. Yamamoto. 1998. A Japanese-to-
English speech translation system: ATR-
MATRIX. In Proceedings of International
Conference on Spoken Language Process-
ing (ICSLP), pages 2779-2782.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method
for speech translation systems and a case
study on ATR-MATRIX from Japanese to
English. In Proceeding of Machine Trans-
lation Summit (MT Summit), pages 299-
307.
T. Takezawa. 1999. Building a bilin-
gual travel conversation database for speech
translation research. In Proceedings of the
2nd International Workshop on East-Asian
Language Resources and Evaluation — Ori-
ental COCOSDA Workshop &apos;99 —, pages
17-20.
TOEIC. 2002. Test of English
for International Communication.
http: //www.toeic . com/.
</reference>
<page confidence="0.998289">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117938">
<title confidence="0.9971235">Applications of Automatic Evaluation Methods to Measuring a Capability of Speech Translation System</title>
<author confidence="0.999634">Keiji Yasuda</author>
<affiliation confidence="0.999531">ATR Spoken Language Translation Research Laboratories</affiliation>
<address confidence="0.98522">2-2-2, Hikaridai,&amp;quot;Keihanna Science City&amp;quot;, Kyoto, 619-0288, Japan</address>
<email confidence="0.526831">keiji.yasuda@atr.co.jp</email>
<note confidence="0.571715">Also at Graduate School of Engineering, Doshisha University</note>
<author confidence="0.939146">Fumiaki Sugaya</author>
<affiliation confidence="0.978781">Laboratorie</affiliation>
<address confidence="0.993401">2-1-15, Ohara, Kamifukuoka-city, Saitama, 356-8502, Japan</address>
<email confidence="0.990366">fsugaya@kddilabs.jp</email>
<author confidence="0.960653">Toshiyuki Takezawa</author>
<affiliation confidence="0.890550666666667">ATR Spoken Language Translation Research Laboratories &amp;quot;Keihanna Science City&amp;quot;,</affiliation>
<address confidence="0.988601">Kyoto, 619-0288, Japan</address>
<author confidence="0.994147">Seiichi Yamamoto Masuzo Yanagida</author>
<affiliation confidence="0.877774">ATR Spoken Language Doshisha University Research Laboratories Kyotanabe,</affiliation>
<address confidence="0.968567">2-2-2, Hikaridai, &amp;quot;Keihanna Science City&amp;quot;, Kyoto, 610-0394, Japan</address>
<author confidence="0.795079">Japan myanagidmail doshisha ac jp</author>
<email confidence="0.769629">seiichi.yamamoto@atr.co.jp</email>
<abstract confidence="0.999389576923077">The main goal of this paper is to propose automatic schemes for the translation paired comparison method. This method was proposed to precisely evaluate a speech translation system&apos;s capability. Furthermore, the method gives an objective evaluation result, i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one&apos;s speech translation capability. However, this method requires tremendous evaluation costs. Accordingly, automatization of this method is an important subject for study. In the proposed method, currently available automatic evaluation methods are applied to automate the translation paired comparison method. In the experiments, several automatic evaluation methods (BLEU, NIST, DPmethod) are applied. The experimental results of these automatic measures show a good correlation with evaluation results of the translation paired comparison method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurence Statistics. http: //www.nist goy/speech/ tests/mt/mt2001/resource/.</title>
<date>2002</date>
<contexts>
<context position="3270" citStr="NIST, 2002" startWordPosition="456" endWordPosition="457">nce expectations. Sugaya et al. (2000) proposed the translation paired comparison method, which is applicable to precise evaluation of speech translation systems with a limited task/domain capability. A major disadvantage of the translation paired comparison method is its subjective approach to evaluation. Such an approach requires large costs and a long evaluation time. Therefore, automatization of this method remains an important issue to solve. Several automatic evaluation methods have been proposed to achieve efficient development of MT technology, (Su et al., 1992; Papineni et al., 2002; NIST, 2002). Both subjective and automatic evaluation methods are useful for making comparisons among different schemes or systems. However, these techniques are unable to objectively measure the performance of practical target application systems. In this paper, we propose an automatization scheme for the translation paired comparison method that employs available automatic evaluation methods. Section 2 explains the translation paired comparison method, and Section 3 introduces the proposed evaluation scheme. Section 4 describes several automatic evaluation methods applied to the proposed method. Sectio</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurence Statistics. http: //www.nist goy/speech/ tests/mt/mt2001/resource/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3257" citStr="Papineni et al., 2002" startWordPosition="452" endWordPosition="455"> order to meet performance expectations. Sugaya et al. (2000) proposed the translation paired comparison method, which is applicable to precise evaluation of speech translation systems with a limited task/domain capability. A major disadvantage of the translation paired comparison method is its subjective approach to evaluation. Such an approach requires large costs and a long evaluation time. Therefore, automatization of this method remains an important issue to solve. Several automatic evaluation methods have been proposed to achieve efficient development of MT technology, (Su et al., 1992; Papineni et al., 2002; NIST, 2002). Both subjective and automatic evaluation methods are useful for making comparisons among different schemes or systems. However, these techniques are unable to objectively measure the performance of practical target application systems. In this paper, we propose an automatization scheme for the translation paired comparison method that employs available automatic evaluation methods. Section 2 explains the translation paired comparison method, and Section 3 introduces the proposed evaluation scheme. Section 4 describes several automatic evaluation methods applied to the proposed m</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>M-W Wu</author>
<author>J-S Chang</author>
</authors>
<title>A new quantitative quality measure for machine translation systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics(COLING),</booktitle>
<pages>433--439</pages>
<contexts>
<context position="3234" citStr="Su et al., 1992" startWordPosition="448" endWordPosition="451">xpanded system in order to meet performance expectations. Sugaya et al. (2000) proposed the translation paired comparison method, which is applicable to precise evaluation of speech translation systems with a limited task/domain capability. A major disadvantage of the translation paired comparison method is its subjective approach to evaluation. Such an approach requires large costs and a long evaluation time. Therefore, automatization of this method remains an important issue to solve. Several automatic evaluation methods have been proposed to achieve efficient development of MT technology, (Su et al., 1992; Papineni et al., 2002; NIST, 2002). Both subjective and automatic evaluation methods are useful for making comparisons among different schemes or systems. However, these techniques are unable to objectively measure the performance of practical target application systems. In this paper, we propose an automatization scheme for the translation paired comparison method that employs available automatic evaluation methods. Section 2 explains the translation paired comparison method, and Section 3 introduces the proposed evaluation scheme. Section 4 describes several automatic evaluation methods ap</context>
<context position="10154" citStr="Su et al., 1992" startWordPosition="1571" endWordPosition="1574">. SBLEU exp E w,„ log(p) — max (Lr*ef L 0) } L sys n=1 Figure 4: Procedure of Test Set Unit Evaluation (3) 4 Automatic Evaluation Method In this section, we briefly describe the automatic evaluation methods that are applied to the proposed method. Basically, these methods are based on the same idea, that is, to compare the target translation for evaluation to high-quality human reference translations. These methods, then, require a corpus of high-quality human reference translations. 4.1 DP-based Method The DP score between a translation output and references can be calculated by DP matching (Su et al., 1992; Takezawa et al., 1999) as follows: =1 to all references 1. max f — — — Di (2) where SDP is the DP score, Ti is the total number of words in reference i, Si is the number of substitution words for comparing reference i to the translation output, /i is the number of inserted words for comparing reference i to the translation output, and Di is the number of deleted words for comparing reference i to the translation output. For the test set unit evaluation using the DP score, we employ the utterance-weighted average of utterance-level scores. where Pn = CE{Candidates} En_,,ame{c} counteiip (n — </context>
</contexts>
<marker>Su, Wu, Chang, 1992</marker>
<rawString>K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992. A new quantitative quality measure for machine translation systems. In Proceedings of the 14th International Conference on Computational Linguistics(COLING), pages 433-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>A Yokoo</author>
<author>S Yamamoto</author>
</authors>
<title>End-to-end evaluation in ATR-MATRIX: speech translation system between English and Japanese.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>2431--2434</pages>
<contexts>
<context position="2300" citStr="Sugaya et al., 1999" startWordPosition="308" endWordPosition="311">s of these automatic measures show a good correlation with evaluation results of the translation paired comparison method. 1 Introduction ATR Interpreting Telecommunications Research Laboratories (ATR-ITL) developed the ATRMATRIX (ATR&apos;s Multilingual Automatic Translation System for Information Exchange) speech translation system (Takezawa et al., 1998), which translates both ways between English and Japanese. ATR-ITL has also been carrying out comprehensive evaluations of this system through dialog tests and analyses and has shown the effectiveness of the system for basic travel conversation (Sugaya et al., 1999). These experiences, however, indicated that it would be difficult to enlarge the evaluation target domain/task by simply adopting the dialog tests which is employed in the same way for ATRMATRIX. Additional measures would be neces371 Figure 1: Diagram of translation paired comparison method sary in the design of an expanded system in order to meet performance expectations. Sugaya et al. (2000) proposed the translation paired comparison method, which is applicable to precise evaluation of speech translation systems with a limited task/domain capability. A major disadvantage of the translation </context>
</contexts>
<marker>Sugaya, Takezawa, Yokoo, Yamamoto, 1999</marker>
<rawString>F. Sugaya, T. Takezawa, A. Yokoo, and S. Yamamoto. 1999. End-to-end evaluation in ATR-MATRIX: speech translation system between English and Japanese. In Proceedings of Eurospeech, pages 2431-2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>A Yokoo</author>
<author>Y Sagisaka</author>
<author>S Yamamoto</author>
</authors>
<title>Evaluation of the atr-matrix speech translation system with a paired comparison method between the system and humans.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>1105--1108</pages>
<contexts>
<context position="2697" citStr="Sugaya et al. (2000)" startWordPosition="372" endWordPosition="375">h and Japanese. ATR-ITL has also been carrying out comprehensive evaluations of this system through dialog tests and analyses and has shown the effectiveness of the system for basic travel conversation (Sugaya et al., 1999). These experiences, however, indicated that it would be difficult to enlarge the evaluation target domain/task by simply adopting the dialog tests which is employed in the same way for ATRMATRIX. Additional measures would be neces371 Figure 1: Diagram of translation paired comparison method sary in the design of an expanded system in order to meet performance expectations. Sugaya et al. (2000) proposed the translation paired comparison method, which is applicable to precise evaluation of speech translation systems with a limited task/domain capability. A major disadvantage of the translation paired comparison method is its subjective approach to evaluation. Such an approach requires large costs and a long evaluation time. Therefore, automatization of this method remains an important issue to solve. Several automatic evaluation methods have been proposed to achieve efficient development of MT technology, (Su et al., 1992; Papineni et al., 2002; NIST, 2002). Both subjective and autom</context>
<context position="20806" citStr="Sugaya et al., 2000" startWordPosition="3429" endWordPosition="3432">of the system&apos;s TOEIC score, the DP-based method gives the best result at 708 points, while the original translation paired comparison method yielded a score of 705. The original BLEU also gives a good result at a system TOEIC score of 712. Considering the reductions in the evaluation costs and time, this automatic scheme shows a good performance and thus is very promising. 6 Conclusions We proposed automatic schemes for the translation paired comparison method. In the experi2The formula of the confidence interval is mentioned in the original paper of the translation paired comparison method (Sugaya et al., 2000). 2 4 8 Number of references 0.9 -I. Il BLEU IOnginaD BLEU ( 2-gram, utterance rnean)-. NIST (Original) NIST (5-gram, utterance mean) DP 0.8 C:1 0.6 16 i Ii CI BLEU Original) IIBLEU 2-gram, utterance mean • NIST (Original) • NIST (5-gram, utterance mean) la DP ilf 11111 Correlation (TOEIC score) 0 0 900 850 g 800 BL EU (0r 1i BLEU(2-grarn. utterance meanA_ MST ;Orginn0 NIST (5-gram, II terance mean) 11- IN/MIL UMW REEK -miff 750 &apos;E 700 c.&amp;quot;1&amp;quot;, 650 600 377 ments, we applied currently available automatic evaluation methods: BLEU, NIST and a DP-based method. The target system evaluated was TDMT. W</context>
</contexts>
<marker>Sugaya, Takezawa, Yokoo, Sagisaka, Yamamoto, 2000</marker>
<rawString>F. Sugaya, T. Takezawa, A. Yokoo, Y. Sagisaka, and S. Yamamoto. 2000. Evaluation of the atr-matrix speech translation system with a paired comparison method between the system and humans. In Proceedings of International Conference on Spoken Language Processing (ICSLP), pages 1105-1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>T Morimoto</author>
<author>Y Sagisaka</author>
<author>N Campbell</author>
<author>H Iida</author>
<author>F Sugaya</author>
<author>A Yokoo</author>
<author>S Yamamoto</author>
</authors>
<title>A Japanese-toEnglish speech translation system: ATRMATRIX.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>2779--2782</pages>
<contexts>
<context position="2034" citStr="Takezawa et al., 1998" startWordPosition="266" endWordPosition="269">study. In the proposed method, currently available automatic evaluation methods are applied to automate the translation paired comparison method. In the experiments, several automatic evaluation methods (BLEU, NIST, DPbased method) are applied. The experimental results of these automatic measures show a good correlation with evaluation results of the translation paired comparison method. 1 Introduction ATR Interpreting Telecommunications Research Laboratories (ATR-ITL) developed the ATRMATRIX (ATR&apos;s Multilingual Automatic Translation System for Information Exchange) speech translation system (Takezawa et al., 1998), which translates both ways between English and Japanese. ATR-ITL has also been carrying out comprehensive evaluations of this system through dialog tests and analyses and has shown the effectiveness of the system for basic travel conversation (Sugaya et al., 1999). These experiences, however, indicated that it would be difficult to enlarge the evaluation target domain/task by simply adopting the dialog tests which is employed in the same way for ATRMATRIX. Additional measures would be neces371 Figure 1: Diagram of translation paired comparison method sary in the design of an expanded system </context>
<context position="11907" citStr="Takezawa et al., 1998" startWordPosition="1884" endWordPosition="1887">over all reference translations L8y8 = the number of words in the translation being scored SDP — 1)] 374 and 13 is chosen to make the brevity penalty factor=0.5 when the number of words in the system translation is 2/3 of the average number of words in the reference translation. For Equations 3 and 4, N indicates the maximum n-gram length. 5 Evaluation Experiments In this section, we show experimental results of the original translation paired comparison method and the proposed method. 5.1 Experimental Conditions The target system to be evaluated is Transfer Driven Machine Translation (TDMT) (Takezawa et al., 1998). TDMT is a language translation subsystem of the Japanese-to-English speech translation system ATR-MATRIX. For evaluation of TDMT, the input included accurate transcriptions. The total number of examinees is 29, and the range of their TOEIC score is between the 300s and 800s. Excepting the 600s, every hundredpoint range has 5 examinees. The test set consists of 330 utterances in 23 conversations from the ATR bilingual travel conversation database (Takezawa, 1999). Consequently, this test set has different features from written language. Most of the utterances in our task contain fewer words t</context>
</contexts>
<marker>Takezawa, Morimoto, Sagisaka, Campbell, Iida, Sugaya, Yokoo, Yamamoto, 1998</marker>
<rawString>T. Takezawa, T. Morimoto, Y. Sagisaka, N. Campbell, H. Iida, F. Sugaya, A. Yokoo, and S. Yamamoto. 1998. A Japanese-toEnglish speech translation system: ATRMATRIX. In Proceedings of International Conference on Spoken Language Processing (ICSLP), pages 2779-2782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>F Sugaya</author>
<author>A Yokoo</author>
<author>S Yamamoto</author>
</authors>
<title>A new evaluation method for speech translation systems and a case study on ATR-MATRIX from Japanese to English.</title>
<date>1999</date>
<booktitle>In Proceeding of Machine Translation Summit (MT Summit),</booktitle>
<pages>299--307</pages>
<contexts>
<context position="10178" citStr="Takezawa et al., 1999" startWordPosition="1575" endWordPosition="1578"> log(p) — max (Lr*ef L 0) } L sys n=1 Figure 4: Procedure of Test Set Unit Evaluation (3) 4 Automatic Evaluation Method In this section, we briefly describe the automatic evaluation methods that are applied to the proposed method. Basically, these methods are based on the same idea, that is, to compare the target translation for evaluation to high-quality human reference translations. These methods, then, require a corpus of high-quality human reference translations. 4.1 DP-based Method The DP score between a translation output and references can be calculated by DP matching (Su et al., 1992; Takezawa et al., 1999) as follows: =1 to all references 1. max f — — — Di (2) where SDP is the DP score, Ti is the total number of words in reference i, Si is the number of substitution words for comparing reference i to the translation output, /i is the number of inserted words for comparing reference i to the translation output, and Di is the number of deleted words for comparing reference i to the translation output. For the test set unit evaluation using the DP score, we employ the utterance-weighted average of utterance-level scores. where Pn = CE{Candidates} En_,,ame{c} counteiip (n — gr ant) E EcE{Candidates</context>
</contexts>
<marker>Takezawa, Sugaya, Yokoo, Yamamoto, 1999</marker>
<rawString>T. Takezawa, F. Sugaya, A. Yokoo, and S. Yamamoto. 1999. A new evaluation method for speech translation systems and a case study on ATR-MATRIX from Japanese to English. In Proceeding of Machine Translation Summit (MT Summit), pages 299-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
</authors>
<title>Building a bilingual travel conversation database for speech translation research.</title>
<date>1999</date>
<booktitle>In Proceedings of the 2nd International Workshop on East-Asian Language Resources and Evaluation — Oriental COCOSDA Workshop &apos;99 —,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="12375" citStr="Takezawa, 1999" startWordPosition="1961" endWordPosition="1962">roposed method. 5.1 Experimental Conditions The target system to be evaluated is Transfer Driven Machine Translation (TDMT) (Takezawa et al., 1998). TDMT is a language translation subsystem of the Japanese-to-English speech translation system ATR-MATRIX. For evaluation of TDMT, the input included accurate transcriptions. The total number of examinees is 29, and the range of their TOEIC score is between the 300s and 800s. Excepting the 600s, every hundredpoint range has 5 examinees. The test set consists of 330 utterances in 23 conversations from the ATR bilingual travel conversation database (Takezawa, 1999). Consequently, this test set has different features from written language. Most of the utterances in our task contain fewer words than the unit of segment used so far in research with BLEU and NIST. One utterance contains 11.9 words on average. The standard deviation of the number of words is 6.5. The shortest utterance consists of 1 word, and the longest consists of 32 words. This test set was not used to train the TDMT system. For the translations of examinees, all misspellings were corrected by humans because, as mentioned in Section 2, the human evaluator ignores misspellings in the origi</context>
</contexts>
<marker>Takezawa, 1999</marker>
<rawString>T. Takezawa. 1999. Building a bilingual travel conversation database for speech translation research. In Proceedings of the 2nd International Workshop on East-Asian Language Resources and Evaluation — Oriental COCOSDA Workshop &apos;99 —, pages 17-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TOEIC</author>
</authors>
<title>Test of English for International Communication. http: //www.toeic .</title>
<date>2002</date>
<tech>com/.</tech>
<contexts>
<context position="4625" citStr="TOEIC, 2002" startWordPosition="664" endWordPosition="665">d The translation paired comparison method can precisely measure the capability of a speech translation system. A brief description of the method is given in this section. Figure 1 shows a diagram of the translation paired comparison method in the case of Japanese to English translation. The Japanese nativespeaking examinees are asked to listen to spoken Japanese text and then write its English translation on paper. The Japanese text is presented twice within one minute, with a pause between the presentations. To measure the English capability of the Japanese native speakers, the TOEIC score (TOEIC, 2002) is used. The examinees arc asked to present an official TOEIC score certificate confirming that they have officially taken the test within the past six months. In the translation paired comparison method, the translations by the examinees and the outputs of the system are printed in rows together with the original Japanese text to form evaluation sheets for comparison by an evaluator, who is a bilingual speaker of English and Japanese. Each transcribed utterance on the evaluation sheets is represented by the Japanese test text and the two translation results (i.e., translations by an examinee</context>
</contexts>
<marker>TOEIC, 2002</marker>
<rawString>TOEIC. 2002. Test of English for International Communication. http: //www.toeic . com/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>